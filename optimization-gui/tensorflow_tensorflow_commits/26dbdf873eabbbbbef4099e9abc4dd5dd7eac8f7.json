{
    "author": "tensorflower-gardener",
    "message": "Implement SPMD DUS as select + pad + collective-permute instead of all-gather. This will remove some unnecessary data copying between devices.\n\nPiperOrigin-RevId: 830624398",
    "sha": "26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7",
    "files": [
        {
            "sha": "dbf8fae5395f777b43ecf3fc281b903c301f4712",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7",
            "patch": "@@ -276,6 +276,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_multiheap_size_constraint_per_heap(-1);\n   opts.set_xla_detailed_logging(true);\n   opts.set_xla_enable_dumping(true);\n+  opts.set_xla_enable_enzyme_comms_opt(false);\n \n   opts.set_xla_gpu_enable_dynamic_slice_fusion(false);\n   opts.set_xla_gpu_nccl_termination_timeout_seconds(-1);\n@@ -1935,6 +1936,12 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n           debug_options->xla_gpu_experimental_pipeline_parallelism_opt_level()),\n       \"Experimental optimizations for SPMD-based pipeline parallelism on \"\n       \"GPU.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_enable_enzyme_comms_opt\",\n+      bool_setter_for(&DebugOptions::set_xla_enable_enzyme_comms_opt),\n+      debug_options->xla_enable_enzyme_comms_opt(),\n+      \"Enable communication optimization patterns specified in Enzyme. More \"\n+      \"details in http://shortn/_jXJ2VFoyMN.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_partitioning_algorithm\", setter_for_xla_partitioning_algorithm,\n       DebugOptions::PartitioningAlgorithm_Name("
        },
        {
            "sha": "9f71267c476f5cd8a51d8261277d831039485624",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 211,
            "deletions": 126,
            "changes": 337,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7",
            "patch": "@@ -3721,17 +3721,57 @@ absl::Status SpmdPartitioningVisitor::HandleDynamicSlice(HloInstruction* hlo) {\n   return absl::OkStatus();\n }\n \n+HloInstruction* PadHelper(SpmdPartitioningVisitor& visitor,\n+                          PartitionedHlo operand,\n+                          HloInstruction* replicated_padding_value,\n+                          const PaddingConfig& padding_config,\n+                          const Shape& base_shape,\n+                          const HloSharding& sharding) {\n+  if (sharding.IsTileMaximal()) {\n+    return nullptr;\n+  }\n+\n+  std::optional<PartitionedHlo::WindowedInputShardReturnValue> reshard_operand =\n+      ReshardDataForPad(replicated_padding_value, padding_config, operand,\n+                        sharding, visitor.builder());\n+\n+  if (!reshard_operand.has_value()) {\n+    return nullptr;\n+  }\n+\n+  HloInstruction* sharded_pad = PadDataFromWindowReshard(\n+      *reshard_operand, replicated_padding_value, visitor.builder());\n+\n+  if (!reshard_operand->dynamic_slice_index_on_output) {\n+    return sharded_pad;\n+  }\n+  Shape shard_shape = MakePartitionedShape(base_shape, sharding);\n+\n+  HloInstruction* result =\n+      visitor.builder()->AddInstruction(HloInstruction::CreateDynamicSlice(\n+          shard_shape, sharded_pad,\n+          *reshard_operand->dynamic_slice_index_on_output,\n+          shard_shape.dimensions()));\n+  return result;\n+}\n+\n+// TODO: b/457492726 - Simplify HandleDynamicUpdateSlice in spmd_partitioner.cc\n absl::Status SpmdPartitioningVisitor::HandleDynamicUpdateSlice(\n     HloInstruction* hlo) {\n   if (hlo->sharding().IsTileMaximal()) {\n     return DefaultAction(hlo);\n   }\n+  auto add_hlo = [&](std::unique_ptr<HloInstruction> to_add) {\n+    return b_.AddInstruction(std::move(to_add));\n+  };\n+  const HloInstruction* input_tensor = hlo->operand(0);\n+  const HloInstruction* update_tensor = hlo->operand(1);\n \n   std::vector<HloInstruction*> new_indices;\n   new_indices.reserve(hlo->shape().dimensions().size());\n   for (int64_t i = 0; i < hlo->shape().dimensions().size(); ++i) {\n     const HloInstruction* index = hlo->operand(i + 2);\n-    if (hlo->operand(1)->shape().dimensions(i) == hlo->shape().dimensions(i)) {\n+    if (update_tensor->shape().dimensions(i) == hlo->shape().dimensions(i)) {\n       new_indices.emplace_back(CreateZero(index->shape(), &b_));\n     } else {\n       // Replicate the indices.\n@@ -3742,12 +3782,8 @@ absl::Status SpmdPartitioningVisitor::HandleDynamicUpdateSlice(\n   DynamicUpdateSliceAnalysis analysis = AnalyzeDynamicUpdateSlice(hlo);\n \n   // Method 1. Replicate the slice dimensions for all involved tensors.\n-  // TODO(b/407610806). Add support if all partitioned slice dimensions have\n-  // constant indices.\n-  if (analysis.method == DynamicUpdateSliceMethod::kDefault ||\n-      analysis.method == DynamicUpdateSliceMethod::\n-                             kAllPartitionedSliceDimsHaveConstantIndices) {\n-    const HloSharding& input_sharding = hlo->operand(0)->sharding();\n+  if (analysis.method == DynamicUpdateSliceMethod::kDefault) {\n+    const HloSharding& input_sharding = input_tensor->sharding();\n     const HloSharding& output_sharding = hlo->sharding();\n     const HloSharding& better_sharding =\n         input_sharding.NumTiles() > output_sharding.NumTiles()\n@@ -3757,9 +3793,9 @@ absl::Status SpmdPartitioningVisitor::HandleDynamicUpdateSlice(\n     HloSharding replicated_sharding =\n         hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n             better_sharding, analysis.slice_dims);\n-    auto base = GetPartitionedHlo(hlo->operand(0)).Reshard(replicated_sharding);\n+    auto base = GetPartitionedHlo(input_tensor).Reshard(replicated_sharding);\n     auto operand =\n-        GetPartitionedHlo(hlo->operand(1)).Reshard(replicated_sharding);\n+        GetPartitionedHlo(update_tensor).Reshard(replicated_sharding);\n     auto dus = b_.AddInstruction(HloInstruction::CreateDynamicUpdateSlice(\n         base.hlo()->shape(), base.hlo(), operand.hlo(), new_indices));\n     dus->set_sharding(replicated_sharding);\n@@ -3770,104 +3806,169 @@ absl::Status SpmdPartitioningVisitor::HandleDynamicUpdateSlice(\n \n   // Method 2. Keep the sharding for input and output since the update is fully\n   // contained in a single partition.\n-  CHECK(analysis.method == DynamicUpdateSliceMethod::kUpdateOnASinglePartition);\n+  if (analysis.method == DynamicUpdateSliceMethod::kUpdateOnASinglePartition) {\n+    // Get partitioned input.\n+    const auto& dus_sharding = hlo->sharding();\n+    const auto& partitioned_input =\n+        GetPartitionedHlo(input_tensor).Reshard(dus_sharding).hlo();\n \n-  auto add_hlo = [&](std::unique_ptr<HloInstruction> to_add) {\n-    return b_.AddInstruction(std::move(to_add));\n-  };\n+    HloSharding update_sharding =\n+        hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n+            dus_sharding, analysis.slice_dims);\n+\n+    HloInstruction* replicate_update =\n+        GetPartitionedHlo(update_tensor).Reshard(update_sharding).hlo();\n+\n+    const Shape& partitioned_shape = partitioned_input->shape();\n+    std::vector<HloInstruction*> partition_ordinals =\n+        MakeTiledPartitionOrdinals(hlo->sharding(),\n+                                   MakePartitioningState().partition_id, &b_);\n+    HloInstruction* all_dims_within_partition = add_hlo(\n+        HloInstruction::CreateConstant(LiteralUtil::CreateR0<bool>(true)));\n+\n+    for (int64_t dim : analysis.partitioned_slice_dims) {\n+      // Calculate per partition size.\n+      const int64_t per_partition_size = partitioned_shape.dimensions(dim);\n+\n+      // within_partition = (offset >= partition_id * per_partition_size) &&\n+      //                    (offset < (partition_id + 1) * per_partition_size)\n+      const Shape& compare_shape =\n+          ShapeUtil::ChangeElementType(partition_id_->shape(), PRED);\n+      auto per_partition_size_hlo = add_hlo(HloInstruction::CreateConstant(\n+          LiteralUtil::CreateR0<int>(per_partition_size)));\n+      const Shape& offset_shape = per_partition_size_hlo->shape();\n+      const Shape& index_shape = new_indices[dim]->shape();\n+      if (offset_shape.element_type() != index_shape.element_type()) {\n+        new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n+            ShapeUtil::ChangeElementType(index_shape,\n+                                         offset_shape.element_type()),\n+            new_indices[dim]));\n+      }\n+      HloInstruction* partition_offset = add_hlo(HloInstruction::CreateBinary(\n+          offset_shape, HloOpcode::kMultiply, partition_ordinals[dim],\n+          per_partition_size_hlo));\n+      // offset >= partition_id * per_partition_size\n+      HloInstruction* offset_ge = add_hlo(HloInstruction::CreateCompare(\n+          compare_shape, new_indices[dim], partition_offset,\n+          ComparisonDirection::kGe));\n+      // offset < (partition_id + 1) * per_partition_size\n+      HloInstruction* offset_lt = add_hlo(HloInstruction::CreateCompare(\n+          compare_shape, new_indices[dim],\n+          add_hlo(HloInstruction::CreateBinary(\n+              offset_shape, HloOpcode::kMultiply,\n+              add_hlo(HloInstruction::CreateBinary(\n+                  offset_shape, HloOpcode::kAdd, partition_ordinals[dim],\n+                  add_hlo(HloInstruction::CreateConstant(\n+                      LiteralUtil::CreateR0<int>(1))))),\n+              per_partition_size_hlo)),\n+          ComparisonDirection::kLt));\n+      HloInstruction* update_within_partition =\n+          add_hlo(HloInstruction::CreateBinary(compare_shape, HloOpcode::kAnd,\n+                                               offset_ge, offset_lt));\n+\n+      all_dims_within_partition = add_hlo(HloInstruction::CreateBinary(\n+          compare_shape, HloOpcode::kAnd, all_dims_within_partition,\n+          update_within_partition));\n+\n+      // Calculate offset.\n+      // slice dim offset = within_partition ?\n+      //                    offset - partition_id * per_partition_size : 0\n+      new_indices[dim] = add_hlo(HloInstruction::CreateTernary(\n+          new_indices[dim]->shape(), HloOpcode::kSelect,\n+          update_within_partition,\n+          add_hlo(HloInstruction::CreateBinary(\n+              new_indices[dim]->shape(), HloOpcode::kSubtract, new_indices[dim],\n+              partition_offset)),\n+          add_hlo(\n+              HloInstruction::CreateConstant(LiteralUtil::CreateR0<int>(0)))));\n+      if (new_indices[dim]->shape().element_type() !=\n+          index_shape.element_type()) {\n+        new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n+            ShapeUtil::ChangeElementType(new_indices[dim]->shape(),\n+                                         index_shape.element_type()),\n+            new_indices[dim]));\n+      }\n+    }\n \n-  // Get partitioned input.\n-  const auto& dus_sharding = hlo->sharding();\n-  const auto& partitioned_input =\n-      GetPartitionedHlo(hlo->operand(0)).Reshard(dus_sharding).hlo();\n-\n-  auto update_sharding =\n-      hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n-          dus_sharding, analysis.slice_dims);\n-\n-  // TODO(wangtao): use collective permute for sharded update.\n-  HloInstruction* replicate_update =\n-      GetPartitionedHlo(hlo->operand(1)).Reshard(update_sharding).hlo();\n-\n-  const auto& partitioned_shape = partitioned_input->shape();\n-  auto partition_ordinals = MakeTiledPartitionOrdinals(\n-      hlo->sharding(), MakePartitioningState().partition_id, &b_);\n-  HloInstruction* all_dims_within_partition = add_hlo(\n-      HloInstruction::CreateConstant(LiteralUtil::CreateR0<bool>(true)));\n-\n-  for (int64_t dim : analysis.partitioned_slice_dims) {\n-    // Calculate per partition size.\n-    const int64_t per_partition_size = partitioned_shape.dimensions(dim);\n-\n-    // within_partition = (offset >= partition_id * per_partition_size) &&\n-    //                    (offset < (partition_id + 1) * per_partition_size)\n-    const Shape& compare_shape =\n-        ShapeUtil::ChangeElementType(partition_id_->shape(), PRED);\n-    auto per_partition_size_hlo = add_hlo(HloInstruction::CreateConstant(\n-        LiteralUtil::CreateR0<int>(per_partition_size)));\n-    const Shape& offset_shape = per_partition_size_hlo->shape();\n-    const Shape& index_shape = new_indices[dim]->shape();\n-    if (offset_shape.element_type() != index_shape.element_type()) {\n-      new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n-          ShapeUtil::ChangeElementType(index_shape,\n-                                       offset_shape.element_type()),\n-          new_indices[dim]));\n-    }\n-    auto partition_offset = add_hlo(HloInstruction::CreateBinary(\n-        offset_shape, HloOpcode::kMultiply, partition_ordinals[dim],\n-        per_partition_size_hlo));\n-    // offset >= partition_id * per_partition_size\n-    auto offset_ge = add_hlo(HloInstruction::CreateCompare(\n-        compare_shape, new_indices[dim], partition_offset,\n-        ComparisonDirection::kGe));\n-    // offset < (partition_id + 1) * per_partition_size\n-    auto offset_lt = add_hlo(HloInstruction::CreateCompare(\n-        compare_shape, new_indices[dim],\n-        add_hlo(HloInstruction::CreateBinary(\n-            offset_shape, HloOpcode::kMultiply,\n-            add_hlo(HloInstruction::CreateBinary(\n-                offset_shape, HloOpcode::kAdd, partition_ordinals[dim],\n-                add_hlo(HloInstruction::CreateConstant(\n-                    LiteralUtil::CreateR0<int>(1))))),\n-            per_partition_size_hlo)),\n-        ComparisonDirection::kLt));\n-    auto update_within_partition = add_hlo(HloInstruction::CreateBinary(\n-        compare_shape, HloOpcode::kAnd, offset_ge, offset_lt));\n-\n-    all_dims_within_partition = add_hlo(HloInstruction::CreateBinary(\n-        compare_shape, HloOpcode::kAnd, all_dims_within_partition,\n-        update_within_partition));\n-\n-    // Calculate offset.\n-    // slice dim offset = within_partition ?\n-    //                    offset - partition_id * per_partition_size : 0\n-    new_indices[dim] = add_hlo(HloInstruction::CreateTernary(\n-        new_indices[dim]->shape(), HloOpcode::kSelect, update_within_partition,\n-        add_hlo(HloInstruction::CreateBinary(\n-            new_indices[dim]->shape(), HloOpcode::kSubtract, new_indices[dim],\n-            partition_offset)),\n-        add_hlo(\n-            HloInstruction::CreateConstant(LiteralUtil::CreateR0<int>(0)))));\n-    if (new_indices[dim]->shape().element_type() !=\n-        index_shape.element_type()) {\n-      new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n-          ShapeUtil::ChangeElementType(new_indices[dim]->shape(),\n-                                       index_shape.element_type()),\n-          new_indices[dim]));\n-    }\n-  }\n-\n-  // Create dynamic update slice.\n-  auto dus = add_hlo(HloInstruction::CreateDynamicUpdateSlice(\n-      partitioned_shape, partitioned_input, replicate_update, new_indices));\n-  // Select if update is needed.\n-  HloInstruction* select = add_hlo(HloInstruction::CreateTernary(\n-      dus->shape(), HloOpcode::kSelect,\n-      add_hlo(HloInstruction::CreateBroadcast(\n-          ShapeUtil::ChangeElementType(dus->shape(), PRED),\n-          all_dims_within_partition, {})),\n-      dus, partitioned_input));\n-  SetPartitionedHlo(hlo, select);\n+    // Create dynamic update slice.\n+    HloInstruction* dus = add_hlo(HloInstruction::CreateDynamicUpdateSlice(\n+        partitioned_shape, partitioned_input, replicate_update, new_indices));\n+    // Select if update is needed\n+    SetPartitionedHlo(hlo,\n+                      add_hlo(HloInstruction::CreateTernary(\n+                          dus->shape(), HloOpcode::kSelect,\n+                          add_hlo(HloInstruction::CreateBroadcast(\n+                              ShapeUtil::ChangeElementType(dus->shape(), PRED),\n+                              all_dims_within_partition, {})),\n+                          dus, partitioned_input)));\n+    return absl::OkStatus();\n+  }\n+\n+  // Method 3: All partitioned slice dimensions have compile-time constant\n+  // indices.\n+  if (analysis.method == DynamicUpdateSliceMethod::\n+                             kAllPartitionedSliceDimsHaveConstantIndices &&\n+      module_->config().debug_options().xla_enable_enzyme_comms_opt()) {\n+    PaddingConfig padding_config;\n+    for (int64_t input_tensor_dim = 0;\n+         input_tensor_dim < hlo->shape().dimensions().size();\n+         ++input_tensor_dim) {\n+      auto padding_dim = padding_config.add_dimensions();\n+      padding_dim->set_interior_padding(0);\n+\n+      const HloInstruction* dus_index = hlo->operand(input_tensor_dim + 2);\n+      CHECK(dus_index->IsConstant());\n+\n+      int64_t start_index = dus_index->literal().GetIntegralAsS64({}).value();\n+      int64_t end_index =\n+          start_index + update_tensor->shape().dimensions(input_tensor_dim);\n+      int64_t padding_high =\n+          hlo->shape().dimensions(input_tensor_dim) - end_index;\n+      padding_dim->set_edge_padding_low(start_index);\n+      padding_dim->set_edge_padding_high(padding_high);\n+    }\n+\n+    const Shape operand_pred_shape =\n+        ShapeUtil::ChangeElementType(hlo->shape(), PRED);\n+    const Shape update_pred_shape =\n+        ShapeUtil::ChangeElementType(update_tensor->shape(), PRED);\n+    const Shape sharded_update_pred_shape =\n+        MakePartitionedShape(update_pred_shape, hlo->sharding());\n+\n+    auto zeroOperand = CreateZero(sharded_update_pred_shape, &b_);\n+    zeroOperand->set_sharding(hlo->sharding());\n+\n+    HloInstruction* paddingValue = CreateOne(Shape(PRED, {}), &b_);\n+    HloInstruction* maskOp = PadHelper(\n+        *this,\n+        PartitionedHlo(zeroOperand, update_pred_shape, MakePartitioningState()),\n+        paddingValue, padding_config, operand_pred_shape, hlo->sharding());\n+    if (!maskOp) {\n+      maskOp = add_hlo(HloInstruction::CreatePad(\n+          operand_pred_shape, zeroOperand, paddingValue, padding_config));\n+      maskOp->set_sharding(hlo->sharding());\n+    }\n+\n+    auto zeroElemOp = add_hlo(HloInstruction::CreateConstant(\n+        LiteralUtil::Zero(hlo->shape().element_type())));\n+    HloInstruction* newOperand =\n+        PadHelper(*this, GetPartitionedHlo(update_tensor), zeroElemOp,\n+                  padding_config, hlo->shape(), hlo->sharding());\n+    if (!newOperand) {\n+      newOperand = add_hlo(HloInstruction::CreatePad(\n+          hlo->shape(), GetPartitionedHlo(update_tensor).hlo(), zeroElemOp,\n+          padding_config));\n+      newOperand->set_sharding(hlo->sharding());\n+    }\n+\n+    auto shard_result_shape =\n+        MakePartitionedShape(hlo->shape(), hlo->sharding());\n+    auto result = add_hlo(HloInstruction::CreateTernary(\n+        shard_result_shape, HloOpcode::kSelect, maskOp,\n+        GetPartitionedHlo(input_tensor).hlo(), newOperand));\n+    SetPartitionedHlo(hlo, result);\n+    return absl::OkStatus();\n+  }\n   return absl::OkStatus();\n }\n \n@@ -4020,29 +4121,13 @@ absl::Status SpmdPartitioningVisitor::HandleInfeed(HloInstruction* hlo) {\n }\n \n absl::Status SpmdPartitioningVisitor::HandlePad(HloInstruction* hlo) {\n-  if (hlo->sharding().IsTileMaximal()) {\n-    return DefaultAction(hlo);\n-  }\n-  auto lhs = GetPartitionedHlo(hlo->operand(0));\n-  auto replicated_rhs = GetPartitionedHlo(hlo->operand(1)).Replicate().hlo();\n-  auto reshard_operand = ReshardDataForPad(\n-      replicated_rhs, hlo->padding_config(), lhs, hlo->sharding(), &b_);\n-  if (!reshard_operand.has_value()) {\n+  auto result = PadHelper(*this, GetPartitionedHlo(hlo->operand(0)),\n+                          GetPartitionedHlo(hlo->operand(1)).Replicate().hlo(),\n+                          hlo->padding_config(), hlo->shape(), hlo->sharding());\n+  if (!result) {\n     return DefaultAction(hlo);\n   }\n-  auto* sharded_pad =\n-      PadDataFromWindowReshard(*reshard_operand, replicated_rhs, &b_);\n-\n-  SetPartitionedHlo(hlo, [&]() {\n-    if (!reshard_operand->dynamic_slice_index_on_output) {\n-      return sharded_pad;\n-    }\n-    auto shard_shape = MakePartitionedShape(hlo->shape(), hlo->sharding());\n-    return b_.AddInstruction(HloInstruction::CreateDynamicSlice(\n-        shard_shape, sharded_pad,\n-        *reshard_operand->dynamic_slice_index_on_output,\n-        shard_shape.dimensions()));\n-  });\n+  SetPartitionedHlo(hlo, result);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "9c7c7cf38a187cfa80582aab44883813c8e4f7ab",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 145,
            "deletions": 9,
            "changes": 154,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7",
            "patch": "@@ -85,7 +85,7 @@ class SpmdPartitioningTest\n   absl::StatusOr<std::unique_ptr<HloModule>> PartitionComputation(\n       absl::string_view hlo_module, int64_t num_devices,\n       SpmdPartitionerOptions options = SpmdPartitionerOptions(),\n-      bool use_all_gather = true) {\n+      bool use_all_gather = true, bool enable_enzyme_opt = false) {\n     options.allow_module_signature_change = true;\n     auto collective_ops_creator =\n         GetDefaultCollectiveOpsCreator(num_devices, /*num_replicas=*/1);\n@@ -96,6 +96,9 @@ class SpmdPartitioningTest\n     HloModuleConfig config = GetModuleConfigForTest();\n     config.set_use_spmd_partitioning(true);\n     config.set_num_partitions(num_devices);\n+    if (enable_enzyme_opt) {\n+      config.mutable_debug_options().set_xla_enable_enzyme_comms_opt(true);\n+    }\n     TF_ASSIGN_OR_RETURN(auto module,\n                         ParseAndReturnVerifiedModule(hlo_module, config));\n \n@@ -8083,7 +8086,138 @@ ENTRY entry {\n                     op::Shape(\"s32[64,32]\")));\n }\n \n-TEST_P(SpmdPartitioningTest, DynamicUpdateSliceSingleDimension) {\n+TEST_P(SpmdPartitioningTest, DynamicUpdateSliceOfConstantInRange) {\n+  absl::string_view hlo_string = R\"(\n+  HloModule module\n+\n+  ENTRY entry {\n+    %input = s32[128,64] parameter(0), sharding={devices=[1,2]<=[2]}\n+    %update = s32[10,10] parameter(1), sharding={devices=[1,2]<=[2]}\n+    %c59 = s32[] constant(59)\n+    %c27 = s32[] constant(27)\n+    ROOT %dynamic-update-slice = s32[128,64]\n+      dynamic-update-slice(%input, %update, %c59, %c27),\n+      sharding={devices=[1,2]<=[2]}\n+  })\";\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(\n+                       hlo_string, /*num_devices=*/2, SpmdPartitionerOptions(),\n+                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n+  const auto root = module->entry_computation()->root_instruction();\n+  auto sharded_input = AllOf(op::Parameter(0), op::Shape(\"s32[128,32]\"));\n+  auto sharded_update = AllOf(op::Parameter(1), op::Shape(\"s32[10,5]\"));\n+  auto zero_one_mask = AllOf(op::Pad(op::Broadcast(_), op::Constant()),\n+                             op::Shape(\"pred[10,59]\"));\n+  auto sliced_mask_based_on_partition_id =\n+      AllOf(op::DynamicSlice(zero_one_mask, _, _), op::Shape(\"pred[10,32]\"));\n+  auto dus_range_for_partition_id =\n+      AllOf(op::Select(op::And(_, _), sliced_mask_based_on_partition_id,\n+                       op::Broadcast(op::Constant())),\n+            op::Shape(\"pred[10,32]\"));\n+  auto padded_dus_range_for_partition_id =\n+      AllOf(op::Pad(dus_range_for_partition_id, op::Constant()),\n+            op::Shape(\"pred[128,32]\"));\n+  auto padded_sharded_update =\n+      AllOf(op::Pad(sharded_update, op::Constant()), op::Shape(\"s32[10,59]\"));\n+  auto sharded_update_for_partition_id =\n+      AllOf(op::DynamicSlice(padded_sharded_update, op::Constant(),\n+                             op::Multiply(_, _)),\n+            op::Shape(\"s32[10,32]\"));\n+  auto fully_padded_sharded_update_for_partition_id =\n+      AllOf(op::Pad(sharded_update_for_partition_id, op::Constant()),\n+            op::Shape(\"s32[128,32]\"));\n+\n+  EXPECT_THAT(root,\n+              AllOf(op::Select(padded_dus_range_for_partition_id, sharded_input,\n+                               fully_padded_sharded_update_for_partition_id),\n+                    op::Shape(\"s32[128,32]\")));\n+}\n+\n+// Out of range DUS is legal. The update index will be recalculated so that the\n+// update tensor fits in the input tensor. Eg. for input[7], update[3],\n+// dus_index = 5, the input tensor will be updated from index 4 to 6. More\n+// details in the StableHlo spec: http://shortn/_g5KIGyMt9X.\n+// TODO: b/457448098 - fix out-of-range indexing test case for\n+// collective_ops_e2e_test.cc\n+TEST_P(SpmdPartitioningTest, DynamicUpdateSliceOfConstantOutOfRange) {\n+  absl::string_view hlo_string = R\"(\n+  HloModule module\n+\n+  ENTRY entry {\n+    %input = s32[128,64] parameter(0), sharding={devices=[1,2]<=[2]}\n+    %update = s32[128,20] parameter(1), sharding={devices=[1,2]<=[2]}\n+    %c20 = s32[] constant(20)\n+    %c60 = s32[] constant(60)\n+    ROOT %dynamic-update-slice = s32[128,64]\n+      dynamic-update-slice(%input, %update, %c20, %c60),\n+      sharding={devices=[1,2]<=[2]}\n+  })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(\n+                       hlo_string, /*num_devices=*/2, SpmdPartitionerOptions(),\n+                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n+  const auto root = module->entry_computation()->root_instruction();\n+  auto sharded_input = AllOf(op::Parameter(0), op::Shape(\"s32[128,32]\"));\n+  auto sharded_update = AllOf(op::Parameter(1), op::Shape(\"s32[128,10]\"));\n+  auto all_gather_input =\n+      AllOf(op::AllGather(sharded_input), op::Shape(\"s32[128,64]\"));\n+  auto all_gather_update =\n+      AllOf(op::AllGather(sharded_update), op::Shape(\"s32[128,20]\"));\n+  auto dus = op::DynamicUpdateSlice(all_gather_input, all_gather_update, _, _);\n+\n+  EXPECT_THAT(root,\n+              AllOf(op::DynamicSlice(dus, op::Constant(),\n+                                     op::Reshape(op::DynamicSlice(\n+                                         op::Constant(), op::PartitionId()))),\n+                    op::Shape(\"s32[128,32]\")));\n+}\n+\n+TEST_P(SpmdPartitioningTest, DynamicUpdateSliceSingleDimensionWithEnzymeOpt) {\n+  absl::string_view hlo_string = R\"(\n+    HloModule module\n+\n+    ENTRY entry {\n+      %input = s32[16] parameter(0), sharding={devices=[4]<=[4]}\n+      %update = s32[8] parameter(1), sharding={devices=[4]<=[4]}\n+      %c3 = s32[] constant(3)\n+      ROOT %dynamic-update-slice = s32[16]\n+        dynamic-update-slice(%input, %update, %c3),\n+        sharding={devices=[4]<=[4]}\n+    })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(\n+                       hlo_string, /*num_devices=*/4, SpmdPartitionerOptions(),\n+                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n+  const auto root = module->entry_computation()->root_instruction();\n+  auto sharded_input = AllOf(op::Parameter(0), op::Shape(\"s32[4]\"));\n+  auto sharded_update = AllOf(op::Parameter(1), op::Shape(\"s32[2]\"));\n+  auto c3 = AllOf(op::Constant(), op::Shape(\"s32[]\"));\n+  auto per_partition_padded_mask = AllOf(op::Select(\n+      _, op::DynamicSlice(op::Pad(_, _), op::Multiply(_, _)), op::Broadcast()));\n+  auto sliced_sharded_update_fwd_edge = AllOf(\n+      op::CollectivePermute(op::Slice(sharded_update)), op::Shape(\"s32[1]\"));\n+  auto sharded_update_bwd_edge =\n+      AllOf(op::CollectivePermute(sharded_update), op::Shape(\"s32[2]\"));\n+  auto sliced_sharded_update_non_neighboring_devices = AllOf(\n+      op::CollectivePermute(op::Slice(sharded_update)), op::Shape(\"s32[1]\"));\n+  EXPECT_THAT(\n+      root,\n+      AllOf(op::Select(\n+                per_partition_padded_mask, sharded_input,\n+                op::DynamicSlice(\n+                    op::Pad(op::Concatenate(\n+                                sliced_sharded_update_fwd_edge, sharded_update,\n+                                sharded_update_bwd_edge,\n+                                sliced_sharded_update_non_neighboring_devices),\n+                            op::Constant()),\n+                    op::Multiply(_, _))),\n+            op::Shape(\"s32[4]\")));\n+}\n+\n+TEST_P(SpmdPartitioningTest,\n+       DynamicUpdateSliceSingleDimensionWithoutEnzymeOpt) {\n   absl::string_view hlo_string = R\"(\n     HloModule module\n \n@@ -14802,15 +14936,17 @@ ENTRY entry {\n   ROOT c = bf16[16,224,224,384]{3,2,1,0} copy(dynamic-update-slice.128), sharding={devices=[2,2,2,1]<=[8]}\n })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(\n+                       hlo_string, /*num_devices=*/8, SpmdPartitionerOptions(),\n+                       /*use_all_gather=*/true, /*enable_enzyme_opt=*/true));\n \n   XLA_VLOG_LINES(1, module->ToString());\n-  EXPECT_THAT(\n-      module->entry_computation()->root_instruction(),\n-      op::Copy(op::DynamicSlice(\n-          AllOf(op::DynamicUpdateSlice(), op::Shape(\"bf16[8,224, 224,384]\")), _,\n-          _, _, _)));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              AllOf(op::Copy(op::Select(op::Select(_, _, _),\n+                                        op::DynamicSlice(_, _, _, _, _),\n+                                        op::DynamicSlice(_, _, _, _, _))),\n+                    op::Shape(\"bf16[8,112,112,384]\")));\n }\n \n TEST_P(SpmdPartitioningTest, CustomCallManualSharding) {"
        },
        {
            "sha": "bdaa4c3fc3515eb513ee0c9a28ee98bd640957b0",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7",
            "patch": "@@ -3163,6 +3163,37 @@ DynamicUpdateSliceAnalysis AnalyzeDynamicUpdateSlice(\n         DynamicUpdateSliceMethod::kAllPartitionedSliceDimsHaveConstantIndices;\n   }\n \n+  // For now, only enable Method 3 if enzyme optimization is enabled.\n+  bool is_enzyme_opt_enabled = hlo->parent()\n+                                   ->parent()\n+                                   ->config()\n+                                   .debug_options()\n+                                   .xla_enable_enzyme_comms_opt();\n+  if (!is_enzyme_opt_enabled &&\n+      analysis.method == DynamicUpdateSliceMethod::\n+                             kAllPartitionedSliceDimsHaveConstantIndices) {\n+    analysis.method = DynamicUpdateSliceMethod::kDefault;\n+    return analysis;\n+  }\n+\n+  // Extra check for out-of-bounds indexing\n+  const HloInstruction* update_tensor = hlo->operand(1);\n+  if (analysis.method ==\n+      DynamicUpdateSliceMethod::kAllPartitionedSliceDimsHaveConstantIndices) {\n+    for (int64_t dim = 0; dim < hlo->shape().dimensions().size(); ++dim) {\n+      const HloInstruction* dus_index = hlo->operand(dim + 2);\n+      CHECK(dus_index->IsConstant());\n+\n+      int64_t start_index = dus_index->literal().GetIntegralAsS64({}).value();\n+      int64_t end_index = start_index + update_tensor->shape().dimensions(dim);\n+      int64_t padding_high = hlo->shape().dimensions(dim) - end_index;\n+      if (start_index < 0 || padding_high < 0) {\n+        analysis.method = DynamicUpdateSliceMethod::kDefault;\n+        return analysis;\n+      }\n+    }\n+  }\n+\n   return analysis;\n }\n "
        },
        {
            "sha": "f6995d72217677411121675a8927b119a34d9f46",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 6,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7",
            "patch": "@@ -1508,8 +1508,9 @@ TEST_F(CollectiveOpsTestE2E, HostMemoryOffloadingWithDonation) {\n // E2E tests comparing the results of sharded and unsharded execution.\n class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsTestE2E {\n  public:\n-  void CollectiveOpsCompareShardedUnsharded(const std::string& hlo_text,\n-                                            const int64_t num_partitions = 2) {\n+  void CollectiveOpsCompareShardedUnsharded(\n+      const std::string& hlo_text, const int64_t num_partitions = 2,\n+      bool enable_enzyme_comms_opt = false) {\n     const int64_t num_replicas = 1;\n     if (hlo_runner_->device_count() < num_replicas * num_partitions) {\n       GTEST_SKIP() << \"Test requires at least \" << num_replicas * num_partitions\n@@ -1521,8 +1522,9 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsTestE2E {\n                             ExecuteUnsharded(hlo_text));\n     ASSERT_EQ(ref_results.size(), 1);\n \n-    TF_ASSERT_OK_AND_ASSIGN(std::vector<Literal> results,\n-                            ExecuteSharded(hlo_text, num_partitions));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::vector<Literal> results,\n+        ExecuteSharded(hlo_text, num_partitions, enable_enzyme_comms_opt));\n     ASSERT_EQ(results.size(), num_partitions);\n \n     ErrorSpec error_spec{1e-4, 1e-4};\n@@ -1568,12 +1570,16 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsTestE2E {\n \n   // Execute the sharded case.\n   absl::StatusOr<std::vector<Literal>> ExecuteSharded(\n-      const std::string& hlo_text, int64_t num_partitions) {\n+      const std::string& hlo_text, int64_t num_partitions,\n+      bool enable_enzyme_comms_opt = false) {\n     HloModuleConfig config = GetModuleConfigForTest();\n     DebugOptions opts = GetDebugOptionsForTest();\n     opts.set_xla_gpu_enable_triton_gemm(false);\n     config.set_debug_options(opts);\n     config.set_num_partitions(num_partitions);\n+    if (enable_enzyme_comms_opt) {\n+      config.mutable_debug_options().set_xla_enable_enzyme_comms_opt(true);\n+    }\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<VerifiedHloModule> module,\n                         ParseAndReturnVerifiedModule(hlo_text, config));\n     const int64_t num_params = module->entry_computation()->num_parameters();\n@@ -1732,7 +1738,11 @@ TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n       %c3 = s32[] constant(3)\n       ROOT %dynamic-update-slice = s32[16] dynamic-update-slice(%input, %update, %c3), sharding={devices=[4]<=[4]}\n     })\";\n-  CollectiveOpsCompareShardedUnsharded(hlo_text, /*num_partitions=*/4);\n+  CollectiveOpsCompareShardedUnsharded(hlo_text, /*num_partitions=*/4,\n+                                       /*enable_enzyme_comms_opt=*/true);\n+  // This test should pass regardless if enzyme comms opt is enabled or not.\n+  CollectiveOpsCompareShardedUnsharded(hlo_text, /*num_partitions=*/4,\n+                                       /*enable_enzyme_comms_opt=*/false);\n }\n \n TEST_F(CollectiveOpsTestE2EShardedUnsharded, DotBatchAndNonContracting) {"
        },
        {
            "sha": "a2a2096ea549f09fe6806aae84717a84fe3d6c00",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=26dbdf873eabbbbbef4099e9abc4dd5dd7eac8f7",
            "patch": "@@ -361,6 +361,10 @@ message DebugOptions {\n   // supported on XLA:GPU and XLA:CPU.\n   optional bool xla_dump_hlo_unoptimized_snapshots = 405;\n \n+  // Enable communication optimization patterns specified in Enzyme. More\n+  // details in http://shortn/_jXJ2VFoyMN.\n+  optional bool xla_enable_enzyme_comms_opt = 429;\n+\n   // Denylist for cuDNN convolutions.\n   optional string xla_gpu_algorithm_denylist_path = 128;\n "
        }
    ],
    "stats": {
        "total": 555,
        "additions": 414,
        "deletions": 141
    }
}