{
    "author": "basioli-k",
    "message": "[XLA:GPU][codegen] Separate out emitting shared dialect and lowering to triton as APIs used for testing.\n\nThis change also migrates some device tests to use the API.\n\nPiperOrigin-RevId: 817624248",
    "sha": "49b26980d75066d5aadcab6005a2459bb73613bd",
    "files": [
        {
            "sha": "52c71484c9dc4d129e54ecbcf0bb9800ee710b7b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=49b26980d75066d5aadcab6005a2459bb73613bd",
            "patch": "@@ -880,6 +880,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n+        \"//xla/tests:hlo_test_base_with_mlir_context\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -987,6 +988,28 @@ xla_test(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"fusion_emitter_shared_dialect_test\",\n+    srcs = if_gpu_is_configured([\"fusion_emitter_shared_dialect_test.cc\"]),\n+    # TODO(b/353912594): this test does not need to run on GPU, but it is broken on CPU in OSS.\n+    # Force it to run on GPU temporarily in order to get important OSS coverage.\n+    tags = [\n+        \"gpu\",\n+        \"no_mac\",\n+    ],\n+    deps = [\n+        \":test_utils\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/tests:hlo_test_base_with_mlir_context\",\n+        \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest\",\n+    ],\n+)\n+\n cc_library(\n     name = \"support\",\n     srcs = ["
        },
        {
            "sha": "e3ce8819b8bb10443e4ffb642981840383e6e3ba",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 141,
            "deletions": 105,
            "changes": 246,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=49b26980d75066d5aadcab6005a2459bb73613bd",
            "patch": "@@ -1963,78 +1963,15 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::MLIRContext& mlir_context) {\n-  LoadMlirDialectsForTriton(mlir_context);\n-  const auto debug_options = fusion->GetModule()->config().debug_options();\n+  TF_ASSIGN_OR_RETURN(\n+      auto triton_module,\n+      ir_emitter_triton_internal::EmitXTileModule(\n+          fn_name, fusion, device_info, block_level_parameters, mlir_context));\n \n   const HloComputation* hlo_computation =\n       fusion->fused_instructions_computation();\n \n-  auto loc = mlir::NameLoc::get(\n-      mlir::StringAttr::get(&mlir_context, hlo_computation->name()));\n-  EmitterLocOpBuilder b(\n-      loc, &mlir_context,\n-      debug_options.xla_gpu_unsupported_annotate_with_emitter_loc());\n-\n-  mlir::OwningOpRef<mlir::ModuleOp> triton_module =\n-      llvm_ir::CreateMlirModuleOp(loc);\n-  b.setInsertionPointToEnd(triton_module->getBody());\n-\n-  auto backend_config =\n-      fusion->backend_config<GpuBackendConfig>()->fusion_backend_config();\n-  absl::string_view fusion_kind = backend_config.kind();\n-\n-  // Build Triton kernel.\n-  SmallVector<Type> fn_arg_types;\n-  for (HloInstruction* p : hlo_computation->parameter_instructions()) {\n-    PrimitiveType type = p->shape().element_type();\n-    Type ir_type;\n-    if (type == U16) {\n-      ir_type = b.getI16Type();\n-    } else if (type == S4) {\n-      ir_type = b.getI4Type();\n-    } else {\n-      TF_ASSIGN_OR_RETURN(ir_type, TritonType(b, type));\n-    }\n-\n-    AppendFuncArgType(p->shape().dimensions(), ir_type, fn_arg_types);\n-  }\n-\n-  for (const ShapeUtil::IndexedShape& s :\n-       ShapeUtil::GetLeafShapes(fusion->shape())) {\n-    TF_ASSIGN_OR_RETURN(Type triton_ty, TritonType(b, s.shape.element_type()));\n-    AppendFuncArgType(s.shape.dimensions(), triton_ty, fn_arg_types);\n-  }\n-\n-  mlir::FunctionOpInterface fn =\n-      CreateFuncOp(b, fn_name, fusion_kind, fn_arg_types);\n-\n-  fn.addEntryBlock();\n-  b.setInsertionPointToStart(&fn.front());\n-\n-  std::string libdevice_path =\n-      GetLibdevicePath(fusion->GetModule()->config(), device_info);\n-\n-  if (fusion_kind == kTritonGemmFusionKind) {\n-    if (absl::c_contains(\n-            fusion->GetModule()\n-                ->config()\n-                .debug_options()\n-                .xla_gpu_unsupported_generic_triton_emitter_features(),\n-            DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n-      return Internal(\"Legacy GEMM emitter is disabled.\");\n-    }\n-    TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info, fusion, fn,\n-                                  block_level_parameters));\n-  } else if (fusion_kind == kTritonFusionKind ||\n-             fusion_kind == kTritonNestedGemmFusionKind ||\n-             fusion_kind == kTritonScaledDotFusionKind) {\n-    TF_RETURN_IF_ERROR(EmitGeneric(b, libdevice_path, device_info, fusion, fn,\n-                                   block_level_parameters));\n-  } else {\n-    return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n-  }\n-\n-  EmitReturnOp(b, fusion_kind);\n+  const auto debug_options = fusion->GetModule()->config().debug_options();\n \n   if (DumpingEnabledForHloModule(*hlo_computation->parent())) {\n     auto suffix = absl::StrCat(fusion->name(), \".before_validation.ttir.txt\");\n@@ -2051,43 +1988,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n         ExtractInstructionIntoNewModule(*fusion)->ToString());\n   }\n \n-  {  // Convert xTile ops to Triton ops.\n-    mlir::PassManager pm(&mlir_context);\n-    // Disable verifier because the Triton code may be invalid due to the\n-    // unsupported types.\n-    pm.enableVerifier(/*enabled=*/false);\n-    pm.addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n-    if (mlir::failed(pm.run(triton_module.get()))) {\n-      return CreateInternalError(\n-          \"Failed to convert xTile ops to Triton ops for fusion:\", fusion,\n-          *triton_module);\n-    }\n-  }\n-\n-  if (debug_options.xla_gpu_experimental_scaled_dot_with_triton()) {\n-    // Convert unsupported types before verification.\n-    mlir::PassManager pm(&mlir_context);\n-    pm.addPass(mlir::triton::xla::CreateTritonXLAConvertUnsupportedTypesPass());\n-    if (mlir::failed(pm.run(triton_module.get()))) {\n-      return CreateInternalError(\n-          \"Failed to fix unsupported types in Triton module for fusion:\",\n-          fusion, *triton_module);\n-    }\n-  }\n-\n-  if (mlir::failed(mlir::verify(*triton_module))) {\n-    return CreateInternalError(\n-        \"Failed to verify Triton module for fusion:\", fusion, *triton_module);\n-  }\n-\n-  mlir::PassManager pm(&mlir_context);\n-\n-  pm.addPass(mlir::createCanonicalizerPass());\n-  pm.addPass(mlir::createCSEPass());\n-  if (mlir::failed(pm.run(triton_module.get()))) {\n-    return CreateInternalError(\n-        \"Failed to create Triton module for fusion:\", fusion, *triton_module);\n-  }\n+  TF_RETURN_IF_ERROR(ir_emitter_triton_internal::LowerXTileToTriton(\n+      triton_module.get(), mlir_context, *fusion));\n \n   VLOG(6) << DumpTritonIR(triton_module.get(),\n                           fusion->GetModule()\n@@ -2331,5 +2233,139 @@ std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n   return \"\";\n }\n \n+namespace ir_emitter_triton_internal {\n+\n+// TODO(b/447133106): Contrary to the name, this function still does a lot of\n+// triton specific things. It should be migrated to use non-triton specific\n+// utilities.\n+absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n+    absl::string_view fn_name, const HloFusionInstruction* fusion,\n+    const se::DeviceDescription& device_info,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::MLIRContext& mlir_context) {\n+  LoadMlirDialectsForTriton(mlir_context);\n+  const auto debug_options = fusion->GetModule()->config().debug_options();\n+\n+  const HloComputation* hlo_computation =\n+      fusion->fused_instructions_computation();\n+\n+  auto loc = mlir::NameLoc::get(\n+      mlir::StringAttr::get(&mlir_context, hlo_computation->name()));\n+  EmitterLocOpBuilder b(\n+      loc, &mlir_context,\n+      debug_options.xla_gpu_unsupported_annotate_with_emitter_loc());\n+\n+  mlir::OwningOpRef<mlir::ModuleOp> triton_module =\n+      llvm_ir::CreateMlirModuleOp(loc);\n+  b.setInsertionPointToEnd(triton_module->getBody());\n+\n+  auto backend_config =\n+      fusion->backend_config<GpuBackendConfig>()->fusion_backend_config();\n+  absl::string_view fusion_kind = backend_config.kind();\n+\n+  // Build Triton kernel.\n+  SmallVector<Type> fn_arg_types;\n+  for (HloInstruction* p : hlo_computation->parameter_instructions()) {\n+    PrimitiveType type = p->shape().element_type();\n+    Type ir_type;\n+    if (type == U16) {\n+      ir_type = b.getI16Type();\n+    } else if (type == S4) {\n+      ir_type = b.getI4Type();\n+    } else {\n+      TF_ASSIGN_OR_RETURN(ir_type, TritonType(b, type));\n+    }\n+\n+    AppendFuncArgType(p->shape().dimensions(), ir_type, fn_arg_types);\n+  }\n+\n+  for (const ShapeUtil::IndexedShape& s :\n+       ShapeUtil::GetLeafShapes(fusion->shape())) {\n+    TF_ASSIGN_OR_RETURN(Type triton_ty, TritonType(b, s.shape.element_type()));\n+    AppendFuncArgType(s.shape.dimensions(), triton_ty, fn_arg_types);\n+  }\n+\n+  mlir::FunctionOpInterface fn =\n+      CreateFuncOp(b, fn_name, fusion_kind, fn_arg_types);\n+\n+  fn.addEntryBlock();\n+  b.setInsertionPointToStart(&fn.front());\n+\n+  std::string libdevice_path =\n+      GetLibdevicePath(fusion->GetModule()->config(), device_info);\n+\n+  if (fusion_kind == kTritonGemmFusionKind) {\n+    if (absl::c_contains(\n+            fusion->GetModule()\n+                ->config()\n+                .debug_options()\n+                .xla_gpu_unsupported_generic_triton_emitter_features(),\n+            DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n+      return Internal(\"Legacy GEMM emitter is disabled.\");\n+    }\n+    TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info, fusion, fn,\n+                                  block_level_parameters));\n+  } else if (fusion_kind == kTritonFusionKind ||\n+             fusion_kind == kTritonNestedGemmFusionKind ||\n+             fusion_kind == kTritonScaledDotFusionKind) {\n+    TF_RETURN_IF_ERROR(EmitGeneric(b, libdevice_path, device_info, fusion, fn,\n+                                   block_level_parameters));\n+  } else {\n+    return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n+  }\n+\n+  EmitReturnOp(b, fusion_kind);\n+\n+  return triton_module;\n+}\n+\n+absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n+                                mlir::MLIRContext& mlir_context,\n+                                const HloFusionInstruction& fusion) {\n+  {  // Convert xTile ops to Triton ops.\n+    mlir::PassManager pm(&mlir_context);\n+    // Disable verifier because the Triton code may be invalid due to the\n+    // unsupported types.\n+    pm.enableVerifier(/*enabled=*/false);\n+    pm.addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n+    if (mlir::failed(pm.run(xtile_dialect_module))) {\n+      return CreateInternalError(\n+          \"Failed to lower from shared dialect to Triton.\", &fusion,\n+          xtile_dialect_module);\n+    }\n+  }\n+\n+  if (fusion.GetModule()\n+          ->config()\n+          .debug_options()\n+          .xla_gpu_experimental_scaled_dot_with_triton()) {\n+    // Convert unsupported types before verification.\n+    mlir::PassManager pm(&mlir_context);\n+    pm.addPass(mlir::triton::xla::CreateTritonXLAConvertUnsupportedTypesPass());\n+    if (mlir::failed(pm.run(xtile_dialect_module))) {\n+      return CreateInternalError(\n+          \"Failed to fix unsupported types in Triton module for fusion:\",\n+          &fusion, xtile_dialect_module);\n+    }\n+  }\n+\n+  if (mlir::failed(mlir::verify(xtile_dialect_module))) {\n+    return CreateInternalError(\"Failed to verify Triton module for fusion:\",\n+                               &fusion, xtile_dialect_module);\n+  }\n+  mlir::PassManager pm(&mlir_context);\n+\n+  pm.addPass(mlir::createCanonicalizerPass());\n+  pm.addPass(mlir::createCSEPass());\n+  if (mlir::failed(pm.run(xtile_dialect_module))) {\n+    return CreateInternalError(\"Failed to create Triton module for fusion:\",\n+                               &fusion, xtile_dialect_module);\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace ir_emitter_triton_internal\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "39eb70fef4c568c2e084fad14a0e817ac814d84f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=49b26980d75066d5aadcab6005a2459bb73613bd",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <cstdint>\n #include <optional>\n \n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -34,6 +35,7 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n@@ -141,6 +143,26 @@ absl::StatusOr<Tiling> TilingFromAnnotatedFusion(\n     const SymbolicTileAnalysis& symbolic_tile_analysis,\n     const BlockLevelParameters& block_level_parameters);\n \n+// This function (or its future equivalent) should emit the MLIR module in the\n+// shared dialect between XLA:CPU and XLA:GPU. At the moment it is still\n+// emitting GPU specific modules. It is currently exposed only for testing\n+// purposes and will only be used to make sure we are properly emitting the\n+// shared dialect.\n+absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n+    absl::string_view fn_name, const HloFusionInstruction* fusion,\n+    const se::DeviceDescription& device_info,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::MLIRContext& mlir_context);\n+\n+// This function lowers the shared dialect module to Triton. It is exposed for\n+// testing with the same motivation as EmitXTileModule.\n+//\n+// The `fusion` instruction should be the one that was used to create the shared\n+// dialect module.\n+absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n+                                mlir::MLIRContext& mlir_context,\n+                                const HloFusionInstruction& fusion);\n+\n }  // namespace ir_emitter_triton_internal\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "73bf293e7ae87b1a309ecd63083a8c9fed04700a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 114,
            "deletions": 21,
            "changes": 135,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=49b26980d75066d5aadcab6005a2459bb73613bd",
            "patch": "@@ -74,6 +74,12 @@ namespace xla {\n namespace gpu {\n namespace {\n \n+const HloFusionInstruction& GetFusionInstruction(\n+    const HloModule& hlo_module, absl::string_view fusion_name) {\n+  return *Cast<HloFusionInstruction>(\n+      hlo_module.GetComputationWithName(fusion_name)->FusionInstruction());\n+}\n+\n constexpr ErrorSpec kExactMatch{/*aabs=*/0, /*arel=*/0};\n \n class TritonEmitterTest : public GpuCodegenTest {\n@@ -2048,14 +2054,26 @@ ENTRY entry_computation {\n         \"num_ctas\":\"1\",\n         \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:     triton_xla.extract\n+CHECK-NOT: stablehlo.transpose\n+CHECK:     tt.reshape\n+CHECK-NOT: stablehlo.transpose\n+CHECK:     triton_xla.insert\n+          )\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:     triton_xla.extract\n CHECK-NOT: tt.trans\n CHECK:     tt.reshape\n CHECK-NOT: tt.trans\n CHECK:     triton_xla.insert\n-)\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }\n@@ -2079,14 +2097,27 @@ ENTRY entry_computation {\n         \"num_ctas\":\"1\",\n         \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:     triton_xla.extract\n+CHECK:     stablehlo.transpose\n+CHECK:     tt.reshape\n+CHECK-NOT: stablehlo.transpose\n+CHECK:     triton_xla.insert\n+          )\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:     triton_xla.extract\n CHECK:     tt.trans\n CHECK:     tt.reshape\n CHECK-NOT: tt.trans\n CHECK:     triton_xla.insert\n-  )\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }\n@@ -2110,14 +2141,27 @@ ENTRY entry_computation {\n         \"num_ctas\":\"1\",\n         \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:     triton_xla.extract\n+CHECK-NOT: stablehlo.transpose\n+CHECK:     tt.reshape\n+CHECK:     stablehlo.transpose\n+CHECK:     triton_xla.insert\n+          )\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:     triton_xla.extract\n CHECK-NOT: tt.trans\n CHECK:     tt.reshape\n CHECK:     tt.trans\n CHECK:     triton_xla.insert\n-)\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }\n@@ -2142,14 +2186,27 @@ ENTRY entry_computation {\n         \"num_ctas\":\"1\",\n         \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:     triton_xla.extract\n+CHECK:     stablehlo.transpose\n+CHECK:     tt.reshape\n+CHECK:     stablehlo.transpose\n+CHECK:     triton_xla.insert\n+          )\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:     triton_xla.extract\n CHECK:     tt.trans\n CHECK:     tt.reshape\n CHECK:     tt.trans\n CHECK:     triton_xla.insert\n-)\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }\n@@ -2173,14 +2230,27 @@ ENTRY entry_computation {\n         \"num_ctas\":\"1\",\n         \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:     triton_xla.extract\n+CHECK:     stablehlo.transpose\n+CHECK-NOT: tt.reshape\n+CHECK-NOT: stablehlo.transpose\n+CHECK:     triton_xla.insert\n+          )\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:     triton_xla.extract\n CHECK:     tt.trans\n CHECK-NOT: tt.reshape\n CHECK-NOT: tt.trans\n CHECK:     triton_xla.insert\n-)\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }\n@@ -2351,11 +2421,21 @@ ENTRY main {\n           \"num_ctas\":\"1\",\n           \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:      %[[TILE:.*]] = triton_xla.extract {{.*}} : tensor<8x4x1xf32>\n+CHECK:      stablehlo.transpose %[[TILE]], dims = [2, 0, 1] : (tensor<8x4x1xf32>) -> tensor<1x8x4xf32>\n+          )\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:      %[[TILE:.*]] = triton_xla.extract {{.*}} : tensor<8x4x1xf32>\n CHECK:      tt.trans %[[TILE]] {order = array<i32: 2, 0, 1>} : tensor<8x4x1xf32> -> tensor<1x8x4xf32>\n-)\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }\n@@ -2386,14 +2466,27 @@ ENTRY entry_computation {\n           \"num_ctas\":\"1\",\n           \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"fused_computation\", R\"(\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"fused_computation\", R\"(\n+CHECK:         %[[TILE:.*]] = triton_xla.extract {{.*}} : tensor<15x7x3xf32> to tensor<8x4x1xf32>\n+CHECK-NOT:     triton_xla.extract\n+CHECK:         %[[ABS:.*]] = math.absf %[[TILE]]\n+CHECK:         stablehlo.transpose %[[ABS]], dims = [2, 0, 1] : (tensor<8x4x1xf32>) -> tensor<1x8x4xf32>\n+CHECK-COUNT-2: triton_xla.insert\n+          )\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:         %[[TILE:.*]] = triton_xla.extract {{.*}} : tensor<15x7x3xf32> to tensor<8x4x1xf32>\n CHECK-NOT:     triton_xla.extract\n CHECK:         %[[ABS:.*]] = math.absf %[[TILE]]\n CHECK:         tt.trans %[[ABS]] {order = array<i32: 2, 0, 1>} : tensor<8x4x1xf32> -> tensor<1x8x4xf32>\n CHECK-COUNT-2: triton_xla.insert\n-)\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"fused_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }"
        },
        {
            "sha": "07ccb041d7c019eda5d7ca89521a0b2dca0c3153",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=49b26980d75066d5aadcab6005a2459bb73613bd",
            "patch": "@@ -0,0 +1,73 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+// *****************************************************************************\n+// Tests for emitting a shared dialect between XLA:CPU and XLA:GPU.\n+//\n+// These tests are currently relying on the triton specific fusion emitter. The\n+// plan is to use these tests whilst migrating the triton emitter to a shared\n+// emitter. The idea is for these tests to be backend agnostic once the shared\n+// emitter becomes a reality.\n+// *****************************************************************************\n+\n+using XTileDialectTest = HloTestBaseWithMlirContext;\n+\n+TEST_F(XTileDialectTest, TestEmittingStableHloTranspose) {\n+  constexpr absl::string_view kHloText = R\"(\n+HloModule t\n+\n+transpose_fusion {\n+  p0 = f32[137,115]{1,0} parameter(0)\n+  ROOT transpose = f32[115,137]{1,0} transpose(p0), dimensions={1, 0}\n+}\n+\n+ENTRY e {\n+  p0 = f32[137,115]{1,0} parameter(0)\n+  ROOT custom-call = f32[115,137]{1,0} fusion(p0), kind=kCustom,\n+    calls=transpose_fusion,\n+    backend_config={\"fusion_backend_config\": {kind: \"__triton\"}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloText));\n+\n+  BlockLevelParameters block_level_parameters;\n+  block_level_parameters.output_tile_sizes = {{16, 32}};\n+\n+  TF_EXPECT_OK(CreateXTileIrAndFileCheck(\n+      this, *module->GetComputationWithName(\"transpose_fusion\"),\n+      block_level_parameters,\n+      R\"(\n+CHECK: %[[RES:.*]] = stablehlo.transpose %[[ARG:.*]], dims = [1, 0] : (tensor<32x16xf32>) -> tensor<16x32xf32>\n+)\"));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "ee5969a11e333abe0e3ca9616c345b335183042e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=49b26980d75066d5aadcab6005a2459bb73613bd",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n+#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n@@ -135,6 +136,68 @@ absl::Status CreateTritonIrAndFileCheck(\n   return absl::OkStatus();\n }\n \n+absl::StatusOr<\n+    std::pair<mlir::OwningOpRef<mlir::ModuleOp>, std::unique_ptr<HloModule>>>\n+CreateXTileIrAndFileCheck(HloTestBaseWithMlirContext* test,\n+                          absl::string_view hlo_text,\n+                          absl::string_view triton_fusion_name,\n+                          absl::string_view filecheck_pattern) {\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> hlo_module,\n+                      test->ParseAndReturnVerifiedModule(hlo_text));\n+  auto* comp = hlo_module->GetComputationWithName(triton_fusion_name);\n+  TF_RET_CHECK(comp != nullptr) << absl::StrCat(\n+      \"Computation '\", triton_fusion_name, \"' is not found in the module\");\n+  auto fusion_backend_config = comp->FusionInstruction()\n+                                   ->backend_config<GpuBackendConfig>()\n+                                   ->fusion_backend_config();\n+  BlockLevelParameters block_level_parameters =\n+      BlockLevelParameters::FromBlockLevelFusionConfig(\n+          fusion_backend_config.block_level_fusion_config());\n+  TF_ASSIGN_OR_RETURN(\n+      mlir::OwningOpRef<mlir::ModuleOp> xtile_dialect_module,\n+      CreateXTileIrAndFileCheck(test, *comp, block_level_parameters,\n+                                filecheck_pattern));\n+  return std::make_pair(std::move(xtile_dialect_module), std::move(hlo_module));\n+}\n+\n+absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n+    HloTestBaseWithMlirContext* test, const HloComputation& computation,\n+    const BlockLevelParameters& block_level_parameters,\n+    absl::string_view filecheck_pattern) {\n+  auto* fusion = Cast<HloFusionInstruction>(computation.FusionInstruction());\n+\n+  TF_ASSIGN_OR_RETURN(\n+      mlir::OwningOpRef<mlir::ModuleOp> xtile_dialect_module,\n+      ir_emitter_triton_internal::EmitXTileModule(\n+          \"xtile_dialect_fn\", fusion, TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n+          block_level_parameters, *test->mlir_context()));\n+\n+  std::string out;\n+  llvm::raw_string_ostream os(out);\n+  xtile_dialect_module->print(os);\n+  TF_ASSIGN_OR_RETURN(bool succeeded, RunFileCheck(out, filecheck_pattern));\n+  if (!succeeded) {\n+    return absl::InternalError(\"FileCheck failed.\");\n+  }\n+  return xtile_dialect_module;\n+}\n+\n+absl::Status LowerXTileIrToTritonAndFileCheck(\n+    HloTestBaseWithMlirContext* test, mlir::ModuleOp xtile_dialect_module,\n+    absl::string_view filecheck_pattern, const HloFusionInstruction& fusion) {\n+  TF_RETURN_IF_ERROR(ir_emitter_triton_internal::LowerXTileToTriton(\n+      xtile_dialect_module, *test->mlir_context(), fusion));\n+\n+  std::string out;\n+  llvm::raw_string_ostream os(out);\n+  xtile_dialect_module->print(os);\n+  TF_ASSIGN_OR_RETURN(bool succeeded, RunFileCheck(out, filecheck_pattern));\n+  if (!succeeded) {\n+    return absl::InternalError(\"FileCheck failed.\");\n+  }\n+  return absl::OkStatus();\n+}\n+\n absl::Status CreateTritonIrAndFileCheckForDot(\n     HloTestBase* test, absl::string_view hlo_text,\n     absl::string_view triton_fusion_name, absl::string_view filecheck_pattern) {"
        },
        {
            "sha": "7eed1acd0ba20708ba1fe543c10d672621d2f6bb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.h",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/49b26980d75066d5aadcab6005a2459bb73613bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h?ref=49b26980d75066d5aadcab6005a2459bb73613bd",
            "patch": "@@ -29,7 +29,9 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/OwningOpRef.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -39,6 +41,7 @@ limitations under the License.\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n+#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -61,6 +64,34 @@ absl::Status CreateTritonIrAndFileCheck(\n     const BlockLevelParameters& block_level_parameters,\n     absl::string_view filecheck_pattern);\n \n+// Creates a shared dialect IR for the fusion `triton_fusion_name` inside the\n+// computation defined by `hlo_text`.\n+// The function returns the shared dialect IR and the HLO module. The HLO module\n+// is returned so that the user can work with the computation that generated the\n+// fusion if needed.\n+// This function also checks the generated shared dialect IR against the\n+// `filecheck_pattern`.\n+absl::StatusOr<\n+    std::pair<mlir::OwningOpRef<mlir::ModuleOp>, std::unique_ptr<HloModule>>>\n+CreateXTileIrAndFileCheck(HloTestBaseWithMlirContext* test,\n+                          absl::string_view hlo_text,\n+                          absl::string_view triton_fusion_name,\n+                          absl::string_view filecheck_pattern);\n+\n+// Creates a shared dialect IR from the given HLO computation and returns it.\n+// This function also checks the generated shared dialect IR against the\n+// `filecheck_pattern`.\n+absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n+    HloTestBaseWithMlirContext* test, const HloComputation& computation,\n+    const BlockLevelParameters& block_level_parameters,\n+    absl::string_view filecheck_pattern);\n+\n+// Lowers the given shared dialect IR to Triton IR and checks the result against\n+// the `filecheck_pattern`.\n+absl::Status LowerXTileIrToTritonAndFileCheck(\n+    HloTestBaseWithMlirContext* test, mlir::ModuleOp xtile_dialect_module,\n+    absl::string_view filecheck_pattern, const HloFusionInstruction& fusion);\n+\n absl::Status CreateTritonIrAndFileCheckForDot(\n     HloTestBase* test, absl::string_view hlo_text,\n     absl::string_view triton_fusion_name, absl::string_view filecheck_pattern);"
        }
    ],
    "stats": {
        "total": 593,
        "additions": 467,
        "deletions": 126
    }
}