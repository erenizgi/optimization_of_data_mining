{
    "author": "GleasonK",
    "message": "Allow stablehlo tokens in async ops\n\nPiperOrigin-RevId: 810246463",
    "sha": "de851b35011925b85d5f7a9a7c0308f3a9753946",
    "files": [
        {
            "sha": "63236cef87baea307369161dcbff808f4b8cab30",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_base.td",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_base.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_base.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_base.td?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -89,6 +89,7 @@ defvar MHLO_TensorOrTokenOrBuffer = AnyTypeOf<[MHLO_TensorOrToken, MHLO_Buffer]>\n defvar MHLO_TensorOrAnyToken = AnyTypeOf<[MHLO_TensorOrToken, StableHLO_TokenType]>;\n \n defvar MHLO_TensorOrTokenOrTuple = AnyTypeOf<[MHLO_Tensor, MHLO_Token, MHLO_Tuple]>;\n+defvar MHLO_TensorOrAnyTokenOrTuple = AnyTypeOf<[MHLO_Tensor, MHLO_AnyToken, MHLO_Tuple]>;\n defvar MHLO_TensorOrTokenOrTupleOrBuffer = AnyTypeOf<[MHLO_TensorOrTokenOrTuple, MHLO_Buffer]>;\n \n defvar MHLO_DimensionValue = HLO_DimensionValue;"
        },
        {
            "sha": "122a18e89f1f430affb7834642a53c4e13d0d814",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_ops.td",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.td?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -1400,7 +1400,7 @@ def MHLO_AsyncStartOp : MHLO_Op<\"async_start\", []> {\n   }];\n \n   let arguments = (ins\n-    Variadic<MHLO_TensorOrTokenOrTuple>:$inputs,\n+    Variadic<MHLO_TensorOrAnyTokenOrTuple>:$inputs,\n     FlatSymbolRefAttr:$called_computation,\n     StrAttr:$execution_thread\n   );\n@@ -1444,7 +1444,7 @@ def MHLO_AsyncDoneOp : MHLO_Op<\"async_done\", [DeclareOpInterfaceMethods<InferTyp\n \n   let arguments = (ins MHLO_AsyncBundle:$bundle);\n \n-  let results = (outs Variadic<MHLO_TensorOrTokenOrTuple>);\n+  let results = (outs Variadic<MHLO_TensorOrAnyTokenOrTuple>);\n   let hasVerifier = 1;\n   let hasCustomHLOConverter = 1;\n }"
        },
        {
            "sha": "7f9a0e0a6cde4cde6e7d56cbbbd60a64ff3087be",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_ops_typedefs.td",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops_typedefs.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops_typedefs.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops_typedefs.td?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -54,7 +54,7 @@ def MHLO_AsyncBundleTypeDef : TypeDef<MHLO_Dialect, \"AsyncBundle\"> {\n def MHLO_IsAsyncBundleTypePred : CPred<\"isa<::mlir::mhlo::AsyncBundleType>($_self)\">;\n \n def MHLO_AsyncBundle :\n-    MixedContainerType<AnyTypeOf<[MHLO_Tensor, MHLO_Token]>, MHLO_IsAsyncBundleTypePred,\n+    MixedContainerType<AnyTypeOf<[MHLO_Tensor, MHLO_AnyToken]>, MHLO_IsAsyncBundleTypePred,\n                        \"AsyncBundleType::getFlattenedTypes(cast<::mlir::mhlo::AsyncBundleType>($_self))\",\n                        \"async_bundle\">;\n "
        },
        {
            "sha": "8f918494161876c0cacfbf63b05b2552ced0818e",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 10,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -573,9 +573,11 @@ class HloToStablehloOpConverter\n \n  public:\n   HloToStablehloOpConverter(TypeConverter& converter, MLIRContext* context,\n-                            bool allowExperimentalFeatures)\n+                            bool allowExperimentalFeatures,\n+                            bool allowXlaFeatures)\n       : OpConversionPattern<HloOpTy>::OpConversionPattern(converter, context),\n-        allowExperimentalFeatures(allowExperimentalFeatures) {}\n+        allowExperimentalFeatures(allowExperimentalFeatures),\n+        allowXlaFeatures(allowXlaFeatures) {}\n \n   LogicalResult matchAndRewrite(\n       HloOpTy hloOp, typename HloOpTy::Adaptor adaptor,\n@@ -589,7 +591,8 @@ class HloToStablehloOpConverter\n     //   2) Features that might be a good fit for StableHLO but haven't yet\n     //      been proposed or approved in StableHLO. Conversion of such features\n     //      should succeed using custom_call extensibility protocol (see below).\n-    if (hasPrivateFeaturesNotInStablehlo(hloOp)) return failure();\n+    if (!allowXlaFeatures && hasPrivateFeaturesNotInStablehlo(hloOp))\n+      return failure();\n \n     // These operands have already been converted to StableHLO by\n     // the dialect conversion infrastructure.\n@@ -611,9 +614,11 @@ class HloToStablehloOpConverter\n     // turn out that the original MHLO op no longer exists or has different\n     // attributes in the current version.\n     bool hasExperimentalFeatures = hasExperimentalFeaturesNotInStablehlo(hloOp);\n-    if (!allowExperimentalFeatures && hasExperimentalFeatures) return failure();\n+    if (!allowXlaFeatures && !allowExperimentalFeatures &&\n+        hasExperimentalFeatures)\n+      return failure();\n     auto hasPublicFeatures = hasPublicFeaturesNotInStablehlo(hloOp);\n-    if (hasPublicFeatures || hasExperimentalFeatures) {\n+    if (!allowXlaFeatures && (hasPublicFeatures || hasExperimentalFeatures)) {\n       return rewriteMhloOpAsCustomCall(\n           hloOp, rewriter, this->getTypeConverter(), stablehloOperands);\n     }\n@@ -667,14 +672,19 @@ class HloToStablehloOpConverter\n   }\n \n   bool allowExperimentalFeatures;\n+  bool allowXlaFeatures;\n };\n \n // Deprecated ops.\n template <>\n class HloToStablehloOpConverter<stablehlo::UnaryEinsumOp>\n     : public OpConversionPattern<stablehlo::UnaryEinsumOp> {\n  public:\n-  using OpConversionPattern::OpConversionPattern;\n+  HloToStablehloOpConverter(TypeConverter& converter, MLIRContext* context,\n+                            bool /*allowExperimentalFeatures*/,\n+                            bool /*allowXlaFeatures*/)\n+      : OpConversionPattern<stablehlo::UnaryEinsumOp>::OpConversionPattern(\n+            converter, context) {}\n   LogicalResult matchAndRewrite(stablehlo::UnaryEinsumOp stablehloOp,\n                                 typename stablehlo::UnaryEinsumOp::Adaptor,\n                                 ConversionPatternRewriter&) const final {\n@@ -687,9 +697,10 @@ template <typename... StablehloOpTypes>\n void populateHloToStablehloPatterns(RewritePatternSet* patterns,\n                                     TypeConverter* converter,\n                                     MLIRContext* context,\n-                                    bool allowExperimentalFeatures) {\n+                                    bool allowExperimentalFeatures,\n+                                    bool allowXlaFeatures) {\n   patterns->add<HloToStablehloOpConverter<StablehloOpTypes>...>(\n-      *converter, context, allowExperimentalFeatures);\n+      *converter, context, allowExperimentalFeatures, allowXlaFeatures);\n }\n \n template <typename... HloOpTypes>\n@@ -706,7 +717,8 @@ void populateHloToStablehloCustomCallPatterns(RewritePatternSet* patterns,\n void populateHloToStablehloPatterns(RewritePatternSet* patterns,\n                                     TypeConverter* converter,\n                                     MLIRContext* context,\n-                                    bool allowExperimentalFeatures) {\n+                                    bool allowExperimentalFeatures,\n+                                    bool allowXlaFeatures) {\n   // Populate conversion patterns for all StableHLO ops.\n   // Our guiding principle is to support all StableHLO functionality in MHLO.\n   // The inverse is not necessarily true - some MHLO ops are missing from\n@@ -716,7 +728,13 @@ void populateHloToStablehloPatterns(RewritePatternSet* patterns,\n   populateHloToStablehloPatterns<\n #define GET_OP_LIST\n #include \"stablehlo/dialect/StablehloOps.cpp.inc\"\n-      >(patterns, converter, context, allowExperimentalFeatures);\n+      >(patterns, converter, context, allowExperimentalFeatures,\n+        allowXlaFeatures);\n+\n+  populateHloToStablehloPatterns<mhlo::AddDependencyOp, mhlo::AsyncStartOp,\n+                                 mhlo::AsyncUpdateOp, mhlo::AsyncDoneOp>(\n+      patterns, converter, context, allowExperimentalFeatures,\n+      allowXlaFeatures);\n \n   populateHloToStablehloCustomCallPatterns<mhlo::AcosOp, mhlo::AcoshOp,\n                                            mhlo::ErfOp, mhlo::TopKOp>("
        },
        {
            "sha": "e4b7a1a91f2b31e5fbd28f34f517638ad82a7ef6",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo_pass.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 41,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -19,17 +19,20 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/Debug.h\"\n #include \"mhlo/IR/hlo_ops.h\"\n #include \"mhlo/transforms/passes.h\"\n #include \"mhlo/transforms/rewriters.h\"\n #include \"mhlo/utils/type_conversion.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/DialectRegistry.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Support/TypeID.h\"\n+#include \"mlir/Support/WalkResult.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n \n@@ -41,44 +44,27 @@ namespace mhlo {\n \n namespace {\n \n-// AddDependencyOp is the only op that doesn't exist in StableHLO but uses\n-// token types. This led to two options (1) support either token type in\n-// AddDependencyOp or (2) Design a token conversion (or unrealized cast) between\n-// MHLO and StableHLO. Option (1) seems safer, and we can hopefully obsolete\n-// mhlo::TokenType all together and just use StableHLO tokens everywhere.\n-//\n-// Note: Only the second argument needs to be converted. All token creation and\n-// propagation is already handled by existing conversions.\n-struct AddDependencyOpToStablehloTokenConverter\n-    : public OpConversionPattern<mhlo::AddDependencyOp> {\n-  using OpConversionPattern::OpConversionPattern;\n-  LogicalResult matchAndRewrite(\n-      mhlo::AddDependencyOp op, mhlo::AddDependencyOpAdaptor adaptor,\n-      ConversionPatternRewriter& rewriter) const override {\n-    // Only convert if input token type is MHLO token\n-    if (!llvm::isa<stablehlo::TokenType>(adaptor.getToken().getType()))\n-      return rewriter.notifyMatchFailure(op, \"nothing to convert\");\n-    rewriter.replaceOpWithNewOp<mhlo::AddDependencyOp>(op, adaptor.getOperand(),\n-                                                       adaptor.getToken());\n-    return success();\n+bool hasMhloTypes(TypeRange types) {\n+  bool hasMhloType = false;\n+  for (Type type : types) {\n+    type.walk([&](Type t) {\n+      if (auto tuple = dyn_cast<TupleType>(t)) {\n+        hasMhloType = hasMhloType || hasMhloTypes(tuple.getTypes());\n+      } else if (auto bundle = dyn_cast<mhlo::AsyncBundleType>(t)) {\n+        hasMhloType = hasMhloType || hasMhloTypes(bundle.getTypes());\n+      } else if (auto rankedTensor = dyn_cast<RankedTensorType>(t)) {\n+        hasMhloType =\n+            hasMhloType || llvm::isa_and_nonnull<mhlo::TypeExtensionsAttr>(\n+                               rankedTensor.getEncoding());\n+      } else if (llvm::isa<mhlo::MhloDialect>(t.getDialect())) {\n+        hasMhloType = true;\n+      }\n+      if (hasMhloType) return WalkResult::interrupt();\n+      return WalkResult::advance();\n+    });\n   }\n-};\n-\n-bool hasMhloOperand(Operation* op) {\n-  return llvm::any_of(op->getOperandTypes(), [](Type type) {\n-    // Check for !stablehlo.token\n-    if (llvm::isa<mhlo::MhloDialect>(type.getDialect())) return true;\n-\n-    // Check for tensor<X, #stablehlo.bounds<...>>\n-    if (auto rankedType = dyn_cast<RankedTensorType>(type)) {\n-      return llvm::isa_and_nonnull<mhlo::TypeExtensionsAttr>(\n-          rankedType.getEncoding());\n-    }\n-    // Not StableHLO\n-    return false;\n-  });\n+  return hasMhloType;\n }\n-\n struct UpdateOperandsInUnknownOp : public ConversionPattern {\n   UpdateOperandsInUnknownOp(TypeConverter& converter, MLIRContext* context)\n       : ConversionPattern(converter, MatchAnyOpTypeTag(), /*benefit=*/1,\n@@ -91,7 +77,7 @@ struct UpdateOperandsInUnknownOp : public ConversionPattern {\n             op->getDialect()))\n       return rewriter.notifyMatchFailure(op, \"op is not an unknown op\");\n \n-    if (!hasMhloOperand(op))\n+    if (!hasMhloTypes(op->getOperandTypes()))\n       return rewriter.notifyMatchFailure(op, \"op has no mhlo operands\");\n \n     rewriter.modifyOpInPlace(op, [&]() { op->setOperands(operands); });\n@@ -115,7 +101,8 @@ struct HloLegalizeToStablehloPass\n     stablehlo::HloToStablehloTypeConverter converter;\n     RewritePatternSet patterns(&getContext());\n     stablehlo::populateHloToStablehloPatterns(\n-        &patterns, &converter, &getContext(), allow_experimental_features_);\n+        &patterns, &converter, &getContext(), allow_experimental_features_,\n+        allow_xla_features_);\n     stablehlo::registerFuncOpsForTypeConversion(target, patterns, converter);\n \n     if (allow_xla_features_) {\n@@ -127,20 +114,32 @@ struct HloLegalizeToStablehloPass\n                         mhlo::StochasticConvertOp, mhlo::TopKOp, mhlo::TraceOp,\n                         mhlo::XlaRngGetAndUpdateStateOp>();\n       target.addDynamicallyLegalOp<mhlo::AddDependencyOp>(\n-          [](mhlo::AddDependencyOp op) { return !hasMhloOperand(op); });\n+          [](mhlo::AddDependencyOp op) {\n+            return !hasMhloTypes(op->getOperandTypes());\n+          });\n+      target.addDynamicallyLegalOp<mhlo::AsyncStartOp>(\n+          [](mhlo::AsyncStartOp op) {\n+            return !hasMhloTypes(op->getResultTypes());\n+          });\n+      target.addDynamicallyLegalOp<mhlo::AsyncUpdateOp>(\n+          [](mhlo::AsyncUpdateOp op) {\n+            return !hasMhloTypes(op->getResultTypes());\n+          });\n+      target.addDynamicallyLegalOp<mhlo::AsyncDoneOp>([](mhlo::AsyncDoneOp op) {\n+        return !hasMhloTypes(op->getResultTypes());\n+      });\n       target.addDynamicallyLegalOp<mhlo::CustomCallOp>(\n           [](mhlo::CustomCallOp op) {\n             return !!op.getCustomCallScheduleAttr();\n           });\n       // TODO: StableHLO AllToAll has different semantics than MHLO AllToAll.\n       target.addDynamicallyLegalOp<mhlo::AllToAllOp>(\n           [](mhlo::AllToAllOp op) { return op.getNumOperands() > 1; });\n-      patterns.add<AddDependencyOpToStablehloTokenConverter>(&getContext());\n     }\n \n     // Handle non-MHLO ops that may have bounded dynamism or token types.\n     target.markUnknownOpDynamicallyLegal(\n-        [](Operation* op) { return !hasMhloOperand(op); });\n+        [](Operation* op) { return !hasMhloTypes(op->getOperandTypes()); });\n     patterns.add<UpdateOperandsInUnknownOp>(converter, &getContext());\n \n     if (failed(applyPartialConversion(getOperation(), target,"
        },
        {
            "sha": "d83cfe7720abc492290c006d4e4cdb15aeaf6c1e",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/map_stablehlo_to_hlo_op.h",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmap_stablehlo_to_hlo_op.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmap_stablehlo_to_hlo_op.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmap_stablehlo_to_hlo_op.h?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -162,6 +162,22 @@ MAP_STABLEHLO_TO_HLO(XorOp)\n \n #undef MAP_STABLEHLO_TO_HLO\n \n+#define MAP_HLO_TO_HLO_TYPE_REWRITE(OpName)   \\\n+  template <>                                 \\\n+  struct HloToStablehloOpImpl<mhlo::OpName> { \\\n+    using Type = mhlo::OpName;                \\\n+  };                                          \\\n+  template <>                                 \\\n+  struct StablehloToHloOpImpl<mhlo::OpName> { \\\n+    using Type = mhlo::OpName;                \\\n+  };\n+MAP_HLO_TO_HLO_TYPE_REWRITE(AddDependencyOp)\n+MAP_HLO_TO_HLO_TYPE_REWRITE(AsyncStartOp)\n+MAP_HLO_TO_HLO_TYPE_REWRITE(AsyncUpdateOp)\n+MAP_HLO_TO_HLO_TYPE_REWRITE(AsyncDoneOp)\n+\n+#undef MAP_HLO_TO_HLO_TYPE_REWRITE\n+\n }  // namespace stablehlo\n }  // namespace mlir\n "
        },
        {
            "sha": "ad758b670c8f8c0de5aca3d0119102822faa1c88",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/rewriters.h",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Frewriters.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Frewriters.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Frewriters.h?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -134,10 +134,14 @@ namespace stablehlo {\n // Also see `stablehlo::registerFuncOpsForTypeConversion` for helper patterns\n // which make sure `func.func`, `func.call` and `func.return` which involve\n // illegal types also get converted.\n-void populateHloToStablehloPatterns(RewritePatternSet *patterns,\n-                                    TypeConverter *converter,\n-                                    MLIRContext *context,\n-                                    bool allowExperimentalFeatures);\n+//\n+// allowXlaFeatures: If true, allows ops that are not in StableHLO to remain as\n+// MHLO ops.\n+void populateHloToStablehloPatterns(RewritePatternSet* patterns,\n+                                    TypeConverter* converter,\n+                                    MLIRContext* context,\n+                                    bool allowExperimentalFeatures,\n+                                    bool allowXlaFeatures);\n \n // Populates StableHLO ops to MHLO ops rewriting patterns.\n // Also see `stablehlo::registerFuncOpsForTypeConversion` for helper patterns"
        },
        {
            "sha": "536d09200dad39c95adc5887be303bf95448096d",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/stablehlo_legalize_to_hlo/stablehlo_legalize_to_hlo.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 42,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fstablehlo_legalize_to_hlo%2Fstablehlo_legalize_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fstablehlo_legalize_to_hlo%2Fstablehlo_legalize_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fstablehlo_legalize_to_hlo%2Fstablehlo_legalize_to_hlo.cc?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -20,22 +20,28 @@ limitations under the License.\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/Debug.h\"\n #include \"mhlo/IR/hlo_ops.h\"\n #include \"mhlo/transforms/map_stablehlo_to_hlo_op.h\"\n #include \"mhlo/transforms/rewriters.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Operation.h\"\n #include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/TypeRange.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Support/WalkResult.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n \n+#define DEBUG_TYPE \"stablehlo-conversions\"\n+\n namespace mlir {\n namespace stablehlo {\n namespace {\n@@ -426,46 +432,30 @@ class StablehloToHloOpConverter : public OpConversionPattern<StablehloOpTy> {\n   }\n };\n \n-// AddDependencyOp is the only op that doesn't exist in StableHLO but uses\n-// token types. This led to two options (1) support either token type in\n-// AddDependencyOp or (2) Design a token conversion (or unrealized cast) between\n-// MHLO and StableHLO. Option (1) seems safer, and we can hopefully obsolete\n-// mhlo::TokenType all together and just use StableHLO tokens everywhere.\n-//\n-// Note: Only the second argument needs to be converted. All token creation and\n-// propagation is already handled by existing conversions.\n-struct AddDependencyOpToMhoTokenConverter\n-    : public OpConversionPattern<mhlo::AddDependencyOp> {\n-  using OpConversionPattern::OpConversionPattern;\n-  LogicalResult matchAndRewrite(\n-      mhlo::AddDependencyOp op, mhlo::AddDependencyOpAdaptor adaptor,\n-      ConversionPatternRewriter& rewriter) const override {\n-    // Only convert if input token type is MHLO token\n-    if (!llvm::isa<mhlo::TokenType>(adaptor.getToken().getType()))\n-      return rewriter.notifyMatchFailure(op, \"nothing to convert\");\n-    rewriter.replaceOpWithNewOp<mhlo::AddDependencyOp>(op, adaptor.getOperand(),\n-                                                       adaptor.getToken());\n-    return success();\n+bool hasStablehloTypes(TypeRange types) {\n+  bool hasStablehloType = false;\n+  for (Type type : types) {\n+    type.walk([&](Type t) {\n+      if (auto tuple = dyn_cast<TupleType>(t)) {\n+        hasStablehloType = hasStablehloTypes(tuple.getTypes());\n+      } else if (auto tuple = dyn_cast<mhlo::AsyncBundleType>(t)) {\n+        hasStablehloType = hasStablehloTypes(tuple.getTypes());\n+      } else if (auto rankedTensor = dyn_cast<RankedTensorType>(t)) {\n+        hasStablehloType = llvm::isa_and_nonnull<stablehlo::TypeExtensionsAttr>(\n+            rankedTensor.getEncoding());\n+      } else if (llvm::isa<stablehlo::StablehloDialect>(t.getDialect())) {\n+        hasStablehloType = true;\n+      }\n+      if (hasStablehloType) return WalkResult::interrupt();\n+      return WalkResult::advance();\n+    });\n   }\n-};\n-\n-bool hasStablehloOperand(Operation* op) {\n-  return llvm::any_of(op->getOperandTypes(), [](Type type) {\n-    // Check for !stablehlo.token\n-    if (llvm::isa<stablehlo::StablehloDialect>(type.getDialect())) return true;\n-\n-    // Check for tensor<X, #stablehlo.bounds<...>>\n-    if (auto rankedType = dyn_cast<RankedTensorType>(type)) {\n-      return llvm::isa_and_nonnull<stablehlo::TypeExtensionsAttr>(\n-          rankedType.getEncoding());\n-    }\n-    // Not StableHLO\n-    return false;\n-  });\n+  LLVM_DEBUG(llvm::dbgs() << \"hasStablehloTypes: \" << hasStablehloType << \"\\n\");\n+  return hasStablehloType;\n }\n \n-struct UpdateOperandsInUnknownOp : public ConversionPattern {\n-  UpdateOperandsInUnknownOp(TypeConverter& converter, MLIRContext* context)\n+struct UpdateOperandsPattern : public ConversionPattern {\n+  UpdateOperandsPattern(TypeConverter& converter, MLIRContext* context)\n       : ConversionPattern(converter, MatchAnyOpTypeTag(), /*benefit=*/1,\n                           context) {}\n   LogicalResult matchAndRewrite(\n@@ -476,7 +466,7 @@ struct UpdateOperandsInUnknownOp : public ConversionPattern {\n             op->getDialect()))\n       return rewriter.notifyMatchFailure(op, \"op is not an unknown op\");\n \n-    if (!hasStablehloOperand(op))\n+    if (!hasStablehloTypes(op->getOperandTypes()))\n       return rewriter.notifyMatchFailure(op, \"op has no stablehlo operands\");\n \n     rewriter.modifyOpInPlace(op, [&]() { op->setOperands(operands); });\n@@ -518,10 +508,13 @@ void populateStablehloToHloPatterns(RewritePatternSet* patterns,\n #include \"stablehlo/dialect/StablehloOps.cpp.inc\"\n       >(patterns, converter, context);\n \n+  populateStablehloToHloPatterns<mhlo::AddDependencyOp, mhlo::AsyncStartOp,\n+                                 mhlo::AsyncUpdateOp, mhlo::AsyncDoneOp>(\n+      patterns, converter, context);\n+\n   // Populate conversion patterns for ops that don't exist in StableHLO\n   // and unknown dialect ops that may have StableHLO operands.\n-  patterns->add<AddDependencyOpToMhoTokenConverter>(context);\n-  patterns->add<UpdateOperandsInUnknownOp>(*converter, context);\n+  patterns->add<UpdateOperandsPattern>(*converter, context);\n }\n \n void setupStablehloToHloConversionTarget(ConversionTarget& target) {\n@@ -530,9 +523,20 @@ void setupStablehloToHloConversionTarget(ConversionTarget& target) {\n \n   // Some ops may have MHLO / StableHLO types in operands\n   target.addDynamicallyLegalOp<mhlo::AddDependencyOp>(\n-      [](mhlo::AddDependencyOp op) { return !hasStablehloOperand(op); });\n+      [](mhlo::AddDependencyOp op) {\n+        return !hasStablehloTypes(op->getOperandTypes());\n+      });\n+  target.addDynamicallyLegalOp<mhlo::AsyncStartOp>([](mhlo::AsyncStartOp op) {\n+    return !hasStablehloTypes(op->getResultTypes());\n+  });\n+  target.addDynamicallyLegalOp<mhlo::AsyncUpdateOp>([](mhlo::AsyncUpdateOp op) {\n+    return !hasStablehloTypes(op->getResultTypes());\n+  });\n+  target.addDynamicallyLegalOp<mhlo::AsyncDoneOp>([](mhlo::AsyncDoneOp op) {\n+    return !hasStablehloTypes(op->getResultTypes());\n+  });\n   target.markUnknownOpDynamicallyLegal(\n-      [](Operation* op) { return !hasStablehloOperand(op); });\n+      [](Operation* op) { return !hasStablehloTypes(op->getOperandTypes()); });\n }\n \n }  // namespace stablehlo"
        },
        {
            "sha": "181a858a7ad94f0032603cb8eca4ba49b24441fd",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/type_conversion.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -201,15 +201,16 @@ HloTypeConverter::HloTypeConverter() {\n     if (failed(convertTypes(type.getTypes(), convertedTypes))) return {};\n     return TupleType::get(type.getContext(), convertedTypes);\n   });\n+  // Similar to tuple, replace contents with StableHLO/MHLO types.\n+  addConversion([&](mhlo::AsyncBundleType bundle) -> Type {\n+    SmallVector<Type> convertedTypes;\n+    if (failed(convertTypes(bundle.getTypes(), convertedTypes))) return {};\n+    return mhlo::AsyncBundleType::get(bundle.getContext(), convertedTypes);\n+  });\n }\n \n HloToStablehloTypeConverter::HloToStablehloTypeConverter()\n     : HloTypeConverter() {\n-  // !mhlo.async_bundle is only used in mhlo.async_start, mhlo.async_update\n-  // and mhlo.async_done which are private to XLA.\n-  // This means that these ops are deliberately not part of StableHLO,\n-  // and as a result this type is not part of StableHLO either.\n-  addConversion([](mhlo::AsyncBundleType) -> Type { return {}; });\n   addConversion([](mhlo::TokenType type) -> Type {\n     return stablehlo::TokenType::get(type.getContext());\n   });"
        },
        {
            "sha": "6357ab37b4122d532d888ce7b315107b6294da9d",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-legalize-to-stablehlo-partial.mlir",
            "status": "modified",
            "additions": 12,
            "deletions": 16,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo-partial.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo-partial.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo-partial.mlir?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -1,23 +1,19 @@\n // RUN: mlir-hlo-opt %s -hlo-legalize-to-stablehlo=allow-xla-features --split-input-file | FileCheck %s\n \n-func.func @async_ops(%arg1: tensor<128x32xf32>) -> tensor<128x128xf32> attributes {execution_thread = \"main\"} {\n-  // CHECK: stablehlo.all_gather\n-  %0 = \"mhlo.all_gather\"(%arg1) {\n-    all_gather_dim = 1 : i64,\n-    channel_handle = #mhlo.channel_handle<handle = 1, type = 0>,\n-    shard_count = 4,\n-    replica_groups = dense<[[0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>,\n-    use_global_device_ids\n-  } : (tensor<128x32xf32>) -> tensor<128x128xf32>\n-  return %0 : tensor<128x128xf32>\n+// CHECK-LABEL: @recv\n+func.func @recv(%arg0: !mhlo.token) -> (tensor<3x4xi32>, !mhlo.token) attributes {execution_thread = \"main\"} {\n+  // CHECK: stablehlo.recv{{.*}}!stablehlo.token\n+  %0:2 = \"mhlo.recv\"(%arg0) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 3>, is_host_transfer = true}> : (!mhlo.token) -> (tensor<3x4xi32>, !mhlo.token)\n+  return %0#0, %0#1 : tensor<3x4xi32>, !mhlo.token\n }\n \n-func.func @main(%arg0: tensor<128x32xf32>) -> tensor<128x128xf32> {\n-  // CHECK: mhlo.async_start\n-  %0 = \"mhlo.async_start\"(%arg0) {called_computation = @async_ops, execution_thread = \"main\"} : (tensor<128x32xf32>) -> !mhlo.async_bundle<tensor<128x32xf32>, tensor<128x128xf32>>\n-  // CHECK: mhlo.async_done\n-  %1 = \"mhlo.async_done\"(%0) : (!mhlo.async_bundle<tensor<128x32xf32>, tensor<128x128xf32>>) -> tensor<128x128xf32>\n-  return %1 : tensor<128x128xf32>\n+// CHECK-LABEL: @async_ops_with_token\n+func.func @async_ops_with_token(%arg0: !mhlo.token) -> (tensor<3x4xi32>, !mhlo.token) {\n+  // CHECK: mhlo.async_start{{.*}} !mhlo.async_bundle<!stablehlo.token, tuple<tensor<3x4xi32>, !stablehlo.token>, tensor<i32>>\n+  %0 = \"mhlo.async_start\"(%arg0) <{called_computation = @recv, execution_thread = \"main\"}> : (!mhlo.token) -> !mhlo.async_bundle<!mhlo.token, tuple<tensor<3x4xi32>, !mhlo.token>, tensor<i32>>\n+  // CHECK: mhlo.async_done{{.*}} -> (tensor<3x4xi32>, !stablehlo.token)\n+  %1:2 = \"mhlo.async_done\"(%0) : (!mhlo.async_bundle<!mhlo.token, tuple<tensor<3x4xi32>, !mhlo.token>, tensor<i32>>) -> (tensor<3x4xi32>, !mhlo.token)\n+  return %1#0, %1#1 : tensor<3x4xi32>, !mhlo.token\n }\n \n // -----"
        },
        {
            "sha": "9fe1b4a7dc92f3822b1d53bbee16eaec74ed59dc",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/stablehlo-legalize-to-hlo.mlir",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de851b35011925b85d5f7a9a7c0308f3a9753946/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir?ref=de851b35011925b85d5f7a9a7c0308f3a9753946",
            "patch": "@@ -458,6 +458,30 @@ func.func @add_dependency(%arg0: tensor<3x4xf32>) -> tensor<3x4xf32> {\n \n // -----\n \n+// CHECK-LABEL: \"recv\"\n+func.func @recv(%token: !stablehlo.token) -> (tensor<3x4xi32>, !stablehlo.token) attributes {execution_thread = \"main\"} {\n+  // CHECK: mhlo.recv{{.*}}!mhlo.token\n+  %0:2 = \"stablehlo.recv\"(%token) {\n+    channel_handle = #stablehlo.channel_handle<\n+      handle = 5,\n+      type = 3  // Host to device channel\n+    >,\n+    is_host_transfer = true\n+  } : (!stablehlo.token) -> (tensor<3x4xi32>, !stablehlo.token)\n+  func.return %0#0, %0#1 : tensor<3x4xi32>, !stablehlo.token\n+}\n+\n+// CHECK-LABEL: \"async_ops_with_token\"\n+func.func @async_ops_with_token(%token: !stablehlo.token) -> (tensor<3x4xi32>, !stablehlo.token) {\n+  // CHECK: mhlo.async_start{{.*}} !mhlo.async_bundle<!mhlo.token, tuple<tensor<3x4xi32>, !mhlo.token>, tensor<i32>>\n+  %0 = \"mhlo.async_start\"(%token) {called_computation = @recv, execution_thread = \"main\"} : (!stablehlo.token) -> !mhlo.async_bundle<!stablehlo.token, tuple<tensor<3x4xi32>, !stablehlo.token>, tensor<i32>>\n+  // CHECK: mhlo.async_done{{.*}} -> (tensor<3x4xi32>, !mhlo.token)\n+  %1, %2 = \"mhlo.async_done\"(%0) : (!mhlo.async_bundle<!stablehlo.token, tuple<tensor<3x4xi32>, !stablehlo.token>, tensor<i32>>) -> (tensor<3x4xi32>, !stablehlo.token)\n+  return %1, %2 : tensor<3x4xi32>, !stablehlo.token\n+}\n+\n+// -----\n+\n // CHECK-LABEL: \"op_after_all\"\n func.func @op_after_all(%arg0: !stablehlo.token) -> !stablehlo.token {\n   // CHECK: \"mhlo.after_all\"([[ARG0:%arg[0-9]+]]) : (!mhlo.token) -> !mhlo.token"
        }
    ],
    "stats": {
        "total": 305,
        "additions": 184,
        "deletions": 121
    }
}