{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 829271059",
    "sha": "d38930f29e30904a5625b2ed392c655cfbb0dc0b",
    "files": [
        {
            "sha": "92ef3e2fc0b454175cc19868a311684c332fec82",
            "filename": "tensorflow/core/grappler/optimizers/inference/batch_op_rewriter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d38930f29e30904a5625b2ed392c655cfbb0dc0b/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Finference%2Fbatch_op_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d38930f29e30904a5625b2ed392c655cfbb0dc0b/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Finference%2Fbatch_op_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Finference%2Fbatch_op_rewriter.cc?ref=d38930f29e30904a5625b2ed392c655cfbb0dc0b",
            "patch": "@@ -47,7 +47,7 @@ constexpr char kBatchTimeoutMicrosAttr[] = \"batch_timeout_micros\";\n constexpr char kAllowedBatchSizesAttr[] = \"allowed_batch_sizes\";\n constexpr char kMaxEnqueuedBatchesAttr[] = \"max_enqueued_batches\";\n constexpr char kEnableLargeBatchSplitting[] = \"enable_large_batch_splitting\";\n-constexpr int64 kBoostMicrosNotSet = -1;\n+constexpr int64_t kBoostMicrosNotSet = -1;\n \n using BatchOpRewriteFunction = std::function<void(NodeDef* batch_op)>;\n \n@@ -61,10 +61,10 @@ using ::tensorflow::grappler::GrapplerItem;\n namespace {\n // Parameters for adaptive batch scheduler only.\n struct AdaptiveBatchSchedulerParams {\n-  int32 initial_inflight_batches;\n-  int32 min_inflight_batches;\n-  int32 max_inflight_batches;\n-  int32 batches_to_average_over;\n+  int32_t initial_inflight_batches;\n+  int32_t min_inflight_batches;\n+  int32_t max_inflight_batches;\n+  int32_t batches_to_average_over;\n   int64_t full_batch_scheduling_boost_micros;\n };\n \n@@ -175,7 +175,7 @@ Status BatchOpRewriter::Optimize(Cluster* cluster, const GrapplerItem& item,\n   bool asbs_overridden = false;\n   if (config_proto_.has_experimental() &&\n       config_proto_.experimental().has_session_metadata()) {\n-    const string model_name =\n+    const std::string model_name =\n         config_proto_.experimental().session_metadata().name();\n \n     if (!config_.model_scheduler_options().empty()) {"
        },
        {
            "sha": "74a4a03aa38ecd4db35782a9c78d4e8b098caf03",
            "filename": "tensorflow/core/grappler/optimizers/inference/batch_op_rewriter_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d38930f29e30904a5625b2ed392c655cfbb0dc0b/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Finference%2Fbatch_op_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d38930f29e30904a5625b2ed392c655cfbb0dc0b/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Finference%2Fbatch_op_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Finference%2Fbatch_op_rewriter_test.cc?ref=d38930f29e30904a5625b2ed392c655cfbb0dc0b",
            "patch": "@@ -46,17 +46,17 @@ using ::tensorflow::grappler::GrapplerItem;\n using ::tensorflow::serving::BatchOpRewriteConfig;\n \n // Add batch op in both GraphDef.node and GraphDef.library.function.node_def.\n-void AddBatchOp(GraphDef* graph, int num_batch_threads = 16,\n-                const absl::flat_hash_map<string, int>& reserved_int_attrs = {},\n-                int max_batch_size = 16, int batch_timeout_micros = 10000,\n-                const std::vector<int32>& allowed_batch_sizes = {8, 16},\n-                int max_enqueued_batches = 1000,\n-                bool disable_large_batch_splitting = false,\n-                std::string_view mixed_priority_policy = \"\",\n-                int low_priority_max_batch_size = -1,\n-                int low_priority_batch_timeout_micros = -1,\n-                const std::vector<int32>& low_priority_allowed_batch_sizes = {},\n-                int low_priority_max_enqueued_batches = -1) {\n+void AddBatchOp(\n+    GraphDef* graph, int num_batch_threads = 16,\n+    const absl::flat_hash_map<std::string, int>& reserved_int_attrs = {},\n+    int max_batch_size = 16, int batch_timeout_micros = 10000,\n+    const std::vector<int32_t>& allowed_batch_sizes = {8, 16},\n+    int max_enqueued_batches = 1000, bool disable_large_batch_splitting = false,\n+    std::string_view mixed_priority_policy = \"\",\n+    int low_priority_max_batch_size = -1,\n+    int low_priority_batch_timeout_micros = -1,\n+    const std::vector<int32_t>& low_priority_allowed_batch_sizes = {},\n+    int low_priority_max_enqueued_batches = -1) {\n   auto set_batch_node_attribute = [&](const int32_t num_batch_threads,\n                                       NodeDef* batch_op) {\n     batch_op->set_name(\"cond/batch/BatchFunction\");\n@@ -288,7 +288,7 @@ TEST_F(BatchOpRewriterTest, UpdateBatchOptions) {\n       128);\n   (*config.mutable_batch_options())[\"model_with_override\"]\n       .set_batch_timeout_micros(5000);\n-  const std::vector<int32> allowed_batch_sizes{4, 32};\n+  const std::vector<int32_t> allowed_batch_sizes{4, 32};\n   (*config.mutable_batch_options())[\"model_with_override\"]\n       .mutable_allowed_batch_sizes()\n       ->Add(allowed_batch_sizes.begin(), allowed_batch_sizes.end());"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 18,
        "deletions": 18
    }
}