{
    "author": "ermilovmaxim",
    "message": "Port more BufferUse users to provide shape\n\nPiperOrigin-RevId: 839508160",
    "sha": "21dff5509ae63585e9d1bb6528d0a51849132cc8",
    "files": [
        {
            "sha": "805d9e55e1b8c35d8ca9da8744ab9641f2061fee",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_fusion_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_fusion_thunk.cc?ref=21dff5509ae63585e9d1bb6528d0a51849132cc8",
            "patch": "@@ -154,10 +154,10 @@ OneDnnFusionThunk::~OneDnnFusionThunk() = default;\n OneDnnFusionThunk::BufferUses OneDnnFusionThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const Argument& argument : arguments_) {\n-    buffer_uses.push_back(BufferUse::Read(argument.slice));\n+    buffer_uses.push_back(BufferUse::Read(argument.slice, argument.shape));\n   }\n   for (const Result& result : results_) {\n-    buffer_uses.push_back(BufferUse::Write(result.slice));\n+    buffer_uses.push_back(BufferUse::Write(result.slice, result.shape));\n   }\n   return buffer_uses;\n }"
        },
        {
            "sha": "8c14c6c4aa99ecc666a0af7adc73d758e5911e1a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass_test.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 13,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc?ref=21dff5509ae63585e9d1bb6528d0a51849132cc8",
            "patch": "@@ -168,11 +168,14 @@ TEST_F(ThunkBufferDebugPassTest, IsNoOpWhenHloModuleIsNull) {\n   debug_options.set_xla_gpu_experimental_enable_checksum_tracing_on_thunks(\n       true);\n   se::DeviceDescription device_info;\n+\n   FakeThunkPassBufferAllocator allocator;\n   BufferAllocation alloc(0, 1024, 0);\n+  Shape arg_shape = ShapeUtil::MakeShape(U8, {1});\n   BufferAllocation::Slice slice(&alloc, 0, 1);\n+\n   auto fake_thunk = std::make_unique<FakeThunk>(\n-      Thunk::ThunkInfo(), Thunk::BufferUses{BufferUse::Read(slice)});\n+      Thunk::ThunkInfo(), Thunk::BufferUses{BufferUse::Read(slice, arg_shape)});\n   Thunk* fake_thunk_ptr = fake_thunk.get();\n   std::vector<std::unique_ptr<Thunk>> thunks;\n   thunks.push_back(std::move(fake_thunk));\n@@ -266,27 +269,28 @@ TEST_F(ThunkBufferDebugPassTest, RecursivelyInsertsBuffersDebugChecksumThunks) {\n   se::DeviceDescription device_info;\n   FakeThunkPassBufferAllocator allocator;\n   // Create a fake thunk with a few different buffer uses.\n+  Shape arg_shape = ShapeUtil::MakeShape(U8, {1});\n   BufferAllocation::Slice slice_while_condition = CreateSlice();\n   BufferAllocation::Slice slice_while_body = CreateSlice();\n   BufferAllocation::Slice slice_branch0 = CreateSlice();\n   BufferAllocation::Slice slice_branch1 = CreateSlice();\n   // Setup a thunk tree.\n   auto while_condition_fake_thunk = std::make_unique<FakeThunk>(\n       ThunkInfoWithId(kWhileConditionFakeThunkId),\n-      Thunk::BufferUses{BufferUse::Read(slice_while_condition)});\n+      Thunk::BufferUses{BufferUse::Read(slice_while_condition, arg_shape)});\n   const Thunk* const while_condition_fake_thunk_ptr =\n       while_condition_fake_thunk.get();\n   auto while_body_fake_thunk = std::make_unique<FakeThunk>(\n       ThunkInfoWithId(kWhileBodyId),\n-      Thunk::BufferUses{BufferUse::Read(slice_while_body)});\n+      Thunk::BufferUses{BufferUse::Read(slice_while_body, arg_shape)});\n   const Thunk* const while_body_fake_thunk_ptr = while_body_fake_thunk.get();\n   auto conditional_branch0_thunk = std::make_unique<FakeThunk>(\n       ThunkInfoWithId(kBranch0ThunkId),\n-      Thunk::BufferUses{BufferUse::Read(slice_branch0)});\n+      Thunk::BufferUses{BufferUse::Read(slice_branch0, arg_shape)});\n   const Thunk* const branch0_thunk_ptr = conditional_branch0_thunk.get();\n   auto conditional_branch1_thunk = std::make_unique<FakeThunk>(\n       ThunkInfoWithId(kBranch1ThunkId),\n-      Thunk::BufferUses{BufferUse::Read(slice_branch1)});\n+      Thunk::BufferUses{BufferUse::Read(slice_branch1, arg_shape)});\n   const Thunk* const branch1_thunk_ptr = conditional_branch1_thunk.get();\n   std::vector<std::unique_ptr<SequentialThunk>> branch_thunks;\n   branch_thunks.push_back(\n@@ -594,16 +598,19 @@ TEST_F(ThunkBufferDebugPassTest, FiltersThunksByIdRanges) {\n   FakeThunkPassBufferAllocator allocator;\n   // Create a fake thunk with a few different buffer uses.\n   BufferAllocation alloc(0, 1024, 0);\n+  Shape slice_shape = ShapeUtil::MakeShape(U8, {1});\n   BufferAllocation::Slice slice1_io(&alloc, 0, 1);\n   BufferAllocation::Slice slice2_io(&alloc, 1, 1);\n   Thunk::ThunkInfo fake_thunk1_info;\n   fake_thunk1_info.thunk_id = ThunkId(1);\n   auto fake_thunk1 = std::make_unique<FakeThunk>(\n-      fake_thunk1_info, Thunk::BufferUses{BufferUse::Read(slice1_io)});\n+      fake_thunk1_info,\n+      Thunk::BufferUses{BufferUse::Read(slice1_io, slice_shape)});\n   Thunk::ThunkInfo fake_thunk2_info;\n   fake_thunk2_info.thunk_id = ThunkId(2);\n   auto fake_thunk2 = std::make_unique<FakeThunk>(\n-      fake_thunk2_info, Thunk::BufferUses{BufferUse::Read(slice2_io)});\n+      fake_thunk2_info,\n+      Thunk::BufferUses{BufferUse::Read(slice2_io, slice_shape)});\n   Thunk* fake_thunk1_ptr = fake_thunk1.get();\n   Thunk* fake_thunk2_ptr = fake_thunk2.get();\n   std::vector<std::unique_ptr<Thunk>> thunks;\n@@ -654,23 +661,27 @@ TEST_F(ThunkBufferDebugPassTest, FiltersThunksByProfileAnnotationRegexes) {\n   FakeThunkPassBufferAllocator allocator;\n   // Create a fake thunk with a few different buffer uses.\n   BufferAllocation alloc(0, 1024, 0);\n+  Shape slice_shape = ShapeUtil::MakeShape(U8, {1});\n   BufferAllocation::Slice slice1_io(&alloc, 0, 1);\n   BufferAllocation::Slice slice2_io(&alloc, 1, 1);\n   Thunk::ThunkInfo fake_thunk1_info;\n   fake_thunk1_info.thunk_id = ThunkId(1);\n   fake_thunk1_info.profile_annotation = \"fake_thunk1\";\n   auto fake_thunk1 = std::make_unique<FakeThunk>(\n-      fake_thunk1_info, Thunk::BufferUses{BufferUse::Read(slice1_io)});\n+      fake_thunk1_info,\n+      Thunk::BufferUses{BufferUse::Read(slice1_io, slice_shape)});\n   Thunk::ThunkInfo fake_thunk2_info;\n   fake_thunk2_info.profile_annotation = \"fake_thunk2\";\n   fake_thunk2_info.thunk_id = ThunkId(2);\n   auto fake_thunk2 = std::make_unique<FakeThunk>(\n-      fake_thunk2_info, Thunk::BufferUses{BufferUse::Read(slice2_io)});\n+      fake_thunk2_info,\n+      Thunk::BufferUses{BufferUse::Read(slice2_io, slice_shape)});\n   Thunk::ThunkInfo fake_thunk3_info;\n   fake_thunk3_info.profile_annotation = \"fake_thunk3\";\n   fake_thunk3_info.thunk_id = ThunkId(3);\n   auto fake_thunk3 = std::make_unique<FakeThunk>(\n-      fake_thunk3_info, Thunk::BufferUses{BufferUse::Read(slice2_io)});\n+      fake_thunk3_info,\n+      Thunk::BufferUses{BufferUse::Read(slice2_io, slice_shape)});\n   Thunk* fake_thunk1_ptr = fake_thunk1.get();\n   Thunk* fake_thunk2_ptr = fake_thunk2.get();\n   Thunk* fake_thunk3_ptr = fake_thunk3.get();\n@@ -740,24 +751,28 @@ TEST_F(ThunkBufferDebugPassTest,\n   FakeThunkPassBufferAllocator allocator;\n   // Create a fake thunk with a few different buffer uses.\n   BufferAllocation alloc(0, 1024, 0);\n+  Shape slice_shape = ShapeUtil::MakeShape(U8, {1});\n   BufferAllocation::Slice slice1_io(&alloc, 0, 1);\n   BufferAllocation::Slice slice2_io(&alloc, 1, 1);\n   BufferAllocation::Slice slice3_io(&alloc, 2, 1);\n   Thunk::ThunkInfo fake_thunk1_info;\n   fake_thunk1_info.thunk_id = ThunkId(1);\n   fake_thunk1_info.profile_annotation = \"instrument_me\";\n   auto fake_thunk1 = std::make_unique<FakeThunk>(\n-      fake_thunk1_info, Thunk::BufferUses{BufferUse::Read(slice1_io)});\n+      fake_thunk1_info,\n+      Thunk::BufferUses{BufferUse::Read(slice1_io, slice_shape)});\n   Thunk::ThunkInfo fake_thunk2_info;\n   fake_thunk2_info.thunk_id = ThunkId(2);\n   fake_thunk2_info.profile_annotation = \"ignore_me\";\n   auto fake_thunk2 = std::make_unique<FakeThunk>(\n-      fake_thunk2_info, Thunk::BufferUses{BufferUse::Read(slice2_io)});\n+      fake_thunk2_info,\n+      Thunk::BufferUses{BufferUse::Read(slice2_io, slice_shape)});\n   Thunk::ThunkInfo fake_thunk3_info;\n   fake_thunk3_info.thunk_id = ThunkId(3);\n   fake_thunk3_info.profile_annotation = \"instrument_me\";\n   auto fake_thunk3 = std::make_unique<FakeThunk>(\n-      fake_thunk3_info, Thunk::BufferUses{BufferUse::Read(slice3_io)});\n+      fake_thunk3_info,\n+      Thunk::BufferUses{BufferUse::Read(slice3_io, slice_shape)});\n   Thunk* fake_thunk1_ptr = fake_thunk1.get();\n   Thunk* fake_thunk2_ptr = fake_thunk2.get();\n   Thunk* fake_thunk3_ptr = fake_thunk3.get();"
        },
        {
            "sha": "23030c71cffac655561489156e4ec4f8a6082296",
            "filename": "third_party/xla/xla/runtime/buffer_use.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.cc?ref=21dff5509ae63585e9d1bb6528d0a51849132cc8",
            "patch": "@@ -15,6 +15,8 @@ limitations under the License.\n \n #include \"xla/runtime/buffer_use.h\"\n \n+#include <vector>\n+\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/types/span.h\"\n@@ -27,20 +29,20 @@ BufferUse::ReadWriteSet::ReadWriteSet() = default;\n void BufferUse::ReadWriteSet::Add(BufferUse use) {\n   switch (use.access()) {\n     case BufferUse::MemoryAccess::kRead:\n-      AddRead(use.slice());\n+      AddRead(use);\n       break;\n     case BufferUse::MemoryAccess::kWrite:\n-      AddWrite(use.slice());\n+      AddWrite(use);\n       break;\n   }\n }\n \n-void BufferUse::ReadWriteSet::AddRead(BufferAllocation::Slice slice) {\n-  read_.insert(slice);\n+void BufferUse::ReadWriteSet::AddRead(const BufferUse& use) {\n+  read_.push_back(use);\n }\n \n-void BufferUse::ReadWriteSet::AddWrite(BufferAllocation::Slice slice) {\n-  write_.insert(slice);\n+void BufferUse::ReadWriteSet::AddWrite(const BufferUse& use) {\n+  write_.push_back(use);\n }\n \n void BufferUse::ReadWriteSet::AddAll(absl::Span<const BufferUse> uses) {\n@@ -51,12 +53,11 @@ void BufferUse::ReadWriteSet::AddAll(absl::Span<const BufferUse> uses) {\n \n bool BufferUse::ReadWriteSet::HasConflicts(const BufferUse& use) const {\n   // Returns true if `use` overlaps with any of the slices in set.\n-  auto overlaps = [](const absl::flat_hash_set<BufferAllocation::Slice>& set,\n-                     const BufferUse& use) {\n-    return set.contains(use.slice()) ||\n-           absl::c_any_of(set, [&](const BufferAllocation::Slice& slice) {\n-             return slice.OverlapsWith(use.slice());\n-           });\n+  auto overlaps = [](const std::vector<BufferUse>& set, const BufferUse& use) {\n+    return absl::c_any_of(set, [&](const BufferUse& other) {\n+      return other.slice_.OverlapsWith(use.slice()) ||\n+             other.slice_ == use.slice_;\n+    });\n   };\n \n   return use.access() == MemoryAccess::kWrite\n@@ -65,14 +66,12 @@ bool BufferUse::ReadWriteSet::HasConflicts(const BufferUse& use) const {\n }\n \n bool BufferUse::ReadWriteSet::HasConflicts(const ReadWriteSet& other) {\n-  return absl::c_any_of(other.read_,\n-                        [&](const BufferAllocation::Slice& slice) {\n-                          return HasConflicts(BufferUse::Read(slice));\n-                        }) ||\n-         absl::c_any_of(other.write_,\n-                        [&](const BufferAllocation::Slice& slice) {\n-                          return HasConflicts(BufferUse::Write(slice));\n-                        });\n+  return absl::c_any_of(\n+             other.read_,\n+             [&](const BufferUse& other) { return HasConflicts(other); }) ||\n+         absl::c_any_of(other.write_, [&](const BufferUse& other) {\n+           return HasConflicts(other);\n+         });\n }\n \n }  // namespace xla"
        },
        {
            "sha": "a00471abf86930c92e65cb23e8968910bd6b826c",
            "filename": "third_party/xla/xla/runtime/buffer_use.h",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use.h?ref=21dff5509ae63585e9d1bb6528d0a51849132cc8",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <cstdint>\n #include <optional>\n #include <tuple>\n+#include <vector>\n \n #include \"absl/base/attributes.h\"\n #include \"absl/container/flat_hash_set.h\"\n@@ -124,8 +125,8 @@ class BufferUse {\n     ReadWriteSet();\n \n     void Add(BufferUse use);\n-    void AddRead(BufferAllocation::Slice slice);\n-    void AddWrite(BufferAllocation::Slice slice);\n+    void AddRead(const BufferUse& use);\n+    void AddWrite(const BufferUse& use);\n \n     void AddAll(absl::Span<const BufferUse> uses);\n \n@@ -135,8 +136,8 @@ class BufferUse {\n     bool HasConflicts(const ReadWriteSet& other);\n \n    private:\n-    absl::flat_hash_set<BufferAllocation::Slice> read_;\n-    absl::flat_hash_set<BufferAllocation::Slice> write_;\n+    std::vector<BufferUse> read_;\n+    std::vector<BufferUse> write_;\n   };\n \n   bool operator==(const BufferUse& other) const {"
        },
        {
            "sha": "2467d4180d218c47e9750907e65a5647fc169e7a",
            "filename": "third_party/xla/xla/runtime/buffer_use_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fbuffer_use_test.cc?ref=21dff5509ae63585e9d1bb6528d0a51849132cc8",
            "patch": "@@ -97,12 +97,12 @@ TEST(BufferUseTest, ReadWriteSet) {\n   BufferAllocation::Slice slice1(&alloc, 4, 8);\n   BufferAllocation::Slice slice2(&alloc, 8, 8);\n \n-  rwset.Add(BufferUse::Read(slice0));\n+  rwset.Add(BufferUse::Read(slice0, slice_shape));\n   EXPECT_FALSE(rwset.HasConflicts({BufferUse::Read(slice1, slice_shape)}));\n   EXPECT_TRUE(rwset.HasConflicts({BufferUse::Write(slice1, slice_shape)}));\n   EXPECT_FALSE(rwset.HasConflicts({BufferUse::Write(slice2, slice_shape)}));\n \n-  rwset.Add(BufferUse::Read(slice1));\n+  rwset.Add(BufferUse::Read(slice1, slice_shape));\n   EXPECT_TRUE(rwset.HasConflicts({BufferUse::Write(slice2, slice_shape)}));\n }\n "
        },
        {
            "sha": "1df370b6e17f51da8a0922e3baf5fae996519d57",
            "filename": "third_party/xla/xla/runtime/execution_graph_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fexecution_graph_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/21dff5509ae63585e9d1bb6528d0a51849132cc8/third_party%2Fxla%2Fxla%2Fruntime%2Fexecution_graph_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fexecution_graph_test.cc?ref=21dff5509ae63585e9d1bb6528d0a51849132cc8",
            "patch": "@@ -182,9 +182,9 @@ TEST(ExecutionGraphTest, CollectivesResourceOrdering) {\n   operations.push_back(Operation({BufferUse::Read(slice1, slice_shape),\n                                   BufferUse::Write(slice1, slice_shape)},\n                                  {ResourceUse::Write(resource)}));\n-  operations.push_back(\n-      Operation({BufferUse::Read(slice1), BufferUse::Write(slice1)},\n-                {ResourceUse::Write(resource)}));\n+  operations.push_back(Operation({BufferUse::Read(slice1, slice_shape),\n+                                  BufferUse::Write(slice1, slice_shape)},\n+                                 {ResourceUse::Write(resource)}));\n \n   TF_ASSERT_OK_AND_ASSIGN(ExecutionGraph execution_graph,\n                           ExecutionGraph::Create<Operation>(operations));"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 59,
        "deletions": 44
    }
}