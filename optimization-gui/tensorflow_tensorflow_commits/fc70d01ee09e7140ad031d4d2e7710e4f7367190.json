{
    "author": "ermilovmaxim",
    "message": "Add proto serialization for AllGatherStartThunk\n\nPiperOrigin-RevId: 845478753",
    "sha": "fc70d01ee09e7140ad031d4d2e7710e4f7367190",
    "files": [
        {
            "sha": "69b1b5d0f589c1aa9a533881ec5d721a001c8fa4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -1246,11 +1246,13 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_communicator\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -1260,6 +1262,23 @@ cc_library(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"all_gather_thunk_test\",\n+    srcs = [\"all_gather_thunk_test.cc\"],\n+    deps = [\n+        \":all_gather_thunk\",\n+        \":collective_thunk\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"collective_kernel_thunk\",\n     srcs = [\"collective_kernel_thunk.cc\"],\n@@ -1802,6 +1821,7 @@ cc_library(\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base\",\n@@ -2842,6 +2862,7 @@ cc_library(\n     srcs = [\"thunk_proto_deserialization.cc\"],\n     hdrs = [\"thunk_proto_deserialization.h\"],\n     deps = [\n+        \":all_gather_thunk\",\n         \":collective_thunk\",\n         \":conditional_thunk\",\n         \":convolution_reorder_thunk\","
        },
        {
            "sha": "f4d8617abaf50ff24da1f0aeedfaf95c26837d23",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_gather_thunk.cc",
            "status": "modified",
            "additions": 76,
            "deletions": 16,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -16,11 +16,14 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n \n #include <cstdint>\n+#include <memory>\n+#include <optional>\n #include <utility>\n #include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n@@ -30,6 +33,7 @@ limitations under the License.\n #include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n@@ -40,11 +44,12 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/casts.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla {\n namespace gpu {\n \n-namespace impl {\n+namespace {\n AllGatherConfig GetAllGatherConfig(const HloAllGatherInstruction* inst) {\n   AllGatherConfig config;\n   config.config = GetCollectiveConfig(inst, inst->use_global_device_ids());\n@@ -55,7 +60,7 @@ absl::Status CheckImplementableInst(const HloAllGatherInstruction* inst) {\n   for (HloInstruction* operand : inst->operands()) {\n     const Shape& shape = operand->shape();\n \n-    TF_RETURN_IF_ERROR(IsValidOperand(shape, Thunk::kAllGather));\n+    RETURN_IF_ERROR(IsValidOperand(shape, Thunk::kAllGather));\n \n     if (!ShapeUtil::IsEffectivelyMostMajorDimension(\n             shape, inst->all_gather_dimension())) {\n@@ -67,7 +72,16 @@ absl::Status CheckImplementableInst(const HloAllGatherInstruction* inst) {\n \n   return absl::OkStatus();\n }\n-}  // namespace impl\n+}  // namespace\n+\n+AllGatherStartThunk::AllGatherStartThunk(\n+    ThunkInfo thunk_info,\n+    std::shared_ptr<CollectiveThunk::AsyncEvents> async_events,\n+    CollectiveConfig config, std::vector<Buffer> buffers)\n+    : CollectiveThunk(Thunk::kAllGatherStart, thunk_info, async_events,\n+                      AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE),\n+      config_(AllGatherConfig{config}),\n+      buffers_(std::move(buffers)) {}\n \n AllGatherStartThunk::AllGatherStartThunk(ThunkInfo thunk_info,\n                                          const HloAllGatherInstruction* inst,\n@@ -76,7 +90,7 @@ AllGatherStartThunk::AllGatherStartThunk(ThunkInfo thunk_info,\n     : CollectiveThunk(Thunk::kAllGatherStart, thunk_info,\n                       IsGPUSyncCollective(*inst),\n                       AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE),\n-      config_(impl::GetAllGatherConfig(inst)),\n+      config_(GetAllGatherConfig(inst)),\n       buffers_(std::move(buffers)) {\n   CHECK_EQ(config_.config.operand_element_type.size(), buffers_.size());\n }\n@@ -85,23 +99,69 @@ AllGatherStartThunk::AllGatherStartThunk(ThunkInfo thunk_info,\n     const HloAllGatherInstruction* inst, int64_t replica_count,\n     int64_t partition_count) {\n   return AddOpDescription<AllGatherStartThunk>(\n-      impl::CheckImplementableInst(inst), inst, replica_count, partition_count);\n+      CheckImplementableInst(inst), inst, replica_count, partition_count);\n }\n \n /*static*/ CollectiveOpGroupMode AllGatherStartThunk::GetGroupMode(\n     const HloAllGatherInstruction* inst) {\n-  return impl::GetAllGatherConfig(inst).config.group_mode;\n+  return GetAllGatherConfig(inst).config.group_mode;\n+}\n+\n+absl::StatusOr<std::unique_ptr<AllGatherStartThunk>>\n+AllGatherStartThunk::FromProto(\n+    ThunkInfo thunk_info, const AllGatherStartThunkProto& thunk_proto,\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    CollectiveThunk::AsyncEventsMap& async_events_map) {\n+  std::vector<CollectiveThunk::Buffer> buffers;\n+  buffers.reserve(thunk_proto.buffers_size());\n+  for (const CollectiveBufferProto& proto : thunk_proto.buffers()) {\n+    ASSIGN_OR_RETURN(\n+        CollectiveThunk::Buffer buffer,\n+        CollectiveThunk::Buffer::FromProto(proto, buffer_allocations));\n+    buffers.push_back(buffer);\n+  }\n+\n+  std::shared_ptr<CollectiveThunk::AsyncEvents>& async_events =\n+      async_events_map[AsyncEventsUniqueId{\n+          thunk_proto.async_events_unique_id()}];\n+  if (!async_events) {\n+    async_events = std::make_shared<CollectiveThunk::AsyncEvents>();\n+  }\n+\n+  return std::make_unique<AllGatherStartThunk>(\n+      std::move(thunk_info), async_events,\n+      CollectiveConfig::FromProto(thunk_proto.collective_config()),\n+      std::move(buffers));\n+}\n+\n+absl::StatusOr<ThunkProto> AllGatherStartThunk::ToProto() const {\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+\n+  AllGatherStartThunkProto* thunk_proto =\n+      proto.mutable_all_gather_start_thunk();\n+\n+  std::optional<AsyncEventsUniqueId> async_events_id = GetAsyncEventsUniqueId();\n+  if (!async_events_id.has_value()) {\n+    return absl::FailedPreconditionError(\"AsyncEvents is not set.\");\n+  }\n+  thunk_proto->set_async_events_unique_id(async_events_id->value());\n+\n+  for (const Buffer& buffer : buffers_) {\n+    ASSIGN_OR_RETURN(*thunk_proto->add_buffers(), buffer.ToProto());\n+  }\n+  *thunk_proto->mutable_collective_config() = config_.config.ToProto();\n+  return proto;\n }\n \n absl::StatusOr<bool> AllGatherStartThunk::RunCollective(\n     const ExecuteParams& params, const GpuCliqueKey& clique_key,\n     se::Stream& stream, Communicator& comm) {\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<DeviceBufferPair> device_buffers,\n-      ConvertToDeviceBuffers(params, buffers_,\n-                             config_.config.operand_element_type));\n-  TF_RETURN_IF_ERROR(xla::gpu::RunAllGather(\n-      device_buffers, stream, comm, config_.config.use_symmetric_buffer));\n+  ASSIGN_OR_RETURN(std::vector<DeviceBufferPair> device_buffers,\n+                   ConvertToDeviceBuffers(params, buffers_,\n+                                          config_.config.operand_element_type));\n+  RETURN_IF_ERROR(xla::gpu::RunAllGather(device_buffers, stream, comm,\n+                                         config_.config.use_symmetric_buffer));\n   return true;\n }\n \n@@ -110,21 +170,21 @@ absl::Status RunAllGather(std::vector<DeviceBufferPair>& buffers,\n                           bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n   XLA_VLOG_DEVICE(3, device_ordinal) << \"Performing all-gather\";\n-  TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, &comm,\n-                                          use_symmetric_buffer));\n+  RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, &comm,\n+                                       use_symmetric_buffer));\n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(&comm);\n   Future<> future = gpu_comm->GroupExecute(\n       [&buffers, &stream](GpuCommunicator* comm) -> absl::Status {\n         for (DeviceBufferPair& buffer : buffers) {\n-          TF_RETURN_IF_ERROR(comm->LaunchAllGather(\n+          RETURN_IF_ERROR(comm->LaunchAllGather(\n               buffer.source_buffer, buffer.destination_buffer,\n               buffer.element_type, buffer.element_count,\n               GpuCollectives::On(stream)));\n         }\n         return absl::OkStatus();\n       });\n \n-  TF_RETURN_IF_ERROR(future.Await());\n+  RETURN_IF_ERROR(future.Await());\n   XLA_VLOG_DEVICE(3, device_ordinal) << \"Done performing all-gather\";\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "2c3c9beb571448ca3fa174647ea90d613ce92080",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_gather_thunk.h",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.h?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_RUNTIME_ALL_GATHER_THUNK_H_\n \n #include <cstdint>\n+#include <memory>\n #include <vector>\n \n #include \"absl/status/status.h\"\n@@ -26,6 +27,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/stream.h\"\n \n namespace xla {\n@@ -41,6 +43,10 @@ class AllGatherStartThunk : public CollectiveThunk {\n   AllGatherStartThunk(ThunkInfo thunk_info, const HloAllGatherInstruction* inst,\n                       std::vector<Buffer> buffers,\n                       bool p2p_memcpy_enabled = false);\n+  AllGatherStartThunk(\n+      ThunkInfo thunk_info,\n+      std::shared_ptr<CollectiveThunk::AsyncEvents> async_events,\n+      CollectiveConfig config, std::vector<Buffer> buffers);\n \n   static const char* GetHloOpName() { return \"all-gather-start\"; }\n \n@@ -54,6 +60,13 @@ class AllGatherStartThunk : public CollectiveThunk {\n   const CollectiveConfig& config() const override { return config_.config; }\n   absl::Span<const Buffer> buffers() const { return buffers_; }\n \n+  static absl::StatusOr<std::unique_ptr<AllGatherStartThunk>> FromProto(\n+      ThunkInfo thunk_info, const AllGatherStartThunkProto& thunk_proto,\n+      absl::Span<const BufferAllocation> buffer_allocations,\n+      CollectiveThunk::AsyncEventsMap& async_events_map);\n+\n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n  protected:\n   absl::StatusOr<bool> RunCollective(const ExecuteParams& params,\n                                      const GpuCliqueKey& clique_key,"
        },
        {
            "sha": "38a08a872fe0f40a03474854161d9fc521ba7e93",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_gather_thunk_test.cc",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk_test.cc?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -0,0 +1,75 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/check.h\"\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::tsl::proto_testing::EqualsProto;\n+\n+TEST(CollectiveThunkTest, ProtoRoundTrip) {\n+  ThunkProto proto = tsl::proto_testing::ParseTextProtoOrDie<ThunkProto>(\n+      R\"pb(\n+        thunk_info {\n+          profile_annotation: \"partition_id_profile_annotation\"\n+          execution_stream_id: 2\n+        }\n+        all_gather_start_thunk {\n+          async_events_unique_id: 3\n+          collective_config {}\n+        }\n+      )pb\");\n+\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.profile_annotation = proto.thunk_info().profile_annotation();\n+  thunk_info.execution_stream_id = xla::gpu::ExecutionStreamId{\n+      static_cast<xla::gpu::ExecutionStreamId::ValueType>(\n+          proto.thunk_info().execution_stream_id())};\n+\n+  CollectiveThunk::AsyncEventsMap async_events_map;\n+  std::vector<BufferAllocation> buffer_allocations = {\n+      BufferAllocation(/*index=*/0, /*size=*/4, /*color=*/0)};\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<AllGatherStartThunk> thunk,\n+      AllGatherStartThunk::FromProto(thunk_info, proto.all_gather_start_thunk(),\n+                                     buffer_allocations, async_events_map));\n+  ASSERT_NE(thunk->async_events(), nullptr);\n+\n+  ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n+\n+  // Ids are unique and expected to differ.\n+  proto.mutable_all_gather_start_thunk()->set_async_events_unique_id(\n+      round_trip_proto.all_gather_start_thunk().async_events_unique_id());\n+  EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "c421e0323641d89f4455aa163af33b5f47e8e1ff",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 98,
            "deletions": 30,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <cstdlib>\n+#include <iterator>\n #include <memory>\n #include <optional>\n #include <string>\n@@ -45,6 +46,7 @@ limitations under the License.\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/runtime/device_id.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/rendezvous.h\"\n@@ -59,6 +61,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -148,6 +151,41 @@ bool CollectiveConfig::IsDegenerate(int64_t replica_count,\n   }\n }\n \n+CollectiveConfigProto CollectiveConfig::ToProto() const {\n+  CollectiveConfigProto proto;\n+\n+  proto.mutable_operand_element_type()->Assign(operand_element_type.begin(),\n+                                               operand_element_type.end());\n+  proto.mutable_replica_groups()->Assign(replica_groups.begin(),\n+                                         replica_groups.end());\n+\n+  proto.set_group_mode(group_mode);\n+  proto.set_use_symmetric_buffer(use_symmetric_buffer);\n+\n+  return proto;\n+}\n+\n+CollectiveConfig CollectiveConfig::FromProto(\n+    const CollectiveConfigProto& proto) {\n+  CollectiveConfig config;\n+\n+  config.operand_element_type.reserve(proto.operand_element_type_size());\n+  for (int element_type : proto.operand_element_type()) {\n+    config.operand_element_type.push_back(\n+        static_cast<PrimitiveType>(element_type));\n+  }\n+\n+  config.replica_groups.assign(proto.replica_groups().begin(),\n+                               proto.replica_groups().end());\n+\n+  absl::c_copy(proto.replica_groups(),\n+               std::back_inserter(config.replica_groups));\n+\n+  config.group_mode = proto.group_mode();\n+  config.use_symmetric_buffer = proto.use_symmetric_buffer();\n+  return config;\n+}\n+\n CollectiveConfig GetCollectiveConfig(\n     const HloInstruction* hlo, std::optional<bool> use_global_device_ids) {\n   CollectiveConfig config;\n@@ -176,6 +214,13 @@ CollectiveThunk::CollectiveThunk(Kind kind, ThunkInfo thunk_info, bool is_sync,\n       stream_kind_(stream_kind),\n       async_events_(is_sync ? nullptr : std::make_shared<AsyncEvents>()) {}\n \n+CollectiveThunk::CollectiveThunk(Kind kind, ThunkInfo thunk_info,\n+                                 std::shared_ptr<AsyncEvents> async_events,\n+                                 AsyncStreamKind stream_kind)\n+    : Thunk(kind, thunk_info),\n+      stream_kind_(stream_kind),\n+      async_events_(async_events) {}\n+\n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n     const CollectiveParams& params, const CollectiveConfig& collective_config,\n     bool include_participant_groups) {\n@@ -217,7 +262,7 @@ absl::Status MaybeRegisterBuffer(se::StreamExecutor* executor,\n                                  const se::DeviceAddressBase& buffer,\n                                  Communicator* comm,\n                                  bool use_symmetric_buffer) {\n-  TF_ASSIGN_OR_RETURN(auto range, executor->GetMemoryRange(buffer));\n+  ASSIGN_OR_RETURN(auto range, executor->GetMemoryRange(buffer));\n   XLA_VLOG_DEVICE(1, executor->device_ordinal())\n       << \"Registering range: \" << range.opaque()\n       << \" with size: \" << range.size() << \" for buffer: \" << buffer.opaque()\n@@ -235,25 +280,52 @@ absl::Status MaybeRegisterBuffers(se::StreamExecutor* executor,\n                                   bool use_symmetric_buffer) {\n   for (int i = 0; i < buffers.size(); ++i) {\n     if (buffers[i].source_memory_space == kCollectiveMemorySpaceColor) {\n-      TF_RETURN_IF_ERROR(MaybeRegisterBuffer(executor, buffers[i].source_buffer,\n-                                             comm, use_symmetric_buffer));\n+      RETURN_IF_ERROR(MaybeRegisterBuffer(executor, buffers[i].source_buffer,\n+                                          comm, use_symmetric_buffer));\n     }\n     if (buffers[i].destination_memory_space == kCollectiveMemorySpaceColor) {\n-      TF_RETURN_IF_ERROR(MaybeRegisterBuffer(\n+      RETURN_IF_ERROR(MaybeRegisterBuffer(\n           executor, buffers[i].destination_buffer, comm, use_symmetric_buffer));\n     }\n   }\n   return absl::OkStatus();\n }\n \n+absl::StatusOr<CollectiveBufferProto> CollectiveThunk::Buffer::ToProto() const {\n+  CollectiveBufferProto proto;\n+  proto.set_element_count(element_count);\n+  ASSIGN_OR_RETURN(*proto.mutable_source_buffer(), source_buffer.ToProto());\n+  ASSIGN_OR_RETURN(*proto.mutable_destination_buffer(),\n+                   destination_buffer.ToProto());\n+  proto.set_source_memory_space(source_memory_space);\n+  proto.set_destination_memory_space(destination_memory_space);\n+  return proto;\n+}\n+\n+absl::StatusOr<CollectiveThunk::Buffer> CollectiveThunk::Buffer::FromProto(\n+    const CollectiveBufferProto& buffer_proto,\n+    absl::Span<const BufferAllocation> buffer_allocations) {\n+  CollectiveThunk::Buffer res;\n+  res.element_count = buffer_proto.element_count();\n+  ASSIGN_OR_RETURN(res.source_buffer,\n+                   BufferAllocation::Slice::FromProto(\n+                       buffer_proto.source_buffer(), buffer_allocations));\n+  ASSIGN_OR_RETURN(res.destination_buffer,\n+                   BufferAllocation::Slice::FromProto(\n+                       buffer_proto.destination_buffer(), buffer_allocations));\n+  res.source_memory_space = buffer_proto.source_memory_space();\n+  res.destination_memory_space = buffer_proto.destination_memory_space();\n+  return res;\n+}\n+\n absl::Status CollectiveThunk::AsyncEvents::Initialize(\n     se::StreamExecutor* executor) {\n   absl::MutexLock lock(mu_);\n   if (events_.contains(executor)) {\n     return absl::OkStatus();\n   }\n \n-  TF_ASSIGN_OR_RETURN(auto event, executor->CreateEvent());\n+  ASSIGN_OR_RETURN(auto event, executor->CreateEvent());\n \n   events_.try_emplace(executor, std::move(event));\n   return absl::OkStatus();\n@@ -274,7 +346,7 @@ absl::StatusOr<se::Event*> CollectiveThunk::AsyncEvents::GetEvent(\n \n absl::Status CollectiveThunk::Prepare(const PrepareParams& params) {\n   TF_RET_CHECK(params.collective_params != nullptr);\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetGpuCliqueKey(*params.collective_params, config().replica_groups,\n                       config().group_mode, GetAsyncStreamKind()));\n@@ -283,7 +355,7 @@ absl::Status CollectiveThunk::Prepare(const PrepareParams& params) {\n \n absl::Status CollectiveThunk::Initialize(const InitializeParams& params) {\n   if (async_events_) {\n-    TF_RETURN_IF_ERROR(async_events_->Initialize(params.executor));\n+    RETURN_IF_ERROR(async_events_->Initialize(params.executor));\n   }\n   return absl::OkStatus();\n }\n@@ -294,15 +366,14 @@ absl::Status CollectiveThunk::ExecuteOnStream(const ExecuteParams& params) {\n       IsAsync() ? \"async\" : \"sync\", Thunk::KindToString(kind()));\n   AsyncStreamKind stream_kind = GetAsyncStreamKind();\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetGpuCliqueKey(*params.collective_params, config().replica_groups,\n                       config().group_mode, stream_kind));\n \n-  TF_ASSIGN_OR_RETURN(\n-      Communicator * comm,\n-      params.collective_cliques->GetComm(\n-          clique_key, params.collective_params->global_device_id));\n+  ASSIGN_OR_RETURN(Communicator * comm,\n+                   params.collective_cliques->GetComm(\n+                       clique_key, params.collective_params->global_device_id));\n   DCHECK(comm) << \"Failed to get communicator for collective operation\";\n \n   se::StreamExecutor* executor = params.stream->parent();\n@@ -315,20 +386,18 @@ absl::Status CollectiveThunk::ExecuteOnStream(const ExecuteParams& params) {\n         *params.collective_params->async_streams.at(async_stream_idx);\n \n     // Wait for main compute stream to make sure all buffers are ready.\n-    TF_RETURN_IF_ERROR(async_stream.WaitFor(params.stream));\n+    RETURN_IF_ERROR(async_stream.WaitFor(params.stream));\n \n-    TF_ASSIGN_OR_RETURN(is_first_rendezvous_needed,\n-                        RunCollective(params, clique_key, async_stream, *comm));\n+    ASSIGN_OR_RETURN(is_first_rendezvous_needed,\n+                     RunCollective(params, clique_key, async_stream, *comm));\n \n     // Record collective operation completion.\n-    TF_ASSIGN_OR_RETURN(se::Event * event, async_events_->GetEvent(executor));\n-    TF_RETURN_IF_ERROR(async_stream.RecordEvent(event));\n-\n+    ASSIGN_OR_RETURN(se::Event * event, async_events_->GetEvent(executor));\n+    RETURN_IF_ERROR(async_stream.RecordEvent(event));\n   } else {\n     // Launch collective operation on a main stream.\n-    TF_ASSIGN_OR_RETURN(\n-        is_first_rendezvous_needed,\n-        RunCollective(params, clique_key, *params.stream, *comm));\n+    ASSIGN_OR_RETURN(is_first_rendezvous_needed,\n+                     RunCollective(params, clique_key, *params.stream, *comm));\n   }\n \n   // After a first execution of this instance of collective operation do a\n@@ -356,7 +425,7 @@ absl::Status CollectiveThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n     const xla::DebugOptions debug_options = xla::GetDebugOptionsFromFlags();\n \n-    TF_RETURN_IF_ERROR(Rendezvous(\n+    RETURN_IF_ERROR(Rendezvous(\n         first_call_rendezvous_flag_, rendezvous_name, rendezvous_key,\n         num_local_participants,\n         /*warn_stuck_timeout=*/\n@@ -378,14 +447,13 @@ absl::StatusOr<std::vector<Communicator*>> CollectiveThunk::GetCommunicators(\n     const ExecuteParams& params) const {\n   AsyncStreamKind stream_kind = GetAsyncStreamKind();\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetGpuCliqueKey(*params.collective_params, config().replica_groups,\n                       config().group_mode, stream_kind));\n-  TF_ASSIGN_OR_RETURN(\n-      Communicator * comm,\n-      params.collective_cliques->GetComm(\n-          clique_key, params.collective_params->global_device_id));\n+  ASSIGN_OR_RETURN(Communicator * comm,\n+                   params.collective_cliques->GetComm(\n+                       clique_key, params.collective_params->global_device_id));\n   return std::vector<Communicator*>{comm};\n }\n \n@@ -436,7 +504,7 @@ CollectiveDoneThunk::CollectiveDoneThunk(\n \n absl::Status CollectiveDoneThunk::ExecuteOnStream(const ExecuteParams& params) {\n   se::StreamExecutor* executor = params.stream->parent();\n-  TF_ASSIGN_OR_RETURN(se::Event * event, async_events_->GetEvent(executor));\n+  ASSIGN_OR_RETURN(se::Event * event, async_events_->GetEvent(executor));\n   return params.stream->WaitFor(event);\n }\n \n@@ -490,8 +558,8 @@ CollectiveDoneThunk::FromProto(\n     async_events = std::make_shared<CollectiveThunk::AsyncEvents>();\n   }\n \n-  TF_ASSIGN_OR_RETURN(Thunk::Kind kind,\n-                      Thunk::KindFromProto(thunk_proto.thunk_kind()));\n+  ASSIGN_OR_RETURN(Thunk::Kind kind,\n+                   Thunk::KindFromProto(thunk_proto.thunk_kind()));\n   return std::make_unique<CollectiveDoneThunk>(kind, std::move(thunk_info),\n                                                async_events,\n                                                thunk_proto.async_stream_kind());"
        },
        {
            "sha": "702db0ab6e5cfddc92b757027de7bc2b534c4200",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -57,6 +57,9 @@ struct CollectiveConfig {\n   std::vector<ReplicaGroup> replica_groups;\n   CollectiveOpGroupMode group_mode;\n   bool use_symmetric_buffer;\n+\n+  CollectiveConfigProto ToProto() const;\n+  static CollectiveConfig FromProto(const CollectiveConfigProto& proto);\n };\n \n CollectiveConfig GetCollectiveConfig(const HloInstruction* hlo,\n@@ -96,6 +99,11 @@ class CollectiveThunk : public Thunk {\n     BufferAllocation::Slice destination_buffer;\n     int64_t source_memory_space;\n     int64_t destination_memory_space;\n+\n+    absl::StatusOr<CollectiveBufferProto> ToProto() const;\n+    static absl::StatusOr<Buffer> FromProto(\n+        const CollectiveBufferProto& buffer_proto,\n+        absl::Span<const BufferAllocation> buffer_allocations);\n   };\n \n   // Completion events for asynchronous collective operations (operations\n@@ -120,6 +128,10 @@ class CollectiveThunk : public Thunk {\n   using AsyncEventsMap =\n       absl::flat_hash_map<AsyncEventsUniqueId, std::shared_ptr<AsyncEvents>>;\n \n+  CollectiveThunk(Kind kind, ThunkInfo thunk_info,\n+                  std::shared_ptr<AsyncEvents> async_events,\n+                  AsyncStreamKind stream_kind);\n+\n   // Logging support.\n   static std::string GetDeviceString(const CollectiveParams& params);\n "
        },
        {
            "sha": "36b2e69ac96c4c1450783800f1706681fd3bf497",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -377,12 +377,33 @@ message CustomKernelThunkProto {\n   CustomKernelProto custom_kernel = 3;\n }\n \n+message CollectiveBufferProto {\n+  int64 element_count = 1;\n+  xla.buffer_assignment.BufferAllocationSliceProto source_buffer = 2;\n+  xla.buffer_assignment.BufferAllocationSliceProto destination_buffer = 3;\n+  int64 source_memory_space = 4;\n+  int64 destination_memory_space = 5;\n+}\n+\n+message CollectiveConfigProto {\n+  repeated PrimitiveType operand_element_type = 1;\n+  repeated ReplicaGroup replica_groups = 2;\n+  CollectiveOpGroupMode group_mode = 3;\n+  bool use_symmetric_buffer = 4;\n+}\n+\n message CollectiveThunkProto {\n   ThunkKindProto thunk_kind = 1;\n   AsyncStreamKind async_stream_kind = 2;\n   uint64 async_events_unique_id = 3;\n }\n \n+message AllGatherStartThunkProto {\n+  uint64 async_events_unique_id = 1;\n+  CollectiveConfigProto collective_config = 2;\n+  repeated CollectiveBufferProto buffers = 3;\n+}\n+\n message CollectiveDoneThunkProto {\n   ThunkKindProto thunk_kind = 1;\n   AsyncStreamKind async_stream_kind = 2;\n@@ -428,6 +449,7 @@ message ThunkProto {\n     HostRecvDoneThunkProto host_recv_done_thunk = 35;\n     CustomKernelThunkProto custom_kernel_thunk = 36;\n     CollectiveDoneThunkProto collective_done_thunk = 37;\n+    AllGatherStartThunkProto all_gather_start_thunk = 38;\n   }\n }\n "
        },
        {
            "sha": "4b1a965a41a449e8f66c9e8ff679740a08441d1c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc70d01ee09e7140ad031d4d2e7710e4f7367190/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc?ref=fc70d01ee09e7140ad031d4d2e7710e4f7367190",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"google/protobuf/descriptor.h\"\n #include \"google/protobuf/message.h\"\n+#include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n #include \"xla/backends/gpu/runtime/convolution_reorder_thunk.h\"\n@@ -242,6 +243,10 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProtoImpl(\n       return CollectiveDoneThunk::FromProto(std::move(thunk_info),\n                                             thunk_proto.collective_done_thunk(),\n                                             collective_async_events_map);\n+    case ThunkProto::kAllGatherStartThunk:\n+      return AllGatherStartThunk::FromProto(\n+          std::move(thunk_info), thunk_proto.all_gather_start_thunk(),\n+          buffer_allocations, collective_async_events_map);\n     default:\n       std::optional<absl::string_view> unsupported_thunk_type =\n           GetStoredThunkTypeName(thunk_proto);"
        }
    ],
    "stats": {
        "total": 368,
        "additions": 322,
        "deletions": 46
    }
}