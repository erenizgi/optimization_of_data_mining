{
    "author": "nputikhin",
    "message": "[XLA:GPU] Add xla_gpu_experimental_enable_checksum_tracing_on_thunks\n\nThe new flag will enable calculating and recording of checksums for thunk inputs/outputs.\n\nIt is not settable via CLI yet\n\nPiperOrigin-RevId: 814226481",
    "sha": "efba7fab7232b5fc27d9ce6734c4312bfc93745d",
    "files": [
        {
            "sha": "0cb1d8ce6d10778479a292326c12a277692b7c77",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=efba7fab7232b5fc27d9ce6734c4312bfc93745d",
            "patch": "@@ -171,12 +171,11 @@ static absl::flat_hash_set<ExecutionStreamId> GetExecutionStreamIds(\n \n static absl::Status RunThunkPasses(const DebugOptions& debug_options,\n                                    const se::DeviceDescription& device_info,\n-                                   bool enable_experimental_checksum_pass,\n                                    SequentialThunk* root_thunk,\n                                    HloModule* hlo_module,\n                                    ThunkPassBufferAllocator& allocator) {\n   ThunkPassPipeline pipeline(\"thunk-passes\");\n-  if (enable_experimental_checksum_pass) {\n+  if (debug_options.xla_gpu_experimental_enable_checksum_tracing_on_thunks()) {\n     pipeline.AddPass(std::make_unique<ThunkChecksumTracingPass>());\n   }\n   if (debug_options.xla_gpu_experimental_enable_command_buffer_on_thunks()) {\n@@ -208,8 +207,7 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(\n   GpuExecutableThunkPassBufferAllocator allocator(next_idx);\n \n   TF_RETURN_IF_ERROR(RunThunkPasses(\n-      params.debug_options, params.device_description,\n-      params.enable_experimental_checksum_pass, params.executable.get(),\n+      params.debug_options, params.device_description, params.executable.get(),\n       params.debug_module.get(), allocator));\n \n   return std::unique_ptr<GpuExecutable>(new GpuExecutable("
        },
        {
            "sha": "35a975bbb1f360b4ac30a743b16b3f2fa352521a",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=efba7fab7232b5fc27d9ce6734c4312bfc93745d",
            "patch": "@@ -113,10 +113,6 @@ class GpuExecutable : public Executable {\n     se::DeviceDescription device_description;\n     std::unique_ptr<HloModule> debug_module = nullptr;\n     bool enable_debug_info_manager = true;\n-    // TODO: b/444183764 - Guard by a flag instead of param.\n-    // For now the pass is so experimental it's not supposed to be possible\n-    // to enable.\n-    bool enable_experimental_checksum_pass = false;\n   };\n \n   static absl::StatusOr<std::unique_ptr<GpuExecutable>> Create(Params params);"
        },
        {
            "sha": "bbc1613051a0913d36cc73945c89205f1e79be80",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=efba7fab7232b5fc27d9ce6734c4312bfc93745d",
            "patch": "@@ -301,7 +301,6 @@ TEST(GpuExecutableTest, ThunkChecksumPassAddsAllocation) {\n   GpuExecutable::Params params_without_pass;\n   params_without_pass.executable =\n       std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n-  params_without_pass.enable_experimental_checksum_pass = false;\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<GpuExecutable> executable_without_pass,\n@@ -312,7 +311,8 @@ TEST(GpuExecutableTest, ThunkChecksumPassAddsAllocation) {\n   GpuExecutable::Params params_with_pass;\n   params_with_pass.executable =\n       std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n-  params_with_pass.enable_experimental_checksum_pass = true;\n+  params_with_pass.debug_options\n+      .set_xla_gpu_experimental_enable_checksum_tracing_on_thunks(true);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable_with_pass,\n                           GpuExecutable::Create(std::move(params_with_pass)));"
        },
        {
            "sha": "b8cd075d87e657d9675bdb6edb6de34a556a088a",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/efba7fab7232b5fc27d9ce6734c4312bfc93745d/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=efba7fab7232b5fc27d9ce6734c4312bfc93745d",
            "patch": "@@ -596,6 +596,10 @@ message DebugOptions {\n   // xla_gpu_multi_streamed_windowed_einsum is set to true.\n   optional bool xla_gpu_experimental_enable_alltoall_windowed_einsum = 360;\n \n+  // Enables an experimental feature to record checksums of selected thunk\n+  // inputs/outputs.\n+  optional bool xla_gpu_experimental_enable_checksum_tracing_on_thunks = 414;\n+\n   // Enables an experimental feature for command buffer conversion on thunks.\n   optional bool xla_gpu_experimental_enable_command_buffer_on_thunks = 394;\n \n@@ -1336,7 +1340,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 414\n+  // Next id: 415\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 20,
        "additions": 9,
        "deletions": 11
    }
}