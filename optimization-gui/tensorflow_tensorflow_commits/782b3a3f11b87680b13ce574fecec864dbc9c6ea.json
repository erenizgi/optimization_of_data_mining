{
    "author": "sergachev",
    "message": "PR #34163: [GPU] Make cuDNN GEMM backend aware of dot algorithms.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34163\n\nüìù Summary of Changes\nMakes the cuDNN backend respect dot algorithms - for now failing on any non-default one requested.\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüß™ Unit Tests:\nyes\n\nüß™ Execution Tests:\nno\nCopybara import of the project:\n\n--\n576bd547db017a0d2c3657aaa8fcffe4216624c2 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Make cuDNN GEMM backend aware of dot algorithms.\n\nMerging this change closes #34163\n\nPiperOrigin-RevId: 836254851",
    "sha": "782b3a3f11b87680b13ce574fecec864dbc9c6ea",
    "files": [
        {
            "sha": "2ceef17cbdf053fedc92f2212edb9792e75eb129",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=782b3a3f11b87680b13ce574fecec864dbc9c6ea",
            "patch": "@@ -559,6 +559,24 @@ ENTRY e {\n })\"));\n }\n \n+TEST_F(CuDnnFusionExecutionTest, NonDefaultDotAlgorithmIsNotSupported) {\n+  EXPECT_FALSE(Run(R\"(\n+fusion1 {\n+  a = bf16[32,96] parameter(0)\n+  b = bf16[96,64] parameter(1)\n+  r = f32[32,64] dot(a, b),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    algorithm=dot_bf16_bf16_f32\n+}\n+\n+e {\n+  a = bf16[32,96] parameter(0)\n+  b = bf16[96,64] parameter(1)\n+  _ = f32[32,64] fusion(a, b), kind=kCustom, calls=fusion1,\n+    backend_config={\"fusion_backend_config\": {kind: \"__cudnn$fusion\"}}\n+})\"));\n+}\n+\n TEST_F(CuDnnFusionExecutionTest,\n        DotF16NegateNonDefaultDimensionsExecutesCorrectly) {\n   EXPECT_TRUE(RunAndCompare(R\"("
        },
        {
            "sha": "0a7da242ced35eb0e60cdacf6cb13ef302546186",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fusion_compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc?ref=782b3a3f11b87680b13ce574fecec864dbc9c6ea",
            "patch": "@@ -217,6 +217,10 @@ class GemmDimensionAdapter {\n       VLOG(3) << \"Non-default precision is not supported.\";\n       return std::nullopt;\n     }\n+    if (dot->precision_config().algorithm() != PrecisionConfig::ALG_UNSET) {\n+      VLOG(3) << \"Non-default algorithm is not supported.\";\n+      return std::nullopt;\n+    }\n     TF_ASSIGN_OR_RETURN(auto analysis,\n                         TritonFusionAnalysis::Execute(computation));\n     return GemmDimensionAdapter{*dot, std::move(analysis)};"
        },
        {
            "sha": "4224eabaa8ea4d73e06094da4808f82150ab778d",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=782b3a3f11b87680b13ce574fecec864dbc9c6ea",
            "patch": "@@ -441,11 +441,6 @@ message DebugOptions {\n   optional string xla_gpu_cuda_data_dir = 61;\n \n   // Let GEMM fusion autotuning probe cuDNN as a backend.\n-  //\n-  // CAUTION:\n-  // * HLO dot precision (e.g. `algorithm=dot_bf16_bf16_f32`) is ignored.\n-  // * `tf32` matmuls are enabled unconditionally.\n-  //\n   // Current levels:\n   // 0: Disabled.\n   // 1: Enabled on Blackwell+ GPUs."
        }
    ],
    "stats": {
        "total": 27,
        "additions": 22,
        "deletions": 5
    }
}