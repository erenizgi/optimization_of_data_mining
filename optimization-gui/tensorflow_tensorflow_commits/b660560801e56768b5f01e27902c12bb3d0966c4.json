{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU] Prevent all-reduce codegen when replica groups are empty\n\nGenerating collective code when participating devices are not specified\nis not possible unless topology information is available during compilation.\n\nThis change bails out of codegen for empty replica_groups for this reason.\n\nPiperOrigin-RevId: 837190862",
    "sha": "b660560801e56768b5f01e27902c12bb3d0966c4",
    "files": [
        {
            "sha": "92b1e32a9bc40a7a0147721bd1d83c2b813f4580",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b660560801e56768b5f01e27902c12bb3d0966c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b660560801e56768b5f01e27902c12bb3d0966c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=b660560801e56768b5f01e27902c12bb3d0966c4",
            "patch": "@@ -1054,6 +1054,7 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n@@ -1063,7 +1064,6 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:NVVMDialect\",\n         \"@llvm-project//mlir:Support\",\n-        \"@llvm-project//mlir:TensorDialect\",\n         \"@triton//:TritonDialects\",\n     ],\n )\n@@ -1080,7 +1080,6 @@ xla_cc_test(\n         \"//xla:status_macros\",\n         \"//xla/backends/gpu/codegen:fusion_emitter\",\n         \"//xla/backends/gpu/codegen:fusions\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/utils:hlo_query\",\n@@ -1095,6 +1094,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "dd9676185585fa1cd1776a0fc8e87889b9c95d1b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 57,
            "deletions": 31,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b660560801e56768b5f01e27902c12bb3d0966c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b660560801e56768b5f01e27902c12bb3d0966c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=b660560801e56768b5f01e27902c12bb3d0966c4",
            "patch": "@@ -19,18 +19,20 @@ limitations under the License.\n #include <optional>\n #include <type_traits>\n #include <utility>\n+#include <vector>\n \n #include \"absl/base/casts.h\"\n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Interfaces/FunctionInterfaces.h\"\n@@ -80,32 +82,68 @@ static constexpr auto kGlobalAddressSpace =\n         mlir::NVVM::NVVMMemorySpace::Global);\n \n // Metadata arguments for the collective emitter.\n-// device_rank, signal-value, signal_buffers.\n+// device_rank, signal_value, signal_buffers.\n static constexpr int32_t kNumCollectiveMetadataArgs = 3;\n \n-bool CanAllReduceBeEmitted(const HloAllReduceInstruction* all_reduce,\n-                           ReductionKind reduction_kind, int64_t num_devices,\n-                           int64_t num_elements, PrimitiveType element_type,\n-                           AllReduceStrategy all_reduce_strategy) {\n+struct AllReduceInfo {\n+  ReductionKind reduction_kind;\n+  int64_t num_devices;\n+  int64_t num_elements;\n+  PrimitiveType element_type;\n+  AllReduceStrategy all_reduce_strategy;\n+};\n+\n+// Returns the AllReduceInfo for the given all-reduce instruction if the\n+// instruction is supported by the codegen.\n+std::optional<AllReduceInfo> MaybeBuildAllReduceInfo(\n+    const HloAllReduceInstruction* all_reduce) {\n   if (!all_reduce->GetModule()\n            ->config()\n            .debug_options()\n            .xla_gpu_unsupported_use_all_reduce_one_shot_kernel()) {\n-    return false;\n+    return std::nullopt;\n+  }\n+  if (all_reduce->device_list().replica_groups().empty()) {\n+    VLOG(1) << \"Replica groups are empty for \" << all_reduce->name()\n+            << \". Codegen will not be supported.\";\n+    return std::nullopt;\n   }\n+  const int64_t num_devices = all_reduce->device_list().num_devices_per_group();\n+  const std::optional<ReductionKind> reduction_kind =\n+      MatchReductionComputation(all_reduce->called_computations().front());\n+  if (!reduction_kind.has_value()) {\n+    return std::nullopt;\n+  }\n+  const int64_t num_elements =\n+      ShapeUtil::ElementsIn(all_reduce->operand(0)->shape());\n+  const PrimitiveType element_type =\n+      all_reduce->operand(0)->shape().element_type();\n+  // NB: We do not codegen multimem kernels for now.\n+  const AllReduceStrategy all_reduce_strategy =\n+      GetAllReduceStrategy(num_elements, /*is_multimem_enabled=*/false);\n   // TODO(b/383125489): Support variadic all-reduce.\n   if (all_reduce->operand_count() > 1) {\n-    return false;\n+    return std::nullopt;\n   }\n   const int64_t byte_size =\n       num_elements * ShapeUtil::ByteSizeOfPrimitiveType(element_type);\n   // TODO(b/457333991): Support twoShot for codegen.\n   if (byte_size >\n       GetMaxSupportedAllReduceSizeBytes(AllReduceStrategy::kOneShot)) {\n-    return false;\n+    return std::nullopt;\n   }\n-  return IsAllReduceKernelSupported(num_devices, num_elements, element_type,\n-                                    reduction_kind, all_reduce_strategy);\n+  if (!IsAllReduceKernelSupported(num_devices, num_elements, element_type,\n+                                  reduction_kind.value(),\n+                                  all_reduce_strategy)) {\n+    return std::nullopt;\n+  }\n+  return AllReduceInfo{\n+      /* .reduction_kind= */ reduction_kind.value(),\n+      /* .num_devices= */ num_devices,\n+      /* .num_elements= */ num_elements,\n+      /* .element_type= */ element_type,\n+      /* .all_reduce_strategy= */ all_reduce_strategy,\n+  };\n }\n \n // The logic here is very naive and assumes a monotonic layout\n@@ -114,27 +152,15 @@ absl::StatusOr<std::optional<BlockLevelFusionConfig>>\n GetBlockLevelFusionConfigForAllReduce(\n     const se::DeviceDescription& device_info,\n     const HloAllReduceInstruction* all_reduce) {\n-  const std::optional<ReductionKind> reduction_kind =\n-      MatchReductionComputation(all_reduce->called_computations().front());\n-  if (!reduction_kind.has_value()) {\n-    return absl::InternalError(\n-        \"Reduction computation not found for all-reduce.\");\n-  }\n-  const int64_t num_devices = all_reduce->device_list().num_devices_per_group();\n-  const int64_t num_elements =\n-      ShapeUtil::ElementsIn(all_reduce->operand(0)->shape());\n-  const PrimitiveType element_type =\n-      all_reduce->operand(0)->shape().element_type();\n-  // NB: We do not codegen multimem kernels for now.\n-  const AllReduceStrategy all_reduce_strategy =\n-      GetAllReduceStrategy(num_elements, /*is_multimem_enabled=*/false);\n-  if (!CanAllReduceBeEmitted(all_reduce, reduction_kind.value(), num_devices,\n-                             num_elements, element_type, all_reduce_strategy)) {\n+  const std::optional<AllReduceInfo> all_reduce_info =\n+      MaybeBuildAllReduceInfo(all_reduce);\n+  if (!all_reduce_info.has_value()) {\n     return std::nullopt;\n   }\n   const Shape& output_shape = all_reduce->shape();\n-  const LaunchDimensions launch_dims =\n-      AllReduceLaunchDimensions(num_elements, num_devices, all_reduce_strategy);\n+  const LaunchDimensions launch_dims = AllReduceLaunchDimensions(\n+      all_reduce_info->num_elements, all_reduce_info->num_devices,\n+      all_reduce_info->all_reduce_strategy);\n   BlockLevelFusionConfig block_level_config;\n   block_level_config.set_num_warps(launch_dims.num_threads_per_block() /\n                                    WarpSize(device_info));\n@@ -143,8 +169,8 @@ GetBlockLevelFusionConfigForAllReduce(\n   Tile* output_tile = block_level_config.add_output_tiles();\n   const int64_t rank = output_shape.dimensions().size();\n \n-  // Tile sizes are rolled up to power of 2 because this is what the triton\n-  // expects (and consequently the tiling infra).\n+  // Tile sizes are rolled up to power of 2 because this is what triton expects\n+  // and consequently the tiling infra.\n   for (int i = 0; i < rank - 1; ++i) {\n     output_tile->add_sizes(llvm::PowerOf2Ceil(output_shape.dimensions(i)));\n   }"
        },
        {
            "sha": "53f0d98a234d4392f9a44f793ec06ea0f99a73f3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 10,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b660560801e56768b5f01e27902c12bb3d0966c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b660560801e56768b5f01e27902c12bb3d0966c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc?ref=b660560801e56768b5f01e27902c12bb3d0966c4",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"llvm/IR/Module.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n@@ -85,8 +86,7 @@ class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n       : device_info_{TestGpuDeviceInfo::RTXH100SXMDeviceInfo()} {}\n \n   absl::StatusOr<ModuleWithFusion> BuildModuleWithFusion(\n-      const Shape& shape) const {\n-    const std::string module_str = GetModuleStr(shape);\n+      std::string module_str) const {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n                         ParseAndReturnVerifiedModule(module_str));\n     const HloInstruction* instr = hlo_query::GetFirstInstructionWithOpcode(\n@@ -100,7 +100,8 @@ class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n   }\n \n  protected:\n-  static std::string GetModuleStr(const Shape& shape) {\n+  static std::string GetModuleStr(const Shape& shape,\n+                                  absl::string_view replica_groups = \"{0,1}\") {\n     return absl::StrFormat(R\"(\n       HloModule test\n       apply_op {\n@@ -111,11 +112,11 @@ class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n \n       ENTRY test_computation {\n         param_0 = %1$s parameter(0)\n-        all-reduce-start = %1$s all-reduce-start(param_0), to_apply=apply_op, replica_groups={{0,1}}\n+        all-reduce-start = %1$s all-reduce-start(param_0), to_apply=apply_op, replica_groups={%2$s}\n         ROOT all-reduce-done = %1$s all-reduce-done(all-reduce-start)\n       }\n     )\",\n-                           shape.ToString());\n+                           shape.ToString(), replica_groups);\n   }\n \n   const se::DeviceDescription device_info_;\n@@ -124,9 +125,9 @@ class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n class CollectiveEmitterTest : public CollectiveBlockLevelConfigTest {\n  public:\n   absl::StatusOr<std::unique_ptr<ModuleWithEmitter>> BuildModuleWithEmitter(\n-      const Shape& shape, const se::DeviceDescription& device_info) const {\n+      std::string module_str, const se::DeviceDescription& device_info) const {\n     TF_ASSIGN_OR_RETURN(ModuleWithFusion module_with_fusion,\n-                        BuildModuleWithFusion(shape));\n+                        BuildModuleWithFusion(std::move(module_str)));\n     TF_ASSIGN_OR_RETURN(\n         bool collective_fusion_config_set,\n         TrySetGpuBackendConfigForCollective(\n@@ -174,7 +175,7 @@ class CollectiveEmitterParameterizedTest\n TEST_P(CollectiveEmitterParameterizedTest, AllReduceBlockLevelConfig) {\n   const auto& param = GetParam();\n   TF_ASSERT_OK_AND_ASSIGN(const auto module_with_fusion,\n-                          BuildModuleWithFusion(param.shape));\n+                          BuildModuleWithFusion(GetModuleStr(param.shape)));\n   TF_ASSERT_OK_AND_ASSIGN(const auto block_level_config,\n                           GetCollectiveBlockLevelFusionConfig(\n                               device_info_, module_with_fusion.FusionInstr()));\n@@ -207,10 +208,22 @@ INSTANTIATE_TEST_SUITE_P(\n       return info.param.test_name;\n     });\n \n+TEST_F(CollectiveEmitterTest, AllReduceBlockLevelConfigNoReplicaGroups) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const auto module_with_fusion,\n+      BuildModuleWithFusion(GetModuleStr(ShapeUtil::MakeShape(F32, {65536}),\n+                                         /* replica_groups= */ \"\")));\n+  TF_ASSERT_OK_AND_ASSIGN(const auto block_level_config,\n+                          GetCollectiveBlockLevelFusionConfig(\n+                              device_info_, module_with_fusion.FusionInstr()));\n+  EXPECT_EQ(block_level_config, std::nullopt);\n+}\n+\n TEST_F(CollectiveEmitterTest, AllReduceWithTritonGetLaunchConfig) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<ModuleWithEmitter> result_ptr,\n-      BuildModuleWithEmitter(ShapeUtil::MakeShape(F32, {65536}), device_info_));\n+      BuildModuleWithEmitter(GetModuleStr(ShapeUtil::MakeShape(F32, {65536})),\n+                             device_info_));\n   auto& result = *result_ptr;\n   const TritonFusion* triton_fusion = result.emitter.get();\n   ASSERT_NE(triton_fusion, nullptr);\n@@ -223,7 +236,8 @@ TEST_F(CollectiveEmitterTest, AllReduceWithTritonGetLaunchConfig) {\n TEST_F(CollectiveEmitterTest, AllReduceWithTritonGenerateTritonKernel) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<ModuleWithEmitter> result,\n-      BuildModuleWithEmitter(ShapeUtil::MakeShape(F32, {65536}), device_info_));\n+      BuildModuleWithEmitter(GetModuleStr(ShapeUtil::MakeShape(F32, {65536})),\n+                             device_info_));\n   const TritonFusion* triton_fusion = result->emitter.get();\n   ASSERT_NE(triton_fusion, nullptr);\n   TF_ASSERT_OK_AND_ASSIGN("
        }
    ],
    "stats": {
        "total": 126,
        "additions": 83,
        "deletions": 43
    }
}