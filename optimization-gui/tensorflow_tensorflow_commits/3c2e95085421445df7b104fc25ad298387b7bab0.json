{
    "author": "ermilovmaxim",
    "message": "switch from deprecated TF_CHECK_OK\n\nPiperOrigin-RevId: 836756822",
    "sha": "3c2e95085421445df7b104fc25ad298387b7bab0",
    "files": [
        {
            "sha": "0ff0f5ec5dd2ce1ebd9603fdda7a0c3070f1238c",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 30,
            "deletions": 54,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -626,19 +626,18 @@ cc_library(\n         \"//xla:window_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -1159,12 +1158,10 @@ cc_library(\n         \"//xla/hlo/builder:xla_computation\",\n         \"//xla/hlo/evaluator:hlo_evaluator\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/ir:hlo_module_group\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n@@ -1174,11 +1171,7 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:protobuf\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/profiler/lib:scoped_annotation\",\n     ],\n     alwayslink = 1,\n@@ -1267,7 +1260,6 @@ cc_library(\n         \"//xla/service/heap_simulator\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -1496,16 +1488,12 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n-        \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/types:span\",\n     ],\n )\n \n@@ -1867,13 +1855,13 @@ xla_cc_test(\n         \"//xla/service/memory_space_assignment\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -1996,6 +1984,7 @@ cc_library(\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n@@ -2038,12 +2027,16 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n+        \"//xla/tsl/platform:errors\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/functional:function_ref\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n     ],\n )\n \n@@ -2146,15 +2139,14 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/functional:function_ref\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -2657,7 +2649,6 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -2715,17 +2706,16 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/tsl/lib/core:bitmap\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -3004,7 +2994,6 @@ cc_library(\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -3068,6 +3057,7 @@ xla_test(\n     deps = [\n         \":dynamic_dimension_inference\",\n         \":dynamic_padder\",\n+        \":hlo_module_config\",\n         \":pattern_matcher\",\n         \"//xla:error_spec\",\n         \"//xla:literal\",\n@@ -3093,7 +3083,6 @@ xla_test(\n         \"//xla/tests:xla_test_backend_predicates\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:error_codes_proto_impl_cc\",\n         \"@com_google_absl//absl/log\",\n@@ -3110,20 +3099,22 @@ xla_cc_test(\n     srcs = [\"dynamic_dimension_inference_test.cc\"],\n     deps = [\n         \":dynamic_dimension_inference\",\n-        \":hlo_runner\",\n-        \"//xla:literal\",\n+        \"//xla:comparison_util\",\n+        \"//xla:literal_util\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/builder:xla_builder\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:test\",\n-        \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-        \"@local_tsl//tsl/platform:test_benchmark\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n     ],\n )\n \n@@ -3330,9 +3321,9 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:test_helpers\",\n         \"//xla/tests:xla_internal_test_main\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n@@ -3392,11 +3383,10 @@ xla_cc_test(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -3597,7 +3587,6 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -3650,7 +3639,6 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -3716,14 +3704,13 @@ xla_cc_test(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test_benchmark\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_benchmark//:benchmark\",\n         \"@com_google_googletest//:gtest\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-        \"@local_tsl//tsl/platform:test_benchmark\",\n     ],\n )\n \n@@ -3923,13 +3910,12 @@ xla_cc_test(\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -3944,7 +3930,6 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n@@ -4307,22 +4292,20 @@ cc_library(\n     srcs = [\"transpose_folding.cc\"],\n     hdrs = [\"transpose_folding.h\"],\n     deps = [\n-        \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:status\",\n     ],\n )\n \n@@ -4459,18 +4442,15 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/ir:hlo_module_group\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/base:nullability\",\n@@ -5507,12 +5487,8 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_protobuf//:any_cc_proto\",\n         \"@local_tsl//tsl/platform\",\n-        \"@local_tsl//tsl/platform:casts\",\n-        \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:logging\",  # fixdeps: keep\n         \"@local_tsl//tsl/platform:protobuf\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -5659,10 +5635,10 @@ xla_cc_test(\n         \":scatter_simplifier\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n "
        },
        {
            "sha": "7830dd4d6e8b8dee6128629ccbd2999195a7ff14",
            "filename": "third_party/xla/xla/service/batchnorm_expander.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fbatchnorm_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fbatchnorm_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbatchnorm_expander.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -37,10 +37,9 @@ limitations under the License.\n #include \"xla/literal_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n \n@@ -146,7 +145,7 @@ bool BatchNormExpanderVisitor::Run(HloComputation* computation,\n       /*rewrite_training_op=*/rewrite_training_op,\n       /*rewrite_inference_op=*/rewrite_inference_op,\n       /*rewrite_grad_op=*/rewrite_grad_op);\n-  TF_CHECK_OK(computation->Accept(&visitor));\n+  CHECK_OK(computation->Accept(&visitor));\n   return visitor.changed();\n }\n \n@@ -294,7 +293,7 @@ absl::Status BatchNormExpanderVisitor::HandleBatchNormTraining(\n     }\n     tuple->set_sharding(sharding);\n   }\n-  TF_CHECK_OK(ReplaceWithNewInstruction(batch_norm, std::move(tuple)));\n+  CHECK_OK(ReplaceWithNewInstruction(batch_norm, std::move(tuple)));\n   return absl::OkStatus();\n }\n \n@@ -386,7 +385,7 @@ absl::Status BatchNormExpanderVisitor::HandleBatchNormInference(\n     }\n     shifted_normalized->set_sharding(sharding);\n   }\n-  TF_CHECK_OK(ReplaceInstruction(batch_norm, shifted_normalized));\n+  CHECK_OK(ReplaceInstruction(batch_norm, shifted_normalized));\n   return absl::OkStatus();\n }\n \n@@ -576,7 +575,7 @@ absl::Status BatchNormExpanderVisitor::HandleBatchNormGrad(\n     tuple->set_sharding(sharding);\n   }\n \n-  TF_CHECK_OK(ReplaceWithNewInstruction(batch_norm, std::move(tuple)));\n+  CHECK_OK(ReplaceWithNewInstruction(batch_norm, std::move(tuple)));\n \n   return absl::OkStatus();\n }"
        },
        {
            "sha": "bc7097e7e3c461840054234e838c5567e57cbbd5",
            "filename": "third_party/xla/xla/service/buffer_assignment_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n@@ -65,7 +66,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -104,7 +104,7 @@ class InstructionListVisitor : public DfsHloVisitorWithDefault {\n \n std::vector<const HloInstruction*> GetInstructions(HloInstruction* root) {\n   InstructionListVisitor main_list(root);\n-  TF_CHECK_OK(root->Accept(&main_list));\n+  CHECK_OK(root->Accept(&main_list));\n   return main_list.GetInstructions();\n }\n "
        },
        {
            "sha": "cbfa2a8aaf88d844a941d7a226885d265538c9e5",
            "filename": "third_party/xla/xla/service/compilation_environments.h",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcompilation_environments.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcompilation_environments.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompilation_environments.h?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -16,19 +16,16 @@ limitations under the License.\n #ifndef XLA_SERVICE_COMPILATION_ENVIRONMENTS_H_\n #define XLA_SERVICE_COMPILATION_ENVIRONMENTS_H_\n \n-#include <cstdint>\n #include <functional>\n #include <memory>\n-#include <typeindex>\n-#include <utility>\n \n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/xla.pb.h\"\n-#include \"tsl/platform/casts.h\"\n-#include \"tsl/platform/platform.h\"\n #include \"tsl/platform/protobuf.h\"\n-#include \"tsl/platform/status.h\"\n \n namespace xla {\n \n@@ -156,7 +153,7 @@ T& CompilationEnvironments::GetMutableEnv() {\n   }\n \n   if (it == environments_.end()) {\n-    TF_CHECK_OK(AddEnvImpl(*descriptor, nullptr));\n+    CHECK_OK(AddEnvImpl(*descriptor, nullptr));\n     DefaultEnvCreatedByCompilationEnvironments(descriptor->full_name());\n     it = environments_.find(descriptor);\n   }"
        },
        {
            "sha": "84d630eefff3f6149f9b62f8217d70b1ad548ae7",
            "filename": "third_party/xla/xla/service/conditional_code_motion.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fconditional_code_motion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fconditional_code_motion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fconditional_code_motion.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -51,7 +51,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n@@ -102,7 +101,7 @@ HloInstruction* CloneNestedTuples(HloInstruction* tuple) {\n                                             /* accept_different_shape =*/true);\n     } else {\n       for (auto tuple_user : tuple_users) {\n-        TF_CHECK_OK(tuple->ReplaceUseWithDifferentShape(tuple_user, new_tuple));\n+        CHECK_OK(tuple->ReplaceUseWithDifferentShape(tuple_user, new_tuple));\n       }\n     }\n     return new_tuple;\n@@ -635,7 +634,7 @@ absl::StatusOr<bool> ConvertSpecialMove(HloInstruction* conditional,\n     // The shape can vary since the operands to convert are now\n     // being returned through the branches' root.\n     cur_branch->set_root_instruction(new_branch_root, true /*new shape*/);\n-    TF_CHECK_OK(cur_branch->RemoveInstruction(old_root));\n+    CHECK_OK(cur_branch->RemoveInstruction(old_root));\n \n     // Only one of the branches needs to change the conditional->parent().\n     if (branch != 0) {\n@@ -651,7 +650,7 @@ absl::StatusOr<bool> ConvertSpecialMove(HloInstruction* conditional,\n     // Ensure that all the users of conditional refer to the new one.\n     TF_RETURN_IF_ERROR(\n         conditional->ReplaceAllUsesWithDifferentShape(newconditional));\n-    TF_CHECK_OK(conditional_parent->RemoveInstruction(conditional));\n+    CHECK_OK(conditional_parent->RemoveInstruction(conditional));\n     conditional = newconditional;\n     // Add the hoisted instructions in the parent.\n     for (HloInstruction* hoist : to_hoist_set) {\n@@ -671,7 +670,7 @@ absl::StatusOr<bool> ConvertSpecialMove(HloInstruction* conditional,\n           hoist->CloneWithNewOperands(hoist->shape(), new_operands));\n       VLOG(2) << \"Hoisted instruction in parent:\" << hoisted->ToString();\n       TF_RETURN_IF_ERROR(gte_hoist->ReplaceAllUsesWith(hoisted));\n-      TF_CHECK_OK(conditional_parent->RemoveInstruction(gte_hoist));\n+      CHECK_OK(conditional_parent->RemoveInstruction(gte_hoist));\n     }\n     // No need to explicitly delete a hoisted instruction since if its dead\n     // then the subsequent DCE will remove it.\n@@ -1063,7 +1062,7 @@ class MoveOperandIntoBranch {\n           } else {\n             VLOG(1) << \"new_param_shape=\" << new_param_shape->ToString();\n             *param_user->mutable_shape() = *new_param_shape;\n-            TF_CHECK_OK(param_user->ReplaceAllUsesWith(branch_param));\n+            CHECK_OK(param_user->ReplaceAllUsesWith(branch_param));\n           }\n         }\n       }"
        },
        {
            "sha": "76b139413ab71b6029f9a7dbed7379c473487a5c",
            "filename": "third_party/xla/xla/service/copy_insertion_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -46,10 +46,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test_benchmark.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/statusor.h\"\n-#include \"tsl/platform/test_benchmark.h\"\n \n namespace op = xla::testing::opcode_matchers;\n \n@@ -804,9 +802,9 @@ ENTRY %DependentTupleElements.While () -> (s32[], f32[8]) {\n       outer_while_body->AddInstruction(HloInstruction::CreateWhile(\n           while_hlo->shape(), while_hlo->while_condition(),\n           while_hlo->while_body(), dual_init));\n-  TF_CHECK_OK(outer_while_body->ReplaceInstruction(\n+  CHECK_OK(outer_while_body->ReplaceInstruction(\n       outer_while_body->root_instruction(), dual_while));\n-  TF_CHECK_OK(while_hlo->parent()->ReplaceInstruction(while_hlo, outer_while));\n+  CHECK_OK(while_hlo->parent()->ReplaceInstruction(while_hlo, outer_while));\n   InsertCopies(module_.get());\n }\n \n@@ -863,9 +861,9 @@ ENTRY %DependentTupleElements.While () -> (s32[], f32[8]) {\n       outer_while_body->AddInstruction(HloInstruction::CreateWhile(\n           while_hlo->shape(), while_hlo->while_condition(),\n           while_hlo->while_body(), outer_param));\n-  TF_CHECK_OK(outer_while_body->ReplaceInstruction(\n+  CHECK_OK(outer_while_body->ReplaceInstruction(\n       outer_while_body->root_instruction(), dual_while));\n-  TF_CHECK_OK(while_hlo->parent()->ReplaceInstruction(while_hlo, outer_while));\n+  CHECK_OK(while_hlo->parent()->ReplaceInstruction(while_hlo, outer_while));\n   InsertCopies(module_.get());\n }\n \n@@ -930,9 +928,9 @@ ENTRY %DependentTupleElements.While () -> (s32[], f32[8]{0}, s32[], f32[8]{0}, s\n       outer_while_body->AddInstruction(HloInstruction::CreateWhile(\n           while_hlo->shape(), while_hlo->while_condition(),\n           while_hlo->while_body(), dual_init));\n-  TF_CHECK_OK(outer_while_body->ReplaceInstruction(\n+  CHECK_OK(outer_while_body->ReplaceInstruction(\n       outer_while_body->root_instruction(), dual_while));\n-  TF_CHECK_OK(while_hlo->parent()->ReplaceInstruction(while_hlo, outer_while));\n+  CHECK_OK(while_hlo->parent()->ReplaceInstruction(while_hlo, outer_while));\n   InsertCopies(module_.get());\n }\n "
        },
        {
            "sha": "a540f23b472b50a46b4cede317034c926543a570",
            "filename": "third_party/xla/xla/service/copy_removal.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_removal.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -48,7 +48,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/util.h\"\n \n using absl::StrAppend;\n@@ -320,7 +319,7 @@ bool ComputeRelativeLocation::AddControlDependenceForUnorderedOps() {\n         VLOG(3) << \"   Adding control dependence between:\";\n         VLOG(3) << \"     predecessor: \" << entry2->name();\n         VLOG(3) << \"       successor: \" << entry1->name();\n-        TF_CHECK_OK(entry2->AddControlDependencyTo(entry1));\n+        CHECK_OK(entry2->AddControlDependencyTo(entry1));\n       }\n       reachability_map.UpdateReachabilityThroughInstruction(entry1);\n       for (HloInstruction* entry2 : instr_it.second) {\n@@ -731,7 +730,7 @@ CopyRemover::CopyRemover(\n   CreateCopyMap(module, value_to_node);\n \n   XLA_VLOG_LINES(3, ToString());\n-  TF_DCHECK_OK(Verify());\n+  DCHECK_OK(Verify());\n }\n \n // Add a list containing the given values to CopyRemover. This\n@@ -1193,7 +1192,7 @@ bool CopyRemover::TryElideCopy(\n   RemoveCopyValue(copy_node.dest);\n \n   XLA_VLOG_LINES(4, ToString());\n-  TF_DCHECK_OK(Verify());\n+  DCHECK_OK(Verify());\n   VLOG(3) << \"TryElideCopy succeeded for: \" << copy->name();\n   return true;\n }"
        },
        {
            "sha": "3bddef3ef2a5d19ae693a5d8d1fc291159368773",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -1265,11 +1265,11 @@ cc_library(\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service/llvm_ir:dynamic_update_slice_util\",\n-        \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "e8336ec3189ef65142d2540b66e1476563dcb986",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include <stack>\n #include <string>\n #include <tuple>\n-#include <type_traits>\n #include <utility>\n #include <vector>\n \n@@ -35,7 +34,6 @@ limitations under the License.\n // IWYU pragma: no_include \"llvm/Config/Disassemblers.def.inc\"\n // IWYU pragma: no_include \"llvm/Config/Targets.def.inc\"\n \n-#include \"absl/cleanup/cleanup.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n@@ -88,18 +86,15 @@ limitations under the License.\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"xla/backends/cpu/alignment.h\"\n #include \"xla/backends/cpu/codegen/builtin_definition_generator.h\"\n-#include \"xla/backends/cpu/codegen/cpu_features.h\"\n #include \"xla/backends/cpu/codegen/emitters/cpu_fusion_emitter_config.h\"\n #include \"xla/backends/cpu/codegen/execution_engine.h\"\n #include \"xla/backends/cpu/codegen/ir_compiler.h\"\n #include \"xla/backends/cpu/codegen/jit_compiler.h\"\n-#include \"xla/backends/cpu/codegen/object_loader.h\"\n #include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/backends/cpu/constant_allocation.h\"\n #include \"xla/backends/cpu/runtime/function_library.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk.pb.h\"\n-#include \"xla/backends/cpu/runtime/thunk_proto_serdes.h\"\n #include \"xla/backends/cpu/target_machine_options.h\"\n #include \"xla/backends/cpu/transforms/collectives/all_reduce_combiner.h\"\n #include \"xla/backends/cpu/transforms/library_rewriter.h\"\n@@ -234,7 +229,6 @@ limitations under the License.\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/util.h\"\n@@ -304,8 +298,8 @@ ModuleComputationsTransitivelyContainCustomCall(const HloModule& module) {\n   std::unique_ptr<CallGraph> call_graph = CallGraph::Build(&module);\n \n   // Can never fail because we always return an OK status from the visitor.\n-  TF_CHECK_OK(call_graph->VisitNodes([&custom_call_map](\n-                                         const CallGraphNode& node) {\n+  CHECK_OK(call_graph->VisitNodes([&custom_call_map](\n+                                      const CallGraphNode& node) {\n     const HloComputation* computation = node.computation();\n \n     for (const HloInstruction* instruction : computation->instructions()) {"
        },
        {
            "sha": "42b3f7e1a09a822c240496ff3aca14420bb19e39",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -106,7 +106,6 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/lib/math/math_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -168,7 +167,7 @@ IrEmitter::IrEmitter(mlir::MLIRContext* mlir_context,\n       &hlo_module, &thread_local_computations_, &global_computations_);\n   absl::c_sort(thread_local_computations_);\n   absl::c_sort(global_computations_);\n-  TF_CHECK_OK(s) << \"Should have failed buffer assignment.\";\n+  CHECK_OK(s) << \"Should have failed buffer assignment.\";\n   SetModuleMemoryRegionName(*module_, \"ir_emitter\");\n }\n "
        },
        {
            "sha": "6386d3ce976c63bbcb7005abec1cec9d5bb25064",
            "filename": "third_party/xla/xla/service/cpu/parallel_task_assignment.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_task_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_task_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_task_assignment.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -40,7 +41,6 @@ limitations under the License.\n #include \"xla/service/llvm_ir/dynamic_update_slice_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_partition.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/cpu_info.h\"\n \n@@ -286,7 +286,7 @@ bool ParallelTaskAssigner::AssignParallelTasksHelper(\n     absl::c_copy(dim_partition_counts,\n                  tsl::protobuf::RepeatedFieldBackInserter(\n                      backend_config.mutable_outer_dimension_partitions()));\n-    TF_CHECK_OK(instruction->set_backend_config(backend_config));\n+    CHECK_OK(instruction->set_backend_config(backend_config));\n \n     VLOG(2) << \"Assigned parallel task count: \" << total_partition_count\n             << \" to instruction: \" << instruction->name();"
        },
        {
            "sha": "5d606be110b90e5812936164275f091eed314f2e",
            "filename": "third_party/xla/xla/service/dynamic_dimension_inference.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -62,7 +62,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/window_util.h\"\n@@ -1352,7 +1351,7 @@ absl::Status DynamicDimensionInferenceVisitor::HandleReshape(\n             auto orig_reshape_pair = find_reshape_group_pair(op, op_dim_index);\n             if (is_reverse_reshape_group_pair(op, orig_reshape_pair, hlo,\n                                               reshape_pair)) {\n-              TF_CHECK_OK(ForEachOperandDynamicDimension(\n+              CHECK_OK(ForEachOperandDynamicDimension(\n                   op,\n                   [&](HloInstruction* operand, ShapeIndex index,\n                       int64_t op_dynamic_dimension, int64_t operand_index,\n@@ -2553,7 +2552,7 @@ absl::Status DynamicDimensionInferenceVisitor::InsertPadToStaticOnInstruction(\n           HloInstruction* tuple =\n               element->AddInstruction(HloInstruction::CreateVariadic(\n                   subshape, HloOpcode::kTuple, children));\n-          TF_CHECK_OK(ForEachOperandDynamicDimension(\n+          CHECK_OK(ForEachOperandDynamicDimension(\n               tuple,\n               [&](HloInstruction* operand, ShapeIndex index, int64_t dimension,\n                   int64_t operand_index, HloInstruction* dynamic_size) {"
        },
        {
            "sha": "ca9c4a19b4e8ed58410296348d1f848dd8f3956a",
            "filename": "third_party/xla/xla/service/dynamic_dimension_inference_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 6,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_dimension_inference_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -15,24 +15,33 @@ limitations under the License.\n \n #include \"xla/service/dynamic_dimension_inference.h\"\n \n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/comparison_util.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n+#include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/test.h\"\n-#include \"xla/hlo/utils/hlo_matchers.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/service/hlo_runner.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/statusor.h\"\n-#include \"tsl/platform/test_benchmark.h\"\n \n namespace xla {\n namespace {\n@@ -620,7 +629,7 @@ TEST_F(DynamicDimensionInferenceTest, ReshapeIntoScalar) {\n   module_->AddEntryComputation(builder.Build());\n \n   SCOPED_TRACE(module_->ToString());\n-  TF_CHECK_OK(RunInference());\n+  CHECK_OK(RunInference());\n }\n \n TEST_F(DynamicDimensionInferenceTest, GatherTest) {"
        },
        {
            "sha": "7d1b36edb04fc677cf2d5b6722ae8337e77c9b35",
            "filename": "third_party/xla/xla/service/dynamic_padder_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_padder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_padder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fdynamic_padder_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -46,14 +46,14 @@ limitations under the License.\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/dynamic_dimension_inference.h\"\n+#include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tests/llvm_irgen_test_base.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/protobuf/error_codes.pb.h\"\n #include \"xla/util.h\"\n@@ -766,9 +766,9 @@ class ExecutionTest : public HloTestBase {\n     DynamicPadderOptions options;\n     options.slice_dynamic_output = slice_dynamic_output;\n     DynamicPadder padder(options);\n-    TF_CHECK_OK(padder.Run(module.get()).status());\n+    CHECK_OK(padder.Run(module.get()).status());\n     HloDCE dce;\n-    TF_CHECK_OK(dce.Run(module.get()).status());\n+    CHECK_OK(dce.Run(module.get()).status());\n     return Execute(std::move(module), {arguments});\n   }\n };\n@@ -825,7 +825,7 @@ ENTRY main {\n   Literal updates_padded = LiteralUtil::CreateR2<int32_t>(\n       {{10, 20, 30}, {70, 80, 90}, {30, 22, 11}, {-1, 20, -1}});\n   DynamicPadder padder;\n-  TF_CHECK_OK(padder.Run(module_padded.get()).status());\n+  CHECK_OK(padder.Run(module_padded.get()).status());\n   TF_ASSERT_OK_AND_ASSIGN(Literal padded,\n                           PadAndExecute(std::move(module_padded),\n                                         {&operand, &scatter_indices_padded,\n@@ -917,7 +917,7 @@ ENTRY main {\n \n   auto module_padded = GetHloModule(hlo_text);\n   DynamicPadder padder;\n-  TF_CHECK_OK(padder.Run(module_padded.get()).status());\n+  CHECK_OK(padder.Run(module_padded.get()).status());\n   TF_ASSERT_OK_AND_ASSIGN(\n       Literal not_padded,\n       PadAndExecute(std::move(module_padded),\n@@ -973,7 +973,7 @@ ENTRY main {\n       LiteralUtil::CreateR3<int32_t>({{{1}, {2}}, {{3}, {4}}, {{5}, {6}}});\n   auto module = GetHloModule(hlo_text);\n   DynamicPadder padder;\n-  TF_CHECK_OK(padder.Run(module.get()).status());\n+  CHECK_OK(padder.Run(module.get()).status());\n   TF_ASSERT_OK_AND_ASSIGN(Literal result,\n                           PadAndExecute(std::move(module), {&operand}));\n \n@@ -1025,7 +1025,7 @@ ENTRY main {\n   Literal operand_padded = LiteralUtil::CreateR2<int32_t>(\n       {{1, 2, 3, 4}, {4, 5, 6, 7}, {1, 2, 3, 4}, {4, 5, 6, 7}});\n   DynamicPadder padder;\n-  TF_CHECK_OK(padder.Run(module_padded.get()).status());\n+  CHECK_OK(padder.Run(module_padded.get()).status());\n   TF_ASSERT_OK_AND_ASSIGN(Literal padded,\n                           PadAndExecute(std::move(module_padded),\n                                         {&operand_padded, &dynamic_size}));"
        },
        {
            "sha": "efd4b94b151518818874c9b4de6f6eccca9f29eb",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -811,7 +811,6 @@ cc_library(\n         \"//xla/tsl/platform:env_time\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/base:nullability\",\n@@ -1342,14 +1341,14 @@ cc_library(\n         \"//xla/tools:hlo_module_loader\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_protobuf//:protobuf\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:protobuf\",\n     ],\n@@ -2010,7 +2009,6 @@ xla_test(\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/testing:temporary_directory\",\n@@ -2619,7 +2617,6 @@ xla_test(\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:log_severity\",\n@@ -2737,6 +2734,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:kernel_metadata\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor:launch_dim\",\n@@ -3034,16 +3032,13 @@ cc_library(\n         \"//xla/service/gpu/autotuning:gpu_autotuning_proto_cc\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@local_tsl//tsl/platform:env\",\n-        \"@local_tsl//tsl/platform:protobuf\",\n-        \"@local_tsl//tsl/platform:status\",\n+        \"@com_google_protobuf//:protobuf\",\n     ],\n )\n \n@@ -3391,10 +3386,9 @@ xla_cc_test(\n         \":fusion_deduplication_cache\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n "
        },
        {
            "sha": "e112824592e305313ece6cc12b4f341a25212bf7",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -331,8 +331,8 @@ xla_cc_test(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n@@ -557,6 +557,7 @@ cc_library(\n     hdrs = [\"conv_algorithm_picker.h\"],\n     tags = [\"gpu\"],\n     deps = [\n+        \":autotune_cache_key\",\n         \":autotuner_util\",\n         \":gpu_autotuning_proto_cc\",\n         \":redzone_buffers\",\n@@ -594,7 +595,6 @@ cc_library(\n         \"//xla/stream_executor/rocm:rocm_platform_id\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"//xla/tsl/util:env_var\",\n@@ -696,7 +696,6 @@ xla_cc_test(\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "6082d2ad156f7dd2cd5392cd5594588e377f29a6",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -49,7 +49,6 @@ limitations under the License.\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"  // IWYU pragma: keep\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/xla.pb.h\"\n@@ -364,16 +363,16 @@ class FileBasedCacheTest : public AutotunerUtilTest {\n \n   static std::string Read(const absl::string_view filepath) {\n     std::string file_content;\n-    TF_CHECK_OK(tsl::ReadFileToString(tsl::Env::Default(),\n-                                      std::string(filepath), &file_content));\n+    CHECK_OK(tsl::ReadFileToString(tsl::Env::Default(), std::string(filepath),\n+                                   &file_content));\n     return file_content;\n   }\n \n   void Write(const absl::string_view filepath,\n              const absl::string_view content) {\n-    TF_CHECK_OK(CreateDirIfNeeded(cache_dir_, tsl::Env::Default()));\n-    TF_CHECK_OK(tsl::WriteStringToFile(tsl::Env::Default(),\n-                                       std::string(filepath), content));\n+    CHECK_OK(CreateDirIfNeeded(cache_dir_, tsl::Env::Default()));\n+    CHECK_OK(tsl::WriteStringToFile(tsl::Env::Default(), std::string(filepath),\n+                                    content));\n   }\n \n   stream_executor::StreamExecutor* executor_ = NewStreamExecutor();"
        },
        {
            "sha": "0f29eea297fb4fa782985a6982ac92386e769155",
            "filename": "third_party/xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/literal_util.h\"\n+#include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/autotuning/gpu_autotuning.pb.h\"\n #include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n@@ -72,7 +73,6 @@ limitations under the License.\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/protobuf/dnn.pb.h\"\n #include \"xla/tsl/util/env_var.h\"\n@@ -112,8 +112,8 @@ class ScratchAllocator : public se::ScratchAllocator {\n \n   static int64_t GetDefaultMemoryLimitInBytes() {\n     int64_t value;\n-    TF_CHECK_OK(tsl::ReadInt64FromEnvVar(\"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\",\n-                                         1LL << 12, &value));\n+    CHECK_OK(tsl::ReadInt64FromEnvVar(\"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\",\n+                                      1LL << 12, &value));\n     return value * (1LL << 20);\n   }\n "
        },
        {
            "sha": "2f684375bd48a48d3f2b5a74dbb32828e08eea5a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/dot_search_space_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n@@ -32,7 +33,6 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n@@ -104,7 +104,7 @@ class DefaultDeviceDotSearchSpaceTest : public HloHardwareIndependentTestBase {\n   se::DeviceDescription device_description_ = []() {\n     auto device_description_ = se::DeviceDescription::FromProto(\n         se::GpuDeviceInfoProto::default_instance());\n-    TF_CHECK_OK(device_description_.status());\n+    CHECK_OK(device_description_.status());\n     return *device_description_;\n   }();\n "
        },
        {
            "sha": "d75efc5a9002fce0e91e82c1d1390be9059daccf",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"google/protobuf/text_format.h\"\n #include \"xla/autotune_results.pb.h\"\n #include \"xla/autotuning.pb.h\"\n@@ -49,7 +50,6 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/fission_backend.h\"\n #include \"xla/backends/gpu/autotuner/triton.h\"\n #include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n@@ -111,7 +111,6 @@ limitations under the License.\n #include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/tsl/util/proto/proto_utils.h\"\n@@ -1167,7 +1166,7 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n                   << Serialize(config) << \"'\";\n           absl::StatusOr<std::unique_ptr<Executable>> executable =\n               compile(fusion, config, gemm_config_set.size() > 1);\n-          TF_CHECK_OK(executable.status())\n+          CHECK_OK(executable.status())\n               << \" - Failure occured when compiling fusion \" << fusion->name()\n               << \" with config '\" << ConfigToString(config)\n               << \"'\\nFused HLO computation:\\n\""
        },
        {
            "sha": "38c0ba3c926b0f36cbb64960296ba0d4bfa1f502",
            "filename": "third_party/xla/xla/service/gpu/fusion_deduplication_cache_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_deduplication_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_deduplication_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_deduplication_cache_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -16,13 +16,12 @@ limitations under the License.\n #include \"xla/service/gpu/fusion_deduplication_cache.h\"\n \n #include <gtest/gtest.h>\n+#include \"absl/log/check.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/statusor.h\"\n-#include \"tsl/platform/test.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n@@ -37,7 +36,7 @@ HloInstruction* Fuse(HloInstruction* producer, HloInstruction* consumer,\n     fusion_instruction =\n         computation->AddInstruction(HloInstruction::CreateFusion(\n             consumer->shape(), HloInstruction::FusionKind::kLoop, consumer));\n-    TF_CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n+    CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n   }\n \n   if (producer->opcode() == HloOpcode::kFusion) {\n@@ -56,7 +55,7 @@ HloInstruction* Fuse(HloInstruction* producer, HloInstruction* consumer,\n \n   // In case of multi-output fusion, `producer` would already be deleted.\n   if (!allow_multi_output && producer->user_count() == 0) {\n-    TF_CHECK_OK(computation->RemoveInstruction(producer));\n+    CHECK_OK(computation->RemoveInstruction(producer));\n   }\n \n   return fusion_instruction;"
        },
        {
            "sha": "559c9734623d39b0ce4c415e0f973c9a14f15e8a",
            "filename": "third_party/xla/xla/service/gpu/fusion_process_dump.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_process_dump.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_process_dump.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_process_dump.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"google/protobuf/text_format.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -32,7 +33,6 @@ limitations under the License.\n #include \"xla/tools/hlo_module_loader.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/path.h\"\n@@ -59,7 +59,7 @@ HloInstruction* AddFusionInstruction(HloInstruction* producer,\n   auto fusion_instruction = computation->AddInstruction(\n       HloInstruction::CreateFusion(consumer->shape(), kind, consumer),\n       /*new_name=*/fusion_name);\n-  TF_CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n+  CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n \n   return fusion_instruction;\n }\n@@ -76,7 +76,7 @@ HloInstruction* Fuse(HloInstruction* producer, HloInstruction* consumer,\n   }\n \n   if (producer->user_count() == 0) {\n-    TF_CHECK_OK(computation->RemoveInstruction(producer));\n+    CHECK_OK(computation->RemoveInstruction(producer));\n   }\n \n   return fusion_instruction;"
        },
        {
            "sha": "e631830d470a7d3415c78ff9e4612b61a8c4fecc",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -83,7 +83,6 @@ limitations under the License.\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n@@ -630,10 +629,10 @@ class GpuCompilerTestWithAutotuneDb : public GpuCompilerTest {\n     std::string tmp_filepath = ::testing::TempDir();\n     ASSERT_TRUE(env->CreateUniqueFileName(&tmp_filepath, \".textproto\"));\n \n-    absl::Cleanup cleanup = [&] { TF_CHECK_OK(env->DeleteFile(tmp_filepath)); };\n+    absl::Cleanup cleanup = [&] { CHECK_OK(env->DeleteFile(tmp_filepath)); };\n \n     std::string contents;\n-    TF_CHECK_OK(tsl::ReadFileToString(env, path, &contents));\n+    CHECK_OK(tsl::ReadFileToString(env, path, &contents));\n \n     // The autotuning cache entries depend on the DNN library version, but this\n     // is not relevant for these tests. Therefore we replace the DNN version"
        },
        {
            "sha": "d7c376a651ebd825601973fe3db29482c5369fbc",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -96,7 +96,6 @@ limitations under the License.\n #include \"xla/tsl/platform/env_time.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/random.h\"\n@@ -708,7 +707,7 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {\n   // destructor will not race with any operations in flight (deallocate\n   // xla::Literal owned by the HLO module).\n   if (submitted_mem_copies) {\n-    TF_CHECK_OK(stream->BlockHostUntilDone());\n+    CHECK_OK(stream->BlockHostUntilDone());\n   }\n \n   module_handles_.emplace(executor,"
        },
        {
            "sha": "4d0823772f502d3fa7fe89c9a6bffa72d14cd9e1",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -56,7 +56,6 @@ limitations under the License.\n #include \"xla/tests/test_utils.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/profiler/protobuf/profiled_instructions.pb.h\"\n \n@@ -87,7 +86,7 @@ class GpuHloScheduleTest : public HloTestBase {\n   }\n \n   SequentialHloOrdering BuildHloOrdering(HloModule* module) {\n-    TF_CHECK_OK(ScheduleGpuModule(module).status());\n+    CHECK_OK(ScheduleGpuModule(module).status());\n     return SequentialHloOrdering{module->schedule()};\n   }\n \n@@ -224,7 +223,7 @@ TEST_P(GpuHloScheduleParameterizedTest, AsyncCustomCall) {\n   static_cast<HloCustomCallInstruction*>(nonblocking_call)\n       ->set_custom_call_schedule(SCHEDULE_EARLIEST);\n   // In addition, add control_dependency: add1->nonblocking_call.\n-  TF_CHECK_OK(add1->AddControlDependencyTo(nonblocking_call));\n+  CHECK_OK(add1->AddControlDependencyTo(nonblocking_call));\n   // Blocking call, which only add4 depends on.\n   HloInstruction* blocking_call =\n       builder.AddInstruction(HloInstruction::CreateCustomCall(\n@@ -295,7 +294,7 @@ TEST_P(GpuHloScheduleParameterizedTest, AsyncCollectivePermute) {\n           collective_permute_start_shape, add0,\n           /*source_target_pairs=*/{{0, 1}}, /*channel_id=*/std::nullopt));\n   // In addition, add control_dependency: add1->nonblocking_call.\n-  TF_CHECK_OK(add1->AddControlDependencyTo(collective_permute_start));\n+  CHECK_OK(add1->AddControlDependencyTo(collective_permute_start));\n   // Blocking call, which only add4 depends on.\n   HloInstruction* collective_permute_done = builder.AddInstruction(\n       HloInstruction::CreateUnary(f32_2x2_, HloOpcode::kCollectivePermuteDone,\n@@ -1388,7 +1387,7 @@ ENTRY e {\n   ROOT t = (f32[1024,1024]{1,0}, f32[1024,1024]{1,0}) tuple(wrapped_exponential, wrapped_negate)\n })\")\n                     .value();\n-  TF_CHECK_OK(ScheduleGpuModule(module.get()).status());\n+  CHECK_OK(ScheduleGpuModule(module.get()).status());\n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n // CHECK: ENTRY\n // CHECK: wrapped_negate = f32[1024,1024]{1,0}\n@@ -1514,7 +1513,7 @@ TEST_P(GpuHloScheduleParameterizedTest, AsyncAllReduce) {\n           /*constrain_layout=*/false,\n           /*channel_id=*/std::nullopt, /*use_global_device_ids=*/true));\n   // In addition, add control_dependency: add1->nonblocking_call.\n-  TF_CHECK_OK(add1->AddControlDependencyTo(all_reduce_start));\n+  CHECK_OK(add1->AddControlDependencyTo(all_reduce_start));\n   // Blocking call, which only add4 depends on.\n   HloInstruction* all_reduce_done =\n       builder.AddInstruction(HloInstruction::CreateUnary(\n@@ -1725,7 +1724,7 @@ TEST_P(GpuHloScheduleParameterizedTest, CopyStartDoneScheduled) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto module, ParseAndReturnVerifiedModule(kHloCopyStartDone,\n                                                 GetModuleConfig(test_config)));\n-  TF_CHECK_OK(ScheduleGpuModule(module.get()).status());\n+  CHECK_OK(ScheduleGpuModule(module.get()).status());\n   EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n // CHECK: ENTRY\n // CHECK: copy-start.3 = (f32[512,1024]{1,0}, f32[512,1024]{1,0:S(5)}, u32[]) copy-start"
        },
        {
            "sha": "ff3f9583cdc6e93d566d6409534bc04c84e0abc4",
            "filename": "third_party/xla/xla/service/gpu/hlo_algorithm_denylist.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"google/protobuf/text_format.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/backend_config.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -33,11 +34,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/tsl/platform/env.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n-#include \"tsl/platform/env.h\"\n-#include \"tsl/platform/protobuf.h\"\n-#include \"tsl/platform/status.h\"\n \n namespace xla {\n namespace gpu {\n@@ -215,11 +212,11 @@ std::vector<stream_executor::dnn::AlgorithmDesc> GetDisabledConvAlgorithms(\n         GetDebugOptionsFromFlags().xla_gpu_algorithm_denylist_path();\n     if (!file_path.empty()) {\n       std::string denylist_text;\n-      TF_CHECK_OK(tsl::ReadFileToString(tsl::Env::Default(), file_path,\n-                                        &denylist_text));\n-      TF_CHECK_OK(ParseTextFormatDenyList(*list, denylist_text));\n+      CHECK_OK(tsl::ReadFileToString(tsl::Env::Default(), file_path,\n+                                     &denylist_text));\n+      CHECK_OK(ParseTextFormatDenyList(*list, denylist_text));\n     }\n-    TF_CHECK_OK(ParseTextFormatDenyList(*list, kDefaultDenylist));\n+    CHECK_OK(ParseTextFormatDenyList(*list, kDefaultDenylist));\n     return list;\n   }();\n "
        },
        {
            "sha": "26d4fcdf334684bc2fddcaec7f1d7662998710aa",
            "filename": "third_party/xla/xla/service/gpu/kernels/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -242,16 +242,17 @@ xla_cc_binary(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_platform\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/platform:test_benchmark\",\n         \"//xla/tsl/platform:test_main\",\n+        \"@com_google_absl//absl/log:check\",\n     ],\n )\n \n@@ -431,14 +432,15 @@ xla_test(\n         \":ptx_custom_kernel\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_platform\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/platform:test_main\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\",\n     ],"
        },
        {
            "sha": "5b6c49d234947a8a2824d85dc2fe2eda4cb75e3f",
            "filename": "third_party/xla/xla/service/gpu/kernels/cutlass_gemm_custom_kernel_benchmarks.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_benchmarks.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_benchmarks.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_custom_kernel_benchmarks.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -17,15 +17,16 @@ limitations under the License.\n #include <cstring>\n #include <vector>\n \n+#include \"absl/log/check.h\"\n #include \"xla/service/gpu/kernels/cutlass_gemm_custom_kernel.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/tsl/platform/test_benchmark.h\"\n@@ -67,18 +68,18 @@ static void BM_RowMajorGemm(benchmark::State& state) {\n   se::DeviceMemory<float> b = executor->AllocateArray<float>(k * n, 0);\n   se::DeviceMemory<float> c = executor->AllocateArray<float>(m * n, 0);\n \n-  TF_CHECK_OK(stream->Memset32(&a, BitPattern(1.1f), a.size()));\n-  TF_CHECK_OK(stream->Memset32(&b, BitPattern(1.2f), b.size()));\n-  TF_CHECK_OK(stream->MemZero(&c, c.size()));\n+  CHECK_OK(stream->Memset32(&a, BitPattern(1.1f), a.size()));\n+  CHECK_OK(stream->Memset32(&b, BitPattern(1.2f), b.size()));\n+  CHECK_OK(stream->MemZero(&c, c.size()));\n \n   se::KernelArgsDeviceMemoryArray args(\n       std::vector<se::DeviceMemoryBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n \n   for (auto s : state) {\n-    TF_CHECK_OK(gemm->Launch(custom_kernel.thread_dims(),\n-                             custom_kernel.block_dims(), stream.get(), args));\n-    TF_CHECK_OK(stream->BlockHostUntilDone());\n+    CHECK_OK(gemm->Launch(custom_kernel.thread_dims(),\n+                          custom_kernel.block_dims(), stream.get(), args));\n+    CHECK_OK(stream->BlockHostUntilDone());\n   }\n }\n "
        },
        {
            "sha": "39beb77d4793ec921bc4707289e1c60e4f1e88d3",
            "filename": "third_party/xla/xla/service/gpu/kernels/ptx_custom_kernel_test.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -22,15 +22,16 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/log/check.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/stream_executor/cuda/cuda_platform.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n \n@@ -98,20 +99,20 @@ TEST(PtxCustomKernelTest, GetPtxCustomKernel) {\n   se::DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n   se::DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n   se::DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n-  TF_CHECK_OK(stream->Memset32(&a, 1, byte_length));\n-  TF_CHECK_OK(stream->Memset32(&b, 2, byte_length));\n-  TF_CHECK_OK(stream->MemZero(&c, byte_length));\n+  CHECK_OK(stream->Memset32(&a, 1, byte_length));\n+  CHECK_OK(stream->Memset32(&b, 2, byte_length));\n+  CHECK_OK(stream->MemZero(&c, byte_length));\n \n   se::KernelArgsDeviceMemoryArray args(\n       std::vector<se::DeviceMemoryBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n-  TF_CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n-                             custom_kernel.block_dims(), stream.get(), args));\n+  CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n+                          custom_kernel.block_dims(), stream.get(), args));\n \n-  TF_CHECK_OK(stream->BlockHostUntilDone());\n+  CHECK_OK(stream->BlockHostUntilDone());\n \n   std::vector<int32_t> dst(4, 42);\n-  TF_CHECK_OK(stream->Memcpy(dst.data(), c, byte_length));\n+  CHECK_OK(stream->Memcpy(dst.data(), c, byte_length));\n \n   std::vector<int32_t> expected = {3, 3, 3, 3};\n   ASSERT_EQ(dst, expected);\n@@ -138,20 +139,20 @@ TEST(PtxCustomKernelTest, GetPtxCustomKernelWithClusterDim) {\n   se::DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n   se::DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n   se::DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n-  TF_CHECK_OK(stream->Memset32(&a, 1, byte_length));\n-  TF_CHECK_OK(stream->Memset32(&b, 2, byte_length));\n-  TF_CHECK_OK(stream->MemZero(&c, byte_length));\n+  CHECK_OK(stream->Memset32(&a, 1, byte_length));\n+  CHECK_OK(stream->Memset32(&b, 2, byte_length));\n+  CHECK_OK(stream->MemZero(&c, byte_length));\n \n   se::KernelArgsDeviceMemoryArray args(\n       std::vector<se::DeviceMemoryBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n-  TF_CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n-                             custom_kernel.block_dims(), stream.get(), args));\n+  CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n+                          custom_kernel.block_dims(), stream.get(), args));\n \n-  TF_CHECK_OK(stream->BlockHostUntilDone());\n+  CHECK_OK(stream->BlockHostUntilDone());\n \n   std::vector<int32_t> dst(4, 42);\n-  TF_CHECK_OK(stream->Memcpy(dst.data(), c, byte_length));\n+  CHECK_OK(stream->Memcpy(dst.data(), c, byte_length));\n \n   ASSERT_THAT(dst, ElementsAre(3, 3, 3, 3));\n   ASSERT_EQ(custom_kernel.ToString(),\n@@ -217,20 +218,20 @@ TEST(PtxCustomKernelTest, GetOwnedPtxCustomKernel) {\n   se::DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n   se::DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n   se::DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n-  TF_CHECK_OK(stream->Memset32(&a, 1, byte_length));\n-  TF_CHECK_OK(stream->Memset32(&b, 2, byte_length));\n-  TF_CHECK_OK(stream->MemZero(&c, byte_length));\n+  CHECK_OK(stream->Memset32(&a, 1, byte_length));\n+  CHECK_OK(stream->Memset32(&b, 2, byte_length));\n+  CHECK_OK(stream->MemZero(&c, byte_length));\n \n   se::KernelArgsDeviceMemoryArray args(\n       std::vector<se::DeviceMemoryBase>({a, b, c}),\n       custom_kernel.shared_memory_bytes());\n-  TF_CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n-                             custom_kernel.block_dims(), stream.get(), args));\n+  CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n+                          custom_kernel.block_dims(), stream.get(), args));\n \n-  TF_CHECK_OK(stream->BlockHostUntilDone());\n+  CHECK_OK(stream->BlockHostUntilDone());\n \n   std::vector<int32_t> dst(4, 42);\n-  TF_CHECK_OK(stream->Memcpy(dst.data(), c, byte_length));\n+  CHECK_OK(stream->Memcpy(dst.data(), c, byte_length));\n \n   ASSERT_THAT(dst, ElementsAre(3, 3, 3, 3));\n }"
        },
        {
            "sha": "c927c1eb6277317d27b08a7bac7054f655f804cc",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/amdgpu_backend.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -206,8 +206,8 @@ absl::StatusOr<std::vector<uint8_t>> EmitModuleToHsaco(\n   VLOG(1) << \"Compile-time artifacts located at: \" << tempdir_name;\n \n   bool keep_tempfiles = false;\n-  TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_ROCM_KEEP_XLA_TEMPFILES\",\n-                                      /*default_val=*/false, &keep_tempfiles));\n+  CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_ROCM_KEEP_XLA_TEMPFILES\",\n+                                   /*default_val=*/false, &keep_tempfiles));\n   // Prepare filenames for all stages of compilation:\n   // IR, binary ISA, and HSACO.\n   std::string random_number = std::to_string(tsl::random::New64());"
        },
        {
            "sha": "48fdba774a7b6fc9db6cdec6a2b6343462aabb11",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -89,7 +89,7 @@ AnalyticalLatencyEstimator::AnalyticalLatencyEstimator(\n                                   /*min_latencies_seconds=*/{},\n                                   /*count_multiple_input_accesses=*/true},\n       gpu_info_);\n-  TF_CHECK_OK(computation->Accept(&cost_analysis_.value()));\n+  CHECK_OK(computation->Accept(&cost_analysis_.value()));\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "fb1f8a122c8da49bea5ce38366acfeb14905d447",
            "filename": "third_party/xla/xla/service/gpu/model/collective_ptable_stats_collection.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_ptable_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_ptable_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_ptable_stats_collection.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -93,13 +93,13 @@ absl::StatusOr<bool> CollectivePerfTableStatsCollection::RunImpl(\n \n         // Set it in the `CollectiveBackendConfig`.\n         auto gpu_config = instr->backend_config<GpuBackendConfig>();\n-        TF_CHECK_OK(gpu_config.status())\n+        CHECK_OK(gpu_config.status())\n             << \"Cannot parse backend config: \" << instr->ToString();\n         auto reification_cost = gpu_config->add_reification_cost();\n         reification_cost->set_exec_time_us(\n             absl::ToDoubleMicroseconds(exec_time));\n         *reification_cost->mutable_name() = name();\n-        TF_CHECK_OK(instr->set_backend_config(*gpu_config));\n+        CHECK_OK(instr->set_backend_config(*gpu_config));\n       });\n \n   return false;"
        },
        {
            "sha": "4cf55afee14665fb434ac560635ffea015c7b92c",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -115,7 +115,7 @@ void RecordGemmCostModelEstimateIfApplicable(\n           << gemm_reification_cost->DebugString()\n           << \"\\nInstruction: \" << instruction.ToString();\n \n-  TF_CHECK_OK(instruction.set_backend_config(*gpu_config));\n+  CHECK_OK(instruction.set_backend_config(*gpu_config));\n }\n \n }  // namespace\n@@ -127,7 +127,7 @@ absl::StatusOr<bool> GpuCostModelStatsCollection::RunImpl(\n \n   GpuPerformanceModelOwning gpu_performance_model{device_info_, mlir_context_};\n   for (auto* computation : module->MakeComputationPostOrder()) {\n-    TF_CHECK_OK(computation->Accept(&cost_analysis_));\n+    CHECK_OK(computation->Accept(&cost_analysis_));\n \n     for (auto* fusion_instr : computation->instructions()) {\n       if (fusion_instr->opcode() != HloOpcode::kFusion) {"
        },
        {
            "sha": "f6d737b033aa14f4f8f855cbce7032a5b20abee2",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -254,7 +254,7 @@ int64_t GpuPerformanceModelWithIndexingAnalysis::FlopsPerElement(\n   }\n \n   // Encountered unexpected instruction, call into `GpuHloCostAnalysis`.\n-  TF_CHECK_OK(\n+  CHECK_OK(\n       cost_analysis_.RevisitInstruction(const_cast<HloInstruction*>(instr)));\n \n   return cost_analysis_.flop_count(*instr) /"
        },
        {
            "sha": "c1b9edf790b618c2927b34059dd2c86d27aa786c",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -305,7 +305,7 @@ void GpuPerformanceModel::RecordEstimatedRunTime(\n       absl::ToDoubleNanoseconds(data.exec_time) * device_info_.clock_rate_ghz();\n \n   auto gpu_config = instruction->backend_config<GpuBackendConfig>();\n-  TF_CHECK_OK(gpu_config.status()) << instruction->ToString();\n+  CHECK_OK(gpu_config.status()) << instruction->ToString();\n   auto reification_cost = gpu_config->add_reification_cost();\n   reification_cost->set_end_to_end_cycles(cycles);\n   reification_cost->set_compute_time_us(\n@@ -314,7 +314,7 @@ void GpuPerformanceModel::RecordEstimatedRunTime(\n       absl::ToDoubleMicroseconds(data.read_time + data.write_time));\n   reification_cost->set_exec_time_us(\n       absl::ToDoubleMicroseconds(data.exec_time));\n-  TF_CHECK_OK(instruction->set_backend_config(*gpu_config));\n+  CHECK_OK(instruction->set_backend_config(*gpu_config));\n \n   VLOG(8) << \"RecordEstimatedRunTime: \" << instruction->ToString();\n }"
        },
        {
            "sha": "e3a4e9f413578ddadc9dfa3bd27ae8abd81de158",
            "filename": "third_party/xla/xla/service/gpu/model/hlo_op_profiler_run.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiler_run.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiler_run.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiler_run.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -58,9 +58,8 @@ void WriteOutput(const DeviceHloInstructionProfiles& literal,\n     file_name = tsl::io::GetTempFilename(absl::StrCat(name, \".textproto\"));\n   }\n   VLOG(0) << \"Writing output to \" << file_name;\n-  TF_CHECK_OK(\n-      tsl::WriteStringToFile(tsl::Env::Default(), file_name,\n-                             tsl::LegacyUnredactedDebugString(literal)));\n+  CHECK_OK(tsl::WriteStringToFile(tsl::Env::Default(), file_name,\n+                                  tsl::LegacyUnredactedDebugString(literal)));\n }\n \n int RunProfiler(int argc, char** argv) {"
        },
        {
            "sha": "a6b5da3f80d33d965d7da7118e46c2e05053f79e",
            "filename": "third_party/xla/xla/service/gpu/stream_executor_util.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/gpu/repeat_buffer_kernel.h\"\n #include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/kernel_metadata.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -490,8 +491,8 @@ static void InitializeTypedBuffer(se::Stream* stream,\n   // Copy the last part of `host_buffer` to the start of `buf` on the device\n   int64_t first_size =\n       std::min<int64_t>(host_buffer_size - host_index, elements_to_fill);\n-  TF_CHECK_OK(stream->Memcpy(&buffer, host_buffer->data() + host_index,\n-                             first_size * sizeof(T)));\n+  CHECK_OK(stream->Memcpy(&buffer, host_buffer->data() + host_index,\n+                          first_size * sizeof(T)));\n   elements_to_fill -= first_size;\n   if (elements_to_fill == 0) {\n     // Nothing more to do\n@@ -502,7 +503,7 @@ static void InitializeTypedBuffer(se::Stream* stream,\n   CHECK_LE(first_size + second_size, host_buffer_size);\n   se::DeviceMemoryBase mem =\n       buffer.GetByteSlice(first_size * sizeof(T), second_size * sizeof(T));\n-  TF_CHECK_OK(stream->Memcpy(&mem, host_buffer->data(), mem.size()));\n+  CHECK_OK(stream->Memcpy(&mem, host_buffer->data(), mem.size()));\n   elements_to_fill -= second_size;\n   if (elements_to_fill == 0) {\n     // Nothing more to do\n@@ -524,10 +525,10 @@ static void InitializeTypedBuffer(se::Stream* stream,\n   constexpr int threads_per_block = 256;\n   constexpr int blocks_per_grid =\n       (host_buffer_bytes + threads_per_block - 1) / threads_per_block;\n-  TF_CHECK_OK(kernel->Launch(se::ThreadDim(threads_per_block, 1, 1),\n-                             se::BlockDim(blocks_per_grid, 1, 1), stream,\n-                             buffer, host_buffer_bytes,\n-                             static_cast<int64_t>(buffer.size())));\n+  CHECK_OK(kernel->Launch(se::ThreadDim(threads_per_block, 1, 1),\n+                          se::BlockDim(blocks_per_grid, 1, 1), stream, buffer,\n+                          host_buffer_bytes,\n+                          static_cast<int64_t>(buffer.size())));\n }\n \n void InitializeBuffer(se::Stream* stream, PrimitiveType buffer_type,"
        },
        {
            "sha": "279abad0e10b12af89d2284ac30877db014a2bc0",
            "filename": "third_party/xla/xla/service/gpu/tests/command_buffer_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -120,7 +120,7 @@ class CommandBufferTest\n       std::unique_ptr<HloModule> module, bool run_hlo_passes,\n       const std::optional<ErrorSpec>& error) {\n     // Verify module then clone for reference.\n-    TF_CHECK_OK(this->verifier().Run(module.get()).status());\n+    CHECK_OK(this->verifier().Run(module.get()).status());\n     std::unique_ptr<HloModule> reference_module = module->Clone();\n \n     // Prepare fake args for both runners."
        },
        {
            "sha": "3a0b94d13fb44b6d9820de10bf236cd1a90b0ab7",
            "filename": "third_party/xla/xla/service/gpu/tests/dynamic_shared_memory_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_shared_memory_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_shared_memory_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdynamic_shared_memory_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -168,22 +168,22 @@ TEST(SharedMemoryUseTest, ArrayReversalWorks) {\n     }\n   }\n \n-  TF_CHECK_OK(\n+  CHECK_OK(\n       stream->Memcpy(&device_buffer, host_buffer.data(), buffer_size_bytes));\n   se::DeviceMemory<uint32_t> dev_n_cols = executor->AllocateScalar<uint32_t>();\n-  TF_CHECK_OK(stream->Memcpy(&dev_n_cols, &n_cols, sizeof(uint32_t)));\n+  CHECK_OK(stream->Memcpy(&dev_n_cols, &n_cols, sizeof(uint32_t)));\n   se::DeviceMemory<uint32_t> dev_n_rows = executor->AllocateScalar<uint32_t>();\n-  TF_CHECK_OK(stream->Memcpy(&dev_n_rows, &n_rows, sizeof(uint32_t)));\n-  TF_CHECK_OK(stream->BlockHostUntilDone());\n+  CHECK_OK(stream->Memcpy(&dev_n_rows, &n_rows, sizeof(uint32_t)));\n+  CHECK_OK(stream->BlockHostUntilDone());\n \n-  TF_CHECK_OK(ExecuteKernelOnStream(\n+  CHECK_OK(ExecuteKernelOnStream(\n       *kernel, {device_buffer, dev_n_cols, dev_n_rows},\n       {/*block_x_count=*/1, /*thread_x_count_per_block=*/n_cols},\n       /*cluster_dim=*/{}, stream.get()));\n-  TF_CHECK_OK(stream->BlockHostUntilDone());\n-  TF_CHECK_OK(\n+  CHECK_OK(stream->BlockHostUntilDone());\n+  CHECK_OK(\n       stream->Memcpy(host_buffer.data(), device_buffer, buffer_size_bytes));\n-  TF_CHECK_OK(stream->BlockHostUntilDone());\n+  CHECK_OK(stream->BlockHostUntilDone());\n \n   for (int row = 0; row < n_rows; ++row) {\n     for (int col = 0; col < n_cols; ++col) {"
        },
        {
            "sha": "dbd359bdef0e9f8ab230a6729ac95f7ab79cd979",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -41,7 +41,6 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:hlo_creation_utils\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n@@ -441,7 +440,6 @@ cc_library(\n         \"//xla/service:hlo_creation_utils\",\n         \"//xla/service:shape_inference\",\n         \"//xla/service/gpu:cublas_cudnn\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n@@ -486,7 +484,6 @@ cc_library(\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n@@ -589,11 +586,11 @@ cc_library(\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n     ],\n@@ -1208,7 +1205,6 @@ cc_library(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -1761,7 +1757,6 @@ cc_library(\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"//xla/tsl/util:env_var\",\n@@ -1880,7 +1875,6 @@ cc_library(\n         \"//xla/service/gpu/model:gpu_performance_model_base\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n@@ -2003,7 +1997,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/hlo/analysis:hlo_dfs_reachability\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/ir:hlo_instruction_utils\",\n         \"//xla/hlo/pass:hlo_pass\",\n@@ -2018,6 +2011,7 @@ cc_library(\n         \"//xla/service/gpu:gpu_fusible\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/model:fusion_analysis_cache\",\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n         \"//xla/service/gpu/model:gpu_indexing_performance_model\",\n@@ -2028,7 +2022,6 @@ cc_library(\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "6bfc1b708b58eb69f3b6381805103845b711ea56",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -120,7 +120,7 @@ TEST_F(CommandBufferSchedulingTest, SingleCommandBuffer) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -197,7 +197,7 @@ TEST_F(CommandBufferSchedulingTest, MultipleCommandBuffers) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -235,7 +235,7 @@ TEST_F(CommandBufferSchedulingTest, AllReduceStartFollowedByDone) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -269,7 +269,7 @@ TEST_F(CommandBufferSchedulingTest, AllGatherStartFollowedByDone) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -309,7 +309,7 @@ TEST_F(CommandBufferSchedulingTest, ReduceScatterStartFollowedByDone) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -349,7 +349,7 @@ TEST_F(CommandBufferSchedulingTest, AllReduceStartFollowedByBitcast) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -393,7 +393,7 @@ TEST_F(CommandBufferSchedulingTest, AllReduceStartFollowedAllReduceStart) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -457,7 +457,7 @@ TEST_F(CommandBufferSchedulingTest, DoNotCaptureUnmatchedAsyncDone) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -659,7 +659,7 @@ TEST_F(CommandBufferSchedulingTest, ForwardControlDependencies) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -698,7 +698,7 @@ TEST_F(CommandBufferSchedulingTest, ForwardControlDependenciesToParams) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -776,7 +776,7 @@ TEST_F(CommandBufferSchedulingTest, WhileNotCommand) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -841,7 +841,7 @@ TEST_F(CommandBufferSchedulingTest, While) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -920,7 +920,7 @@ TEST_F(CommandBufferSchedulingTest, Conditional) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -972,7 +972,7 @@ ENTRY e {\n   RunAndFilecheckHloRewrite(kHloText, CommandBufferScheduling(device_desc()),\n                             kExpected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -1003,7 +1003,7 @@ TEST_F(CommandBufferSchedulingTest, AsyncCustomCall) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -1047,7 +1047,7 @@ TEST_F(CommandBufferSchedulingTest, AsyncFusion) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -1076,7 +1076,7 @@ TEST_F(CommandBufferSchedulingTest, AsyncAlltoAll) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -1344,7 +1344,7 @@ TEST_F(CommandBufferSchedulingTest, MoveGTEs) {\n   RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n                             expected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n \n@@ -1378,7 +1378,7 @@ ENTRY e {\n   RunAndFilecheckHloRewrite(kHloText, CommandBufferScheduling(device_desc()),\n                             kExpected, [](HloModule* module) {\n                               EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n+                              CHECK_OK(module->schedule().Verify());\n                             });\n }\n "
        },
        {
            "sha": "dddd0c6a6d105e54a029a61ce3ea64d22f543398",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_padding_legalization.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_padding_legalization.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -35,7 +35,6 @@ limitations under the License.\n #include \"xla/service/shape_inference.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/window_util.h\"\n@@ -213,7 +212,7 @@ bool ConvPaddingLegalization::CanonicalizeForwardConvolution(\n   new_conv->set_window(new_conv_window);\n   VLOG(1) << \"Replacing:\\n  \" << conv->ToString() << \"\\nwith:\\n  \"\n           << new_conv->ToString();\n-  TF_CHECK_OK(conv->parent()->ReplaceInstruction(conv, new_conv));\n+  CHECK_OK(conv->parent()->ReplaceInstruction(conv, new_conv));\n   return true;\n }\n \n@@ -295,8 +294,7 @@ bool ConvPaddingLegalization::CanonicalizeBackwardFilterConvolution(\n   VLOG(1) << \"Replacing:\\n  \" << backward_conv->ToString() << \"\\nwith:\\n  \"\n           << new_backward_conv->ToString();\n \n-  TF_CHECK_OK(\n-      computation->ReplaceInstruction(backward_conv, new_backward_conv));\n+  CHECK_OK(computation->ReplaceInstruction(backward_conv, new_backward_conv));\n   return true;\n }\n \n@@ -422,7 +420,7 @@ bool ConvPaddingLegalization::CanonicalizeBackwardInputConvolution(\n   VLOG(1) << \"Replacing:\\n  \" << backward_conv->ToString() << \"\\nwith:\\n  \"\n           << new_tuple->ToString();\n \n-  TF_CHECK_OK(computation->ReplaceInstruction(backward_conv, new_tuple));\n+  CHECK_OK(computation->ReplaceInstruction(backward_conv, new_tuple));\n   return true;\n }\n "
        },
        {
            "sha": "47c986922fada800994be74480d91bc12ec5b3c9",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_rewriter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include <string>\n #include <tuple>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -46,7 +45,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/window_util.h\"\n@@ -629,7 +627,7 @@ ConvolutionMatch MatchBackwardInput(HloInstruction* conv) {\n     reverse_filter = c->AddInstruction(\n         HloInstruction::CreateReverse(reverse_filter->shape(), reverse_filter,\n                                       dnums.kernel_spatial_dimensions()));\n-    TF_CHECK_OK(conv->ReplaceOperandWith(/*operand_num=*/1, reverse_filter));\n+    CHECK_OK(conv->ReplaceOperandWith(/*operand_num=*/1, reverse_filter));\n   }\n \n   // Calculate the 'rhs' that goes into the backward input convolution."
        },
        {
            "sha": "e08a0a788843c6ea791811d005bbb13dedd6f254",
            "filename": "third_party/xla/xla/service/gpu/transforms/cublas_pad_for_gemms.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_pad_for_gemms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_pad_for_gemms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_pad_for_gemms.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n@@ -32,7 +33,6 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/gemm_fusion.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -121,7 +121,7 @@ static absl::StatusOr<bool> PadForGemm(HloDotInstruction* dot,\n \n   bool is_root = dot->user_count() == 0;\n \n-  TF_CHECK_OK(parent->ReplaceInstruction(dot, slice));\n+  CHECK_OK(parent->ReplaceInstruction(dot, slice));\n \n   if (is_root) {\n     parent->set_root_instruction(slice);"
        },
        {
            "sha": "5cc56611ae4181da7279089744d2aa197b750149",
            "filename": "third_party/xla/xla/service/gpu/transforms/dot_algorithm_rewriter.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_algorithm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_algorithm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_algorithm_rewriter.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -36,7 +36,6 @@ limitations under the License.\n #include \"xla/service/hlo_creation_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -186,8 +185,8 @@ void RewriteF32ToBF16X3(HloInstruction* instr) {\n   HloInstruction* low_sum = sum(low_high_dot, high_low_dot);\n   low_sum = ReplaceNaNWithZeros(low_sum);\n   HloInstruction* result = sum(low_sum, high_high_dot);\n-  TF_CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n-  TF_CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n+  CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n+  CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n }\n \n void RewriteF32ToBF16X6(HloInstruction* instr) {\n@@ -226,8 +225,8 @@ void RewriteF32ToBF16X6(HloInstruction* instr) {\n   result = ReplaceNaNWithZeros(result);\n   result = sum(result, high_high_dot);\n \n-  TF_CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n-  TF_CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n+  CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n+  CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n }\n \n void RewriteF32ToBF16X9(HloInstruction* instr) {\n@@ -272,8 +271,8 @@ void RewriteF32ToBF16X9(HloInstruction* instr) {\n   result = ReplaceNaNWithZeros(result);\n   result = sum(result, high_high_dot);\n \n-  TF_CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n-  TF_CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n+  CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n+  CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n }\n \n void RewriteF32ToTF32X3(HloInstruction* instr) {\n@@ -303,8 +302,8 @@ void RewriteF32ToTF32X3(HloInstruction* instr) {\n   HloInstruction* low_sum = sum(low_high_dot, high_low_dot);\n   low_sum = ReplaceNaNWithZeros(low_sum);\n   HloInstruction* result = sum(low_sum, high_high_dot);\n-  TF_CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n-  TF_CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n+  CHECK_OK(original_dot->ReplaceAllUsesWith(result));\n+  CHECK_OK(original_dot->parent()->RemoveInstruction(original_dot));\n }\n \n }  // namespace"
        },
        {
            "sha": "aa5c0c396ef34ec567685194f7b2a60ea57e6c3d",
            "filename": "third_party/xla/xla/service/gpu/transforms/double_buffer_loop_unrolling.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -44,7 +44,6 @@ limitations under the License.\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -407,7 +406,7 @@ absl::Status PeelInstructionsForOddTripCount(HloModule* module,\n             old_instr->shape(), new_operands, suffix));\n \n     SetChannelIdForNewCollective(new_instr, module);\n-    TF_CHECK_OK(SetSendRecvValidationForPeeledInstr(new_instr, old_instr));\n+    CHECK_OK(SetSendRecvValidationForPeeledInstr(new_instr, old_instr));\n     old_to_new_map[old_instr] = new_instr;\n     VLOG(2) << \"Added instruction \" << new_instr->ToString()\n             << \" to parent computation.\";\n@@ -499,8 +498,8 @@ absl::StatusOr<bool> DoubleBufferingUnroll(HloInstruction* while_instr,\n       skip_control_dep_injection.insert(old_instr);\n     }\n     SetChannelIdForNewCollective(new_instr, module);\n-    TF_CHECK_OK(SetSendRecvValidation(old_instr, new_instr,\n-                                      /*is_peeled=*/peel_one_iteration));\n+    CHECK_OK(SetSendRecvValidation(old_instr, new_instr,\n+                                   /*is_peeled=*/peel_one_iteration));\n     old_to_new_map[old_instr] = new_instr;\n     VLOG(2) << \"Added instruction \" << new_instr->ToString();\n   }"
        },
        {
            "sha": "5343b67cb0a7f4f5c00eb1e97ed3372a98927e2c",
            "filename": "third_party/xla/xla/service/gpu/transforms/dynamic_slice_fusion_rewriter_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdynamic_slice_fusion_rewriter_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -1005,7 +1005,7 @@ TEST_F(DynamicSliceFusionRewriterTest, SimpleCustomCallLegacy) {\n   //     ScheduleModule(hlo.get(), [](const BufferValue& buffer) {\n   //       return ShapeUtil::ByteSizeOf(buffer.shape(), /*pointer_size=*/8);\n   //     }));\n-  // TF_CHECK_OK(hlo->set_schedule(std::move(schedule)));\n+  // CHECK_OK(hlo->set_schedule(std::move(schedule)));\n \n   const char* expected = R\"(\n     ; CHECK:     %dynamic-slice-fusion{{.*}} {\n@@ -1067,7 +1067,7 @@ TEST_F(DynamicSliceFusionRewriterTest, TupleSliceCustomCallLegacy) {\n   //     ScheduleModule(hlo.get(), [](const BufferValue& buffer) {\n   //       return ShapeUtil::ByteSizeOf(buffer.shape(), /*pointer_size=*/8);\n   //     }));\n-  // TF_CHECK_OK(hlo->set_schedule(std::move(schedule)));\n+  // CHECK_OK(hlo->set_schedule(std::move(schedule)));\n \n   const char* expected = R\"(\n     ; CHECK:     %dynamic-slice-fusion{{.*}} {\n@@ -1141,7 +1141,7 @@ TEST_F(DynamicSliceFusionRewriterTest, TupledOutputCustomCallLegacy) {\n   //     ScheduleModule(hlo.get(), [](const BufferValue& buffer) {\n   //       return ShapeUtil::ByteSizeOf(buffer.shape(), /*pointer_size=*/8);\n   //     }));\n-  // TF_CHECK_OK(hlo->set_schedule(std::move(schedule)));\n+  // CHECK_OK(hlo->set_schedule(std::move(schedule)));\n \n   const char* expected = R\"(\n     ; CHECK:     %dynamic-slice-fusion{{.*}} {\n@@ -1204,7 +1204,7 @@ TEST_F(DynamicSliceFusionRewriterTest, UnalignedSlice) {\n   //     ScheduleModule(hlo.get(), [](const BufferValue& buffer) {\n   //       return ShapeUtil::ByteSizeOf(buffer.shape(), /*pointer_size=*/8);\n   //     }));\n-  // TF_CHECK_OK(hlo->set_schedule(std::move(schedule)));\n+  // CHECK_OK(hlo->set_schedule(std::move(schedule)));\n \n   auto device = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   RunAndFilecheckHloRewrite(hlo->ToString(), DynamicSliceFusionRewriter(\"gpu\"),"
        },
        {
            "sha": "93e8b37be9a8c5a421233b1ff2cb54e7e2f5f817",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include <string>\n #include <tuple>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -58,7 +57,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/protobuf/dnn.pb.h\"\n #include \"xla/tsl/util/env_var.h\"\n@@ -181,8 +179,8 @@ HeuristicLayoutAssignment(const HloInstruction* instr,\n     // If we do not have NHWC layout support or not fp16/bfloat16, or not\n     // conv2D, or ROCm NHWC is disabled the decision is to use NCHW.\n     bool is_enabled = false;\n-    TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_USE_ROCM_NHWC\",\n-                                        /*default_val=*/false, &is_enabled));\n+    CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_USE_ROCM_NHWC\",\n+                                     /*default_val=*/false, &is_enabled));\n     if (!isFloat16 || (!rocm_compute_capability->has_nhwc_layout_support()) ||\n         instr->shape().tuple_shapes(0).dimensions().size() != 4 ||\n         !is_enabled) {"
        },
        {
            "sha": "a2a2142148f44fc53bf6511a0cc653bb5b52e7c9",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -43,7 +43,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -387,8 +386,8 @@ bool MultiOutputFusion::FuseSiblings(HloInstruction* parent,\n       fusion_info_cache->Invalidate(*j);\n       HloInstruction* remaining = *i;\n       HloInstruction* fused = *j;\n-      TF_CHECK_OK(cost_analysis->RemoveInstruction(remaining));\n-      TF_CHECK_OK(cost_analysis->RemoveInstruction(fused));\n+      CHECK_OK(cost_analysis->RemoveInstruction(remaining));\n+      CHECK_OK(cost_analysis->RemoveInstruction(fused));\n \n       DumpFusionState(*remaining,\n                       absl::StrCat(\"About to fuse sibling |\", fused->name(),\n@@ -404,12 +403,12 @@ bool MultiOutputFusion::FuseSiblings(HloInstruction* parent,\n       } else {\n         remaining->FuseInstructionIntoMultiOutput(fused);\n         CHECK_EQ(0, fused->user_count());\n-        TF_CHECK_OK(computation_->RemoveInstruction(fused));\n+        CHECK_OK(computation_->RemoveInstruction(fused));\n       }\n       DumpFusionState(*remaining,\n                       absl::StrCat(\"Fused into |\", remaining->name(),\n                                    \"| inside multi-output fusion\"));\n-      TF_CHECK_OK(cost_analysis->RevisitInstruction(remaining));\n+      CHECK_OK(cost_analysis->RevisitInstruction(remaining));\n       changed = true;\n       siblings.erase(j);\n       RecomputeReachability();\n@@ -485,7 +484,7 @@ absl::StatusOr<bool> MultiOutputFusion::DoMultiOutputFusion() {\n       VLOG(2) << \"Fuse producer \" << producer->name() << \" and its consumer \"\n               << consumer_for_fusion->name() << \" into \"\n               << input_fusion->name();\n-      TF_CHECK_OK(\n+      CHECK_OK(\n           computation_->ReplaceInstruction(consumer_for_fusion, input_fusion));\n     }\n \n@@ -500,7 +499,7 @@ absl::StatusOr<bool> MultiOutputFusion::DoMultiOutputFusion() {\n     } else {\n       input_fusion->FuseInstructionIntoMultiOutput(producer);\n       CHECK_EQ(0, producer->user_count());\n-      TF_CHECK_OK(computation_->RemoveInstruction(producer));\n+      CHECK_OK(computation_->RemoveInstruction(producer));\n     }\n     TF_RETURN_IF_ERROR(cost_analysis.RevisitInstruction(input_fusion));\n "
        },
        {
            "sha": "e5669c1fa4440f0084c9df089c5b99e62b518ac7",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -40,10 +40,10 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/analysis/hlo_dfs_reachability.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instruction_utils.h\"\n@@ -58,6 +58,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_indexing_performance_model.h\"\n@@ -71,7 +72,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -179,7 +179,7 @@ class PriorityFusionQueue {\n         reachability_(HloDfsReachability::Build(computation)),\n         triton_heroless_fusion_enabled_(triton_heroless_fusion_enabled) {\n     VLOG(2) << \"Running full HLO cost analysis for \" << computation_->name();\n-    TF_CHECK_OK(computation_->Accept(&cost_analysis_));\n+    CHECK_OK(computation_->Accept(&cost_analysis_));\n \n     dump_fusion_visualization_ = computation->parent()\n                                      ->config()\n@@ -189,7 +189,7 @@ class PriorityFusionQueue {\n     // Initializes the priority queue.\n     std::vector<HloInstruction*> instructions;\n     for (auto* instruction : computation->MakeInstructionPostOrder()) {\n-      TF_CHECK_OK(UpdatePerformanceModelCache(instruction));\n+      CHECK_OK(UpdatePerformanceModelCache(instruction));\n       if (HloPredicateIsOp<HloOpcode::kParameter>(instruction) ||\n           instruction->user_count() == 0 || !instruction->IsFusible() ||\n           HloPredicateIsOp<HloOpcode::kTuple, HloOpcode::kGetTupleElement>(\n@@ -1317,7 +1317,7 @@ HloInstruction* PriorityFusion::Fuse(HloInstruction* producer,\n   if (HloPredicateIsNotOp<HloOpcode::kFusion>(fusion_instruction)) {\n     fusion_instruction = computation->AddInstruction(\n         HloInstruction::CreateFusion(consumer->shape(), kind, consumer));\n-    TF_CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n+    CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n   } else if (kind != fusion_instruction->fusion_kind()) {\n     fusion_instruction->set_fusion_kind(kind);\n   }\n@@ -1338,7 +1338,7 @@ HloInstruction* PriorityFusion::Fuse(HloInstruction* producer,\n       // the computation. Do the same here, so that we have the invariant that\n       // the producer has been cleaned up when multi-output fusion is used.\n       CHECK_EQ(0, producer->user_count());\n-      TF_CHECK_OK(producer->parent()->RemoveInstruction(producer));\n+      CHECK_OK(producer->parent()->RemoveInstruction(producer));\n     } else {\n       fusion_instruction->FuseInstruction(producer);\n     }"
        },
        {
            "sha": "82485966cac6a26f17a0464f565180584654000e",
            "filename": "third_party/xla/xla/service/hlo_computation_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_computation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_computation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_computation_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -24,13 +24,15 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/comparison_util.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n@@ -44,8 +46,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n \n@@ -588,7 +588,7 @@ TEST_F(HloComputationTest, CloneWithControlDependency) {\n   auto computation =\n       module->AddEntryComputation(builder.Build(/*root_instruction=*/add));\n \n-  TF_CHECK_OK(negate->AddControlDependencyTo(add));\n+  CHECK_OK(negate->AddControlDependencyTo(add));\n \n   auto clone = computation->Clone();\n "
        },
        {
            "sha": "dbb91230a98d956432250b5a006f49ee67b84a6e",
            "filename": "third_party/xla/xla/service/hlo_cost_analysis_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_cost_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_cost_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_cost_analysis_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -44,7 +45,6 @@ limitations under the License.\n #include \"xla/service/service.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -71,7 +71,7 @@ class HloCostAnalysisTest : public ::testing::Test {\n       auto half = ConstantR0<float>(&builder, 0.5);\n       Exp(Add(x, half));\n       auto computation_status = builder.Build();\n-      TF_CHECK_OK(computation_status.status());\n+      CHECK_OK(computation_status.status());\n       add_and_exp_ = std::move(computation_status).value();\n     }\n \n@@ -82,7 +82,7 @@ class HloCostAnalysisTest : public ::testing::Test {\n       auto y = Parameter(&builder, 1, ShapeUtil::MakeShape(F32, {}), \"y\");\n       Add(x, y);\n       auto computation_status = builder.Build();\n-      TF_CHECK_OK(computation_status.status());\n+      CHECK_OK(computation_status.status());\n       add_ = std::move(computation_status).value();\n     }\n \n@@ -93,7 +93,7 @@ class HloCostAnalysisTest : public ::testing::Test {\n       auto one = ConstantR0<float>(&builder, 1.0);\n       Div(one, Add(one, Exp(Neg(x))));\n       auto computation_status = builder.Build();\n-      TF_CHECK_OK(computation_status.status());\n+      CHECK_OK(computation_status.status());\n       sigmoid_ = std::move(computation_status).value();\n     }\n \n@@ -104,7 +104,7 @@ class HloCostAnalysisTest : public ::testing::Test {\n       auto y = Parameter(&builder, 1, ShapeUtil::MakeShape(F32, {}), \"y\");\n       Max(x, y);\n       auto computation_status = builder.Build();\n-      TF_CHECK_OK(computation_status.status());\n+      CHECK_OK(computation_status.status());\n       max_ = std::move(computation_status).value();\n     }\n \n@@ -115,15 +115,15 @@ class HloCostAnalysisTest : public ::testing::Test {\n       auto y = Parameter(&builder, 1, ShapeUtil::MakeShape(F32, {}), \"y\");\n       Gt(x, y);\n       auto computation_status = builder.Build();\n-      TF_CHECK_OK(computation_status.status());\n+      CHECK_OK(computation_status.status());\n       gt_ = std::move(computation_status).value();\n     }\n   }\n \n   // Build HLO graph from the given builder and return the HLO module.\n   std::unique_ptr<HloModule> BuildHloGraph(XlaBuilder* builder) {\n     auto computation_status = builder->Build();\n-    TF_CHECK_OK(computation_status.status());\n+    CHECK_OK(computation_status.status());\n     auto computation = std::move(computation_status).value();\n     auto config = HloModule::CreateModuleConfigFromProto(computation.proto(),\n                                                          DebugOptions())\n@@ -1632,7 +1632,7 @@ TEST_F(HloCostAnalysisTest, MultioutputScatter) {\n     auto y1 = Parameter(&builder, 3, ShapeUtil::MakeShape(S32, {}), \"y1\");\n     Tuple(&builder, {Add(x0, y0), Add(x1, y1)});\n     auto computation_status = builder.Build();\n-    TF_CHECK_OK(computation_status.status());\n+    CHECK_OK(computation_status.status());\n     return std::move(computation_status).value();\n   }();\n   Scatter({operand0, operand1}, indices, {values0, values1}, add, dim_numbers);"
        },
        {
            "sha": "aeb45642bbd3085db248d85b3caca8c4086616da",
            "filename": "third_party/xla/xla/service/hlo_cse.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_cse.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_cse.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_cse.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -40,7 +40,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -116,8 +115,8 @@ absl::StatusOr<bool> CombineConstants(\n \n     if (match != nullptr) {\n       // Match found, replace this instruction with the one in the set.\n-      TF_CHECK_OK(instruction->ReplaceAllUsesWith(match));\n-      TF_CHECK_OK(computation->RemoveInstruction(instruction));\n+      CHECK_OK(instruction->ReplaceAllUsesWith(match));\n+      CHECK_OK(computation->RemoveInstruction(instruction));\n       ++combined;\n     }\n   }"
        },
        {
            "sha": "e45d69b4127f6e45e9f4885a1d349bb1a41fcb93",
            "filename": "third_party/xla/xla/service/hlo_runner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/backend.h\"\n@@ -50,7 +49,6 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n@@ -59,7 +57,6 @@ limitations under the License.\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n \n@@ -564,7 +561,7 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicatedImpl(\n         VLOG(1) << \"Starting infeed on device \" << device;\n         for (int64_t step = 1;\n              options.infeed_steps < 0 || step <= options.infeed_steps; ++step) {\n-          TF_CHECK_OK(backend().transfer_manager()->TransferLiteralToInfeed(\n+          CHECK_OK(backend().transfer_manager()->TransferLiteralToInfeed(\n               executor, *options.infeed_values[i]));\n           if (step % 100 == 0) {\n             VLOG(1) << \"Infeed step \" << step;\n@@ -587,7 +584,7 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicatedImpl(\n         for (int64_t step = 1;\n              options.infeed_steps < 0 || step <= options.infeed_steps; ++step) {\n           Literal literal(options.outfeed_shape);\n-          TF_CHECK_OK(backend().transfer_manager()->TransferLiteralFromOutfeed(\n+          CHECK_OK(backend().transfer_manager()->TransferLiteralFromOutfeed(\n               executor, &literal));\n           if (options.outfeed_values) {\n             options.outfeed_values->at(i) = std::move(literal);"
        },
        {
            "sha": "1d9b568f88f9f0c7556d8c0105b42f544d6787aa",
            "filename": "third_party/xla/xla/service/instruction_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n@@ -53,9 +54,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/util.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/status.h\"\n \n namespace xla {\n \n@@ -800,7 +798,7 @@ HloInstruction* InstructionFusion::AddFusionInstruction(\n     // have the same value as the root of the fused computation. However, we\n     // copy the value nontheless to simplify some use cases that involve\n     // fusions.\n-    TF_CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n+    CHECK_OK(computation->ReplaceInstruction(consumer, fusion_instruction));\n   }\n   return fusion_instruction;\n }"
        },
        {
            "sha": "163f4499b46930cddbbe5b787b4febb992e1647f",
            "filename": "third_party/xla/xla/service/latency_hiding_scheduler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -66,7 +66,6 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n@@ -2972,7 +2971,7 @@ void HloScheduleGraph::AnnotateGraph(\n     for (const HloInstruction* instr :\n          annotation_tracker->GetInstructions(comp, annotation)) {\n       HloGraphNode& node = GetNode(instr);\n-      TF_CHECK_OK(node.SetAnnotation(annotation));\n+      CHECK_OK(node.SetAnnotation(annotation));\n     }\n   }\n }"
        },
        {
            "sha": "8c8de371fa20183c56348e260e011c2a6d143b95",
            "filename": "third_party/xla/xla/service/layout_assignment.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -58,7 +58,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -1412,7 +1411,7 @@ std::unique_ptr<Layout> LayoutAssignment::ChooseOperandLayoutFromOutputLayout(\n         ShapeUtil::AlignLayouts(output_shape_with_layout, operand_shape);\n     if (aligned_operand_shape) {\n       auto operand_layout = aligned_operand_shape.value().layout();\n-      TF_CHECK_OK(\n+      CHECK_OK(\n           LayoutUtil::ValidateLayoutForShape(operand_layout, operand_shape));\n       return std::make_unique<Layout>(operand_layout);\n     }\n@@ -1428,7 +1427,7 @@ std::unique_ptr<Layout> LayoutAssignment::ChooseOperandLayoutFromOutputLayout(\n       new_minor_to_major[i] = operand_dim;\n     }\n     Layout operand_layout = LayoutUtil::MakeLayout(new_minor_to_major);\n-    TF_CHECK_OK(\n+    CHECK_OK(\n         LayoutUtil::ValidateLayoutForShape(operand_layout, operand->shape()));\n     return std::make_unique<Layout>(operand_layout);\n   }\n@@ -1458,7 +1457,7 @@ std::unique_ptr<Layout> LayoutAssignment::ChooseOperandLayoutFromOutputLayout(\n       new_minor_to_major.push_back(output_to_operand_mapping[output_dim]);\n     }\n     Layout operand_layout = LayoutUtil::MakeLayout(new_minor_to_major);\n-    TF_CHECK_OK(\n+    CHECK_OK(\n         LayoutUtil::ValidateLayoutForShape(operand_layout, operand->shape()));\n     return std::make_unique<Layout>(operand_layout);\n   }\n@@ -1551,8 +1550,7 @@ std::unique_ptr<Layout> LayoutAssignment::ChooseOutputLayoutFromOperandLayout(\n         ShapeUtil::AlignLayouts(operand_shape_with_layout, output_shape);\n     if (aligned_user_shape) {\n       auto user_layout = aligned_user_shape.value().layout();\n-      TF_CHECK_OK(\n-          LayoutUtil::ValidateLayoutForShape(user_layout, output_shape));\n+      CHECK_OK(LayoutUtil::ValidateLayoutForShape(user_layout, output_shape));\n       return std::make_unique<Layout>(user_layout);\n     }\n   }\n@@ -1568,7 +1566,7 @@ std::unique_ptr<Layout> LayoutAssignment::ChooseOutputLayoutFromOperandLayout(\n       new_minor_to_major[i] = user_dim;\n     }\n     Layout user_layout = LayoutUtil::MakeLayout(new_minor_to_major);\n-    TF_CHECK_OK(LayoutUtil::ValidateLayoutForShape(user_layout, user->shape()));\n+    CHECK_OK(LayoutUtil::ValidateLayoutForShape(user_layout, user->shape()));\n     return std::make_unique<Layout>(user_layout);\n   }\n "
        },
        {
            "sha": "d3bc72af548f790808440b6060ba680959f3a43b",
            "filename": "third_party/xla/xla/service/layout_assignment_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -47,11 +47,10 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace {\n@@ -310,7 +309,7 @@ TEST_F(LayoutAssignmentTest, ConflictingLayoutTuple) {\n       ShapeUtil::MakeShapeWithDenseLayout(F32, {2, 2}, {1, 0});\n   *ShapeUtil::GetMutableSubshape(&result_shape, /*index=*/{1, 0}) =\n       ShapeUtil::MakeShapeWithDenseLayout(F32, {2, 2}, {0, 1});\n-  TF_CHECK_OK(computation_layout.mutable_result_layout()->CopyLayoutFromShape(\n+  CHECK_OK(computation_layout.mutable_result_layout()->CopyLayoutFromShape(\n       result_shape));\n \n   LayoutAssignment layout_assignment(&computation_layout);\n@@ -2092,12 +2091,11 @@ ENTRY main {\n       std::unique_ptr<HloModule> m,\n       ParseAndReturnUnverifiedModule(\n           module_str, {}, HloParserOptions().set_fill_missing_layouts(false)));\n-  TF_CHECK_OK(backend()\n-                  .compiler()\n-                  ->RunHloPasses(m->Clone(),\n-                                 backend().default_stream_executor(),\n-                                 /*device_allocator=*/nullptr)\n-                  .status());\n+  CHECK_OK(backend()\n+               .compiler()\n+               ->RunHloPasses(m->Clone(), backend().default_stream_executor(),\n+                              /*device_allocator=*/nullptr)\n+               .status());\n }\n \n TEST_F(LayoutAssignmentTest, HloBufferLayoutUnconstrained) {"
        },
        {
            "sha": "6669dc42b4708172cee946e43eb7670b06a11d88",
            "filename": "third_party/xla/xla/service/layout_normalization_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 23,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flayout_normalization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Flayout_normalization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flayout_normalization_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -18,12 +18,13 @@ limitations under the License.\n #include <functional>\n #include <optional>\n \n+#include <gtest/gtest.h>\n+#include \"absl/log/check.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/scatter_simplifier.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/test.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace {\n@@ -875,16 +876,15 @@ ENTRY main.17 {\n }\n )\";\n \n-  CheckLayoutNormalization(\n-      hlo, R\"(\n+  CheckLayoutNormalization(hlo, R\"(\n // CHECK: scatter({{.*}}),\n // CHECK-SAME: update_window_dims={2,0}, inserted_window_dims={0,1,3}, scatter_dims_to_operand_dims={2,4}, index_vector_dim=1, to_apply=%region_0.10\n )\",\n-      // Run the ScatterSimplifier afterwards, otherwise the verifier will\n-      // complain!\n-      [](HloModule* module) {\n-        TF_CHECK_OK(ScatterSimplifier().Run(module).status());\n-      });\n+                           // Run the ScatterSimplifier afterwards, otherwise\n+                           // the verifier will complain!\n+                           [](HloModule* module) {\n+                             CHECK_OK(ScatterSimplifier().Run(module).status());\n+                           });\n }\n \n TEST_F(LayoutNormalizationTest, SimplifiedScatter) {\n@@ -905,16 +905,15 @@ ENTRY main.17 {\n }\n )\";\n \n-  CheckLayoutNormalization(\n-      hlo, R\"(\n+  CheckLayoutNormalization(hlo, R\"(\n // CHECK: scatter({{.*}}),\n // CHECK-SAME: update_window_dims={4,0,1,2,5}, inserted_window_dims={}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=1, to_apply=%region_0.10\n )\",\n-      // Run the ScatterSimplifier afterwards, otherwise the verifier will\n-      // complain!\n-      [](HloModule* module) {\n-        TF_CHECK_OK(ScatterSimplifier().Run(module).status());\n-      });\n+                           // Run the ScatterSimplifier afterwards, otherwise\n+                           // the verifier will complain!\n+                           [](HloModule* module) {\n+                             CHECK_OK(ScatterSimplifier().Run(module).status());\n+                           });\n }\n \n TEST_F(LayoutNormalizationTest, VariadicScatter) {\n@@ -941,16 +940,15 @@ ENTRY main.17 {\n }\n )\";\n \n-  CheckLayoutNormalization(\n-      hlo, R\"(\n+  CheckLayoutNormalization(hlo, R\"(\n // CHECK: scatter({{.*}}),\n // CHECK-SAME: update_window_dims={4,0,1,2,5}, inserted_window_dims={}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=1, to_apply=%region_0.10\n )\",\n-      // Run the ScatterSimplifier afterwards, otherwise the verifier will\n-      // complain!\n-      [](HloModule* module) {\n-        TF_CHECK_OK(ScatterSimplifier().Run(module).status());\n-      });\n+                           // Run the ScatterSimplifier afterwards, otherwise\n+                           // the verifier will complain!\n+                           [](HloModule* module) {\n+                             CHECK_OK(ScatterSimplifier().Run(module).status());\n+                           });\n }\n \n TEST_F(LayoutNormalizationTest, CompareInt4) {"
        },
        {
            "sha": "06d0fe7da59eabeab5e5fe1b48d3d29152728b48",
            "filename": "third_party/xla/xla/service/llvm_ir/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -127,7 +127,6 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n@@ -282,7 +281,6 @@ cc_library(\n         \":llvm_util\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "e8b23a4f4f2ca280c87a6723c3e40088aa3a1998",
            "filename": "third_party/xla/xla/service/llvm_ir/ir_array.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fir_array.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fir_array.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fir_array.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -43,7 +43,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -199,7 +198,7 @@ IrArray::IrArray(llvm::Value* base_ptr, llvm::Type* pointee_type, Shape shape)\n     : base_ptr_(base_ptr),\n       pointee_type_(pointee_type),\n       shape_(std::move(shape)) {\n-  TF_CHECK_OK(ShapeUtil::ValidateShape(shape_));\n+  CHECK_OK(ShapeUtil::ValidateShape(shape_));\n   CHECK(base_ptr_->getType()->isPointerTy());\n   int depth = 0;\n   element_type_ = pointee_type;"
        },
        {
            "sha": "83607d8c40e51a035dd7a6b011f03991023b87f2",
            "filename": "third_party/xla/xla/service/llvm_ir/kernel_support_library.h",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fkernel_support_library.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fkernel_support_library.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_ir%2Fkernel_support_library.h?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -18,20 +18,16 @@ limitations under the License.\n \n #include <cstdint>\n #include <functional>\n-#include <string>\n \n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"llvm/IR/BasicBlock.h\"\n #include \"llvm/IR/Constants.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Value.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/llvm_ir/llvm_loop.h\"\n-#include \"xla/service/llvm_ir/llvm_util.h\"\n-#include \"xla/tsl/platform/status.h\"\n \n namespace xla {\n // A thin wrapper around llvm_loop.h to make code generating structured control\n@@ -184,7 +180,7 @@ class KernelSupportLibrary {\n           const std::function<void()>& true_block_generator,\n           const std::function<void()>& false_block_generator = nullptr) {\n     if (false_block_generator != nullptr) {\n-      TF_CHECK_OK(IfWithStatus(\n+      CHECK_OK(IfWithStatus(\n           name, condition,\n           [&]() {\n             true_block_generator();\n@@ -195,7 +191,7 @@ class KernelSupportLibrary {\n             return absl::OkStatus();\n           }));\n     } else {\n-      TF_CHECK_OK(IfWithStatus(name, condition, [&]() {\n+      CHECK_OK(IfWithStatus(name, condition, [&]() {\n         true_block_generator();\n         return absl::OkStatus();\n       }));"
        },
        {
            "sha": "4a4f3c46ee2682ed558816e9bb767cfd04aad11d",
            "filename": "third_party/xla/xla/service/memory_space_assignment/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -63,7 +63,6 @@ cc_library(\n         \"//xla/service:hlo_value\",\n         \"//xla/service/heap_simulator\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -126,7 +125,6 @@ xla_test(\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -171,8 +169,6 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:status\",\n     ],\n )\n \n@@ -200,10 +196,10 @@ cc_library(\n         \"//xla/service/cost_modelling:op_cost\",\n         \"//xla/tests:hlo_pjrt_test_base\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_for_library\","
        },
        {
            "sha": "8420562281df43fa7882e5f566f1d869b0047ace",
            "filename": "third_party/xla/xla/service/memory_space_assignment/best_fit_repacker.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fbest_fit_repacker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fbest_fit_repacker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fbest_fit_repacker.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -136,8 +136,6 @@ Step 5: Update AllocationBlocks with the repacking placements\n #include \"xla/comparison_util.h\"\n #include \"xla/service/heap_simulator/allocation_block.h\"\n #include \"xla/service/heap_simulator/heap_simulator.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/status.h\"\n \n namespace xla {\n namespace {\n@@ -578,7 +576,7 @@ class BestFitRepacker\n   }\n \n   bool Repack() {\n-    TF_CHECK_OK(Finish().status());\n+    CHECK_OK(Finish().status());\n     bool success = result_.heap_size <= max_size_;\n     if (!success) {\n       VLOG(1) << \"Repacking unsuccessful with heap size \" << result_.heap_size;"
        },
        {
            "sha": "1fd08fc9c60b7c4445e601a574c1d675ea3fe67d",
            "filename": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -61,7 +61,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -428,7 +427,7 @@ MemorySpaceAssignment::RunMemorySpaceAssignment(\n     LOG(INFO) << \"Module after memory space assignment: \";\n     XLA_LOG_LINES(INFO, module_->ToString());\n   }\n-  TF_CHECK_OK(module_->schedule().Verify());\n+  CHECK_OK(module_->schedule().Verify());\n   if (VLOG_IS_ON(1)) {\n     TF_ASSIGN_OR_RETURN(AsyncCopyStats stats,\n                         CalculateAsyncCopyStats(alias->dataflow_analysis()));"
        },
        {
            "sha": "b60ef8872a01e7ef0949bdf63bd05eff2dfe6315",
            "filename": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment_test.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 56,
            "changes": 111,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -85,7 +85,6 @@ limitations under the License.\n #include \"xla/tests/test_utils.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -179,7 +178,7 @@ TEST_F(MemorySpaceAssignmentTest, ParameterOnly) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -208,7 +207,7 @@ TEST_F(MemorySpaceAssignmentTest, Simple) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, add, sub, mul});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n   options.post_module_scoped_alternate_memory_size_in_bytes = 10;\n@@ -352,7 +351,7 @@ TEST_F(MemorySpaceAssignmentTest, NegateChain) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -1873,7 +1872,7 @@ TEST_F(MemorySpaceAssignmentTest, FilterUpdatePreferredPrefetchTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n \n@@ -1949,7 +1948,7 @@ TEST_F(MemorySpaceAssignmentTest, FilterUpdateConfigExactMatchBeforeTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n \n@@ -2027,7 +2026,7 @@ TEST_F(MemorySpaceAssignmentTest, FilterUpdateConfigExactMatchAfterTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n \n@@ -2105,7 +2104,7 @@ TEST_F(MemorySpaceAssignmentTest, FilterUpdateConfigExactMatchTooLateTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n \n@@ -2175,7 +2174,7 @@ TEST_F(MemorySpaceAssignmentTest, FilterUpdateConfigPrecedenceTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n \n@@ -2257,7 +2256,7 @@ TEST_F(MemorySpaceAssignmentTest, FilterUpdateConfigExactMatchPrecedenceTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n \n@@ -2340,7 +2339,7 @@ TEST_F(MemorySpaceAssignmentTest, FilterUpdatePreferredPrefetchNoMatchTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n \n@@ -2488,7 +2487,7 @@ TEST_F(MemorySpaceAssignmentTest,\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, tanh, a, b, c, d, e, f, g, h, i,\n                                       j, k, l, m, n, o, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(),\n                     UpdateMaxAsyncCopies(DefaultMemorySpaceOptions(), 1));\n@@ -2577,7 +2576,7 @@ TEST_F(MemorySpaceAssignmentTest, EvictAndPrefetchAndPrefetch) {\n        f,       g,       h,       i,       j,       k,       l,       m,\n        n,       o,       add0,    negate0, negate1, negate2, negate3, negate4,\n        negate5, negate6, negate7, negate8, negate9, add1});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -2668,7 +2667,7 @@ TEST_F(MemorySpaceAssignmentTest, While) {\n                          body_iter_next, body_data_increment, body_data_mul,\n                          body_data_add, body_data_next, body_out});\n   schedule.set_sequence(entry_computation, {iter, data, tuple, while_op});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   LOG(INFO) << module->ToString(HloPrintOptions::ShortParsable());\n \n@@ -2724,7 +2723,7 @@ TEST_F(MemorySpaceAssignmentTest, Tuple) {\n   schedule.set_sequence(\n       computation, {p, p0, negate0, negate1, negate2, negate3, negate4, negate5,\n                     negate6, p1, add, p2, p2_0, mul});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -2760,7 +2759,7 @@ TEST_F(MemorySpaceAssignmentTest, Bitcast) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate, bitcast, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -2798,7 +2797,7 @@ TEST_F(MemorySpaceAssignmentTest, Bitcast2) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, bitcast, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -2846,7 +2845,7 @@ TEST_F(MemorySpaceAssignmentTest, Bitcast3) {\n   schedule.set_sequence(computation,\n                         {p0, p1, negate0, negate1, negate2, negate3, negate4,\n                          bitcast1, add, bitcast2, bitcast3, bitcast4, mul});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -2918,7 +2917,7 @@ TEST_F(MemorySpaceAssignmentTest, BitcastTuple) {\n   schedule.set_sequence(computation,\n                         {p0, p1, negate0, negate1, negate2, negate3, negate4,\n                          bitcast, tuple, fusion});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n }\n@@ -3011,7 +3010,7 @@ TEST_F(MemorySpaceAssignmentTest, BitcastMultiUse) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, bitcast, negate0, negate1, negate2,\n                                       negate3, negate4, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n   Shape shape_in_alternate_mem = ShapeUtil::MakeShapeWithDenseLayout(\n@@ -3067,7 +3066,7 @@ TEST_F(MemorySpaceAssignmentTest, BitcastMultiUseTuple) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, bitcast, negate0, negate1, negate2,\n                                       negate3, negate4, tuple, fusion});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n   Shape shape_in_alternate_mem = ShapeUtil::MakeShapeWithDenseLayout(\n@@ -3131,7 +3130,7 @@ TEST_F(MemorySpaceAssignmentTest, BitcastScheduleBug) {\n   schedule.set_sequence(\n       computation, {p0, p1, bitcast, negate0, negate1, negate2, negate3,\n                     negate4, negate5, negate6, negate7, negate8, negate9, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(),\n                     /*max_prefetch_interval=*/5, /*min_prefetch_interval=*/4);\n@@ -4423,7 +4422,7 @@ TEST_F(MemorySpaceAssignmentTest, LastUseOpt) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, add1, sub1, mul1, add2, mul2,\n                                       padding_value, padded_mul2, add3});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -4515,7 +4514,7 @@ TEST_F(MemorySpaceAssignmentTest, NonEntryComputationSchedule1) {\n                          body_data_add, body_data_next, body_out});\n   schedule.set_sequence(entry_computation,\n                         {iter, data, p2, tuple, while_op, while_data, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(), 50);\n }\n@@ -4586,7 +4585,7 @@ TEST_F(MemorySpaceAssignmentTest, NonEntryComputationSchedule2) {\n        negate4, negate5, negate6, negate7, add0});\n   schedule.set_sequence(entry_computation,\n                         {p0, p1, add1, add2, negate8, call, add3, add4, add5});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(), 5);\n }\n@@ -4652,7 +4651,7 @@ TEST_F(MemorySpaceAssignmentTest, NonEntryComputationSchedule3) {\n       {call_param, iota, slice, mul, negate0, negate1, negate2, negate3,\n        negate4, negate5, negate6, negate7, add0});\n   schedule.set_sequence(entry_computation, {p0, add1, add2, call, add3});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(), 5);\n }\n@@ -4726,7 +4725,7 @@ TEST_F(MemorySpaceAssignmentTest, DISABLED_NonEntryComputationSchedule4) {\n   schedule.set_sequence(false_computation, {false_param});\n   schedule.set_sequence(entry_computation,\n                         {p0, add1, add2, pred, conditional, add3});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(), 5);\n }\n@@ -4845,7 +4844,7 @@ TEST_F(MemorySpaceAssignmentTest, NonEntryComputationSchedule5) {\n       entry_computation,\n       {iter, data, data2, negate0, negate1, negate2, negate3, negate4, negate5,\n        negate6, negate7, sub, tuple, while_op, while_data, root});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   // Set a large max prefetch interval so that the buffer can be kept in\n   // alternate memory.\n@@ -4950,7 +4949,7 @@ TEST_F(MemorySpaceAssignmentTest, NonEntryComputationSchedule6) {\n       entry_computation,\n       {iter, data, negate0, negate1, negate2, negate3, negate4, negate5,\n        negate6, negate7, tuple, while_op, while_data, while_data2, root});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   // Pick a large max prefetch interval to ensure all the while inputs are\n   // allocated in the alternate memory.\n@@ -5035,7 +5034,7 @@ TEST_F(MemorySpaceAssignmentTest, DanglingCopy) {\n   schedule.set_sequence(\n       computation, {p, p0, negate0, negate1, negate2, negate3, negate4, negate5,\n                     negate6, p1a, copy, p1b, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n }\n@@ -5072,7 +5071,7 @@ TEST_F(MemorySpaceAssignmentTest, MultiOutputFusion) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, fusion, element0, element1, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n }\n@@ -5112,7 +5111,7 @@ TEST_F(MemorySpaceAssignmentTest, TupleInput) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, tuple, fusion});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n }\n@@ -5185,7 +5184,7 @@ TEST_F(MemorySpaceAssignmentTest, TupleToTuple1) {\n       computation,\n       {p0, fusion0, element0, element1, negate0, negate1, negate2, negate3,\n        negate4, negate5, negate6, add0, add1, fusion1, mul});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(), 5);\n   EXPECT_THAT(fusion1,\n@@ -5258,7 +5257,7 @@ TEST_F(MemorySpaceAssignmentTest, TupleToTuple2) {\n   schedule.set_sequence(\n       computation, {p0, fusion0, negate0, negate1, negate2, negate3, negate4,\n                     negate5, negate6, fusion1});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(), 5);\n \n@@ -5317,7 +5316,7 @@ TEST_F(MemorySpaceAssignmentTest, TupleToTuple3) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, fusion0, fusion1});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n   EXPECT_THAT(fusion1, op::Fusion(op::Fusion()));\n@@ -5361,11 +5360,11 @@ TEST_F(MemorySpaceAssignmentTest, InputOutputAlias) {\n   schedule.set_sequence(\n       computation, {p, p0, negate0, negate1, negate2, negate3, negate4, negate5,\n                     negate6, p1, add, negate7, tuple});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   // Make input {0} alias with output {0} and input {1} alias with output {1}.\n-  TF_CHECK_OK(module->input_output_alias_config().SetUpAlias({0}, 0, {0}));\n-  TF_CHECK_OK(module->input_output_alias_config().SetUpAlias({1}, 0, {1}));\n+  CHECK_OK(module->input_output_alias_config().SetUpAlias({0}, 0, {0}));\n+  CHECK_OK(module->input_output_alias_config().SetUpAlias({1}, 0, {1}));\n \n   AssignMemorySpace(module.get());\n \n@@ -5408,7 +5407,7 @@ TEST_F(MemorySpaceAssignmentTest, CostAnalysis) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {p0, p1, negate0, negate1, negate2,\n                                       negate3, negate4, negate5, negate6, add});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpaceUsingCostAnalysis(module.get());\n   // Parameters are in the default memory space.\n@@ -5480,7 +5479,7 @@ TEST_F(MemorySpaceAssignmentTest, MemoryBoundednessBufferIntervalCompare) {\n   schedule.set_sequence(computation,\n                         {p0, p1, tanh0, negate0, tanh1, negate1, tanh2, negate2,\n                          tanh3, negate3, tanh4, negate4, tuple});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpaceUsingCostAnalysis(module.get());\n   // Parameters are in the default memory space.\n@@ -6475,7 +6474,7 @@ TEST_F(MemorySpaceAssignmentTest, SimpleWhileTupleTest) {\n \n   HloComputation* computation = module->AddEntryComputation(builder.Build());\n   schedule.set_sequence(computation, {param, gte0, gte1, tuple, while0});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get(), DefaultMemorySpaceOptions(),\n                     /*max_prefetch_interval=*/50);\n@@ -6558,7 +6557,7 @@ TEST_F(MemorySpaceAssignmentTest, EvictionsShouldntBeDelayed) {\n       {p0, tanh0, tanh_redundant0, tanh_redundant1, tanh_redundant2,\n        tanh_redundant3, tanh_redundant4, tanh_redundant5, tanh_redundant6,\n        negate0, tanh1, negate1, tanh2, negate2, tanh3, negate3, tuple});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpaceUsingCostAnalysis(module.get());\n \n@@ -6647,7 +6646,7 @@ TEST_F(MemorySpaceAssignmentTest,\n   schedule.set_sequence(computation,\n                         {p0, p1, negate0, negate1, negate2, negate3, negate4,\n                          negate5, negate6, add, tuple});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n   options.post_module_scoped_alternate_memory_size_in_bytes = 32;\n@@ -10701,7 +10700,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {lhs, rhs, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -10754,7 +10753,7 @@ TEST_F(MemorySpaceAssignmentTest, MultiCrossProgramPrefetchTest) {\n   HloSchedule schedule(module.get());\n   schedule.set_sequence(\n       computation, {lhs, first_weight, second_weight, first_dot, second_dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n   options.max_cross_program_prefetches = -1;\n@@ -10809,7 +10808,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchTupleTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {param, lhs, rhs, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -10849,7 +10848,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchBitcastTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {lhs, rhs, bitcast, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -10893,7 +10892,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchBitcastTupleTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {param, lhs, rhs, bitcast, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -10937,7 +10936,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchNestedTupleTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {param, gte, lhs, rhs, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -10960,7 +10959,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchUnusedParamTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {param});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -10994,7 +10993,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchTooBigTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {lhs, rhs, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -11032,7 +11031,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchTooBigTupleTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {param, lhs, rhs, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -11080,7 +11079,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchFusionTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {activations, weights, fusion});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -11133,7 +11132,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchFusionTupleTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {activations, weights, tuple, fusion});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   AssignMemorySpace(module.get());\n \n@@ -11171,7 +11170,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchPinnedTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {lhs, rhs, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n   options.is_allowed_in_alternate_mem_fn = [](const HloValue& value) {\n@@ -11218,7 +11217,7 @@ TEST_F(MemorySpaceAssignmentTest, CrossProgramPrefetchPinnedTupleTest) {\n \n   HloSchedule schedule(module.get());\n   schedule.set_sequence(computation, {param, lhs, rhs, dot});\n-  TF_CHECK_OK(module->set_schedule(schedule));\n+  CHECK_OK(module->set_schedule(schedule));\n \n   Options options = DefaultMemorySpaceOptions();\n   options.is_allowed_in_alternate_mem_fn = [](const HloValue& value) {"
        },
        {
            "sha": "4c6ab79c09b42144b2f976765bccddfac3593d75",
            "filename": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment_test_base.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test_base.h?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n@@ -56,7 +57,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/tests/hlo_pjrt_test_base.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -190,9 +190,9 @@ class MemorySpaceAssignmentTestBase : public HloPjRtTestBase {\n     HloCostAnalysis hlo_cost_analysis(hlo_cost_options);\n     HloCostAnalysisWithAcceptState hlo_cost_analysis_wrapper(hlo_cost_analysis);\n     for (HloComputation* computation : module->MakeNonfusionComputations()) {\n-      TF_CHECK_OK(computation->Accept(&hlo_cost_analysis));\n+      CHECK_OK(computation->Accept(&hlo_cost_analysis));\n     }\n-    TF_CHECK_OK(HloAliasAnalysis::Run(module, &alias_info_).status());\n+    CHECK_OK(HloAliasAnalysis::Run(module, &alias_info_).status());\n \n     Options memory_space_options = DefaultMemorySpaceOptions();\n     if (memory_space_options_override) {\n@@ -214,7 +214,7 @@ class MemorySpaceAssignmentTestBase : public HloPjRtTestBase {\n \n     auto status_or_cost_analysis = CostAnalysis::Create(\n         op_cost_manager, cost_analysis_options, &alias_info_, *module);\n-    TF_CHECK_OK(status_or_cost_analysis.status());\n+    CHECK_OK(status_or_cost_analysis.status());\n     auto cost_analysis = std::move(status_or_cost_analysis.value());\n \n     memory_space_options.cost_analysis = cost_analysis.get();\n@@ -243,7 +243,7 @@ class MemorySpaceAssignmentTestBase : public HloPjRtTestBase {\n       HloModule* module, std::optional<Options> options_override = std::nullopt,\n       int64_t max_prefetch_interval = 10, int64_t min_prefetch_interval = 2) {\n     InstructionHoister instruction_hoister;\n-    TF_CHECK_OK(instruction_hoister.Run(module).status());\n+    CHECK_OK(instruction_hoister.Run(module).status());\n     InstructionCountPrefetchIntervalPicker prefetch_interval_picker(\n         min_prefetch_interval, max_prefetch_interval);\n     return AssignMemorySpace(module, std::move(options_override),\n@@ -493,7 +493,7 @@ class MemorySpaceAssignmentTestBase : public HloPjRtTestBase {\n     const HloModule* module = instruction->GetModule();\n     AliasInfo alias_info;\n     auto status_or_alias_analysis = HloAliasAnalysis::Run(module, &alias_info);\n-    TF_CHECK_OK(status_or_alias_analysis.status());\n+    CHECK_OK(status_or_alias_analysis.status());\n     auto alias_analysis = std::move(status_or_alias_analysis.value());\n     const HloBuffer& buffer =\n         alias_analysis->GetUniqueBufferAt(instruction, index);\n@@ -562,7 +562,7 @@ class MemorySpaceAssignmentTestBase : public HloPjRtTestBase {\n     HloSchedule schedule(module.get());\n     schedule.set_sequence(computation, {p0, p1, tanh, a, b, c, d, e, f, g, h, i,\n                                         j, k, l, m, n, o, add});\n-    TF_CHECK_OK(module->set_schedule(schedule));\n+    CHECK_OK(module->set_schedule(schedule));\n     return module;\n   }\n "
        },
        {
            "sha": "63c51d82e0b5ca45db72f3bdbb1c556b2087c84a",
            "filename": "third_party/xla/xla/service/multi_output_fusion.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmulti_output_fusion.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -18,20 +18,26 @@ limitations under the License.\n #include <algorithm>\n #include <cstdint>\n #include <optional>\n+#include <utility>\n #include <vector>\n \n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/functional/function_ref.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/analysis/hlo_reachability.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n #include \"xla/map_util.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n@@ -109,7 +115,7 @@ HloInstruction* MultiOutputFusion::CreateFusion(HloInstruction* base,\n   reachability_->Replace(base, input_fusion);\n   all_fusion_candidates_.emplace_back(input_fusion,\n                                       reachability_->GetIndex(input_fusion));\n-  TF_CHECK_OK(computation()->ReplaceInstruction(base, input_fusion));\n+  CHECK_OK(computation()->ReplaceInstruction(base, input_fusion));\n   return input_fusion;\n }\n "
        },
        {
            "sha": "44d7898631fe6f55fb2f10b654b41e9f9ba67471",
            "filename": "third_party/xla/xla/service/service.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -18,9 +18,7 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <functional>\n-#include <map>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <set>\n #include <string>\n@@ -41,7 +39,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/layout.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/literal.h\"\n@@ -68,12 +65,10 @@ limitations under the License.\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/protobuf.h\"\n #include \"tsl/profiler/lib/scoped_annotation.h\"\n \n namespace xla {\n@@ -903,7 +898,7 @@ absl::StatusOr<Literal> Service::ComputeConstantGraph(\n   TF_ASSIGN_OR_RETURN(\n       ProgramShape program_shape,\n       ProgramShape::FromProto(computation.proto().host_program_shape()));\n-  TF_DCHECK_OK(ShapeUtil::ValidateShape(program_shape.result()));\n+  DCHECK_OK(ShapeUtil::ValidateShape(program_shape.result()));\n \n   if (output_layout) {\n     TF_RETURN_IF_ERROR(LayoutUtil::ValidateLayoutForShape("
        },
        {
            "sha": "b5dc87cbf44ee714249c26fe063fe86a1d7407c2",
            "filename": "third_party/xla/xla/service/shape_inference.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 19,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fshape_inference.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fshape_inference.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fshape_inference.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #include \"xla/service/shape_inference.h\"\n \n #include <algorithm>\n-#include <array>\n #include <cstddef>\n #include <cstdint>\n #include <iterator>\n@@ -32,26 +31,24 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/permutation_util.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/window_util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace {\n@@ -345,7 +342,7 @@ absl::StatusOr<DimAndBound> InferMostSpecificDimAndBound(int64_t dim,\n \n   TF_RETURN_IF_ERROR(ExpectArray(shape, \"operand of unary operation\"));\n \n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n   switch (opcode) {\n     case HloOpcode::kFloor:\n     case HloOpcode::kCbrt:  // Complex cbrt is not implemented in either of the\n@@ -664,8 +661,8 @@ absl::StatusOr<DimAndBound> InferMostSpecificDimAndBound(int64_t dim,\n /* static */ absl::StatusOr<Shape> ShapeInference::InferStochasticConvertShape(\n     const Shape& operand_shape, const Shape& random_shape,\n     PrimitiveType new_element_type) {\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(operand_shape));\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(random_shape));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(operand_shape));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(random_shape));\n \n   TF_RETURN_IF_ERROR(\n       ExpectArray(operand_shape, \"lhs of stochastic convert operation\"));\n@@ -967,7 +964,7 @@ void GenerateDotResultDimensions(\n       ShapeUtil::HigherPrecisionElementType(lhs, rhs));\n   Shape result = ShapeUtil::MakeShape(type, dimensions, is_dynamic);\n \n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(result));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(result));\n   VLOG(2) << \"inferred dot shape: \" << ShapeUtil::HumanString(result);\n   return result;\n }\n@@ -1173,7 +1170,7 @@ void GenerateDotResultDimensions(\n                               is_dynamic, rhs_group_dimensions);\n \n   Shape result = ShapeUtil::MakeShape(type, dimensions, is_dynamic);\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(result));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(result));\n   VLOG(2) << \"inferred ragged dot shape: \" << ShapeUtil::HumanString(result);\n   return result;\n }\n@@ -1465,8 +1462,8 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n       ShapeUtil::HumanStringWithLayout(rhs),\n       StrJoin(broadcast_dimensions, \", \"));\n \n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(lhs));\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(rhs));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(lhs));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(rhs));\n \n   TF_RETURN_IF_ERROR(ExpectArray(\n       lhs, absl::StrCat(\"lhs of binary operation \", HloOpcodeString(opcode))));\n@@ -1548,9 +1545,9 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n \n /* static */ absl::StatusOr<Shape> ShapeInference::InferTernaryOpShape(\n     HloOpcode opcode, const Shape& lhs, const Shape& rhs, const Shape& ehs) {\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(lhs));\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(rhs));\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(ehs));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(lhs));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(rhs));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(ehs));\n   switch (opcode) {\n     case HloOpcode::kClamp:\n       return InferClampShape(lhs, rhs, ehs);\n@@ -1574,7 +1571,7 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n /* static */ absl::StatusOr<Shape> ShapeInference::InferVariadicOpShape(\n     HloOpcode opcode, absl::Span<const Shape* const> operand_shapes) {\n   for (const Shape* shape : operand_shapes) {\n-    TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(*shape));\n+    DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(*shape));\n   }\n   switch (opcode) {\n     case HloOpcode::kTuple: {\n@@ -2184,8 +2181,8 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n         \"The RHS argument to a convolution should have rank %d; rhs: %s.\",\n         num_dims, ShapeUtil::HumanString(rhs));\n   }\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(lhs));\n-  TF_DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(rhs));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(lhs));\n+  DCHECK_OK(ShapeUtil::ValidateShapeWithOptionalLayout(rhs));\n \n   // Verifies that the input and window dimensions are a permutation of\n   // the dimension numbers."
        },
        {
            "sha": "0fe09a04c8aeddc39a06f2a1a84b63247910bed0",
            "filename": "third_party/xla/xla/service/shaped_buffer.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fshaped_buffer.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n@@ -28,7 +29,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -181,7 +181,7 @@ void ScopedShapedBuffer::Deallocate() {\n     se::DeviceMemoryBase& memory_base = pair.second;\n     if (!memory_base.is_null() &&\n         deallocated_ptrs.insert(memory_base.opaque()).second) {\n-      TF_CHECK_OK(allocator_->Deallocate(device_ordinal(), memory_base));\n+      CHECK_OK(allocator_->Deallocate(device_ordinal(), memory_base));\n     }\n   }\n }"
        },
        {
            "sha": "754fb426442ee0829d1d863f8241b0448a1bc4c9",
            "filename": "third_party/xla/xla/service/space_to_batch_converter.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 31,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fspace_to_batch_converter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Fspace_to_batch_converter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspace_to_batch_converter.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -15,19 +15,18 @@ limitations under the License.\n #include \"xla/service/space_to_batch_converter.h\"\n \n #include <algorithm>\n-#include <cstddef>\n #include <cstdint>\n+#include <cstdlib>\n #include <iterator>\n #include <map>\n-#include <memory>\n #include <queue>\n-#include <tuple>\n #include <utility>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -45,11 +44,9 @@ limitations under the License.\n #include \"xla/service/shape_inference.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/bitmap.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/status.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n \n@@ -912,7 +909,7 @@ absl::StatusOr<bool> ConvolutionVisitor::Run() {\n       convs_to_visit_.erase(conv);\n     }\n     if (convs_to_visit_.count(conv) > 0) {\n-      TF_CHECK_OK(PerformSpaceToBatchOnConvolution(conv));\n+      CHECK_OK(PerformSpaceToBatchOnConvolution(conv));\n       changed_ = true;\n     }\n   }\n@@ -938,8 +935,8 @@ absl::StatusOr<bool> ConvolutionVisitor::Run() {\n           bool needs_further_propagation;\n           TF_ASSIGN_OR_RETURN(needs_further_propagation,\n                               Propagate(instr, producer));\n-          TF_CHECK_OK(computation_->ReplaceInstruction(\n-              instr, old_to_new_instrs_[instr]));\n+          CHECK_OK(computation_->ReplaceInstruction(instr,\n+                                                    old_to_new_instrs_[instr]));\n           continue;\n         }\n       }\n@@ -953,7 +950,7 @@ absl::StatusOr<bool> ConvolutionVisitor::Run() {\n       }\n     }\n     for (auto entry : operand_map) {\n-      TF_CHECK_OK(instr->ReplaceOperandWith(entry.first, entry.second));\n+      CHECK_OK(instr->ReplaceOperandWith(entry.first, entry.second));\n     }\n   }\n   non_propagatable_instrs_.clear();\n@@ -1901,7 +1898,7 @@ absl::StatusOr<bool> ConvolutionVisitor::Propagate(HloInstruction* consumer,\n           }\n         }\n         CHECK_NE(new_broadcast, nullptr);\n-        TF_CHECK_OK(\n+        CHECK_OK(\n             new_consumer->ReplaceOperandWithDifferentShape(i, new_broadcast));\n       } else if (old_to_new_instrs_.contains(consumer->mutable_operand(i))) {\n         HloInstruction* operand_to_use = nullptr;\n@@ -1965,21 +1962,21 @@ absl::StatusOr<bool> ConvolutionVisitor::Propagate(HloInstruction* consumer,\n         } else {\n           operand_to_use = old_to_new_instrs_[consumer->mutable_operand(i)];\n         }\n-        TF_CHECK_OK(\n+        CHECK_OK(\n             new_consumer->ReplaceOperandWithDifferentShape(i, operand_to_use));\n       } else if (consumer->IsElementwiseBinary() &&\n                  consumer->mutable_operand(i)->opcode() ==\n                      HloOpcode::kBroadcast &&\n                  IsBroadcastTree(consumer->mutable_operand(i), producer,\n                                  instructions_to_transform)) {\n         RewriteBroadcastTree(producer, instructions_to_transform);\n-        TF_CHECK_OK(new_consumer->ReplaceOperandWithDifferentShape(\n+        CHECK_OK(new_consumer->ReplaceOperandWithDifferentShape(\n             i, old_to_new_instrs_[consumer->mutable_operand(i)]));\n       } else if (consumer->operand(i)->opcode() == HloOpcode::kConstant) {\n         TF_ASSIGN_OR_RETURN(\n             auto new_constant,\n             PropagateOnConstant(consumer->mutable_operand(i), producer));\n-        TF_CHECK_OK(\n+        CHECK_OK(\n             new_consumer->ReplaceOperandWithDifferentShape(i, new_constant));\n       }\n     }\n@@ -2005,21 +2002,21 @@ absl::StatusOr<bool> ConvolutionVisitor::Propagate(HloInstruction* consumer,\n \n   if (consumer->opcode() == HloOpcode::kConvolution) {\n     if (IsConvSuitableForSpaceToBatch(consumer)) {\n-      TF_CHECK_OK(PropagateOnConv(consumer));\n+      CHECK_OK(PropagateOnConv(consumer));\n       return true;\n     } else {\n-      TF_CHECK_OK(PropagateOnBackpropFilterConv(consumer));\n+      CHECK_OK(PropagateOnBackpropFilterConv(consumer));\n       return false;\n     }\n   }\n \n   if (consumer->opcode() == HloOpcode::kConcatenate) {\n-    TF_CHECK_OK(PropagateOnConcat(consumer));\n+    CHECK_OK(PropagateOnConcat(consumer));\n     return true;\n   }\n \n   if (consumer->opcode() == HloOpcode::kReverse) {\n-    TF_CHECK_OK(PropagateOnReverse(consumer));\n+    CHECK_OK(PropagateOnReverse(consumer));\n     return true;\n   }\n \n@@ -2070,12 +2067,12 @@ absl::StatusOr<bool> ConvolutionVisitor::Propagate(HloInstruction* consumer,\n   // TODO(b/189500737) : Consider a common way of propagation for\n   // slice/pad/reduce-window.\n   if (consumer->opcode() == HloOpcode::kPad) {\n-    TF_CHECK_OK(PropagateOnPad(consumer));\n+    CHECK_OK(PropagateOnPad(consumer));\n     return true;\n   }\n \n   if (consumer->opcode() == HloOpcode::kSlice) {\n-    TF_CHECK_OK(PropagateOnSlice(consumer));\n+    CHECK_OK(PropagateOnSlice(consumer));\n     return true;\n   }\n \n@@ -2204,8 +2201,7 @@ absl::StatusOr<bool> ConvolutionVisitor::Propagate(HloInstruction* consumer,\n     }\n     *(new_consumer->mutable_dimensions()) = changed_dims;\n     // Replace operand 0.\n-    TF_CHECK_OK(\n-        new_consumer->ReplaceOperandWithDifferentShape(0, first_operand));\n+    CHECK_OK(new_consumer->ReplaceOperandWithDifferentShape(0, first_operand));\n     // We do not set instr_to_dim_permute_map_ here because no further\n     // propagation is needed here.\n     old_to_new_instrs_[consumer] = new_consumer;\n@@ -2385,10 +2381,10 @@ absl::StatusOr<bool> ConvolutionVisitor::Propagate(HloInstruction* consumer,\n               second_operand, init_val, scatter_comp),\n           &consumer->metadata(), &consumer->frontend_attributes());\n       // Replace operand 0.\n-      TF_CHECK_OK(\n+      CHECK_OK(\n           new_consumer->ReplaceOperandWithDifferentShape(0, first_operand));\n       // Replace operand 1.\n-      TF_CHECK_OK(\n+      CHECK_OK(\n           new_consumer->ReplaceOperandWithDifferentShape(1, second_operand));\n       VLOG(2) << \"New select and scatter \" << new_consumer->ToString();\n \n@@ -2557,7 +2553,7 @@ absl::StatusOr<bool> ConvolutionVisitor::Propagate(HloInstruction* consumer,\n                                              reduce_comp),\n           &consumer->metadata(), &consumer->frontend_attributes());\n       // Replace operand 0.\n-      TF_CHECK_OK(\n+      CHECK_OK(\n           new_consumer->ReplaceOperandWithDifferentShape(0, first_operand));\n       VLOG(1) << \"New reduce window \" << new_consumer->ToString();\n     }\n@@ -2750,7 +2746,7 @@ absl::Status ConvolutionVisitor::PropagateOnUsers(HloInstruction* old_conv) {\n                         BatchToSpace(old_conv));\n     VLOG(1) << \"Replacing the root instruction to \"\n             << batch_to_space->ToString();\n-    TF_CHECK_OK(computation_->ReplaceInstruction(old_conv, batch_to_space));\n+    CHECK_OK(computation_->ReplaceInstruction(old_conv, batch_to_space));\n     VLOG(1) << \"Replacement successful\";\n     return absl::OkStatus();\n   }\n@@ -2783,18 +2779,18 @@ absl::Status ConvolutionVisitor::PropagateOnUsers(HloInstruction* old_conv) {\n       if (!needs_further_propagation) {\n         VLOG(1) << \"Replacing the root instruction to \"\n                 << old_to_new_instrs_[node]->ToString();\n-        TF_CHECK_OK(\n+        CHECK_OK(\n             computation_->ReplaceInstruction(node, old_to_new_instrs_[node]));\n         continue;\n       }\n \n       TF_ASSIGN_OR_RETURN(HloInstruction * batch_to_space, BatchToSpace(node));\n       VLOG(1) << \"Replacing the root instruction to \"\n               << batch_to_space->ToString();\n-      TF_CHECK_OK(computation_->ReplaceInstruction(node, batch_to_space));\n+      CHECK_OK(computation_->ReplaceInstruction(node, batch_to_space));\n     } else {\n       if (!needs_further_propagation) {\n-        TF_CHECK_OK(\n+        CHECK_OK(\n             computation_->ReplaceInstruction(node, old_to_new_instrs_[node]));\n         continue;\n       }\n@@ -2825,7 +2821,7 @@ absl::Status ConvolutionVisitor::PropagateOnUsers(HloInstruction* old_conv) {\n         for (auto user : unsupported_users) {\n           for (int64_t i = 0; i < user->operand_count(); ++i) {\n             if (user->operand(i) == node) {\n-              TF_CHECK_OK(user->ReplaceOperandWith(i, batch_to_space));\n+              CHECK_OK(user->ReplaceOperandWith(i, batch_to_space));\n             }\n           }\n         }\n@@ -4187,7 +4183,7 @@ absl::Status ConvolutionVisitor::PerformSpaceToBatchOnConvolution(\n   if (non_propagatable_instrs_.count(convolution) > 0) {\n     non_propagatable_instrs_.erase(convolution);\n   }\n-  TF_CHECK_OK(PropagateOnUsers(original_conv));\n+  CHECK_OK(PropagateOnUsers(original_conv));\n \n   return absl::OkStatus();\n }"
        },
        {
            "sha": "a04814c31dc2626d6708325176bc6948862a7a5c",
            "filename": "third_party/xla/xla/service/transpose_folding.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Ftranspose_folding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c2e95085421445df7b104fc25ad298387b7bab0/third_party%2Fxla%2Fxla%2Fservice%2Ftranspose_folding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Ftranspose_folding.cc?ref=3c2e95085421445df7b104fc25ad298387b7bab0",
            "patch": "@@ -29,15 +29,12 @@ limitations under the License.\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/status.h\"\n \n namespace xla {\n namespace {\n@@ -179,7 +176,7 @@ bool FoldTransposeIntoConvolution(InstructionOperandsPair& pair) {\n       convolution.shape(), new_lhs, new_rhs, convolution.feature_group_count(),\n       convolution.batch_group_count(), convolution.window(), new_dnums,\n       convolution.precision_config());\n-  TF_CHECK_OK(convolution.parent()->ReplaceWithNewInstruction(\n+  CHECK_OK(convolution.parent()->ReplaceWithNewInstruction(\n       &convolution, std::move(new_conv)));\n \n   return true;"
        }
    ],
    "stats": {
        "total": 940,
        "additions": 422,
        "deletions": 518
    }
}