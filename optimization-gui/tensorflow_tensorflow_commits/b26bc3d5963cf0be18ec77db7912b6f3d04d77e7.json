{
    "author": "tensorflower-gardener",
    "message": "Reverts 7cede9bb538c990006e36395080e95d11a28710d\n\nPiperOrigin-RevId: 850419890",
    "sha": "b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
    "files": [
        {
            "sha": "db0708210980ca4512b800e0104862f9f30881d6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -26,7 +26,6 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n-        \"//xla/service:gpu_topology\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "f18968f152b644b02bfcd676170b6b4019c9a076",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n-#include \"xla/service/gpu_topology.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -78,7 +77,7 @@ class GpuCodegenBackend : public CodegenBackend {\n         allow_register_spills_);\n \n     Compiler::CompileOptions options;\n-    options.gpu_topology = GetSingleDeviceGpuTopology(\"\", target_config_);\n+    options.gpu_target_config = target_config_;\n     options.embed_hlo_module = false;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));"
        },
        {
            "sha": "61a60fe1070437891ccb25d2ab14a163eb89b4ee",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -450,7 +450,6 @@ cc_library(\n         \"//xla/pjrt:utils\",\n         \"//xla/service:compiler\",\n         \"//xla/service:dump\",\n-        \"//xla/service:gpu_topology\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_module_util\",\n         \"//xla/service:hlo_proto_cc\","
        },
        {
            "sha": "dd31e129319c43d84a39ce1c8a2326156bb7d2b3",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"xla/pjrt/utils.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/dump.h\"\n-#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_module_util.h\"\n@@ -180,9 +179,7 @@ StreamExecutorGpuCompiler::Compile(CompileOptions options,\n   DumpHloModuleIfEnabled(*hlo_module, kBeforeOptimizationsDumpName);\n \n   AotCompilationOptions aot_options(gpu_compiler->PlatformId());\n-  GpuTopology xla_gpu_topology = GetSingleDeviceGpuTopology(\n-      /*platform_version=*/\"\", *options.gpu_target_config);\n-  aot_options.set_gpu_topology(xla_gpu_topology);\n+  aot_options.set_gpu_target_config(*options.gpu_target_config);\n   aot_options.set_run_backend_only(\n       options.executable_build_options.run_backend_only());\n "
        },
        {
            "sha": "31d618c31b886bffef5adb8122da0a0fc16846ab",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -1612,14 +1612,14 @@ cc_library(\n         \":compiled_module\",\n         \":computation_placer\",\n         \":executable\",\n-        \":gpu_topology\",\n         \":hlo_cost_analysis\",\n         \":hlo_module_config\",\n         \":metrics_hook_interface\",\n         \"//xla:debug_options_flags\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/backends/cpu:target_machine_options\",\n+        \"//xla/backends/gpu/target_config\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/stream_executor:device_address_allocator\",\n@@ -6275,8 +6275,6 @@ cc_library(\n         \"//learning/pathways/compilation_service:__subpackages__\",\n         \"//tensorflow/core/common_runtime/eager:__subpackages__\",\n         \":__subpackages__\",\n-        \"//xla/tools:__subpackages__\",\n-        \"//xla/backends/gpu/autotuner:__subpackages__\",\n     ]),\n     deps = [\n         \":gpu_topology_proto_cc\","
        },
        {
            "sha": "865237a1eff7fd7cc3c3e719e3d7a127c029fe50",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"google/protobuf/message.h\"\n #include \"xla/backends/cpu/target_machine_options.h\"\n+#include \"xla/backends/gpu/target_config/target_config.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -45,7 +46,6 @@ limitations under the License.\n #include \"xla/service/compiled_module.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/executable.h\"\n-#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/metrics_hook_interface.h\"\n@@ -130,9 +130,9 @@ class Compiler {\n         const HloModule& module)>\n         layout_canonicalization_callback = {};\n \n-    // GPU topology. If provided, used instead of querying the device on which\n-    // compilation is performed.\n-    std::optional<GpuTopology> gpu_topology;\n+    // AOT device description. If provided, used instead of querying the device\n+    // on which compilation is performed.\n+    std::optional<GpuTargetConfig> gpu_target_config;\n \n     // CPU specific target information.\n     std::optional<CpuTargetConfig> cpu_target_config;\n@@ -440,11 +440,11 @@ class AotCompilationOptions {\n     sanitize_abilists_dataflow_ = abilists;\n   }\n \n-  const std::optional<GpuTopology>& gpu_topology() const {\n-    return gpu_topology_;\n+  const std::optional<gpu::GpuTargetConfig>& gpu_target_config() const {\n+    return gpu_target_config_;\n   }\n-  void set_gpu_topology(const GpuTopology& gpu_topology) {\n-    gpu_topology_ = gpu_topology;\n+  void set_gpu_target_config(const gpu::GpuTargetConfig& gpu_target_config) {\n+    gpu_target_config_ = gpu_target_config;\n   }\n \n   // Provides a way to end compilation early and get partial outputs.\n@@ -477,7 +477,7 @@ class AotCompilationOptions {\n   bool sanitize_dataflow_ = false;\n   std::vector<std::string> sanitize_abilists_dataflow_;\n   // Contains target-specific information required by AOT compilation.\n-  std::optional<GpuTopology> gpu_topology_;\n+  std::optional<gpu::GpuTargetConfig> gpu_target_config_;\n   EarlyExitPoint early_exit_point_ = EarlyExitPoint::kNone;\n };\n "
        },
        {
            "sha": "c546696f7549364a8d6d2efbf3ae59d4c1d84e03",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -2341,7 +2341,6 @@ xla_cc_test(\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:gpu_plugin\",\n-        \"//xla/service:gpu_topology\",\n         \"//xla/service:hlo_runner_interface\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:platform\","
        },
        {
            "sha": "3acb11b387fabb90699cebc0c0f2f284bc2b05a8",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -35,7 +35,6 @@ limitations under the License.\n #include \"xla/literal_util.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n-#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo_runner_interface.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -130,8 +129,7 @@ TEST_P(GpuAotCompilationTest, AotCompilationWithoutGpuDevice) {\n   // Stream executor is not passed as an option.\n   Compiler::GpuTargetConfig gpu_target_config(stream_exec);\n   AotCompilationOptions aot_options(compiler->PlatformId());\n-  aot_options.set_gpu_topology(\n-      GetSingleDeviceGpuTopology(\"\", gpu_target_config));\n+  aot_options.set_gpu_target_config(gpu_target_config);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::vector<std::unique_ptr<AotCompilationResult>> aot_results,"
        },
        {
            "sha": "01d37fd0a07d0415f1888a29810aec73c3a28c14",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 69,
            "deletions": 88,
            "changes": 157,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -383,21 +383,6 @@ se::GpuComputeCapability GetGpuVersion(const se::StreamExecutor* stream_exec) {\n   return stream_exec->GetDeviceDescription().gpu_compute_capability();\n }\n \n-bool IsDevicelessCompilation(const Compiler::CompileOptions& options,\n-                             const se::StreamExecutor* stream_exec) {\n-  return options.early_exit_with_layouts || stream_exec == nullptr;\n-}\n-\n-int GetNumVisibleDevices(const Compiler::CompileOptions& options,\n-                         const se::StreamExecutor* stream_exec,\n-                         const se::Platform* platform) {\n-  if (IsDevicelessCompilation(options, stream_exec) &&\n-      options.gpu_topology.has_value()) {\n-    return options.gpu_topology->num_devices_per_host();\n-  }\n-  return platform->VisibleDeviceCount();\n-}\n-\n }  // namespace\n \n GpuCompiler::GpuCompiler(se::Platform::Id platform_id,\n@@ -547,16 +532,16 @@ absl::Status RunSPMDPasses(\n namespace {\n \n absl::Status SetHostDeviceType(HloInstruction* instr) {\n-  ASSIGN_OR_RETURN(auto backend_config,\n-                   instr->backend_config<GpuBackendConfig>());\n+  TF_ASSIGN_OR_RETURN(auto backend_config,\n+                      instr->backend_config<GpuBackendConfig>());\n   backend_config.set_device_type(DEVICE_TYPE_HOST);\n   RETURN_IF_ERROR(instr->set_backend_config(backend_config));\n   return absl::OkStatus();\n }\n \n absl::Status ClearBackendConfigDeviceType(HloInstruction* instr) {\n-  ASSIGN_OR_RETURN(auto backend_config,\n-                   instr->backend_config<GpuBackendConfig>());\n+  TF_ASSIGN_OR_RETURN(auto backend_config,\n+                      instr->backend_config<GpuBackendConfig>());\n   backend_config.clear_device_type();\n   return instr->set_backend_config(backend_config);\n }\n@@ -1365,8 +1350,8 @@ absl::Status RunDynamicSliceFusionPasses(HloModule* hlo_module,\n   const DebugOptions& opts = hlo_module->config().debug_options();\n   if (opts.xla_gpu_enable_dynamic_slice_fusion()) {\n     HloPassPipeline pipeline(\"dynamic-slice\");\n-    ASSIGN_OR_RETURN(se::Platform * platform,\n-                     se::PlatformManager::PlatformWithId(platform_id));\n+    TF_ASSIGN_OR_RETURN(se::Platform * platform,\n+                        se::PlatformManager::PlatformWithId(platform_id));\n     pipeline.AddPass<GpuReduceScatterCombiner>(\n         kDefaultReduceScatterCombineThreshold,\n         opts.xla_gpu_reduce_scatter_combine_threshold_bytes(),\n@@ -1520,7 +1505,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   // Dump the HLO module after SPMD partitioning. There should be no more Python\n   // callbacks at this point.\n   DumpHloModuleIfEnabled(*hlo_module, \"after_spmd_partitioner\");\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       const stream_executor::Platform* platform,\n       stream_executor::PlatformManager::PlatformWithId(PlatformId()));\n \n@@ -1539,17 +1524,16 @@ absl::Status GpuCompiler::OptimizeHloModule(\n       platform->Name(), enable_sort_rewriter));\n   se::GpuComputeCapability gpu_version =\n       device_description.gpu_compute_capability();\n-  int device_count = GetNumVisibleDevices(options, stream_exec, platform);\n   RETURN_IF_ERROR(RunCollectiveOptimizationPasses(\n       hlo_module, options, layout_insensitive_algsimp_opts, gpu_version,\n-      device_count, pointer_size_));\n+      platform->VisibleDeviceCount(), pointer_size_));\n \n   // Run target-specific HLO optimization passes for convolution\n   // canonicalization.\n   se::dnn::VersionInfo dnn_version = gpu_target_config.dnn_version_info;\n   if (stream_exec != nullptr) {\n     gpu_version = GetGpuVersion(stream_exec);\n-    ASSIGN_OR_RETURN(dnn_version, GetDnnVersionInfo(stream_exec));\n+    TF_ASSIGN_OR_RETURN(dnn_version, GetDnnVersionInfo(stream_exec));\n   }\n \n   RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(\n@@ -1679,7 +1663,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n           gpu_target_config.platform_name == \"ROCM\");\n   DeviceOrDevicelessConfig device_config =\n       GetDeviceConfig(stream_exec, options, gpu_target_config);\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       AutotuneConfig autotune_config,\n       AutotuneConfig::FromDebugOptions(device_config, debug_options));\n   // Lambdas and related constants:\n@@ -1954,10 +1938,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n /*static*/ absl::StatusOr<GpuTargetConfig> GpuCompiler::GetTargetConfig(\n     const Compiler::CompileOptions& options, const DebugOptions& debug_opts,\n     se::StreamExecutor* executor) {\n-  if (options.gpu_topology.has_value()) {\n-    if (options.gpu_topology->has_gpu_target_config()) {\n-      return options.gpu_topology->gpu_target_config();\n-    }\n+  if (options.gpu_target_config.has_value()) {\n+    return *options.gpu_target_config;\n   }\n   if (!debug_opts.xla_gpu_target_config_filename().empty()) {\n     std::string gpu_target_config_string;\n@@ -2004,11 +1986,11 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n \n   const DebugOptions debug_opts = module->config().debug_options();\n   RETURN_IF_ERROR(LoadAutotuneResultsFromFile(debug_opts));\n-  bool is_deviceless = options.gpu_topology.has_value() ||\n+  bool is_deviceless = options.gpu_target_config.has_value() ||\n                        !debug_opts.xla_gpu_target_config_filename().empty();\n \n-  ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n-                   GetTargetConfig(options, debug_opts, stream_exec));\n+  TF_ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n+                      GetTargetConfig(options, debug_opts, stream_exec));\n   const std::optional<std::string> unoptimized_fingerprint =\n       MaybeUploadUnoptimizedGpuSymbols(module.get(),\n                                        gpu_target_config.ToProto());\n@@ -2046,8 +2028,9 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   AutotuneResults autotune_results;\n   DeviceOrDevicelessConfig device_config =\n       GetDeviceConfig(stream_exec, options, gpu_target_config);\n-  ASSIGN_OR_RETURN(AutotuneConfig autotune_config,\n-                   AutotuneConfig::FromDebugOptions(device_config, debug_opts));\n+  TF_ASSIGN_OR_RETURN(\n+      AutotuneConfig autotune_config,\n+      AutotuneConfig::FromDebugOptions(device_config, debug_opts));\n   if (!is_deviceless) {\n     RETURN_IF_ERROR(AutotunerUtil::SerializeAutotuneResults(&autotune_results));\n     RETURN_IF_ERROR(SerializeAutotuneResultsToFile(debug_opts));\n@@ -2061,7 +2044,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   }\n \n   if (DumpingEnabledForHloModule(*module)) {\n-    ASSIGN_OR_RETURN(\n+    TF_ASSIGN_OR_RETURN(\n         std::string autotune_results,\n         AutotunerUtil::SerializeAutotuneResults(/*as_textproto=*/true));\n     DumpToFileInDirOrStdout(*module, \"\", \"autotune_results.pbtxt\",\n@@ -2214,7 +2197,7 @@ GpuCompiler::CompileSingleModule(\n                 : \".\");\n   }\n \n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       BackendCompileResult result,\n       CompileTargetBinary(module_config, llvm_module, device_description,\n                           relocatable, debug_module, options, shard_number));\n@@ -2431,7 +2414,7 @@ absl::StatusOr<GpuCompiler::BackendCompileResult> GpuCompiler::CompileAndLink(\n   std::vector<KernelReuseCache::NamedBinary> binaries_to_cache;\n   binaries_to_cache.reserve(single_function_module_count);\n   for (const auto& [name, maybe_result] : compile_results) {\n-    ASSIGN_OR_RETURN(auto result, maybe_result);\n+    TF_ASSIGN_OR_RETURN(auto result, maybe_result);\n     if (result.binary.empty()) {\n       continue;\n     }\n@@ -2507,10 +2490,10 @@ absl::StatusOr<xla::cpu::CompilationResultProto> GetCpuCompilationResult(\n     const HloModuleProto& hlo_proto) {\n   xla::cpu::NanoRtClient client;\n   XlaComputation computation(hlo_proto);\n-  ASSIGN_OR_RETURN(std::unique_ptr<xla::cpu::NanoRtExecutable> executable,\n-                   client.Compile(computation));\n-  ASSIGN_OR_RETURN(std::unique_ptr<AotCompilationResult> result,\n-                   client.Export(executable.get()));\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<xla::cpu::NanoRtExecutable> executable,\n+                      client.Compile(computation));\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<AotCompilationResult> result,\n+                      client.Export(executable.get()));\n   xla::cpu::CpuAotCompilationResult* cpu_aot_compilation_result =\n       tsl::down_cast<xla::cpu::CpuAotCompilationResult*>(result.get());\n   return cpu_aot_compilation_result->proto();\n@@ -2526,9 +2509,9 @@ GpuCompiler::CompileToBackendResult(\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n   RETURN_IF_ERROR(\n       RunPreSchedulingPasses(module, gpu_device_info, alias_info.get()));\n-  ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n-                   ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n-                                     &mlir_context_, alias_info.get()));\n+  TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n+                      ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n+                                        &mlir_context_, alias_info.get()));\n   HloPassPipeline pipeline(\"scheduled-gpu-module\");\n   AddHloVerifier(&pipeline);\n   RETURN_IF_ERROR(pipeline.Run(module).status());\n@@ -2546,8 +2529,8 @@ GpuCompiler::CompileToBackendResult(\n             \". Are you missing gpu_plugin or stream_executor dependency?\"));\n   }\n \n-  ASSIGN_OR_RETURN(bool can_use_link_modules,\n-                   CanUseLinkModules(module->config(), gpu_device_info));\n+  TF_ASSIGN_OR_RETURN(bool can_use_link_modules,\n+                      CanUseLinkModules(module->config(), gpu_device_info));\n   const bool split_modules =\n       can_use_link_modules &&\n       module->config()\n@@ -2565,12 +2548,12 @@ GpuCompiler::CompileToBackendResult(\n     BufferValue::SizeFunction buffer_size_bytes_function =\n         BufferSizeBytesFunction();\n     // Compile the module to thnks and llvm IR.\n-    ASSIGN_OR_RETURN(compile_module_results,\n-                     CompileModuleToLlvmIr(\n-                         module, llvm_context, target_triple_, data_layout_,\n-                         *platform, gpu_device_info, alias_info.get(),\n-                         std::move(buffer_size_bytes_function),\n-                         /*split_constants_module=*/use_cache));\n+    TF_ASSIGN_OR_RETURN(compile_module_results,\n+                        CompileModuleToLlvmIr(\n+                            module, llvm_context, target_triple_, data_layout_,\n+                            *platform, gpu_device_info, alias_info.get(),\n+                            std::move(buffer_size_bytes_function),\n+                            /*split_constants_module=*/use_cache));\n   }\n \n   if (user_pre_optimization_hook_) {\n@@ -2594,12 +2577,12 @@ GpuCompiler::CompileToBackendResult(\n   // TODO(anlunx): Enable multi-threading once deviceless AOT compilation is\n   // enabled.\n   if (split_modules) {\n-    ASSIGN_OR_RETURN(backend_result,\n-                     CompileAndLink(module->config(), compile_module_results,\n-                                    gpu_device_info, options, module));\n+    TF_ASSIGN_OR_RETURN(backend_result,\n+                        CompileAndLink(module->config(), compile_module_results,\n+                                       gpu_device_info, options, module));\n   } else {\n     CHECK(compile_module_results.llvm_module_constants == nullptr);\n-    ASSIGN_OR_RETURN(\n+    TF_ASSIGN_OR_RETURN(\n         backend_result,\n         CompileSingleModule(module->config(), gpu_device_info, module,\n                             &*compile_module_results.llvm_module,\n@@ -2656,8 +2639,8 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   }\n \n   const DebugOptions& debug_opts = module->config().debug_options();\n-  ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n-                   GetTargetConfig(options, debug_opts, stream_exec));\n+  TF_ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n+                      GetTargetConfig(options, debug_opts, stream_exec));\n \n   if (DumpingEnabledForHloModule(*module)) {\n     std::string textproto;\n@@ -2689,9 +2672,9 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n         tsl::strings::HumanReadableNumBytes(cost_analysis.bytes_accessed()));\n   }\n \n-  ASSIGN_OR_RETURN(CompileResultWithMetadata res,\n-                   CompileToBackendResult(module.get(), &llvm_context, options,\n-                                          gpu_device_info));\n+  TF_ASSIGN_OR_RETURN(CompileResultWithMetadata res,\n+                      CompileToBackendResult(module.get(), &llvm_context,\n+                                             options, gpu_device_info));\n   ModuleStats module_stats = res.backend_result.module_stats;\n \n   if (DumpingEnabledForHloModule(*module)) {\n@@ -2711,7 +2694,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   });\n \n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<GpuExecutable> gpu_executable,\n       GpuExecutable::Create(GpuExecutable::Params{\n           /*asm_text=*/embed_debug_info ? std::move(res.backend_result.asm_text)\n@@ -2788,14 +2771,14 @@ GpuCompiler::NewCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n                                    const AotCompilationOptions& options) {\n   CompileOptions compile_options;\n   compile_options.device_allocator = options.device_allocator();\n-  compile_options.gpu_topology = options.gpu_topology();\n+  compile_options.gpu_target_config = options.gpu_target_config();\n \n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<Executable> executable,\n       RunBackend(std::move(hlo_module), options.executor(), compile_options));\n \n   std::vector<std::unique_ptr<AotCompilationResult>> results;\n-  ASSIGN_OR_RETURN(results.emplace_back(), Export(executable.get()));\n+  TF_ASSIGN_OR_RETURN(results.emplace_back(), Export(executable.get()));\n   return results;\n }\n \n@@ -2807,11 +2790,11 @@ GpuCompiler::EarlyExitCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n       AotCompilationOptions::EarlyExitPoint::kAfterLayoutAssignment;\n   CompileOptions compile_options;\n   compile_options.device_allocator = options.device_allocator();\n-  compile_options.gpu_topology = options.gpu_topology();\n+  compile_options.gpu_target_config = options.gpu_target_config();\n   compile_options.early_exit_with_layouts = early_exit_with_layouts;\n \n   std::vector<std::unique_ptr<AotCompilationResult>> results;\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       auto optimized_module,\n       RunHloPasses(std::move(hlo_module), options.executor(), compile_options));\n   results.push_back(std::make_unique<EarlyExitCompilationResult>(\n@@ -2831,32 +2814,30 @@ GpuCompiler::LegacyCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n     }};\n     CompileOptions compile_options;\n     compile_options.device_allocator = options.device_allocator();\n-    compile_options.gpu_topology = options.gpu_topology();\n-    ASSIGN_OR_RETURN(optimized_module,\n-                     RunHloPasses(std::move(hlo_module), options.executor(),\n-                                  compile_options));\n+    compile_options.gpu_target_config = options.gpu_target_config();\n+    TF_ASSIGN_OR_RETURN(optimized_module,\n+                        RunHloPasses(std::move(hlo_module), options.executor(),\n+                                     compile_options));\n   } else {\n     optimized_module = std::move(hlo_module);\n   }\n \n-  std::optional<Compiler::GpuTargetConfig> target_config;\n-  if (options.gpu_topology().has_value() &&\n-      options.gpu_topology()->has_gpu_target_config()) {\n-    target_config = options.gpu_topology()->gpu_target_config();\n-  }\n+\n+  const std::optional<Compiler::GpuTargetConfig>& target_config =\n+      options.gpu_target_config();\n   CHECK(target_config.has_value() || options.executor() != nullptr);\n   const se::DeviceDescription& gpu_device_info =\n       target_config.has_value() ? target_config->device_description\n                                 : options.executor()->GetDeviceDescription();\n   llvm::LLVMContext llvm_context;\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       CompileResultWithMetadata res,\n       CompileToBackendResult(optimized_module.get(), &llvm_context,\n                              {options.device_allocator()}, gpu_device_info));\n \n   // Create GpuThunkAotCompilationResult if thunk runtime is enabled.\n   std::vector<std::unique_ptr<AotCompilationResult>> results;\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       results.emplace_back(),\n       LegacyGpuAotCompilationResult::FromModule(\n           optimized_module.get(),\n@@ -2883,7 +2864,7 @@ absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(\n           .config()\n           .debug_options()\n           .xla_gpu_experimental_aot_compiled_thunks()) {\n-    ASSIGN_OR_RETURN(GpuExecutableProto proto, gpu_executable->ToProto());\n+    TF_ASSIGN_OR_RETURN(GpuExecutableProto proto, gpu_executable->ToProto());\n     return GpuAotCompilationResult::FromProto(std::move(proto));\n   }\n \n@@ -3144,7 +3125,7 @@ GpuCompiler::LoadExecutableFromAotResult(\n   const GpuExecutableProto& proto = gpu_aot_result->GetGpuExecutableProto();\n \n   // Recreate HloModule+HloModuleConfig from proto.\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<HloModule> hlo_module,\n       HloModule::CreateFromProtoWithConfig(proto.hlo_module_with_config()));\n \n@@ -3153,8 +3134,8 @@ GpuCompiler::LoadExecutableFromAotResult(\n   std::vector<uint8_t> binary(proto.binary().begin(), proto.binary().end());\n \n   // Build the executable, which should be a thunk sequence.\n-  ASSIGN_OR_RETURN(se::Platform * platform,\n-                   se::PlatformManager::PlatformWithId(PlatformId()));\n+  TF_ASSIGN_OR_RETURN(se::Platform * platform,\n+                      se::PlatformManager::PlatformWithId(PlatformId()));\n   std::string platform_name = platform->Name();\n \n   const se::DeviceDescription& gpu_device_info =\n@@ -3163,7 +3144,7 @@ GpuCompiler::LoadExecutableFromAotResult(\n \n   // Recreate BufferAssignment from proto.\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n-  ASSIGN_OR_RETURN(\n+  TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<BufferAssignment> buffer_assignment,\n       BufferAssignment::FromProto(proto.buffer_assignment(), hlo_module.get(),\n                                   BufferSizeBytesFunction(), alias_info.get()));\n@@ -3183,14 +3164,14 @@ GpuCompiler::LoadExecutableFromAotResult(\n   }\n \n   ThunkEmitter thunk_emitter(&ir_emitter_context);\n-  ASSIGN_OR_RETURN(auto sequential_thunk,\n-                   thunk_emitter.EmitHloEntryComputation(hlo_module.get()));\n+  TF_ASSIGN_OR_RETURN(auto sequential_thunk,\n+                      thunk_emitter.EmitHloEntryComputation(hlo_module.get()));\n \n   // Get all other fields required by GpuExecutable.\n   std::vector<GpuExecutable::ConstantInfo> constants =\n       std::move(ir_emitter_context.constants());\n-  ASSIGN_OR_RETURN(auto output_info,\n-                   GetOutputInfo(*hlo_module, *buffer_assignment));\n+  TF_ASSIGN_OR_RETURN(auto output_info,\n+                      GetOutputInfo(*hlo_module, *buffer_assignment));\n   ProgramShape program_shape =\n       hlo_module->entry_computation_layout().ComputeProgramShape();\n   *program_shape.mutable_result() = hlo_module->result_shape();"
        },
        {
            "sha": "08ef78d42de6030cb4daa3252a6417868c062b92",
            "filename": "third_party/xla/xla/service/gpu_topology.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.cc?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -88,10 +88,4 @@ absl::StatusOr<GpuTopology> GetGpuTopologyForPlatform(\n                      num_devices_per_host, std::move(gpu_target_config));\n }\n \n-GpuTopology GetSingleDeviceGpuTopology(\n-    absl::string_view platform_version,\n-    const gpu::GpuTargetConfig& gpu_target_config) {\n-  return GpuTopology(platform_version, 1, 1, 1, gpu_target_config);\n-}\n-\n }  // namespace xla"
        },
        {
            "sha": "0be662df014392406b64b50a904e09de379dbeb4",
            "filename": "third_party/xla/xla/service/gpu_topology.h",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.h?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -76,11 +76,11 @@ class GpuTopology {\n   }\n \n  private:\n-  std::string platform_version_;\n-  int32_t num_partitions_;\n-  int32_t num_hosts_per_partition_;\n-  int32_t num_devices_per_host_;\n-  std::optional<gpu::GpuTargetConfig> gpu_target_config_;\n+  const std::string platform_version_;\n+  const int32_t num_partitions_;\n+  const int32_t num_hosts_per_partition_;\n+  const int32_t num_devices_per_host_;\n+  const std::optional<gpu::GpuTargetConfig> gpu_target_config_;\n \n   bool is_topology_symmetric() const {\n     return num_partitions_ != -1 && num_hosts_per_partition_ != -1 &&\n@@ -92,10 +92,6 @@ absl::StatusOr<GpuTopology> GetGpuTopologyForPlatform(\n     absl::string_view platform_version, int32_t num_partitions,\n     int32_t num_hosts_per_partition, int32_t num_devices_per_host);\n \n-GpuTopology GetSingleDeviceGpuTopology(\n-    absl::string_view platform_version,\n-    const gpu::GpuTargetConfig& gpu_target_config);\n-\n }  // namespace xla\n \n #endif  // XLA_SERVICE_GPU_TOPOLOGY_H_"
        },
        {
            "sha": "de88faf03904248bed4df17fb78a57e1621696cd",
            "filename": "third_party/xla/xla/tools/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2FBUILD?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -1004,7 +1004,6 @@ tsl_gpu_library(\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:export_hlo\",\n-        \"//xla/service:gpu_topology\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:platform_util\","
        },
        {
            "sha": "60c82a6b17301320ade11e643bfb7b7e97ae1352",
            "filename": "third_party/xla/xla/tools/xla_compile_lib.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b26bc3d5963cf0be18ec77db7912b6f3d04d77e7/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc?ref=b26bc3d5963cf0be18ec77db7912b6f3d04d77e7",
            "patch": "@@ -44,7 +44,6 @@ limitations under the License.\n #include \"xla/service/export_hlo.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/gpu_symbol_repository.h\"\n-#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/platform_util.h\"\n@@ -95,9 +94,7 @@ static absl::StatusOr<std::string> CompileGpuExecutable(\n \n   if (aot) {\n     AotCompilationOptions aot_options(platform->id());\n-    GpuTopology topology =\n-        GetSingleDeviceGpuTopology(/*platform_version=*/\"\", *target_config);\n-    aot_options.set_gpu_topology(topology);\n+    aot_options.set_gpu_target_config(*target_config);\n     // We need the optimized module, so we call RunHloPasses ourselves above.\n     aot_options.set_run_backend_only(true);\n "
        }
    ],
    "stats": {
        "total": 220,
        "additions": 88,
        "deletions": 132
    }
}