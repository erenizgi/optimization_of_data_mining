{
    "author": "tensorflower-gardener",
    "message": "[IFRT] Use compile options to check if sdy partitioned\n\nPiperOrigin-RevId: 805399708",
    "sha": "5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
    "files": [
        {
            "sha": "fc2d2c07f8652c90820e8ba2a70e355ddbe8725c",
            "filename": "third_party/xla/xla/python/ifrt/ir/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2FBUILD?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -177,12 +177,14 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n+        \"@local_tsl//tsl/platform:human_readable_json\",\n     ],\n )\n "
        },
        {
            "sha": "6c175a0746fdf5bfb0c85b7388d7931c660dd409",
            "filename": "third_party/xla/xla/python/ifrt/ir/ifrt_ir_program.cc",
            "status": "modified",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fifrt_ir_program.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fifrt_ir_program.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fifrt_ir_program.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/python/ifrt/ir/ifrt_ir_program.h\"\n \n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <string>\n@@ -23,10 +24,14 @@ limitations under the License.\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/memory/memory.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"llvm/ADT/StringRef.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/CommandLine.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/python/ifrt/basic_device_list.h\"\n@@ -38,6 +43,7 @@ limitations under the License.\n #include \"xla/python/ifrt/serdes_version.h\"\n #include \"xla/python/pjrt_ifrt/xla_compiler.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"tsl/platform/human_readable_json.h\"\n \n namespace xla {\n namespace ifrt {\n@@ -140,5 +146,72 @@ absl::StatusOr<IfrtIrCompileOptionsProto> IfrtIRCompileOptions::ToProto(\n   return proto;\n }\n \n+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,\n+                              const IfrtIRCompileOptions& options) {\n+  absl::StatusOr<xla::ifrt::IfrtIrCompileOptionsProto> proto_or =\n+      options.ToProto();\n+  if (!proto_or.ok()) {\n+    os << \"Failed to convert IfrtIRCompileOptions to proto: \"\n+       << proto_or.status().ToString();\n+  } else {\n+    os << absl::StrCat(proto_or.value());\n+  }\n+  return os;\n+}\n+\n+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,\n+                              std::shared_ptr<IfrtIRCompileOptions> options) {\n+  os << *options;\n+  return os;\n+}\n+\n }  // namespace ifrt\n }  // namespace xla\n+\n+namespace llvm::cl {\n+\n+using ::xla::ifrt::IfrtIRCompileOptions;\n+\n+//===----------------------------------------------------------------------===//\n+// IfrtIRCompileOptions\n+//===----------------------------------------------------------------------===//\n+\n+template class basic_parser<std::shared_ptr<IfrtIRCompileOptions>>;\n+\n+bool parser<std::shared_ptr<IfrtIRCompileOptions>>::parse(\n+    Option& opt, StringRef, StringRef arg,\n+    std::shared_ptr<IfrtIRCompileOptions>& value) {\n+  auto proto = std::make_unique<xla::ifrt::IfrtIrCompileOptionsProto>();\n+  absl::Status decode_json_status =\n+      tsl::HumanReadableJsonToProto(arg.str(), proto.get());\n+  if (!decode_json_status.ok()) {\n+    return opt.error(\n+        \"Failed to parse IfrtIRCompileOptions from JSON \"\n+        \"string.\\n\\nParsing error: \" +\n+        decode_json_status.ToString() + \".\\n\\n String input: \" + arg);\n+  }\n+  absl::StatusOr<std::unique_ptr<IfrtIRCompileOptions>> options_or =\n+      IfrtIRCompileOptions::FromProto(*proto);\n+  if (!options_or.ok()) {\n+    return opt.error(\"Failed to create IfrtIRCompileOptions from proto: \" +\n+                     options_or.status().ToString());\n+  }\n+\n+  value = absl::ShareUniquePtr(std::move(*options_or));\n+  return false;\n+}\n+\n+void parser<std::shared_ptr<IfrtIRCompileOptions>>::printOptionDiff(\n+    const Option& opt, const std::shared_ptr<IfrtIRCompileOptions>& value,\n+    const OptVal& defaultValue, size_t globalWidth) const {\n+  printOptionName(opt, globalWidth);\n+  outs() << \"= \" << value;\n+  if (defaultValue.hasValue()) {\n+    outs().indent(2) << \" (default: \" << defaultValue.getValue() << \")\";\n+  }\n+  outs() << \"\\n\";\n+}\n+\n+void parser<std::shared_ptr<IfrtIRCompileOptions>>::anchor() {}\n+\n+}  // namespace llvm::cl"
        },
        {
            "sha": "6897cdf121fc2f69478b3d1b16edcdc8647414dd",
            "filename": "third_party/xla/xla/python/ifrt/ir/ifrt_ir_program.h",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fifrt_ir_program.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fifrt_ir_program.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fifrt_ir_program.h?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_PYTHON_IFRT_IR_IFRT_IR_PROGRAM_H_\n #define XLA_PYTHON_IFRT_IR_IFRT_IR_PROGRAM_H_\n \n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <string>\n@@ -25,7 +26,10 @@ limitations under the License.\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n@@ -176,11 +180,39 @@ struct IfrtIRCompileOptions\n   static char ID;  // NOLINT\n };\n \n+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,\n+                              const IfrtIRCompileOptions& options);\n+\n+llvm::raw_ostream& operator<<(llvm::raw_ostream& os,\n+                              std::shared_ptr<IfrtIRCompileOptions> options);\n+\n // Gets `xla::ifrt::IfrtIRCompileOptions` from `xla::ifrt::CompileOptions`.\n absl::StatusOr<std::unique_ptr<IfrtIRCompileOptions>> GetIfrtIRCompileOptions(\n     std::unique_ptr<CompileOptions> options);\n \n }  // namespace ifrt\n }  // namespace xla\n \n+namespace llvm::cl {\n+\n+extern template class basic_parser<\n+    std::shared_ptr<xla::ifrt::IfrtIRCompileOptions>>;\n+\n+template <>\n+class parser<std::shared_ptr<xla::ifrt::IfrtIRCompileOptions>>\n+    : public basic_parser<std::shared_ptr<xla::ifrt::IfrtIRCompileOptions>> {\n+ public:\n+  explicit parser(Option& opt) : basic_parser(opt) {}\n+  bool parse(Option& opt, StringRef argName, StringRef arg,\n+             std::shared_ptr<xla::ifrt::IfrtIRCompileOptions>& value);\n+  StringRef getValueName() const override { return \"ifrt-ir-compile-options\"; }\n+  void printOptionDiff(\n+      const Option& opt,\n+      const std::shared_ptr<xla::ifrt::IfrtIRCompileOptions>& value,\n+      const OptVal& defaultValue, size_t globalWidth) const;\n+  void anchor() override;\n+};\n+\n+}  // namespace llvm::cl\n+\n #endif  // XLA_PYTHON_IFRT_IR_IFRT_IR_PROGRAM_H_"
        },
        {
            "sha": "ee92a4e0d3f01a01d2e1d401dd0884001c7b5ae1",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt_lower_atom_program_metadata_to_xla-gspmd_partitioned_compile_override.mlir",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-gspmd_partitioned_compile_override.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-gspmd_partitioned_compile_override.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-gspmd_partitioned_compile_override.mlir?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -0,0 +1,136 @@\n+// RUN: ifrt-opt %s -ifrt-lower-atom-program-metadata-to-xla='compile_options={{\"compile_option_overrides\": {\"test_override\": {\"executable_build_options\": {\"use_shardy_partitioner\": false}}}}}' -split-input-file -verify-diagnostics 2>&1 | FileCheck %s\n+\n+// CHECK-LABEL: @arg_metadata\n+module @arg_metadata attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: %arg0: tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-DAG:    mhlo.sharding = \"{devices=[2,1]<=[2]}\"\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-DAG:    ifrt.memory_kind = \"device\"\n+  // CHECK-DAG:    mhlo.memory_kind = \"device\"\n+  // CHECK-SAME: }\n+  // CHECK: %arg1: tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-DAG:    mhlo.sharding = \"{replicated}\"\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<1x1 to [0] on 2>\n+  // CHECK-SAME: }\n+  func.func @main(\n+      %arg0: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>,\n+        ifrt.memory_kind = \"device\"},\n+      %arg1: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<1x1 to [0] on 2>}) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @arg_unspecified_sharding\n+module @arg_unspecified_sharding attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: %arg0: tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-DAG:    mhlo.sharding = \"{devices=[2,1]<=[2]}\"\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-SAME: }\n+  // CHECK: %arg1: tensor<2x2xi32> {ifrt.sharding = #ifrt.sharding_unspecified})\n+  func.func @main(\n+      %arg0: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>},\n+      %arg1: tensor<2x2xi32> {ifrt.sharding=#ifrt.sharding_unspecified}) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @result_metadata\n+module @result_metadata attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: -> (tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-DAG:    mhlo.sharding = \"{devices=[2,1]<=[2]}\"\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-DAG:    ifrt.memory_kind = \"device\"\n+  // CHECK-DAG:    mhlo.memory_kind = \"device\"\n+  // CHECK-SAME: }\n+  func.func @main()\n+      -> (tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>,\n+        ifrt.memory_kind = \"device\"}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @result_unspecified_sharding\n+module @result_unspecified_sharding attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: -> (tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-DAG:    mhlo.sharding = \"{devices=[2,1]<=[2]}\"\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-SAME: }\n+  // CHECK: tensor<2x2xi32> {ifrt.sharding = #ifrt.sharding_unspecified})\n+  func.func @main()\n+      -> (tensor<2x2xi32> {\n+            ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>},\n+          tensor<2x2xi32> {ifrt.sharding=#ifrt.sharding_unspecified}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0, %0 : tensor<2x2xi32>, tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+module @arg_missing_sharding attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // expected-error @+1 {{'func.func' op can't find `ifrt.sharding` attribute of input #0 to set `mhlo.sharding` attribute}}\n+  func.func @main(%arg0: tensor<2x2xi32>) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+module @result_missing_sharding attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // expected-error @+1 {{'func.func' op can't find `ifrt.sharding` attribute of output #0 to set `mhlo.sharding` attribute}}\n+  func.func @main() -> (tensor<2x2xi32>) {\n+     %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+     return %0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// expected-error @+1 {{'builtin.module' op module `module_missing_devices` must have `ifrt.num_devices` attribute}}\n+module @module_missing_devices attributes {ifrt.compile_options_key = \"test_override\"} {\n+  func.func @main() -> (tensor<2x2xi32>\n+     {ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>,\n+       ifrt.devices=#ifrt<devices[1]>}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// expected-error @+2 {{'func.func' op can't lower sharding of input #0. Sharding: #ifrt.sharding_param<1x1 to [0] on 1> uses 1 devices while computation uses 2 devices}}\n+module @arg_w_different_num_devices attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  func.func @main(\n+      %arg0: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<1x1 to [0] on 1>}) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// expected-error @+2 {{'func.func' op can't lower sharding of output #0. Sharding: #ifrt.sharding_param<2x1 to [0] on 2> uses 2 devices while computation uses 4 devices}}\n+module @res_w_different_num_devices attributes {ifrt.num_devices = 4, ifrt.compile_options_key = \"test_override\"} {\n+  func.func @main()\n+      -> (tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0 : tensor<2x2xi32>\n+  }\n+}"
        },
        {
            "sha": "398e829ee30f98a3bcaf1721fd107a27a6f9f73e",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt_lower_atom_program_metadata_to_xla-sdy_partitioned_compile_override.mlir",
            "status": "added",
            "additions": 140,
            "deletions": 0,
            "changes": 140,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-sdy_partitioned_compile_override.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-sdy_partitioned_compile_override.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-sdy_partitioned_compile_override.mlir?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -0,0 +1,140 @@\n+// RUN: ifrt-opt %s -ifrt-lower-atom-program-metadata-to-xla='compile_options={{\"compile_option_overrides\": {\"test_override\": {\"executable_build_options\": {\"use_shardy_partitioner\": true}}}}}' -split-input-file -verify-diagnostics 2>&1 | FileCheck %s\n+\n+// CHECK-LABEL: @arg_metadata_sdy_partitioned\n+module @arg_metadata_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: %arg0: tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-NOT:    mhlo.sharding\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-DAG:    ifrt.memory_kind = \"device\"\n+  // CHECK-DAG:    mhlo.memory_kind = \"device\"\n+  // CHECK-SAME: }\n+  // CHECK: %arg1: tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-NOT:    mhlo.sharding\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<1x1 to [0] on 2>\n+  // CHECK-SAME: }\n+  func.func @main(\n+      %arg0: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>,\n+        ifrt.memory_kind = \"device\"},\n+      %arg1: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<1x1 to [0] on 2>}) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @arg_unspecified_sharding_sdy_partitioned\n+module @arg_unspecified_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: %arg0: tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-NOT:    mhlo.sharding\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-SAME: }\n+  // CHECK: %arg1: tensor<2x2xi32> {ifrt.sharding = #ifrt.sharding_unspecified})\n+  func.func @main(\n+      %arg0: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>},\n+      %arg1: tensor<2x2xi32> {ifrt.sharding=#ifrt.sharding_unspecified}) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @result_metadata_sdy_partitioned\n+module @result_metadata_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: -> (tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-NOT:    mhlo.sharding\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-DAG:    ifrt.memory_kind = \"device\"\n+  // CHECK-DAG:    mhlo.memory_kind = \"device\"\n+  // CHECK-SAME: }\n+  func.func @main()\n+      -> (tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>,\n+        ifrt.memory_kind = \"device\"}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @result_unspecified_sharding_sdy_partitioned\n+module @result_unspecified_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: -> (tensor<2x2xi32>\n+  // CHECK-SAME: {\n+  // CHECK-NOT:    mhlo.sharding\n+  // CHECK-DAG:    ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-SAME: }\n+  // CHECK: tensor<2x2xi32> {ifrt.sharding = #ifrt.sharding_unspecified})\n+  func.func @main()\n+      -> (tensor<2x2xi32> {\n+            ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>},\n+          tensor<2x2xi32> {ifrt.sharding=#ifrt.sharding_unspecified}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0, %0 : tensor<2x2xi32>, tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @arg_missing_sharding_sdy_partitioned\n+module @arg_missing_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: %arg0: tensor<2x2xi32>\n+  // CHECK-NOT: mhlo.sharding\n+  func.func @main(%arg0: tensor<2x2xi32>) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @result_missing_sharding_sdy_partitioned\n+module @result_missing_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  // CHECK: -> tensor<2x2xi32>\n+  // CHECK-NOT: mhlo.sharding\n+  func.func @main() -> (tensor<2x2xi32>) {\n+     %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+     return %0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// expected-error @+1 {{'builtin.module' op module `module_missing_devices_sdy_partitioned` must have `ifrt.num_devices` attribute}}\n+module @module_missing_devices_sdy_partitioned attributes {ifrt.compile_options_key = \"test_override\"} {\n+  func.func @main() -> (tensor<2x2xi32>\n+     {ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>,\n+       ifrt.devices=#ifrt<devices[1]>}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+// expected-error @+2 {{'func.func' op can't lower sharding of input #0. Sharding: #ifrt.sharding_param<1x1 to [0] on 1> uses 1 devices while computation uses 2 devices}}\n+module @arg_w_different_num_devices_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.compile_options_key = \"test_override\"} {\n+  func.func @main(\n+      %arg0: tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<1x1 to [0] on 1>}) {\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// expected-error @+2 {{'func.func' op can't lower sharding of output #0. Sharding: #ifrt.sharding_param<2x1 to [0] on 2> uses 2 devices while computation uses 4 devices}}\n+module @res_w_different_num_devices_sdy_partitioned attributes {ifrt.num_devices = 4, ifrt.compile_options_key = \"test_override\"} {\n+  func.func @main()\n+      -> (tensor<2x2xi32> {\n+        ifrt.sharding=#ifrt.sharding_param<2x1 to [0] on 2>}) {\n+    %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n+    return %0 : tensor<2x2xi32>\n+  }\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "2d40e7bdc217be11b82187af305996b3b4404ef9",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt_lower_atom_program_metadata_to_xla-use_partitioned_attr.mlir",
            "status": "renamed",
            "additions": 9,
            "deletions": 34,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-use_partitioned_attr.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-use_partitioned_attr.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_lower_atom_program_metadata_to_xla-use_partitioned_attr.mlir?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -1,5 +1,5 @@\n-// RUN: ifrt-opt %s -ifrt-lower-atom-program-metadata-to-xla -split-input-file -verify-diagnostics | FileCheck %s\n \n+// RUN: ifrt-opt %s -ifrt-lower-atom-program-metadata-to-xla -split-input-file -verify-diagnostics | FileCheck %s\n // CHECK-LABEL: @arg_metadata\n module @arg_metadata attributes {ifrt.num_devices = 2} {\n   // CHECK: %arg0: tensor<2x2xi32>\n@@ -23,9 +23,8 @@ module @arg_metadata attributes {ifrt.num_devices = 2} {\n     return\n   }\n }\n-\n // -----\n-\n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // CHECK-LABEL: @arg_metadata_sdy_partitioned\n module @arg_metadata_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_sdy_partitioned} {\n   // CHECK: %arg0: tensor<2x2xi32>\n@@ -50,7 +49,6 @@ module @arg_metadata_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_s\n   }\n }\n // -----\n-\n // CHECK-LABEL: @arg_unspecified_sharding\n module @arg_unspecified_sharding attributes {ifrt.num_devices = 2} {\n   // CHECK: %arg0: tensor<2x2xi32>\n@@ -66,9 +64,8 @@ module @arg_unspecified_sharding attributes {ifrt.num_devices = 2} {\n     return\n   }\n }\n-\n // -----\n-\n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // CHECK-LABEL: @arg_unspecified_sharding_sdy_partitioned\n module @arg_unspecified_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_sdy_partitioned} {\n   // CHECK: %arg0: tensor<2x2xi32>\n@@ -84,9 +81,7 @@ module @arg_unspecified_sharding_sdy_partitioned attributes {ifrt.num_devices =\n     return\n   }\n }\n-\n // -----\n-\n // CHECK-LABEL: @result_metadata\n module @result_metadata attributes {ifrt.num_devices = 2} {\n   // CHECK: -> (tensor<2x2xi32>\n@@ -104,9 +99,8 @@ module @result_metadata attributes {ifrt.num_devices = 2} {\n     return %0 : tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // CHECK-LABEL: @result_metadata_sdy_partitioned\n module @result_metadata_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_sdy_partitioned} {\n   // CHECK: -> (tensor<2x2xi32>\n@@ -124,9 +118,7 @@ module @result_metadata_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.i\n     return %0 : tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n // CHECK-LABEL: @result_unspecified_sharding\n module @result_unspecified_sharding attributes {ifrt.num_devices = 2} {\n   // CHECK: -> (tensor<2x2xi32>\n@@ -143,9 +135,8 @@ module @result_unspecified_sharding attributes {ifrt.num_devices = 2} {\n     return %0, %0 : tensor<2x2xi32>, tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // CHECK-LABEL: @result_unspecified_sharding_sdy_partitioned\n module @result_unspecified_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_sdy_partitioned} {\n   // CHECK: -> (tensor<2x2xi32>\n@@ -162,19 +153,15 @@ module @result_unspecified_sharding_sdy_partitioned attributes {ifrt.num_devices\n     return %0, %0 : tensor<2x2xi32>, tensor<2x2xi32>\n   }\n }\n-\n-\n // -----\n-\n module @arg_missing_sharding attributes {ifrt.num_devices = 2} {\n   // expected-error @+1 {{'func.func' op can't find `ifrt.sharding` attribute of input #0 to set `mhlo.sharding` attribute}}\n   func.func @main(%arg0: tensor<2x2xi32>) {\n     return\n   }\n }\n-\n // -----\n-\n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // CHECK-LABEL: @arg_missing_sharding_sdy_partitioned\n module @arg_missing_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_sdy_partitioned} {\n   // CHECK: %arg0: tensor<2x2xi32>\n@@ -183,19 +170,16 @@ module @arg_missing_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, i\n     return\n   }\n }\n-\n // -----\n-\n module @result_missing_sharding attributes {ifrt.num_devices = 2} {\n   // expected-error @+1 {{'func.func' op can't find `ifrt.sharding` attribute of output #0 to set `mhlo.sharding` attribute}}\n   func.func @main() -> (tensor<2x2xi32>) {\n      %0 = mhlo.constant dense<1> : tensor<2x2xi32>\n      return %0 : tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // CHECK-LABEL: @result_missing_sharding_sdy_partitioned\n module @result_missing_sharding_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_sdy_partitioned} {\n   // CHECK: -> tensor<2x2xi32>\n@@ -205,9 +189,7 @@ module @result_missing_sharding_sdy_partitioned attributes {ifrt.num_devices = 2\n      return %0 : tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n // expected-error @+1 {{'builtin.module' op module `module_missing_devices` must have `ifrt.num_devices` attribute}}\n module @module_missing_devices {\n   func.func @main() -> (tensor<2x2xi32>\n@@ -217,9 +199,7 @@ module @module_missing_devices {\n     return %0 : tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n // expected-error @+1 {{'builtin.module' op module `module_missing_devices_sdy_partitioned` must have `ifrt.num_devices` attribute}}\n module @module_missing_devices_sdy_partitioned attributes {ifrt.is_sdy_partitioned} {\n   func.func @main() -> (tensor<2x2xi32>\n@@ -229,9 +209,7 @@ module @module_missing_devices_sdy_partitioned attributes {ifrt.is_sdy_partition\n     return %0 : tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n // expected-error @+2 {{'func.func' op can't lower sharding of input #0. Sharding: #ifrt.sharding_param<1x1 to [0] on 1> uses 1 devices while computation uses 2 devices}}\n module @arg_w_different_num_devices attributes {ifrt.num_devices = 2} {\n   func.func @main(\n@@ -240,9 +218,9 @@ module @arg_w_different_num_devices attributes {ifrt.num_devices = 2} {\n     return\n   }\n }\n-\n // -----\n \n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // expected-error @+2 {{'func.func' op can't lower sharding of input #0. Sharding: #ifrt.sharding_param<1x1 to [0] on 1> uses 1 devices while computation uses 2 devices}}\n module @arg_w_different_num_devices_sdy_partitioned attributes {ifrt.num_devices = 2, ifrt.is_sdy_partitioned} {\n   func.func @main(\n@@ -251,9 +229,7 @@ module @arg_w_different_num_devices_sdy_partitioned attributes {ifrt.num_devices\n     return\n   }\n }\n-\n // -----\n-\n // expected-error @+2 {{'func.func' op can't lower sharding of output #0. Sharding: #ifrt.sharding_param<2x1 to [0] on 2> uses 2 devices while computation uses 4 devices}}\n module @res_w_different_num_devices attributes {ifrt.num_devices = 4} {\n   func.func @main()\n@@ -263,9 +239,8 @@ module @res_w_different_num_devices attributes {ifrt.num_devices = 4} {\n     return %0 : tensor<2x2xi32>\n   }\n }\n-\n // -----\n-\n+// expected-warning @+2 {{`ifrt.is_sdy_partitioned` attribute is deprecated and will be removed. See b/433244129. Please use `compile_options_override` to specify sharding.}}\n // expected-error @+2 {{'func.func' op can't lower sharding of output #0. Sharding: #ifrt.sharding_param<2x1 to [0] on 2> uses 2 devices while computation uses 4 devices}}\n module @res_w_different_num_devices_sdy_partitioned attributes {ifrt.num_devices = 4, ifrt.is_sdy_partitioned} {\n   func.func @main()",
            "previous_filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt_lower_atom_program_metadata_to_xla.mlir"
        },
        {
            "sha": "55bce577dbf0c15c9a35c17d6c19b2ff426a798e",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt_populate_atom_program_metadata.mlir",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_populate_atom_program_metadata.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_populate_atom_program_metadata.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_populate_atom_program_metadata.mlir?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -57,6 +57,34 @@ module @populate_sdy_partitioned_attr {\n \n // -----\n \n+!array = !ifrt.array<tensor<2x2xi32>,\n+                     #ifrt.sharding_param<2x1 to [0] on 2>, [0,1]>\n+// CHECK-LABEL: @populate_compile_options_key_attr\n+module @populate_compile_options_key_attr {\n+  func.func @main(%arg0: !array) attributes {ifrt.function} {\n+    // CHECK: ifrt.Call @[[CALLEE:.+]]::@main(%arg0)\n+    %ctrl_0 = ifrt.Call @callee::@main(%arg0) on devices [0,1] {ifrt.compile_options_key = \"foo\"} : (!array) -> ()\n+    return\n+  }\n+\n+  // CHECK: module @[[CALLEE]]\n+  // CHECK-SAME: attributes {\n+  // CHECK-DAG:    ifrt.compile_options_key = \"foo\"\n+  // CHECK-DAG:    ifrt.num_devices = 2\n+  // CHECK-DAG:    sym_visibility = \"private\"\n+  // CHECK-SAME: }\n+  // CHECK: func.func private @main\n+  // CHECK-DAG: ifrt.sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n+  // CHECK-NOT: ifrt\n+  module @callee attributes {sym_visibility = \"private\"} {\n+    func.func private @main(%arg0: tensor<2x2xi32>) {\n+      return\n+    }\n+  }\n+}\n+\n+// -----\n+\n // CHECK-LABEL: @populate_result_metadata\n module @populate_result_metadata {\n   func.func @main() attributes {ifrt.function} {"
        },
        {
            "sha": "e38670526121a31b48812dd2cef8ba888b5e8e93",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt_precompile_atom_program_preprocessing.mlir",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_precompile_atom_program_preprocessing.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_precompile_atom_program_preprocessing.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_precompile_atom_program_preprocessing.mlir?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -1,4 +1,5 @@\n-// RUN: ifrt-opt %s -ifrt-precompile-atom-program-preprocessing='platform_names=tpu,tpu' -split-input-file -verify-diagnostics | FileCheck %s\n+// RUN: ifrt-opt %s -ifrt-precompile-atom-program-preprocessing='platform_names=tpu,tpu compile_options={{\"compile_option_overrides\": {\"test_override\": {\"executable_build_options\": {\"use_shardy_partitioner\": true}}}}}' -split-input-file -verify-diagnostics | FileCheck %s --check-prefixes=CHECK,CHECK-SDY\n+// RUN: ifrt-opt %s -ifrt-precompile-atom-program-preprocessing='platform_names=tpu,tpu compile_options={{\"compile_option_overrides\": {\"test_override\": {\"executable_build_options\": {\"use_shardy_partitioner\": false}}}}}' -split-input-file -verify-diagnostics | FileCheck %s --check-prefixes=CHECK,CHECK-MHLO\n \n #sharding = #ifrt.sharding_param<2x1 to [0] on 2>\n !array = !ifrt.array<tensor<2x2xi32>, #sharding, [0, 1]>\n@@ -16,12 +17,17 @@ module @call_twice {\n \n   // CHECK: module @[[MODULE]] attributes {sym_visibility = \"private\"}\n   // CHECK: func.func @main\n-  // CHECK: %arg0: tensor<2x2xi32> {mhlo.sharding = \"{devices=[2,1]<=[2]}\"}\n-  // CHECK: (tensor<2x2xi32> {mhlo.sharding = \"{devices=[2,1]<=[2]}\"})\n+  // CHECK:  %arg0: tensor<2x2xi32>\n+  // CHECK-SDY-NOT: mhlo.sharding\n+  // CHECK-MHLO: mhlo.sharding = \"{devices=[2,1]<=[2]}\"\n+  // CHECK: tensor<2x2xi32>\n+  // CHECK-SDY-NOT:  mhlo.sharding\n+  // CHECK-MHLO: mhlo.sharding = \"{devices=[2,1]<=[2]}\"}\n   // CHECK-NOT: ifrt\n   module @add_one attributes {\n         ifrt.num_devices = 2,\n-        sym_visibility = \"private\"} {\n+        sym_visibility = \"private\",\n+        ifrt.compile_options_key = \"test_override\"} {\n     func.func @main(%arg0: tensor<2x2xi32> {\n         ifrt.sharding = #sharding, ifrt.devices = #ifrt<devices[0, 1]>\n     }) -> (tensor<2x2xi32> {"
        },
        {
            "sha": "434abc53f13cd43ebf4311c85f6f9154463cbe28",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt_remove_ifrt_attrs.mlir",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_remove_ifrt_attrs.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_remove_ifrt_attrs.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt_remove_ifrt_attrs.mlir?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -2,7 +2,11 @@\n \n // CHECK-LABEL: @ifrt_attributes_are_removed\n // CHECK-NOT: ifrt\n-module @ifrt_attributes_are_removed attributes {ifrt.num_devices = 2} {\n+module @ifrt_attributes_are_removed attributes {\n+        ifrt.num_devices = 2,\n+        ifrt.compile_options_key = \"test_override\",\n+        ifrt.is_sdy_partitioned,\n+        ifrt.local_view} {\n   func.func @main(\n       %arg0: tensor<2x2xi32> {\n         ifrt.sharding = #ifrt.sharding_param<1x1 to [0] on 1>})"
        },
        {
            "sha": "9cc1a6924bb626b33908ab841978440b34553d87",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2FBUILD?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -84,7 +84,6 @@ cc_library(\n         \"//xla/python/ifrt/ir:version\",\n         \"//xla/python/ifrt/ir:vifrt\",\n         \"//xla/python/ifrt/support:sharding_conversions\",\n-        \"//xla/python/pjrt_ifrt:xla_ifrt\",\n         \"//xla/service:compilation_environments\",\n         \"//xla/service:computation_placer_hdr\",\n         \"//xla/service:hlo_proto_cc\",\n@@ -112,7 +111,6 @@ cc_library(\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n-        \"@llvm-project//mlir:RegisterAllDialects\",\n         \"@llvm-project//mlir:Rewrite\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TransformUtils\",\n@@ -149,9 +147,12 @@ cc_library(\n     deps = [\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/mlir/utils:type_util\",\n+        \"//xla/pjrt:pjrt_executable\",\n         \"//xla/python/ifrt\",\n         \"//xla/python/ifrt/ir\",\n         \"//xla/python/pjrt_ifrt:pjrt_dtype\",\n+        \"//xla/python/pjrt_ifrt:xla_ifrt\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\","
        },
        {
            "sha": "66c8aa2d6a3eb92ee98539c360b9a872da152931",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/ifrt_lower_atom_program_metadata_to_xla_pass.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 7,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_lower_atom_program_metadata_to_xla_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_lower_atom_program_metadata_to_xla_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_lower_atom_program_metadata_to_xla_pass.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -14,7 +14,9 @@ limitations under the License.\n ==============================================================================*/\n \n #include <memory>\n+#include <optional>\n \n+#include \"absl/status/statusor.h\"\n #include \"llvm/Support/Casting.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/Builders.h\"\n@@ -23,6 +25,7 @@ limitations under the License.\n #include \"mlir/IR/Visitors.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/python/ifrt/ir/constants.h\"\n #include \"xla/python/ifrt/ir/ifrt_dialect.h\"\n #include \"xla/python/ifrt/ir/ifrt_interfaces.h\"\n@@ -51,6 +54,10 @@ class IfrtLowerAtomProgramMetadataToXlaPass\n     : public impl::IfrtLowerAtomProgramMetadataToXlaPassBase<\n           IfrtLowerAtomProgramMetadataToXlaPass> {\n  public:\n+  using impl::IfrtLowerAtomProgramMetadataToXlaPassBase<\n+      IfrtLowerAtomProgramMetadataToXlaPass>::\n+      IfrtLowerAtomProgramMetadataToXlaPassBase;\n+\n   void runOnOperation() override;\n };\n \n@@ -66,7 +73,37 @@ void IfrtLowerAtomProgramMetadataToXlaPass::runOnOperation() {\n     signalPassFailure();\n     return;\n   }\n-  const bool is_sdy = module_op->hasAttr(kIsSdyPartitioned);\n+\n+  // If the ModuleOp has a compile options key, then try to use the provided\n+  // compile options.\n+  auto compile_options_key =\n+      module_op->getAttrOfType<mlir::StringAttr>(kIfrtCompileOptionsKey);\n+  absl::StatusOr<std::optional<xla::CompileOptions>>\n+      compile_options_override_or = GetModuleXlaCompileOverrides(\n+          compile_options_key, compile_options->compile_options_overrides);\n+\n+  if (!compile_options_override_or.ok()) {\n+    module_op.emitOpError()\n+        << \"Unexpected error getting compile options override: \"\n+        << compile_options_override_or.status().message();\n+    signalPassFailure();\n+    return;\n+  }\n+\n+  // TODO(b/433244129) - Do not use kIsSdyPartitioned attr and default to true\n+  // after 6 month bwd compatibility window.\n+  bool is_sdy = module_op->hasAttr(kIsSdyPartitioned);\n+  if (is_sdy) {\n+    module_op.emitWarning()\n+        << \"`\" << kIsSdyPartitioned\n+        << \"` attribute is deprecated and will be removed. See b/433244129.\"\n+           \" Please use `compile_options_override` to specify sharding.\";\n+  }\n+  if (compile_options_override_or->has_value()) {\n+    is_sdy = compile_options_override_or->value()\n+                 .executable_build_options.use_shardy_partitioner();\n+  }\n+\n   int num_devices = num_devices_attr.getInt();\n   mlir::func::FuncOp func_op = GetMainFunction(module_op);\n   auto local_view_attr =\n@@ -209,11 +246,5 @@ void IfrtLowerAtomProgramMetadataToXlaPass::runOnOperation() {\n }\n \n }  // namespace\n-\n-std::unique_ptr<mlir::OperationPass<mlir::ModuleOp>>\n-CreateIfrtLowerAtomProgramMetadataToXlaPass() {\n-  return std::make_unique<IfrtLowerAtomProgramMetadataToXlaPass>();\n-}\n-\n }  // namespace ifrt\n }  // namespace xla"
        },
        {
            "sha": "85f345abfcab1921873c22b3676d19a308761a3b",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/ifrt_populate_atom_program_metadata_pass.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_populate_atom_program_metadata_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_populate_atom_program_metadata_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_populate_atom_program_metadata_pass.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -55,6 +55,11 @@ mlir::LogicalResult PopulateMetadata(CallOp call_op, mlir::ModuleOp module_op,\n   if (call_op->hasAttr(kIsSdyPartitioned)) {\n     module_op->setAttr(kIsSdyPartitioned, builder.getUnitAttr());\n   }\n+  // Copy ifrt.compile_options_key if it exists.\n+  if (call_op->hasAttr(kIfrtCompileOptionsKey)) {\n+    module_op->setAttr(kIfrtCompileOptionsKey,\n+                       call_op->getAttr(kIfrtCompileOptionsKey));\n+  }\n   // Copy `ifrt.local_view` attribute if it exists.\n   if (call_op->hasAttrOfType<mlir::UnitAttr>(kIfrtLocalViewAttrName)) {\n     module_op->setAttr(kIfrtLocalViewAttrName,"
        },
        {
            "sha": "ebf998409631a474fa8d80ef0037e0a85d7d4bae",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/ifrt_precompile_atom_program_preprocessing_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_precompile_atom_program_preprocessing_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_precompile_atom_program_preprocessing_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_precompile_atom_program_preprocessing_pass.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -177,7 +177,7 @@ void IfrtPrecompileAtomProgramPreprocessingPass::runOnOperation() {\n     auto callee_module = llvm::dyn_cast<mlir::ModuleOp>(callee->getParentOp());\n     mlir::OpPassManager pm(mlir::ModuleOp::getOperationName());\n     if (module_type_attr == kIfrtModuleTypeXla) {\n-      createIfrtCompileXlaPreprocessingPipeline(pm);\n+      createIfrtCompileXlaPreprocessingPipeline(pm, compile_options);\n     } else if (module_type_attr != kIfrtModuleTypeMpmdReshard) {\n       return call_op.emitOpError()\n              << \"module type \" << module_type_attr << \" is not supported\";"
        },
        {
            "sha": "8f1eef5a7121c54dae51125758f50c364644c740",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/ifrt_remove_ifrt_attrs_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_remove_ifrt_attrs_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_remove_ifrt_attrs_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_remove_ifrt_attrs_pass.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -40,6 +40,7 @@ void IfrtRemoveIfrtAttrsPass::runOnOperation() {\n   mlir::ModuleOp module_op = getOperation();\n   module_op->removeAttr(kIfrtNumDevicesAttrName);\n   module_op->removeAttr(kIfrtLocalViewAttrName);\n+  module_op->removeAttr(kIfrtCompileOptionsKey);\n   module_op->removeAttr(kIsSdyPartitioned);\n   module_op.walk([&](mlir::func::FuncOp func_op) {\n     // Remove from function attributes."
        },
        {
            "sha": "20ed09e4b663d0e34bbd819f12478c1687d98b7b",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/multi_threaded_atom_program_compiler.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 56,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/python/ifrt/ir/transforms/multi_threaded_atom_program_compiler.h\"\n \n #include <memory>\n+#include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n@@ -27,7 +28,6 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n-#include \"llvm/Support/Casting.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n@@ -47,7 +47,6 @@ limitations under the License.\n #include \"xla/python/ifrt/ir/ifrt_ops.h\"\n #include \"xla/python/ifrt/ir/transforms/utils.h\"\n #include \"xla/python/ifrt/shape.h\"\n-#include \"xla/python/pjrt_ifrt/xla_compiler.h\"\n #include \"xla/service/compilation_environments.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/hlo.pb.h\"\n@@ -147,55 +146,43 @@ MultiThreadedAtomProgramCompiler::GetXlaCompileOptions(\n   // compile options.\n   auto compile_options_key =\n       call_op->getAttrOfType<mlir::StringAttr>(kIfrtCompileOptionsKey);\n-  bool has_compile_options = false;\n-  if (compile_options_overrides_ != nullptr && compile_options_key != nullptr) {\n-    if (auto compile_options_override =\n-            compile_options_overrides_->find(compile_options_key.str());\n-        compile_options_override != compile_options_overrides_->end()) {\n-      if (auto xla_options = llvm::dyn_cast<XlaCompileOptions>(\n-              compile_options_override->second.get())) {\n-        compile_options = xla_options->compile_options;\n-        has_compile_options = true;\n-      } else {\n-        return absl::InvalidArgumentError(absl::StrCat(\n-            \"The `\", kIfrtCompileOptionsKey.str(), \"` compile options key `\",\n-            compile_options_key.str(),\n-            \"` has an entry that is not of type `XlaCompileOptions`, but the \"\n-            \"atom program is an XLA program.\"));\n-      }\n-    }\n+  TF_ASSIGN_OR_RETURN(\n+      std::optional<xla::CompileOptions> compile_options_override,\n+      GetModuleXlaCompileOverrides(compile_options_key,\n+                                   compile_options_overrides_));\n+\n+  if (compile_options_override.has_value()) {\n+    return compile_options_override.value();\n   }\n \n-  if (!has_compile_options) {\n-    auto& exec_build_options = compile_options.executable_build_options;\n-    // Executable build options are constructed using logical ids, which are\n-    // later converted into real Device ids by using the logical ids as\n-    // indices into the device list given at compilation invocation time.\n-    llvm::ArrayRef<int> logical_device_ids = call_op.getDevices();\n-    if (call_op->hasAttrOfType<mlir::UnitAttr>(kIfrtLocalViewAttrName)) {\n-      exec_build_options.set_num_replicas(logical_device_ids.size());\n-      exec_build_options.set_num_partitions(1);\n-      xla::DeviceAssignment device_assignment(logical_device_ids.size(), 1);\n-      for (const auto [i, device_id] : llvm::enumerate(logical_device_ids)) {\n-        device_assignment(i, 0) = device_id;\n-      }\n-      exec_build_options.set_device_assignment(device_assignment);\n-    } else {\n-      exec_build_options.set_num_replicas(1);\n-      exec_build_options.set_num_partitions(logical_device_ids.size());\n-      xla::DeviceAssignment device_assignment(1, logical_device_ids.size());\n-      for (const auto [i, device_id] : llvm::enumerate(logical_device_ids)) {\n-        device_assignment(0, i) = device_id;\n-      }\n-      exec_build_options.set_device_assignment(device_assignment);\n-      exec_build_options.set_use_spmd_partitioning(true);\n-      if (enable_sharding_propagation_) {\n-        mlir::func::FuncOp main_func = GetMainFunction(module_op);\n-        exec_build_options.set_allow_spmd_sharding_propagation_to_parameters(\n-            GetInputShardingPropagation(main_func));\n-        exec_build_options.set_allow_spmd_sharding_propagation_to_output(\n-            GetOutputShardingPropagation(main_func));\n-      }\n+  auto& exec_build_options = compile_options.executable_build_options;\n+  // Executable build options are constructed using logical ids, which are\n+  // later converted into real Device ids by using the logical ids as\n+  // indices into the device list given at compilation invocation time.\n+  llvm::ArrayRef<int> logical_device_ids = call_op.getDevices();\n+  if (call_op->hasAttrOfType<mlir::UnitAttr>(kIfrtLocalViewAttrName)) {\n+    exec_build_options.set_num_replicas(logical_device_ids.size());\n+    exec_build_options.set_num_partitions(1);\n+    xla::DeviceAssignment device_assignment(logical_device_ids.size(), 1);\n+    for (const auto [i, device_id] : llvm::enumerate(logical_device_ids)) {\n+      device_assignment(i, 0) = device_id;\n+    }\n+    exec_build_options.set_device_assignment(device_assignment);\n+  } else {\n+    exec_build_options.set_num_replicas(1);\n+    exec_build_options.set_num_partitions(logical_device_ids.size());\n+    xla::DeviceAssignment device_assignment(1, logical_device_ids.size());\n+    for (const auto [i, device_id] : llvm::enumerate(logical_device_ids)) {\n+      device_assignment(0, i) = device_id;\n+    }\n+    exec_build_options.set_device_assignment(device_assignment);\n+    exec_build_options.set_use_spmd_partitioning(true);\n+    if (enable_sharding_propagation_) {\n+      mlir::func::FuncOp main_func = GetMainFunction(module_op);\n+      exec_build_options.set_allow_spmd_sharding_propagation_to_parameters(\n+          GetInputShardingPropagation(main_func));\n+      exec_build_options.set_allow_spmd_sharding_propagation_to_output(\n+          GetOutputShardingPropagation(main_func));\n     }\n   }\n \n@@ -217,13 +204,12 @@ absl::StatusOr<CompileFuture> MultiThreadedAtomProgramCompiler::CompileXla(\n       mlir::OwningOpRef<mlir::ModuleOp>(module_op.clone()));\n   Promise<AtomProgramCompileResult> promise = CompileFuture::CreatePromise();\n   CompileFuture future(promise);\n-  ScheduleWork(\n-      thread_pool, [this, hlo_program = std::move(hlo_program),\n-                    compile_options = std::move(compile_options),\n-                    promise = std::move(promise)]() mutable {\n-        promise.Set(compiler_->CompileXla(std::move(hlo_program),\n-                                          std::move(compile_options)));\n-      });\n+  ScheduleWork(thread_pool, [this, hlo_program = std::move(hlo_program),\n+                             compile_options = std::move(compile_options),\n+                             promise = std::move(promise)]() mutable {\n+    promise.Set(compiler_->CompileXla(std::move(hlo_program),\n+                                      std::move(compile_options)));\n+  });\n   return future;\n }\n "
        },
        {
            "sha": "2e51d3f9667fc1216602202818cd715972b612d5",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/passes.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -96,8 +96,11 @@ void createIfrtPopulateAtomProgramMetadataPipeline(mlir::OpPassManager& pm) {\n   pm.addPass(mlir::createSymbolDCEPass());\n }\n \n-void createIfrtCompileXlaPreprocessingPipeline(mlir::OpPassManager& pm) {\n-  pm.addPass(createIfrtLowerAtomProgramMetadataToXlaPass());\n+void createIfrtCompileXlaPreprocessingPipeline(\n+    mlir::OpPassManager& pm,\n+    std::shared_ptr<xla::ifrt::IfrtIRCompileOptions> compile_options) {\n+  pm.addPass(createIfrtLowerAtomProgramMetadataToXlaPass(\n+      {/*compile_options=*/compile_options}));\n   pm.addPass(createIfrtRemoveIfrtAttrsPass());\n }\n \n@@ -121,7 +124,8 @@ absl::Status createOutlinedAtomProgramsToCompiledPipeline(\n     pm.addPass(createIfrtLowerMpmdReshardToCallPass());\n   }\n   pm.addPass(createIfrtPrecompileAtomProgramPreprocessingPass(\n-      {/*platform_names=*/llvm::to_vector(options.platform_names)}));\n+      {/*platform_names=*/llvm::to_vector(options.platform_names),\n+       /*compile_options=*/compile_options}));\n   if (options.propagate_shardings) {\n     pm.addPass(createIfrtCompileAndPropagateShardingsPass(\n         compiler, compile_options->compile_options_overrides,\n@@ -205,7 +209,9 @@ void registerIfrtPassesAndPipelines(\n   mlir::PassPipelineRegistration<>(\n       \"ifrt-compile-xla-preprocessing-pipeline\",\n       \"Run passes to lower an IFRT XLA program for XLA compilation\",\n-      createIfrtCompileXlaPreprocessingPipeline);\n+      [compile_options](mlir::OpPassManager& pm) mutable {\n+        createIfrtCompileXlaPreprocessingPipeline(pm, compile_options);\n+      });\n   // Do not move to lambda captures because the pass pipeline registration is\n   // invoked for each module in a test file.\n   mlir::PassPipelineRegistration<OutlinedAtomProgramsToCompiledPipelineOptions>("
        },
        {
            "sha": "006f1172def4b2366439c8870796a68e63f6bee8",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/passes.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.h?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -153,7 +153,9 @@ void createIfrtToOutlinedAtomProgramsPipeline(\n void createIfrtPopulateAtomProgramMetadataPipeline(mlir::OpPassManager& pm);\n \n // Creates pipeline to lower an IFRT XLA program to be ready for compilation.\n-void createIfrtCompileXlaPreprocessingPipeline(mlir::OpPassManager& pm);\n+void createIfrtCompileXlaPreprocessingPipeline(\n+    mlir::OpPassManager& pm,\n+    std::shared_ptr<xla::ifrt::IfrtIRCompileOptions> compile_options);\n \n struct OutlinedAtomProgramsToCompiledPipelineOptions\n     : mlir::PassPipelineOptions<OutlinedAtomProgramsToCompiledPipelineOptions> {"
        },
        {
            "sha": "545ea2e2b98a69dbce27ccd679e1a80052bfc506",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/passes.td",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fpasses.td?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -19,6 +19,12 @@ limitations under the License.\n \n include \"mlir/Pass/PassBase.td\"\n \n+def IfrtIRCompileOptions :\n+    Option<\"compile_options\", \"compile_options\",\n+           \"std::shared_ptr<xla::ifrt::IfrtIRCompileOptions>\",\n+           \"std::make_shared<xla::ifrt::IfrtIRCompileOptions>()\",\n+           \"The IFRT IR compile options used for compilation.\">;\n+\n def SpmdExpandableInterfaceVerificationPass\n     : Pass<\"spmd-expandable-interface-verification\", \"mlir::ModuleOp\"> {\n   let summary = \"Verify all ops have IFRT SpmdExpandableInterface implemented\";\n@@ -294,8 +300,8 @@ def IfrtLowerAtomProgramMetadataToXlaPass :\n     Pass<\"ifrt-lower-atom-program-metadata-to-xla\", \"mlir::ModuleOp\"> {\n   let summary = \"Converts IFRT metadata to XLA-compatible attributes\";\n   let description = [{\n-Replaces `ifrt.sharding` attr by `mhlo.sharding` attr, and `ifrt.memory_kind`\n-by `mhlo.memory_kind`.\n+Replaces `ifrt.sharding` attr by `mhlo.sharding` attr (if Shardy is disabled)\n+and `ifrt.memory_kind` by `mhlo.memory_kind`.\n \n For example, the following code\n \n@@ -319,6 +325,10 @@ module attributes {\n }\n ```\n   }];\n+\n+  let options = [\n+    IfrtIRCompileOptions\n+  ];\n }\n \n def IfrtRemoveIfrtAttrsPass :\n@@ -422,7 +432,8 @@ This passes visits each CallOp and does the following:\n \n   let options = [\n     ListOption<\"platform_names\", \"platform_names\", \"std::string\",\n-               \"The mapping from logical device ID to platform name (e.g., tpu, cuda) corresponding to the logical device ID.\">\n+               \"The mapping from logical device ID to platform name (e.g., tpu, cuda) corresponding to the logical device ID.\">,\n+    IfrtIRCompileOptions,\n   ];\n }\n "
        },
        {
            "sha": "3ddeb3893424d1d19f5eb5c27d6dd8faa40feb34",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/utils.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Futils.cc?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -16,10 +16,12 @@ limitations under the License.\n #include \"xla/python/ifrt/ir/transforms/utils.h\"\n \n #include <cstdint>\n+#include <memory>\n #include <optional>\n #include <string>\n #include <vector>\n \n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/numbers.h\"\n@@ -28,6 +30,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"llvm/ADT/Hashing.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/Attributes.h\"\n@@ -41,10 +44,14 @@ limitations under the License.\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/mlir/utils/type_util.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n+#include \"xla/python/ifrt/compiler.h\"\n #include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/ir/constants.h\"\n #include \"xla/python/ifrt/ir/ifrt_dialect.h\"\n #include \"xla/python/ifrt/ir/ifrt_ops.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_dtype.h\"\n+#include \"xla/python/pjrt_ifrt/xla_compiler.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/fingerprint.h\"\n \n@@ -281,5 +288,31 @@ uint64_t MlirModuleFingerprint(mlir::ModuleOp module) {\n   return tsl::Fingerprint64(os.str());\n }\n \n+absl::StatusOr<std::optional<xla::CompileOptions>> GetModuleXlaCompileOverrides(\n+    mlir::StringAttr compile_options_key,\n+    std::shared_ptr<\n+        absl::flat_hash_map<std::string, std::unique_ptr<CompileOptions>>>\n+        compile_options_overrides) {\n+  std::optional<xla::CompileOptions> compile_options = std::nullopt;\n+  if (compile_options_overrides != nullptr && compile_options_key != nullptr) {\n+    if (auto option_override =\n+            compile_options_overrides->find(compile_options_key.str());\n+        option_override != compile_options_overrides->end()) {\n+      if (auto xla_options = llvm::dyn_cast<XlaCompileOptions>(\n+              option_override->second.get())) {\n+        compile_options = xla_options->compile_options;\n+      } else {\n+        return absl::InvalidArgumentError(absl::StrCat(\n+            \"The `\", kIfrtCompileOptionsKey.str(), \"` compile options key `\",\n+            compile_options_key.str(),\n+            \"` has an entry that is not of type `XlaCompileOptions`, but the \"\n+            \"atom program is an XLA program.\"));\n+      }\n+    }\n+  }\n+\n+  return compile_options;\n+}\n+\n }  // namespace ifrt\n }  // namespace xla"
        },
        {
            "sha": "9397f7edb332bc72425aad3826f278f84baba799",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/utils.h",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Futils.h?ref=5a7fb851dcb61b5bdb3654444ed4b5dca8d0c05f",
            "patch": "@@ -17,9 +17,11 @@ limitations under the License.\n #define XLA_PYTHON_IFRT_IR_TRANSFORMS_UTILS_H_\n \n #include <cstdint>\n+#include <memory>\n #include <string>\n #include <vector>\n \n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/statusor.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n@@ -28,6 +30,7 @@ limitations under the License.\n #include \"mlir/IR/OperationSupport.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/Pass/Pass.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/python/ifrt/dtype.h\"\n #include \"xla/python/ifrt/ir/ifrt_dialect.h\"\n #include \"xla/python/ifrt/ir/ifrt_ops.h\"\n@@ -76,6 +79,14 @@ std::string GetPrettyLocation(mlir::Location loc);\n // Returns a fingerprint of the provided module.\n uint64_t MlirModuleFingerprint(mlir::ModuleOp module);\n \n+// Extracts the XLA compile options overrides for the given atom program module.\n+// Returns std::nullopt if no overrides are found.\n+absl::StatusOr<std::optional<xla::CompileOptions>> GetModuleXlaCompileOverrides(\n+    mlir::StringAttr compile_options_key,\n+    std::shared_ptr<\n+        absl::flat_hash_map<std::string, std::unique_ptr<CompileOptions>>>\n+        compile_options_overrides);\n+\n }  // namespace ifrt\n }  // namespace xla\n "
        }
    ],
    "stats": {
        "total": 709,
        "additions": 596,
        "deletions": 113
    }
}