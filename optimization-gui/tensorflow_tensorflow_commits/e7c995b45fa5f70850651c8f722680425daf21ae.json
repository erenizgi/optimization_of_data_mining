{
    "author": "unknown",
    "message": "[XLA:GPU] Use BufferDebugLogEntryMetadataStore in checksumming\n\nEncoding extra metadata about an debug log entry limits how much information we\ncan pass. To remove the limitation without the need to pass extra data between\nhost and device, identify the log entries using an opaque 32-bit ID.\n\n- BuffersChecksumThunks put the metadata into the store and use the returned\n  entry_ids to identify the checksums from BufferDebugLog,\n- xla_gpu_buffer_debug_log_dump reads the BufferDebugLog and uses the store to\n  resolve the entry_ids into the metadata.\n\nThis also makes ThunkBufferId unnecessary, and removes the limitations on numeric\nvalues of ThunkId/buffer_idx.\n\nPiperOrigin-RevId: 827492847",
    "sha": "e7c995b45fa5f70850651c8f722680425daf21ae",
    "files": [
        {
            "sha": "f3958351cc079c7bcf7893201f5eaac8b816b2d1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 15,
            "deletions": 37,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -344,15 +344,12 @@ xla_test(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:gpu_solver_context\",\n-        \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n-        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n \n@@ -725,7 +722,6 @@ cc_library(\n         \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -2901,16 +2897,18 @@ cc_library(\n     srcs = [\"thunk_buffer_debug_pass.cc\"],\n     hdrs = [\"thunk_buffer_debug_pass.h\"],\n     deps = [\n+        \":buffer_debug_log_entry_metadata_store\",\n+        \":buffer_debug_log_structs\",\n         \":buffers_checksum_thunk\",\n         \":buffers_nan_count_thunk\",\n         \":custom_call_thunk\",\n         \":sequential_thunk\",\n         \":thunk\",\n-        \":thunk_buffer_id\",\n         \":thunk_pass_pipeline\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/ffi\",\n+        \"//xla/ffi:attribute_map\",\n         \"//xla/ffi/api:c_api\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n@@ -2922,6 +2920,7 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/functional:bind_front\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n@@ -2940,7 +2939,6 @@ xla_cc_test(\n         \":sequential_thunk\",\n         \":thunk\",\n         \":thunk_buffer_debug_pass\",\n-        \":thunk_buffer_id\",\n         \":thunk_id\",\n         \":thunk_pass_pipeline\",\n         \"//xla:literal_util\",\n@@ -3006,8 +3004,10 @@ cc_library(\n     srcs = [\"buffers_checksum_thunk.cc\"],\n     hdrs = [\"buffers_checksum_thunk.h\"],\n     deps = [\n+        \":buffer_debug_log_entry_metadata_store\",\n+        \":buffer_debug_log_structs\",\n         \":thunk\",\n-        \":thunk_buffer_id\",\n+        \":thunk_id\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:launch_dim\",\n@@ -3035,10 +3035,10 @@ xla_test(\n         \"gpu\",\n     ],\n     deps = [\n+        \":buffer_debug_log_entry_metadata_store\",\n         \":buffer_debug_log_structs\",\n         \":buffers_checksum_thunk\",\n         \":thunk\",\n-        \":thunk_buffer_id\",\n         \":thunk_id\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:executable\",\n@@ -3052,6 +3052,7 @@ xla_test(\n         \"//xla/stream_executor/gpu:buffer_debug_log\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )\n@@ -3061,8 +3062,10 @@ cc_library(\n     srcs = [\"buffers_nan_count_thunk.cc\"],\n     hdrs = [\"buffers_nan_count_thunk.h\"],\n     deps = [\n+        \":buffer_debug_log_entry_metadata_store\",\n+        \":buffer_debug_log_structs\",\n         \":thunk\",\n-        \":thunk_buffer_id\",\n+        \":thunk_id\",\n         \"//xla:types\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:device_memory\",\n@@ -3093,10 +3096,10 @@ xla_test(\n         \"gpu\",\n     ],\n     deps = [\n+        \":buffer_debug_log_entry_metadata_store\",\n         \":buffer_debug_log_structs\",\n         \":buffers_nan_count_thunk\",\n         \":thunk\",\n-        \":thunk_buffer_id\",\n         \":thunk_id\",\n         \"//xla:types\",\n         \"//xla/service:buffer_assignment\",\n@@ -3125,38 +3128,14 @@ xla_py_proto_library(\n     deps = [\":buffer_debug_log_proto\"],\n )\n \n-cc_library(\n-    name = \"thunk_buffer_id\",\n-    hdrs = [\"thunk_buffer_id.h\"],\n-    compatible_with = get_compatible_with_portable(),\n-    deps = [\n-        \":thunk_id\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"thunk_buffer_id_test\",\n-    srcs = [\"thunk_buffer_id_test.cc\"],\n-    deps = [\n-        \":thunk_buffer_id\",\n-        \":thunk_id\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:status_matchers\",\n-        \"@com_google_googletest//:gtest_main\",\n-    ],\n-)\n-\n cc_library(\n     name = \"buffer_debug_log_structs\",\n     hdrs = [\"buffer_debug_log_structs.h\"],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n-        \":thunk_buffer_id\",\n+        \":buffer_debug_log_proto_cc\",\n         \"//xla/tsl/lib/gtl:int_type\",\n+        \"@com_google_absl//absl/strings:str_format\",\n     ],\n )\n \n@@ -3182,7 +3161,6 @@ xla_cc_test(\n     deps = [\n         \":buffer_debug_log_entry_metadata_store\",\n         \":buffer_debug_log_structs\",\n-        \":thunk_buffer_id\",\n         \":thunk_id\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_googletest//:gtest_main\","
        },
        {
            "sha": "b3a6197c83292788d9e948c3fe62300dfad9ba65",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -63,9 +63,7 @@ BufferDebugLogProto BufferDebugLogEntryMetadataStore::EntriesToProto(\n \n   BufferDebugLogProto proto;\n   for (const BufferDebugLogEntry& entry : entries) {\n-    // TODO: b/447080910 - simplify once entry_id is a BufferDebugLogEntryId.\n-    std::optional<Metadata> metadata =\n-        GetEntryMetadataLocked(BufferDebugLogEntryId{entry.entry_id.value()});\n+    std::optional<Metadata> metadata = GetEntryMetadataLocked(entry.entry_id);\n     if (!metadata.has_value()) {\n       continue;\n     }"
        },
        {
            "sha": "101339aa1e20d38e026bc5e2cd5c2ce9826f6394",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_entry_metadata_store_test.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n \n@@ -76,13 +75,11 @@ TEST(BufferDebugLogEntryMetadataStoreTest, EntriesToProto) {\n   });\n   std::vector<BufferDebugLogEntry> entries = {\n       {\n-          // TODO: b/447080910 - use BufferDebugLogEntryId directly.\n-          /*entry_id=*/reinterpret_cast<const ThunkBufferId&>(entry_id1),\n+          /*entry_id=*/entry_id1,\n           /*checksum=*/12341234,\n       },\n       {\n-          // TODO: b/447080910 - use BufferDebugLogEntryId directly.\n-          /*entry_id=*/reinterpret_cast<const ThunkBufferId&>(entry_id2),\n+          /*entry_id=*/entry_id2,\n           /*checksum=*/56785678,\n       },\n   };"
        },
        {
            "sha": "6ce5ab3b6533a0af5b8939e48c4f90e0d4179024",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_debug_log_structs.h",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_structs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_structs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_structs.h?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -20,23 +20,22 @@ limitations under the License.\n #include <cstdint>\n #include <tuple>\n \n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log.pb.h\"\n #include \"xla/tsl/lib/gtl/int_type.h\"\n \n namespace xla::gpu {\n \n-// TODO: b/447080910 - use this instead of ThunkBufferId.\n TSL_LIB_GTL_DEFINE_INT_TYPE(BufferDebugLogEntryId, uint32_t)\n \n struct BufferDebugLogEntry {\n-  // An ID that uniquely identifies a thunk and its specific input or output\n-  // buffer.\n-  ThunkBufferId entry_id;\n+  // An ID that uniquely identifies a log entry within a HLO module execution.\n+  BufferDebugLogEntryId entry_id;\n   uint32_t value;\n \n   template <typename Sink>\n   friend void AbslStringify(Sink& sink, const BufferDebugLogEntry& entry) {\n-    absl::Format(&sink, \"{entry_id: %v, value: %u}\", entry.entry_id,\n+    absl::Format(&sink, \"{entry_id: %v, value: %u}\", entry.entry_id.value(),\n                  entry.value);\n   }\n "
        },
        {
            "sha": "1e5b5e342ca7f65c12c52dc6d07266d145ed72b3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n@@ -74,6 +75,7 @@ absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n   }\n \n   VLOG(1) << \"BuffersDebugChecksumThunk::ExecuteOnStream\";\n+  const uint32_t execution_id = execution_count_.fetch_add(1);\n \n   const se::ThreadDim thread_dim(\n       executor->GetDeviceDescription().threads_per_block_limit(), 1, 1);\n@@ -83,12 +85,19 @@ absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n   se::gpu::BufferDebugLog buffer_debug_log =\n       se::gpu::BufferDebugLog::FromDeviceMemoryUnchecked(log_ptr);\n \n-  for (const auto& [entry_id, buffer] : buffers_) {\n+  for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {\n+    const BufferDebugLogEntryId log_entry_id = metadata_store_->AssignId({\n+        checked_thunk_id_,\n+        buffer_idx,\n+        execution_id,\n+        /*is_input=*/runs_before_checked_thunk_,\n+    });\n+\n     se::DeviceMemory<uint8_t> device_buffer(\n         params.buffer_allocations->GetDeviceAddress(buffer));\n \n     TF_RETURN_IF_ERROR(kernel_->Launch(\n-        thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n+        thread_dim, se::BlockDim(1, 1, 1), params.stream, log_entry_id,\n         device_buffer, device_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n         buffer_debug_log.GetDeviceEntries()));\n   }\n@@ -98,10 +107,11 @@ absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n \n std::string BuffersDebugChecksumThunk::ToString(int indent) const {\n   std::string result;\n-  absl::StrAppend(&result, \", buffers = \", buffers_.size());\n-  for (const auto& [buffer_id, buffer] : buffers_) {\n+  absl::StrAppend(&result, \", buffers = \", checked_thunk_buffers_.size());\n+  for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {\n     absl::StrAppend(&result, \"\\n\", std::string(indent + 2, ' '),\n-                    \"buffer_id: \", buffer_id, \", buffer: \", buffer.ToString());\n+                    \"buffer_idx: \", buffer_idx,\n+                    \", buffer: \", buffer.ToString());\n   }\n   return result;\n }"
        },
        {
            "sha": "a43c202c077bbab866b485f27e4dfb6b59fd18b5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk.h",
            "status": "modified",
            "additions": 35,
            "deletions": 7,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.h?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -16,14 +16,18 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_RUNTIME_BUFFERS_CHECKSUM_THUNK_H_\n #define XLA_BACKENDS_GPU_RUNTIME_BUFFERS_CHECKSUM_THUNK_H_\n \n+#include <atomic>\n+#include <cstddef>\n+#include <memory>\n #include <optional>\n #include <string>\n #include <utility>\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h\"\n \n@@ -33,10 +37,18 @@ class BuffersDebugChecksumThunk : public Thunk {\n  public:\n   explicit BuffersDebugChecksumThunk(\n       ThunkInfo info, BufferAllocation::Slice log_slice,\n-      absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers)\n+      ThunkId checked_thunk_id,\n+      // buffer_idx => buffer slice\n+      absl::flat_hash_map<size_t, BufferAllocation::Slice>\n+          checked_thunk_buffers,\n+      bool runs_before_checked_thunk,\n+      std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store)\n       : Thunk(Thunk::Kind::kBuffersDebugChecksum, std::move(info)),\n         log_slice_(log_slice),\n-        buffers_(std::move(buffers)) {}\n+        metadata_store_(std::move(metadata_store)),\n+        checked_thunk_id_(checked_thunk_id),\n+        checked_thunk_buffers_(std::move(checked_thunk_buffers)),\n+        runs_before_checked_thunk_(runs_before_checked_thunk) {}\n \n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n@@ -48,17 +60,33 @@ class BuffersDebugChecksumThunk : public Thunk {\n     return {};\n   }\n \n-  const absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice>&\n-  buffer_slices() const {\n-    return buffers_;\n+  const absl::flat_hash_map<size_t, BufferAllocation::Slice>& buffer_slices()\n+      const {\n+    return checked_thunk_buffers_;\n+  }\n+\n+  template <typename Sink>\n+  friend void AbslStringify(Sink& sink,\n+                            const BuffersDebugChecksumThunk& thunk) {\n+    absl::Format(&sink, \"BuffersDebugChecksumThunk{buffers=%s}\",\n+                 absl::StrJoin(thunk.checked_thunk_buffers_, \", \",\n+                               [](std::string* out, const auto& buffer) {\n+                                 const auto& [id, slice] = buffer;\n+                                 absl::StrAppend(out, id, \"=\",\n+                                                 slice.ToString());\n+                               }));\n   }\n \n  private:\n   // Loaded in Initialize.\n   std::optional<stream_executor::gpu::BufferDebugXorChecksumKernel::KernelType>\n       kernel_;\n   BufferAllocation::Slice log_slice_;\n-  absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers_;\n+  std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store_;\n+  ThunkId checked_thunk_id_;\n+  absl::flat_hash_map<size_t, BufferAllocation::Slice> checked_thunk_buffers_;\n+  bool runs_before_checked_thunk_;\n+  std::atomic<size_t> execution_count_ = 0;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "cf9120944fad56b8330ec62c91d2cc9f0d1151ac",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk_test.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 14,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -19,13 +19,15 @@ limitations under the License.\n #include <cstdint>\n #include <memory>\n #include <optional>\n+#include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n@@ -46,8 +48,45 @@ namespace {\n namespace se = stream_executor;\n \n using ::stream_executor::gpu::BufferDebugLog;\n+using Metadata = BufferDebugLogEntryMetadataStore::Metadata;\n+\n+using ::testing::AllOf;\n+using ::testing::Field;\n using ::testing::UnorderedElementsAre;\n \n+MATCHER_P2(IsEntryWithMetadata, store, metadata, \"\") {\n+  std::optional<Metadata> actual_metadata =\n+      store->GetEntryMetadata(arg.entry_id);\n+  if (!actual_metadata.has_value()) {\n+    *result_listener << \"metadata not found for entry_id \"\n+                     << arg.entry_id.value();\n+    return false;\n+  }\n+\n+  return ExplainMatchResult(\n+      AllOf(Field(&Metadata::thunk_id, metadata.thunk_id),\n+            Field(&Metadata::buffer_idx, metadata.buffer_idx),\n+            Field(&Metadata::execution_id, metadata.execution_id),\n+            Field(&Metadata::is_input, metadata.is_input)),\n+      *actual_metadata, result_listener);\n+}\n+\n+class FakeThunk : public Thunk {\n+ public:\n+  explicit FakeThunk(ThunkInfo info, BufferUses buffer_uses)\n+      : Thunk(Thunk::Kind::kGemm, std::move(info)),\n+        buffer_uses_(std::move(buffer_uses)) {}\n+\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override {\n+    return absl::OkStatus();\n+  }\n+\n+  BufferUses buffer_uses() const override { return buffer_uses_; }\n+\n+ private:\n+  BufferUses buffer_uses_;\n+};\n+\n class BuffersDebugChecksumThunkTest : public ::testing::Test {\n  protected:\n   void SetUp() override {\n@@ -115,11 +154,13 @@ TEST_F(BuffersDebugChecksumThunkTest, CalculatesChecksums) {\n       ServiceExecutableRunOptions(), allocations, stream_.get(),\n       /*command_buffer_trace_stream=*/stream_.get(),\n       /*collective_params=*/nullptr, /*collective_cliques=*/nullptr);\n+  auto metadata_store = std::make_shared<BufferDebugLogEntryMetadataStore>();\n \n   BuffersDebugChecksumThunk thunk(\n       Thunk::ThunkInfo(), log_slice,\n-      {{ThunkBufferId::Create(ThunkId(123), 4).value(), inputs[0]},\n-       {ThunkBufferId::Create(ThunkId(456), 8).value(), inputs[1]}});\n+      /*checked_thunk_id=*/ThunkId(123),\n+      {{/*buffer_idx=*/0, inputs[0]}, {/*buffer_idx=*/1, inputs[1]}},\n+      /*runs_before_checked_thunk=*/true, metadata_store);\n   TF_ASSERT_OK(thunk.Initialize(init_params));\n   TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}, resource_requests));\n   TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));\n@@ -128,17 +169,24 @@ TEST_F(BuffersDebugChecksumThunkTest, CalculatesChecksums) {\n \n   // BuffersDebugChecksumThunk launches a kernel for each input buffer, they may\n   // complete in any order.\n-  EXPECT_THAT(\n-      entries,\n-      UnorderedElementsAre(\n-          BufferDebugLogEntry{\n-              /*entry_id=*/ThunkBufferId::Create(ThunkId(123), 4).value(),\n-              /*value=*/12341234,\n-          },\n-          BufferDebugLogEntry{\n-              /*entry_id=*/ThunkBufferId::Create(ThunkId(456), 8).value(),\n-              /*value=*/56785678,\n-          }));\n+  EXPECT_THAT(entries,\n+              UnorderedElementsAre(\n+                  AllOf(IsEntryWithMetadata(metadata_store,\n+                                            Metadata{\n+                                                /*thunk_id=*/ThunkId(123),\n+                                                /*buffer_idx=*/0,\n+                                                /*execution_id=*/0,\n+                                                /*is_input=*/true,\n+                                            }),\n+                        Field(&BufferDebugLogEntry::value, 12341234)),\n+                  AllOf(IsEntryWithMetadata(metadata_store,\n+                                            Metadata{\n+                                                /*thunk_id=*/ThunkId(123),\n+                                                /*buffer_idx=*/1,\n+                                                /*execution_id=*/0,\n+                                                /*is_input=*/true,\n+                                            }),\n+                        Field(&BufferDebugLogEntry::value, 56785678))));\n }\n \n }  // namespace"
        },
        {
            "sha": "b3f2bc342ad397789610d3df58b4db8d930a8db1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n@@ -86,8 +87,16 @@ absl::Status BuffersDebugNanCountThunk::ExecuteOnStream(\n       params.buffer_allocations->GetDeviceAddress(log_slice_));\n   se::gpu::BufferDebugLog buffer_debug_log =\n       se::gpu::BufferDebugLog::FromDeviceMemoryUnchecked(log_ptr);\n+  const uint32_t execution_id = execution_count_.fetch_add(1);\n+\n+  for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {\n+    const BufferDebugLogEntryId entry_id = metadata_store_->AssignId({\n+        checked_thunk_id_,\n+        buffer_idx,\n+        execution_id,\n+        /*is_input=*/runs_before_checked_thunk_,\n+    });\n \n-  for (const auto& [entry_id, buffer] : buffers_) {\n     PrimitiveType buffer_type = buffer.element_type();\n     se::DeviceMemoryBase device_buffer =\n         params.buffer_allocations->GetDeviceAddress(buffer);\n@@ -118,10 +127,11 @@ absl::Status BuffersDebugNanCountThunk::ExecuteOnStream(\n \n std::string BuffersDebugNanCountThunk::ToString(int indent) const {\n   std::string result;\n-  absl::StrAppend(&result, \", buffers = \", buffers_.size());\n-  for (const auto& [buffer_id, buffer] : buffers_) {\n+  absl::StrAppend(&result, \", buffers = \", checked_thunk_buffers_.size());\n+  for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {\n     absl::StrAppend(&result, \"\\n\", std::string(indent + 2, ' '),\n-                    \"buffer_id: \", buffer_id, \", buffer: \", buffer.ToString());\n+                    \"buffer_idx: \", buffer_idx,\n+                    \", buffer: \", buffer.ToString());\n   }\n   return result;\n }"
        },
        {
            "sha": "621fe507e917b1b7b1892ad796705548db239388",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk.h",
            "status": "modified",
            "additions": 22,
            "deletions": 7,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk.h?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -16,14 +16,18 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_RUNTIME_BUFFERS_NAN_COUNT_THUNK_H_\n #define XLA_BACKENDS_GPU_RUNTIME_BUFFERS_NAN_COUNT_THUNK_H_\n \n+#include <atomic>\n+#include <cstddef>\n+#include <memory>\n #include <optional>\n #include <string>\n #include <utility>\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_nan_count_kernel.h\"\n \n@@ -33,10 +37,17 @@ class BuffersDebugNanCountThunk : public Thunk {\n  public:\n   explicit BuffersDebugNanCountThunk(\n       ThunkInfo info, BufferAllocation::Slice log_slice,\n-      absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers)\n+      ThunkId checked_thunk_id,\n+      absl::flat_hash_map<size_t, BufferAllocation::Slice>\n+          checked_thunk_buffers,\n+      bool runs_before_checked_thunk,\n+      std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store)\n       : Thunk(Thunk::Kind::kBuffersDebugNanCount, std::move(info)),\n         log_slice_(log_slice),\n-        buffers_(std::move(buffers)) {}\n+        checked_thunk_id_(checked_thunk_id),\n+        checked_thunk_buffers_(std::move(checked_thunk_buffers)),\n+        runs_before_checked_thunk_(runs_before_checked_thunk),\n+        metadata_store_(std::move(metadata_store)) {}\n \n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n@@ -48,9 +59,9 @@ class BuffersDebugNanCountThunk : public Thunk {\n     return {};\n   }\n \n-  const absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice>&\n-  buffer_slices() const {\n-    return buffers_;\n+  const absl::flat_hash_map<size_t, BufferAllocation::Slice>& buffer_slices()\n+      const {\n+    return checked_thunk_buffers_;\n   }\n \n  private:\n@@ -60,7 +71,11 @@ class BuffersDebugNanCountThunk : public Thunk {\n   std::optional<stream_executor::gpu::BufferDebugNanCountBf16Kernel::KernelType>\n       kernel_bf16_;\n   BufferAllocation::Slice log_slice_;\n-  absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers_;\n+  ThunkId checked_thunk_id_;\n+  absl::flat_hash_map<size_t, BufferAllocation::Slice> checked_thunk_buffers_;\n+  bool runs_before_checked_thunk_;\n+  std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store_;\n+  std::atomic<size_t> execution_count_ = 0;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "9752a5b2cca52e7924e19cf6388b596dcd824036",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_nan_count_thunk_test.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 11,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_nan_count_thunk_test.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -24,9 +24,9 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n@@ -47,9 +47,30 @@ namespace {\n \n namespace se = stream_executor;\n \n+using Metadata = BufferDebugLogEntryMetadataStore::Metadata;\n+\n using ::stream_executor::gpu::BufferDebugLog;\n+using ::testing::AllOf;\n+using ::testing::Field;\n using ::testing::UnorderedElementsAre;\n \n+MATCHER_P2(IsEntryWithMetadata, store, metadata, \"\") {\n+  std::optional<Metadata> actual_metadata =\n+      store->GetEntryMetadata(arg.entry_id);\n+  if (!actual_metadata.has_value()) {\n+    *result_listener << \"metadata not found for entry_id \"\n+                     << arg.entry_id.value();\n+    return false;\n+  }\n+\n+  return ExplainMatchResult(\n+      AllOf(Field(&Metadata::thunk_id, metadata.thunk_id),\n+            Field(&Metadata::buffer_idx, metadata.buffer_idx),\n+            Field(&Metadata::execution_id, metadata.execution_id),\n+            Field(&Metadata::is_input, metadata.is_input)),\n+      *actual_metadata, result_listener);\n+}\n+\n class BuffersDebugNanCountThunkTest : public ::testing::Test {\n  protected:\n   void SetUp() override {\n@@ -131,11 +152,13 @@ TEST_F(BuffersDebugNanCountThunkTest, CalculatesNanCounts) {\n       ServiceExecutableRunOptions(), allocations, stream_.get(),\n       /*command_buffer_trace_stream=*/stream_.get(),\n       /*collective_params=*/nullptr, /*collective_cliques=*/nullptr);\n+  auto metadata_store = std::make_shared<BufferDebugLogEntryMetadataStore>();\n \n   BuffersDebugNanCountThunk thunk(\n       Thunk::ThunkInfo(), log_slice,\n-      {{ThunkBufferId::Create(ThunkId(123), 4).value(), inputs[0]},\n-       {ThunkBufferId::Create(ThunkId(456), 8).value(), inputs[1]}});\n+      /*checked_thunk_id=*/ThunkId(123),\n+      {{/*buffer_idx=*/0, inputs[0]}, {/*buffer_idx=*/1, inputs[1]}},\n+      /*runs_before_checked_thunk=*/true, metadata_store);\n   TF_ASSERT_OK(thunk.Initialize(init_params));\n   TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}, resource_requests));\n   TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));\n@@ -147,14 +170,18 @@ TEST_F(BuffersDebugNanCountThunkTest, CalculatesNanCounts) {\n   EXPECT_THAT(\n       entries,\n       UnorderedElementsAre(\n-          BufferDebugLogEntry{\n-              /*entry_id=*/ThunkBufferId::Create(ThunkId(123), 4).value(),\n-              /*value=*/1,\n-          },\n-          BufferDebugLogEntry{\n-              /*entry_id=*/ThunkBufferId::Create(ThunkId(456), 8).value(),\n-              /*value=*/2,\n-          }));\n+          IsEntryWithMetadata(metadata_store, Metadata{\n+                                                  /*thunk_id=*/ThunkId(123),\n+                                                  /*buffer_idx=*/0,\n+                                                  /*execution_id=*/0,\n+                                                  /*is_input=*/true,\n+                                              }),\n+          IsEntryWithMetadata(metadata_store, Metadata{\n+                                                  /*thunk_id=*/ThunkId(123),\n+                                                  /*buffer_idx=*/1,\n+                                                  /*execution_id=*/0,\n+                                                  /*is_input=*/true,\n+                                              })));\n }\n \n }  // namespace"
        },
        {
            "sha": "5e8b36997cbdd4d0e5fe1dbb63d7ab740b10d9c9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -358,6 +358,12 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {\n   return absl::OkStatus();\n }\n \n+// Builds a call frame for the custom call.\n+//\n+// If `buffer_allocations` is provided, the call frame will contain the actual\n+// device memory addresses of the buffers. Otherwise, the call frame will\n+// contain placeholders - this should only be the case when calling Prepare()\n+// stage handler.\n absl::StatusOr<ObjectPool<CallFrame>::BorrowedObject>\n CustomCallThunk::BuildCallFrame(\n     const BufferAllocations* absl_nullable buffer_allocations) {\n@@ -395,6 +401,10 @@ CustomCallThunk::BuildCallFrame(\n   return call_frame;\n }\n \n+// Builds call options object for the custom call.\n+//\n+// `stream` and `buffer_allocations may only be non-null for options passed to\n+// Prepare()_stage handler.\n CallOptions CustomCallThunk::BuildCallOptions(\n     RunId run_id, se::Stream* absl_nullable stream,\n     const BufferAllocations* absl_nullable buffer_allocations,"
        },
        {
            "sha": "5aedc23ddb8a4367af6502ff74b03fe515250ed7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 74,
            "changes": 152,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -23,18 +23,21 @@ limitations under the License.\n \n #include \"absl/base/nullability.h\"\n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/functional/bind_front.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/buffers_checksum_thunk.h\"\n #include \"xla/backends/gpu/runtime/buffers_nan_count_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n #include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/attribute_map.h\"\n #include \"xla/ffi/ffi.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -66,35 +69,28 @@ namespace {\n // If the thunk got wrapped, the data dependencies between the thunks will be\n // configured to ensure `predecessor_thunk` executes before the wrapped thunk\n // and `successor_thunk` executes after.\n-absl::StatusOr<std::unique_ptr<Thunk>> WrapWithChecksumThunk(\n+//\n+// If the thunk has no interesting buffers to check, it is returned as is. It\n+// can never return nullptr.\n+std::unique_ptr<Thunk> WrapWithChecksumThunk(\n     std::unique_ptr<Thunk> thunk, BufferAllocation::Slice log_slice,\n-    const Thunk& predecessor_thunk, Thunk& successor_thunk) {\n+    const Thunk& predecessor_thunk, Thunk& successor_thunk,\n+    std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store) {\n   const auto& thunk_buffers = thunk->buffer_uses();\n   if (thunk_buffers.empty()) {\n     return thunk;\n   }\n \n-  absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice>\n-      buffers_to_check_before;\n-  absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice>\n-      buffers_to_check_after;\n+  absl::flat_hash_map<size_t, BufferAllocation::Slice> buffers_to_check_before;\n+  absl::flat_hash_map<size_t, BufferAllocation::Slice> buffers_to_check_after;\n \n   for (size_t buffer_idx = 0; buffer_idx < thunk_buffers.size(); ++buffer_idx) {\n-    absl::StatusOr<ThunkBufferId> buffer_id =\n-        ThunkBufferId::Create(thunk->thunk_info().thunk_id, buffer_idx);\n-    if (!buffer_id.ok()) {\n-      LOG(WARNING) << \"Skipping buffer \" << buffer_idx << \" in thunk \"\n-                   << thunk->thunk_info().thunk_id << \": \"\n-                   << buffer_id.status();\n-      continue;\n-    }\n-\n     const BufferUse& use = thunk_buffers[buffer_idx];\n     if (use.HasDefinedContentsOnInput()) {\n-      buffers_to_check_before.emplace(buffer_id.value(), use.slice());\n+      buffers_to_check_before.emplace(buffer_idx, use.slice());\n     }\n     if (use.HasDefinedContentsOnOutput()) {\n-      buffers_to_check_after.emplace(buffer_id.value(), use.slice());\n+      buffers_to_check_after.emplace(buffer_idx, use.slice());\n     }\n   }\n \n@@ -106,7 +102,9 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapWithChecksumThunk(\n   if (!buffers_to_check_before.empty()) {\n     auto buffer_debug_before_thunk =\n         std::make_unique<BuffersDebugChecksumThunk>(\n-            Thunk::ThunkInfo(), log_slice, std::move(buffers_to_check_before));\n+            Thunk::ThunkInfo(), log_slice, thunk->thunk_info().thunk_id,\n+            std::move(buffers_to_check_before),\n+            /*runs_before_checked_thunk=*/true, metadata_store);\n     thunk->add_control_predecessor(buffer_debug_before_thunk.get());\n     thunk_and_checks.push_back(std::move(buffer_debug_before_thunk));\n   }\n@@ -116,7 +114,9 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapWithChecksumThunk(\n \n   if (!buffers_to_check_after.empty()) {\n     auto buffer_debug_after_thunk = std::make_unique<BuffersDebugChecksumThunk>(\n-        Thunk::ThunkInfo(), log_slice, std::move(buffers_to_check_after));\n+        Thunk::ThunkInfo(), log_slice, thunk_ptr->thunk_info().thunk_id,\n+        std::move(buffers_to_check_after),\n+        /*runs_before_checked_thunk=*/false, metadata_store);\n     buffer_debug_after_thunk->add_control_predecessor(thunk_ptr);\n     thunk_and_checks.push_back(std::move(buffer_debug_after_thunk));\n   }\n@@ -128,17 +128,18 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapWithChecksumThunk(\n   return wrapped_thunk;\n }\n \n-absl::StatusOr<std::unique_ptr<Thunk>> WrapWithNanCounterThunk(\n+std::unique_ptr<Thunk> WrapWithNanCounterThunk(\n     std::unique_ptr<Thunk> thunk, BufferAllocation::Slice log_slice,\n-    const Thunk& predecessor_thunk, Thunk& successor_thunk) {\n+    const Thunk& predecessor_thunk, Thunk& successor_thunk,\n+    std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store) {\n   const auto& thunk_buffers = thunk->buffer_uses();\n   if (thunk_buffers.empty()) {\n     VLOG(1) << \"No buffers in thunk \" << thunk->thunk_info().thunk_id\n             << \", skipping\";\n     return thunk;\n   }\n \n-  absl::flat_hash_map<ThunkBufferId, BufferAllocation::Slice> buffers_to_check;\n+  absl::flat_hash_map<size_t, BufferAllocation::Slice> buffers_to_check;\n   for (size_t buffer_idx = 0; buffer_idx < thunk_buffers.size(); ++buffer_idx) {\n     VLOG(1) << \"Buffer \" << buffer_idx << \" in thunk \"\n             << thunk->thunk_info().thunk_id;\n@@ -150,14 +151,6 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapWithNanCounterThunk(\n               << \" has null allocation, skipping\";\n       continue;\n     }\n-    auto buffer_id =\n-        ThunkBufferId::Create(thunk->thunk_info().thunk_id, buffer_idx);\n-    if (!buffer_id.ok()) {\n-      LOG(WARNING) << \"ThunkBufferId::Create failed: Skipping buffer \"\n-                   << buffer_idx << \" in thunk \" << thunk->thunk_info().thunk_id\n-                   << \": \" << buffer_id.status();\n-      continue;\n-    }\n     if (slice.element_type() != PrimitiveType::F32 &&\n         slice.element_type() != PrimitiveType::BF16) {\n       VLOG(1) << \"Buffer \" << buffer_idx << \" in thunk \"\n@@ -172,7 +165,7 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapWithNanCounterThunk(\n               << \" has no defined contents on output, skipping\";\n       continue;\n     }\n-    buffers_to_check.emplace(buffer_id.value(), use.slice());\n+    buffers_to_check.emplace(buffer_idx, use.slice());\n     VLOG(1) << \"Found buffer \" << buffer_idx << \" in thunk \"\n             << thunk->thunk_info().thunk_id << \" with element type \"\n             << PrimitiveType_Name(slice.element_type()) << \" and size \"\n@@ -190,8 +183,10 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapWithNanCounterThunk(\n   Thunk* thunk_ptr = thunk.get();\n   thunk_and_checks.push_back(std::move(thunk));\n   auto buffer_debug_nan_counter_thunk =\n-      std::make_unique<BuffersDebugNanCountThunk>(Thunk::ThunkInfo(), log_slice,\n-                                                  std::move(buffers_to_check));\n+      std::make_unique<BuffersDebugNanCountThunk>(\n+          Thunk::ThunkInfo(), log_slice, thunk_ptr->thunk_info().thunk_id,\n+          std::move(buffers_to_check),\n+          /*runs_before_checked_thunk=*/false, std::move(metadata_store));\n   buffer_debug_nan_counter_thunk->add_control_predecessor(thunk_ptr);\n   thunk_and_checks.push_back(std::move(buffer_debug_nan_counter_thunk));\n   auto wrapped_thunk = std::make_unique<SequentialThunk>(\n@@ -201,6 +196,36 @@ absl::StatusOr<std::unique_ptr<Thunk>> WrapWithNanCounterThunk(\n   return wrapped_thunk;\n }\n \n+// Saves the contents of the BufferDebugLog stored in `log_buffer` to a file..\n+//\n+// `metadata_store` is used to retrieve the metadata for the log entries.\n+// The filename is derived from the HLO module name and the log dump path\n+// configured in `debug_options`.\n+absl::Status DumpBufferDebugLog(\n+    std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store,\n+    se::Stream* stream, const HloComputation* absl_nonnull hlo_computation,\n+    xla::ffi::Buffer<U8> log_buffer) {\n+  VLOG(1) << \"HLO computation ptr: \" << hlo_computation;\n+  const HloModule* hlo_module = hlo_computation->parent();\n+  VLOG(1) << \"HLO module ptr: \" << hlo_module;\n+  VLOG(1) << \"HLO module name: \" << hlo_module->name();\n+  CHECK(hlo_module != nullptr);\n+  const DebugOptions& debug_options = hlo_module->config().debug_options();\n+\n+  se::gpu::BufferDebugLog buffer_debug_log =\n+      se::gpu::BufferDebugLog::FromDeviceMemoryUnchecked(\n+          log_buffer.device_memory());\n+  TF_ASSIGN_OR_RETURN(std::vector<BufferDebugLogEntry> log_entries,\n+                      buffer_debug_log.ReadFromDevice(*stream));\n+  BufferDebugLogProto buffer_debug_log_proto =\n+      metadata_store->EntriesToProto(log_entries);\n+\n+  VLOG(1) << \"read \" << buffer_debug_log_proto.entries_size() << \" entries\";\n+  DumpPerExecutionProtobufToFile(*hlo_module, buffer_debug_log_proto,\n+                                 debug_options, \"buffer_debug_log\", nullptr);\n+  return absl::OkStatus();\n+}\n+\n XLA_FFI_DEFINE_HANDLER_SYMBOL(\n     kDebugLogInitHandler,\n     [](se::Stream* absl_nonnull stream, xla::ffi::Buffer<U8> log_buffer) {\n@@ -210,33 +235,6 @@ XLA_FFI_DEFINE_HANDLER_SYMBOL(\n     },\n     xla::ffi::Ffi::Bind().Ctx<xla::ffi::Stream>().Arg<xla::ffi::Buffer<U8>>());\n \n-XLA_FFI_DEFINE_HANDLER_SYMBOL(\n-    kDebugLogDumpHandler,\n-    [](se::Stream* stream, const HloComputation* absl_nonnull hlo_computation,\n-       xla::ffi::Buffer<U8> log_buffer) {\n-      VLOG(1) << \"HLO computation ptr: \" << hlo_computation;\n-      const HloModule* hlo_module = hlo_computation->parent();\n-      VLOG(1) << \"HLO module ptr: \" << hlo_module;\n-      VLOG(1) << \"HLO module name: \" << hlo_module->name();\n-      CHECK(hlo_module != nullptr);\n-      const DebugOptions& debug_options = hlo_module->config().debug_options();\n-\n-      se::gpu::BufferDebugLog buffer_debug_log =\n-          se::gpu::BufferDebugLog::FromDeviceMemoryUnchecked(\n-              log_buffer.device_memory());\n-      TF_ASSIGN_OR_RETURN(xla::gpu::BufferDebugLogProto buffer_debug_log_proto,\n-                          buffer_debug_log.ReadProto(*stream));\n-      VLOG(1) << \"read \" << buffer_debug_log_proto.entries_size() << \" entries\";\n-      DumpPerExecutionProtobufToFile(*hlo_module, buffer_debug_log_proto,\n-                                     debug_options, \"buffer_debug_log\",\n-                                     nullptr);\n-      return absl::OkStatus();\n-    },\n-    xla::ffi::Ffi::Bind()\n-        .Ctx<xla::ffi::Stream>()\n-        .Ctx<xla::ffi::CalledComputation>()\n-        .Arg<xla::ffi::Buffer<U8>>());\n-\n }  // namespace\n \n absl::StatusOr<bool> ThunkBufferDebugPass::Run(\n@@ -252,6 +250,9 @@ absl::StatusOr<bool> ThunkBufferDebugPass::Run(\n     return false;\n   }\n \n+  std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store =\n+      std::make_shared<BufferDebugLogEntryMetadataStore>();\n+\n   TF_ASSIGN_OR_RETURN(BufferAllocation * log_alloc,\n                       allocator.NewEmptyAllocation(kLogSizeBytes));\n   BufferAllocation::Slice log_slice(log_alloc, 0, log_alloc->size());\n@@ -269,12 +270,17 @@ absl::StatusOr<bool> ThunkBufferDebugPass::Run(\n           buffer_debug_init_bundle, /*operands=*/{shaped_log_slice},\n           /*results=*/{}, /*attributes=*/{}, hlo_module->entry_computation()));\n \n-  XLA_FFI_Handler_Bundle buffer_debug_dump_bundle{};\n-  buffer_debug_dump_bundle.execute = kDebugLogDumpHandler;\n+  CustomCallThunk::OwnedHandlerBundle dump_bundle{};\n+  dump_bundle.execute =\n+      xla::ffi::Ffi::Bind()\n+          .Ctx<xla::ffi::Stream>()\n+          .Ctx<xla::ffi::CalledComputation>()\n+          .Arg<xla::ffi::Buffer<U8>>()\n+          .To(absl::bind_front(DumpBufferDebugLog, metadata_store));\n   TF_ASSIGN_OR_RETURN(auto buffer_debug_dump_thunk,\n                       CustomCallThunk::Create(Thunk::ThunkInfo(),\n                                               \"xla_gpu_buffer_debug_log_dump\",\n-                                              buffer_debug_dump_bundle,\n+                                              std::move(dump_bundle),\n                                               /*operands=*/{shaped_log_slice},\n                                               /*results=*/{}, /*attributes=*/{},\n                                               hlo_module->entry_computation()));\n@@ -283,18 +289,16 @@ absl::StatusOr<bool> ThunkBufferDebugPass::Run(\n   for (auto& thunk : thunks) {\n     if (mode_ == Mode::kChecksum) {\n       VLOG(1) << \"Wrapping with checksum thunk\";\n-      TF_ASSIGN_OR_RETURN(\n-          thunk, WrapWithChecksumThunk(\n-                     std::move(thunk), log_slice,\n-                     /*predecessor_thunk=*/*buffer_debug_init_thunk.get(),\n-                     /*successor_thunk=*/*buffer_debug_dump_thunk.get()));\n+      thunk = WrapWithChecksumThunk(\n+          std::move(thunk), log_slice,\n+          /*predecessor_thunk=*/*buffer_debug_init_thunk,\n+          /*successor_thunk=*/*buffer_debug_dump_thunk, metadata_store);\n     } else if (mode_ == Mode::kNanCounter) {\n       VLOG(1) << \"Wrapping with nan counter thunk\";\n-      TF_ASSIGN_OR_RETURN(\n-          thunk, WrapWithNanCounterThunk(\n-                     std::move(thunk), log_slice,\n-                     /*predecessor_thunk=*/*buffer_debug_init_thunk.get(),\n-                     /*successor_thunk=*/*buffer_debug_dump_thunk.get()));\n+      thunk = WrapWithNanCounterThunk(\n+          std::move(thunk), log_slice,\n+          /*predecessor_thunk=*/*buffer_debug_init_thunk,\n+          /*successor_thunk=*/*buffer_debug_dump_thunk, metadata_store);\n     }\n   }\n "
        },
        {
            "sha": "9cc388db65864fb08682e7ea39b5f569549fc8ce",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 16,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -29,7 +29,6 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -191,19 +190,13 @@ TEST(ThunkBufferDebugPassTest, InsertsBuffersDebugChecksumThunks) {\n \n   const BuffersDebugChecksumThunk& buffer_debug_before_fake_thunk =\n       static_cast<const BuffersDebugChecksumThunk&>(*sub_thunks[0]);\n-  EXPECT_THAT(\n-      buffer_debug_before_fake_thunk.buffer_slices(),\n-      UnorderedElementsAre(\n-          Pair(ThunkBufferId::Create(kTestThunkId, 0).value(), slice_i),\n-          Pair(ThunkBufferId::Create(kTestThunkId, 2).value(), slice_io)));\n+  EXPECT_THAT(buffer_debug_before_fake_thunk.buffer_slices(),\n+              UnorderedElementsAre(Pair(0, slice_i), Pair(2, slice_io)));\n \n   const BuffersDebugChecksumThunk& buffer_debug_after_fake_thunk =\n       static_cast<const BuffersDebugChecksumThunk&>(*sub_thunks[2]);\n-  EXPECT_THAT(\n-      buffer_debug_after_fake_thunk.buffer_slices(),\n-      UnorderedElementsAre(\n-          Pair(ThunkBufferId::Create(kTestThunkId, 1).value(), slice_o),\n-          Pair(ThunkBufferId::Create(kTestThunkId, 2).value(), slice_io)));\n+  EXPECT_THAT(buffer_debug_after_fake_thunk.buffer_slices(),\n+              UnorderedElementsAre(Pair(1, slice_o), Pair(2, slice_io)));\n }\n \n TEST(ThunkBufferDebugPassTest, InsertsBuffersDebugNanCounterThunks) {\n@@ -284,11 +277,8 @@ TEST(ThunkBufferDebugPassTest, InsertsBuffersDebugNanCounterThunks) {\n \n   const BuffersDebugNanCountThunk& buffer_debug_after_fake_thunk =\n       static_cast<const BuffersDebugNanCountThunk&>(*sub_thunks[1]);\n-  EXPECT_THAT(\n-      buffer_debug_after_fake_thunk.buffer_slices(),\n-      UnorderedElementsAre(\n-          Pair(ThunkBufferId::Create(kTestThunkId, 1).value(), slice_o),\n-          Pair(ThunkBufferId::Create(kTestThunkId, 2).value(), slice_io)));\n+  EXPECT_THAT(buffer_debug_after_fake_thunk.buffer_slices(),\n+              UnorderedElementsAre(Pair(1, slice_o), Pair(2, slice_io)));\n }\n \n }  // namespace"
        },
        {
            "sha": "95ae27c31f6a2b9ddb6b954a6f81388b8babc7a4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_id.h",
            "status": "removed",
            "additions": 0,
            "deletions": 101,
            "changes": 101,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/69a998d2cf7071cd219afd0c2b2dc60fede181ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_id.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/69a998d2cf7071cd219afd0c2b2dc60fede181ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_id.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_id.h?ref=69a998d2cf7071cd219afd0c2b2dc60fede181ac",
            "patch": "@@ -1,101 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_GPU_RUNTIME_THUNK_BUFFER_ID_H_\n-#define XLA_BACKENDS_GPU_RUNTIME_THUNK_BUFFER_ID_H_\n-\n-#include <cstddef>\n-#include <cstdint>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_format.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n-\n-namespace xla::gpu {\n-\n-// An ID that identifies a buffer within a program. It's a combination of the\n-// thunk ID and the buffer index within the thunk.\n-//\n-// A single buffer can be referred to by multiple ThunkBufferIds, when it's\n-// being used in different thunks.\n-class ThunkBufferId {\n- public:\n-  ThunkBufferId() = default;\n-\n-  // Creates a ThunkBufferId that represents the `buffer_idx`-th buffer of a\n-  // thunk with `thunk_info`.\n-  //\n-  // Returns an error if `buffer_idx` is too large to be represented in a\n-  // ThunkBufferId.\n-  static absl::StatusOr<ThunkBufferId> Create(ThunkId thunk_id,\n-                                              size_t buffer_idx) {\n-    if (buffer_idx >= (1 << kBitsReservedForBufferIndex)) {\n-      return absl::InvalidArgumentError(absl::StrFormat(\n-          \"Buffer index (%u) is too large to be represented in a ThunkBufferId \"\n-          \"(max = %u)\",\n-          buffer_idx, (1 << kBitsReservedForBufferIndex) - 1));\n-    }\n-\n-    const uint32_t value = (static_cast<uint32_t>(thunk_id.value())\n-                            << kBitsReservedForBufferIndex) |\n-                           static_cast<uint32_t>(buffer_idx);\n-    return ThunkBufferId(value);\n-  }\n-\n-  ThunkId thunk_id() const {\n-    return ThunkId(value_ >> kBitsReservedForBufferIndex);\n-  }\n-  size_t buffer_idx() const {\n-    return value_ & ((1 << kBitsReservedForBufferIndex) - 1);\n-  }\n-\n-  // Raw numeric value of the ID, for use in BufferDebugLogEntry::entry_id.\n-  uint32_t value() const { return value_; }\n-\n-  bool operator==(const ThunkBufferId& other) const {\n-    return value_ == other.value_;\n-  }\n-  bool operator!=(const ThunkBufferId& other) const {\n-    return !(*this == other);\n-  }\n-\n-  template <typename Sink>\n-  friend void AbslStringify(Sink& sink, const ThunkBufferId& buffer_id) {\n-    absl::Format(&sink, \"{thunk_id: %u, buffer_idx: %u}\",\n-                 buffer_id.thunk_id().value(), buffer_id.buffer_idx());\n-  }\n-\n-  template <typename H>\n-  friend H AbslHashValue(H h, const ThunkBufferId& buffer_id) {\n-    return H::combine(std::move(h), buffer_id.value_);\n-  }\n-\n- private:\n-  // Out of 32 bits available in id, reserve that much for the buffer index.\n-  // This limits us to:\n-  // - 2^kBitsReservedForBufferIndex max buffers per thunk\n-  // - 2^(32-kBitsReservedForBufferIndex) max thunks\n-  // Which hopefully is enough.\n-  static constexpr size_t kBitsReservedForBufferIndex = 8;\n-\n-  explicit ThunkBufferId(uint32_t value) : value_(value) {}\n-\n-  uint32_t value_ = 0;\n-};\n-\n-}  // namespace xla::gpu\n-\n-#endif  // XLA_BACKENDS_GPU_RUNTIME_THUNK_BUFFER_ID_H_"
        },
        {
            "sha": "5627c3ac9256314666c8aa837b2df41d3289e328",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_id_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/69a998d2cf7071cd219afd0c2b2dc60fede181ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_id_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/69a998d2cf7071cd219afd0c2b2dc60fede181ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_id_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_id_test.cc?ref=69a998d2cf7071cd219afd0c2b2dc60fede181ac",
            "patch": "@@ -1,49 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n-\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-#include \"absl/status/status.h\"\n-#include \"absl/status/status_matchers.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-\n-namespace {\n-\n-TEST(ThunkBufferIdTest, CreateFailsForLargeBufferIndex) {\n-  EXPECT_THAT(xla::gpu::ThunkBufferId::Create(xla::gpu::ThunkId(123),\n-                                              /*buffer_idx=*/256),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n-}\n-\n-TEST(ThunkBufferIdTest, CreateSucceedsForSmallBufferIndex) {\n-  EXPECT_THAT(xla::gpu::ThunkBufferId::Create(xla::gpu::ThunkId(123),\n-                                              /*buffer_idx=*/255),\n-              absl_testing::IsOk());\n-}\n-\n-TEST(ThunkBufferIdTest, CorrectlyStoresAndExtractsThunkIdAndBufferIndex) {\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      xla::gpu::ThunkBufferId buffer_id,\n-      xla::gpu::ThunkBufferId::Create(xla::gpu::ThunkId(123),\n-                                      /*buffer_idx=*/45));\n-\n-  EXPECT_THAT(buffer_id.thunk_id(), xla::gpu::ThunkId(123));\n-  EXPECT_THAT(buffer_id.buffer_idx(), 45);\n-}\n-\n-}  // namespace"
        },
        {
            "sha": "1e2acff185472a3d34611af433a17252ad391414",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -433,7 +433,6 @@ cuda_library(\n     deps = [\n         \":cuda_platform\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/backends/gpu/runtime:thunk_buffer_id\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor/gpu:buffer_debug_xor_checksum_kernel\",\n         \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n@@ -452,8 +451,6 @@ xla_test(\n     deps = [\n         \":buffer_debug_xor_checksum_kernel_cuda\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/backends/gpu/runtime:thunk_buffer_id\",\n-        \"//xla/backends/gpu/runtime:thunk_id\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n@@ -489,7 +486,6 @@ cuda_library(\n     deps = [\n         \":cuda_platform\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/backends/gpu/runtime:thunk_buffer_id\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor/gpu:buffer_debug_nan_count_kernel\",\n         \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n@@ -509,7 +505,6 @@ xla_test(\n         \":buffer_debug_nan_count_kernel_cuda\",\n         \"//xla:types\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/backends/gpu/runtime:thunk_buffer_id\",\n         \"//xla/backends/gpu/runtime:thunk_id\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:launch_dim\","
        },
        {
            "sha": "2adb9d91cc7d96a0d45b229a44f0c09068424c10",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_nan_count_kernel_cuda.cu.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_nan_count_kernel_cuda.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_nan_count_kernel_cuda.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_nan_count_kernel_cuda.cu.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #include \"absl/base/casts.h\"\n #include \"third_party/gpus/cuda/include/cuda/atomic\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/stream_executor/cuda/cuda_platform.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_nan_count_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -124,8 +123,8 @@ __device__ void ReduceSum(const T* input, uint64_t input_size,\n // - Only a single thread block is supported.\n // - Block dimensions must be a power of 2.\n template <typename T>\n-__global__ void AppendNanCount(xla::gpu::ThunkBufferId entry_id, const T* input,\n-                               uint64_t input_size_in_bytes,\n+__global__ void AppendNanCount(xla::gpu::BufferDebugLogEntryId entry_id,\n+                               const T* input, uint64_t input_size_in_bytes,\n                                xla::gpu::BufferDebugLogHeader* log_header,\n                                xla::gpu::BufferDebugLogEntry* log_entries) {\n   const uint32_t block_size = blockDim.x * blockDim.y * blockDim.z;"
        },
        {
            "sha": "dfdb01060bd57771e564ef8dccbb527fb584851f",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_nan_count_kernel_cuda_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_nan_count_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_nan_count_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_nan_count_kernel_cuda_test.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n+#include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n@@ -49,7 +49,7 @@ namespace se = stream_executor;\n namespace stream_executor::cuda {\n namespace {\n \n-using xla::gpu::ThunkBufferId;\n+using xla::gpu::BufferDebugLogEntryId;\n using xla::gpu::ThunkId;\n \n class NanCountKernelTest : public ::testing::Test {\n@@ -83,7 +83,7 @@ class NanCountKernelTest : public ::testing::Test {\n \n   template <typename Kernel, typename T>\n   absl::Status AppendNanCountOnDevice(\n-      ThunkBufferId entry_id, const std::vector<T>& input,\n+      BufferDebugLogEntryId entry_id, const std::vector<T>& input,\n       se::gpu::BufferDebugLog& buffer_debug_log,\n       stream_executor::ThreadDim dim = stream_executor::ThreadDim(1, 1, 1)) {\n     // Load kernel\n@@ -127,7 +127,7 @@ TEST_F(NanCountKernelTest, CountsNansForF32) {\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n   TF_EXPECT_OK(AppendNanCountOnDevice<gpu::BufferDebugNanCountF32Kernel>(\n-      ThunkBufferId(), input, device_log));\n+      BufferDebugLogEntryId{123}, input, device_log));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 1);\n@@ -146,7 +146,7 @@ TEST_F(NanCountKernelTest, CountsNansForBf16) {\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n   TF_EXPECT_OK(AppendNanCountOnDevice<gpu::BufferDebugNanCountBf16Kernel>(\n-      ThunkBufferId(), input, device_log));\n+      BufferDebugLogEntryId{0}, input, device_log));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 1);\n@@ -165,9 +165,9 @@ TEST_F(NanCountKernelTest, CountsNansInParallel) {\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n   TF_EXPECT_OK(AppendNanCountOnDevice<gpu::BufferDebugNanCountF32Kernel>(\n-      ThunkBufferId(), input, device_log, se::ThreadDim(2, 4, 8)));\n+      BufferDebugLogEntryId{0}, input, device_log, se::ThreadDim(2, 4, 8)));\n   TF_EXPECT_OK(AppendNanCountOnDevice<gpu::BufferDebugNanCountF32Kernel>(\n-      ThunkBufferId(), input, device_log, se::ThreadDim(2, 4, 8)));\n+      BufferDebugLogEntryId{0}, input, device_log, se::ThreadDim(2, 4, 8)));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 2);"
        },
        {
            "sha": "c6a2b2c3d9db8250a11cab0d78e8ad0e0e952f52",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_xor_checksum_kernel_cuda.cu.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda.cu.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #include \"absl/base/casts.h\"\n #include \"third_party/gpus/cuda/include/cuda/atomic\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/stream_executor/cuda/cuda_platform.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -117,7 +116,7 @@ __device__ void ReduceXor(const uint32_t* input, uint64_t input_size,\n // LIMITATIONS:\n // - Only a single thread block is supported.\n // - Block dimensions must be a power of 2.\n-__global__ void AppendChecksum(xla::gpu::ThunkBufferId entry_id,\n+__global__ void AppendChecksum(xla::gpu::BufferDebugLogEntryId entry_id,\n                                const uint8_t* input, uint64_t input_size,\n                                xla::gpu::BufferDebugLogHeader* log_header,\n                                xla::gpu::BufferDebugLogEntry* log_entries) {"
        },
        {
            "sha": "b08a5ae7795beb0b1ec58b7cc05ddd7883f8388c",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_xor_checksum_kernel_cuda_test.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 28,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda_test.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -27,8 +27,6 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n-#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h\"\n@@ -50,9 +48,8 @@ namespace stream_executor::cuda {\n namespace {\n \n using xla::gpu::BufferDebugLogEntry;\n+using xla::gpu::BufferDebugLogEntryId;\n using xla::gpu::BufferDebugLogHeader;\n-using xla::gpu::ThunkBufferId;\n-using xla::gpu::ThunkId;\n \n class ChecksumKernelTest : public ::testing::Test {\n  protected:\n@@ -85,7 +82,7 @@ class ChecksumKernelTest : public ::testing::Test {\n \n   template <typename T>\n   absl::Status AppendChecksumOnDevice(\n-      ThunkBufferId entry_id, const T& input,\n+      BufferDebugLogEntryId entry_id, const T& input,\n       se::gpu::BufferDebugLog& buffer_debug_log,\n       stream_executor::ThreadDim dim = stream_executor::ThreadDim(1, 1, 1)) {\n     // Load kernel\n@@ -138,7 +135,8 @@ TEST_F(ChecksumKernelTest, ComputesCorrectChecksumForMultipleOf32Bit) {\n       se::gpu::BufferDebugLog device_log,\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n-  TF_EXPECT_OK(AppendChecksumOnDevice(ThunkBufferId(), input, device_log));\n+  TF_EXPECT_OK(\n+      AppendChecksumOnDevice(BufferDebugLogEntryId{0}, input, device_log));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 1);\n@@ -153,7 +151,8 @@ TEST_F(ChecksumKernelTest,\n       se::gpu::BufferDebugLog device_log,\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n-  TF_EXPECT_OK(AppendChecksumOnDevice(ThunkBufferId(), kInput, device_log));\n+  TF_EXPECT_OK(\n+      AppendChecksumOnDevice(BufferDebugLogEntryId{0}, kInput, device_log));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 1);\n@@ -172,8 +171,8 @@ TEST_F(ChecksumKernelTest, ComputesCorrectChecksumInParallel) {\n       se::gpu::BufferDebugLog device_log,\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n-  TF_EXPECT_OK(AppendChecksumOnDevice(ThunkBufferId(), input, device_log,\n-                                      se::ThreadDim(2, 4, 8)));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{0}, input,\n+                                      device_log, se::ThreadDim(2, 4, 8)));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 1);\n@@ -191,8 +190,8 @@ TEST_F(ChecksumKernelTest, ComputesCorrectChecksumInParallelWithMaxThreads) {\n       se::gpu::BufferDebugLog device_log,\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n-  TF_EXPECT_OK(AppendChecksumOnDevice(ThunkBufferId(), input, device_log,\n-                                      se::ThreadDim(128, 4, 2)));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{0}, input,\n+                                      device_log, se::ThreadDim(128, 4, 2)));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 1);\n@@ -201,53 +200,53 @@ TEST_F(ChecksumKernelTest, ComputesCorrectChecksumInParallelWithMaxThreads) {\n \n TEST_F(ChecksumKernelTest, AppendsChecksumsToLog) {\n   se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n-  ThunkBufferId kId123 = ThunkBufferId::Create(ThunkId(123), 0).value();\n-  ThunkBufferId kId456 = ThunkBufferId::Create(ThunkId(456), 0).value();\n-  ThunkBufferId kId789 = ThunkBufferId::Create(ThunkId(789), 0).value();\n   constexpr std::array<uint32_t, 1> kInput123 = {0x01230123};\n   constexpr std::array<uint32_t, 1> kInput456 = {0x04560456};\n   constexpr std::array<uint32_t, 1> kInput789 = {0x07890789};\n   TF_ASSERT_OK_AND_ASSIGN(\n       se::gpu::BufferDebugLog device_log,\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n-  TF_EXPECT_OK(AppendChecksumOnDevice(kId123, kInput123, device_log));\n-  TF_EXPECT_OK(AppendChecksumOnDevice(kId456, kInput456, device_log));\n-  TF_EXPECT_OK(AppendChecksumOnDevice(kId789, kInput789, device_log));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{123}, kInput123,\n+                                      device_log));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{456}, kInput456,\n+                                      device_log));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{789}, kInput789,\n+                                      device_log));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 3);\n-  EXPECT_EQ(host_log[0].entry_id, kId123);\n+  EXPECT_EQ(host_log[0].entry_id, 123);\n   EXPECT_EQ(host_log[0].value, 0x01230123);\n-  EXPECT_EQ(host_log[1].entry_id, kId456);\n+  EXPECT_EQ(host_log[1].entry_id, 456);\n   EXPECT_EQ(host_log[1].value, 0x04560456);\n-  EXPECT_EQ(host_log[2].entry_id, kId789);\n+  EXPECT_EQ(host_log[2].entry_id, 789);\n   EXPECT_EQ(host_log[2].value, 0x07890789);\n }\n \n TEST_F(ChecksumKernelTest, DiscardsOverflowingChecksums) {\n   se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(\n       sizeof(BufferDebugLogHeader) + sizeof(BufferDebugLogEntry) * 2);\n-  ThunkBufferId kId123 = ThunkBufferId::Create(ThunkId(123), 0).value();\n-  ThunkBufferId kId456 = ThunkBufferId::Create(ThunkId(456), 0).value();\n-  ThunkBufferId kId789 = ThunkBufferId::Create(ThunkId(789), 0).value();\n   constexpr std::array<uint32_t, 1> kInput123 = {0x01230123};\n   constexpr std::array<uint32_t, 1> kInput456 = {0x04560456};\n   constexpr std::array<uint32_t, 1> kInput789 = {0x07890789};\n   TF_ASSERT_OK_AND_ASSIGN(\n       se::gpu::BufferDebugLog device_log,\n       se::gpu::BufferDebugLog::CreateOnDevice(*stream_, mem));\n \n-  TF_EXPECT_OK(AppendChecksumOnDevice(kId123, kInput123, device_log));\n-  TF_EXPECT_OK(AppendChecksumOnDevice(kId456, kInput456, device_log));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{123}, kInput123,\n+                                      device_log));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{456}, kInput456,\n+                                      device_log));\n   // This entry will be discarded.\n-  TF_EXPECT_OK(AppendChecksumOnDevice(kId789, kInput789, device_log));\n+  TF_EXPECT_OK(AppendChecksumOnDevice(BufferDebugLogEntryId{789}, kInput789,\n+                                      device_log));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 2);\n-  EXPECT_EQ(host_log[0].entry_id, kId123);\n+  EXPECT_EQ(host_log[0].entry_id, 123);\n   EXPECT_EQ(host_log[0].value, 0x01230123);\n-  EXPECT_EQ(host_log[1].entry_id, kId456);\n+  EXPECT_EQ(host_log[1].entry_id, 456);\n   EXPECT_EQ(host_log[1].value, 0x04560456);\n }\n "
        },
        {
            "sha": "f774802d4b97597c19f0ff57c10f1fcb3f9d3895",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -954,7 +954,6 @@ cc_library(\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n@@ -969,20 +968,16 @@ xla_test(\n         \":buffer_debug_log\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_proto_cc\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/backends/gpu/runtime:thunk_buffer_id\",\n         \"//xla/backends/gpu/runtime:thunk_id\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n-        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n-        \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )\n@@ -992,7 +987,6 @@ cc_library(\n     hdrs = [\"buffer_debug_xor_checksum_kernel.h\"],\n     deps = [\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/backends/gpu/runtime:thunk_buffer_id\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n     ],\n@@ -1004,7 +998,6 @@ cc_library(\n     deps = [\n         \"//xla:types\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/backends/gpu/runtime:thunk_buffer_id\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n     ],"
        },
        {
            "sha": "59d79c7c686036aa92038f623a3d0bc4d3be1ffd",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_log.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -29,7 +29,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n \n namespace stream_executor::gpu {\n \n@@ -91,22 +90,4 @@ absl::StatusOr<std::vector<BufferDebugLogEntry>> BufferDebugLog::ReadFromDevice(\n   return entries;\n }\n \n-absl::StatusOr<xla::gpu::BufferDebugLogProto> BufferDebugLog::ReadProto(\n-    Stream& stream) const {\n-  TF_ASSIGN_OR_RETURN(std::vector<BufferDebugLogEntry> entries,\n-                      ReadFromDevice(stream));\n-\n-  xla::gpu::BufferDebugLogProto buffer_debug_log_proto;\n-  buffer_debug_log_proto.mutable_entries()->Reserve(entries.size());\n-  for (const auto& entry : entries) {\n-    xla::gpu::BufferDebugLogEntryProto* entry_proto =\n-        buffer_debug_log_proto.add_entries();\n-    entry_proto->set_thunk_id(entry.entry_id.thunk_id().value());\n-    entry_proto->set_buffer_idx(entry.entry_id.buffer_idx());\n-    entry_proto->set_checksum(entry.value);\n-  }\n-\n-  return buffer_debug_log_proto;\n-}\n-\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "9fa7be420c7594a4732ff9085494b9d72dda68b4",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_log.h",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.h?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -81,12 +81,6 @@ class BufferDebugLog {\n   absl::StatusOr<std::vector<xla::gpu::BufferDebugLogEntry>> ReadFromDevice(\n       Stream& stream) const;\n \n-  // Reads all entries from the device log into a proto dump.\n-  //\n-  // `stream` must be associated with the same device as the one used to create\n-  // the log.\n-  absl::StatusOr<xla::gpu::BufferDebugLogProto> ReadProto(Stream& stream) const;\n-\n   // Returns a view of the `BufferDebugLogHeader`.\n   //\n   // The returned `DeviceMemory` gets invalidated when the `BufferDebugLog` is"
        },
        {
            "sha": "2275aae13849e4237bae8c4f3e76eb995889a15c",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_log_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log_test.cc?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -26,28 +26,22 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log.pb.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n-#include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/util/proto/proto_matchers.h\"\n \n namespace stream_executor::gpu {\n namespace {\n \n-using ::tsl::proto_testing::EqualsProto;\n using ::xla::gpu::BufferDebugLogEntry;\n using ::xla::gpu::BufferDebugLogHeader;\n-using ::xla::gpu::ThunkBufferId;\n using ::xla::gpu::ThunkId;\n \n class BufferDebugLogTest : public ::testing::Test {\n@@ -121,33 +115,5 @@ TEST_F(BufferDebugLogTest,\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n-TEST_F(BufferDebugLogTest, ReadAsProto) {\n-  DeviceMemory<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(\n-      BufferDebugLog::RequiredSizeForEntries(10));\n-  const BufferDebugLogHeader header = {/*write_idx=*/2,\n-                                       /*capacity=*/10};\n-  const BufferDebugLogEntry entries[] = {\n-      {/*entry_id=*/ThunkBufferId::Create(ThunkId(123), 4).value(),\n-       /*value=*/12341234},\n-      {/*entry_id=*/ThunkBufferId::Create(ThunkId(567), 8).value(),\n-       /*value=*/56785678},\n-  };\n-  std::vector<uint8_t> log_data(sizeof(header) + sizeof(entries));\n-  memcpy(log_data.data(), &header, sizeof(header));\n-  memcpy(log_data.data() + sizeof(header), entries, sizeof(entries));\n-  TF_ASSERT_OK(stream_->MemcpyH2D(absl::MakeConstSpan(log_data), &log_buffer));\n-  TF_ASSERT_OK(stream_->BlockHostUntilDone());\n-\n-  BufferDebugLog device_log =\n-      BufferDebugLog::FromDeviceMemoryUnchecked(log_buffer);\n-  TF_ASSERT_OK_AND_ASSIGN(xla::gpu::BufferDebugLogProto log_proto,\n-                          device_log.ReadProto(*stream_));\n-\n-  EXPECT_THAT(log_proto, EqualsProto(R\"pb(\n-                entries { thunk_id: 123 buffer_idx: 4 checksum: 12341234 }\n-                entries { thunk_id: 567 buffer_idx: 8 checksum: 56785678 }\n-              )pb\"));\n-}\n-\n }  // namespace\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "812fecc3075915ed3d0f115df6ca8ad088ce104c",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_nan_count_kernel.h",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_nan_count_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_nan_count_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_nan_count_kernel.h?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n #include <cstdint>\n \n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/types.h\"\n@@ -32,16 +31,16 @@ namespace stream_executor::gpu {\n // This kernel MUST execute on a single thread block.\n struct BufferDebugNanCountF32Kernel {\n   using KernelType =\n-      TypedKernel<xla::gpu::ThunkBufferId, DeviceMemory<float>, uint64_t,\n-                  DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n+      TypedKernel<xla::gpu::BufferDebugLogEntryId, DeviceMemory<float>,\n+                  uint64_t, DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n                   DeviceMemory<xla::gpu::BufferDebugLogEntry>>;\n };\n \n struct BufferDebugNanCountBf16Kernel {\n-  using KernelType =\n-      TypedKernel<xla::gpu::ThunkBufferId, DeviceMemory<Eigen::bfloat16>,\n-                  uint64_t, DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n-                  DeviceMemory<xla::gpu::BufferDebugLogEntry>>;\n+  using KernelType = TypedKernel<xla::gpu::BufferDebugLogEntryId,\n+                                 DeviceMemory<Eigen::bfloat16>, uint64_t,\n+                                 DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n+                                 DeviceMemory<xla::gpu::BufferDebugLogEntry>>;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "0cce7b2e57588b09d85c49c9e6bda13b0d4f51c8",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_xor_checksum_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7c995b45fa5f70850651c8f722680425daf21ae/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_xor_checksum_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_xor_checksum_kernel.h?ref=e7c995b45fa5f70850651c8f722680425daf21ae",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n #include <cstdint>\n \n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/backends/gpu/runtime/thunk_buffer_id.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n@@ -31,8 +30,8 @@ namespace stream_executor::gpu {\n // This kernel MUST execute on a single thread block.\n struct BufferDebugXorChecksumKernel {\n   using KernelType =\n-      TypedKernel<xla::gpu::ThunkBufferId, DeviceMemory<uint8_t>, uint64_t,\n-                  DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n+      TypedKernel<xla::gpu::BufferDebugLogEntryId, DeviceMemory<uint8_t>,\n+                  uint64_t, DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n                   DeviceMemory<xla::gpu::BufferDebugLogEntry>>;\n };\n "
        }
    ],
    "stats": {
        "total": 808,
        "additions": 348,
        "deletions": 460
    }
}