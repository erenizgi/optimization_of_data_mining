{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 812274823",
    "sha": "8745d364843e7562738ef9f60d71536153440490",
    "files": [
        {
            "sha": "8d74988a9a275d89ff3a82669facf347e3d6cc8c",
            "filename": "third_party/xla/xla/stream_executor/rocm/hip_blas_lt.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.cc?ref=8745d364843e7562738ef9f60d71536153440490",
            "patch": "@@ -149,7 +149,7 @@ static absl::StatusOr<hipblasLtEpilogue_t> AsHipblasLtEpilogue(\n absl::Status BlasLt::Init() {\n   hipblasLtHandle_t blas_lt;\n   SE_HIPBLAS_RETURN_IF_ERROR(wrap::hipblasLtCreate(&blas_lt));\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   blas_lt_.reset(blas_lt);\n   return absl::OkStatus();\n }\n@@ -223,7 +223,7 @@ auto BlasLt::MatmulPlan::GetAlgorithms(const Stream* stream,\n   std::vector<hipblasLtMatmulHeuristicResult_t> results(max_algorithm_count);\n   {\n     auto blas_lt = static_cast<BlasLt*>(gpu::BlasLt::Get(stream));\n-    absl::MutexLock lock(&blas_lt->mu_);\n+    absl::MutexLock lock(blas_lt->mu_);\n     TF_RET_CHECK(blas_lt->blas_lt_ != nullptr);\n \n     hipblasLtMatmulPreference_t hip_preference;\n@@ -426,7 +426,7 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n \n   auto palgo = std::any_cast<hipblasLtMatmulAlgo_t>(&algorithm_->opaque_algo);\n   {\n-    absl::MutexLock lock(&blas_lt->mu_);\n+    absl::MutexLock lock(blas_lt->mu_);\n     TF_RET_CHECK(blas_lt->blas_lt_ != nullptr);\n     // We must set the bias and aux pointers while holding the mutex, to avoid a\n     // potential race condition from multiple threads sharing the same plan."
        },
        {
            "sha": "732454d86d21815bc920add9aa64ac240a65586d",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_blas.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.cc?ref=8745d364843e7562738ef9f60d71536153440490",
            "patch": "@@ -183,7 +183,7 @@ bool ROCMBlas::SetStream(Stream *stream) {\n }\n \n absl::StatusOr<bool> ROCMBlas::IsMainStreamSet() const {\n-  absl::MutexLock lock{&mu_};\n+  absl::MutexLock lock{mu_};\n   CHECK(blas_ != nullptr);\n   hipStream_t handle{};\n   if (auto ret = wrap::rocblas_get_stream(blas_, &handle);\n@@ -365,7 +365,7 @@ template <typename FuncT, typename... Args>\n absl::Status ROCMBlas::DoBlasInternalImpl(FuncT rocblas_func, Stream *stream,\n                                           bool pointer_mode_host,\n                                           bool err_on_failure, Args &&...args) {\n-  absl::MutexLock lock{&mu_};\n+  absl::MutexLock lock{mu_};\n \n   CHECK(blas_ != nullptr);\n   std::unique_ptr<ActivateContext> activation = parent_->Activate();\n@@ -1260,7 +1260,7 @@ IMPL_DoBlasGemmBatched(float, wrap::rocblas_sgemm_strided_batched)\n }\n \n absl::Status ROCMBlas::GetVersion(std::string *version) {\n-  absl::MutexLock lock{&mu_};\n+  absl::MutexLock lock{mu_};\n   size_t len = 0;\n   if (auto res = wrap::rocblas_get_version_string_size(&len);\n       res != rocblas_status_success) {"
        },
        {
            "sha": "aeb38183f084955c5189608fa7ae346628317db2",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_dnn.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc?ref=8745d364843e7562738ef9f60d71536153440490",
            "patch": "@@ -628,7 +628,7 @@ class CachedFusionPlans {\n                            miopenFusionPlanDescriptor_t* fusion_plan,\n                            miopenFusionDirection_t fusion_direction,\n                            miopenTensorDescriptor_t input_descriptor) {\n-    absl::MutexLock lock{&cached_plans_mutex};\n+    absl::MutexLock lock{cached_plans_mutex};\n \n     bool found_cached_plan = false;\n \n@@ -654,7 +654,7 @@ class CachedFusionPlans {\n \n   // Need to figure out the right place to call this routine\n   static void Clear() {\n-    absl::MutexLock lock{&cached_plans_mutex};\n+    absl::MutexLock lock{cached_plans_mutex};\n \n     for (auto it : cached_plans) {\n       auto status = wrap::miopenDestroyFusionPlan(it.second);\n@@ -671,13 +671,13 @@ class CachedFusionPlans {\n \n   // Is the Fusion plan corresponding to this hash unsupported\n   static bool IsUnsupportedFusionPlan(uint64_t hash) {\n-    absl::MutexLock lock{&cached_plans_mutex};\n+    absl::MutexLock lock{cached_plans_mutex};\n     return unsupported_plans.count(hash) > 0;\n   }\n \n   // Mark the given hash value as corresponding to an unsupported fusion plan\n   static void MarkFusionPlanUnsupported(uint64_t hash) {\n-    absl::MutexLock lock{&cached_plans_mutex};\n+    absl::MutexLock lock{cached_plans_mutex};\n     unsupported_plans.insert(hash);\n   }\n \n@@ -747,7 +747,7 @@ class MIOpenAccess {\n   explicit MIOpenAccess(miopenHandle_t handle) : handle_(handle) {}\n \n   ~MIOpenAccess() {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     wrap::miopenDestroy(handle_);\n   }\n "
        },
        {
            "sha": "07e8bd2031a36926d0ab2a6799d179d297e85fa6",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc?ref=8745d364843e7562738ef9f60d71536153440490",
            "patch": "@@ -504,7 +504,7 @@ std::unique_ptr<ActivateContext> RocmExecutor::Activate() {\n }\n \n bool RocmExecutor::UnloadModule(ModuleHandle module_handle) {\n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   return UnloadGpuBinary(module_handle);\n }\n \n@@ -533,7 +533,7 @@ absl::StatusOr<DeviceMemoryBase> RocmExecutor::GetMemoryRange(\n absl::StatusOr<std::shared_ptr<DeviceMemoryBase>>\n RocmExecutor::CreateOrShareConstant(Stream* stream,\n                                     absl::Span<const uint8_t> content) {\n-  absl::MutexLock lock{&shared_constants_mu_};\n+  absl::MutexLock lock{shared_constants_mu_};\n   // We assume all constants are uniquely identified by this hash. In the\n   // (highly unlikely) event of a hash collision, the program will likely crash\n   // (because the cached constant that will be returned by mistake is unlikely\n@@ -615,7 +615,7 @@ bool RocmExecutor::UnloadGpuBinary(ModuleHandle module_handle) {\n void RocmExecutor::UnloadKernel(const Kernel* kernel) {\n   VLOG(3) << \"Unloading kernel \" << kernel << \" : \" << kernel->name();\n \n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   loaded_kernels_.erase(kernel);\n   auto gpu_binary_it = kernel_to_gpu_binary_.find(kernel);\n   if (kernel_to_gpu_binary_.end() == gpu_binary_it) {\n@@ -651,7 +651,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> RocmExecutor::LoadKernel(\n   if (spec.has_cuda_cubin_in_memory()) {\n     const char* hsaco = reinterpret_cast<const char*>(\n         spec.cuda_cubin_in_memory()->cubin_bytes.data());\n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     ModuleHandle module_handle{hsaco};\n     hipModule_t& module = in_memory_modules_[module_handle];\n \n@@ -686,7 +686,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> RocmExecutor::LoadKernel(\n     return absl::InternalError(\"No method of loading ROCM kernel provided\");\n   }\n \n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   loaded_kernels_.insert(rocm_kernel.get());\n \n   // We have to trust the kernel loader spec arity because there doesn't appear\n@@ -710,7 +710,7 @@ absl::StatusOr<ModuleHandle> RocmExecutor::LoadModule(\n \n   // TODO(ROCm): Need  generic term instead of cubin/cuda/ptx\n   if (spec.has_cuda_cubin_in_memory()) {\n-    absl::MutexLock lock{&in_memory_modules_mu_};\n+    absl::MutexLock lock{in_memory_modules_mu_};\n     return LoadModuleFromHsaco(\n         reinterpret_cast<const char*>(spec.cuda_cubin_in_memory().data()));\n   } else {\n@@ -918,18 +918,18 @@ absl::Status RocmExecutor::SynchronousMemcpy(void* host_dst,\n \n void RocmExecutor::DeallocateStream(Stream* stream) {\n   {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     if (dnn_ != nullptr) {\n       dnn_->NotifyStreamDestroyed(stream);\n     }\n   }\n   RocmStream* rocm_stream = static_cast<RocmStream*>(stream);\n-  absl::MutexLock l(&alive_gpu_streams_mu_);\n+  absl::MutexLock l(alive_gpu_streams_mu_);\n   alive_gpu_streams_.erase(rocm_stream->stream_handle());\n }\n \n absl::Status RocmExecutor::InitBlas() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   PluginRegistry* registry = PluginRegistry::Instance();\n   TF_ASSIGN_OR_RETURN(\n       auto factory,\n@@ -939,12 +939,12 @@ absl::Status RocmExecutor::InitBlas() {\n }\n \n blas::BlasSupport* RocmExecutor::AsBlas() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   return blas_.get();\n }\n \n dnn::DnnSupport* RocmExecutor::AsDnn() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (dnn_ != nullptr) {\n     return dnn_.get();\n   }\n@@ -965,7 +965,7 @@ dnn::DnnSupport* RocmExecutor::AsDnn() {\n }\n \n fft::FftSupport* RocmExecutor::AsFft() {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (fft_ != nullptr) {\n     return fft_.get();\n   }\n@@ -1003,7 +1003,7 @@ absl::StatusOr<DeviceMemoryBase> RocmExecutor::GetSymbol(\n   void* mem = nullptr;\n   size_t bytes = 0;\n \n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   if (static_cast<bool>(module_handle)) {\n     auto it = gpu_binary_to_module_.find(module_handle);\n     CHECK(it != gpu_binary_to_module_.end());\n@@ -1052,7 +1052,7 @@ absl::StatusOr<std::unique_ptr<Event>> RocmExecutor::CreateEvent() {\n absl::StatusOr<std::unique_ptr<Stream>> RocmExecutor::CreateStream(\n     std::optional<std::variant<StreamPriority, int>> priority) {\n   TF_ASSIGN_OR_RETURN(auto stream, RocmStream::Create(this, priority));\n-  absl::MutexLock l(&alive_gpu_streams_mu_);\n+  absl::MutexLock l(alive_gpu_streams_mu_);\n   alive_gpu_streams_[stream->stream_handle()] = stream.get();\n   return std::move(stream);\n }\n@@ -1201,7 +1201,7 @@ absl::StatusOr<MemoryType> RocmExecutor::GetPointerMemorySpace(\n \n absl::StatusOr<const RocmKernel*> RocmExecutor::GetRocmKernel(\n     const Kernel* kernel) {\n-  absl::MutexLock lock{&in_memory_modules_mu_};\n+  absl::MutexLock lock{in_memory_modules_mu_};\n   auto it = loaded_kernels_.find(kernel);\n   if (it == loaded_kernels_.end()) {\n     return absl::NotFoundError(\"Kernel not loaded in this executor.\");"
        },
        {
            "sha": "41843efe4fee07d2ed29b78a8a1fcc73123e177a",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8745d364843e7562738ef9f60d71536153440490/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h?ref=8745d364843e7562738ef9f60d71536153440490",
            "patch": "@@ -116,7 +116,7 @@ class RocmExecutor : public GpuExecutor {\n   absl::StatusOr<MemoryType> GetPointerMemorySpace(const void* ptr) override;\n \n   Stream* FindAllocatedStream(void* gpu_stream) override {\n-    absl::MutexLock lock(&alive_gpu_streams_mu_);\n+    absl::MutexLock lock(alive_gpu_streams_mu_);\n     auto it = alive_gpu_streams_.find(gpu_stream);\n     if (it == alive_gpu_streams_.end()) {\n       return nullptr;"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 27,
        "deletions": 27
    }
}