{
    "author": "penpornk",
    "message": "[xla:cpu:onednn] Remove INTEL_MKL ifdef guards around `onednn_contraction_rewriter`.\n\n+ Also the guards in `onednn_float_support` and oneDNN contraction tests.\n+ Minor ClangTidy/Linter errors/warnings fixes.\n\nPiperOrigin-RevId: 812294970",
    "sha": "472031b8dc198227f2d9a2203b06a91d7b99d8d6",
    "files": [
        {
            "sha": "580b7c1f4f6a38d2c54e4c3f1520033890a478e0",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -1,5 +1,6 @@\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\"//xla/tsl:tsl.bzl\", \"tsl_copts\")\n+load(\"//xla/tsl/mkl:graph.bzl\", \"onednn_cc_test\")\n load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n \n package(\n@@ -473,12 +474,10 @@ xla_cc_test(\n     ],\n )\n \n-xla_cc_test(\n+onednn_cc_test(\n     name = \"onednn_matmul_benchmark_test\",\n     srcs = [\"onednn_matmul_benchmark_test.cc\"],\n     copts = tsl_copts(),\n-    fail_if_no_test_linked = False,  # NOLINT=This contains benchmarks only, no tests.\n-    fail_if_no_test_selected = False,  # NOLINT=This contains benchmarks only, no tests.\n     deps = [\n         \":hlo_benchmark_runner\",\n         \"//xla:literal\",\n@@ -488,6 +487,8 @@ xla_cc_test(\n         \"//xla/service/cpu:onednn_util\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/platform:test_benchmark\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\","
        },
        {
            "sha": "19a1eba1bc89485f00a6844e521a54b05df4c3de",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/onednn_matmul_benchmark_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fonednn_matmul_benchmark_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fonednn_matmul_benchmark_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fonednn_matmul_benchmark_test.cc?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -13,24 +13,23 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#if defined(INTEL_MKL)\n-\n #include <cstdint>\n #include <random>\n #include <vector>\n \n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/cpu/benchmarks/hlo_benchmark_runner.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n+#include \"xla/primitive_util.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/test_benchmark.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace xla::cpu {\n \n@@ -93,5 +92,3 @@ BENCHMARK_ONEDNN_MM(BF16);\n BENCHMARK_ONEDNN_MM(F16);\n \n }  // namespace xla::cpu\n-\n-#endif  // INTEL_MKL"
        },
        {
            "sha": "a84125dbf3c33de6f515c8248b3899075e780f05",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -1856,7 +1856,7 @@ onednn_cc_library(\n     ],\n )\n \n-cc_library(\n+onednn_cc_library(\n     name = \"onednn_contraction_rewriter\",\n     srcs = [\"onednn_contraction_rewriter.cc\"],\n     hdrs = [\n@@ -1875,19 +1875,29 @@ cc_library(\n         \":onednn_pattern_utils\",\n         \":onednn_util\",\n         \"//xla:executable_run_options\",\n+        \"//xla:literal\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n+        \"//xla:types\",\n+        \"//xla:util\",\n         \"//xla/hlo/evaluator:hlo_evaluator\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:pattern_matcher\",\n         \"//xla/tsl/mkl:onednn\",\n         \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@eigen_archive//:eigen3\",\n-        \"@local_tsl//tsl/platform:env\",\n         \"@local_tsl//tsl/platform:logging\",\n         \"@local_tsl//tsl/platform:platform_port\",\n     ],\n@@ -1918,13 +1928,14 @@ cc_library(\n     ],\n )\n \n-cc_library(\n+onednn_cc_library(\n     name = \"onednn_float_support\",\n     srcs = [\"onednn_float_support.cc\"],\n     hdrs = [\"onednn_float_support.h\"],\n     copts = tsl_copts(),\n     deps = [\n         \":onednn_contraction_rewriter\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/service:float_support\",\n     ],\n )"
        },
        {
            "sha": "c50575650310ba8e4b44547bb2819d90d1b6fbea",
            "filename": "third_party/xla/xla/service/cpu/onednn_contraction_rewriter.cc",
            "status": "modified",
            "additions": 121,
            "deletions": 68,
            "changes": 189,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -13,45 +13,72 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#if defined(INTEL_MKL)\n+#include <algorithm>\n+#include <cstdint>\n+#include <cstdlib>\n+#include <limits>\n+#include <memory>\n+#include <optional>\n+#include <type_traits>\n+#include <vector>\n \n #define EIGEN_USE_THREADS\n \n-#include \"xla/service/cpu/onednn_contraction_rewriter.h\"\n-\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"Eigen/Core\"\n+#include \"oneapi/dnnl/dnnl.hpp\"\n+#include \"oneapi/dnnl/dnnl_common.hpp\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/evaluator/hlo_evaluator.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/literal.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n #include \"xla/service/cpu/onednn_config.pb.h\"\n+#include \"xla/service/cpu/onednn_contraction_rewriter.h\"\n #include \"xla/service/cpu/onednn_convolution.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n #include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/onednn_pattern_utils.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/pattern_matcher.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/tsl/util/onednn_threadpool.h\"\n+#include \"xla/types.h\"\n+#include \"xla/util.h\"\n+#include \"tsl/platform/cpu_info.h\"\n #include \"tsl/platform/logging.h\"  // IWYU pragma: keep\n \n namespace xla {\n namespace cpu {\n \n namespace {\n+\n namespace m = match;\n namespace pu = ::xla::cpu::onednn_pattern_utils_internal;\n \n inline absl::Status ValidateDotDimensionNumbers(\n     const DotDimensionNumbers& dim_numbers) {\n   // Checks some invariants that do not hold in general, but DotDecomposer\n   // should have established for us.\n-  TF_RET_CHECK(dim_numbers.lhs_contracting_dimensions_size() == 1);\n+  TF_RET_CHECK(dim_numbers.lhs_contracting_dimensions().size() == 1);\n   std::vector<int64_t> batch_dim_numbers(\n-      dim_numbers.lhs_batch_dimensions_size());\n+      dim_numbers.lhs_batch_dimensions().size());\n   absl::c_iota(batch_dim_numbers, 0);\n   TF_RET_CHECK(\n       absl::c_equal(batch_dim_numbers, dim_numbers.lhs_batch_dimensions()));\n@@ -61,7 +88,7 @@ inline absl::Status ValidateDotDimensionNumbers(\n }\n \n // Whether the element type of instr is compatible with oneDNN kernels.\n-// TODO(intel-tf): Restict compatible types based on instruction kind.\n+// TODO(intel-tf): Restrict compatible types based on instruction kind.\n inline bool CompatibleElementType(const HloInstruction* instr) {\n   PrimitiveType element_type = instr->shape().element_type();\n   return element_type == BF16 || element_type == F32 || element_type == F16;\n@@ -79,7 +106,9 @@ inline auto BitcastWithReshapeSemantics(HloInstruction** bitcast,\n   // the layouts are checked to be rowmajor since the current pass runs after\n   // the layout assignment and oneDNN matmul is enabled for rowmajor layouts.\n   auto is_reshape = [](const HloInstruction* instr) -> bool {\n-    if (!instr) return false;\n+    if (!instr) {\n+      return false;\n+    }\n     auto input_shape = instr->operand(0)->shape();\n     auto output_shape = instr->shape();\n     bool is_same_type = ShapeUtil::SameElementType(input_shape, output_shape);\n@@ -148,7 +177,7 @@ inline auto BcastConvertConstScalar(double value) {\n \n inline bool IsBatchDot(const HloInstruction& instr) {\n   if (auto* dot_instr = DynCast<HloDotInstruction>(&instr)) {\n-    return dot_instr->dot_dimension_numbers().lhs_batch_dimensions_size() > 0;\n+    return !dot_instr->dot_dimension_numbers().lhs_batch_dimensions().empty();\n   }\n   return false;\n }\n@@ -162,7 +191,9 @@ auto ConstScalarNear(double value) {\n             static_cast<const HloConstantInstruction*>(instr)\n                 ->literal()\n                 .GetAsDouble({});\n-        if (!actual.has_value()) return false;\n+        if (!actual.has_value()) {\n+          return false;\n+        }\n         double epsilon;\n         switch (instr->shape().element_type()) {\n           case F16:\n@@ -312,11 +343,10 @@ auto GELUActivation(HloInstruction* instr, HloInstruction** src) {\n                        .WithOneUser())\n             .WithOneUser();\n \n-    if (Match(errf, errf_apprx_pattern)) {\n-      // Matched Gelu-approximate pattern\n+    if (Match(errf, errf_apprx_pattern)) {  // Gelu-approximate pattern.\n       return OneDnnFusionConfig::GELU_TANH;\n-    } else if (Match(errf, errf_exact_pattern)) {\n-      // Matched Gelu-exact pattern\n+    }\n+    if (Match(errf, errf_exact_pattern)) {  // Gelu-exact pattern.\n       return OneDnnFusionConfig::GELU_ERF;\n     }\n   }\n@@ -335,7 +365,7 @@ absl::StatusOr<Shape> AdjustAddendShape(const HloInstruction* contraction,\n     // Add is enabled.\n     if (IsOneDnnConvolutionInstr(contraction) &&\n         ShapeUtil::TrueNumDimensions(addend->shape()) == 1 &&\n-        addend->shape().dimensions_size() != 1) {\n+        addend->shape().dimensions().size() != 1) {\n       return ShapeUtil::FilterDimensions(\n           [&addend](int64_t dim) {\n             return ShapeUtil::GetDimension(addend->shape(), dim) != 1;\n@@ -361,7 +391,7 @@ absl::StatusOr<Shape> AdjustAddendShape(const HloInstruction* contraction,\n   //      bitcast = f32[3,1,1,6]{3,2,1,0} bitcast(arg)\n   //      fused = f32[3,4,5,6]{3,2,1,0} custom-call((..., bitcast)\n   auto kept_dimensions = bcast->dimensions();\n-  for (int i = 0; i < new_shape.dimensions_size(); i++) {\n+  for (int i = 0; i < new_shape.dimensions().size(); i++) {\n     if (!absl::c_linear_search(kept_dimensions, i)) {\n       new_shape.set_dimensions(i, 1);\n     }\n@@ -371,7 +401,7 @@ absl::StatusOr<Shape> AdjustAddendShape(const HloInstruction* contraction,\n   // deleted from the new_shape.\n   auto instr_shape = contraction->shape();\n   int64_t rank_difference =\n-      new_shape.dimensions_size() - instr_shape.dimensions_size();\n+      new_shape.dimensions().size() - instr_shape.dimensions().size();\n   auto new_dims = new_shape.dimensions();\n   std::vector<int64_t> dims_to_delete;\n   for (int i = 0; i < rank_difference; ++i) {\n@@ -383,7 +413,7 @@ absl::StatusOr<Shape> AdjustAddendShape(const HloInstruction* contraction,\n \n   // New shape for bias should satisfy the condition:\n   //   rank(new_shape) <= rank(instr).\n-  if (new_shape.dimensions_size() > instr_shape.dimensions_size()) {\n+  if (new_shape.dimensions().size() > instr_shape.dimensions().size()) {\n     return absl::CancelledError(\n         \"Bias shape could not be adjusted for a fusion.\");\n   }\n@@ -406,19 +436,22 @@ absl::StatusOr<Shape> AdjustBinaryOperandShape(\n \n inline bool IsOperandFusible(HloInstruction* operand, HloInstruction* instr) {\n   // Check if the operand's shape is compatible for fusion.\n-  // An operand is fusable if\n+  // An operand is fusible if\n   //    1. rank(operand) <= rank(instr) and\n   //    2. Starting from the last dim in backward direction, the dimension\n   //       size of operand is either 1 or same to dot.\n   auto operand_dims = operand->shape().dimensions();\n   auto instr_dims = instr->shape().dimensions();\n-  if (operand_dims.size() > instr_dims.size()) return false;\n+  if (operand_dims.size() > instr_dims.size()) {\n+    return false;\n+  }\n   int operand_idx = operand_dims.size() - 1;\n   int instr_idx = instr_dims.size() - 1;\n   for (; operand_idx >= 0; --operand_idx, --instr_idx) {\n     if (operand_dims[operand_idx] != 1 &&\n-        operand_dims[operand_idx] != instr_dims[instr_idx])\n+        operand_dims[operand_idx] != instr_dims[instr_idx]) {\n       return false;\n+    }\n   }\n   return true;\n }\n@@ -446,56 +479,64 @@ inline auto OptionalConvertAndBitcast(HloInstruction** optional_convert,\n \n bool OneDnnContractionRewriter::ShouldRewriteDot(\n     const HloInstruction* dot_instr, bool before_layout_assignment) {\n-  if (dot_instr->opcode() != HloOpcode::kDot) return false;\n-  // Currently, blocking control dependencies\n-  if (dot_instr->HasControlDependencies()) return false;\n-  if (!IsSupportedType(dot_instr->shape().element_type())) return false;\n-  if (dot_instr->operands().size() != 2) return false;\n-\n-  // Currently, we rewrite when the data type is F32 or BF16. Note we do not\n-  // need to check equality of contraction dim-size of the operands. HLO\n-  // verifier already does the job. We, however, need to check if contraction\n-  // is over only 1 dimension (a.k.a. K dimension in matrix-multiplication\n-  // parlance). We also restrict that batch dimensions of the operands\n-  // match.\n+  if (dot_instr->opcode() != HloOpcode::kDot) {\n+    return false;\n+  }\n+  // Blocking control dependencies.\n+  if (dot_instr->HasControlDependencies() ||\n+      !IsSupportedType(dot_instr->shape().element_type()) ||\n+      dot_instr->operands().size() != 2) {\n+    return false;\n+  }\n+\n+  // We rewrite when the data type is F32 or BF16. We do not need to check\n+  // equality of contraction dim-size of the operands. HLO verifier already does\n+  // the job. We, however, need to check if contraction is over only 1 dimension\n+  // (i.e., K dimension in matrix-multiplication parlance). We also restrict\n+  // that batch dimensions of the operands match.\n   const Shape& lhs_shape = dot_instr->operand(0)->shape();\n   const Shape& rhs_shape = dot_instr->operand(1)->shape();\n   const Shape& output_shape = dot_instr->shape();\n+\n   // None of the operands and result should be ZeroElementArray.\n   if (ShapeUtil::IsZeroElementArray(lhs_shape) ||\n       ShapeUtil::IsZeroElementArray(rhs_shape) ||\n       ShapeUtil::IsZeroElementArray(output_shape)) {\n     return false;\n   }\n+\n   // OneDNN only supports rank <= kOneDnnMaxNDims and singular non-contracting\n   // dimensions. We should not rewrite if any of these conditions are violated.\n-  if (lhs_shape.dimensions_size() <= 0 ||\n-      lhs_shape.dimensions_size() > kOneDnnMaxNDims ||\n-      rhs_shape.dimensions_size() <= 0 ||\n-      rhs_shape.dimensions_size() > kOneDnnMaxNDims ||\n-      output_shape.dimensions_size() >\n-          std::min({lhs_shape.dimensions_size(), rhs_shape.dimensions_size(),\n-                    kOneDnnMaxNDims})) {\n+  if (lhs_shape.dimensions().empty() ||\n+      lhs_shape.dimensions().size() > kOneDnnMaxNDims ||\n+      rhs_shape.dimensions().empty() ||\n+      rhs_shape.dimensions().size() > kOneDnnMaxNDims ||\n+      output_shape.dimensions().size() >\n+          std::min<uint64_t>({lhs_shape.dimensions().size(),\n+                              rhs_shape.dimensions().size(),\n+                              kOneDnnMaxNDims})) {\n     return false;\n   }\n \n-  // Layout should be row-major, contraction dimensions captures transpose\n-  // scenarios in last two dimensions.\n-  // Col-major layouts are corrected to row-major for BatchDot operation as\n-  // part of the layout-assignment pass.\n-  // Skip row-major layout check before layout-assignment pass\n+  // Layout should be row-major, contraction dimensions capture transpose\n+  // scenarios in the last two dimensions. Col-major layouts are corrected to\n+  // row-major for BatchDot operation as part of the layout-assignment pass.\n+  // Skip row-major layout check before layout-assignment pass.\n   if (!before_layout_assignment) {\n     bool row_major = IsRowMajor(lhs_shape) && IsRowMajor(rhs_shape) &&\n                      IsRowMajor(output_shape);\n-    if (!row_major) return false;\n+    if (!row_major) {\n+      return false;\n+    }\n   }\n \n   auto dot_dim_numbers = dot_instr->dot_dimension_numbers();\n   int64_t lhs_dim_k = dot_dim_numbers.lhs_contracting_dimensions(0);\n   int64_t rhs_dim_k = dot_dim_numbers.rhs_contracting_dimensions(0);\n+\n   // Supported contraction is only in one of last two dimensions.\n-  if (lhs_dim_k < lhs_shape.dimensions_size() - 2 ||\n-      rhs_dim_k < rhs_shape.dimensions_size() - 2) {\n+  if (lhs_dim_k < lhs_shape.dimensions().size() - 2 ||\n+      rhs_dim_k < rhs_shape.dimensions().size() - 2) {\n     return false;\n   }\n \n@@ -506,7 +547,7 @@ bool OneDnnContractionRewriter::ShouldRewriteDot(\n   // matmul is achieved.\n   auto num_flops = xla::HloCostAnalysis::GetDotFlops(lhs_shape, output_shape,\n                                                      dot_dim_numbers);\n-  auto rank = output_shape.dimensions_size();\n+  auto rank = output_shape.dimensions().size();\n   auto flops_threshold = (rank <= 2) ? (1 << 24) : (1 << 19);\n   return (num_flops >= flops_threshold);\n }\n@@ -516,14 +557,20 @@ bool OneDnnContractionRewriter::ShouldRewriteConv(\n   // TODO(intel-tf): remove this restriction after enabling oneDNN convolution\n   // support in thunk runtime.\n   return false;\n-  if (conv_instr->opcode() != HloOpcode::kConvolution) return false;\n-  if (conv_instr->HasControlDependencies()) return false;\n-  if (!IsSupportedType(conv_instr->shape().element_type())) return false;\n-  if (conv_instr->batch_group_count() != 1) return false;\n+\n+  // NOLINTBEGIN(clang-diagnostic-unreachable-code)\n+  if (conv_instr->opcode() != HloOpcode::kConvolution ||\n+      conv_instr->HasControlDependencies() ||\n+      !IsSupportedType(conv_instr->shape().element_type()) ||\n+      conv_instr->batch_group_count() != 1) {\n+    return false;\n+  }\n \n   // TODO(intel-tf): Remove this restriction after enabling backward weights\n   // support\n-  if (conv_instr->operand(1)->opcode() == HloOpcode::kReverse) return false;\n+  if (conv_instr->operand(1)->opcode() == HloOpcode::kReverse) {\n+    return false;\n+  }\n \n   const Shape& inp_shape = conv_instr->operand(0)->shape();\n   const Shape& ker_shape = conv_instr->operand(1)->shape();\n@@ -535,14 +582,17 @@ bool OneDnnContractionRewriter::ShouldRewriteConv(\n   }\n \n   auto dims = conv_instr->window().dimensions().size();\n-  if (dims >= 4 || dims <= 0) return false;\n+  if (dims >= 4 || dims <= 0) {\n+    return false;\n+  }\n \n-  if (inp_shape.dimensions_size() != ker_shape.dimensions_size() ||\n-      inp_shape.dimensions_size() != out_shape.dimensions_size()) {\n+  if (inp_shape.dimensions().size() != ker_shape.dimensions().size() ||\n+      inp_shape.dimensions().size() != out_shape.dimensions().size()) {\n     return false;\n   }\n \n   return true;\n+  // NOLINTEND(clang-diagnostic-unreachable-code)\n }\n \n class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n@@ -552,7 +602,9 @@ class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n   absl::Status HandleDot(HloInstruction* instr) override {\n     HloInstruction* dot_instr;\n     auto pattern = m::Op(&dot_instr).WithOpcode(HloOpcode::kDot);\n-    if (!Match(instr, pattern)) return absl::OkStatus();\n+    if (!Match(instr, pattern)) {\n+      return absl::OkStatus();\n+    }\n \n     TF_RETURN_IF_ERROR(\n         ValidateDotDimensionNumbers(dot_instr->dot_dimension_numbers()));\n@@ -574,12 +626,13 @@ class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n             output_shape,\n             {dot_instr->mutable_operand(0), dot_instr->mutable_operand(1)},\n             \"__onednn$matmul\"));\n+\n     // Set additional info via config, e.g., transpose and fusion info.\n     BackendConfig backend_config;\n     OneDnnMatMulConfig* matmul_config =\n         backend_config.mutable_onednn_matmul_config();\n-    bool transpose_a = (lhs_dim_k != lhs_shape.dimensions_size() - 1);\n-    bool transpose_b = (rhs_dim_k != rhs_shape.dimensions_size() - 2);\n+    bool transpose_a = (lhs_dim_k != lhs_shape.dimensions().size() - 1);\n+    bool transpose_b = (rhs_dim_k != rhs_shape.dimensions().size() - 2);\n     matmul_config->set_transpose_a(transpose_a);\n     matmul_config->set_transpose_b(transpose_b);\n     TF_RETURN_IF_ERROR(matmul_call->set_backend_config(backend_config));\n@@ -601,7 +654,7 @@ class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n     OneDnnConvolutionConfig* conv_config =\n         backend_config.mutable_onednn_conv_config();\n \n-    conv_config->set_dims(conv_shape.dimensions_size());\n+    conv_config->set_dims(conv_shape.dimensions().size());\n     conv_config->set_feature_groups(conv->feature_group_count());\n     conv_config->mutable_input()->mutable_data()->set_batch_dim(\n         conv_dims.input_batch_dimension());\n@@ -687,8 +740,9 @@ class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n         m::Op(&addend_intermediate));\n \n     if (Match(instr, pattern)) {\n-      if (!IsSupportedType(contraction->shape().element_type()))\n+      if (!IsSupportedType(contraction->shape().element_type())) {\n         return absl::OkStatus();\n+      }\n \n       std::vector<HloInstruction*> new_operands;\n       for (auto operand : contraction->operands()) {\n@@ -720,13 +774,14 @@ class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n       // oneDNN library requires Convolution biases to always have rank 1.\n       // Therefore, these bias shapes should remain unchanged.\n       if (IsOneDnnMatmulInstr(contraction) ||\n-          addend->shape().dimensions_size() != 1) {\n+          addend->shape().dimensions().size() != 1) {\n         auto new_shape =\n             AdjustAddendShape(contraction, addend, optional_addend_broadcast);\n         if (!new_shape.ok()) {\n           VLOG(2) << new_shape.status();\n           return absl::OkStatus();\n-        } else if (!ShapeUtil::Equal(*new_shape, addend->shape())) {\n+        }\n+        if (!ShapeUtil::Equal(*new_shape, addend->shape())) {\n           addend = addend->AddInstruction(\n               HloInstruction::CreateBitcast(new_shape.value(), addend));\n         }\n@@ -1139,12 +1194,12 @@ class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n \n     auto lhs_batch_dims = dim_numbers.lhs_batch_dimensions();\n     auto lhs_contraction_dims = dim_numbers.lhs_contracting_dimensions();\n-    bool is_lhs_vector = lhs->shape().dimensions_size() ==\n+    bool is_lhs_vector = lhs->shape().dimensions().size() ==\n                          (lhs_batch_dims.size() + lhs_contraction_dims.size());\n \n     auto rhs_batch_dims = dim_numbers.rhs_batch_dimensions();\n     auto rhs_contraction_dims = dim_numbers.rhs_contracting_dimensions();\n-    bool is_rhs_vector = rhs->shape().dimensions_size() ==\n+    bool is_rhs_vector = rhs->shape().dimensions().size() ==\n                          (rhs_batch_dims.size() + rhs_contraction_dims.size());\n \n     if (!is_lhs_vector && !is_rhs_vector) return dot_instr;\n@@ -1357,7 +1412,7 @@ class OneDnnPostRewriteVisitor : public DfsHloRewriteVisitor {\n     auto weights = custom_call->operand(1);\n     auto weights_shape = weights->shape();\n     Literal weights_literal;\n-    if (!(weights_shape.dimensions_size() == 2 &&\n+    if (!(weights_shape.dimensions().size() == 2 &&\n           evaluator_.TryEvaluate(weights, &weights_literal, true))) {\n       return absl::CancelledError(\n           \"Cannot prepack weights. Not constant 2D weights.\");\n@@ -1488,5 +1543,3 @@ absl::StatusOr<bool> OneDnnContractionRewriter::Run(\n \n }  // namespace cpu\n }  // namespace xla\n-\n-#endif  // INTEL_MKL"
        },
        {
            "sha": "d84c24020c240baa71e67a5a1868669bda2ee414",
            "filename": "third_party/xla/xla/service/cpu/onednn_contraction_rewriter.h",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.h?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -15,18 +15,20 @@ limitations under the License.\n \n #ifndef XLA_SERVICE_CPU_ONEDNN_CONTRACTION_REWRITER_H_\n #define XLA_SERVICE_CPU_ONEDNN_CONTRACTION_REWRITER_H_\n-#if defined(INTEL_MKL)\n \n-#include <optional>\n+#include <variant>\n \n-#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"unsupported/Eigen/CXX11/Tensor\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/cpu/onednn_convolution.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n-#include \"tsl/platform/threadpool.h\"\n+#include \"xla/service/cpu/onednn_util.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n \n namespace xla {\n namespace cpu {\n@@ -86,5 +88,4 @@ struct PrimitiveTrait<config, OneDnnOptimizationConfig*> {\n }  // namespace cpu\n }  // namespace xla\n \n-#endif  // INTEL_MKL\n #endif  // XLA_SERVICE_CPU_ONEDNN_CONTRACTION_REWRITER_H_"
        },
        {
            "sha": "84495f0def5b02674f86e46eaaf155cc0cb50c6b",
            "filename": "third_party/xla/xla/service/cpu/onednn_float_support.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_float_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_float_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_float_support.cc?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -13,10 +13,10 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#if defined(INTEL_MKL)\n-\n #include \"xla/service/cpu/onednn_float_support.h\"\n \n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/cpu/onednn_contraction_rewriter.h\"\n \n namespace xla {\n@@ -64,5 +64,3 @@ bool OneDnnFloatSupport::IsSupported(const HloInstruction& hlo) const {\n \n }  // namespace cpu\n }  // namespace xla\n-\n-#endif  // INTEL_MKL"
        },
        {
            "sha": "9552973825b764f69a73fb7982e0d778d37cbaea",
            "filename": "third_party/xla/xla/service/cpu/onednn_float_support.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_float_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_float_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_float_support.h?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -16,8 +16,9 @@ limitations under the License.\n #ifndef XLA_SERVICE_CPU_ONEDNN_FLOAT_SUPPORT_H_\n #define XLA_SERVICE_CPU_ONEDNN_FLOAT_SUPPORT_H_\n \n-#if defined(INTEL_MKL)\n+#include <cstdint>\n \n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/float_support.h\"\n \n namespace xla {\n@@ -48,5 +49,4 @@ class OneDnnFloatSupport : public FloatSupport {\n }  // namespace cpu\n }  // namespace xla\n \n-#endif  // INTEL_MKL\n #endif  // XLA_SERVICE_CPU_ONEDNN_FLOAT_SUPPORT_H_"
        },
        {
            "sha": "66b9f470afff5864a1948f646ea1821d453558ed",
            "filename": "third_party/xla/xla/service/cpu/tests/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -296,7 +296,7 @@ xla_cc_test(\n     ],\n )\n \n-xla_cc_test(\n+onednn_cc_test(\n     name = \"onednn_matmul_test\",\n     srcs = [\"onednn_matmul_test.cc\"],\n     copts = tsl_copts(),\n@@ -306,23 +306,17 @@ xla_cc_test(\n         \"notap\",\n     ],\n     deps = [\n-        \"//xla:literal\",\n-        \"//xla:shape_util\",\n-        \"//xla/hlo/testlib:filecheck\",\n+        \"//xla:error_spec\",\n         \"//xla/hlo/testlib:test\",\n-        \"//xla/hlo/testlib:test_helpers\",\n-        \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/service:cpu_plugin\",\n-        \"//xla/service/cpu:onednn_contraction_rewriter\",\n         \"//xla/service/cpu:onednn_util\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_absl//absl/strings\",\n-        \"@local_tsl//tsl/platform:platform_port\",\n     ],\n )\n \n-xla_cc_test(\n+onednn_cc_test(\n     name = \"onednn_convolution_test\",\n     srcs = [\"onednn_convolution_test.cc\"],\n     copts = tsl_copts(),"
        },
        {
            "sha": "84dc670cf80a8e0bcc4e5437314a8b4338564a21",
            "filename": "third_party/xla/xla/service/cpu/tests/onednn_convolution_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_convolution_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_convolution_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_convolution_test.cc?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -30,8 +30,6 @@ limitations under the License.\n namespace xla {\n namespace cpu {\n \n-#if defined(INTEL_MKL)\n-\n class ConvolutionTest : public HloTestBase,\n                         public ::testing::WithParamInterface<PrimitiveType> {\n  protected:\n@@ -762,8 +760,6 @@ INSTANTIATE_TEST_SUITE_P(\n       return test_name;\n     });\n \n-#endif  // INTEL_MKL\n-\n // Ensure at least one test case is linked to avoid test failures.\n TEST(Dummy, Test) {}\n "
        },
        {
            "sha": "bbcaca9e0c326bd5b56ce872836bc7954b0721e2",
            "filename": "third_party/xla/xla/service/cpu/tests/onednn_matmul_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_matmul_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/472031b8dc198227f2d9a2203b06a91d7b99d8d6/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_matmul_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_matmul_test.cc?ref=472031b8dc198227f2d9a2203b06a91d7b99d8d6",
            "patch": "@@ -13,24 +13,13 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#if defined(INTEL_MKL)\n-\n #include <string>\n-#include <utility>\n \n #include \"absl/strings/str_replace.h\"\n-#include \"xla/hlo/testlib/filecheck.h\"\n+#include \"xla/error_spec.h\"\n #include \"xla/hlo/testlib/test.h\"\n-#include \"xla/hlo/testlib/test_helpers.h\"\n-#include \"xla/hlo/utils/hlo_matchers.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/service/cpu/onednn_contraction_rewriter.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/tests/hlo_test_base.h\"\n-#include \"tsl/platform/cpu_info.h\"\n-\n-namespace op = xla::testing::opcode_matchers;\n \n namespace xla {\n namespace cpu {\n@@ -1892,5 +1881,3 @@ TEST_F(MatmulTest, MulTanhMul) {\n \n }  // namespace cpu\n }  // namespace xla\n-\n-#endif  // INTEL_MKL"
        }
    ],
    "stats": {
        "total": 276,
        "additions": 157,
        "deletions": 119
    }
}