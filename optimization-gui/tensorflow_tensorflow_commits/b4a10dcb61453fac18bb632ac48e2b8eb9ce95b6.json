{
    "author": "tensorflower-gardener",
    "message": "Remove CustomKernelFusionAutotuner as it is redundant with gemm_fusion_autotuner.\n\n- Deprecating the xla_gpu_enable_custom_fusions flag as it will be no-op after this change.\n\nPiperOrigin-RevId: 805877165",
    "sha": "b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6",
    "files": [
        {
            "sha": "c3a4d08847f28460e419bc9b4ecb62f3814e8e88",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6",
            "patch": "@@ -271,7 +271,6 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_detailed_logging(true);\n   opts.set_xla_enable_dumping(true);\n \n-  opts.set_xla_gpu_enable_custom_fusions(false);\n   opts.set_xla_gpu_nccl_termination_timeout_seconds(-1);\n   opts.set_xla_gpu_enable_shared_constants(true);\n   opts.set_xla_gpu_enable_nccl_user_buffers(false);\n@@ -1686,11 +1685,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n                 bool_setter_for(&DebugOptions::set_xla_dump_full_hlo_config),\n                 debug_options->xla_dump_full_hlo_config(),\n                 \"Enable dumping the full HloModuleConfig proto.\"));\n-  flag_list->push_back(tsl::Flag(\n-      \"xla_gpu_enable_custom_fusions\",\n-      bool_setter_for(&DebugOptions::set_xla_gpu_enable_custom_fusions),\n-      debug_options->xla_gpu_enable_custom_fusions(),\n-      \"Whether to enable XLA custom fusions\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_enable_custom_fusions_re\",\n       string_setter_for(&DebugOptions::set_xla_gpu_enable_custom_fusions_re),"
        },
        {
            "sha": "01846de07fa7cad9cebcc949e8da9d6b692dc5a7",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6",
            "patch": "@@ -1579,7 +1579,6 @@ cc_library(\n         \"//xla/service/debug:unstable_reduction_detector\",\n         \"//xla/service/gpu/autotuning:autotuner_pass\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n-        \"//xla/service/gpu/autotuning:custom_kernel_fusion_autotuner\",\n         \"//xla/service/gpu/model:collective_ptable_stats_collection\",\n         \"//xla/service/gpu/model:gpu_cost_model_stats_collection\",\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\","
        },
        {
            "sha": "22c2ce2eb0e563a8abec6a5b1f5b8b47dd36cbda",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6",
            "patch": "@@ -629,66 +629,6 @@ xla_test(\n     ],\n )\n \n-cc_library(\n-    name = \"custom_kernel_fusion_autotuner\",\n-    srcs = [\"custom_kernel_fusion_autotuner.cc\"],\n-    hdrs = [\"custom_kernel_fusion_autotuner.h\"],\n-    tags = [\"gpu\"],\n-    deps = [\n-        \":autotuner_compile_util\",\n-        \":autotuner_util\",\n-        \":redzone_buffers\",\n-        \"//xla:autotuning_proto_cc\",\n-        \"//xla:status_macros\",\n-        \"//xla:util\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/pass:hlo_pass\",\n-        \"//xla/service:executable\",\n-        \"//xla/service:shaped_buffer\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu:ir_emission_utils\",\n-        \"//xla/service/gpu/kernels:custom_kernel\",\n-        \"//xla/service/gpu/kernels:custom_kernel_fusion\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n-        \"//xla/stream_executor:stream\",\n-        \"//xla/stream_executor:stream_executor_memory_allocator\",\n-        \"//xla/tools:hlo_decomposer_lib\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/time\",\n-        \"@local_tsl//tsl/platform:path\",\n-    ],\n-)\n-\n-xla_test(\n-    name = \"custom_kernel_fusion_autotuner_test\",\n-    srcs = [\"custom_kernel_fusion_autotuner_test.cc\"],\n-    backends = [\n-        \"gpu\",\n-    ],\n-    tags = [\"cuda-only\"],\n-    deps = [\n-        \":autotuner_util\",\n-        \":custom_kernel_fusion_autotuner\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/pass:hlo_pass_pipeline\",\n-        \"//xla/tests:hlo_test_base\",\n-        \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n-        \"//xla/tsl/platform:test\",\n-        \"@com_google_googletest//:gtest\",\n-        \"@local_tsl//tsl/platform:path\",\n-    ],\n-)\n-\n tf_proto_library(\n     name = \"gpu_autotuning_proto\",\n     srcs = [\"gpu_autotuning.proto\"],"
        },
        {
            "sha": "05613318419ca063190f94f2a058319f36524126",
            "filename": "third_party/xla/xla/service/gpu/autotuning/custom_kernel_fusion_autotuner.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 250,
            "changes": 250,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7259e6b2a619a11f61a219fd23b9483822922bda/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7259e6b2a619a11f61a219fd23b9483822922bda/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner.cc?ref=7259e6b2a619a11f61a219fd23b9483822922bda",
            "patch": "@@ -1,250 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/autotuning/custom_kernel_fusion_autotuner.h\"\n-\n-#include <cstdint>\n-#include <memory>\n-#include <tuple>\n-#include <vector>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/time/time.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/service/executable.h\"\n-#include \"xla/service/gpu/autotuning/autotuner_compile_util.h\"\n-#include \"xla/service/gpu/autotuning/autotuner_util.h\"\n-#include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n-#include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/service/gpu/kernels/custom_kernel.h\"\n-#include \"xla/service/gpu/kernels/custom_kernel_fusion.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n-#include \"xla/stream_executor/stream.h\"\n-#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n-#include \"xla/tools/hlo_decomposer.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-namespace {\n-absl::StatusOr<std::unique_ptr<HloModule>> ExtractFusionModule(\n-    HloInstruction* fusion_instruction, int64_t kernel_index) {\n-  std::unique_ptr<HloModule> hlo_module =\n-      ExtractInstructionIntoNewModule(*fusion_instruction);\n-\n-  HloInstruction* instruction =\n-      hlo_module->entry_computation()->root_instruction();\n-  GpuBackendConfig gpu_config =\n-      instruction->backend_config<GpuBackendConfig>().value();\n-  gpu_config.mutable_fusion_backend_config()\n-      ->mutable_custom_fusion_config()\n-      ->set_kernel_index(kernel_index);\n-  TF_RETURN_IF_ERROR(instruction->set_backend_config(gpu_config));\n-\n-  return hlo_module;\n-}\n-\n-absl::StatusOr<std::vector<std::tuple<int, absl::Duration>>> ProfileKernels(\n-    std::vector<CustomKernel>& kernels, HloInstruction* fusion_instruction,\n-    AutotunerCompileUtil& compile_util, const AutotuneConfig& autotune_config,\n-    const DebugOptions& debug_options) {\n-  se::StreamExecutor* stream_exec = autotune_config.GetExecutor();\n-  std::vector<std::tuple<int, absl::Duration>> results;\n-  for (int i = 0; i < kernels.size(); ++i) {\n-    TF_ASSIGN_OR_RETURN(absl::StatusOr<std::unique_ptr<Executable>> executable,\n-                        compile_util.Compile([&](const DebugOptions& opt) {\n-                          return ExtractFusionModule(fusion_instruction, i);\n-                        }));\n-\n-    se::DeviceMemoryAllocator* allocator = autotune_config.GetAllocator();\n-    std::unique_ptr<se::DeviceMemoryAllocator> owned_allocator;\n-    if (allocator == nullptr) {\n-      owned_allocator =\n-          std::make_unique<se::StreamExecutorMemoryAllocator>(stream_exec);\n-      allocator = owned_allocator.get();\n-    }\n-\n-    bool should_init_buffers = autotune_config.should_init_buffers();\n-    bool should_check_correctness = autotune_config.should_check_correctness();\n-    int redzone_padding_bytes = debug_options.xla_gpu_redzone_padding_bytes();\n-    TF_ASSIGN_OR_RETURN(se::Stream* const stream, autotune_config.GetStream());\n-    TF_ASSIGN_OR_RETURN(auto rz_buffers,\n-                        RedzoneBuffers::FromInstruction(\n-                            *fusion_instruction, allocator, stream,\n-                            RedzoneBuffers::kAllInputs, should_init_buffers,\n-                            should_check_correctness, redzone_padding_bytes));\n-\n-    TF_ASSIGN_OR_RETURN(\n-        AutotunerCompileUtil::ProfilingOutput profiling_output,\n-        compile_util.ProfileExecutable(executable->get(), stream,\n-                                       rz_buffers.input_buffers(),\n-                                       rz_buffers.input_shapes()));\n-    results.push_back({i, profiling_output.duration});\n-  }\n-  return results;\n-}\n-\n-absl::StatusOr<int> FindFastestKernel(\n-    const std::vector<std::tuple<int, absl::Duration>>& results) {\n-  auto iter = absl::c_min_element(\n-      results, [](const std::tuple<int, absl::Duration>& lhs,\n-                  const std::tuple<int, absl::Duration>& rhs) {\n-        return std::get<1>(lhs) < std::get<1>(rhs);\n-      });\n-  if (iter == results.end()) {\n-    return absl::InternalError(\"Failed to find fastest kernel.\");\n-  }\n-  return std::get<0>(*iter);\n-}\n-\n-absl::Status UpdateFusionInstructionKernelIndex(\n-    HloInstruction* fusion_instruction, int kernel_index) {\n-  GpuBackendConfig gpu_config =\n-      fusion_instruction->backend_config<GpuBackendConfig>().value();\n-  gpu_config.mutable_fusion_backend_config()\n-      ->mutable_custom_fusion_config()\n-      ->set_kernel_index(kernel_index);\n-  TF_RETURN_IF_ERROR(fusion_instruction->set_backend_config(gpu_config));\n-\n-  return absl::OkStatus();\n-}\n-\n-absl::StatusOr<std::vector<CustomKernel>> LoadKernels(\n-    const HloInstruction* fusion_instruction,\n-    const AutotuneConfig& autotune_config) {\n-  auto config = fusion_instruction->backend_config<GpuBackendConfig>()\n-                    ->fusion_backend_config()\n-                    .custom_fusion_config();\n-  auto* registry = CustomKernelFusionRegistry::Default();\n-  auto* custom_kernel_fusion = registry->Lookup(config.name());\n-\n-  // If custom fusion is not found it means that some of the build targets might\n-  // not be statically linked into the binary.\n-  if (custom_kernel_fusion == nullptr) {\n-    return absl::InternalError(\n-        absl::StrCat(\"Custom kernel fusion \", config.name(),\n-                     \" not found in a default registry.\"));\n-  }\n-\n-  se::StreamExecutor* stream_exec = autotune_config.GetExecutor();\n-  if (!stream_exec->SynchronizeAllActivity()) {\n-    return Internal(\"Failed to synchronize GPU for autotuning.\");\n-  }\n-  se::DeviceDescription device_description =\n-      stream_exec->GetDeviceDescription();\n-\n-  // Load custom kernels that can implement a fusion computation.\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<CustomKernel> kernels,\n-      custom_kernel_fusion->LoadKernels(\n-          device_description,\n-          fusion_instruction->fused_instructions_computation()));\n-\n-  return kernels;\n-}\n-\n-absl::StatusOr<bool> AutotuneCustomKernelFusion(\n-    HloInstruction* fusion_instruction, const AutotuneConfig& autotune_config,\n-    AutotunerCompileUtil& compile_util, const DebugOptions& debug_options) {\n-  int previous_kernel_index =\n-      fusion_instruction->backend_config<GpuBackendConfig>()\n-          ->fusion_backend_config()\n-          .custom_fusion_config()\n-          .kernel_index();\n-\n-  TF_ASSIGN_OR_RETURN(std::vector<CustomKernel> kernels,\n-                      LoadKernels(fusion_instruction, autotune_config));\n-\n-  std::vector<std::tuple<int, absl::Duration>> results;\n-  TF_ASSIGN_OR_RETURN(results,\n-                      ProfileKernels(kernels, fusion_instruction, compile_util,\n-                                     autotune_config, debug_options));\n-\n-  TF_ASSIGN_OR_RETURN(int fastest_kernel_index, FindFastestKernel(results));\n-\n-  TF_RETURN_IF_ERROR(UpdateFusionInstructionKernelIndex(fusion_instruction,\n-                                                        fastest_kernel_index));\n-\n-  return previous_kernel_index != fastest_kernel_index;\n-}\n-\n-bool IsCustomFusion(const HloComputation* computation) {\n-  if (!computation->IsFusionComputation()) {\n-    return false;\n-  }\n-\n-  HloInstruction* instruction = computation->FusionInstruction();\n-  absl::StatusOr<GpuBackendConfig> gpu_backend_config =\n-      instruction->backend_config<GpuBackendConfig>();\n-  if (!gpu_backend_config.ok()) {\n-    return false;\n-  }\n-\n-  if (instruction->fusion_kind() != HloInstruction::FusionKind::kCustom) {\n-    return false;\n-  }\n-\n-  if (!gpu_backend_config->has_fusion_backend_config()) {\n-    return false;\n-  }\n-\n-  return gpu_backend_config->fusion_backend_config().kind() ==\n-         kCustomFusionKind;\n-}\n-}  // namespace\n-\n-absl::StatusOr<bool> CustomKernelFusionAutotuner::Run(\n-    HloModule* module,\n-    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  if (config_.IsDeviceless()) {\n-    return false;\n-  }\n-\n-  const DebugOptions& debug_options = module->config().debug_options();\n-  TF_ASSIGN_OR_RETURN(\n-      AutotunerCompileUtil compile_util,\n-      AutotunerCompileUtil::Create(config_.DeviceConfig(), debug_options));\n-\n-  bool hlo_changed = false;\n-  for (const HloComputation* computation : module->computations()) {\n-    if (IsCustomFusion(computation)) {\n-      TF_ASSIGN_OR_RETURN(\n-          bool instruction_changed,\n-          AutotuneCustomKernelFusion(computation->FusionInstruction(), config_,\n-                                     compile_util, debug_options));\n-      if (instruction_changed) {\n-        hlo_changed = true;\n-      }\n-    }\n-  }\n-\n-  return hlo_changed;\n-}\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "7ea7f4b51beea34bd88ba09c69f42bb6a37f727c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/custom_kernel_fusion_autotuner.h",
            "status": "removed",
            "additions": 0,
            "deletions": 53,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7259e6b2a619a11f61a219fd23b9483822922bda/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7259e6b2a619a11f61a219fd23b9483822922bda/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner.h?ref=7259e6b2a619a11f61a219fd23b9483822922bda",
            "patch": "@@ -1,53 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-#ifndef XLA_SERVICE_GPU_AUTOTUNING_CUSTOM_KERNEL_FUSION_AUTOTUNER_H_\n-#define XLA_SERVICE_GPU_AUTOTUNING_CUSTOM_KERNEL_FUSION_AUTOTUNER_H_\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/autotuning.pb.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/pass/hlo_pass_interface.h\"\n-#include \"xla/service/gpu/autotuning/autotuner_util.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// Find best custom kernel for custom kernel fusions.\n-class CustomKernelFusionAutotuner : public HloModulePass {\n- public:\n-  explicit CustomKernelFusionAutotuner(const AutotuneConfig& config)\n-      : config_(config) {}\n-\n-  absl::string_view name() const override {\n-    return \"custom_kernel-fusion-autotuner\";\n-  }\n-\n-  using HloPassInterface::Run;\n-  absl::StatusOr<bool> Run(\n-      HloModule* module,\n-      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n-\n- private:\n-  const AutotuneConfig config_;\n-};\n-\n-}  // namespace gpu\n-}  // namespace xla\n-\n-#endif  // XLA_SERVICE_GPU_AUTOTUNING_CUSTOM_KERNEL_FUSION_AUTOTUNER_H_"
        },
        {
            "sha": "629a5755c4c086b40b388936c4516748eb93f2dd",
            "filename": "third_party/xla/xla/service/gpu/autotuning/custom_kernel_fusion_autotuner_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 148,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7259e6b2a619a11f61a219fd23b9483822922bda/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7259e6b2a619a11f61a219fd23b9483822922bda/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fcustom_kernel_fusion_autotuner_test.cc?ref=7259e6b2a619a11f61a219fd23b9483822922bda",
            "patch": "@@ -1,148 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/autotuning/custom_kernel_fusion_autotuner.h\"\n-\n-#include <memory>\n-#include <string>\n-#include <utility>\n-\n-#include <gtest/gtest.h>\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n-#include \"xla/service/gpu/autotuning/autotuner_util.h\"\n-#include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-namespace {\n-\n-class CustomKernelFusionAutotunerTest : public HloTestBase {\n- public:\n-  CustomKernelFusionAutotunerTest()\n-      : HloTestBase(/*verifier_layout_sensitive=*/false,\n-                    /*allow_mixed_precision_in_hlo_verifier=*/true) {}\n-\n-  void SetUp() override { HloTestBase::SetUp(); }\n-\n-  void TearDown() override { HloTestBase::TearDown(); }\n-};\n-\n-TEST_F(CustomKernelFusionAutotunerTest, DontRunOnNonCustomFusions) {\n-  const std::string hlo_string = R\"(\n-  HloModule test_module, entry_computation_layout={(f32[20000,20000]{1,0}, f32[20000,20000]{1,0})->(f32[20000,20000]{1,0}, f32[20000,20000]{1,0})}\n-\n-    // Not a CustomFusion!\n-    %fused_computation (p0.param_0: f32[20000,20000], p1.param_1: f32[20000,20000]) -> (f32[20000,20000], f32[20000,20000]) {\n-      %p0.param_0 = f32[20000,20000]{1,0} parameter(0)\n-      %p1.param_1 = f32[20000,20000]{1,0} parameter(1)\n-      %add = f32[20000,20000]{1,0} add(f32[20000,20000]{1,0} %p0.param_0, f32[20000,20000]{1,0} %p1.param_1)\n-      %mul = f32[20000,20000]{1,0} multiply(f32[20000,20000]{1,0} %p0.param_0, f32[20000,20000]{1,0} %p1.param_1)\n-      ROOT %tuple = (f32[20000,20000]{1,0}, f32[20000,20000]{1,0}) tuple(f32[20000,20000]{1,0} %add, f32[20000,20000]{1,0} %mul)\n-    }\n-\n-    ENTRY %BroadcastIntoAdd (p0: f32[20000,20000], p1: f32[20000,20000]) -> (f32[20000,20000], f32[20000,20000]) {\n-      %p0 = f32[20000,20000]{1,0} parameter(0)\n-      %p1 = f32[20000,20000]{1,0} parameter(1)\n-      ROOT %fusion = (f32[20000,20000]{1,0}, f32[20000,20000]{1,0}) fusion(f32[20000,20000]{1,0} %p0, f32[20000,20000]{1,0} %p1), kind=kLoop, calls=%fused_computation\n-    }\n-  )\";\n-  std::unique_ptr<HloModule> hlo_module =\n-      ParseAndReturnVerifiedModule(hlo_string).value();\n-\n-  HloPassPipeline pipeline(\"custom_kernel_fusion_autotuner\");\n-  DebugOptions debug_options;\n-  AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{backend().default_stream_executor(),\n-                                            backend().memory_allocator()}},\n-      debug_options);\n-  pipeline.AddPass<CustomKernelFusionAutotuner>(autotune_config);\n-\n-  // Check that that an HLO computation, which is a non custom fusion gets\n-  // filtered out and passes. If the autotuner would try to run on a non custom\n-  // fusion it would fail.\n-  ASSERT_TRUE(pipeline.Run(hlo_module.get()).ok());\n-}\n-\n-TEST_F(CustomKernelFusionAutotunerTest,\n-       CustomKernelFusionAutotunerPassSucceeds) {\n-  const std::string hlo_string = R\"(\n-    HloModule extracted\n-\n-    cutlass_gemm {\n-      p0 = f32[15,19]{1,0} parameter(0)\n-      p1 = f32[19,17]{1,0} parameter(1)\n-      ROOT r = f32[15, 17]{1,0} dot(p0, p1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY region_198.14436 {\n-      p.0 = f32[15,19]{1,0} parameter(0)\n-      p.1 = f32[19,17]{1,0} parameter(1)\n-      ROOT cutlass_gemm = f32[15,17]{1,0} fusion(p.0, p.1), kind=kCustom, calls=cutlass_gemm, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__custom_fusion\",\"custom_fusion_config\":{\"name\":\"cutlass_gemm\",\"kernel_index\":0}},\"force_earliest_schedule\":false}\n-    }\n-  )\";\n-  std::unique_ptr<HloModule> hlo_module =\n-      ParseAndReturnVerifiedModule(hlo_string).value();\n-\n-  HloPassPipeline pipeline(\"custom_kernel_fusion_autotuner\");\n-  DebugOptions debug_options;\n-  AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{backend().default_stream_executor(),\n-                                            backend().memory_allocator()}},\n-      debug_options);\n-  pipeline.AddPass<CustomKernelFusionAutotuner>(autotune_config);\n-  ASSERT_TRUE(pipeline.Run(hlo_module.get()).ok());\n-}\n-\n-TEST_F(CustomKernelFusionAutotunerTest,\n-       CustomKernelFusionAutotunerPassUpdatesUpdatesKernelIndex) {\n-  const std::string hlo_string = R\"(\n-    HloModule extracted\n-\n-    cutlass_gemm {\n-      p0 = f32[15,19]{1,0} parameter(0)\n-      p1 = f32[19,17]{1,0} parameter(1)\n-      ROOT r = f32[15, 17]{1,0} dot(p0, p1), lhs_contracting_dims={1},\n-      rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY region_198.14436 {\n-      p.0 = f32[15,19]{1,0} parameter(0)\n-      p.1 = f32[19,17]{1,0} parameter(1)\n-      ROOT cutlass_gemm = f32[15,17]{1,0} fusion(p.0, p.1), kind=kCustom,\n-      calls=cutlass_gemm,\n-      backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__custom_fusion\",\"custom_fusion_config\":{\"name\":\"cutlass_gemm\",\"kernel_index\":-1}},\"force_earliest_schedule\":false}\n-    }\n-  )\";\n-\n-  HloPassPipeline pipeline(\"custom_kernel_fusion_autotuner\");\n-  DebugOptions debug_options;\n-  AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{backend().default_stream_executor(),\n-                                            backend().memory_allocator()}},\n-      debug_options);\n-  pipeline.AddPass<CustomKernelFusionAutotuner>(autotune_config);\n-\n-  std::string expected = R\"(\n-    CHECK: \"kernel_index\":0\n-  )\";\n-  RunAndFilecheckHloRewrite(hlo_string, std::move(pipeline), expected);\n-}\n-\n-}  // namespace\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "35853b94d5316a9c105ef8e092e03751408ae24a",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6",
            "patch": "@@ -162,7 +162,6 @@ limitations under the License.\n #include \"xla/service/gather_expander.h\"\n #include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n-#include \"xla/service/gpu/autotuning/custom_kernel_fusion_autotuner.h\"\n #include \"xla/service/gpu/compile_module_to_llvm_ir.h\"\n #include \"xla/service/gpu/conv_layout_normalization.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n@@ -1718,16 +1717,6 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n           *r, gpu_target_config.device_description);\n     });\n \n-    // Greedy pattern matching for custom kernel fusions. We run it before\n-    // Triton rewriter or a regular Gemm rewriter to be able to match compatible\n-    // GEMMs before they matched into Triton gemm or a cuBLAS custom call.\n-    if (debug_options.xla_gpu_enable_custom_fusions()) {\n-      pipeline.AddPass<SimplifyFPConversions>();\n-      pipeline.AddPass<CustomKernelFusionRewriter>(\n-          &gpu_target_config.device_description);\n-      pipeline.AddPass<CustomKernelFusionAutotuner>(autotune_config);\n-    }\n-\n     // Rewrite GEMMs into custom calls.\n     se::GpuComputeCapability gpu_version =\n         gpu_target_config.device_description.gpu_compute_capability();\n@@ -1760,7 +1749,6 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       pipeline.AddPass<SimplifyFPConversions>();\n       pipeline.AddPass<CustomKernelFusionRewriter>(\n           &gpu_target_config.device_description);\n-      pipeline.AddPass<CustomKernelFusionAutotuner>(autotune_config);\n     }\n \n     // Rewrite GEMMs into custom calls."
        },
        {
            "sha": "84df369ed8351396e9f4c444d543ef5f1d986719",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=b4a10dcb61453fac18bb632ac48e2b8eb9ce95b6",
            "patch": "@@ -472,11 +472,6 @@ message DebugOptions {\n   // Rewrite layer norm patterns into cuDNN library calls.\n   optional bool xla_gpu_enable_cudnn_layer_norm = 262;\n \n-  // If true, XLA will try to pattern match subgraphs of HLO operations into\n-  // custom fusions registered in the current process (pre-compiled hand written\n-  // kernels, e.g. various GEMM fusions written in CUTLASS).\n-  optional bool xla_gpu_enable_custom_fusions = 263;\n-\n   // A regular expression enabling only a subset of custom fusions. Enabled only\n   // if `xla_gpu_enable_custom_fusion` set to true.\n   optional string xla_gpu_enable_custom_fusions_re = 264;\n@@ -1195,6 +1190,7 @@ message DebugOptions {\n   reserved 230;  // Was xla_gpu_graph_eviction_timeout_seconds\n   reserved 168;  // Was xla_gpu_simplify_all_fp_conversions.\n   reserved 172;  // Was xla_gpu_normalize_layouts.\n+  reserved 263;  // Was xla_gpu_enable_custom_fusions\n \n   // Generate calls to Arm Compute Library in the CPU backend.\n   optional bool xla_cpu_use_acl = 174;"
        }
    ],
    "stats": {
        "total": 536,
        "additions": 1,
        "deletions": 535
    }
}