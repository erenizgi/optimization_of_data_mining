{
    "author": "WillFroom",
    "message": "[XLA:CPU] Add missing vectorization sizes from tanh and exp approximation.\n\nPiperOrigin-RevId: 842252816",
    "sha": "2009f3930d65c32cf010cd685ce9a988b831ef67",
    "files": [
        {
            "sha": "947566d3de871555de7e276921ffcce3d3721a14",
            "filename": "third_party/xla/xla/backends/cpu/codegen/polynomial_approximations.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2009f3930d65c32cf010cd685ce9a988b831ef67/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fpolynomial_approximations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2009f3930d65c32cf010cd685ce9a988b831ef67/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fpolynomial_approximations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fpolynomial_approximations.cc?ref=2009f3930d65c32cf010cd685ce9a988b831ef67",
            "patch": "@@ -521,12 +521,13 @@ void RewriteToPolynomialApproximations(llvm::Module* module,\n \n   rewrite_calls(\"expf\", GenerateVF32Exp, /*vector_width=*/1);\n   rewrite_calls(\"llvm.exp.f32\", GenerateVF32Exp, /*vector_width=*/1);\n-  rewrite_calls(kExpV4F32Sym, GenerateVF32Exp, /*vector_width=*/4);\n+  rewrite_calls(\"llvm.exp.v2f32\", GenerateVF32Exp, /*vector_width=*/2);\n   rewrite_calls(\"llvm.exp.v4f32\", GenerateVF32Exp, /*vector_width=*/4);\n-  rewrite_calls(kExpV8F32Sym, GenerateVF32Exp, /*vector_width=*/8);\n   rewrite_calls(\"llvm.exp.v8f32\", GenerateVF32Exp, /*vector_width=*/8);\n-  rewrite_calls(kExpV16F32Sym, GenerateVF32Exp, /*vector_width=*/16);\n   rewrite_calls(\"llvm.exp.v16f32\", GenerateVF32Exp, /*vector_width=*/16);\n+  rewrite_calls(kExpV4F32Sym, GenerateVF32Exp, /*vector_width=*/4);\n+  rewrite_calls(kExpV8F32Sym, GenerateVF32Exp, /*vector_width=*/8);\n+  rewrite_calls(kExpV16F32Sym, GenerateVF32Exp, /*vector_width=*/16);\n \n   rewrite_calls(\"llvm.exp.f16\", UpcastF16ToF32<GenerateVF32Exp>,\n                 /*vector_width=*/1);"
        },
        {
            "sha": "022a09951f3d3a1de96c10e332ac5052532c3a48",
            "filename": "third_party/xla/xla/codegen/intrinsic/tanh.h",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2009f3930d65c32cf010cd685ce9a988b831ef67/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2009f3930d65c32cf010cd685ce9a988b831ef67/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh.h?ref=2009f3930d65c32cf010cd685ce9a988b831ef67",
            "patch": "@@ -33,12 +33,13 @@ class Tanh : public Intrinsic<Tanh> {\n   static std::vector<std::vector<Type>> SupportedVectorTypes() {\n     // F16 via upcast to F32.\n     return {\n-        {Type::S(xla::F16)},    {Type::V(xla::F16, 8)}, {Type::V(xla::F16, 16)},\n-        {Type::S(xla::F32)},\n-\n-        {Type::V(xla::F32, 4)}, {Type::V(xla::F32, 8)}, {Type::V(xla::F32, 16)},\n-        {Type::S(xla::F64)},    {Type::V(xla::F64, 2)}, {Type::V(xla::F64, 4)},\n-        {Type::V(xla::F64, 8)},\n+        {Type::S(xla::F16)},     {Type::V(xla::F16, 2)},\n+        {Type::V(xla::F16, 4)},  {Type::V(xla::F16, 8)},\n+        {Type::V(xla::F16, 16)}, {Type::S(xla::F32)},\n+        {Type::V(xla::F32, 2)},  {Type::V(xla::F32, 4)},\n+        {Type::V(xla::F32, 8)},  {Type::V(xla::F32, 16)},\n+        {Type::S(xla::F64)},     {Type::V(xla::F64, 2)},\n+        {Type::V(xla::F64, 4)},  {Type::V(xla::F64, 8)},\n     };\n   }\n   static absl::StatusOr<llvm::Function*> CreateDefinition(llvm::Module* module,"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 11,
        "deletions": 9
    }
}