{
    "author": "tensorflower-gardener",
    "message": "Add `other` to `AutotuneResult` to accommodate any backend config.\n\n- It will help us log/dump any config instead of the limited ones which are part one of the key.\n- It also helps us to use legacy cache for backendsBut which are not supported in existing autotuners, example: BlockLevelFusionConfig\n- I changed the Config struct to own the string as we had to construct Configs from AutotuneResult on the fly.\n\nPiperOrigin-RevId: 811395195",
    "sha": "18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
    "files": [
        {
            "sha": "f606982d14542f3b75e060ba426f1af5c54c04a5",
            "filename": "third_party/xla/xla/autotuning.proto",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fautotuning.proto?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -88,6 +88,11 @@ message AutotuneResult {\n     int64 kernel_index = 1;\n   }\n \n+  message BackendConfigKey {\n+    string name = 1;\n+    google.protobuf.Any config = 2;\n+  }\n+\n   int64 scratch_bytes = 8;\n   google.protobuf.Duration run_time = 9;\n \n@@ -100,6 +105,7 @@ message AutotuneResult {\n     CudaConvPlanKey cuda_conv_plan = 15;\n     CustomKernelFusionKey custom_kernel_fusion = 18;\n     stream_executor.dnn.AlgorithmProto algorithm = 16;\n+    BackendConfigKey other = 19;\n   }\n }\n "
        },
        {
            "sha": "bbceb68ce8987039b5c94499ae73783d14adbf52",
            "filename": "third_party/xla/xla/backends/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -126,7 +126,6 @@ cc_library(\n         \":autotuner_cache_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n "
        },
        {
            "sha": "e90f4c532a0102511f0ebd0747b42687ded1b8e9",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -476,6 +476,9 @@ AutotuneResult Autotuner::ConfigResult::ToProto() const {\n   } else if (config.backend_config\n                  ->Is<stream_executor::dnn::AlgorithmProto>()) {\n     config.backend_config->UnpackTo(result.mutable_algorithm());\n+  } else {\n+    result.mutable_other()->set_name(config.codegen_backend->name());\n+    *result.mutable_other()->mutable_config() = *config.backend_config;\n   }\n   if (failure.has_value()) {\n     *result.mutable_failure() = failure->ToProto();"
        },
        {
            "sha": "f07399f0b18501fab4c264dfafbac86ad7d68e73",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_cache_interface.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -17,9 +17,9 @@ limitations under the License.\n #define XLA_BACKENDS_AUTOTUNER_AUTOTUNER_CACHE_INTERFACE_H_\n \n #include <optional>\n+#include <string>\n \n #include \"absl/status/status.h\"\n-#include \"absl/strings/string_view.h\"\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n \n@@ -32,7 +32,7 @@ class AutotunerCacheInterface {\n  public:\n   // Serializable config. Will be changed to a proto in the future.\n   struct Config {\n-    absl::string_view codegen_backend_name;\n+    std::string codegen_backend_name;\n     google::protobuf::Any backend_config;\n   };\n \n@@ -41,7 +41,7 @@ class AutotunerCacheInterface {\n   virtual std::optional<Config> Lookup(const HloInstruction* instr) = 0;\n \n   virtual absl::Status Insert(const HloInstruction* instr,\n-                              Config& best_config) = 0;\n+                              const Config& best_config) = 0;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "68d806747ee01b72389529803f6affa81dbcd8df",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -120,7 +120,7 @@ class MockAutotunerCache : public AutotunerCacheInterface {\n               (const HloInstruction* instr), (override));\n   MOCK_METHOD(absl::Status, Insert,\n               (const HloInstruction* instr,\n-               AutotunerCacheInterface::Config& best_config),\n+               const AutotunerCacheInterface::Config& best_config),\n               (override));\n };\n \n@@ -574,9 +574,15 @@ TEST_F(AutotunerTest, DumpLogsToFile) {\n   AutotuningLog* log = expected_logs.add_logs();\n   log->mutable_instr()->PackFrom(dummy_instr->ToProto());\n   AutotuneResult* result_1 = log->add_results();\n+  result_1->mutable_other()->set_name(\"mock_backend\");\n+  *result_1->mutable_other()->mutable_config() =\n+      *GetTestConfig(\"test_config_1\");\n   *result_1->mutable_run_time() = ToDurationProto(absl::Seconds(2));\n   result_1->set_scratch_bytes(100);\n   AutotuneResult* result_2 = log->add_results();\n+  result_2->mutable_other()->set_name(\"mock_backend\");\n+  *result_2->mutable_other()->mutable_config() =\n+      *GetTestConfig(\"test_config_2\");\n   *result_2->mutable_run_time() = ToDurationProto(absl::Seconds(1));\n \n   EXPECT_THAT(actual_logs, tsl::proto_testing::EqualsProto(expected_logs));"
        },
        {
            "sha": "d7aa821265f9cf4b2a8b38a3fd03ad93d86bfa95",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -147,7 +147,8 @@ std::optional<AutotunerCacheInterface::Config> FileBasedAutotunerCache::Lookup(\n }\n \n absl::Status FileBasedAutotunerCache::Insert(\n-    const HloInstruction* instr, AutotunerCacheInterface::Config& best_config) {\n+    const HloInstruction* instr,\n+    const AutotunerCacheInterface::Config& best_config) {\n   if (cache_config_.autotune_cache_mode ==\n       FileBasedCacheConfig::CacheMode::READ) {\n     return absl::OkStatus();"
        },
        {
            "sha": "1f1fa769c73bee74a8da63deb1ab15fed9adc9f2",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -82,7 +82,8 @@ class FileBasedAutotunerCache : public AutotunerCacheInterface {\n   std::optional<Config> Lookup(const HloInstruction* instr) override\n       ABSL_LOCKS_EXCLUDED(mutex_);\n \n-  absl::Status Insert(const HloInstruction* instr, Config& best_config) override\n+  absl::Status Insert(const HloInstruction* instr,\n+                      const Config& best_config) override\n       ABSL_LOCKS_EXCLUDED(mutex_);\n \n  private:"
        },
        {
            "sha": "a5de5c4a0b630ff72752ddea3c2eb5a8e90e9e5c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -46,7 +46,7 @@ std::optional<LegacyCache::Config> LegacyCache::Lookup(\n }\n \n absl::Status LegacyCache::Insert(const HloInstruction* instr,\n-                                 Config& best_config) {\n+                                 const Config& best_config) {\n   AutotuneCacheKey key = GetAutotuneCacheKey(*instr);\n   std::optional<AutotuneResult> opt_result = GetAutotuneResult(best_config);\n   if (!opt_result.has_value()) {\n@@ -80,6 +80,9 @@ std::optional<LegacyCache::Config> LegacyCache::GetConfig(\n   } else if (result.has_algorithm()) {\n     config.codegen_backend_name = \"Cudnn\";\n     config.backend_config.PackFrom(result.algorithm());\n+  } else if (result.has_other()) {\n+    config.codegen_backend_name = result.other().name();\n+    config.backend_config = result.other().config();\n   } else {\n     return std::nullopt;\n   }\n@@ -96,7 +99,8 @@ std::optional<AutotuneResult> LegacyCache::GetAutotuneResult(\n   } else if (config.codegen_backend_name == \"Cudnn\") {\n     config.backend_config.UnpackTo(result.mutable_algorithm());\n   } else {\n-    return std::nullopt;\n+    result.mutable_other()->set_name(config.codegen_backend_name);\n+    *result.mutable_other()->mutable_config() = config.backend_config;\n   }\n   return result;\n }"
        },
        {
            "sha": "48b79d7fee666a7ab9564ec00d3e44953e6ffe64",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.h",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -33,9 +33,7 @@ namespace xla {\n namespace gpu {\n \n // Wrapper around the legacy autotune cache from the AutotunerUtil which uses\n-// AutotuneResult proto. The insert for backends which are not supported in\n-// AutotuneResult proto will be a no-op. The lookup will return nullopt if the\n-// backend is not supported in AutotuneResult proto.\n+// AutotuneResult proto.\n class LegacyCache : public AutotunerCacheInterface {\n  public:\n   LegacyCache(std::string cache_dir, DebugOptions::AutotuneCacheMode cache_mode,\n@@ -45,7 +43,7 @@ class LegacyCache : public AutotunerCacheInterface {\n         device_desc_(std::move(device_desc)) {}\n   std::optional<Config> Lookup(const HloInstruction* instr) override;\n   absl::Status Insert(const HloInstruction* instr,\n-                      Config& best_config) override;\n+                      const Config& best_config) override;\n \n  private:\n   AutotuneCacheKey GetAutotuneCacheKey(const HloInstruction& instr);"
        },
        {
            "sha": "b1379a62e26b44e1d3820ebe598e38e381d024f4",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/18429c542cbfd651c47efbfd65f9b88b6dcc05f9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc?ref=18429c542cbfd651c47efbfd65f9b88b6dcc05f9",
            "patch": "@@ -104,6 +104,14 @@ class LegacyCacheTest : public ::testing::Test {\n     config.backend_config.PackFrom(stream_executor::dnn::AlgorithmProto());\n     return config;\n   }\n+\n+  Config CreateDummyBackendConfig() {\n+    using DummyOtherConfig = AutotuneResult::CustomKernelFusionKey;\n+    Config config;\n+    config.codegen_backend_name = \"OtherBackend\";\n+    config.backend_config.PackFrom(DummyOtherConfig());\n+    return config;\n+  }\n };\n \n // Matcher for Config.\n@@ -165,18 +173,18 @@ TEST_F(LegacyCacheTest, InsertAndLookupCudnn) {\n   EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n }\n \n-TEST_F(LegacyCacheTest, InsertAndLookupForUnsupportedBackend) {\n+TEST_F(LegacyCacheTest, InsertAndLookupOther) {\n   auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n   auto instr = CreateDummyInstr(\"hlo4\");\n-  Config config;\n-  config.codegen_backend_name = \"UnsupportedBackend\";\n+  Config config = CreateDummyBackendConfig();\n \n   TF_ASSERT_OK(cache.Insert(instr.get(), config));\n-  EXPECT_THAT(cache.Lookup(instr.get()), Eq(std::nullopt));\n+  std::optional<Config> actual_config = cache.Lookup(instr.get());\n+  EXPECT_THAT(actual_config, Optional(ConfigEq(config)));\n }\n \n TEST_F(LegacyCacheTest, PersistAcrossInstances) {\n-  auto instr = CreateDummyInstr(\"hlo5\");\n+  auto instr = CreateDummyInstr(\"hlo6\");\n   Config config = CreateDummyTritonConfig();\n \n   // Create cache, insert, and let it save.\n@@ -193,7 +201,7 @@ TEST_F(LegacyCacheTest, PersistAcrossInstances) {\n }\n \n TEST_F(LegacyCacheTest, LoadWithDifferentDevice) {\n-  auto instr = CreateDummyInstr(\"hlo6\");\n+  auto instr = CreateDummyInstr(\"hlo7\");\n   Config config = CreateDummyTritonConfig();\n \n   // Create cache, insert, and let it save.\n@@ -213,7 +221,7 @@ TEST_F(LegacyCacheTest, LoadWithDifferentDevice) {\n \n TEST_F(LegacyCacheTest, OnlyInsertOncePerHlo) {\n   auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n-  auto instr = CreateDummyInstr(\"hlo7\");\n+  auto instr = CreateDummyInstr(\"hlo8\");\n \n   Config config = CreateDummyTritonConfig();\n   TF_ASSERT_OK(cache.Insert(instr.get(), config));"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 46,
        "deletions": 20
    }
}