{
    "author": "othakkar",
    "message": "PR #31707: [XLA:CPU][oneDNN] Enable oneDNN Layer-Norm Custom Calls in Thunk Runtime\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31707\n\nThis PR enables support for oneDNN Layer-Norm operations in the XLA:CPU Thunk runtime, building upon the foundational implementation of `OneDnnOpThunk`.\n\nKey changes:\n- Updated thunk emitter to emit `OneDnnOpThunk` for oneDNN layer-norm during compilation.\n- Re-enable the custom call rewrite to oneDNN layer-norm in `onednn_ops_rewriter.cc`.\n- Added support for oneDNN Layer-Norm op via `ExecuteOneDnnLayerNorm(...)` in `onednn_layer_norm.cc`.\nCopybara import of the project:\n\n--\n35147eca21496734c5ea5270a246d3c5aba30ad2 by Om Thakkar <om.thakkar@intel.com>:\n\nenable oneDNN layer norm in thunk runtime\n\n--\n52660485dbc4af6ff58bad1c3c79549eeea3ad59 by Om Thakkar <om.thakkar@intel.com>:\n\naddress review comments\n\n--\nda5df978a7af41b03798b5e459af2c1b0b0cae31 by Om Thakkar <om.thakkar@intel.com>:\n\nadding a unit test for LN\n\n--\ne329036b050a7a2dbec33800fb3b9807de2e864b by Om Thakkar <om.thakkar@intel.com>:\n\nremove dead-code for layer-norm related to legacy runtime\n\n--\n00ffbac879ca18ec6c411b94bfab4c3025168dec by Om Thakkar <om.thakkar@intel.com>:\n\nadd lint-related tags to onednn_ops_rewriter\n\n--\ne212f9b62d30139c19993bb2ac747628cbf95763 by Om Thakkar <om.thakkar@intel.com>:\n\nreplace auto with corresponding type\n\n--\n54a65cd673b330a7e82a86c580fc93644d0cc093 by Om Thakkar <om.thakkar@intel.com>:\n\nminor fix in onednn/BUILD\n\n--\n79f1381b1b353c4f665b9c4ad7c08875b4e7cea4 by Om Thakkar <om.thakkar@intel.com>:\n\nmove the NOLINTBEGIN position in onednn_ops_rewriter\n\n--\n3fcacb5022dcef493b75726bc793d01e6ef385fa by Om Thakkar <om.thakkar@intel.com>:\n\nclang-format checks\n\n--\n26e6cd4481d8c59e7185bfa61138d4c34fde02f3 by Om Thakkar <om.thakkar@intel.com>:\n\nremove some unused includes\n\nMerging this change closes #31707\n\nPiperOrigin-RevId: 816110262",
    "sha": "779593f0958373209b46674347bf8b9ad8fb5393",
    "files": [
        {
            "sha": "8b06602e49b85293ada7895efd00c0959562176c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -129,6 +129,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/cpu:onednn_config_proto_cc\",\n         \"//xla/service/cpu:onednn_convolution\",\n+        \"//xla/service/cpu:onednn_layer_norm\",\n         \"//xla/service/cpu:onednn_matmul\",\n         \"//xla/service/cpu:onednn_memory_util\",\n         \"//xla/stream_executor:device_memory\","
        },
        {
            "sha": "f6cb85b1c81bdcf8076857ecebaebec0c5fc53b3",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/cpu/onednn_convolution.h\"\n+#include \"xla/service/cpu/onednn_layer_norm.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n #include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -105,6 +106,10 @@ OneDnnOpThunk::OneDnnRuntime::Invoke(\n     const auto& conv_config = std::get<OneDnnConvolutionConfig>(config);\n     ExecuteOneDnnConvolution(arguments, results, conv_config, cpu_engine,\n                              onednn_stream, resources);\n+  } else if (target == \"__onednn$layernorm\") {\n+    const auto& ln_config = std::get<OneDnnNormConfig>(config);\n+    ExecuteOneDnnLayerNorm(arguments, results, ln_config, cpu_engine,\n+                           onednn_stream, resources);\n   } else {\n     return absl::InvalidArgumentError(\n         absl::StrFormat(\"Unsupported oneDNN operation target: `%s`\", target));\n@@ -200,4 +205,3 @@ tsl::AsyncValueRef<OneDnnOpThunk::ExecuteEvent> OneDnnOpThunk::Execute(\n }\n \n }  // namespace xla::cpu\n-"
        },
        {
            "sha": "3f3ace4654c8d10c75d7ef959d8d56c6571ad12b",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -47,7 +47,8 @@ class OneDnnOpThunk : public Thunk {\n   // Variant config for supported oneDNN ops.\n   // TODO(intel-tf): Add more oneDNN operation configs as needed.\n   using OneDnnOpConfig =\n-      std::variant<OneDnnMatMulConfig, OneDnnConvolutionConfig>;\n+      std::variant<OneDnnMatMulConfig, OneDnnConvolutionConfig,\n+                   OneDnnNormConfig>;\n \n   static absl::StatusOr<std::unique_ptr<OneDnnOpThunk>> Create(\n       const std::string& custom_call_target, Info info, OpBuffers buffers,"
        },
        {
            "sha": "3074f8ffd6b08ea4ef7178e430a7ce115b7e90b0",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk_test.cc",
            "status": "modified",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -241,5 +241,87 @@ TEST(OneDnnOpThunkTest, SimpleOneDnnConvolutionThunk) {\n   EXPECT_EQ(output_literal, expected_literal);\n }\n \n+TEST(OneDnnOpThunkTest, SimpleOneDnnLayerNormThunk) {\n+  // Set up a thread pool for parallel execution\n+  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 8);\n+  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n+                                 threads.NumThreads());\n+\n+  // Shapes: input [2,3], gamma [3], beta [3], output [2,3]\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {2, 3});\n+  Shape gamma_shape = ShapeUtil::MakeShape(F32, {3});\n+  Shape beta_shape = ShapeUtil::MakeShape(F32, {3});\n+  Shape out_shape = ShapeUtil::MakeShape(F32, {2, 3});\n+\n+  // Input:\n+  // [[1,2,3],\n+  //  [4,5,6]]\n+  // gamma = [1,1,1]\n+  // beta  = [0,0,0]\n+  // Expected (per-row LN): [[-1.2247449, 0, 1.2247449],\n+  //                         [-1.2247449, 0, 1.2247449]]\n+  Literal input_literal = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{1.f, 2.f, 3.f}, {4.f, 5.f, 6.f}}));\n+  Literal gamma_literal = LiteralUtil::CreateR1<float>({1.f, 1.f, 1.f});\n+  Literal beta_literal = LiteralUtil::CreateR1<float>({0.f, 0.f, 0.f});\n+  Literal out_literal = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{0.f, 0.f, 0.f}, {0.f, 0.f, 0.f}}));\n+\n+  // Buffer allocations\n+  auto [input_alloc, gamma_alloc, beta_alloc, out_alloc] =\n+      CreateBufferAllocation(input_literal, gamma_literal, beta_literal,\n+                             out_literal);\n+\n+  auto [input_slice, gamma_slice, beta_slice, out_slice] =\n+      CreateBufferAllocationSlice(input_alloc, gamma_alloc, beta_alloc,\n+                                  out_alloc);\n+\n+  BufferAllocations allocations = CreateBufferAllocations(\n+      input_literal, gamma_literal, beta_literal, out_literal);\n+\n+  // Op buffers\n+  OneDnnOpThunk::OpBuffers op_buffers;\n+  op_buffers.arguments_buffers = {input_slice, gamma_slice, beta_slice};\n+  op_buffers.arguments_shapes = {input_shape, gamma_shape, beta_shape};\n+  op_buffers.results_buffers = {out_slice};\n+  op_buffers.results_shapes = {out_shape};\n+\n+  // oneDNN LayerNorm config\n+  OneDnnNormConfig ln_cfg;\n+  ln_cfg.set_rescale(OneDnnNormConfig::SCALE_AND_SHIFT);\n+  float epsilon = 1e-5f;\n+  int32_t epsilon_bits = *reinterpret_cast<int32_t*>(&epsilon);\n+  ln_cfg.set_epsilon_typecast(epsilon_bits);\n+\n+  OneDnnOpThunk::OneDnnOpConfig config = ln_cfg;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto thunk, OneDnnOpThunk::Create(\"__onednn$layernorm\", Thunk::Info(),\n+                                        op_buffers, config));\n+\n+  // Execute\n+  Thunk::ExecuteParams params;\n+  params.buffer_allocations = &allocations;\n+  params.intra_op_threadpool = &device;\n+\n+  tsl::AsyncValueRef<Thunk::ExecuteEvent> exec_event = thunk->Execute(params);\n+  tsl::BlockUntilReady(exec_event);\n+  ASSERT_FALSE(exec_event.IsError()) << \"oneDNN LayerNorm thunk failed\";\n+\n+  // Expected output\n+  const float ref = 1.2247449f;  // sqrt(3/2)\n+  Literal expected = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{-ref, 0.f, ref}, {-ref, 0.f, ref}}));\n+\n+  // Compare with tolerance\n+  const float* exp_view = expected.data<float>().data();\n+  float* out_view = out_literal.data<float>().data();\n+  for (int i = 0; i < 6; ++i) {\n+    EXPECT_NEAR(out_view[i], exp_view[i], 1e-4)\n+        << \"Mismatch at index \" << i << \": got \" << out_view[i] << \" expected \"\n+        << exp_view[i];\n+  }\n+}\n+\n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "e4bf15409b8062a5fa9fae51e3a586467dda4ca7",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -611,7 +611,6 @@ cc_library(\n         \"@local_tsl//tsl/platform:logging\",\n     ] + if_onednn([\n         \":onednn_convolution\",\n-        \":onednn_layer_norm\",\n         \":onednn_matmul\",\n         \":onednn_softmax\",\n     ]),\n@@ -1801,7 +1800,6 @@ onednn_cc_library(\n     srcs = [\"onednn_layer_norm.cc\"],\n     hdrs = [\n         \"onednn_layer_norm.h\",\n-        \"//xla/tsl/util:onednn_util_hdrs\",\n     ],\n     copts = runtime_copts() + tsl_copts(),\n     visibility = [\"//visibility:public\"],"
        },
        {
            "sha": "063ba52f71c6b681781c45d3cc807bdebea68258",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -684,14 +684,12 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n \n   // Rewrite to custom calls with target as oneDNN library calls.\n #ifdef XLA_ONEDNN\n-  // This pass is not supported in the thunk runtime yet.\n-  bool is_thunk_runtime = true;\n   bool use_onednn_custom_call =\n       module->config()\n           .debug_options()\n           .xla_cpu_experimental_onednn_custom_call() &&\n       is_onednn_compatible;\n-  if (use_onednn_custom_call && !is_thunk_runtime) {\n+  if (use_onednn_custom_call) {\n     // Placing OneDnnOpsRewriter here to match the flax patterns\n     // TODO: Decide where would be the appropriate place for this pass to make\n     // it more generic"
        },
        {
            "sha": "72bdcf4b30cc43442c72dabd2b43df6d8e39bc00",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -171,8 +171,6 @@ extern const char* const kOneDnnMatMulSymbolName =\n     \"__xla_cpu_runtime_OneDnnMatMul\";\n extern const char* const kOneDnnSoftmaxSymbolName =\n     \"__xla_cpu_runtime_OneDnnSoftmax\";\n-extern const char* const kOneDnnLayerNormSymbolName =\n-    \"__xla_cpu_runtime_OneDnnLayerNorm\";\n extern const char* const kOneDnnMatMulReorderSymbolName =\n     \"__xla_cpu_runtime_OneDnnMatMulReorder\";\n extern const char* const kHandleFfiCallSymbolName ="
        },
        {
            "sha": "8633babaa649fe0d584b9fea5aef8f175aff823c",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -89,7 +89,6 @@ extern const char* const kAllGatherSymbolName;\n extern const char* const kReduceScatterSymbolName;\n extern const char* const kOneDnnMatMulSymbolName;\n extern const char* const kOneDnnSoftmaxSymbolName;\n-extern const char* const kOneDnnLayerNormSymbolName;\n extern const char* const kOneDnnMatMulReorderSymbolName;\n extern const char* const kHandleFfiCallSymbolName;\n "
        },
        {
            "sha": "8744c1829370f8fd5f61b820fa44ec2c85de3b14",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -2563,76 +2563,6 @@ absl::Status IrEmitter::HandleOneDnnMatMulCalls(\n   return absl::OkStatus();\n }\n \n-absl::Status IrEmitter::HandleOneDnnLayerNorm(HloInstruction* custom_call) {\n-  //      args[0]: ptr to nargs\n-  //      args[1]: ptr to ExecutableRunOptions\n-  //      args[2]: ptr to OneDnnNormConfig\n-  //      args[3...]: ptrs to operands\n-\n-  // First three arguments: nargs, ExecutableRunOptions, and\n-  // OneDnnNormConfig.\n-  const int nargs_offset = 3;\n-  const int num_operands = custom_call->operand_count();\n-  const int nargs = nargs_offset + num_operands;\n-  int arg_indx = 0;\n-\n-  llvm::Type* i64_type = b()->getInt64Ty();\n-  llvm::Type* ptr_type = b()->getPtrTy();\n-  llvm::ArrayType* ptr_array_type = llvm::ArrayType::get(ptr_type, nargs);\n-  llvm::Value* args_val = llvm::UndefValue::get(ptr_array_type);\n-\n-  // Insert nargs.\n-  llvm::Value* nargs_val = b()->getInt64(nargs);\n-  llvm::Value* nargs_ptr =\n-      llvm_ir::EmitAllocaAtFunctionEntry(i64_type, \"nargs\", b());\n-  b()->CreateLifetimeStart(nargs_ptr);\n-  b()->CreateStore(nargs_val, nargs_ptr);\n-  args_val = b()->CreateInsertValue(args_val, nargs_ptr, arg_indx++);\n-\n-  // Insert ExecutableRunOptions.\n-  llvm::Value* run_opts_val = GetExecutableRunOptionsArgument();\n-  args_val = b()->CreateInsertValue(args_val, run_opts_val, arg_indx++);\n-\n-  // Insert OneDnnNormConfig.\n-  auto typed_custom_call = Cast<HloCustomCallInstruction>(custom_call);\n-  auto backend_config = typed_custom_call->backend_config<BackendConfig>();\n-  OneDnnNormConfig ln_config;\n-  ln_config.CopyFrom(backend_config->onednn_layer_norm_config());\n-  std::string str_config;\n-  ln_config.SerializeToString(&str_config);\n-  llvm::Value* ln_config_val =\n-      b()->CreateGlobalStringPtr(llvm_ir::AsStringRef(str_config));\n-  args_val = b()->CreateInsertValue(args_val, ln_config_val, arg_indx++);\n-\n-  // Insert operands.\n-  auto operands_stack_alloca =\n-      EmitOneDnnOperandsAlloca(custom_call, args_val, arg_indx);\n-  TF_RET_CHECK(nargs == arg_indx)\n-      << \"Number of arguments don't equal the last argument index.\";\n-\n-  llvm::Value* args_ptr =\n-      llvm_ir::EmitAllocaAtFunctionEntry(ptr_array_type, \"layernorm.args\", b());\n-  b()->CreateLifetimeStart(args_ptr);\n-  b()->CreateStore(args_val, args_ptr);\n-\n-  TF_RETURN_IF_ERROR(EmitTargetAddressForOp(custom_call));\n-  llvm_ir::IrArray result_array = GetIrArrayFor(custom_call);\n-  auto result_stack_alloca = GetAllocaAndEmitMemrefInfo(*b(), result_array);\n-\n-  EmitCallToFunc(runtime::kOneDnnLayerNormSymbolName,\n-                 {result_stack_alloca.value, args_ptr}, b()->getVoidTy());\n-\n-  // Lifetime ends for all stack allocations.\n-  b()->CreateLifetimeEnd(nargs_ptr);\n-  for (int i = 0; i < num_operands; ++i) {\n-    operands_stack_alloca[i].EmitLifetimeEnd();\n-  }\n-  b()->CreateLifetimeEnd(args_ptr);\n-  result_stack_alloca.EmitLifetimeEnd();\n-\n-  return absl::OkStatus();\n-}\n-\n absl::Status IrEmitter::HandleOneDnnSoftmax(HloInstruction* custom_call) {\n   // Serialize and emit OneDnnSoftmaxConfig.\n   auto typed_custom_call = Cast<HloCustomCallInstruction>(custom_call);\n@@ -2682,9 +2612,6 @@ absl::Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {\n   if (custom_call->custom_call_target() == \"__onednn$softmax\") {\n     return HandleOneDnnSoftmax(custom_call);\n   }\n-  if (custom_call->custom_call_target() == \"__onednn$layernorm\") {\n-    return HandleOneDnnLayerNorm(custom_call);\n-  }\n   if (custom_call->custom_call_target() == \"__onednn$matmul_reorder\") {\n     return HandleOneDnnMatMulCalls(custom_call,\n                                    runtime::kOneDnnMatMulReorderSymbolName);"
        },
        {
            "sha": "71bbf9ed83ed53f0576df3748c57a2d03f96c523",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -345,7 +345,6 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n   absl::Status HandleOneDnnMatMulCalls(HloInstruction* hlo,\n                                        std::string runtime_symbol_name);\n   absl::Status HandleOneDnnSoftmax(HloInstruction* hlo);\n-  absl::Status HandleOneDnnLayerNorm(HloInstruction* hlo);\n #endif  // XLA_ONEDNN\n   // Private helper to initialize an IR function for the computation.\n   void InitializeIrFunction(const std::string& function_name);"
        },
        {
            "sha": "4ed330444444468150ccd89f504ac21c8ce6449a",
            "filename": "third_party/xla/xla/service/cpu/onednn_layer_norm.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 44,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_layer_norm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_layer_norm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_layer_norm.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -26,10 +26,7 @@ limitations under the License.\n #include \"oneapi/dnnl/dnnl_types.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n-#include \"xla/service/cpu/onednn_config.pb.h\"\n-#include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/runtime_lightweight_check.h\"\n-#include \"xla/tsl/util/onednn_threadpool.h\"\n \n // Eigen Tensor must come after `onednn_threadpool.h`\n #include \"unsupported/Eigen/CXX11/Tensor\"  // NOLINT\n@@ -42,65 +39,48 @@ using dnnl::engine;\n using dnnl::layer_normalization_forward;\n using dnnl::memory;\n using dnnl::normalization_flags;\n+using dnnl::primitive;\n using dnnl::prop_kind;\n using dnnl::stream;\n \n }  // namespace\n \n-ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnLayerNorm(\n-    void* result, void** args) {\n-  // args[0]: ptr to nargs. We don't use nargs here.\n-  // args[1]: ptr to ExecutableRunOptions\n-  // args[2]: ptr to OneDnnNormConfig\n-  // args[3...]: ptrs to operands\n-  int arg_indx = 1;\n-  const xla::ExecutableRunOptions* run_options =\n-      static_cast<const xla::ExecutableRunOptions*>(args[arg_indx++]);\n-  XLA_LIGHTWEIGHT_CHECK(run_options != nullptr);\n-  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n-  tsl::OneDnnThreadPool thread_pool(\n-      run_options->intra_op_thread_pool()->getPool(), false);\n-  engine cpu_engine(engine::kind::cpu, 0);\n-#ifndef ENABLE_ONEDNN_OPENMP\n-  auto onednn_stream =\n-      stream(dnnl::threadpool_interop::make_stream(cpu_engine, &thread_pool));\n-#else\n-  auto onednn_stream = stream(cpu_engine);\n-#endif  // ENABLE_ONEDNN_OPENMP\n-  std::string config_str(static_cast<const char*>(args[arg_indx++]));\n-  OneDnnNormConfig ln_config;\n-  ln_config.ParseFromString(config_str);\n-\n-  MemrefInfo layer_minfo(args[arg_indx++]);\n-  MemrefInfo gamma_minfo(args[arg_indx++]);\n-  MemrefInfo beta_minfo(args[arg_indx++]);\n-  MemrefInfo result_minfo(result);\n+void ExecuteOneDnnLayerNorm(absl::Span<MemrefInfoHandler> arguments,\n+                            absl::Span<MemrefInfoHandler> results,\n+                            OneDnnNormConfig ln_config,\n+                            const dnnl::engine& cpu_engine,\n+                            dnnl::stream& onednn_stream,\n+                            OneDnnResources& resources) {\n+  MemrefInfo layer_minfo(arguments[0].get());\n+  MemrefInfo gamma_minfo(arguments[1].get());\n+  MemrefInfo beta_minfo(arguments[2].get());\n+  MemrefInfo result_minfo(results[0].get());\n \n   auto src_md = layer_minfo.GetOneDnnMemDesc();\n   auto dst_md = result_minfo.GetOneDnnMemDesc();\n   auto scaleshift_md = beta_minfo.GetOneDnnMemDesc();\n \n-  auto src_mem = memory(src_md, cpu_engine, layer_minfo.Data());\n-  auto dst_mem = memory(dst_md, cpu_engine, result_minfo.Data());\n-  auto scale_mem = memory(scaleshift_md, cpu_engine, gamma_minfo.Data());\n-  auto shift_mem = memory(scaleshift_md, cpu_engine, beta_minfo.Data());\n+  resources.src_mem = memory(src_md, cpu_engine, layer_minfo.Data());\n+  resources.dst_mem = memory(dst_md, cpu_engine, result_minfo.Data());\n+  resources.scale_mem = memory(scaleshift_md, cpu_engine, gamma_minfo.Data());\n+  resources.shift_mem = memory(scaleshift_md, cpu_engine, beta_minfo.Data());\n \n-  float epsilon;\n-  *(reinterpret_cast<int32_t*>(&epsilon)) = ln_config.epsilon_typecast();\n+  float epsilon = absl::bit_cast<float>(ln_config.epsilon_typecast());\n \n   auto lnorm_pd = layer_normalization_forward::primitive_desc(\n       cpu_engine, prop_kind::forward_inference, src_md, dst_md, epsilon,\n       normalization_flags::use_scale | normalization_flags::use_shift);\n \n-  auto lnorm_prim = layer_normalization_forward(lnorm_pd);\n+  resources.primitive = primitive(lnorm_pd);\n \n-  std::unordered_map<int, memory> ln_args;\n-  ln_args.insert({DNNL_ARG_SRC, src_mem});\n-  ln_args.insert({DNNL_ARG_SCALE, scale_mem});\n-  ln_args.insert({DNNL_ARG_SHIFT, shift_mem});\n-  ln_args.insert({DNNL_ARG_DST, dst_mem});\n+  std::unordered_map<int, memory> ln_args = {\n+      {DNNL_ARG_SRC, resources.src_mem},\n+      {DNNL_ARG_SCALE, resources.scale_mem},\n+      {DNNL_ARG_SHIFT, resources.shift_mem},\n+      {DNNL_ARG_DST, resources.dst_mem},\n+  };\n \n-  lnorm_prim.execute(onednn_stream, ln_args);\n+  resources.primitive.execute(onednn_stream, ln_args);\n }\n \n }  // namespace cpu"
        },
        {
            "sha": "8659b02cf4f3a1d3fe631b2e064b37e5ddfd3e85",
            "filename": "third_party/xla/xla/service/cpu/onednn_layer_norm.h",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_layer_norm.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_layer_norm.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_layer_norm.h?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -16,12 +16,19 @@ limitations under the License.\n #ifndef XLA_SERVICE_CPU_ONEDNN_LAYER_NORM_H_\n #define XLA_SERVICE_CPU_ONEDNN_LAYER_NORM_H_\n \n+#include \"dnnl.hpp\"\n+#include \"xla/service/cpu/onednn_config.pb.h\"\n+#include \"xla/service/cpu/onednn_memory_util.h\"\n+\n namespace xla {\n namespace cpu {\n \n-extern \"C\" {\n-extern void __xla_cpu_runtime_OneDnnLayerNorm(void* result, void** args);\n-}  // extern \"C\"\n+void ExecuteOneDnnLayerNorm(absl::Span<MemrefInfoHandler> arguments,\n+                            absl::Span<MemrefInfoHandler> results,\n+                            OneDnnNormConfig ln_config,\n+                            const dnnl::engine& cpu_engine,\n+                            dnnl::stream& onednn_stream,\n+                            OneDnnResources& resources);\n \n }  // namespace cpu\n }  // namespace xla"
        },
        {
            "sha": "fc1280b5b60bd10d6852db30ead5a3b46cb04b0d",
            "filename": "third_party/xla/xla/service/cpu/onednn_memory_util.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_memory_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_memory_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_memory_util.h?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -145,6 +145,8 @@ struct OneDnnResources {\n   dnnl::memory wei_mem;\n   dnnl::memory dst_mem;\n   dnnl::memory scratch_mem;\n+  dnnl::memory scale_mem;\n+  dnnl::memory shift_mem;\n \n   // Post-operation arguments\n   std::vector<std::pair<int, dnnl::memory>> postop_args;\n@@ -160,6 +162,8 @@ struct OneDnnResources {\n         wei_mem(dnnl::memory()),\n         dst_mem(dnnl::memory()),\n         scratch_mem(dnnl::memory()),\n+        scale_mem(dnnl::memory()),\n+        shift_mem(dnnl::memory()),\n         postop_args(),\n         arg_memrefs(),\n         result_memrefs() {}"
        },
        {
            "sha": "d11dfdeae42c809480d8184cef9181f375a2a34a",
            "filename": "third_party/xla/xla/service/cpu/onednn_ops_rewriter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -596,6 +596,11 @@ class OneDnnOpsRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   absl::Status HandleDivide(HloInstruction* divide_instr) override {\n+    // TODO(intel-tf): remove this restriction after adding oneDNN softmax\n+    // support in thunk runtime.\n+    return absl::OkStatus();\n+\n+    // NOLINTBEGIN(clang-diagnostic-unreachable-code)\n     if (divide_instr->HasControlDependencies()) return absl::OkStatus();\n     if (!IsSupportedType(divide_instr->shape().element_type())) {\n       return absl::OkStatus();\n@@ -618,6 +623,7 @@ class OneDnnOpsRewriterVisitor : public DfsHloRewriteVisitor {\n     TF_RETURN_IF_ERROR(ReplaceInstruction(divide_instr, softmax_call));\n \n     return absl::OkStatus();\n+    // NOLINTEND(clang-diagnostic-unreachable-code)\n   }\n };\n "
        },
        {
            "sha": "29c5b06bcad60cb5d21303bdd354822176f8defe",
            "filename": "third_party/xla/xla/service/cpu/runtime_symbol_generator.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -56,7 +56,6 @@ limitations under the License.\n \n #ifdef XLA_ONEDNN\n #include \"xla/service/cpu/onednn_convolution.h\"\n-#include \"xla/service/cpu/onednn_layer_norm.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n #include \"xla/service/cpu/onednn_softmax.h\"\n #endif  // XLA_ONEDNN\n@@ -190,7 +189,6 @@ static bool RegisterKnownJITSymbols() {\n #ifdef XLA_ONEDNN\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMul);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnSoftmax);\n-  REGISTER_CPU_RUNTIME_SYMBOL(OneDnnLayerNorm);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMulReorder);\n #endif  // XLA_ONEDNN\n "
        },
        {
            "sha": "68bcf4129abbbe5bbf7b748c74210de0fa5e1900",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/779593f0958373209b46674347bf8b9ad8fb5393/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=779593f0958373209b46674347bf8b9ad8fb5393",
            "patch": "@@ -1217,6 +1217,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOneDnnOpThunk(\n     config = backend_config->onednn_matmul_config();\n   } else if (custom_call_target == \"__onednn$convolution\") {\n     config = backend_config->onednn_conv_config();\n+  } else if (custom_call_target == \"__onednn$layernorm\") {\n+    config = backend_config->onednn_layer_norm_config();\n   } else {\n     return Unimplemented(\n         \"Custom call target %s is not supported in thunk runtime\",\n@@ -1249,8 +1251,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCustomCallThunk(\n   // TODO(penporn): Support these existing targets.\n   auto custom_call_target = custom_call->custom_call_target();\n   if (custom_call_target == \"PadToStatic\" ||\n-      custom_call_target == \"__onednn$softmax\" ||\n-      custom_call_target == \"__onednn$layernorm\") {\n+      custom_call_target == \"__onednn$softmax\") {\n     return Unimplemented(\"Custom call target %s is not implemented.\",\n                          custom_call_target);\n   }"
        }
    ],
    "stats": {
        "total": 273,
        "additions": 138,
        "deletions": 135
    }
}