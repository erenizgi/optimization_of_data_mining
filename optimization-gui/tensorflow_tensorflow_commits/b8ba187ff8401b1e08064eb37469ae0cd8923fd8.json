{
    "author": "tensorflower-gardener",
    "message": "Refactor call_library_for_dot -> library_supports_dot\n\nThis enables BF16 to be sent to YNNPACK without casting to F32.\n\nPiperOrigin-RevId: 824667930",
    "sha": "b8ba187ff8401b1e08064eb37469ae0cd8923fd8",
    "files": [
        {
            "sha": "2b683f28f6ea78bbb0c82ca8de4123d63ac6f44b",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 52,
            "changes": 100,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8ba187ff8401b1e08064eb37469ae0cd8923fd8/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8ba187ff8401b1e08064eb37469ae0cd8923fd8/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=b8ba187ff8401b1e08064eb37469ae0cd8923fd8",
            "patch": "@@ -533,6 +533,49 @@ std::unique_ptr<HloPassFix<HloPassPipeline>> CreateSimplificationPipeline(\n   return pipeline;\n }\n \n+auto LibrarySupportsDot(HloModule* module,\n+                        TargetMachineFeatures* target_machine_features) {\n+  // TODO(b/406806134): Stop calling XNNPACK from regular Dot thunks. All XNN\n+  // Dots should be wrapped in an `__xnn_fusion` fusion region and processed in\n+  // `XnnFusionThunk`.\n+  const bool xnnpack_enabled =\n+      module->config().debug_options().xla_cpu_use_xnnpack();\n+  const auto xnn_graph_fusion_mode =\n+      module->config()\n+          .debug_options()\n+          .xla_cpu_experimental_xnn_graph_fusion_mode();\n+  const bool xnnpack_use_cost_model =\n+      xnn_graph_fusion_mode !=\n+      DebugOptions::XNN_GRAPH_FUSION_MODE_BYPASS_COST_MODEL;\n+  const bool xnnpack_dot_enabled =\n+      xnnpack_enabled &&\n+      xnn_graph_fusion_mode != DebugOptions::XNN_GRAPH_FUSION_MODE_DISABLED;\n+  const bool ynnpack_dot_enabled = absl::c_linear_search(\n+      module->config().debug_options().xla_cpu_experimental_ynn_fusion_type(),\n+      DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_DOT);\n+  return [=](const HloInstruction& instr) {\n+#ifdef XLA_YNNPACK\n+    if (ynnpack_dot_enabled &&\n+        IsDotSupportedByYnn(instr.dot_dimension_numbers(),\n+                            instr.operand(0)->shape(),\n+                            instr.operand(1)->shape(), instr.shape())\n+            .value_or(false)) {\n+      return true;\n+    }\n+#endif  // XLA_YNNPACK\n+\n+    if (xnnpack_dot_enabled &&\n+        IsDotSupportedByXnn(instr.dot_dimension_numbers(),\n+                            instr.operand(0)->shape(),\n+                            instr.operand(1)->shape(), instr.shape(),\n+                            target_machine_features, xnnpack_use_cost_model)\n+            .value_or(false)) {\n+      return true;\n+    }\n+    return false;\n+  };\n+}\n+\n }  // namespace\n \n absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n@@ -614,60 +657,14 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   // If XNNPACK is enabled, we only need to upcast dots that XnnDotThunk does\n   // not support. `upcaster_filter` returns false if the instruction shouldn't\n   // be processed.\n-  // TODO(b/406806134): Stop calling XNNPACK from regular Dot thunks. All XNN\n-  // Dots should be wrapped in an `__xnn_fusion` fusion region and processed in\n-  // `XnnFusionThunk`.\n-  bool xnnpack_enabled = module->config().debug_options().xla_cpu_use_xnnpack();\n-  auto call_library_for_dot = [&](const HloInstruction& instr) {\n-    if (!xnnpack_enabled) return false;\n-    DotImplementationStrategy strategy = GetDotImplementationStrategy(\n-        module->config(), instr, *target_machine_features,\n-        /*allow_runtime_calls=*/true);\n-    return strategy == DotImplementationStrategy::kEigen;\n-  };\n+  auto library_supports_dot =\n+      LibrarySupportsDot(module, target_machine_features);\n+\n   HloPredicate upcaster_filter = [&](const HloInstruction* instr) {\n     if (instr->opcode() != HloOpcode::kDot) {\n       return true;\n     }\n-    if (!call_library_for_dot(*instr)) {\n-      return true;\n-    }\n-\n-#ifdef XLA_YNNPACK\n-    if (absl::c_linear_search(\n-            module->config()\n-                .debug_options()\n-                .xla_cpu_experimental_ynn_fusion_type(),\n-            DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_DOT)) {\n-      if (IsDotSupportedByYnn(instr->dot_dimension_numbers(),\n-                              instr->operand(0)->shape(),\n-                              instr->operand(1)->shape(), instr->shape())\n-              .value_or(false)) {\n-        return false;\n-      }\n-    }\n-#endif  // XLA_YNNPACK\n-\n-    auto xnn_graph_fusion_mode =\n-        module->config()\n-            .debug_options()\n-            .xla_cpu_experimental_xnn_graph_fusion_mode();\n-    if (xnn_graph_fusion_mode != DebugOptions::XNN_GRAPH_FUSION_MODE_DISABLED) {\n-      bool use_cost_model =\n-          module->config()\n-              .debug_options()\n-              .xla_cpu_experimental_xnn_graph_fusion_mode() !=\n-          DebugOptions::XNN_GRAPH_FUSION_MODE_BYPASS_COST_MODEL;\n-      if (IsDotSupportedByXnn(instr->dot_dimension_numbers(),\n-                              instr->operand(0)->shape(),\n-                              instr->operand(1)->shape(), instr->shape(),\n-                              target_machine_features, use_cost_model)\n-              .value_or(false)) {\n-        return false;\n-      }\n-    }\n-\n-    return true;\n+    return !library_supports_dot(*instr);\n   };\n \n   // xla::cpu::GetDotImplementationStrategy (used by call_library_for_dot)\n@@ -731,8 +728,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   // Convert BF16 and F8 operations to F32 and F16 respectively so that the CPU\n   // backend can support BF16/F8 operations without directly implementing a\n   // BF16/F8 lowering for most ops.\n-  CpuFloatSupport bf16_support(BF16, call_library_for_dot,\n-                               target_machine_features);\n+  CpuFloatSupport bf16_support(BF16, library_supports_dot);\n #ifdef XLA_ONEDNN\n   OneDnnFloatSupport onednn_bf16_support(BF16);\n   if (use_onednn_custom_call) {"
        },
        {
            "sha": "7447220cb6a0fe11e269a872da1236bd785daa88",
            "filename": "third_party/xla/xla/service/cpu/cpu_float_support.h",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8ba187ff8401b1e08064eb37469ae0cd8923fd8/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_float_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8ba187ff8401b1e08064eb37469ae0cd8923fd8/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_float_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_float_support.h?ref=b8ba187ff8401b1e08064eb37469ae0cd8923fd8",
            "patch": "@@ -18,8 +18,6 @@ limitations under the License.\n \n #include <functional>\n \n-#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n-#include \"xla/backends/cpu/xnn_support.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -34,20 +32,13 @@ class CpuFloatSupport : public FloatSupport {\n   using DotStrategyChecker = std::function<bool(const HloInstruction& hlo)>;\n \n   explicit CpuFloatSupport(PrimitiveType low_precision_type,\n-                           DotStrategyChecker call_library_for_dot,\n-                           TargetMachineFeatures* cpu_features)\n+                           DotStrategyChecker library_supports_dot)\n       : FloatSupport(low_precision_type),\n-        call_library_for_dot_(call_library_for_dot),\n-        cpu_features_(cpu_features) {}\n+        library_supports_dot_(library_supports_dot) {}\n \n-  // Skip trying to upcast the dot if XNNPACK is enabled and the dot is\n-  // supported by XNNPACK.\n+  // Skip trying to upcast the dot if the dot is supported by a library.\n   bool ShouldSkipInstruction(const HloInstruction& hlo) const override {\n-    return hlo.opcode() == HloOpcode::kDot && call_library_for_dot_(hlo) &&\n-           IsDotSupportedByXnn(hlo.dot_dimension_numbers(),\n-                               hlo.operand(0)->shape(), hlo.operand(1)->shape(),\n-                               hlo.shape(), cpu_features_)\n-               .value_or(false);\n+    return hlo.opcode() == HloOpcode::kDot && library_supports_dot_(hlo);\n   }\n \n   // Makes FloatNormalization skip custom fusion computations for CPU backend.\n@@ -58,8 +49,7 @@ class CpuFloatSupport : public FloatSupport {\n   }\n \n  private:\n-  DotStrategyChecker call_library_for_dot_;\n-  TargetMachineFeatures* cpu_features_;\n+  DotStrategyChecker library_supports_dot_;\n };\n \n }  // namespace cpu"
        },
        {
            "sha": "2c294d1d38388e0544e96c261aa160cc91ea1f0e",
            "filename": "third_party/xla/xla/service/cpu/cpu_float_support_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8ba187ff8401b1e08064eb37469ae0cd8923fd8/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_float_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8ba187ff8401b1e08064eb37469ae0cd8923fd8/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_float_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_float_support_test.cc?ref=b8ba187ff8401b1e08064eb37469ae0cd8923fd8",
            "patch": "@@ -42,8 +42,6 @@ namespace {\n struct SkipInstructionTestSpec {\n   HloOpcode op;\n   bool call_library_for_dot;\n-  std::string cpu_name;\n-  std::string features;\n   bool upcast;\n };\n \n@@ -56,10 +54,7 @@ class SkipInstructionTest\n     absl::string_view op = HloOpcodeString(info.param.op);\n     absl::string_view dot_strategy =\n         info.param.call_library_for_dot ? \"LibDot\" : \"NoLibDot\";\n-    absl::string_view bf16_strategy =\n-        absl::StrContains(info.param.features, \"+avx512bf16\") ? \"Bf16\"\n-                                                              : \"NoBf16\";\n-    return absl::StrCat(op, \"_\", dot_strategy, \"_\", bf16_strategy);\n+    return absl::StrCat(op, \"_\", dot_strategy);\n   }\n \n   void SetUp() override { TargetMachineTestBase::SetUp(); }\n@@ -105,9 +100,7 @@ TEST_P(SkipInstructionTest, Bf16InF32Out) {\n   // Create CpuFloatSupport.\n   CpuFloatSupport::DotStrategyChecker call_library_for_dot =\n       [&spec](const HloInstruction& hlo) { return spec.call_library_for_dot; };\n-  std::unique_ptr<TargetMachineFeatures> features = CreateTargetMachineFeatures(\n-      \"x86_64-unknown-linux-gnu\", spec.cpu_name, spec.features);\n-  CpuFloatSupport cpu_float_support(BF16, call_library_for_dot, features.get());\n+  CpuFloatSupport cpu_float_support(BF16, call_library_for_dot);\n \n   // Run FloatNormalization and check the results.\n   FloatNormalization float_normalization(&cpu_float_support);\n@@ -122,27 +115,16 @@ std::vector<SkipInstructionTestSpec> GetSkipInstructionTestSpecs() {\n       // Add op, always upcast.\n       SkipInstructionTestSpec{HloOpcode::kAdd,\n                               /*call_library_for_dot=*/true,\n-                              /*cpu_name=*/\"sapphirerapids\",\n-                              /*features=*/\"+avx512bf16\",\n                               /*upcast=*/true},\n-      // CPU has BF16, but library dot is disabled.\n+      // Library dot is disabled.\n       SkipInstructionTestSpec{HloOpcode::kDot,\n                               /*call_library_for_dot=*/false,\n-                              /*cpu_name=*/\"sapphirerapids\",\n-                              /*features=*/\"+avx512bf16\",\n                               /*upcast=*/true},\n-      // Library dot is enabled, but CPU does not have BF16.\n+      // Library dot is enabled.\n       SkipInstructionTestSpec{HloOpcode::kDot,\n                               /*call_library_for_dot=*/true,\n-                              /*cpu_name=*/\"znver3\",\n-                              /*features=*/\"+avx2\",\n-                              /*upcast=*/true},\n-      // Library dot is enabled and CPU has BF16. Use mixed precision.\n-      SkipInstructionTestSpec{HloOpcode::kDot,\n-                              /*call_library_for_dot=*/true,\n-                              /*cpu_name=*/\"sapphirerapids\",\n-                              /*features=*/\"+avx512bf16\",\n-                              /*upcast=*/false}};\n+                              /*upcast=*/false},\n+  };\n }\n \n INSTANTIATE_TEST_SUITE_P(SkipInstructionTestSuite, SkipInstructionTest,"
        }
    ],
    "stats": {
        "total": 150,
        "additions": 59,
        "deletions": 91
    }
}