{
    "author": "loislo",
    "message": "[XLA:GPU] Refactor: Update composite_rewriter to parse dot dimension numbers from a structured MLIR attribute.\n\nThe `ParseDimensionNumbers` function now expects a single `dimension_numbers` attribute in the composite attributes, structured as an array of arrays representing contracting and batch dimensions. This change simplifies the attribute parsing. Additionally, checks are added to ensure that the parsed dimension numbers are supported and to handle cases where scale factors are not divisible by 32 for non-BF16 types, preventing rewriting in those scenarios. The test case is updated to reflect the new attribute format.\n\nPiperOrigin-RevId: 834777398",
    "sha": "e9b743832d07b5d74260605f3ad24b04e6ed684b",
    "files": [
        {
            "sha": "7afd2c386667608c03593cca1497b53d0dc9beb6",
            "filename": "third_party/xla/xla/service/gpu/transforms/composite_rewriter.cc",
            "status": "modified",
            "additions": 81,
            "deletions": 34,
            "changes": 115,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e9b743832d07b5d74260605f3ad24b04e6ed684b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e9b743832d07b5d74260605f3ad24b04e6ed684b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc?ref=e9b743832d07b5d74260605f3ad24b04e6ed684b",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"mlir/AsmParser/AsmParser.h\"\n #include \"mlir/IR/Attributes.h\"\n@@ -50,43 +49,52 @@ absl::StatusOr<DotDimensionNumbers> ParseDimensionNumbers(\n   mlir::MLIRContext context;\n   mlir::Attribute attr = mlir::parseAttribute(composite_attributes, &context);\n   mlir::DictionaryAttr dict_attrs = mlir::dyn_cast<mlir::DictionaryAttr>(attr);\n-  auto get_int = [&](absl::string_view key) -> absl::StatusOr<int32_t> {\n-    if (!dict_attrs.contains(key)) {\n-      return absl::InvalidArgumentError(absl::StrCat(key, \" is not set\"));\n-    }\n-    mlir::Attribute attr = dict_attrs.get(key);\n-    if (!mlir::isa<mlir::IntegerAttr>(attr)) {\n-      return absl::InvalidArgumentError(\n-          absl::StrCat(key, \" is not an integer\"));\n-    }\n-    return mlir::cast<mlir::IntegerAttr>(attr).getInt();\n-  };\n-  DotDimensionNumbers dot_dimension_numbers;\n-  TF_ASSIGN_OR_RETURN(int32_t lhs_contracting_dim_index,\n-                      get_int(\"lhs_contracting_dim_index\"));\n-  dot_dimension_numbers.add_lhs_contracting_dimensions(\n-      lhs_contracting_dim_index);\n-  TF_ASSIGN_OR_RETURN(int32_t rhs_contracting_dim_index,\n-                      get_int(\"rhs_contracting_dim_index\"));\n-  dot_dimension_numbers.add_rhs_contracting_dimensions(\n-      rhs_contracting_dim_index);\n-\n-  if (dict_attrs.contains(\"lhs_batch_dim_index\")) {\n-    TF_ASSIGN_OR_RETURN(int32_t lhs_batch_dim_index,\n-                        get_int(\"lhs_batch_dim_index\"));\n-    dot_dimension_numbers.add_lhs_batch_dimensions(lhs_batch_dim_index);\n+  if (!dict_attrs.contains(\"dimension_numbers\")) {\n+    return absl::InvalidArgumentError(\n+        \"dimension_numbers are not set in composite attributes\");\n   }\n-  if (dict_attrs.contains(\"rhs_batch_dim_index\")) {\n-    TF_ASSIGN_OR_RETURN(int32_t rhs_batch_dim_index,\n-                        get_int(\"rhs_batch_dim_index\"));\n-    dot_dimension_numbers.add_rhs_batch_dimensions(rhs_batch_dim_index);\n+\n+  mlir::ArrayAttr dim_numbers =\n+      mlir::dyn_cast<mlir::ArrayAttr>(dict_attrs.get(\"dimension_numbers\"));\n+  if (!dim_numbers || dim_numbers.size() != 2) {\n+    return absl::InvalidArgumentError(\n+        \"dimension_numbers must be array of size 2\");\n   }\n-  if (dot_dimension_numbers.lhs_batch_dimensions_size() !=\n-      dot_dimension_numbers.rhs_batch_dimensions_size()) {\n+\n+  mlir::ArrayAttr contracting = mlir::dyn_cast<mlir::ArrayAttr>(dim_numbers[0]);\n+  mlir::ArrayAttr batch = mlir::dyn_cast<mlir::ArrayAttr>(dim_numbers[1]);\n+  if (!contracting || contracting.size() != 2 || !batch || batch.size() != 2) {\n     return absl::InvalidArgumentError(\n-        \"batch dimension should be specified for both lhs and rhs.\");\n+        \"invalid contracting or batch dimensions\");\n+  }\n+\n+  mlir::ArrayAttr lhs_contracting =\n+      mlir::dyn_cast<mlir::ArrayAttr>(contracting[0]);\n+  mlir::ArrayAttr rhs_contracting =\n+      mlir::dyn_cast<mlir::ArrayAttr>(contracting[1]);\n+  mlir::ArrayAttr lhs_batch = mlir::dyn_cast<mlir::ArrayAttr>(batch[0]);\n+  mlir::ArrayAttr rhs_batch = mlir::dyn_cast<mlir::ArrayAttr>(batch[1]);\n+\n+  if (!lhs_contracting || !rhs_contracting || !lhs_batch || !rhs_batch) {\n+    return absl::InvalidArgumentError(\"Invalid dimension_numbers structure\");\n+  }\n+\n+  DotDimensionNumbers dnums;\n+  for (mlir::Attribute dim : lhs_contracting) {\n+    dnums.add_lhs_contracting_dimensions(\n+        mlir::cast<mlir::IntegerAttr>(dim).getInt());\n   }\n-  return dot_dimension_numbers;\n+  for (mlir::Attribute dim : rhs_contracting) {\n+    dnums.add_rhs_contracting_dimensions(\n+        mlir::cast<mlir::IntegerAttr>(dim).getInt());\n+  }\n+  for (mlir::Attribute dim : lhs_batch) {\n+    dnums.add_lhs_batch_dimensions(mlir::cast<mlir::IntegerAttr>(dim).getInt());\n+  }\n+  for (mlir::Attribute dim : rhs_batch) {\n+    dnums.add_rhs_batch_dimensions(mlir::cast<mlir::IntegerAttr>(dim).getInt());\n+  }\n+  return dnums;\n }\n \n }  // namespace\n@@ -120,6 +128,45 @@ absl::StatusOr<bool> CompositeRewriter::RewriteComputation(\n     TF_ASSIGN_OR_RETURN(\n         DotDimensionNumbers dot_dimension_numbers,\n         ParseDimensionNumbers(frontend_attrs.at(\"composite.attributes\")));\n+\n+    if (dot_dimension_numbers.lhs_contracting_dimensions_size() != 1 ||\n+        dot_dimension_numbers.rhs_contracting_dimensions_size() != 1 ||\n+        dot_dimension_numbers.lhs_batch_dimensions_size() > 1 ||\n+        dot_dimension_numbers.rhs_batch_dimensions_size() > 1) {\n+      LOG(ERROR) << \"Unsupported dimension numbers: \"\n+                 << dot_dimension_numbers.DebugString();\n+      continue;\n+    }\n+\n+    const HloInstruction* lhs = call->operand(0);\n+    const HloInstruction* rhs = call->operand(1);\n+    const HloInstruction* lhs_scale = call->operand(2);\n+    const HloInstruction* rhs_scale = call->operand(3);\n+\n+    if (lhs->shape().element_type() != BF16) {\n+      int64_t contracting_dim =\n+          dot_dimension_numbers.lhs_contracting_dimensions(0);\n+      int64_t scale_factor = lhs->shape().dimensions(contracting_dim) /\n+                             lhs_scale->shape().dimensions(contracting_dim);\n+      if (scale_factor != 32) {\n+        VLOG(2) << \"LHS scale_factor is not 32: \" << scale_factor\n+                << \" ignore such scaled_dot. It will be inlined later.\";\n+        continue;\n+      }\n+    }\n+\n+    if (rhs->shape().element_type() != BF16) {\n+      int64_t contracting_dim =\n+          dot_dimension_numbers.rhs_contracting_dimensions(0);\n+      int64_t scale_factor = rhs->shape().dimensions(contracting_dim) /\n+                             rhs_scale->shape().dimensions(contracting_dim);\n+      if (scale_factor != 32) {\n+        VLOG(2) << \"RHS scale_factor is not 32: \" << scale_factor\n+                << \" ignore such scaled_dot for now. It will be inlined later.\";\n+        continue;\n+      }\n+    }\n+\n     PrecisionConfig precision{};\n     precision.mutable_operand_precision()->Resize(2, PrecisionConfig::DEFAULT);\n     auto* scaled_dot ="
        },
        {
            "sha": "a2935374ef2c6b7af49ba08809adfe0c83f1eed4",
            "filename": "third_party/xla/xla/service/gpu/transforms/composite_rewriter_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e9b743832d07b5d74260605f3ad24b04e6ed684b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e9b743832d07b5d74260605f3ad24b04e6ed684b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc?ref=e9b743832d07b5d74260605f3ad24b04e6ed684b",
            "patch": "@@ -56,20 +56,14 @@ TEST(CompositeRewriterTest, ScaledDotCompositeRewrite) {\n \n     ENTRY %main {\n       %lhs = f8e4m3fn[3,128,256]{2,1,0} parameter(0)\n-      %rhs = f8e4m3fn[3,128,256]{2,1,0} parameter(1)\n+      %rhs = f8e4m3fn[3,256,128]{2,1,0} parameter(1)\n       %lhs_scales = f8e8m0fnu[3,128,8]{2,1,0} parameter(2)\n-      %rhs_scales = f8e8m0fnu[3,128,8]{2,1,0} parameter(3)\n+      %rhs_scales = f8e8m0fnu[3,8,128]{2,1,0} parameter(3)\n       ROOT %call.1 = bf16[3,128,128]{2,1,0} call(%lhs, %rhs, %lhs_scales, %rhs_scales),\n           to_apply=%xla.scaled_dot.1,\n           is_composite=true,\n           frontend_attributes={\n-            composite.attributes={\n-              preferred_element_type = bf16,\n-              lhs_contracting_dim_index = 2 : i64,\n-              rhs_contracting_dim_index = 1 : i64,\n-              lhs_batch_dim_index = 0 : i64,\n-              rhs_batch_dim_index = 0 : i64\n-            },\n+            composite.attributes=\"{dimension_numbers=[[[2],[1]],[[0],[0]]]}\",\n             composite.name=\"xla.scaled_dot\",\n             composite.version=\"1\"\n           }"
        }
    ],
    "stats": {
        "total": 127,
        "additions": 84,
        "deletions": 43
    }
}