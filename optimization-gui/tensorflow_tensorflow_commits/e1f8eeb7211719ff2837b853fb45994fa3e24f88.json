{
    "author": "hyeontaek",
    "message": "[IFRT] Migrate `Client::GetDefaultLayout()` calls to use `Client::GetDefaultPjRtLayout()`.\n\nExisting users of `Client::GetDefaultLayout()` are migrated to\n`Client::GetDefaultPjRtLayout()`. `Client::GetDefaultLayout()` will be\nre-introduced with a new version that uses IFRT `CustomLayoutRef`.\n\nPiperOrigin-RevId: 812237385",
    "sha": "e1f8eeb7211719ff2837b853fb45994fa3e24f88",
    "files": [
        {
            "sha": "f6856aa6c74dd65e1bad94ede28aba9865cf1d2f",
            "filename": "third_party/xla/xla/python/ifrt/array_impl_test_lib.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -302,9 +302,9 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferDefaultLayout) {\n   for (Memory* const memory : device->Memories()) {\n     SCOPED_TRACE(absl::StrCat(memory->Kind()));\n \n-    TF_ASSERT_OK_AND_ASSIGN(\n-        auto default_layout,\n-        client->GetDefaultLayout(dtype, shape.dims(), device, memory->Kind()));\n+    TF_ASSERT_OK_AND_ASSIGN(auto default_layout,\n+                            client->GetDefaultPjRtLayout(\n+                                dtype, shape.dims(), device, memory->Kind()));\n \n     TF_ASSERT_OK_AND_ASSIGN(\n         auto array,\n@@ -1451,8 +1451,8 @@ TEST(ArrayImplTest, CopyPreservesDefaultLayouts) {\n       TF_ASSERT_OK_AND_ASSIGN(auto src_layout, array->pjrt_layout());\n       TF_ASSERT_OK_AND_ASSIGN(\n           auto src_default_layout,\n-          client->GetDefaultLayout(dtype, shape.dims(), device,\n-                                   src_memory->Kind()));\n+          client->GetDefaultPjRtLayout(dtype, shape.dims(), device,\n+                                       src_memory->Kind()));\n       EXPECT_EQ(*src_layout, *src_default_layout);\n \n       TF_ASSERT_OK_AND_ASSIGN(\n@@ -1463,8 +1463,8 @@ TEST(ArrayImplTest, CopyPreservesDefaultLayouts) {\n       TF_ASSERT_OK_AND_ASSIGN(auto dst_layout, new_arrays[0]->pjrt_layout());\n       TF_ASSERT_OK_AND_ASSIGN(\n           auto dst_default_layout,\n-          client->GetDefaultLayout(dtype, shape.dims(), device,\n-                                   dst_memory->Kind()));\n+          client->GetDefaultPjRtLayout(dtype, shape.dims(), device,\n+                                       dst_memory->Kind()));\n       EXPECT_EQ(*dst_layout, *dst_default_layout);\n     }\n   }"
        },
        {
            "sha": "6bfc54565184c4b2d9f9fbe0c7d6ff38e17d7305",
            "filename": "third_party/xla/xla/python/ifrt/ir/compiled_ifrt_ir_program.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fcompiled_ifrt_ir_program.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fcompiled_ifrt_ir_program.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fcompiled_ifrt_ir_program.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -169,7 +169,7 @@ GetParameterLayoutFromConsumer(\n     const auto& out_spec = out_specs[param_operand.getOperandNumber()];\n     TF_ASSIGN_OR_RETURN(auto shard_shape,\n                         out_spec.sharding->GetShardShape(out_spec.shape));\n-    return client->GetDefaultLayout(\n+    return client->GetDefaultPjRtLayout(\n         out_spec.dtype, shard_shape.dims(),\n         out_spec.sharding->devices()->devices().front(),\n         out_spec.sharding->memory_kind());\n@@ -180,7 +180,7 @@ GetParameterLayoutFromConsumer(\n       const auto& arg_spec = in_specs[arg.getArgNumber()];\n       TF_ASSIGN_OR_RETURN(auto shard_shape,\n                           arg_spec.sharding->GetShardShape(arg_spec.shape));\n-      return client->GetDefaultLayout(\n+      return client->GetDefaultPjRtLayout(\n           arg_spec.dtype, shard_shape.dims(),\n           arg_spec.sharding->devices()->devices().front(),\n           arg_spec.sharding->memory_kind());\n@@ -216,7 +216,7 @@ absl::Status PopulateLayouts(mlir::ModuleOp mlir_module,\n       TF_ASSIGN_OR_RETURN(auto shard_shape,\n                           arg_spec.sharding->GetShardShape(arg_spec.shape));\n       TF_ASSIGN_OR_RETURN(parameter_layout,\n-                          client->GetDefaultLayout(\n+                          client->GetDefaultPjRtLayout(\n                               arg_spec.dtype, shard_shape.dims(),\n                               arg_spec.sharding->devices()->devices().front(),\n                               arg_spec.sharding->memory_kind()));\n@@ -256,7 +256,7 @@ absl::Status PopulateLayouts(mlir::ModuleOp mlir_module,\n       TF_ASSIGN_OR_RETURN(auto shard_shape,\n                           out_spec.sharding->GetShardShape(out_spec.shape));\n       TF_ASSIGN_OR_RETURN(out_spec.layout,\n-                          client->GetDefaultLayout(\n+                          client->GetDefaultPjRtLayout(\n                               out_spec.dtype, shard_shape.dims(),\n                               out_spec.sharding->devices()->devices().front(),\n                               out_spec.sharding->memory_kind()));"
        },
        {
            "sha": "2d48fa163f39411e63b39d8c4af7d19e3f54fef9",
            "filename": "third_party/xla/xla/python/ifrt/layout.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Flayout.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Flayout.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Flayout.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -148,15 +148,15 @@ absl::StatusOr<bool> EquivalentLayouts(DType dtype1, const Shape& shape1,\n       return true;\n     }\n     // TODO(hyeontaek): Change to IFRT `Layout` comparison once\n-    // `Client::GetDefaultLayout()` returns a `CustomLayoutRef`.\n+    // we add `Client::GetDefaultLayout()` that returns a `CustomLayoutRef`.\n     TF_ASSIGN_OR_RETURN(\n         std::shared_ptr<const xla::PjRtLayout> pjrt_layout1,\n-        device1->client()->GetDefaultLayout(dtype1, shape1.dims(), device1,\n-                                            sharding1->memory_kind()));\n+        device1->client()->GetDefaultPjRtLayout(dtype1, shape1.dims(), device1,\n+                                                sharding1->memory_kind()));\n     TF_ASSIGN_OR_RETURN(\n         std::shared_ptr<const xla::PjRtLayout> pjrt_layout2,\n-        device2->client()->GetDefaultLayout(dtype2, shape2.dims(), device2,\n-                                            sharding2->memory_kind()));\n+        device2->client()->GetDefaultPjRtLayout(dtype2, shape2.dims(), device2,\n+                                                sharding2->memory_kind()));\n     return *pjrt_layout1 == *pjrt_layout2;\n   }\n   if (layout1 != nullptr && layout2 != nullptr) {"
        },
        {
            "sha": "8376ba8eb402ed8d9494825d41d085b2832ba983",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/array.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -946,9 +946,9 @@ absl::StatusOr<std::shared_ptr<const PjRtLayout>> Array::pjrt_layout() const {\n   }\n \n   TF_ASSIGN_OR_RETURN(auto shard_shape, sharding_->GetShardShape(shape_));\n-  return client_->GetDefaultLayout(dtype_, shard_shape.dims(),\n-                                   sharding_->devices()->devices().front(),\n-                                   sharding_->memory_kind());\n+  return client_->GetDefaultPjRtLayout(dtype_, shard_shape.dims(),\n+                                       sharding_->devices()->devices().front(),\n+                                       sharding_->memory_kind());\n }\n \n xla::ifrt::Client* Array::client() const { return client_; }"
        },
        {
            "sha": "0cb9cec538cf9e366b93a27d85ebfb76d2f562af",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/client_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient_test.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -316,8 +316,8 @@ TEST_P(ClientTest, GetDefaultLayoutSuccess) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto resolved_layout,\n-      client_->GetDefaultLayout(DType(DType::kF64), {1, 2, 3}, device_,\n-                                MemoryKind(\"mock\")));\n+      client_->GetDefaultPjRtLayout(DType(DType::kF64), {1, 2, 3}, device_,\n+                                    MemoryKind(\"mock\")));\n   EXPECT_EQ(resolved_layout->ToString(), layout.ToString());\n }\n \n@@ -331,13 +331,13 @@ TEST_P(ClientTest, GetCachedDefaultLayoutSuccess) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto resolved_layout,\n-      client_->GetDefaultLayout(DType(DType::kF64), {1, 2, 3}, device_,\n-                                MemoryKind(\"mock\")));\n+      client_->GetDefaultPjRtLayout(DType(DType::kF64), {1, 2, 3}, device_,\n+                                    MemoryKind(\"mock\")));\n   EXPECT_EQ(resolved_layout->ToString(), layout_1_->ToString());\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      resolved_layout, client_->GetDefaultLayout(DType(DType::kF64), {1, 2, 3},\n-                                                 device_, MemoryKind(\"mock\")));\n+  TF_ASSERT_OK_AND_ASSIGN(resolved_layout, client_->GetDefaultPjRtLayout(\n+                                               DType(DType::kF64), {1, 2, 3},\n+                                               device_, MemoryKind(\"mock\")));\n   EXPECT_EQ(resolved_layout->ToString(), layout_1_->ToString());\n }\n \n@@ -347,8 +347,8 @@ TEST_P(ClientTest, GetDefaultLayoutFailure) {\n       .WillOnce(Return(Future<ClientSession::Response>(\n           absl::InternalError(\"injected from test\"))));\n \n-  EXPECT_THAT(client_->GetDefaultLayout(DType(DType::kF64), {1, 2, 3}, device_,\n-                                        MemoryKind(\"mock\")),\n+  EXPECT_THAT(client_->GetDefaultPjRtLayout(DType(DType::kF64), {1, 2, 3},\n+                                            device_, MemoryKind(\"mock\")),\n               Not(absl_testing::IsOk()));\n }\n "
        },
        {
            "sha": "3e0e0421556158798ebfeb1becee1bfef965c15d",
            "filename": "third_party/xla/xla/python/ifrt_proxy/server/ifrt_backend.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2Fifrt_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2Fifrt_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2Fifrt_backend.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -2006,8 +2006,8 @@ IfrtBackend::HandleGetDefaultLayoutRequest(\n           : MemoryKind(get_default_layout_request.memory_kind());\n   TF_ASSIGN_OR_RETURN(\n       std::shared_ptr<const xla::PjRtLayout> layout,\n-      client_->GetDefaultLayout(dtype, get_default_layout_request.dims(),\n-                                device, memory_kind));\n+      client_->GetDefaultPjRtLayout(dtype, get_default_layout_request.dims(),\n+                                    device, memory_kind));\n \n   auto ifrt_resp = NewIfrtResponse(request->request_metadata().op_id());\n "
        },
        {
            "sha": "22fac4b5566231fcaa8aa173d9b26ad04426a09e",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -734,7 +734,7 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n           layout = std::make_shared<xla::PjRtLayout>(xla::Layout());\n         } else {\n           TF_ASSIGN_OR_RETURN(layout,\n-                              client_->GetDefaultLayout(\n+                              client_->GetDefaultPjRtLayout(\n                                   output_dtypes_[i], output_shapes_[i].dims(),\n                                   devices_->devices().front(),\n                                   output_shardings_[i]->memory_kind()));"
        },
        {
            "sha": "e7573061d13cf7169cf0e0dae4becbb7ee671767",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_layout.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_layout.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f8eeb7211719ff2837b853fb45994fa3e24f88/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_layout.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_layout.cc?ref=e1f8eeb7211719ff2837b853fb45994fa3e24f88",
            "patch": "@@ -101,8 +101,8 @@ absl::StatusOr<absl_nonnull std::shared_ptr<const xla::PjRtLayout>>\n ToPjRtLayout(DType dtype, const Shape& shard_shape, Device* device,\n              MemoryKind memory_kind, const LayoutRef& layout) {\n   if (layout == nullptr) {\n-    return device->client()->GetDefaultLayout(dtype, shard_shape.dims(), device,\n-                                              memory_kind);\n+    return device->client()->GetDefaultPjRtLayout(dtype, shard_shape.dims(),\n+                                                  device, memory_kind);\n   }\n   return ToPjRtLayout(dtype, shard_shape, layout);\n }"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 33,
        "deletions": 33
    }
}