{
    "author": "derdrdirk",
    "message": "[Autotuner] Use default configuration if there is no device available.\n\nPiperOrigin-RevId: 813765385",
    "sha": "e79680f284cde197b549ccee84d9d913b5f83776",
    "files": [
        {
            "sha": "126837dd9e7a47cb244818a1a136f774948d9d89",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=e79680f284cde197b549ccee84d9d913b5f83776",
            "patch": "@@ -47,21 +47,29 @@ namespace gpu {\n \n namespace {\n \n-AutotuneConfig GetAutotuneConfig(const DebugOptions& debug_options) {\n+AutotuneConfig GetAutotuneConfig(const DebugOptions& debug_options,\n+                                 bool is_deviceless) {\n   AutotuneConfig autotune_config;\n   autotune_config.check_buffers = debug_options.xla_gpu_autotune_level() >= 4;\n   autotune_config.relative_tolerance =\n       debug_options.xla_gpu_autotune_gemm_rtol();\n   autotune_config.crash_on_check_failure =\n       debug_options.xla_gpu_crash_on_verification_failures();\n-  autotune_config.expect_all_instructions_in_cache =\n-      debug_options.xla_gpu_require_complete_aot_autotune_results();\n   autotune_config.dump_logs_to = debug_options.xla_gpu_dump_autotune_logs_to();\n   autotune_config.exclude_cublas_config =\n       !debug_options.xla_gpu_cublas_fallback();\n   autotune_config.select_first_config =\n       debug_options.xla_gpu_deterministic_ops() ||\n       debug_options.xla_gpu_exclude_nondeterministic_ops();\n+\n+  if (is_deviceless) {\n+    // If we are running on a deviceless target, we want to use default configs.\n+    autotune_config.use_default_config = true;\n+  }\n+\n+  autotune_config.expect_all_instructions_in_cache =\n+      debug_options.xla_gpu_require_complete_aot_autotune_results();\n+\n   return autotune_config;\n }\n \n@@ -80,15 +88,13 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     stream_executor::StreamExecutor* stream_executor,\n     tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n     const Compiler::TargetConfig* target_config,\n-    se::DeviceMemoryAllocator* allocator, bool cache_only) {\n+    se::DeviceMemoryAllocator* allocator) {\n   std::unique_ptr<Profiler> profiler = nullptr;\n-  AutotuneConfig autotune_config = GetAutotuneConfig(debug_options);\n-  if (cache_only) {\n-    autotune_config.expect_all_instructions_in_cache = true;\n-  } else {\n-    // If not cache_only, at least one of stream_executor or allocator must be\n-    // provided.\n-    CHECK(stream_executor != nullptr || allocator != nullptr);\n+  bool is_deviceless = stream_executor == nullptr;\n+  AutotuneConfig autotune_config =\n+      GetAutotuneConfig(debug_options, is_deviceless);\n+\n+  if (!is_deviceless) {\n     profiler = GpuProfiler::Create(stream_executor,\n                                    GetProfileOptions(debug_options), allocator);\n   }"
        },
        {
            "sha": "93d1fa160539bcdaff10acbb5f6fe971528b8a49",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=e79680f284cde197b549ccee84d9d913b5f83776",
            "patch": "@@ -38,15 +38,13 @@ namespace gpu {\n \n class AutotunerPass : public HloModulePass {\n  public:\n-  // If 'cache_only' is true, tuning is disabled and only cache lookups are\n-  // performed. In this mode, 'stream_executor' and 'allocator' can be null.\n-  // target_config must outlive the pass.\n+  // Note: the target_config must outlive the pass.\n   static absl::StatusOr<std::unique_ptr<AutotunerPass>> Create(\n       std::vector<std::unique_ptr<CodegenBackend>> backends,\n       const DebugOptions& debug_options, se::StreamExecutor* stream_executor,\n       tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n       const Compiler::TargetConfig* target_config,\n-      se::DeviceMemoryAllocator* allocator = nullptr, bool cache_only = false);\n+      se::DeviceMemoryAllocator* allocator = nullptr);\n \n   absl::string_view name() const override { return \"autotuner\"; }\n "
        },
        {
            "sha": "6117282b7bc83514353277d6e8c278466580e8d4",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 1,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=e79680f284cde197b549ccee84d9d913b5f83776",
            "patch": "@@ -293,7 +293,7 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedWithCacheOnly) {\n         AutotunerPass::Create(\n             std::move(backends2), module_2->config().debug_options(),\n             /*stream_executor=*/nullptr, &thread_pool, IsCublasGemmInstruction,\n-            &target_config, /*allocator=*/nullptr, /*cache_only=*/true));\n+            &target_config, /*allocator=*/nullptr));\n     EXPECT_THAT(pass2->Run(module_2.get(), /*execution_threads=*/{}),\n                 absl_testing::IsOkAndHolds(true));\n   }\n@@ -308,6 +308,44 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedWithCacheOnly) {\n   EXPECT_TRUE(hlo_backend_config.has_selected_algorithm());\n }\n \n+TEST_F(AutotunerPassTest, DevicelessUsesDefaultConfigIfNoCache) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kCublasCustomCallHlo));\n+\n+  std::string cache_dir = ::testing::TempDir();\n+  module->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_experimental_autotuner_cache_dir(cache_dir);\n+\n+  tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n+                                      /*num_threads=*/4);\n+  GpuCompiler::TargetConfig target_config(stream_executor_);\n+\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::make_unique<CublasBackend>(\n+      stream_executor_, &module->config().debug_options(), &compiler_,\n+      &target_config));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<AutotunerPass> pass,\n+      AutotunerPass::Create(std::move(backends),\n+                            module->config().debug_options(),\n+                            /*stream_executor=*/nullptr, &thread_pool,\n+                            IsCublasGemmInstruction, &target_config,\n+                            /*allocator=*/nullptr));\n+  EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n+              absl_testing::IsOkAndHolds(true));\n+\n+  // Verify that the backend config has been updated in the HLO with default\n+  // config.\n+  auto gemm =\n+      module->entry_computation()->GetInstructionWithName(\"custom-call.1\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_backend_config,\n+                          gemm->backend_config<GpuBackendConfig>());\n+  ASSERT_TRUE(\n+      gpu_backend_config.gemm_backend_config().has_selected_algorithm());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "87c40204bcdb477a147f98e55c884a7e8cf6db09",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e79680f284cde197b549ccee84d9d913b5f83776/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=e79680f284cde197b549ccee84d9d913b5f83776",
            "patch": "@@ -356,12 +356,11 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n            IsCublasGemm(instruction);\n   };\n \n-  bool cache_only = stream_exec == nullptr;\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<AutotunerPass> autotuner_pass,\n       AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n                             thread_pool, should_autotune, target_config,\n-                            options.device_allocator, cache_only));\n+                            options.device_allocator));\n   pipeline->AddPass(std::move(autotuner_pass));\n   return absl::OkStatus();\n }\n@@ -429,12 +428,11 @@ absl::Status NVPTXCompiler::AddFusionAutotuningPass(\n   backends.push_back(std::make_unique<NativeEmitterBackend>(\n       &debug_options, this, target_config));\n \n-  bool cache_only = stream_executor == nullptr;\n-  TF_ASSIGN_OR_RETURN(std::unique_ptr<AutotunerPass> autotuner_pass,\n-                      AutotunerPass::Create(\n-                          std::move(backends), debug_options, stream_executor,\n-                          thread_pool, ShouldAutotuneBetweenFusionEmitters,\n-                          target_config, options.device_allocator, cache_only));\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<AutotunerPass> autotuner_pass,\n+      AutotunerPass::Create(std::move(backends), debug_options, stream_executor,\n+                            thread_pool, ShouldAutotuneBetweenFusionEmitters,\n+                            target_config, options.device_allocator));\n   pipeline->AddPass(std::move(autotuner_pass));\n   return absl::OkStatus();\n }"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 64,
        "deletions": 24
    }
}