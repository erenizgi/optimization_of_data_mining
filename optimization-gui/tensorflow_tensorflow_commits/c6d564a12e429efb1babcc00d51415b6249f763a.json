{
    "author": "GleasonK",
    "message": "Integrate StableHLO at openxla/stablehlo@96acdcb7\n\nPiperOrigin-RevId: 834068744",
    "sha": "c6d564a12e429efb1babcc00d51415b6249f763a",
    "files": [
        {
            "sha": "d1ef4a9861f638379a01fbe3675a35e0eacae170",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 252,
            "deletions": 1136,
            "changes": 1388,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c6d564a12e429efb1babcc00d51415b6249f763a/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c6d564a12e429efb1babcc00d51415b6249f763a/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=c6d564a12e429efb1babcc00d51415b6249f763a",
            "patch": "@@ -1,50 +1,6 @@\n-diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel\n---- stablehlo/BUILD.bazel\n-+++ stablehlo/BUILD.bazel\n-@@ -1105,6 +1105,24 @@\n-     tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n-     td_file = \"stablehlo/transforms/Passes.td\",\n-     deps = [\"@llvm-project//mlir:PassBaseTdFiles\"],\n-+)\n-+\n-+cc_library(\n-+    name = \"stablehlo_broadcast_lowering\",\n-+    srcs = [\n-+        \"stablehlo/transforms/StablehloBroadcastLowering.cpp\",\n-+    ],\n-+    hdrs = [\n-+        \"stablehlo/transforms/StablehloBroadcastLowering.h\",\n-+    ],\n-+    strip_include_prefix = \".\",\n-+    deps = [\n-+        \":stablehlo_ops\",\n-+        \"@llvm-project//llvm:Support\",\n-+        \"@llvm-project//mlir:IR\",\n-+        \"@llvm-project//mlir:ShapeDialect\",\n-+        \"@llvm-project//mlir:Support\",\n-+    ],\n- )\n- \n- cc_library(\n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp\n --- stablehlo/stablehlo/dialect/StablehloOps.cpp\n +++ stablehlo/stablehlo/dialect/StablehloOps.cpp\n-@@ -3275,12 +3275,12 @@\n- // Entry point for Attribute printing, TableGen generated code will handle the\n- // dispatch to the individual classes.\n- void StablehloDialect::printAttribute(Attribute attr,\n--                                      DialectAsmPrinter& os) const {\n-+                                      DialectAsmPrinter& printer) const {\n-   if (auto type_extensions = dyn_cast<TypeExtensionsAttr>(attr)) {\n--    hlo::printTypeExtensions(cast<hlo::BoundedAttrInterface>(attr), os);\n-+    hlo::printTypeExtensions(cast<hlo::BoundedAttrInterface>(attr), printer);\n-     return;\n-   }\n--  LogicalResult result = generatedAttributePrinter(attr, os);\n-+  LogicalResult result = generatedAttributePrinter(attr, printer);\n-   (void)result;\n-   assert(succeeded(result));\n- }\n @@ -4024,6 +4024,61 @@\n    ReturnOp::create(*builder, loc, compare);\n  }\n@@ -110,24 +66,7 @@ diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/\n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h\n --- stablehlo/stablehlo/dialect/StablehloOps.h\n +++ stablehlo/stablehlo/dialect/StablehloOps.h\n-@@ -93,13 +93,14 @@\n-   Type parseType(DialectAsmParser& parser) const override;\n- \n-   // Prints a type registered to this dialect.\n--  void printType(Type type, DialectAsmPrinter& os) const override;\n-+  void printType(Type type, DialectAsmPrinter& printer) const override;\n- \n-   // Parses an attribute registered to this dialect.\n-   Attribute parseAttribute(DialectAsmParser& parser, Type type) const override;\n- \n-   // Prints an attribute registered to this dialect.\n--  void printAttribute(Attribute attr, DialectAsmPrinter& os) const override;\n-+  void printAttribute(Attribute attr,\n-+                      DialectAsmPrinter& printer) const override;\n- \n-   // Get the set dialect version.\n-   std::optional<StablehloDialectVersion> getVersion() const;\n-@@ -203,6 +204,16 @@\n+@@ -204,6 +204,16 @@\n    stablehlo::ReturnOp::create(builder, loc, reducer.getResult());\n  }\n  \n@@ -221,419 +160,22 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c\n  ////////\n  // Custom Attribute Tests\n  ////////\n-diff --ruN a/stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td b/stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td\n---- stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td\n-+++ stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td\n-@@ -17,6 +17,6 @@\n- #ifndef STABLEHLO_INTEGRATIONS_PYTHON_INTERPRETER_OPS\n- #define STABLEHLO_INTEGRATIONS_PYTHON_INTERPRETER_OPS\n- \n--include \"third_party/stablehlo/stablehlo/reference/InterpreterOps.h\"\n-+include \"stablehlo/reference/InterpreterOps.h\"\n- \n- #endif\n-diff --ruN a/stablehlo/stablehlo/tests/BUILD.bazel b/stablehlo/stablehlo/tests/BUILD.bazel\n---- stablehlo/stablehlo/tests/BUILD.bazel\n-+++ stablehlo/stablehlo/tests/BUILD.bazel\n-@@ -102,6 +102,8 @@\n-     deps = [\n-         \":test_utils_inc_gen\",\n-         \"//:stablehlo_assembly_format\",\n-+        \"//:stablehlo_broadcast_lowering\",\n-+        \"//:stablehlo_ops\",\n-         \"@llvm-project//llvm:Support\",\n-         \"@llvm-project//mlir:FuncDialect\",\n-         \"@llvm-project//mlir:IR\",\n-diff --ruN a/stablehlo/stablehlo/tests/TestUtils.cpp b/stablehlo/stablehlo/tests/TestUtils.cpp\n---- stablehlo/stablehlo/tests/TestUtils.cpp\n-+++ stablehlo/stablehlo/tests/TestUtils.cpp\n-@@ -19,6 +19,7 @@\n- #include <utility>\n- \n- #include \"llvm/ADT/STLExtras.h\"\n-+#include \"llvm/ADT/SmallVector.h\"\n- #include \"llvm/Support/Casting.h\"\n- #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n- #include \"mlir/Dialect/Shape/IR/Shape.h\"\n-@@ -28,6 +29,7 @@\n- #include \"mlir/IR/Operation.h\"\n- #include \"mlir/IR/OperationSupport.h\"\n- #include \"mlir/IR/PatternMatch.h\"\n-+#include \"mlir/IR/TypeRange.h\"\n- #include \"mlir/Interfaces/InferTypeOpInterface.h\"\n- #include \"mlir/Interfaces/SideEffectInterfaces.h\"\n- #include \"mlir/Pass/Pass.h\"\n-@@ -35,11 +37,34 @@\n- #include \"mlir/Support/LLVM.h\"\n- #include \"mlir/Support/LogicalResult.h\"\n- #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-+#include \"stablehlo/dialect/StablehloOps.h\"\n-+#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n- \n- namespace mlir {\n- namespace hlo {\n- \n- namespace {\n-+\n-+struct BroadcastValuesPattern : public RewritePattern {\n-+  explicit BroadcastValuesPattern(MLIRContext* context)\n-+      : RewritePattern(\"hlo_test_broadcast.numpy_broadcast\", 1, context) {}\n-+  LogicalResult matchAndRewrite(Operation* op,\n-+                                PatternRewriter& rewriter) const override {\n-+    // Process all operands\n-+    SmallVector<Value> operands = llvm::to_vector(op->getOperands());\n-+    auto broadcastedOperands =\n-+        stablehlo::numpyBroadcastIfNeeded(rewriter, operands);\n-+    if (failed(broadcastedOperands)) return failure();\n-+\n-+    // Replace with custom call to avoid pattern reapplication\n-+    auto customCall = stablehlo::CustomCallOp::create(\n-+        rewriter, op->getLoc(), op->getResultTypes(), *broadcastedOperands);\n-+    customCall.setCallTargetName(\"numpy_broadcasted\");\n-+    customCall.setHasSideEffect(true);\n-+    rewriter.replaceOp(op, customCall);\n-+    return success();\n-+  }\n-+};\n- \n- struct InferReturnTypesPattern : public RewritePattern {\n-   explicit InferReturnTypesPattern(MLIRContext *context)\n-@@ -137,36 +162,55 @@\n-   }\n- };\n- \n-+#define GEN_PASS_DEF_HLOTESTBROADCASTPASS\n- #define GEN_PASS_DEF_HLOTESTINFERPASS\n- #define GEN_PASS_DEF_HLOTESTSPECULATABILITYPASS\n- #include \"stablehlo/tests/TestUtils.h.inc\"\n- \n-+struct HloTestBroadcastPass\n-+    : public impl::HloTestBroadcastPassBase<HloTestBroadcastPass> {\n-+  LogicalResult initialize(MLIRContext* context) override {\n-+    RewritePatternSet patterns(context);\n-+    patterns.add<BroadcastValuesPattern>(context);\n-+    patterns_ = std::move(patterns);\n-+    return success();\n-+  }\n-+\n-+  void runOnOperation() override {\n-+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns_))))\n-+      return signalPassFailure();\n-+  }\n-+\n-+ private:\n-+  FrozenRewritePatternSet patterns_;\n-+};\n-+\n- struct HloTestInferPass : public impl::HloTestInferPassBase<HloTestInferPass> {\n-   LogicalResult initialize(MLIRContext *context) override {\n--    RewritePatternSet patterns_(context);\n--    patterns_.add<InferReturnTypesPattern>(context);\n--    patterns_.add<ReifyReturnTypeShapesPattern>(context);\n--    patterns = std::move(patterns_);\n-+    RewritePatternSet patterns(context);\n-+    patterns.add<InferReturnTypesPattern>(context);\n-+    patterns.add<ReifyReturnTypeShapesPattern>(context);\n-+    patterns_ = std::move(patterns);\n-     return success();\n-   }\n- \n-   void runOnOperation() override {\n--    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns))))\n-+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns_))))\n-       return signalPassFailure();\n-   }\n- \n-  private:\n--  FrozenRewritePatternSet patterns;\n-+  FrozenRewritePatternSet patterns_;\n- };\n- \n- struct HloTestSpeculatabilityPass\n-     : public impl::HloTestSpeculatabilityPassBase<HloTestSpeculatabilityPass> {\n-   LogicalResult initialize(MLIRContext *context) override {\n--    RewritePatternSet patterns_(context);\n--    patterns_.add<IsSpeculatablePattern>(context);\n--    patterns_.add<IsNotSpeculatablePattern>(context);\n--    patterns_.add<IsRecursivelySpeculatablePattern>(context);\n--    patterns = std::move(patterns_);\n-+    RewritePatternSet patterns(context);\n-+    patterns.add<IsSpeculatablePattern>(context);\n-+    patterns.add<IsNotSpeculatablePattern>(context);\n-+    patterns.add<IsRecursivelySpeculatablePattern>(context);\n-+    patterns_ = std::move(patterns);\n-     return success();\n-   }\n- \n-@@ -175,11 +219,11 @@\n-     config.setMaxIterations(1)\n-         .setUseTopDownTraversal(true)\n-         .setRegionSimplificationLevel(GreedySimplifyRegionLevel::Disabled);\n--    (void)applyPatternsGreedily(getOperation(), std::move(patterns));\n-+    (void)applyPatternsGreedily(getOperation(), std::move(patterns_));\n-   }\n- \n-  private:\n--  FrozenRewritePatternSet patterns;\n-+  FrozenRewritePatternSet patterns_;\n- };\n- \n- #define GEN_PASS_REGISTRATION\n-diff --ruN a/stablehlo/stablehlo/tests/TestUtils.td b/stablehlo/stablehlo/tests/TestUtils.td\n---- stablehlo/stablehlo/tests/TestUtils.td\n-+++ stablehlo/stablehlo/tests/TestUtils.td\n-@@ -16,6 +16,11 @@\n- \n- include \"mlir/Pass/PassBase.td\"\n- \n-+def HloTestBroadcastPass : Pass<\"hlo-test-broadcast\", \"func::FuncOp\"> {\n-+  let summary = \"Uses test ops to invoke BroadcastUtils methods.\";\n-+  let dependentDialects = [\"stablehlo::StablehloDialect\"];\n-+}\n-+\n- def HloTestInferPass : Pass<\"hlo-test-infer\", \"func::FuncOp\"> {\n-   let summary = \"Uses test ops to invoke InferShapedTypeOpInterface methods.\";\n-   let dependentDialects = [\"shape::ShapeDialect\"];\n diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stablehlo/tests/ops_broadcasting.mlir\n --- stablehlo/stablehlo/tests/ops_broadcasting.mlir\n +++ stablehlo/stablehlo/tests/ops_broadcasting.mlir\n-@@ -0,0 +1,322 @@\n-+// RUN: stablehlo-opt %s --hlo-test-broadcast --split-input-file --allow-unregistered-dialect | FileCheck %s\n-+\n-+/////////\n-+// Scalar broadcast tests.\n-+\n-+// [] x [1] => [1]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_1\n-+func.func @scalar_broadcast_scalar_x_1(%arg0: tensor<f64>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<1xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [1] x [] => [1]\n-+// CHECK-LABEL: func @scalar_broadcast_1_x_scalar\n-+func.func @scalar_broadcast_1_x_scalar(%arg0: tensor<1xf64>, %arg1: tensor<f64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<1xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<f64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [] x [10] => [10]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_10\n-+func.func @scalar_broadcast_scalar_x_10(%arg0: tensor<f64>, %arg1: tensor<10xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<10xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<10xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [<=10] x [] => [<=10]\n-+// CHECK-LABEL: func @scalar_broadcast_b10_x_scalar\n-+func.func @scalar_broadcast_b10_x_scalar(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<f64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<f64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [] x [<=10] => [<=10]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_b10\n-+func.func @scalar_broadcast_scalar_x_b10(%arg0: tensor<f64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 0\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [] x [1, <=10, 1] => [1, <=10, 1]\n-+// CHECK-LABEL: func @scalar_broadcast_scalar_x_1_b10_1\n-+func.func @scalar_broadcast_scalar_x_1_b10_1(%arg0: tensor<f64>, %arg1: tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<1x10x1xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 1\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// [10, 1, <=5] x [] => [10, 1, <=5]\n-+// CHECK-LABEL: func @scalar_broadcast_10_1_b5_x_scalar\n-+func.func @scalar_broadcast_10_1_b5_x_scalar(%arg0: tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, %arg1: tensor<f64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10x1x5xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 2\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 2\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, tensor<f64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+//////\n-+// 1-D SCALAR TESTS\n-+\n-+// [1] x [1] => [1]\n-+// [1] x [10] => [1]\n-+// [<=10] x [1] => [<=10]\n-+// [1] x [<=10] => [<=10]\n-+// [1] x [1, <=10, 1] => [1, <=10, 1]\n+@@ -92,6 +92,8 @@\n+ // [<=10] x [1] => [<=10]\n+ // [1] x [<=10] => [<=10]\n+ // [1] x [1, <=10, 1] => [1, <=10, 1]\n +// [5] x [10, 1] => [10, 5]\n +// [5] x [<=10, 1] => [<=10, 5]\n-+\n-+\n-+// [1] x [1] => [1]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_1\n-+func.func @single_dim_scalar_1_x_1(%arg0: tensor<1xf64>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [1] x [10] => [10]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_10\n-+func.func @single_dim_scalar_1_x_10(%arg0: tensor<1xf64>, %arg1: tensor<10xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<10xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [<=10] x [1] => [<=10]\n-+// CHECK-LABEL: func @single_dim_scalar_b10_x_1\n-+func.func @single_dim_scalar_b10_x_1(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [1] x [<=10] => [<=10]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_b10\n-+func.func @single_dim_scalar_1_x_b10(%arg0: tensor<1xf64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 0\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// [<=10] x [<=10] => [<=10] // PT layer must ensure these are identical!\n-+// CHECK-LABEL: func @single_dim_scalar_b10_x_b10\n-+func.func @single_dim_scalar_b10_x_b10(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [1] x [1, <=10, 1] => [1, <=10, 1]\n-+// CHECK-LABEL: func @single_dim_scalar_1_x_1_b10_1\n-+func.func @single_dim_scalar_1_x_1_b10_1(%arg0: tensor<1xf64>, %arg1: tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<1xf64>) -> tensor<1x10x1xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 1\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [10, 1, <=5] x [1] => [10, 1, <=5]\n-+// CHECK-LABEL: func @single_dim_scalar_10_1_b5_x_1\n-+func.func @single_dim_scalar_10_1_b5_x_1(%arg0: tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1xf64>) -> tensor<10x1x5xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 2\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 2\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+\n-+//////\n-+// N-D Tests\n-+\n-+// [1, 2] x [1, 2] => [1, 2]\n-+// CHECK-LABEL: func @tensor_no_broadcast_match\n-+func.func @tensor_no_broadcast_match(%arg0: tensor<1x2xf64>, %arg1: tensor<1x2xf64>) -> !stablehlo.token {\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1x2xf64>, tensor<1x2xf64>) ->  !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// [10, 1] x [1, 1] => [10, 1]\n-+// CHECK-LABEL: func @tensor_broadcast_10_1_x_1_1\n-+func.func @tensor_broadcast_10_1_x_1_1(%arg0: tensor<10x1xf64>, %arg1: tensor<1x1xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x1xf64>) -> tensor<10x1xf64>\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1xf64>, tensor<1x1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [<=10, 1] x [1, 10] => [<=10, 10]\n-+// CHECK-LABEL: func @tensor_broadcast_b10_1_x_1_10\n-+func.func @tensor_broadcast_b10_1_x_1_10(%arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>, %arg1: tensor<1x10xf64>) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x10xf64, #stablehlo.bounds<10, ?>>\n-+  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x10xf64>) -> tensor<10x10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>, tensor<1x10xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n-+// [<=10, 1] x [1, <=10] => [<=10, <=10]\n-+// CHECK-LABEL: func @tensor_broadcast_b10_1_x_1_b10\n-+func.func @tensor_broadcast_b10_1_x_1_b10(\n-+  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+  %arg1: tensor<1x?xf64, #stablehlo.bounds<?, 10>>\n-+) -> !stablehlo.token {\n-+  // CHECK: %[[LHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x10xf64, #stablehlo.bounds<10, ?>>\n-+  // CHECK: %[[ARG1_DIM1_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n-+  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST_STATIC]], %[[ARG1_DIM1_SIZE]], dim = 1\n-+  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x?xf64, #stablehlo.bounds<?, 10>>) -> tensor<10x?xf64, #stablehlo.bounds<?, 10>>\n-+  // CHECK: %[[ARG0_DIM0_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[ARG0_DIM0_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %[[RHS_BCAST_DYN]])\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (\n-+    tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+    tensor<1x?xf64, #stablehlo.bounds<?, 10>>\n-+  ) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n-+// -----\n-+\n+ \n+ \n+ // [1] x [1] => [1]\n+@@ -232,6 +234,38 @@\n+ \n+ // -----\n+ \n +// [5] x [10, 1] => [10, 5]\n +// CHECK-LABEL: func @tensor_broadcast_5_x_10_1\n +func.func @tensor_broadcast_5_x_10_1(%arg0: tensor<5xf64>, %arg1: tensor<10x1xf64>) -> !stablehlo.token {\n@@ -666,21 +208,13 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n +\n +// -----\n +\n-+//////\n-+// N-ary broadcast tests.\n-+\n-+\n-+// [<=10, 1] x [1, <=10] x [1] => [<=10, <=10]\n-+// CHECK-LABEL: func @nary_broadcast_b10_1_x_1_b10_x_1\n-+func.func @nary_broadcast_b10_1_x_1_b10_x_1(\n-+  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n-+  %arg1: tensor<1x?xf64, #stablehlo.bounds<?, 10>>,\n-+  %arg2: tensor<1xf64>\n-+) -> !stablehlo.token {\n-+  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1, %arg2) : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>, tensor<1x?xf64, #stablehlo.bounds<?, 10>>, tensor<1xf64>) -> !stablehlo.token\n-+  return %0 : !stablehlo.token\n-+}\n-+\n+ //////\n+ // N-ary broadcast tests.\n+ \n+@@ -247,3 +281,42 @@\n+   return %0 : !stablehlo.token\n+ }\n+ \n +// -----\n +\n +/////\n@@ -720,101 +254,10 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n +  return %0 : !stablehlo.token\n +}\n +\n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -473,6 +473,44 @@\n-   return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xi32>, tensor<3x3xi32>, tensor<2x5xi32>\n- }\n- \n-+// CHECK-LABEL: func.func @fold_concatenate_splat_leading\n-+func.func @fold_concatenate_splat_leading(%arg0: tensor<1xi32>) -> tensor<3xi32> {\n-+  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate [[CST0]], %arg0, dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n-+  %0 = stablehlo.concatenate %cst0, %cst0, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>\n-+  return %0 : tensor<3xi32>\n-+}\n-+\n-+// CHECK-LABEL: func.func @fold_concatenate_splat_trailing\n-+func.func @fold_concatenate_splat_trailing(%arg0: tensor<2xi32>) -> tensor<6xi32> {\n-+  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<4xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate %arg0, [[CST0]], dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<2xi32>\n-+  %0 = stablehlo.concatenate %arg0, %cst0, %cst0, dim = 0 : (tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<6xi32>\n-+  return %0 : tensor<6xi32>\n-+}\n-+\n-+// CHECK-LABEL: func.func @fold_concatenate_splat_middle\n-+func.func @fold_concatenate_splat_middle(%arg0: tensor<1xi32>) -> tensor<4xi32> {\n-+  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate %arg0, [[CST0]], %arg0, dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n-+  %0 = stablehlo.concatenate %arg0, %cst0, %cst0, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>\n-+  return %0 : tensor<4xi32>\n-+}\n-+\n-+// CHECK-LABEL: func.func @fold_concatenate_splat_multiple\n-+func.func @fold_concatenate_splat_multiple(%arg0: tensor<1xi32>) -> tensor<5xi32> {\n-+  // CHECK-DAG: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n-+  // CHECK-DAG: [[CST1:%.+]] = stablehlo.constant dense<1> : tensor<2xi32>\n-+  // CHECK-NEXT: stablehlo.concatenate [[CST0]], [[CST1]], %arg0, dim = 0\n-+  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n-+  %cst1 = stablehlo.constant dense<1> : tensor<1xi32>\n-+  %0 = stablehlo.concatenate %cst0, %cst0, %cst1, %cst1, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<5xi32>\n-+  return %0 : tensor<5xi32>\n-+}\n-+\n- // -----\n- \n- ////////\n-@@ -576,16 +614,19 @@\n- // ReshapeOp\n- \n- // CHECK-LABEL: func @reshape_fold\n--func.func @reshape_fold() -> (tensor<1xi32>, tensor<2x2xi32>) {\n--  %c0 = stablehlo.constant dense<2> : tensor<i32>\n-+func.func @reshape_fold() -> (tensor<1xf32>, tensor<2x2xi32>, tensor<3x2xcomplex<f32>>) {\n-+  %c0 = stablehlo.constant dense<2.0> : tensor<f32>\n-   %c1 = stablehlo.constant dense<[1, 2, 3, 4]> : tensor<4xi32>\n--  %0 = stablehlo.reshape %c0 : (tensor<i32>) -> tensor<1xi32>\n-+  %c2 = stablehlo.constant dense<(1.0,2.0)> : tensor<2x3xcomplex<f32>>\n-+  %0 = stablehlo.reshape %c0 : (tensor<f32>) -> tensor<1xf32>\n-   %1 = stablehlo.reshape %c1 : (tensor<4xi32>) -> tensor<2x2xi32>\n--\n--  // CHECK-DAG:  [[CST1:%.+]] = stablehlo.constant dense<2> : tensor<1xi32>\n--  // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\\[\\[1, 2\\], \\[3, 4\\]\\]}}> : tensor<2x2xi32>\n--  // CHECK-NEXT: return [[CST1]], [[CST2]]\n--  return %0, %1 : tensor<1xi32>, tensor<2x2xi32>\n-+  %2 = stablehlo.reshape %c2 : (tensor<2x3xcomplex<f32>>) -> tensor<3x2xcomplex<f32>>\n-+\n-+  // CHECK-DAG:  [[RESULT0:%.+]] = stablehlo.constant dense<2.0{{.*}}> : tensor<1xf32>\n-+  // CHECK-DAG:  [[RESULT1:%.+]] = stablehlo.constant dense<{{\\[\\[1, 2\\], \\[3, 4\\]\\]}}> : tensor<2x2xi32>\n-+  // CHECK-DAG:  [[RESULT2:%.+]] = stablehlo.constant dense<(1.0{{.*}},2.0{{.*}})> : tensor<3x2xcomplex<f32>>\n-+  // CHECK-NEXT: return [[RESULT0]], [[RESULT1]], [[RESULT2]]\n-+  return %0, %1, %2 : tensor<1xf32>, tensor<2x2xi32>, tensor<3x2xcomplex<f32>>\n- }\n- \n- // -----\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n-@@ -27,6 +27,14 @@\n-   // CHECK-NOT: stablehlo.constant\n-   // CHECK: return %arg0\n-   return %1 : tensor<f32>\n-+}\n-+\n-+// CHECK-LABEL: @add_cst_on_rhs_with_attrs\n-+func.func @add_cst_on_rhs_with_attrs(%arg0: tensor<f32>) -> tensor<f32> {\n-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n-+  // CHECK: stablehlo.add %arg0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  %0 = stablehlo.add %cst, %arg0 {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  return %0 : tensor<f32>\n- }\n- \n- // -----\n-@@ -120,6 +128,16 @@\n+@@ -128,6 +128,16 @@\n    return %7 : tensor<3x2x3x3xi32>\n  }\n  \n@@ -831,7 +274,7 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplific\n  // CHECK-LABEL: func.func @broadcast_in_dim_reshape\n  // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x6xi32>)\n  func.func @broadcast_in_dim_reshape(%arg0: tensor<3x6xi32>)\n-@@ -132,6 +150,15 @@\n+@@ -140,6 +150,15 @@\n  \n    // CHECK-NEXT: return [[R0]], [[R5]]\n    return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>\n@@ -847,345 +290,186 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplific\n  }\n  \n  // CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape\n-@@ -976,6 +1003,26 @@\n-   // CHECK-NOT: stablehlo.constant\n-   // CHECK: return %arg0 : tensor<f32>\n-   return %0 : tensor<f32>\n-+}\n-+\n-+// CHECK-LABEL: @multiply_by_one_merge_attrs\n-+func.func @multiply_by_one_merge_attrs(%arg0: tensor<f32>) -> tensor<f32> {\n-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n-+  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\"}} : tensor<f32>\n-+  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"1\"}} : tensor<f32>\n-+  // CHECK: return %[[ADD]] : tensor<f32>\n-+  return %1 : tensor<f32>\n-+}\n-+\n-+// CHECK-LABEL: @multiply_by_one_merge_attrs_conflict\n-+func.func @multiply_by_one_merge_attrs_conflict(%arg0: tensor<f32>) -> tensor<f32> {\n-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n-+  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"0\"}} : tensor<f32>\n-+  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n-+  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"1\"}} : tensor<f32>\n-+  // CHECK: return %[[ADD]] : tensor<f32>\n-+  return %1 : tensor<f32>\n- }\n- \n- // -----\n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n-@@ -9,6 +9,32 @@\n-   // CHECK: stablehlo.add %arg0, %cst : tensor<f32>\n-   %1 = stablehlo.add %0, %arg0 : tensor<f32>\n-   return %1 : tensor<f32>\n-+}\n-+\n-+// -----\n-+\n-+func.func @concatenate_fold_splat_flatten_integ(%arg0: tensor<8xf32>) -> tensor<64xf32> {\n-+  // CHECK-DAG: [[CST0:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<8xf32>\n-+  // CHECK-DAG: [[CST1:%.+]] = stablehlo.constant dense<1.000000e+00> : tensor<8xf32>\n-+  // CHECK-DAG: [[CST2:%.+]] = stablehlo.constant dense<2.000000e+00> : tensor<8xf32>\n-+  // CHECK-DAG: [[CST3:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<8xf32>\n-+  // CHECK: stablehlo.concatenate [[CST0]], [[CST1]], [[CST2]], [[CST3]], %arg0, %arg0, %arg0, %arg0,\n-+  %cst0 = stablehlo.constant dense<0.0> : tensor<f32>\n-+  %cst1 = stablehlo.constant dense<1.0> : tensor<f32>\n-+  %cst2 = stablehlo.constant dense<2.0> : tensor<f32>\n-+  %cst3 = stablehlo.constant dense<3.0> : tensor<f32>\n-+  %0 = stablehlo.reshape %cst0 : (tensor<f32>) -> tensor<1xf32>\n-+  %1 = stablehlo.reshape %cst1 : (tensor<f32>) -> tensor<1xf32>\n-+  %2 = stablehlo.reshape %cst2 : (tensor<f32>) -> tensor<1xf32>\n-+  %3 = stablehlo.reshape %cst3 : (tensor<f32>) -> tensor<1xf32>\n-+  %4 = stablehlo.concatenate %0, %0, %0, %0, %0, %0, %0, %0, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %5 = stablehlo.concatenate %1, %1, %1, %1, %1, %1, %1, %1, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %6 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %7 = stablehlo.concatenate %3, %3, %3, %3, %3, %3, %3, %3, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n-+  %8 = stablehlo.concatenate %4, %5, %6, %7, dim = 0 : (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> tensor<32xf32>\n-+  %9 = stablehlo.concatenate %arg0, %arg0, %arg0, %arg0, dim = 0 : (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> tensor<32xf32>\n-+  %10 = stablehlo.concatenate %8, %9, dim = 0 : (tensor<32xf32>, tensor<32xf32>) -> tensor<64xf32>\n-+  return %10 : tensor<64xf32>\n- }\n- \n- // -----\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-@@ -0,0 +1,298 @@\n-+/* Copyright 2025 The StableHLO Authors.\n-+\n-+Licensed under the Apache License, Version 2.0 (the \"License\");\n-+you may not use this file except in compliance with the License.\n-+You may obtain a copy of the License at\n-+\n-+    http://www.apache.org/licenses/LICENSE-2.0\n-+\n-+Unless required by applicable law or agreed to in writing, software\n-+distributed under the License is distributed on an \"AS IS\" BASIS,\n-+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-+See the License for the specific language governing permissions and\n-+limitations under the License.\n-+==============================================================================*/\n-+\n-+#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n-+\n-+#include <algorithm>\n-+#include <cassert>\n-+#include <cstddef>\n-+#include <cstdint>\n-+#include <string>\n-+#include <utility>\n-+\n-+#include \"llvm/ADT/STLExtras.h\"\n-+#include \"llvm/ADT/Sequence.h\"\n-+#include \"llvm/ADT/SmallVector.h\"\n-+#include \"llvm/Support/Debug.h\"\n-+#include \"llvm/Support/raw_ostream.h\"\n-+#include \"mlir/IR/Builders.h\"\n-+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n-+#include \"mlir/IR/BuiltinTypes.h\"\n-+#include \"mlir/IR/Diagnostics.h\"\n-+#include \"mlir/IR/Location.h\"\n-+#include \"mlir/IR/Types.h\"\n-+#include \"mlir/IR/Value.h\"\n-+#include \"mlir/Support/LLVM.h\"\n-+#include \"stablehlo/dialect/StablehloOps.h\"\n-+\n-+#define DEBUG_TYPE \"stablehlo-broadcast-lowering\"\n-+\n-+namespace mlir {\n-+namespace stablehlo {\n-+\n-+/////\n-+// Bounded dynamism broadcasting\n-+\n-+namespace {\n-+\n-+DimensionInfo getDimensionInfo(Value op, mlir::RankedTensorType tensorType,\n-+                               TypeExtensionsAttr encoding,\n-+                               int64_t dim) {\n-+  if (!encoding || !mlir::ShapedType::isDynamic(tensorType.getDimSize(dim)))\n-+    return DimensionInfo{tensorType.getDimSize(dim)};\n-+\n-+  return DimensionInfo{\n-+      encoding.getBounds()[dim],\n-+      op,\n-+      dim,\n-+  };\n-+}\n-+\n-+FailureOr<Dimensions> getDimensions(Value op) {\n-+  // Get tensor type\n-+  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n-+  if (!tensor_type)\n+@@ -63,7 +63,8 @@\n+   // Get tensor type\n+   mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n+   if (!tensor_type)\n+-    return emitError(op.getLoc(), \"expected ranked tensor type\");\n +    return emitError(op.getLoc(),\n +                     \"expected ranked tensor type for broadcast inputs\");\n-+\n-+  auto encoding =\n-+      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n-+          tensor_type.getEncoding());\n-+\n-+  Dimensions dimensions;\n-+  dimensions.reserve(tensor_type.getRank());\n-+  for (size_t idx = 0; idx < tensor_type.getRank(); ++idx) {\n-+    auto dimInfo = getDimensionInfo(op, tensor_type, encoding, idx);\n-+    dimensions.push_back(dimInfo);\n-+  }\n-+  return dimensions;\n-+}\n-+\n+ \n+   auto encoding =\n+       mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n+@@ -78,10 +79,11 @@\n+   return dimensions;\n+ }\n+ \n+-FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(const Dimensions& a,\n +FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(Value op,\n +                                                       const Dimensions& a,\n-+                                                       const Dimensions& b) {\n-+  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] inputs: \"\n+                                                        const Dimensions& b) {\n+   LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] inputs: \"\n+-                          << toString(a) << \" * \" << toString(b));\n +                          << toString(a) << \" * \" << toString(b) << \"\\n\");\n-+  size_t max_rank = std::max(a.size(), b.size());\n-+  Dimensions result(max_rank);\n-+\n-+  // Iterate from right to left (NumPy-style broadcasting)\n-+  for (int i = 1; i <= max_rank; ++i) {\n-+    size_t a_idx = a.size() - i;\n-+    size_t b_idx = b.size() - i;\n-+    size_t res_idx = max_rank - i;\n-+\n-+    // Get DimensionInfo for the current index, padding with size 1 if out of\n-+    // bounds.\n-+    DimensionInfo dim_a =\n-+        (a_idx >= 0 && a_idx < a.size()) ? a[a_idx] : DimensionInfo{1};\n-+    DimensionInfo dim_b =\n-+        (b_idx >= 0 && b_idx < b.size()) ? b[b_idx] : DimensionInfo{1};\n-+\n-+    // Short circuit on size 1 dimensions.\n-+    if (dim_a.size == 1) {\n-+      result[res_idx] = dim_b;\n-+      continue;\n-+    }\n-+    if (dim_b.size == 1) {\n-+      result[res_idx] = dim_a;\n-+      continue;\n-+    }\n-+\n-+    // If both LHS and RHS are not 1, dim size must match.\n-+    if (dim_a.size != dim_b.size) {\n+   size_t max_rank = std::max(a.size(), b.size());\n+   Dimensions result(max_rank);\n+ \n+@@ -110,14 +112,14 @@\n+ \n+     // If both LHS and RHS are not 1, dim size must match.\n+     if (dim_a.size != dim_b.size) {\n+-      return emitError(a[a_idx].boundOp.value().getLoc(),\n+-                       \"incompatible shapes for broadcasting \")\n +      // FIXME\n +      return emitError(op.getLoc(), \"incompatible shapes for broadcasting \")\n-+             << dim_a.size << \" and \" << dim_b.size;\n-+    }\n-+\n-+    // If bounded both must be bounded\n-+    if (dim_a.boundOp.has_value() != dim_b.boundOp.has_value()) {\n+              << dim_a.size << \" and \" << dim_b.size;\n+     }\n+ \n+     // If bounded both must be bounded\n+     if (dim_a.boundOp.has_value() != dim_b.boundOp.has_value()) {\n+-      return emitError(a[a_idx].boundOp.value().getLoc(),\n +      return emitError(op.getLoc(),\n-+                       \"cannot mix bounded and static dimensions in broadcast\");\n-+    }\n-+\n-+    // LHS and RHS match, populate with one of the dimensions.\n-+    result[res_idx] = dim_a;\n-+  }\n-+\n-+  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n+                        \"cannot mix bounded and static dimensions in broadcast\");\n+     }\n+ \n+@@ -126,7 +128,7 @@\n+   }\n+ \n+   LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n+-                          << toString(result));\n +                          << toString(result) << \"\\n\");\n-+  return result;\n-+}\n-+\n-+mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n-+                                           mlir::Type element_type) {\n-+  mlir::SmallVector<int64_t> shape;\n-+  mlir::SmallVector<int64_t> bounds;\n-+  shape.reserve(dims.size());\n-+  for (const DimensionInfo& dim : dims) {\n-+    if (dim.boundOp.has_value()) {\n-+      shape.push_back(mlir::ShapedType::kDynamic);\n-+      bounds.push_back(dim.size);\n-+    } else {\n-+      shape.push_back(dim.size);\n-+      bounds.push_back(mlir::ShapedType::kDynamic);\n-+    }\n-+  }\n-+  mlir::stablehlo::TypeExtensionsAttr encoding;\n-+  if (!llvm::all_of(\n-+          bounds, [](int64_t b) { return b == mlir::ShapedType::kDynamic; })) {\n-+    encoding = mlir::stablehlo::TypeExtensionsAttr::get(\n-+        element_type.getContext(), bounds);\n-+  }\n-+  return mlir::RankedTensorType::get(shape, element_type, encoding);\n-+}\n-+\n-+}  // namespace\n-+\n+   return result;\n+ }\n+ \n+@@ -155,8 +157,11 @@\n+ \n+ }  // namespace\n+ \n+-FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops) {\n+-  if (ops.empty()) return failure();\n +FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n +                                             ArrayRef<Value> ops) {\n +  if (ops.empty())\n +    return emitError(builder.getInsertionPoint()->getLoc(),\n +                     \"requires at least one operand to broadcast\");\n-+\n-+  Value first = ops[0];\n-+  auto bcastShapeOrFail = getDimensions(first);\n-+  if (failed(bcastShapeOrFail)) return failure();\n-+  Dimensions bcastShape = std::move(*bcastShapeOrFail);\n-+\n-+  for (int i = 1; i < ops.size(); ++i) {\n-+    Value currOp = ops[i];\n-+    auto dims = getDimensions(currOp);\n-+    if (failed(dims)) return failure();\n-+    auto currBcastShapeOrFail =\n+ \n+   Value first = ops[0];\n+   auto bcastShapeOrFail = getDimensions(first);\n+@@ -168,7 +173,7 @@\n+     auto dims = getDimensions(currOp);\n+     if (failed(dims)) return failure();\n+     auto currBcastShapeOrFail =\n+-        getNumpyBroadcastShapeWithBounds(bcastShape, *dims);\n +        getNumpyBroadcastShapeWithBounds(currOp, bcastShape, *dims);\n-+    if (failed(currBcastShapeOrFail)) return failure();\n-+    bcastShape = std::move(*currBcastShapeOrFail);\n-+  }\n-+  return std::move(bcastShape);\n-+}\n-+\n-+std::string toString(const Dimensions& dims) {\n-+  std::string result;\n-+  llvm::raw_string_ostream os(result);\n-+  os << \"tensor<\";\n-+  llvm::interleave(\n-+      dims, os,\n-+      [&](const DimensionInfo& dim) {\n-+        os << (dim.boundOp.has_value() ? \"b\" : \"\") << dim.size;\n-+      },\n-+      \"x\");\n-+  os << \">\";\n-+  return result;\n-+}\n-+\n-+FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n-+                                                     ArrayRef<Value> operands) {\n-+  // Figure out the broadcast shape\n+     if (failed(currBcastShapeOrFail)) return failure();\n+     bcastShape = std::move(*currBcastShapeOrFail);\n+   }\n+@@ -192,7 +197,7 @@\n+ FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n+                                                      ArrayRef<Value> operands) {\n+   // Figure out the broadcast shape\n+-  auto bcastShapeOrFail = getNumpyBroadcastShape(operands);\n +  auto bcastShapeOrFail = getNumpyBroadcastShape(builder, operands);\n-+  if (failed(bcastShapeOrFail)) return failure();\n-+  Dimensions bcastShape = std::move(*bcastShapeOrFail);\n-+\n-+  // Apply to all operands\n-+  SmallVector<Value> broadcastedOperands;\n-+  for (auto operand : operands) {\n-+    auto bcastOperand = numpyBroadcastIfNeeded(builder, operand, bcastShape);\n-+    if (failed(bcastOperand)) return failure();\n-+    broadcastedOperands.push_back(*bcastOperand);\n-+  }\n-+  return std::move(broadcastedOperands);\n-+}\n-+\n-+FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n-+                                        const Dimensions& shape) {\n+   if (failed(bcastShapeOrFail)) return failure();\n+   Dimensions bcastShape = std::move(*bcastShapeOrFail);\n+ \n+@@ -208,35 +213,34 @@\n+ \n+ FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n+                                         const Dimensions& shape) {\n+-  LLVM_DEBUG(llvm::dbgs() << \"[BroadcastIfNeeded] input: \" << input\n+-                          << \" shape: \" << toString(shape));\n +  LLVM_DEBUG(llvm::dbgs() << \"[numpyBroadcastIfNeeded] Broadcasting input \"\n +                          << input.getType() << \" => \" << toString(shape)\n +                          << \"\\n\");\n-+  auto loc = input.getLoc();\n+   auto loc = input.getLoc();\n+-  mlir::RankedTensorType input_type =\n +  mlir::RankedTensorType inputType =\n-+      dyn_cast<RankedTensorType>(input.getType());\n+       dyn_cast<RankedTensorType>(input.getType());\n+-  if (!input_type) return emitError(input.getLoc(), \"expected tensor type\");\n+-  mlir::RankedTensorType output_type =\n+-      getRankedTensorType(shape, input_type.getElementType());\n +  if (!inputType)\n +    return emitError(loc, \"expected ranked tensor type for broadcast inputs\");\n +  mlir::RankedTensorType outputType =\n +      getRankedTensorType(shape, inputType.getElementType());\n-+\n-+  // Short circuit if no broadcasting is needed.\n+ \n+   // Short circuit if no broadcasting is needed.\n+-  if (input_type == output_type) return input;\n+-\n+-  int64_t input_rank = input_type.getRank();\n+-  int64_t output_rank = output_type.getRank();\n+-  if (input_rank > output_rank)\n +  if (inputType == outputType) return input;\n +\n +  int64_t inputRank = inputType.getRank();\n +  int64_t outputRank = outputType.getRank();\n +  if (inputRank > outputRank)\n-+    return emitError(loc, \"input rank must be <= output rank, got \")\n+     return emitError(loc, \"input rank must be <= output rank, got \")\n+-           << input_rank << \" vs \" << output_rank;\n+-\n+-  size_t rank_diff = output_rank - input_rank;\n+-  SmallVector<int64_t> bcast_dims;\n+-  bcast_dims.reserve(input_rank);\n+-\n +           << inputRank << \" vs \" << outputRank;\n +\n +  size_t rankDiff = outputRank - inputRank;\n-+  auto inputShapeOrFail = getDimensions(input);\n-+  if (failed(inputShapeOrFail)) return failure();\n-+  Dimensions inputShape = std::move(*inputShapeOrFail);\n-+\n-+  // Construct broadcast dimensions.\n-+  auto broadcastDimensions = llvm::to_vector(\n+   auto inputShapeOrFail = getDimensions(input);\n+   if (failed(inputShapeOrFail)) return failure();\n+   Dimensions inputShape = std::move(*inputShapeOrFail);\n+ \n+   // Construct broadcast dimensions.\n+   auto broadcastDimensions = llvm::to_vector(\n+-      llvm::seq<int64_t>(output_rank - input_rank, output_rank));\n +      llvm::seq<int64_t>(outputRank - inputRank, outputRank));\n-+\n-+  // Construct the result type of the broadcast\n-+  //  - If input is static and target shape is static, use static shape.\n-+  //  - If input has bounded dim, target shape must be bounded, use bounded dim.\n-+  //  - If input is not bounded, but target shape is bounded, broadcast to\n-+  //    the padded shape then call SetDimensionSize to make dynamic.\n-+  auto bcastShape = shape;\n-+  for (size_t i = 0; i < inputRank; ++i) {\n+ \n+   // Construct the result type of the broadcast\n+   //  - If input is static and target shape is static, use static shape.\n+@@ -244,33 +248,35 @@\n+   //  - If input is not bounded, but target shape is bounded, broadcast to\n+   //    the padded shape then call SetDimensionSize to make dynamic.\n+   auto bcastShape = shape;\n+-  for (int64_t i = 0; i < input_rank; ++i) {\n+-    int64_t input_dim_size = inputShape[i].size;\n+-    int64_t result_idx = i + rank_diff;\n+-    int64_t result_dim_size = shape[result_idx].size;\n+-    if (input_dim_size != 1 && input_dim_size != result_dim_size)\n++  for (int64_t i = 0; i < inputRank; ++i) {\n +    int64_t inputDimSize = inputShape[i].size;\n +    int64_t resultIdx = i + rankDiff;\n +    int64_t resultDimSize = shape[resultIdx].size;\n +    if (inputDimSize != 1 && inputDimSize != resultDimSize)\n-+      return emitError(loc, \"Cannot broadcast input: \")\n+       return emitError(loc, \"Cannot broadcast input: \")\n+-             << input_type << \" to target shape \" << toString(shape);\n +             << inputType << \" to target shape \" << toString(shape);\n-+\n-+    if (!inputShape[i].boundOp.has_value() &&\n+ \n+     if (!inputShape[i].boundOp.has_value() &&\n+-        shape[result_idx].boundOp.has_value()) {\n +        shape[resultIdx].boundOp.has_value()) {\n-+      // Use padded shape in broadcast.\n+       // Use padded shape in broadcast.\n+-      bcastShape[result_idx] = DimensionInfo{shape[result_idx].size};\n+-    }\n+-    bcast_dims.push_back(result_idx);\n +      bcastShape[resultIdx] = DimensionInfo{shape[resultIdx].size};\n +    }\n-+  }\n-+\n-+  // Broadcast to padded size for remaining dimensions.\n+   }\n+ \n+   // Broadcast to padded size for remaining dimensions.\n+-  for (size_t i = input_rank; i < shape.size(); ++i) {\n +  for (size_t i = 0; i < rankDiff; ++i) {\n-+    bcastShape[i] = DimensionInfo{shape[i].size};\n-+  }\n-+\n-+  // Insert broadcast ops\n+     bcastShape[i] = DimensionInfo{shape[i].size};\n+   }\n+ \n+   // Insert broadcast ops\n+-  mlir::RankedTensorType bcast_type =\n+-      getRankedTensorType(bcastShape, input_type.getElementType());\n+-  Value bcast_op = stablehlo::BroadcastInDimOp::create(\n+-      builder, loc, bcast_type, input, broadcastDimensions);\n+-  if (bcast_op.getType() == output_type) return bcast_op;\n +  mlir::RankedTensorType bcastType =\n +      getRankedTensorType(bcastShape, inputType.getElementType());\n +  LLVM_DEBUG(\n@@ -1194,239 +478,42 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +  Value bcastOp = stablehlo::BroadcastInDimOp::create(\n +      builder, loc, bcastType, input, broadcastDimensions);\n +  if (bcastOp.getType() == outputType) return bcastOp;\n-+\n-+  // Mark the padded broadcast as dynamic where the result is bounded.\n-+  // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n-+  // dimensions that required broadcasting.\n-+  for (size_t i = 0; i < shape.size(); ++i) {\n-+    if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n-+      Value boundOp = shape[i].boundOp.value();\n+ \n+   // Mark the padded broadcast as dynamic where the result is bounded.\n+   // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n+@@ -278,13 +284,13 @@\n+   for (size_t i = 0; i < shape.size(); ++i) {\n+     if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n+       Value boundOp = shape[i].boundOp.value();\n+-      auto dim_size = stablehlo::GetDimensionSizeOp::create(\n +      auto dimSize = stablehlo::GetDimensionSizeOp::create(\n-+          builder, loc, boundOp, shape[i].boundOpDim);\n+           builder, loc, boundOp, shape[i].boundOpDim);\n+-      bcast_op = stablehlo::SetDimensionSizeOp::create(builder, loc, bcast_op,\n+-                                                       dim_size, i);\n+-    }\n+-  }\n+-  return bcast_op;\n +      bcastOp = stablehlo::SetDimensionSizeOp::create(builder, loc, bcastOp,\n +                                                       dimSize, i);\n +    }\n +  }\n +  return bcastOp;\n-+}\n-+\n-+}  // namespace stablehlo\n-+}  // namespace mlir\n+ }\n+ \n+ }  // namespace stablehlo\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n-@@ -0,0 +1,69 @@\n-+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-+   Copyright 2022 The StableHLO Authors.\n-+\n-+Licensed under the Apache License, Version 2.0 (the \"License\");\n-+you may not use this file except in compliance with the License.\n-+You may obtain a copy of the License at\n-+\n-+    http://www.apache.org/licenses/LICENSE-2.0\n-+\n-+Unless required by applicable law or agreed to in writing, software\n-+distributed under the License is distributed on an \"AS IS\" BASIS,\n-+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-+See the License for the specific language governing permissions and\n-+limitations under the License.\n-+==============================================================================*/\n-+\n-+\n-+#ifndef STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n-+#define STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n-+\n-+#include <cstdint>\n-+#include <optional>\n-+#include <string>\n-+\n-+#include \"mlir/IR/Builders.h\"\n-+#include \"mlir/IR/Value.h\"\n-+#include \"mlir/Support/LLVM.h\"\n-+\n-+namespace mlir {\n-+namespace stablehlo {\n-+\n-+///////\n-+// Numpy broadcasting with support for bounded dynamism.\n-+\n-+// Struct that represents a dim size of a tensor and possible dynamic value to\n-+// match. If dimension is not dynamic, bound_op is set to std::nullopt. If\n-+// dimension is bounded, the resulting dimension should be padded to `size` then\n-+// marked dynamic using:\n-+//   runtime_size = get_dimension_size(bound_op, dim=bound_op_dim)\n-+//   T = set_dimension_size(T, dim=bound_op_dim, runtime_size)\n-+//\n-+struct DimensionInfo {\n-+  int64_t size;\n-+  std::optional<Value> boundOp = std::nullopt;\n-+  int64_t boundOpDim = -1;\n-+};\n-+\n-+using Dimensions = SmallVector<DimensionInfo>;\n-+std::string toString(const Dimensions& dims);\n-+\n-+// Returns the common shape these ops would broadcast to, or an error if the\n-+// ops are not broadcastable.\n+@@ -49,7 +49,8 @@\n+ \n+ // Returns the common shape these ops would broadcast to, or an error if the\n+ // ops are not broadcastable.\n+-FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops);\n +FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n +                                             ArrayRef<Value> ops);\n-+\n-+// Apply numpy broadcasting to the given operands, returning an error if any\n-+// operands are not broadcastable.\n-+FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n-+                                                     ArrayRef<Value> operands);\n-+\n-+// Apply numpy broadcasting to the given operand, returning an error if the\n-+// operand is not broadcastable.\n-+FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n-+                                        const Dimensions& shape);\n-+\n-+}  // namespace stablehlo\n-+}  // namespace mlir\n-+\n-+#endif  // STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -822,6 +822,61 @@\n-   int64_t foldOpElementLimit;\n- };\n  \n-+// Pattern: concat(splat_a, splat_a, X) -> concat(splat_a_resize, X)\n-+struct FoldConcatenateAdjacentSplatsOpPattern final\n-+    : ShapeOpRewritePattern<mlir::stablehlo::ConcatenateOp> {\n-+  using ShapeOpRewritePattern::ShapeOpRewritePattern;\n-+\n-+  LogicalResult matchAndRewrite(ConcatenateOp op,\n-+                                PatternRewriter& rewriter) const override {\n-+    SmallVector<Value> newOperands;\n-+    SplatElementsAttr currSplat;\n-+    for (size_t i = 0; i < op.getNumOperands(); ++i) {\n-+      Value operand = op.getOperand(i);\n-+      // Match a splat and look ahead for adjacent identical splats.\n-+      if (matchPattern(operand, m_Constant(&currSplat)) && currSplat) {\n-+        size_t j = i+1;\n-+        SplatElementsAttr lookaheadSplat;\n-+        int64_t nOccurrences = 1;\n-+        for (; j < op.getNumOperands(); ++j) {\n-+          if (matchPattern(op.getOperand(j), m_Constant(&lookaheadSplat)) &&\n-+              lookaheadSplat && lookaheadSplat == currSplat) {\n-+            ++nOccurrences;\n-+            continue;\n-+          }\n-+          break;\n-+        }\n-+\n-+        // Special case for a single occurrence, no new constants\n-+        if (nOccurrences == 1) {\n-+          newOperands.push_back(operand);\n-+          continue;\n-+        }\n-+\n-+        // Resize the splat and append it to the new operands.\n-+        SmallVector<int64_t> newShape =\n-+            llvm::to_vector(currSplat.getType().getShape());\n-+        newShape[op.getDimension()] *= nOccurrences;\n-+        newOperands.push_back(ConstantOp::create(\n-+            rewriter, op.getLoc(),\n-+            currSplat.resizeSplat(currSplat.getType().clone(newShape))));\n-+\n-+        // Set `i` to j-1 so that next iteration processes the next operand.\n-+        i = j - 1;\n-+        continue;\n-+      }\n-+      // Not splat, append the operand.\n-+      newOperands.push_back(operand);\n-+    }\n-+    if (newOperands.size() == op.getNumOperands()) {\n-+      return rewriter.notifyMatchFailure(op, \"No splats to fold\");\n-+    }\n-+    rewriter.replaceOpWithNewOp<ConcatenateOp>(op, op.getType(), newOperands,\n-+                                               op.getDimension());\n-+    return success();\n-+  }\n-+};\n-+\n- struct FoldConvertOpPattern : public ShapeOpRewritePattern<ConvertOp> {\n-   using ShapeOpRewritePattern::ShapeOpRewritePattern;\n- \n-@@ -1108,7 +1163,8 @@\n-                                 PatternRewriter& rewriter) const override {\n-     auto resultType = op.getType();\n-     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||\n--        failed(validateShapeFoldDtype(rewriter, op, resultType)))\n-+        failed(validateShapeFoldDtype(rewriter, op, resultType,\n-+                                      /*allowComplex=*/true)))\n-       return failure();\n- \n-     DenseElementsAttr attr;\n-@@ -1923,6 +1979,8 @@\n-   patterns->add<FoldClampOpPattern>(context, options, benefit);\n-   patterns->add<FoldCompareOpPattern>(context, options, benefit);\n-   patterns->add<FoldConcatenateOpPattern>(context, options, benefit);\n-+  patterns->add<FoldConcatenateAdjacentSplatsOpPattern>(context, options,\n-+                                                        benefit);\n-   patterns->add<FoldConvertOpPattern>(context, options, benefit);\n-   patterns->add<FoldDivOpPattern>(context, options, benefit);\n-   patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n-@@ -69,6 +69,54 @@\n-   });\n- }\n- \n-+bool mergeDiscardableAttributes(ValueRange sourceValues,\n-+                                ValueRange destValues) {\n-+  if (sourceValues.size() != destValues.size()) return false;\n-+  bool changed = false;\n-+  for (auto [source, dest] : llvm::zip(sourceValues, destValues)) {\n-+    if (mergeDiscardableAttributes(source, dest)) changed = true;\n-+  }\n-+  return changed;\n-+}\n-+\n-+bool mergeDiscardableAttributes(Value sourceValue, Value destValue) {\n-+  Operation* sourceOp = sourceValue.getDefiningOp();\n-+  Operation* destOp = destValue.getDefiningOp();\n-+  if (!sourceOp || !destOp) return false;\n-+\n-+  auto sourceAttrs = sourceOp->getDiscardableAttrDictionary();\n-+  if (!sourceAttrs) return true;\n-+\n-+  auto destAttrs = destOp->getDiscardableAttrDictionary();\n-+  if (!destAttrs) {\n-+    destOp->setDiscardableAttrs(sourceAttrs);\n-+    return true;\n-+  }\n-+\n-+  NamedAttrList mergedAttrs(destAttrs);\n-+  for (auto attr : sourceAttrs.getValue()) {\n-+    if (attr.getName() == \"mhlo.frontend_attributes\" &&\n-+        mergedAttrs.get(\"mhlo.frontend_attributes\")) {\n-+      // Merge frontend attributes, prioritizing source attributes.\n-+      auto destFrontendAttrs =\n-+          cast<DictionaryAttr>(mergedAttrs.get(\"mhlo.frontend_attributes\"));\n-+      auto sourceFrontendAttrs = cast<DictionaryAttr>(attr.getValue());\n-+      NamedAttrList frontendAttrs(destFrontendAttrs);\n-+      for (auto sourceAttr : sourceFrontendAttrs) {\n-+        frontendAttrs.set(sourceAttr.getName(), sourceAttr.getValue());\n-+      }\n-+      mergedAttrs.set(\"mhlo.frontend_attributes\",\n-+                      frontendAttrs.getDictionary(destOp->getContext()));\n-+    } else {\n-+      // Otherwise prioritize source attributes\n-+      mergedAttrs.set(attr.getName(), attr.getValue());\n-+    }\n-+  }\n-+\n-+  destOp->setDiscardableAttrs(mergedAttrs.getDictionary(destOp->getContext()));\n-+  return true;\n-+}\n-+\n- template <typename OpType>\n- struct SimplifyOpRewritePattern : OpRewritePattern<OpType> {\n-   SimplifyOpRewritePattern(\n+ // Apply numpy broadcasting to the given operands, returning an error if any\n+ // operands are not broadcastable.\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n@@ -1440,70 +527,83 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n            \"!(\"\n              \"llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && \"\n              \"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\"\n-@@ -134,6 +135,8 @@\n+@@ -134,8 +135,7 @@\n  \n  def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n  \n+-def MergeDiscardableAttributes\n+-    : NativeCodeCall<\"mergeDiscardableAttributes($0, $1)\">;\n +def MergeDiscardableAttributes : NativeCodeCall<\"mergeDiscardableAttributes($0, $1)\">;\n-+\n+ \n  def StableHLO_ConvertOpWithShape : NativeCodeCall<\n      \"stablehlo::ConvertOp::create($_builder, $_loc, $0.getType(), $1)\">;\n+@@ -151,10 +151,10 @@\n  \n-@@ -149,8 +152,9 @@\n  // op(cst, X) -> op(X, cst)\n  class CanonicalizeConstantToRhs<Op StableHLO_OpType>\n-   : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n--        (StableHLO_OpType $rhs, $lhs),\n--        [(NotConstantOp $rhs), (CommutativeOp $op)]>;\n+-    : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n+-          (StableHLO_OpType:$new_op $rhs, $lhs),\n+-          [(NotConstantOp $rhs), (CommutativeOp $op)],\n+-          [(MergeDiscardableAttributes $op, $new_op)]>;\n++  : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n +        (StableHLO_OpType:$new_op $rhs, $lhs),\n +        [(NotConstantOp $rhs), (CommutativeOp $op)],\n +        [(MergeDiscardableAttributes $op, $new_op)]>;\n  \n  ////////\n  // AddOp\n-@@ -161,8 +165,9 @@\n+@@ -165,9 +165,9 @@\n  \n  // Pattern: add(X, 0) -> X\n  def AddOp_RemoveNoop\n--  : Pat<(StableHLO_AddOp $lhs, (ConstantLikeMatcher AnyZero:$value)),\n--        (replaceWithValue $lhs)>;\n+-    : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),\n+-          (replaceWithValue $lhs), [],\n+-          [(MergeDiscardableAttributes $op, $lhs)]>;\n +  : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;\n  \n  ////////\n  // AndOp\n-@@ -173,13 +178,15 @@\n+@@ -177,25 +177,26 @@\n+   : CanonicalizeConstantToRhs<StableHLO_AndOp>;\n  \n  // Pattern: and(X, 0) -> 0\n- def AndOp_FoldToZero\n--  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n--        (replaceWithValue $zero)>;\n+-def AndOp_FoldToZero : Pat<(StableHLO_AndOp:$op $lhs,\n+-                               (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-                           (replaceWithValue $zero), [],\n+-                           [(MergeDiscardableAttributes $op, $zero)]>;\n++def AndOp_FoldToZero\n +  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n +        (replaceWithValue $zero), [],\n +        [(MergeDiscardableAttributes $op, $zero)]>;\n  \n  // Pattern: and(X, 1) -> X\n- def AndOp_RemoveNoop\n--  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n--        (replaceWithValue $lhs)>;\n+-def AndOp_RemoveNoop : Pat<(StableHLO_AndOp:$op $lhs,\n+-                               (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n+-                           (replaceWithValue $lhs), [],\n+-                           [(MergeDiscardableAttributes $op, $lhs)]>;\n++def AndOp_RemoveNoop\n +  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;\n  \n  ////////\n  // BroadcastInDimOp\n-@@ -188,7 +195,8 @@\n+ \n+ // Pattern: broadcast_in_dim(X, [iota...]) -> X\n  def BroadcastInDimOp_RemoveNoop\n-   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n-         (replaceWithValue $operand),\n--        [(TypesEqual $op, $operand)]>;\n+-    : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n+-          (replaceWithValue $operand), [(TypesEqual $op, $operand)],\n+-          [(MergeDiscardableAttributes $op, $operand)]>;\n++  : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n++        (replaceWithValue $operand),\n +        [(TypesEqual $op, $operand)],\n +        [(MergeDiscardableAttributes $op, $operand)]>;\n  \n  // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])\n  //       -> broadcast_in_dim(X, merge(dimsA, dimsB))\n-@@ -203,8 +211,10 @@\n+@@ -210,8 +211,10 @@\n  \n  // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])\n  //          [if same numel]\n@@ -1515,7 +615,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n          (StableHLO_ReshapeOpWithShape $op, $operand),\n          [(NumberOfElementsEqual $op, $operand)],\n          [],\n-@@ -213,7 +223,7 @@\n+@@ -220,7 +223,7 @@\n  // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])\n  //          [if same numel & rank]\n  def BroadcastInDimOp_ReplaceWithTranspose\n@@ -1524,62 +624,78 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n          (StableHLO_TransposeOp $operand, (InvertBroadcastDims $dims)),\n          [(NumberOfElementsEqual $op, $operand), (RankEqual $op, $operand)]>;\n  \n-@@ -254,7 +264,8 @@\n+@@ -259,9 +262,10 @@\n+ \n+ // Pattern: convert(X, [X.type]) -> X\n  def ConvertOp_RemoveNoop\n-   : Pat<(StableHLO_ConvertOp:$convert $operand),\n-         (replaceWithValue $operand),\n--        [(TypesEqual $convert, $operand)]>;\n+-    : Pat<(StableHLO_ConvertOp:$convert $operand),\n+-          (replaceWithValue $operand), [(TypesEqual $convert, $operand)],\n+-          [(MergeDiscardableAttributes $convert, $operand)]>;\n++  : Pat<(StableHLO_ConvertOp:$convert $operand),\n++        (replaceWithValue $operand),\n +        [(TypesEqual $convert, $operand)],\n +        [(MergeDiscardableAttributes $convert, $operand)]>;\n  \n  ////////\n  // DynamicBroadcastInDimOp\n-@@ -441,13 +452,15 @@\n+@@ -447,16 +451,16 @@\n+ //\n  // Multiplication by 0. This fold is not trivial for floats in presence of NaNs,\n  // so we currently only enable it for ints.\n- def MulOp_FoldToZero\n--  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n--        (replaceWithValue $zero)>;\n+-def MulOp_FoldToZero : Pat<(StableHLO_MulOp:$mul_op $lhs,\n+-                               (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-                           (replaceWithValue $zero), [],\n+-                           [(MergeDiscardableAttributes $mul_op, $zero)]>;\n++def MulOp_FoldToZero\n +  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n +        (replaceWithValue $zero), [],\n +        [(MergeDiscardableAttributes $mul_op, $zero)]>;\n  \n  // Pattern: multiply(X, 1i) -> X\n  def MulOp_RemoveNoop\n--  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n--        (replaceWithValue $lhs)>;\n+-    : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n+-          (replaceWithValue $lhs), [],\n+-          [(MergeDiscardableAttributes $mul_op, $lhs)]>;\n +  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $mul_op, $lhs)]>;\n  \n  ////////\n  // OrOp\n-@@ -457,13 +470,15 @@\n+@@ -465,16 +469,16 @@\n+ def OrOp_CanonicalizeConstantToRhs : CanonicalizeConstantToRhs<StableHLO_OrOp>;\n  \n  // Pattern: or(X, 1) -> 1\n- def OrOp_FoldToOne\n--  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n--        (replaceWithValue $one)>;\n+-def OrOp_FoldToOne : Pat<(StableHLO_OrOp:$op $lhs,\n+-                             (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n+-                         (replaceWithValue $one), [],\n+-                         [(MergeDiscardableAttributes $op, $one)]>;\n++def OrOp_FoldToOne\n +  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n +        (replaceWithValue $one), [],\n +        [(MergeDiscardableAttributes $op, $one)]>;\n  \n  // Pattern: or(X, 0) -> X\n- def OrOp_RemoveNoop\n--  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n--        (replaceWithValue $lhs)>;\n+-def OrOp_RemoveNoop : Pat<(StableHLO_OrOp:$op $lhs,\n+-                              (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-                          (replaceWithValue $lhs), [],\n+-                          [(MergeDiscardableAttributes $op, $lhs)]>;\n++def OrOp_RemoveNoop\n +  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;\n  \n  ////////\n  // PadOp\n-@@ -564,8 +579,9 @@\n+@@ -574,10 +578,10 @@\n+         (StableHLO_ConstantLike<\"0\"> $operand)>;\n  \n  // Pattern: subtract(X, 0) -> X\n- def SubtractOp_RemoveNoop\n--  : Pat<(StableHLO_SubtractOp $lhs, (StableHLO_ConstantOp AnyZero:$value)),\n--        (replaceWithValue $lhs)>;\n+-def SubtractOp_RemoveNoop : Pat<(StableHLO_SubtractOp:$op $lhs,\n+-                                    (StableHLO_ConstantOp AnyZero:$value)),\n+-                                (replaceWithValue $lhs), [],\n+-                                [(MergeDiscardableAttributes $op, $lhs)]>;\n++def SubtractOp_RemoveNoop\n +  : Pat<(StableHLO_SubtractOp:$op $lhs, (StableHLO_ConstantOp AnyZero:$value)),\n +        (replaceWithValue $lhs), [],\n +        [(MergeDiscardableAttributes $op, $lhs)]>;"
        },
        {
            "sha": "6012798b53e02ef962736f4d5a1a359494640a2e",
            "filename": "third_party/xla/third_party/stablehlo/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c6d564a12e429efb1babcc00d51415b6249f763a/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c6d564a12e429efb1babcc00d51415b6249f763a/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl?ref=c6d564a12e429efb1babcc00d51415b6249f763a",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n     # LINT.IfChange\n-    STABLEHLO_COMMIT = \"3f27c53c20b9021ccab8b5f673e2c72e5b9cd6aa\"\n-    STABLEHLO_SHA256 = \"915e05e79d9764c048557a929c64e090ab58a5c7334da2c2650cd6378aa4d166\"\n+    STABLEHLO_COMMIT = \"96acdcb7724f4a9eec6d2e5af2597b0750c13948\"\n+    STABLEHLO_SHA256 = \"68e068a78d71f0764d5dd385ef434df922050530de99001969493298a00d64a0\"\n     # LINT.ThenChange(Google-internal path)\n \n     tf_http_archive("
        }
    ],
    "stats": {
        "total": 1392,
        "additions": 254,
        "deletions": 1138
    }
}