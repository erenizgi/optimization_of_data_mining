{
    "author": "hawkinsp",
    "message": "[JAX] Split _jax extension into _xla and _jax extensions.\n\nThe goal is to move bindings for XLA types back to the XLA repository. Previously we wholesale moved everything JAX depends on to the JAX repository, including some things that are at least nominally the property of XLA. Now that we've done some work teasing apart the dependencies a bit better, we can move bindings for XLA types back to XLA.\n\nPiperOrigin-RevId: 839743369",
    "sha": "daaf047d90ab7fc50e6890a961e22efbdb517307",
    "files": [
        {
            "sha": "6b5611ea1eb0f88bebdbbe7d4437570fc643f138",
            "filename": "third_party/xla/xla/python/BUILD",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2FBUILD?ref=daaf047d90ab7fc50e6890a961e22efbdb517307",
            "patch": "@@ -102,8 +102,10 @@ cc_library(\n     deps = [\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@dlpack\",\n+        \"@nanobind\",\n     ],\n )\n \n@@ -259,6 +261,60 @@ cc_library(\n     alwayslink = 1,\n )\n \n+nanobind_pywrap_extension(\n+    name = \"_xla\",\n+    srcs = [\"xla.cc\"],\n+    additional_stubgen_deps = [\n+        \"//third_party/py/numpy\",\n+    ],\n+    enable_stub_generation = True,\n+    pytype_deps = [\"//third_party/py/numpy\"],\n+    pytype_srcs = [\"_xla.pyi\"],\n+    visibility = internal_visibility([\n+        \":jax\",\n+    ]),\n+    deps = [\n+        \":dlpack_types\",\n+        # placeholder for index annotation deps\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/hash\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@nanobind\",\n+        \"//xla:array\",\n+        \"//xla:debug_options_flags\",\n+        \"//xla:literal\",\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/client:executable_build_options\",\n+        \"//xla/hlo/builder:xla_computation\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n+        \"//xla/pjrt:exceptions\",\n+        \"//xla/pjrt:pjrt_executable\",\n+        \"//xla/pjrt:status_casters\",\n+        \"//xla/pjrt/proto:compile_options_proto_cc\",\n+        \"//xla/python:nb_absl_span\",\n+        \"//xla/python:nb_numpy\",\n+        \"//xla/python:types\",\n+        \"//xla/service:computation_placer\",\n+        \"//xla/service:hlo_graph_dumper\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/service:hlo_proto_cc\",\n+        \"//xla/service/spmd/shardy/stablehlo_round_trip:stablehlo_import\",\n+        \"//xla/tsl/lib/strings:proto_serialization\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:statusor\",\n+    ],\n+)\n+\n nanobind_pywrap_extension(\n     name = \"_hlo_pass\",\n     srcs = [\"hlo_pass.cc\"],"
        },
        {
            "sha": "41ee389d1e3f2665a8b75097b9a17cf2b31347aa",
            "filename": "third_party/xla/xla/python/_xla.pyi",
            "status": "added",
            "additions": 722,
            "deletions": 0,
            "changes": 722,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2F_xla.pyi",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2F_xla.pyi",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2F_xla.pyi?ref=daaf047d90ab7fc50e6890a961e22efbdb517307",
            "patch": "@@ -0,0 +1,722 @@\n+# Copyright 2025 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from collections.abc import Sequence\n+import enum\n+from typing import Annotated, TypeAlias, overload\n+import numpy\n+from numpy.typing import NDArray\n+\n+class PrimitiveType(enum.IntEnum):\n+  PRIMITIVE_TYPE_INVALID = 0\n+\n+  PRED = 1\n+\n+  S4 = 21\n+\n+  S8 = 2\n+\n+  S16 = 3\n+\n+  S32 = 4\n+\n+  S64 = 5\n+\n+  U4 = 22\n+\n+  U8 = 6\n+\n+  U16 = 7\n+\n+  U32 = 8\n+\n+  U64 = 9\n+\n+  F16 = 10\n+\n+  F4E2M1FN = 32\n+\n+  F8E3M4 = 29\n+\n+  F8E4M3 = 28\n+\n+  F8E4M3FN = 20\n+\n+  F8E4M3B11FNUZ = 23\n+\n+  F8E4M3FNUZ = 25\n+\n+  F8E5M2 = 19\n+\n+  F8E5M2FNUZ = 24\n+\n+  F8E8M0FNU = 33\n+\n+  BF16 = 16\n+\n+  F32 = 11\n+\n+  F64 = 12\n+\n+  C64 = 15\n+\n+  C128 = 18\n+\n+  TUPLE = 13\n+\n+  OPAQUE_TYPE = 14\n+\n+  TOKEN = 17\n+\n+class Layout:\n+  @overload\n+  def __init__(self, arg: Sequence[int], /) -> None: ...\n+  @overload\n+  def __init__(\n+      self, arg0: Sequence[int], arg1: Sequence[tuple[int, ...]], arg2: int, /\n+  ) -> None: ...\n+  def minor_to_major(self) -> tuple[int, ...]: ...\n+  def element_size_in_bits(self) -> int: ...\n+  def tiling(self) -> list[tuple[int, ...]]: ...\n+  def __eq__(self, other: object, /) -> bool: ...\n+  def __ne__(self, other: object, /) -> bool: ...\n+  def __str__(self) -> str: ...\n+  def __hash__(self) -> int: ...\n+  def to_string(self) -> str: ...\n+  def __getstate__(self) -> tuple: ...\n+  def __setstate__(self, arg: tuple, /) -> None: ...\n+\n+class Shape:\n+  def __init__(self, arg: str, /) -> None: ...\n+  @staticmethod\n+  def tuple_shape(arg: Sequence[Shape], /) -> Shape:\n+    \"\"\"Constructs a tuple shape.\"\"\"\n+\n+  @overload\n+  @staticmethod\n+  def array_shape(\n+      type: PrimitiveType,\n+      dims: Sequence[int],\n+      layout: Sequence[int] | None = ...,\n+      dynamic_dimensions: Sequence[bool] | None = ...,\n+  ) -> Shape:\n+    \"\"\"Constructs an array shape.\"\"\"\n+\n+  @overload\n+  @staticmethod\n+  def array_shape(\n+      type: numpy.dtype,\n+      dims: Sequence[int],\n+      layout: Sequence[int] | None = ...,\n+      dynamic_dimensions: Sequence[bool] | None = ...,\n+  ) -> Shape: ...\n+  @staticmethod\n+  def token_shape() -> Shape: ...\n+  @overload\n+  @staticmethod\n+  def scalar_shape(type: PrimitiveType) -> Shape:\n+    \"\"\"Constructs a scalar shape.\"\"\"\n+\n+  @overload\n+  @staticmethod\n+  def scalar_shape(type: numpy.dtype) -> Shape: ...\n+  def dimensions(self) -> tuple[int, ...]: ...\n+  def layout(self) -> Layout: ...\n+  def xla_element_type(self) -> PrimitiveType: ...\n+  def element_type(self) -> numpy.dtype: ...\n+  def numpy_dtype(self) -> numpy.dtype: ...\n+  def is_tuple(self) -> bool: ...\n+  def is_array(self) -> bool: ...\n+  def is_token(self) -> bool: ...\n+  def is_static(self) -> bool: ...\n+  def is_dynamic(self) -> bool: ...\n+  def is_dynamic_dimension(self, dimension: int) -> bool: ...\n+  def set_dynamic_dimension(self, dimension: int, is_dynamic: bool) -> None: ...\n+  def rank(self) -> int: ...\n+  def to_serialized_proto(self) -> bytes: ...\n+  def tuple_shapes(self) -> list[Shape]: ...\n+  def leaf_count(self) -> int: ...\n+  def with_major_to_minor_layout_if_absent(self) -> Shape:\n+    \"\"\"Returns a copy of a shape with missing layouts set to major-to-minor.\"\"\"\n+\n+  def __eq__(self, other: object, /) -> bool: ...\n+  def __ne__(self, other: object, /) -> bool: ...\n+  def __hash__(self) -> int: ...\n+  def __repr__(self) -> str: ...\n+\n+class ProgramShape:\n+  def __init__(self, arg0: Sequence[Shape], arg1: Shape, /) -> None: ...\n+  def parameter_shapes(self) -> list[Shape]: ...\n+  def result_shape(self) -> Shape: ...\n+  def __repr__(self) -> str: ...\n+\n+class Literal:\n+  def __init__(self, arg: Shape, /) -> None: ...\n+  def __repr__(self) -> str: ...\n+  def __array__(\n+      self, dtype: object | None = ..., copy: bool | None = ...\n+  ) -> NDArray: ...\n+  def shape(self) -> Shape: ...\n+\n+class XlaComputation:\n+  def __init__(self, arg: bytes, /) -> None: ...\n+  def get_hlo_module(self) -> HloModule: ...\n+  def program_shape(self) -> ProgramShape: ...\n+  def name(self) -> str: ...\n+  def as_serialized_hlo_module_proto(self) -> bytes: ...\n+  def as_hlo_text(self, print_large_constants: bool = ...) -> str: ...\n+  def as_hlo_dot_graph(self) -> str: ...\n+  def hash(self) -> int: ...\n+  def as_hlo_module(self) -> HloModule: ...\n+\n+class HloPrintOptions:\n+  def __init__(self) -> None: ...\n+  @staticmethod\n+  def short_parsable() -> HloPrintOptions: ...\n+  @staticmethod\n+  def canonical() -> HloPrintOptions: ...\n+  @staticmethod\n+  def fingerprint() -> HloPrintOptions: ...\n+  @property\n+  def print_large_constants(self) -> bool: ...\n+  @print_large_constants.setter\n+  def print_large_constants(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_metadata(self) -> bool: ...\n+  @print_metadata.setter\n+  def print_metadata(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_backend_config(self) -> bool: ...\n+  @print_backend_config.setter\n+  def print_backend_config(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_result_shape(self) -> bool: ...\n+  @print_result_shape.setter\n+  def print_result_shape(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_operand_shape(self) -> bool: ...\n+  @print_operand_shape.setter\n+  def print_operand_shape(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_operand_names(self) -> bool: ...\n+  @print_operand_names.setter\n+  def print_operand_names(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_ids(self) -> bool: ...\n+  @print_ids.setter\n+  def print_ids(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_extra_attributes(self) -> bool: ...\n+  @print_extra_attributes.setter\n+  def print_extra_attributes(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_program_shape(self) -> bool: ...\n+  @print_program_shape.setter\n+  def print_program_shape(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_percent(self) -> bool: ...\n+  @print_percent.setter\n+  def print_percent(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def print_control_dependencies(self) -> bool: ...\n+  @print_control_dependencies.setter\n+  def print_control_dependencies(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def compact_operands(self) -> bool: ...\n+  @compact_operands.setter\n+  def compact_operands(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def include_layout_in_shapes(self) -> bool: ...\n+  @include_layout_in_shapes.setter\n+  def include_layout_in_shapes(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def canonicalize_instruction_names(self) -> bool: ...\n+  @canonicalize_instruction_names.setter\n+  def canonicalize_instruction_names(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def canonicalize_computations(self) -> bool: ...\n+  @canonicalize_computations.setter\n+  def canonicalize_computations(self, arg: bool, /) -> HloPrintOptions: ...\n+  @property\n+  def indent_amount(self) -> int: ...\n+  @indent_amount.setter\n+  def indent_amount(self, arg: int, /) -> HloPrintOptions: ...\n+  @property\n+  def is_in_nested_computation(self) -> int: ...\n+  @is_in_nested_computation.setter\n+  def is_in_nested_computation(self, arg: bool, /) -> HloPrintOptions: ...\n+\n+class HloComputation:\n+  @property\n+  def name(self) -> str: ...\n+  def render_html(self, arg: str, /) -> None: ...\n+\n+class HloModule:\n+  @property\n+  def name(self) -> str: ...\n+  def to_string(self, options: HloPrintOptions = ...) -> str: ...\n+  def as_serialized_hlo_module_proto(self) -> bytes: ...\n+  def from_serialized_hlo_module_proto(self) -> HloModule: ...\n+  def computations(self) -> list[HloComputation]: ...\n+  @property\n+  def spmd_output_sharding(self) -> OpSharding | None: ...\n+  @property\n+  def spmd_parameters_shardings(self) -> list[OpSharding] | None: ...\n+\n+def hlo_module_to_dot_graph(arg: HloModule, /) -> str: ...\n+def hlo_module_from_text(arg: str, /) -> HloModule: ...\n+\n+class DeviceAssignment:\n+  @staticmethod\n+  def create(\n+      arg: Annotated[NDArray[numpy.int32], dict(shape=(None, None))], /\n+  ) -> DeviceAssignment: ...\n+  def replica_count(self) -> int: ...\n+  def computation_count(self) -> int: ...\n+  def __repr__(self) -> str: ...\n+  def serialize(self) -> bytes: ...\n+\n+class CompileOptions:\n+  def __init__(self) -> None: ...\n+  def __getstate__(self) -> tuple: ...\n+  def __setstate__(self, arg: tuple, /) -> None: ...\n+  def SerializeAsString(self) -> bytes: ...\n+  @staticmethod\n+  def ParseFromString(arg: bytes, /) -> CompileOptions: ...\n+  @property\n+  def argument_layouts(self) -> list[Shape] | None: ...\n+  @argument_layouts.setter\n+  def argument_layouts(self, arg: Sequence[Shape], /) -> None: ...\n+  @property\n+  def parameter_is_tupled_arguments(self) -> bool: ...\n+  @parameter_is_tupled_arguments.setter\n+  def parameter_is_tupled_arguments(self, arg: bool, /) -> None: ...\n+  @property\n+  def compile_portable_executable(self) -> bool: ...\n+  @compile_portable_executable.setter\n+  def compile_portable_executable(self, arg: bool, /) -> None: ...\n+  @property\n+  def executable_build_options(self) -> ExecutableBuildOptions: ...\n+  @property\n+  def env_option_overrides(\n+      self,\n+  ) -> list[tuple[str, str | bool | int | float]]: ...\n+  @env_option_overrides.setter\n+  def env_option_overrides(\n+      self, arg: Sequence[tuple[str, str | bool | int | float]], /\n+  ) -> None: ...\n+  @property\n+  def num_replicas(self) -> int: ...\n+  @num_replicas.setter\n+  def num_replicas(self, arg: int, /) -> None: ...\n+  @property\n+  def num_partitions(self) -> int: ...\n+  @num_partitions.setter\n+  def num_partitions(self, arg: int, /) -> None: ...\n+  @property\n+  def profile_version(self) -> int: ...\n+  @profile_version.setter\n+  def profile_version(self, arg: int, /) -> None: ...\n+  @property\n+  def device_assignment(self) -> DeviceAssignment | None: ...\n+  @device_assignment.setter\n+  def device_assignment(self, arg: DeviceAssignment, /) -> None: ...\n+\n+class AutotuneCacheMode(enum.Enum):\n+  UNSPECIFIED = 0\n+\n+  UPDATE = 1\n+\n+  READ = 2\n+\n+class DebugOptions:\n+  def __repr__(self) -> str: ...\n+  @property\n+  def xla_backend_optimization_level(self) -> int: ...\n+  @xla_backend_optimization_level.setter\n+  def xla_backend_optimization_level(self, arg: int, /) -> None: ...\n+  @property\n+  def xla_cpu_enable_fast_math(self) -> bool: ...\n+  @xla_cpu_enable_fast_math.setter\n+  def xla_cpu_enable_fast_math(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_cpu_enable_xprof_traceme(self) -> bool: ...\n+  @xla_cpu_enable_xprof_traceme.setter\n+  def xla_cpu_enable_xprof_traceme(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_cpu_fast_math_honor_infs(self) -> bool: ...\n+  @xla_cpu_fast_math_honor_infs.setter\n+  def xla_cpu_fast_math_honor_infs(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_cpu_fast_math_honor_nans(self) -> bool: ...\n+  @xla_cpu_fast_math_honor_nans.setter\n+  def xla_cpu_fast_math_honor_nans(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_cpu_fast_math_honor_division(self) -> bool: ...\n+  @xla_cpu_fast_math_honor_division.setter\n+  def xla_cpu_fast_math_honor_division(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_cpu_fast_math_honor_functions(self) -> bool: ...\n+  @xla_cpu_fast_math_honor_functions.setter\n+  def xla_cpu_fast_math_honor_functions(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_detailed_logging(self) -> bool: ...\n+  @xla_detailed_logging.setter\n+  def xla_detailed_logging(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_enable_dumping(self) -> bool: ...\n+  @xla_enable_dumping.setter\n+  def xla_enable_dumping(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_gpu_enable_fast_min_max(self) -> bool: ...\n+  @xla_gpu_enable_fast_min_max.setter\n+  def xla_gpu_enable_fast_min_max(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_gpu_dump_autotune_results_to(self) -> str: ...\n+  @xla_gpu_dump_autotune_results_to.setter\n+  def xla_gpu_dump_autotune_results_to(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_gpu_load_autotune_results_from(self) -> str: ...\n+  @xla_gpu_load_autotune_results_from.setter\n+  def xla_gpu_load_autotune_results_from(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_gpu_cuda_data_dir(self) -> str: ...\n+  @xla_gpu_cuda_data_dir.setter\n+  def xla_gpu_cuda_data_dir(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_llvm_disable_expensive_passes(self) -> bool: ...\n+  @xla_llvm_disable_expensive_passes.setter\n+  def xla_llvm_disable_expensive_passes(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_disable_hlo_passes(self) -> str: ...\n+  @xla_disable_hlo_passes.setter\n+  def xla_disable_hlo_passes(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_enable_hlo_passes_only(self) -> str: ...\n+  @xla_enable_hlo_passes_only.setter\n+  def xla_enable_hlo_passes_only(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_test_all_input_layouts(self) -> bool: ...\n+  @xla_test_all_input_layouts.setter\n+  def xla_test_all_input_layouts(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_force_host_platform_device_count(self) -> int: ...\n+  @xla_force_host_platform_device_count.setter\n+  def xla_force_host_platform_device_count(self, arg: int, /) -> None: ...\n+  @property\n+  def xla_dump_to(self) -> str: ...\n+  @xla_dump_to.setter\n+  def xla_dump_to(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_module_re(self) -> str: ...\n+  @xla_dump_hlo_module_re.setter\n+  def xla_dump_hlo_module_re(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_pass_re(self) -> str: ...\n+  @xla_dump_hlo_pass_re.setter\n+  def xla_dump_hlo_pass_re(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_as_text(self) -> bool: ...\n+  @xla_dump_hlo_as_text.setter\n+  def xla_dump_hlo_as_text(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_as_proto(self) -> bool: ...\n+  @xla_dump_hlo_as_proto.setter\n+  def xla_dump_hlo_as_proto(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_as_dot(self) -> bool: ...\n+  @xla_dump_hlo_as_dot.setter\n+  def xla_dump_hlo_as_dot(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_as_url(self) -> bool: ...\n+  @xla_dump_hlo_as_url.setter\n+  def xla_dump_hlo_as_url(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_as_html(self) -> bool: ...\n+  @xla_dump_hlo_as_html.setter\n+  def xla_dump_hlo_as_html(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_fusion_visualization(self) -> bool: ...\n+  @xla_dump_fusion_visualization.setter\n+  def xla_dump_fusion_visualization(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_snapshots(self) -> bool: ...\n+  @xla_dump_hlo_snapshots.setter\n+  def xla_dump_hlo_snapshots(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_max_hlo_modules(self) -> int: ...\n+  @xla_dump_max_hlo_modules.setter\n+  def xla_dump_max_hlo_modules(self, arg: int, /) -> None: ...\n+  @property\n+  def xla_dump_module_metadata(self) -> bool: ...\n+  @xla_dump_module_metadata.setter\n+  def xla_dump_module_metadata(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_compress_protos(self) -> bool: ...\n+  @xla_dump_compress_protos.setter\n+  def xla_dump_compress_protos(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_as_long_text(self) -> bool: ...\n+  @xla_dump_hlo_as_long_text.setter\n+  def xla_dump_hlo_as_long_text(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_disable_metadata(self) -> bool: ...\n+  @xla_dump_disable_metadata.setter\n+  def xla_dump_disable_metadata(self, arg: bool, /) -> None: ...\n+  @property\n+  def xla_dump_hlo_pipeline_re(self) -> str: ...\n+  @xla_dump_hlo_pipeline_re.setter\n+  def xla_dump_hlo_pipeline_re(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_gpu_dump_autotune_logs_to(self) -> str: ...\n+  @xla_gpu_dump_autotune_logs_to.setter\n+  def xla_gpu_dump_autotune_logs_to(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_gpu_kernel_cache_file(self) -> str: ...\n+  @xla_gpu_kernel_cache_file.setter\n+  def xla_gpu_kernel_cache_file(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_gpu_enable_llvm_module_compilation_parallelism(self) -> bool: ...\n+  @xla_gpu_enable_llvm_module_compilation_parallelism.setter\n+  def xla_gpu_enable_llvm_module_compilation_parallelism(\n+      self, arg: bool, /\n+  ) -> None: ...\n+  @property\n+  def xla_gpu_per_fusion_autotune_cache_dir(self) -> str: ...\n+  @xla_gpu_per_fusion_autotune_cache_dir.setter\n+  def xla_gpu_per_fusion_autotune_cache_dir(self, arg: str, /) -> None: ...\n+  @property\n+  def xla_gpu_experimental_autotune_cache_mode(self) -> AutotuneCacheMode: ...\n+  @xla_gpu_experimental_autotune_cache_mode.setter\n+  def xla_gpu_experimental_autotune_cache_mode(\n+      self, arg: AutotuneCacheMode, /\n+  ) -> None: ...\n+\n+class ExecutableBuildOptions:\n+  def __init__(self) -> None: ...\n+  def __repr__(self) -> str: ...\n+  @property\n+  def fdo_profile(self) -> bytes: ...\n+  @fdo_profile.setter\n+  def fdo_profile(self, arg: bytes, /) -> None: ...\n+  @property\n+  def result_layout(self) -> Shape | None: ...\n+  @result_layout.setter\n+  def result_layout(self, arg: Shape, /) -> ExecutableBuildOptions: ...\n+  @property\n+  def num_replicas(self) -> int: ...\n+  @num_replicas.setter\n+  def num_replicas(self, arg: int, /) -> ExecutableBuildOptions: ...\n+  @property\n+  def num_partitions(self) -> int: ...\n+  @num_partitions.setter\n+  def num_partitions(self, arg: int, /) -> ExecutableBuildOptions: ...\n+  @property\n+  def debug_options(self) -> DebugOptions: ...\n+  @property\n+  def device_assignment(self) -> DeviceAssignment | None: ...\n+  @device_assignment.setter\n+  def device_assignment(\n+      self, arg: DeviceAssignment, /\n+  ) -> ExecutableBuildOptions: ...\n+  def compilation_environments_from_serialized_proto(\n+      self, arg: bytes, /\n+  ) -> None: ...\n+  @property\n+  def exec_time_optimization_effort(self) -> float: ...\n+  @exec_time_optimization_effort.setter\n+  def exec_time_optimization_effort(\n+      self, arg: float, /\n+  ) -> ExecutableBuildOptions: ...\n+  @property\n+  def memory_fitting_effort(self) -> float: ...\n+  @memory_fitting_effort.setter\n+  def memory_fitting_effort(self, arg: float, /) -> ExecutableBuildOptions: ...\n+  @property\n+  def optimization_level(self) -> int: ...\n+  @optimization_level.setter\n+  def optimization_level(self, arg: int, /) -> None: ...\n+  @property\n+  def memory_fitting_level(self) -> int: ...\n+  @memory_fitting_level.setter\n+  def memory_fitting_level(self, arg: int, /) -> None: ...\n+  @property\n+  def use_spmd_partitioning(self) -> bool: ...\n+  @use_spmd_partitioning.setter\n+  def use_spmd_partitioning(self, arg: bool, /) -> ExecutableBuildOptions: ...\n+  @property\n+  def use_auto_spmd_partitioning(self) -> bool: ...\n+  @use_auto_spmd_partitioning.setter\n+  def use_auto_spmd_partitioning(\n+      self, arg: bool, /\n+  ) -> ExecutableBuildOptions: ...\n+  @property\n+  def auto_spmd_partitioning_mesh_shape(self) -> list[int]: ...\n+  @auto_spmd_partitioning_mesh_shape.setter\n+  def auto_spmd_partitioning_mesh_shape(\n+      self, arg: Sequence[int], /\n+  ) -> ExecutableBuildOptions: ...\n+  @property\n+  def auto_spmd_partitioning_mesh_ids(self) -> list[int]: ...\n+  @auto_spmd_partitioning_mesh_ids.setter\n+  def auto_spmd_partitioning_mesh_ids(\n+      self, arg: Sequence[int], /\n+  ) -> ExecutableBuildOptions: ...\n+  @property\n+  def allow_spmd_sharding_propagation_to_parameters(self) -> list[bool]: ...\n+  @allow_spmd_sharding_propagation_to_parameters.setter\n+  def allow_spmd_sharding_propagation_to_parameters(\n+      self, arg: Sequence[bool], /\n+  ) -> None: ...\n+  @property\n+  def allow_spmd_sharding_propagation_to_output(self) -> list[bool]: ...\n+  @allow_spmd_sharding_propagation_to_output.setter\n+  def allow_spmd_sharding_propagation_to_output(\n+      self, arg: Sequence[bool], /\n+  ) -> None: ...\n+  @property\n+  def use_shardy_partitioner(self) -> bool: ...\n+  @use_shardy_partitioner.setter\n+  def use_shardy_partitioner(self, arg: bool, /) -> ExecutableBuildOptions: ...\n+\n+class OpSharding_Type(enum.IntEnum):\n+  REPLICATED = 0\n+\n+  MAXIMAL = 1\n+\n+  MANUAL = 4\n+\n+  UNREDUCED = 6\n+\n+  TUPLE = 2\n+\n+  OTHER = 3\n+\n+  UNKNOWN = 5\n+\n+class OpSharding_ShardGroupType(enum.Enum):\n+  AS = 0\n+\n+  LIKE = 1\n+\n+class OpSharding:\n+  def __init__(self) -> None: ...\n+\n+  Type: TypeAlias = OpSharding_Type\n+\n+  ShardGroupType: TypeAlias = OpSharding_ShardGroupType\n+\n+  def __getstate__(self) -> tuple: ...\n+  def __setstate__(self, arg: tuple, /) -> None: ...\n+  @property\n+  def type(self) -> OpSharding_Type: ...\n+  @type.setter\n+  def type(self, arg: OpSharding_Type, /) -> None: ...\n+  @property\n+  def replicate_on_last_tile_dim(self) -> bool: ...\n+  @replicate_on_last_tile_dim.setter\n+  def replicate_on_last_tile_dim(self, arg: bool, /) -> None: ...\n+  @property\n+  def is_shard_group(self) -> bool: ...\n+  @is_shard_group.setter\n+  def is_shard_group(self, arg: bool, /) -> None: ...\n+  @property\n+  def shard_group_id(self) -> int: ...\n+  @shard_group_id.setter\n+  def shard_group_id(self, arg: int, /) -> None: ...\n+  @property\n+  def shard_group_type(self) -> OpSharding_ShardGroupType: ...\n+  @shard_group_type.setter\n+  def shard_group_type(self, arg: OpSharding_ShardGroupType, /) -> None: ...\n+  def __repr__(self) -> str: ...\n+  def ParseFromString(self, arg: bytes, /) -> None: ...\n+  def SerializeToString(self) -> bytes: ...\n+  def clone(self) -> OpSharding: ...\n+  @property\n+  def tile_assignment_dimensions(self) -> list[int]: ...\n+  @tile_assignment_dimensions.setter\n+  def tile_assignment_dimensions(self, arg: Sequence[int], /) -> None: ...\n+  @property\n+  def tile_assignment_devices(self) -> list[int]: ...\n+  @tile_assignment_devices.setter\n+  def tile_assignment_devices(self, arg: Sequence[int], /) -> None: ...\n+  @property\n+  def iota_reshape_dims(self) -> list[int]: ...\n+  @iota_reshape_dims.setter\n+  def iota_reshape_dims(self, arg: Sequence[int], /) -> None: ...\n+  @property\n+  def iota_transpose_perm(self) -> list[int]: ...\n+  @iota_transpose_perm.setter\n+  def iota_transpose_perm(self, arg: Sequence[int], /) -> None: ...\n+  @property\n+  def tuple_shardings(self) -> list[OpSharding]: ...\n+  @tuple_shardings.setter\n+  def tuple_shardings(self, arg: Sequence[OpSharding], /) -> None: ...\n+  @property\n+  def last_tile_dims(self) -> list[int]: ...\n+  @last_tile_dims.setter\n+  def last_tile_dims(self, arg: Sequence[int], /) -> None: ...\n+\n+class HloSharding:\n+  @staticmethod\n+  def from_proto(arg: OpSharding, /) -> HloSharding: ...\n+  @staticmethod\n+  def from_string(arg: str, /) -> HloSharding: ...\n+  @staticmethod\n+  def tuple_sharding(\n+      arg0: Shape, arg1: Sequence[HloSharding], /\n+  ) -> HloSharding:\n+    \"\"\"Constructs a tuple sharding.\"\"\"\n+\n+  @staticmethod\n+  def iota_tile(\n+      dims: Sequence[int],\n+      reshape_dims: Sequence[int] = ...,\n+      transpose_perm: Sequence[int] = ...,\n+      subgroup_types: Sequence[OpSharding_Type] = ...,\n+  ) -> HloSharding: ...\n+  @staticmethod\n+  def manual() -> HloSharding: ...\n+  @staticmethod\n+  def replicate() -> HloSharding: ...\n+  @staticmethod\n+  def unreduced() -> HloSharding: ...\n+  @staticmethod\n+  def unknown() -> HloSharding: ...\n+  @staticmethod\n+  def subgroup_with_device_ordering(\n+      tile_assignment: Annotated[NDArray[numpy.int64], dict(order='C')],\n+      subgroup_types: Sequence[OpSharding_Type] = ...,\n+  ) -> HloSharding: ...\n+  def __eq__(self, other: object, /) -> bool: ...\n+  def __ne__(self, other: object, /) -> bool: ...\n+  def __hash__(self) -> int: ...\n+  def is_replicated(self) -> bool: ...\n+  def is_manual(self) -> bool: ...\n+  def is_unreduced(self) -> bool: ...\n+  def is_unknown(self) -> bool: ...\n+  def is_tiled(self) -> bool: ...\n+  def is_maximal(self) -> bool: ...\n+  def tile(self, arg: Shape, /) -> Shape: ...\n+  def tuple_elements(self) -> list[HloSharding]: ...\n+  def num_devices(self) -> int: ...\n+  def num_dimensions(self) -> int: ...\n+  def is_tile_assignment_iota(self) -> bool: ...\n+  def tile_assignment_dimensions(self) -> Sequence[int]: ...\n+  def tile_assignment_devices(self) -> Sequence[int]: ...\n+  def replicate_on_last_tile_dim(self) -> bool: ...\n+  def subgroup_types(self) -> list[OpSharding_Type]: ...\n+  def __repr__(self) -> str: ...\n+  def to_proto(self) -> OpSharding: ...\n+  def get_axis_sizes(self) -> list[int]: ..."
        },
        {
            "sha": "592db1110acf94c88665ef47dc172d925130bc4c",
            "filename": "third_party/xla/xla/python/dlpack_types.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fdlpack_types.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fdlpack_types.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fdlpack_types.cc?ref=daaf047d90ab7fc50e6890a961e22efbdb517307",
            "patch": "@@ -17,6 +17,8 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"include/dlpack/dlpack.h\"\n+#include \"nanobind/ndarray.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -220,4 +222,16 @@ absl::StatusOr<PrimitiveType> DLDataTypeToPrimitiveType(DLDataType type) {\n   }\n }\n \n+absl::StatusOr<nanobind::dlpack::dtype> PrimitiveTypeToNbDLDataType(\n+    xla::PrimitiveType type) {\n+  TF_ASSIGN_OR_RETURN(DLDataType dl_type, PrimitiveTypeToDLDataType(type));\n+\n+  nanobind::dlpack::dtype nb_type;\n+  nb_type.lanes = dl_type.lanes;\n+  nb_type.bits = dl_type.bits;\n+  nb_type.code = dl_type.code;\n+\n+  return nb_type;\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "0c412c05e834e831daf0381ff8aa8de90e98b917",
            "filename": "third_party/xla/xla/python/dlpack_types.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fdlpack_types.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fdlpack_types.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fdlpack_types.h?ref=daaf047d90ab7fc50e6890a961e22efbdb517307",
            "patch": "@@ -18,13 +18,19 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"include/dlpack/dlpack.h\"\n+#include \"nanobind/ndarray.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n \n absl::StatusOr<DLDataType> PrimitiveTypeToDLDataType(PrimitiveType type);\n absl::StatusOr<PrimitiveType> DLDataTypeToPrimitiveType(DLDataType type);\n \n+// Converts a PrimitiveType to the nanobind specific implementation of\n+// DLDataType.\n+absl::StatusOr<nanobind::dlpack::dtype> PrimitiveTypeToNbDLDataType(\n+    xla::PrimitiveType type);\n+\n }  // namespace xla\n \n #endif  // XLA_PYTHON_DLPACK_TYPES_H_"
        },
        {
            "sha": "19193d9bb9e4b67086bf6e461a207288bda7fe4d",
            "filename": "third_party/xla/xla/python/pywrap.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fpywrap.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fpywrap.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpywrap.bzl?ref=daaf047d90ab7fc50e6890a961e22efbdb517307",
            "patch": "@@ -34,6 +34,8 @@ def nanobind_pywrap_extension(\n         deps = [],\n         pytype_srcs = [],\n         pytype_deps = [],  # @unused\n+        additional_stubgen_deps = [],  # @unused\n+        enable_stub_generation = False,  # @unused\n         copts = [],\n         linkopts = [],\n         visibility = None):"
        },
        {
            "sha": "0a6ed76173d19a2ca8849dca1dfa0e5a8359f3fc",
            "filename": "third_party/xla/xla/python/version.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h?ref=daaf047d90ab7fc50e6890a961e22efbdb517307",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n \n // An increasing version number to protect jax code against breaking changes.\n // In JAX, reference this via jax._src.lib.ifrt_version.\n-#define JAX_IFRT_VERSION_NUMBER \\\n-  37  // `LoadedExecutable::devices()` returns nullopt for portable executables.\n+#define JAX_IFRT_VERSION_NUMBER 38  // _xla Python extension module added\n \n #endif  // XLA_PYTHON_VERSION_H_"
        },
        {
            "sha": "2aa9ad05d42d9854d8592e0a21bb6e1fed090399",
            "filename": "third_party/xla/xla/python/xla.cc",
            "status": "added",
            "additions": 1361,
            "deletions": 0,
            "changes": 1361,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fxla.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/daaf047d90ab7fc50e6890a961e22efbdb517307/third_party%2Fxla%2Fxla%2Fpython%2Fxla.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fxla.cc?ref=daaf047d90ab7fc50e6890a961e22efbdb517307",
            "patch": "@@ -0,0 +1,1361 @@\n+/* Copyright 2020 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/hash/hash.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/strings/str_split.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"nanobind/nanobind.h\"\n+#include \"nanobind/ndarray.h\"\n+#include \"nanobind/stl/optional.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/pair.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/shared_ptr.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/string.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/string_view.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/variant.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/vector.h\"  // IWYU pragma: keep\n+#include \"xla/array.h\"\n+#include \"xla/client/executable_build_options.h\"\n+#include \"xla/debug_options_flags.h\"\n+#include \"xla/hlo/builder/xla_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/hlo/parser/hlo_parser.h\"\n+#include \"xla/layout.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/pjrt/exceptions.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n+#include \"xla/pjrt/proto/compile_options.pb.h\"\n+#include \"xla/pjrt/status_casters.h\"\n+#include \"xla/python/dlpack_types.h\"\n+#include \"xla/python/nb_absl_span.h\"  // IWYU pragma: keep\n+#include \"xla/python/nb_numpy.h\"\n+#include \"xla/python/types.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/service/hlo.pb.h\"\n+#include \"xla/service/hlo_graph_dumper.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_import.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/lib/strings/proto_serialization.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/logging.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace {\n+\n+namespace nb = nanobind;\n+\n+// Converts a computation to a serialized HloModuleProto.\n+absl::StatusOr<nb::bytes> GetComputationSerializedProto(\n+    const XlaComputation& computation) {\n+  std::string result;\n+  if (!tsl::SerializeToStringDeterministic(computation.proto(), &result)) {\n+    return Unknown(\"Failed to serialize the HloModuleProto.\");\n+  }\n+  return nb::bytes(result.data(), result.size());\n+}\n+\n+// Converts a hlo module to a serialized HloModuleProto.\n+absl::StatusOr<nb::bytes> GetHloModuleSerializedProto(const HloModule& module) {\n+  std::string result;\n+  if (!tsl::SerializeToStringDeterministic(module.ToProto(), &result)) {\n+    return Unknown(\"Failed to serialize the HloModuleProto.\");\n+  }\n+  return nb::bytes(result.data(), result.size());\n+}\n+\n+// Converts a serialized HloModuleProto into a HloModule.\n+absl::StatusOr<std::shared_ptr<HloModule>> HloModuleFromSerializedProto(\n+    const nb::bytes& bytes) {\n+  HloModuleProto proto;\n+  proto.ParseFromArray(bytes.c_str(), bytes.size());\n+  TF_ASSIGN_OR_RETURN(const HloModuleConfig module_config,\n+                      HloModule::CreateModuleConfigFromProto(\n+                          proto, GetDebugOptionsFromFlags()));\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n+                      HloModule::CreateFromProto(proto, module_config));\n+  return std::shared_ptr<HloModule>(std::move(module));\n+}\n+\n+absl::StatusOr<std::shared_ptr<HloModule>> GetHloModule(\n+    const XlaComputation& computation) {\n+  TF_ASSIGN_OR_RETURN(const HloModuleConfig module_config,\n+                      HloModule::CreateModuleConfigFromProto(\n+                          computation.proto(), GetDebugOptionsFromFlags()));\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<HloModule> module,\n+      HloModule::CreateFromProto(computation.proto(), module_config));\n+  return std::shared_ptr<HloModule>(std::move(module));\n+}\n+\n+// Converts a computation to textual HLO form.\n+absl::StatusOr<std::string> GetComputationHloText(\n+    const XlaComputation& computation, bool print_large_constants = false) {\n+  TF_ASSIGN_OR_RETURN(std::shared_ptr<HloModule> hlo_module,\n+                      GetHloModule(computation));\n+  HloPrintOptions options;\n+  options = HloPrintOptions::ShortParsable();\n+  options.set_print_large_constants(print_large_constants);\n+  return hlo_module->ToString(options);\n+}\n+\n+// Converts a computation to HLO dot graph form.\n+absl::StatusOr<std::string> GetComputationHloDotGraph(\n+    const XlaComputation& computation) {\n+  TF_ASSIGN_OR_RETURN(std::shared_ptr<HloModule> hlo_module,\n+                      GetHloModule(computation));\n+  return RenderGraph(*hlo_module->entry_computation(), /*label=*/\"\",\n+                     hlo_module->config().debug_options(),\n+                     RenderedGraphFormat::kDot);\n+}\n+\n+// Hashes the HLO module.\n+absl::StatusOr<uint64_t> HashComputation(const XlaComputation& computation) {\n+  TF_ASSIGN_OR_RETURN(std::shared_ptr<HloModule> hlo_module,\n+                      GetHloModule(computation));\n+  return absl::HashOf(*hlo_module);\n+}\n+// Safe version of ShapeUtil::MakeShapeWithDenseLayout that fails gracefully on\n+// invalid input.\n+absl::StatusOr<Shape> MakeShapeWithDenseLayout(\n+    PrimitiveType element_type, absl::Span<const int64_t> dims,\n+    std::optional<absl::Span<const int64_t>> minor_to_major,\n+    std::optional<const std::vector<bool>> dynamic_dimensions) {\n+  Shape shape;\n+  if (dynamic_dimensions) {\n+    TF_ASSIGN_OR_RETURN(\n+        shape, ShapeUtil::MakeValidatedShape(element_type, dims,\n+                                             dynamic_dimensions.value()));\n+  } else {\n+    TF_ASSIGN_OR_RETURN(shape,\n+                        ShapeUtil::MakeValidatedShape(element_type, dims));\n+  }\n+  if (minor_to_major) {\n+    *shape.mutable_layout() = LayoutUtil::MakeLayout(*minor_to_major);\n+    TF_RETURN_IF_ERROR(\n+        LayoutUtil::ValidateLayoutForShape(shape.layout(), shape));\n+  }\n+\n+  return shape;\n+}\n+\n+// Pybind function for HloSharding.iota_tile, which is a non-crashing factory\n+// that produces a HloSharding instance backed by tile assignment of a\n+// transposed and reshaped iota array of device ids. More specifically the tile\n+// assignment array is as if it is produced by the following numpy code:\n+// numpy.arange(math.prod(dims)).reshape(reshape_dims)\n+//      .transpose(transpose_perm).reshape(math.prod(dims))\n+// where:\n+// `dims`: is the dimensions of the tile assignment array, which corresponds to\n+//   OpSharding.tile_assignment_dimensions.\n+// `reshape_dims`: is the dimensions the 1D iota array is reshaped to.\n+// `transpose_perm`: is the dimension permutation to transpose `reshape_dims`.\n+// `subgroup_types`: indicates the subgroups of the last `subgroup_types.size()`\n+//   dimensions in `dims`.\n+//\n+// In practice, `reshape_dims` often maps to the axes of user defined device\n+// mesh, and `transpose_perm` often maps to the user specification of how a\n+// tensor is partitioned based on the axes defined in the mesh, e.g. for a mesh\n+// of size 4x2x2 as AxBxC:\n+// PartitionSpec('A', 'B', 'C') corresponds to reshape_dims=[4,2,2],\n+// transpose_perm=[0,1,2] (no transpose)\n+// PartitionSpec('B', 'A', 'C') corresponds to reshape_dims=[4,2,2],\n+// transpose_perm=[1,0,2] (swap A and B)\n+absl::StatusOr<HloSharding> IotaTileHelper(\n+    absl::Span<const int64_t> dims, absl::Span<const int64_t> reshape_dims,\n+    absl::Span<const int> transpose_perm,\n+    absl::Span<const OpSharding::Type> subgroup_types) {\n+  if (dims.empty()) {\n+    return InvalidArgument(\"`dims` should not be empty.\");\n+  }\n+  if (reshape_dims.size() != transpose_perm.size()) {\n+    return InvalidArgument(\n+        \"`reshape_dims` and `transpose_perm` should have the same size, saw \"\n+        \"[%s] v.s. [%s]\",\n+        absl::StrJoin(reshape_dims, \",\"), absl::StrJoin(transpose_perm, \",\"));\n+  }\n+  if (!reshape_dims.empty() && Product(dims) != Product(reshape_dims)) {\n+    return InvalidArgument(\n+        \"Cannot reshape from `dims` [%s] to `reshape_dims` [%s].\",\n+        absl::StrJoin(dims, \",\"), absl::StrJoin(reshape_dims, \",\"));\n+  }\n+  if (subgroup_types.size() > dims.size()) {\n+    return InvalidArgument(\n+        \"`subgroup_types`(%lld) should not have more dimensions than \"\n+        \"`dims`(%lld).\",\n+        subgroup_types.size(), dims.size());\n+  }\n+  if (reshape_dims.empty()) {\n+    return subgroup_types.empty()\n+               ? HloSharding::IotaTile(dims)\n+               : HloSharding::Subgroup(TileAssignment(dims), subgroup_types);\n+  }\n+  return subgroup_types.empty()\n+             ? HloSharding::IotaTile(dims, reshape_dims, transpose_perm)\n+             : HloSharding::Subgroup(\n+                   TileAssignment(dims, reshape_dims, transpose_perm),\n+                   subgroup_types);\n+}\n+\n+template <typename T, typename Container>\n+void DefRepeatedProperty(nb::class_<T>& cls, const char* name,\n+                         Container* (T::*getter)()) {\n+  cls.def_prop_rw(\n+      name,\n+      [getter](T& obj) {\n+        Container* elems = (obj.*getter)();\n+        std::vector<typename Container::value_type> result;\n+        result.reserve(elems->size());\n+        std::copy(elems->begin(), elems->end(), std::back_inserter(result));\n+        return result;\n+      },\n+      [getter](T& obj, std::vector<typename Container::value_type> new_elems) {\n+        Container* elems = (obj.*getter)();\n+        elems->Clear();\n+        elems->Reserve(new_elems.size());\n+        for (typename Container::value_type& e : new_elems) {\n+          elems->Add(std::move(e));\n+        }\n+      });\n+}\n+\n+template <typename T, typename Container>\n+void DefRepeatedEnumProperty(nb::class_<T>& cls, const char* name,\n+                             Container* (T::*getter)()) {\n+  cls.def_prop_rw(\n+      name,\n+      [getter](T& obj) {\n+        Container* elems = (obj.*getter)();\n+        std::vector<typename Container::value_type> result;\n+        result.reserve(elems->size());\n+        std::copy(elems->begin(), elems->end(), std::back_inserter(result));\n+        return result;\n+      },\n+      [getter](\n+          T& obj,\n+          nb::typed<nb::sequence, typename Container::value_type> new_elems) {\n+        Container* elems = (obj.*getter)();\n+        elems->Clear();\n+        for (nb::handle e : new_elems) {\n+          elems->Add(nb::cast<int>(e.attr(\"value\")));\n+        }\n+      });\n+}\n+\n+template <typename T>\n+Array<T> NDArrayToArray(nb::ndarray<T, nb::c_contig> ndarray) {\n+  std::vector<int64_t> shapes;\n+  shapes.reserve(ndarray.ndim());\n+  for (int i = 0; i < ndarray.ndim(); ++i) {\n+    shapes.push_back(ndarray.shape(i));\n+  }\n+  xla::Array<int64_t> array(shapes);\n+  array.Each([&](absl::Span<const int64_t> indices, int64_t* val) {\n+    int64_t offset = indices.back();\n+    int64_t multiplier = 1;\n+    for (int i = ndarray.ndim() - 1; i > 0; --i) {\n+      multiplier *= ndarray.shape(i);\n+      offset += indices[i - 1] * multiplier;\n+    }\n+    *val = *(ndarray.data() + offset);\n+  });\n+  return array;\n+}\n+\n+absl::StatusOr<HloSharding> SubgroupWithTileAssignmentHelper(\n+    nb::ndarray<int64_t, nb::c_contig> tile_assignment,\n+    absl::Span<const OpSharding::Type> subgroup_types) {\n+  return HloSharding::Subgroup(NDArrayToArray(tile_assignment), subgroup_types);\n+}\n+\n+nb::ndarray<> LiteralToNdarray(Literal& obj) {\n+  const Shape& shape = obj.shape();\n+\n+  if (!shape.has_layout()) {\n+    throw XlaRuntimeError(\n+        \"Creating an array is only supported for Literals with a layout.\");\n+  }\n+\n+  const Layout& layout = shape.layout();\n+\n+  if (!layout.tiles().empty()) {\n+    throw XlaRuntimeError(\n+        \"Creating an array from a tiled Literal is not supported.\");\n+  }\n+\n+  if (!shape.IsArray()) {\n+    throw XlaRuntimeError(\n+        \"Creating an array is only supported for dense Literals.\");\n+  }\n+\n+  xla::PrimitiveType primitive_type = shape.element_type();\n+  nb::dlpack::dtype dtype =\n+      ValueOrThrow(PrimitiveTypeToNbDLDataType(primitive_type));\n+\n+  absl::Span<const int64_t> dimensions = shape.dimensions();\n+  std::vector<size_t> unsigned_dimensions(dimensions.begin(), dimensions.end());\n+  auto strides = StridesForShape(primitive_type, dimensions, layout);\n+\n+  return nb::ndarray<>(obj.untyped_data(), unsigned_dimensions.size(),\n+                       unsigned_dimensions.data(), {}, strides.data(), dtype,\n+                       nb::device::cpu::value, 0);\n+}\n+\n+struct Descriptor {};\n+\n+}  // namespace\n+\n+NB_MODULE(_xla, m) {\n+  // Types\n+  nb::enum_<PrimitiveType>(m, \"PrimitiveType\", nb::is_arithmetic())\n+      .value(\"PRIMITIVE_TYPE_INVALID\", PRIMITIVE_TYPE_INVALID)\n+      .value(\"PRED\", PRED)\n+      .value(\"S4\", S4)\n+      .value(\"S8\", S8)\n+      .value(\"S16\", S16)\n+      .value(\"S32\", S32)\n+      .value(\"S64\", S64)\n+      .value(\"U4\", U4)\n+      .value(\"U8\", U8)\n+      .value(\"U16\", U16)\n+      .value(\"U32\", U32)\n+      .value(\"U64\", U64)\n+      .value(\"F16\", F16)\n+      .value(\"F4E2M1FN\", F4E2M1FN)\n+      .value(\"F8E3M4\", F8E3M4)\n+      .value(\"F8E4M3\", F8E4M3)\n+      .value(\"F8E4M3FN\", F8E4M3FN)\n+      .value(\"F8E4M3B11FNUZ\", F8E4M3B11FNUZ)\n+      .value(\"F8E4M3FNUZ\", F8E4M3FNUZ)\n+      .value(\"F8E5M2\", F8E5M2)\n+      .value(\"F8E5M2FNUZ\", F8E5M2FNUZ)\n+      .value(\"F8E8M0FNU\", F8E8M0FNU)\n+      .value(\"BF16\", BF16)\n+      .value(\"F32\", F32)\n+      .value(\"F64\", F64)\n+      .value(\"C64\", C64)\n+      .value(\"C128\", C128)\n+      .value(\"TUPLE\", TUPLE)\n+      .value(\"OPAQUE_TYPE\", OPAQUE_TYPE)\n+      .value(\"TOKEN\", TOKEN);\n+\n+  // Shapes\n+  nb::class_<Layout> layout_class(m, \"Layout\");\n+  layout_class.def(nb::init<absl::Span<const int64_t>>())\n+      .def(\"__init__\",\n+           [](Layout* self, nb::typed<nb::sequence, int> minor_to_major,\n+              nb::typed<nb::sequence, nb::typed<nb::tuple, int, nb::ellipsis>>\n+                  tiling,\n+              int64_t element_size_in_bits) {\n+             std::vector<Tile> xla_tiles;\n+             xla_tiles.reserve(nb::len(tiling.ptr()));\n+             for (auto tile : tiling) {\n+               xla_tiles.push_back(Tile(\n+                   SequenceToVector<int64_t>(nb::cast<nb::sequence>(tile))));\n+             }\n+             std::vector<int64_t> xla_minor_to_major =\n+                 SequenceToVector<int64_t>(minor_to_major);\n+             new (self)\n+                 Layout(xla_minor_to_major, xla_tiles, element_size_in_bits);\n+           })\n+      .def(\"minor_to_major\",\n+           [](Layout layout) { return SpanToNbTuple(layout.minor_to_major()); })\n+      .def(\"element_size_in_bits\", &Layout::element_size_in_bits)\n+      .def(\"tiling\",\n+           [](Layout layout) {\n+             std::vector<nb::typed<nb::tuple, int, nb::ellipsis>> result;\n+             result.reserve(layout.tiles().size());\n+             for (auto& t : layout.tiles()) {\n+               result.push_back(SpanToNbTuple(t.dimensions()));\n+             }\n+             return result;\n+           })\n+      .def(\n+          \"__eq__\",\n+          [](const Layout& layout, const Layout& other) {\n+            return layout == other;\n+          },\n+          nb::is_operator(),\n+          nb::sig(\"def __eq__(self, other: object, /) -> bool\"))\n+      .def(\n+          \"__ne__\",\n+          [](const Layout& layout, const Layout& other) {\n+            return layout != other;\n+          },\n+          nb::is_operator(),\n+          nb::sig(\"def __ne__(self, other: object, /) -> bool\"))\n+      .def(\"__str__\", &Layout::ToString)\n+      .def(\"__hash__\",\n+           [](const Layout& layout) { return absl::HashOf(layout); })\n+      .def(\"to_string\", &Layout::ToString)\n+      .def(\"__getstate__\",\n+           [](const Layout& self) -> nb::tuple {\n+             auto proto = self.ToProto();\n+             std::string result;\n+             if (!tsl::SerializeToStringDeterministic(proto, &result)) {\n+               // throw converted by PyBind to a Python RuntimeError.\n+               throw XlaRuntimeError(\n+                   absl::StrCat(\"Layout.py_pickle: \",\n+                                \"SerializeToStringDeterministic failed\"));\n+             }\n+             return nb::make_tuple(nb::bytes(result.data(), result.size()));\n+           })\n+      .def(\"__setstate__\", [](Layout* self, nb::tuple t) {\n+        LayoutProto result;\n+        nb::bytes serialized = nb::cast<nb::bytes>(t[0]);\n+        result.ParseFromArray(serialized.c_str(), serialized.size());\n+        new (self) Layout(ValueOrThrow(Layout::FromProto(result)));\n+      });\n+\n+  nb::class_<Shape> shape_class(m, \"Shape\");\n+  shape_class\n+      .def(\"__init__\",\n+           [](Shape* self, const std::string& s) {\n+             new (self) Shape(ValueOrThrow(ParseShape(s)));\n+           })\n+      .def_static(\n+          \"tuple_shape\",\n+          [](std::vector<Shape> shapes) -> Shape {\n+            return ShapeUtil::MakeTupleShape(shapes);\n+          },\n+          \"Constructs a tuple shape.\")\n+      .def_static(\n+          \"array_shape\",\n+          xla::ValueOrThrowWrapper(\n+              [](PrimitiveType type, nb::typed<nb::sequence, int> dims_seq,\n+                 std::optional<nb::typed<nb::sequence, int>> layout_seq,\n+                 std::optional<std::vector<bool>> dynamic_dimensions)\n+                  -> absl::StatusOr<Shape> {\n+                std::vector<int64_t> dims = SequenceToVector<int64_t>(dims_seq);\n+                if (layout_seq) {\n+                  std::vector<int64_t> layout =\n+                      SequenceToVector<int64_t>(*layout_seq);\n+                  return MakeShapeWithDenseLayout(type, dims, layout,\n+                                                  dynamic_dimensions);\n+                } else {\n+                  return MakeShapeWithDenseLayout(type, dims, std::nullopt,\n+                                                  dynamic_dimensions);\n+                }\n+              }),\n+          \"Constructs an array shape.\", nb::arg(\"type\"), nb::arg(\"dims\"),\n+          nb::arg(\"layout\").none() = std::nullopt,\n+          nb::arg(\"dynamic_dimensions\").none() = std::nullopt)\n+      .def_static(\n+          \"array_shape\",\n+          xla::ValueOrThrowWrapper(\n+              [](nb_dtype dtype, nb::typed<nb::sequence, int> dims_seq,\n+                 std::optional<nb::typed<nb::sequence, int>> layout_seq,\n+                 std::optional<std::vector<bool>> dynamic_dimensions)\n+                  -> absl::StatusOr<Shape> {\n+                PrimitiveType type = ValueOrThrow(DtypeToPrimitiveType(dtype));\n+                std::vector<int64_t> dims = SequenceToVector<int64_t>(dims_seq);\n+                if (layout_seq) {\n+                  std::vector<int64_t> layout =\n+                      SequenceToVector<int64_t>(*layout_seq);\n+                  return MakeShapeWithDenseLayout(type, dims, layout,\n+                                                  dynamic_dimensions);\n+                } else {\n+                  return MakeShapeWithDenseLayout(type, dims, std::nullopt,\n+                                                  dynamic_dimensions);\n+                }\n+              }),\n+          \"Constructs an array shape.\", nb::arg(\"type\"), nb::arg(\"dims\"),\n+          nb::arg(\"layout\").none() = std::nullopt,\n+          nb::arg(\"dynamic_dimensions\").none() = std::nullopt)\n+      .def_static(\"token_shape\", []() { return ShapeUtil::MakeTokenShape(); })\n+      .def_static(\n+          \"scalar_shape\",\n+          [](PrimitiveType type) -> Shape {\n+            return ShapeUtil::MakeScalarShape(type);\n+          },\n+          \"Constructs a scalar shape.\", nb::arg(\"type\"))\n+      .def_static(\n+          \"scalar_shape\",\n+          [](nb_dtype dtype) -> Shape {\n+            PrimitiveType type = xla::ValueOrThrow(DtypeToPrimitiveType(dtype));\n+            return ShapeUtil::MakeScalarShape(type);\n+          },\n+          \"Constructs a scalar shape.\", nb::arg(\"type\"))\n+      .def(\"dimensions\",\n+           [](const Shape& shape) { return SpanToNbTuple(shape.dimensions()); })\n+      .def(\"layout\",\n+           [](const Shape& shape) -> Layout { return shape.layout(); })\n+      .def(\"xla_element_type\", &Shape::element_type)\n+      .def(\"element_type\",\n+           [](const Shape& shape) {\n+             return xla::ValueOrThrow(\n+                 PrimitiveTypeToNbDtype(shape.element_type()));\n+           })\n+      .def(\"numpy_dtype\",\n+           [](const Shape& shape) {\n+             if (shape.IsTuple()) {\n+               return nb_dtype(\"O\");\n+             }\n+             return xla::ValueOrThrow(\n+                 PrimitiveTypeToNbDtype(shape.element_type()));\n+           })\n+      .def(\"is_tuple\", &Shape::IsTuple)\n+      .def(\"is_array\", &Shape::IsArray)\n+      .def(\"is_token\", &Shape::IsToken)\n+      .def(\"is_static\", &Shape::is_static)\n+      .def(\"is_dynamic\", &Shape::is_dynamic)\n+      .def(\"is_dynamic_dimension\", &Shape::is_dynamic_dimension,\n+           nb::arg(\"dimension\"))\n+      .def(\"set_dynamic_dimension\", &Shape::set_dynamic_dimension,\n+           nb::arg(\"dimension\"), nb::arg(\"is_dynamic\"))\n+      .def(\"rank\", &Shape::dimensions_size)\n+      .def(\"to_serialized_proto\",\n+           [](const Shape& shape) {\n+             ShapeProto proto = shape.ToProto();\n+             std::string s = proto.SerializeAsString();\n+             return nb::bytes(s.data(), s.size());\n+           })\n+      .def(\"tuple_shapes\",\n+           [](const Shape& shape) {\n+             return std::vector<Shape>(shape.tuple_shapes());\n+           })\n+      .def(\"leaf_count\",\n+           [](const Shape& shape) { return ShapeUtil::GetLeafCount(shape); })\n+      .def(\n+          \"with_major_to_minor_layout_if_absent\",\n+          [](const Shape& shape) {\n+            Shape out = shape;\n+            ShapeUtil::ForEachMutableSubshape(\n+                &out, [](Shape* subshape, const ShapeIndex&) {\n+                  if (!subshape->has_layout()) {\n+                    LayoutUtil::SetToDefaultLayout(subshape);\n+                  }\n+                });\n+            return out;\n+          },\n+          \"Returns a copy of a shape with missing layouts set to \"\n+          \"major-to-minor.\")\n+      .def(\n+          \"__eq__\",\n+          [](const Shape& shape, const Shape& other) { return shape == other; },\n+          nb::is_operator(),\n+          nb::sig(\"def __eq__(self, other: object, /) -> bool\"))\n+      .def(\n+          \"__ne__\",\n+          [](const Shape& shape, const Shape& other) { return shape != other; },\n+          nb::is_operator(),\n+          nb::sig(\"def __ne__(self, other: object, /) -> bool\"))\n+      .def(\"__hash__\", [](const Shape& shape) { return absl::HashOf(shape); })\n+      .def(\"__repr__\", [](const Shape& shape) {\n+        return shape.ToString(/*print_layout=*/true);\n+      });\n+\n+  nb::class_<ProgramShape>(m, \"ProgramShape\")\n+      .def(\n+          \"__init__\",\n+          [](ProgramShape* self, absl::Span<const Shape> params, Shape result) {\n+            new (self) ProgramShape();\n+            for (const Shape& param : params) {\n+              self->AddParameter(param, \"\");\n+            }\n+            *self->mutable_result() = result;\n+          })\n+      .def(\"parameter_shapes\",\n+           static_cast<const std::vector<Shape>& (ProgramShape::*)() const>(\n+               &ProgramShape::parameters))\n+      .def(\"result_shape\", &ProgramShape::result)\n+      .def(\"__repr__\", &ProgramShape::ToString);\n+\n+  // Literals\n+  nb::class_<Literal>(m, \"Literal\")\n+      .def(nb::init<const Shape&>())\n+      .def(\"__repr__\", &Literal::ToString)\n+      .def(\n+          \"__array__\",\n+          [](std::shared_ptr<Literal> obj, std::optional<nb::object> dtype,\n+             std::optional<bool> copy) {\n+            // Provides the interface required by numpy to create a np.ndarray.\n+            // Currently don't support the __dl_pack__ interface but can be\n+            // added with very little effort it if needed.\n+\n+            nb::ndarray<nb::numpy> np_array(LiteralToNdarray(*obj));\n+\n+            if (dtype.has_value()) {\n+              throw XlaRuntimeError(\n+                  \"Passing of dtype to __array__ not currently supported.\");\n+            }\n+\n+            if (copy.has_value() && *copy) {\n+              // when a copy is requested we _must_ return a copy:\n+              // https://numpy.org/doc/2.1/reference/generated/numpy.ndarray.__array__.html\n+              return np_array.cast(nb::rv_policy::copy);\n+            }\n+\n+            return np_array.cast(nb::rv_policy::reference_internal,\n+                                 nb::cast(obj));\n+          },\n+          nb::arg(\"dtype\").none() = nb::none(),\n+          nb::arg(\"copy\").none() = nb::none())\n+      .def(\"shape\", &Literal::shape);\n+\n+  nb::class_<XlaComputation>(m, \"XlaComputation\")\n+      .def(\"__init__\",\n+           [](XlaComputation* self,\n+              const nb::bytes& serialized_hlo_module_proto) {\n+             HloModuleProto proto;\n+             proto.ParseFromArray(serialized_hlo_module_proto.c_str(),\n+                                  serialized_hlo_module_proto.size());\n+             new (self) XlaComputation(proto);\n+           })\n+      .def(\"get_hlo_module\", xla::ValueOrThrowWrapper(GetHloModule))\n+      .def(\"program_shape\",\n+           xla::ValueOrThrowWrapper(&XlaComputation::GetProgramShape))\n+      .def(\"name\", &XlaComputation::name)\n+      .def(\"as_serialized_hlo_module_proto\",\n+           xla::ValueOrThrowWrapper(GetComputationSerializedProto))\n+      .def(\"as_hlo_text\", xla::ValueOrThrowWrapper(GetComputationHloText),\n+           nb::arg(\"print_large_constants\") = false)\n+      .def(\"as_hlo_dot_graph\",\n+           xla::ValueOrThrowWrapper(GetComputationHloDotGraph))\n+      .def(\"hash\", xla::ValueOrThrowWrapper(HashComputation))\n+      .def(\"as_hlo_module\", xla::ValueOrThrowWrapper(GetHloModule));\n+\n+  nb::class_<HloPrintOptions> hlo_print_options_class(m, \"HloPrintOptions\");\n+  hlo_print_options_class.def(nb::init<>())\n+      .def_static(\"short_parsable\", &HloPrintOptions::ShortParsable)\n+      .def_static(\"canonical\", &HloPrintOptions::Canonical)\n+      .def_static(\"fingerprint\", &HloPrintOptions::Fingerprint)\n+      .def_prop_rw(\"print_large_constants\",\n+                   &HloPrintOptions::print_large_constants,\n+                   &HloPrintOptions::set_print_large_constants)\n+      .def_prop_rw(\"print_metadata\", &HloPrintOptions::print_metadata,\n+                   &HloPrintOptions::set_print_metadata)\n+      .def_prop_rw(\"print_backend_config\",\n+                   &HloPrintOptions::print_backend_config,\n+                   &HloPrintOptions::set_print_backend_config)\n+      .def_prop_rw(\"print_result_shape\", &HloPrintOptions::print_result_shape,\n+                   &HloPrintOptions::set_print_result_shape)\n+      .def_prop_rw(\"print_operand_shape\", &HloPrintOptions::print_operand_shape,\n+                   &HloPrintOptions::set_print_operand_shape)\n+      .def_prop_rw(\"print_operand_names\", &HloPrintOptions::print_operand_names,\n+                   &HloPrintOptions::set_print_operand_names)\n+      .def_prop_rw(\"print_ids\", &HloPrintOptions::print_ids,\n+                   &HloPrintOptions::set_print_ids)\n+      .def_prop_rw(\"print_extra_attributes\",\n+                   &HloPrintOptions::print_extra_attributes,\n+                   &HloPrintOptions::set_print_extra_attributes)\n+      .def_prop_rw(\"print_program_shape\", &HloPrintOptions::print_program_shape,\n+                   &HloPrintOptions::set_print_program_shape)\n+      .def_prop_rw(\"print_percent\", &HloPrintOptions::print_percent,\n+                   &HloPrintOptions::set_print_percent)\n+      .def_prop_rw(\"print_control_dependencies\",\n+                   &HloPrintOptions::print_control_dependencies,\n+                   &HloPrintOptions::set_print_control_dependencies)\n+      .def_prop_rw(\"compact_operands\", &HloPrintOptions::compact_operands,\n+                   &HloPrintOptions::set_compact_operands)\n+      .def_prop_rw(\"include_layout_in_shapes\",\n+                   &HloPrintOptions::include_layout_in_shapes,\n+                   &HloPrintOptions::set_include_layout_in_shapes)\n+      .def_prop_rw(\"canonicalize_instruction_names\",\n+                   &HloPrintOptions::canonicalize_instruction_names,\n+                   &HloPrintOptions::set_canonicalize_instruction_names)\n+      .def_prop_rw(\"canonicalize_computations\",\n+                   &HloPrintOptions::canonicalize_computations,\n+                   &HloPrintOptions::set_canonicalize_computations)\n+      .def_prop_rw(\"indent_amount\", &HloPrintOptions::indent_amount,\n+                   &HloPrintOptions::set_indent_amount)\n+      .def_prop_rw(\"is_in_nested_computation\",\n+                   &HloPrintOptions::is_in_nested_computation,\n+                   &HloPrintOptions::set_is_in_nested_computation);\n+\n+  // HloModule.computations() returns raw pointers.\n+  // pybind seems to prefer smart pointers.\n+  // We give pybind a smart pointer to a wrapper around a raw pointer to satisfy\n+  // pybind and avoid double frees.\n+  class ComputationWrapper {\n+   public:\n+    ComputationWrapper(const HloComputation* comp,\n+                       const std::shared_ptr<HloModule> module)\n+        : comp_(comp), module_(module) {}\n+    absl::string_view name() const { return comp_->name(); }\n+    void render_html(const std::string& filename) {\n+      std::string html = xla::ValueOrThrow(RenderGraph(\n+          *comp_, /*label=*/\"\", comp_->parent()->config().debug_options(),\n+          RenderedGraphFormat::kHtml, HloRenderOptions()));\n+      xla::ThrowIfError(tsl::WriteStringToFile(\n+          tsl::Env::Default(), absl::StrCat(filename, \".html\"), html));\n+    }\n+\n+   private:\n+    const HloComputation* comp_;\n+    // The module owns the computations: if its destructor is called, the\n+    // computations are freed. To prevent that from happening in cases where the\n+    // module Python object goes out of scope and gets garbage collected before\n+    // the computations, we keep a shared_ptr to the module that originated the\n+    // computation.\n+    const std::shared_ptr<HloModule> module_;\n+  };\n+\n+  nb::class_<ComputationWrapper> hlo_computation_class(m, \"HloComputation\");\n+\n+  hlo_computation_class.def_prop_ro(\"name\", &ComputationWrapper::name)\n+      .def(\"render_html\", &ComputationWrapper::render_html);\n+\n+  nb::class_<HloModule> hlo_module_class(m, \"HloModule\");\n+  hlo_module_class.def_prop_ro(\"name\", &HloModule::name)\n+      .def(\n+          \"to_string\",\n+          static_cast<std::string (HloModule::*)(const HloPrintOptions&) const>(\n+              &HloModule::ToString),\n+          nb::arg(\"options\") = HloPrintOptions())\n+      .def(\"as_serialized_hlo_module_proto\",\n+           xla::ValueOrThrowWrapper(GetHloModuleSerializedProto))\n+      .def(\"from_serialized_hlo_module_proto\",\n+           xla::ValueOrThrowWrapper(HloModuleFromSerializedProto))\n+      .def(\"computations\",\n+           [](const std::shared_ptr<HloModule> m)\n+               -> std::vector<std::shared_ptr<ComputationWrapper>> {\n+             std::vector<std::shared_ptr<ComputationWrapper>> computations;\n+             for (HloComputation* comp : m->computations())\n+               computations.push_back(\n+                   std::make_shared<ComputationWrapper>(comp, m));\n+             return computations;\n+           })\n+      .def_prop_ro(\"spmd_output_sharding\",\n+                   [](const HloModule& m) -> std::optional<xla::OpSharding> {\n+                     if (!m.has_spmd_output_sharding()) return std::nullopt;\n+                     return m.spmd_output_sharding().ToProto();\n+                   })\n+      .def_prop_ro(\"spmd_parameters_shardings\",\n+                   [](const HloModule& m)\n+                       -> std::optional<std::vector<xla::OpSharding>> {\n+                     if (!m.has_spmd_parameters_shardings())\n+                       return std::nullopt;\n+                     std::vector<xla::OpSharding> param_shardings;\n+                     for (const auto& parameter_sharding :\n+                          m.spmd_parameters_shardings()) {\n+                       param_shardings.push_back(parameter_sharding.ToProto());\n+                     }\n+                     return param_shardings;\n+                   });\n+\n+  m.def(\"hlo_module_to_dot_graph\",\n+        [](const HloModule& hlo_module) -> std::string {\n+          return xla::ValueOrThrow(RenderGraph(\n+              *hlo_module.entry_computation(), /*label=*/\"\",\n+              hlo_module.config().debug_options(), RenderedGraphFormat::kDot));\n+        });\n+  m.def(\"hlo_module_from_text\",\n+        xla::ValueOrThrowWrapper(\n+            [](const std::string& hlo_module_text)\n+                -> absl::StatusOr<std::shared_ptr<HloModule>> {\n+              auto hlo_module =\n+                  xla::ParseAndReturnUnverifiedModule(hlo_module_text);\n+              TF_RETURN_IF_ERROR(hlo_module.status());\n+              std::shared_ptr<HloModule> result(std::move(*hlo_module));\n+              return result;\n+            }));\n+\n+  // Device assignments\n+  nb::class_<DeviceAssignment>(m, \"DeviceAssignment\")\n+      .def_static(\n+          \"create\",\n+          xla::ValueOrThrowWrapper([](nb::ndarray<int, nb::ndim<2>> array)\n+                                       -> absl::StatusOr<DeviceAssignment> {\n+            if (array.ndim() != 2) {\n+              return InvalidArgument(\n+                  \"Argument to DeviceAssignment constructor must be a \"\n+                  \"2D array, received an %dD array.\",\n+                  array.ndim());\n+            }\n+            DeviceAssignment result(array.shape(0), array.shape(1));\n+            for (int i = 0; i < array.shape(0); ++i) {\n+              for (int j = 0; j < array.shape(1); ++j) {\n+                result(i, j) = array(i, j);\n+              }\n+            }\n+            return result;\n+          }))\n+      .def(\"replica_count\", &DeviceAssignment::replica_count)\n+      .def(\"computation_count\", &DeviceAssignment::computation_count)\n+      .def(\"__repr__\", &DeviceAssignment::ToString)\n+      .def(\"serialize\",\n+           xla::ValueOrThrowWrapper(\n+               [](const DeviceAssignment& da) -> absl::StatusOr<nb::bytes> {\n+                 DeviceAssignmentProto proto;\n+                 da.Serialize(&proto);\n+                 std::string result;\n+                 if (!tsl::SerializeToStringDeterministic(proto, &result)) {\n+                   return Unknown(\n+                       \"Failed to serialize the DeviceAssignmentProto.\");\n+                 }\n+                 return nb::bytes(result.data(), result.size());\n+               }));\n+\n+  nb::class_<CompileOptions> compile_options(m, \"CompileOptions\");\n+  compile_options\n+      .def(\"__init__\",\n+           [](CompileOptions* self) {\n+             new (self) CompileOptions();\n+             DebugOptions* debug_options =\n+                 self->executable_build_options.mutable_debug_options();\n+             // Sets fast-math-disabling default options expected by JAX.\n+             debug_options->set_xla_cpu_enable_fast_min_max(false);\n+             debug_options->set_xla_gpu_enable_fast_min_max(false);\n+           })\n+      .def(\"__getstate__\",\n+           [](const CompileOptions& self) -> nb::tuple {\n+             auto proto = ValueOrThrow(self.ToProto());\n+             std::string result;\n+             if (!tsl::SerializeToStringDeterministic(proto, &result)) {\n+               // throw converted by PyBind to a Python RuntimeError.\n+               throw XlaRuntimeError(\n+                   absl::StrCat(\"CompileOptions.py_pickle: \",\n+                                \"SerializeToStringDeterministic failed\"));\n+             }\n+             return nb::make_tuple(nb::bytes(result.data(), result.size()));\n+           })\n+      .def(\"__setstate__\",\n+           [](CompileOptions* self, nb::tuple t) {\n+             CompileOptionsProto result;\n+             nb::bytes serialized = nb::cast<nb::bytes>(t[0]);\n+             result.ParseFromArray(serialized.c_str(), serialized.size());\n+             new (self) CompileOptions(\n+                 ValueOrThrow(CompileOptions::FromProto(result)));\n+           })\n+      .def(\"SerializeAsString\",\n+           [](const CompileOptions& self) -> nb::bytes {\n+             auto proto = ValueOrThrow(self.ToProto());\n+             std::string result;\n+             if (!tsl::SerializeToStringDeterministic(proto, &result)) {\n+               // throw converted by PyBind to a Python RuntimeError.\n+               throw XlaRuntimeError(\n+                   absl::StrCat(\"CompileOptions.SerializeAsString: \",\n+                                \"SerializeToStringDeterministic failed\"));\n+             }\n+             return nb::bytes(result.data(), result.size());\n+           })\n+      .def_static(\"ParseFromString\",\n+                  [](nb::bytes s) {\n+                    CompileOptionsProto result;\n+                    result.ParseFromArray(s.c_str(), s.size());\n+                    return ValueOrThrow(CompileOptions::FromProto(result));\n+                  })\n+      .def_rw(\"argument_layouts\", &CompileOptions::argument_layouts)\n+      .def_rw(\"parameter_is_tupled_arguments\",\n+              &CompileOptions::parameter_is_tupled_arguments)\n+      .def_rw(\"compile_portable_executable\",\n+              &CompileOptions::compile_portable_executable)\n+      .def_ro(\"executable_build_options\",\n+              &CompileOptions::executable_build_options)\n+      .def_rw(\"env_option_overrides\", &CompileOptions::env_option_overrides)\n+      .def_prop_rw(\n+          \"num_replicas\",\n+          [](const CompileOptions& options) {\n+            return options.executable_build_options.num_replicas();\n+          },\n+          [](CompileOptions& options, int num_replicas) {\n+            options.executable_build_options.set_num_replicas(num_replicas);\n+          })\n+      .def_prop_rw(\n+          \"num_partitions\",\n+          [](const CompileOptions& options) {\n+            return options.executable_build_options.num_partitions();\n+          },\n+          [](CompileOptions& options, int num_partitions) {\n+            options.executable_build_options.set_num_partitions(num_partitions);\n+          })\n+      .def_prop_rw(\n+          \"profile_version\",\n+          [](const CompileOptions& options) { return options.profile_version; },\n+          [](CompileOptions& options, int64_t profile_version) {\n+            options.profile_version = profile_version;\n+          })\n+      .def_prop_rw(\n+          \"device_assignment\",\n+          [](const CompileOptions& options) -> std::optional<DeviceAssignment> {\n+            return options.executable_build_options.has_device_assignment()\n+                       ? std::optional<DeviceAssignment>(\n+                             options.executable_build_options\n+                                 .device_assignment())\n+                       : std::nullopt;\n+          },\n+          [](CompileOptions& options,\n+             const DeviceAssignment& device_assignment) {\n+            options.executable_build_options.set_device_assignment(\n+                device_assignment);\n+          });\n+\n+  nb::enum_<DebugOptions::AutotuneCacheMode>(m, \"AutotuneCacheMode\")\n+      .value(\"UNSPECIFIED\", DebugOptions::AUTOTUNE_CACHE_MODE_UNSPECIFIED)\n+      .value(\"UPDATE\", DebugOptions::AUTOTUNE_CACHE_MODE_UPDATE)\n+      .value(\"READ\", DebugOptions::AUTOTUNE_CACHE_MODE_READ);\n+\n+  nb::class_<DebugOptions>(m, \"DebugOptions\")\n+      .def(\"__repr__\", &DebugOptions::DebugString)\n+      .def_prop_rw(\"xla_backend_optimization_level\",\n+                   &DebugOptions::xla_backend_optimization_level,\n+                   &DebugOptions::set_xla_backend_optimization_level)\n+      .def_prop_rw(\"xla_cpu_enable_fast_math\",\n+                   &DebugOptions::xla_cpu_enable_fast_math,\n+                   &DebugOptions::set_xla_cpu_enable_fast_math)\n+      .def_prop_rw(\"xla_cpu_enable_xprof_traceme\",\n+                   &DebugOptions::xla_cpu_enable_xprof_traceme,\n+                   &DebugOptions::set_xla_cpu_enable_xprof_traceme)\n+      .def_prop_rw(\"xla_cpu_fast_math_honor_infs\",\n+                   &DebugOptions::xla_cpu_fast_math_honor_infs,\n+                   &DebugOptions::set_xla_cpu_fast_math_honor_infs)\n+      .def_prop_rw(\"xla_cpu_fast_math_honor_nans\",\n+                   &DebugOptions::xla_cpu_fast_math_honor_nans,\n+                   &DebugOptions::set_xla_cpu_fast_math_honor_nans)\n+      .def_prop_rw(\"xla_cpu_fast_math_honor_division\",\n+                   &DebugOptions::xla_cpu_fast_math_honor_division,\n+                   &DebugOptions::set_xla_cpu_fast_math_honor_division)\n+      .def_prop_rw(\"xla_cpu_fast_math_honor_functions\",\n+                   &DebugOptions::xla_cpu_fast_math_honor_functions,\n+                   &DebugOptions::set_xla_cpu_fast_math_honor_functions)\n+      .def_prop_rw(\"xla_detailed_logging\", &DebugOptions::xla_detailed_logging,\n+                   &DebugOptions::set_xla_detailed_logging)\n+      .def_prop_rw(\"xla_enable_dumping\", &DebugOptions::xla_enable_dumping,\n+                   &DebugOptions::set_xla_enable_dumping)\n+      .def_prop_rw(\"xla_gpu_enable_fast_min_max\",\n+                   &DebugOptions::xla_gpu_enable_fast_min_max,\n+                   &DebugOptions::set_xla_gpu_enable_fast_min_max)\n+      .def_prop_rw(\"xla_gpu_dump_autotune_results_to\",\n+                   &DebugOptions::xla_gpu_dump_autotune_results_to,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_gpu_dump_autotune_results_to(value);\n+                   })\n+      .def_prop_rw(\"xla_gpu_load_autotune_results_from\",\n+                   &DebugOptions::xla_gpu_load_autotune_results_from,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_gpu_load_autotune_results_from(value);\n+                   })\n+      .def_prop_rw(\"xla_gpu_cuda_data_dir\",\n+                   &DebugOptions::xla_gpu_cuda_data_dir,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_gpu_cuda_data_dir(value);\n+                   })\n+      .def_prop_rw(\"xla_llvm_disable_expensive_passes\",\n+                   &DebugOptions::xla_llvm_disable_expensive_passes,\n+                   &DebugOptions::set_xla_llvm_disable_expensive_passes)\n+      .def_prop_rw(\n+          \"xla_disable_hlo_passes\",\n+          [](DebugOptions* self) {\n+            return absl::StrJoin(self->xla_disable_hlo_passes(), \",\");\n+          },\n+          [](DebugOptions* self, std::string value) {\n+            self->clear_xla_disable_hlo_passes();\n+            for (const auto& passname :\n+                 std::vector<std::string>(absl::StrSplit(value, ','))) {\n+              self->add_xla_disable_hlo_passes(passname);\n+            }\n+          })\n+      .def_prop_rw(\n+          \"xla_enable_hlo_passes_only\",\n+          [](DebugOptions* self) {\n+            return absl::StrJoin(self->xla_enable_hlo_passes_only(), \",\");\n+          },\n+          [](DebugOptions* self, std::string value) {\n+            self->clear_xla_enable_hlo_passes_only();\n+            for (const auto& passname :\n+                 std::vector<std::string>(absl::StrSplit(value, ','))) {\n+              self->add_xla_enable_hlo_passes_only(passname);\n+            }\n+          })\n+      .def_prop_rw(\"xla_test_all_input_layouts\",\n+                   &DebugOptions::xla_test_all_input_layouts,\n+                   &DebugOptions::set_xla_test_all_input_layouts)\n+      .def_prop_rw(\"xla_force_host_platform_device_count\",\n+                   &DebugOptions::xla_force_host_platform_device_count,\n+                   &DebugOptions::set_xla_force_host_platform_device_count)\n+      .def_prop_rw(\"xla_dump_to\", &DebugOptions::xla_dump_to,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_dump_to(value);\n+                   })\n+      .def_prop_rw(\"xla_dump_hlo_module_re\",\n+                   &DebugOptions::xla_dump_hlo_module_re,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_dump_hlo_module_re(value);\n+                   })\n+      .def_prop_rw(\"xla_dump_hlo_pass_re\", &DebugOptions::xla_dump_hlo_pass_re,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_dump_hlo_pass_re(value);\n+                   })\n+      .def_prop_rw(\"xla_dump_hlo_as_text\", &DebugOptions::xla_dump_hlo_as_text,\n+                   &DebugOptions::set_xla_dump_hlo_as_text)\n+      .def_prop_rw(\"xla_dump_hlo_as_proto\",\n+                   &DebugOptions::xla_dump_hlo_as_proto,\n+                   &DebugOptions::set_xla_dump_hlo_as_proto)\n+      .def_prop_rw(\"xla_dump_hlo_as_dot\", &DebugOptions::xla_dump_hlo_as_dot,\n+                   &DebugOptions::set_xla_dump_hlo_as_dot)\n+      .def_prop_rw(\"xla_dump_hlo_as_url\", &DebugOptions::xla_dump_hlo_as_url,\n+                   &DebugOptions::set_xla_dump_hlo_as_url)\n+      .def_prop_rw(\"xla_dump_hlo_as_html\", &DebugOptions::xla_dump_hlo_as_html,\n+                   &DebugOptions::set_xla_dump_hlo_as_html)\n+      .def_prop_rw(\"xla_dump_fusion_visualization\",\n+                   &DebugOptions::xla_dump_fusion_visualization,\n+                   &DebugOptions::set_xla_dump_fusion_visualization)\n+      .def_prop_rw(\"xla_dump_hlo_snapshots\",\n+                   &DebugOptions::xla_dump_hlo_snapshots,\n+                   &DebugOptions::set_xla_dump_hlo_snapshots)\n+      .def_prop_rw(\"xla_dump_max_hlo_modules\",\n+                   &DebugOptions::xla_dump_max_hlo_modules,\n+                   &DebugOptions::set_xla_dump_max_hlo_modules)\n+      .def_prop_rw(\"xla_dump_module_metadata\",\n+                   &DebugOptions::xla_dump_module_metadata,\n+                   &DebugOptions::set_xla_dump_module_metadata)\n+      .def_prop_rw(\"xla_dump_compress_protos\",\n+                   &DebugOptions::xla_dump_compress_protos,\n+                   &DebugOptions::set_xla_dump_compress_protos)\n+      .def_prop_rw(\"xla_dump_hlo_as_long_text\",\n+                   &DebugOptions::xla_dump_hlo_as_long_text,\n+                   &DebugOptions::set_xla_dump_hlo_as_long_text)\n+      .def_prop_rw(\"xla_dump_disable_metadata\",\n+                   &DebugOptions::xla_dump_disable_metadata,\n+                   &DebugOptions::set_xla_dump_disable_metadata)\n+      .def_prop_rw(\"xla_dump_hlo_pipeline_re\",\n+                   &DebugOptions::xla_dump_hlo_pipeline_re,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_dump_hlo_pipeline_re(value);\n+                   })\n+      .def_prop_rw(\"xla_gpu_dump_autotune_logs_to\",\n+                   &DebugOptions::xla_gpu_dump_autotune_logs_to,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_gpu_dump_autotune_logs_to(value);\n+                   })\n+      .def_prop_rw(\"xla_gpu_kernel_cache_file\",\n+                   &DebugOptions::xla_gpu_kernel_cache_file,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_gpu_kernel_cache_file(value);\n+                   })\n+      .def_prop_rw(\n+          \"xla_gpu_enable_llvm_module_compilation_parallelism\",\n+          &DebugOptions::xla_gpu_enable_llvm_module_compilation_parallelism,\n+          &DebugOptions::set_xla_gpu_enable_llvm_module_compilation_parallelism)\n+      .def_prop_rw(\"xla_gpu_per_fusion_autotune_cache_dir\",\n+                   &DebugOptions::xla_gpu_per_fusion_autotune_cache_dir,\n+                   [](DebugOptions* self, std::string value) {\n+                     self->set_xla_gpu_per_fusion_autotune_cache_dir(value);\n+                   })\n+      .def_prop_rw(\"xla_gpu_experimental_autotune_cache_mode\",\n+                   &DebugOptions::xla_gpu_experimental_autotune_cache_mode,\n+                   &DebugOptions::set_xla_gpu_experimental_autotune_cache_mode);\n+\n+  nb::class_<ExecutableBuildOptions>(m, \"ExecutableBuildOptions\")\n+      .def(nb::init<>())\n+      .def(\"__repr__\", &ExecutableBuildOptions::ToString)\n+      .def_prop_rw(\n+          \"fdo_profile\",\n+          [](const ExecutableBuildOptions& options) {\n+            return nb::bytes(options.fdo_profile().data(),\n+                             options.fdo_profile().size());\n+          },\n+          [](ExecutableBuildOptions& options, nb::bytes fdo_profile) {\n+            options.set_fdo_profile(\n+                std::string(fdo_profile.c_str(), fdo_profile.size()));\n+          })\n+      .def_prop_rw(\n+          \"result_layout\",\n+          [](const ExecutableBuildOptions& options) -> std::optional<Shape> {\n+            return options.result_layout()\n+                       ? std::optional<Shape>(*options.result_layout())\n+                       : std::nullopt;\n+          },\n+          &ExecutableBuildOptions::set_result_layout)\n+      .def_prop_rw(\"num_replicas\", &ExecutableBuildOptions::num_replicas,\n+                   &ExecutableBuildOptions::set_num_replicas)\n+      .def_prop_rw(\"num_partitions\", &ExecutableBuildOptions::num_partitions,\n+                   &ExecutableBuildOptions::set_num_partitions)\n+      .def_prop_ro(\"debug_options\",\n+                   &ExecutableBuildOptions::mutable_debug_options,\n+                   nb::rv_policy::reference, nb::keep_alive<1, 0>())\n+      .def_prop_rw(\n+          \"device_assignment\",\n+          [](const ExecutableBuildOptions& options)\n+              -> std::optional<DeviceAssignment> {\n+            return options.has_device_assignment()\n+                       ? std::optional<DeviceAssignment>(\n+                             options.device_assignment())\n+                       : std::nullopt;\n+          },\n+          &ExecutableBuildOptions::set_device_assignment)\n+      .def(\"compilation_environments_from_serialized_proto\",\n+           [](ExecutableBuildOptions& options,\n+              const nb::bytes& serialized_proto) {\n+             xla::CompilationEnvironmentsProto env_proto;\n+             env_proto.ParseFromArray(serialized_proto.c_str(),\n+                                      serialized_proto.size());\n+             auto comp_envs = xla::ValueOrThrow(\n+                 xla::CompilationEnvironments::CreateFromProto(env_proto));\n+             *options.mutable_comp_envs() = std::move(*comp_envs);\n+           })\n+      .def_prop_rw(\"exec_time_optimization_effort\",\n+                   &ExecutableBuildOptions::exec_time_optimization_effort,\n+                   &ExecutableBuildOptions::set_exec_time_optimization_effort)\n+      .def_prop_rw(\"memory_fitting_effort\",\n+                   &ExecutableBuildOptions::memory_fitting_effort,\n+                   &ExecutableBuildOptions::set_memory_fitting_effort)\n+      .def_prop_rw(\n+          \"optimization_level\",\n+          [](ExecutableBuildOptions& options) {\n+            return static_cast<int>(options.optimization_level());\n+          },\n+          [](ExecutableBuildOptions& options, int value) {\n+            options.set_optimization_level(\n+                static_cast<xla::ExecutionOptions::EffortLevel>(value));\n+          })\n+      .def_prop_rw(\n+          \"memory_fitting_level\",\n+          [](ExecutableBuildOptions& options) {\n+            return static_cast<int>(options.memory_fitting_level());\n+          },\n+          [](ExecutableBuildOptions& options, int value) {\n+            options.set_memory_fitting_level(\n+                static_cast<xla::ExecutionOptions::EffortLevel>(value));\n+          })\n+      .def_prop_rw(\"use_spmd_partitioning\",\n+                   &ExecutableBuildOptions::use_spmd_partitioning,\n+                   &ExecutableBuildOptions::set_use_spmd_partitioning)\n+      .def_prop_rw(\"use_auto_spmd_partitioning\",\n+                   &ExecutableBuildOptions::use_auto_spmd_partitioning,\n+                   &ExecutableBuildOptions::set_use_auto_spmd_partitioning)\n+      .def_prop_rw(\n+          \"auto_spmd_partitioning_mesh_shape\",\n+          &ExecutableBuildOptions::auto_spmd_partitioning_mesh_shape,\n+          &ExecutableBuildOptions::set_auto_spmd_partitioning_mesh_shape)\n+      .def_prop_rw(\"auto_spmd_partitioning_mesh_ids\",\n+                   &ExecutableBuildOptions::auto_spmd_partitioning_mesh_ids,\n+                   &ExecutableBuildOptions::set_auto_spmd_partitioning_mesh_ids)\n+      .def_prop_rw(\n+          \"allow_spmd_sharding_propagation_to_parameters\",\n+          [](const ExecutableBuildOptions& options) -> std::vector<bool> {\n+            return std::vector<bool>(\n+                options.allow_spmd_sharding_propagation_to_parameters().begin(),\n+                options.allow_spmd_sharding_propagation_to_parameters().end());\n+          },\n+          [](ExecutableBuildOptions& options, std::vector<bool> values) {\n+            absl::InlinedVector<bool, 1> v(values.begin(), values.end());\n+            options.set_allow_spmd_sharding_propagation_to_parameters(v);\n+          })\n+      .def_prop_rw(\n+          \"allow_spmd_sharding_propagation_to_output\",\n+          [](const ExecutableBuildOptions& options) -> std::vector<bool> {\n+            return std::vector<bool>(\n+                options.allow_spmd_sharding_propagation_to_output().begin(),\n+                options.allow_spmd_sharding_propagation_to_output().end());\n+          },\n+          [](ExecutableBuildOptions& options, std::vector<bool> values) {\n+            absl::InlinedVector<bool, 1> v(values.begin(), values.end());\n+            options.set_allow_spmd_sharding_propagation_to_output(v);\n+          })\n+      .def_prop_rw(\"use_shardy_partitioner\",\n+                   &ExecutableBuildOptions::use_shardy_partitioner,\n+                   &ExecutableBuildOptions::set_use_shardy_partitioner);\n+\n+  nb::enum_<OpSharding::Type> op_sharding_type(m, \"OpSharding_Type\",\n+                                               nb::is_arithmetic());\n+  op_sharding_type.value(\"REPLICATED\", OpSharding::REPLICATED)\n+      .value(\"MAXIMAL\", OpSharding::MAXIMAL)\n+      .value(\"MANUAL\", OpSharding::MANUAL)\n+      .value(\"UNREDUCED\", OpSharding::UNREDUCED)\n+      .value(\"TUPLE\", OpSharding::TUPLE)\n+      .value(\"OTHER\", OpSharding::OTHER)\n+      .value(\"UNKNOWN\", OpSharding::UNKNOWN);\n+\n+  nb::enum_<OpSharding::ShardGroupType> op_sharding_shard_group_type(\n+      m, \"OpSharding_ShardGroupType\");\n+  op_sharding_shard_group_type.value(\"AS\", OpSharding::AS)\n+      .value(\"LIKE\", OpSharding::LIKE);\n+\n+  nb::class_<OpSharding> op_sharding(m, \"OpSharding\");\n+  op_sharding.attr(\"Type\") = op_sharding_type;\n+  op_sharding.attr(\"ShardGroupType\") = op_sharding_shard_group_type;\n+  op_sharding.def(nb::init<>())\n+      .def(\"__getstate__\",\n+           [](const OpSharding& self) {\n+             std::string serialized = self.SerializeAsString();\n+             return nb::make_tuple(\n+                 nb::bytes(serialized.data(), serialized.size()));\n+           })\n+      .def(\"__setstate__\",\n+           [](OpSharding* self, nb::tuple t) {\n+             new (self) OpSharding();\n+             nb::bytes serialized = nb::cast<nb::bytes>(t[0]);\n+             self->ParseFromArray(serialized.c_str(), serialized.size());\n+           })\n+      .def_prop_rw(\"type\", &xla::OpSharding::type, &xla::OpSharding::set_type)\n+      .def_prop_rw(\"replicate_on_last_tile_dim\",\n+                   &xla::OpSharding::replicate_on_last_tile_dim,\n+                   &xla::OpSharding::set_replicate_on_last_tile_dim)\n+      .def_prop_rw(\"is_shard_group\", &xla::OpSharding::is_shard_group,\n+                   &xla::OpSharding::set_is_shard_group)\n+      .def_prop_rw(\"shard_group_id\", &xla::OpSharding::shard_group_id,\n+                   &xla::OpSharding::set_shard_group_id)\n+      .def_prop_rw(\"shard_group_type\", &xla::OpSharding::shard_group_type,\n+                   &xla::OpSharding::set_shard_group_type)\n+      .def(\"__repr__\",\n+           [](const xla::OpSharding& self) { return self.DebugString(); })\n+      .def(\"ParseFromString\",\n+           [](OpSharding& sharding, const nb::bytes& s) {\n+             sharding.ParseFromArray(s.c_str(), s.size());\n+           })\n+      .def(\"SerializeToString\",\n+           [](const OpSharding& sharding) {\n+             std::string serialized = sharding.SerializeAsString();\n+             return nb::bytes(serialized.data(), serialized.size());\n+           })\n+      .def(\"clone\",\n+           [](const OpSharding& sharding) { return OpSharding(sharding); });\n+  DefRepeatedProperty(op_sharding, \"tile_assignment_dimensions\",\n+                      &xla::OpSharding::mutable_tile_assignment_dimensions);\n+  DefRepeatedProperty(op_sharding, \"tile_assignment_devices\",\n+                      &xla::OpSharding::mutable_tile_assignment_devices);\n+  DefRepeatedProperty(op_sharding, \"iota_reshape_dims\",\n+                      &xla::OpSharding::mutable_iota_reshape_dims);\n+  DefRepeatedProperty(op_sharding, \"iota_transpose_perm\",\n+                      &xla::OpSharding::mutable_iota_transpose_perm);\n+  DefRepeatedProperty(op_sharding, \"tuple_shardings\",\n+                      &xla::OpSharding::mutable_tuple_shardings);\n+  DefRepeatedEnumProperty(op_sharding, \"last_tile_dims\",\n+                          &xla::OpSharding::mutable_last_tile_dims);\n+\n+  nb::class_<HloSharding> hlo_sharding(m, \"HloSharding\");\n+  hlo_sharding\n+      .def_static(\"from_proto\",\n+                  xla::ValueOrThrowWrapper(xla::HloSharding::FromProto))\n+      .def_static(\"from_string\", xla::ValueOrThrowWrapper(xla::ParseSharding))\n+      .def_static(\n+          \"tuple_sharding\",\n+          [](xla::Shape shape,\n+             std::vector<xla::HloSharding> shardings) -> xla::HloSharding {\n+            return HloSharding::Tuple(shape, shardings);\n+          },\n+          \"Constructs a tuple sharding.\")\n+      .def_static(\n+          \"iota_tile\", xla::ValueOrThrowWrapper(IotaTileHelper),\n+          nb::arg(\"dims\"),\n+          nb::arg(\"reshape_dims\") = absl::Span<const int64_t>(),\n+          nb::arg(\"transpose_perm\") = absl::Span<const int>(),\n+          nb::arg(\"subgroup_types\") = absl::Span<const xla::OpSharding::Type>())\n+      .def_static(\"manual\", [] { return HloSharding::Manual(); })\n+      .def_static(\"replicate\", [] { return HloSharding::Replicate(); })\n+      .def_static(\"unreduced\", [] { return HloSharding::Unreduced(); })\n+      .def_static(\"unknown\", [] { return HloSharding::Unknown(); })\n+      .def_static(\n+          \"subgroup_with_device_ordering\",\n+          xla::ValueOrThrowWrapper(SubgroupWithTileAssignmentHelper),\n+          nb::arg(\"tile_assignment\"),\n+          nb::arg(\"subgroup_types\") = absl::Span<const xla::OpSharding::Type>())\n+      .def(\n+          \"__eq__\",\n+          [](const xla::HloSharding& a, const xla::HloSharding& b) {\n+            return a == b;\n+          },\n+          nb::is_operator(),\n+          nb::sig(\"def __eq__(self, other: object, /) -> bool\"))\n+      .def(\n+          \"__ne__\",\n+          [](const xla::HloSharding& a, const xla::HloSharding& b) {\n+            return a != b;\n+          },\n+          nb::is_operator(),\n+          nb::sig(\"def __ne__(self, other: object, /) -> bool\"))\n+      .def(\"__hash__\",\n+           [](const xla::HloSharding& self) { return absl::HashOf(self); })\n+      .def(\"is_replicated\", &xla::HloSharding::IsReplicated)\n+      .def(\"is_manual\", &xla::HloSharding::IsManual)\n+      .def(\"is_unreduced\", &xla::HloSharding::IsUnreduced)\n+      .def(\"is_unknown\", &xla::HloSharding::IsUnknown)\n+      .def(\"is_tiled\", &xla::HloSharding::IsTiled)\n+      .def(\"is_maximal\", &xla::HloSharding::IsTileMaximal)\n+      .def(\"tile\", [](const xla::HloSharding& self,\n+                      xla::Shape shape) { return self.TileShape(shape); })\n+      // tile_assignment.array() is computed using an internal cache,\n+      // which is why nb::lock_self() is required. It may be preferable to move\n+      // this locking into the TileAssignment class if we find it to race with\n+      // non-Python users of that class.\n+      .def(\n+          \"tuple_elements\",\n+          [](const xla::HloSharding& self) { return self.tuple_elements(); },\n+          nb::lock_self())\n+      .def(\n+          \"num_devices\",\n+          [](const xla::HloSharding& self) {\n+            return self.tile_assignment().num_elements();\n+          },\n+          nb::lock_self())\n+      .def(\n+          \"num_dimensions\",\n+          [](const xla::HloSharding& self) {\n+            return self.tile_assignment().num_dimensions();\n+          },\n+          nb::lock_self())\n+      .def(\"is_tile_assignment_iota\",\n+           [](const xla::HloSharding& self) {\n+             return self.tile_assignment().iota().has_value();\n+           })\n+      .def(\n+          \"tile_assignment_dimensions\",\n+          [](const xla::HloSharding& self) {\n+            absl::Span<int64_t const> span =\n+                self.tile_assignment().dimensions();\n+            CHECK(span.data());\n+            return span;\n+          },\n+          nb::lock_self())\n+      .def(\n+          \"tile_assignment_devices\",\n+          [](const xla::HloSharding& self) {\n+            auto span =\n+                absl::MakeConstSpan(self.tile_assignment().array().data(),\n+                                    self.tile_assignment().num_elements());\n+            CHECK(span.data());\n+            return span;\n+          },\n+          nb::lock_self())\n+      .def(\"replicate_on_last_tile_dim\",\n+           &xla::HloSharding::ReplicateOnLastTileDim)\n+      .def(\"subgroup_types\", &xla::HloSharding::subgroup_types)\n+      .def(\"__repr__\",\n+           [](const xla::HloSharding& self) { return self.ToString(); })\n+      .def(\"to_proto\", &xla::HloSharding::ToProto)\n+      .def(\"get_axis_sizes\", [](const xla::HloSharding& self) {\n+        // If returning the SmallVector, we encounter the error \"unable to\n+        // convert function return value to a Python type!\".\n+        mlir::SmallVector<int64_t> mesh_shape =\n+            xla::sdy::getAxisSizes(self.tile_assignment());\n+        return std::vector<int64_t>(mesh_shape.begin(), mesh_shape.end());\n+      });\n+}  // NOLINT(readability/fn_size)\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 2164,
        "additions": 2162,
        "deletions": 2
    }
}