{
    "author": "draganmladjenovic",
    "message": "PR #31409: [ROCm] Restore hipblaslt gemm support\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31409\n\nüìù Summary of Changes\nInclude cublaslt (hipblaslt) in gemm autotuning for rocm\n\nüéØ Justification\nRestore old behaviour\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nN\\A\n\nüß™ Unit Tests:\nNone\n\nüß™ Execution Tests:\nNone\n\nCopybara import of the project:\n\n--\n3dd13a937b05cd6678a81dd1eada6892befc7138 by Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>:\n\n[ROCm] Restore hipblaslt gemm support\n\nMerging this change closes #31409\n\nPiperOrigin-RevId: 810037609",
    "sha": "f0400b1e93430672abc568baff232ab6864bd8dd",
    "files": [
        {
            "sha": "be07178c311dbb56ebeb9525167ceff6b9a5faf8",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f0400b1e93430672abc568baff232ab6864bd8dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f0400b1e93430672abc568baff232ab6864bd8dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=f0400b1e93430672abc568baff232ab6864bd8dd",
            "patch": "@@ -2137,6 +2137,7 @@ cc_library(\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/backends/gpu/autotuner:cublas\",\n+        \"//xla/backends/gpu/autotuner:cublaslt\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\","
        },
        {
            "sha": "51474af68876b29d36949d8ed0d5e81ceaa5e56e",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f0400b1e93430672abc568baff232ab6864bd8dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f0400b1e93430672abc568baff232ab6864bd8dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=f0400b1e93430672abc568baff232ab6864bd8dd",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"llvm/IR/Module.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n+#include \"xla/backends/gpu/autotuner/cublaslt.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -266,6 +267,8 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n   // backend uses the same API as rocBLAS.\n   backends.push_back(\n       std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n+  backends.push_back(\n+      std::make_unique<CublasLtBackend>(stream_exec, &debug_options, this));\n   auto should_autotune = [](const HloInstruction& instruction) -> bool {\n     return instruction.opcode() == HloOpcode::kCustomCall &&\n            IsCublasGemm(instruction);"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}