{
    "author": "ezhulenev",
    "message": "[xla:ffi] Split internal XLA FFI API implementation into separate target\n\nIn preparation for moving backend-specific FFI implementation into CPU and GPU backends, split XLA_FFI_INTERNAL APIs into separate target.\n\nPiperOrigin-RevId: 836851323",
    "sha": "c851c888f25518efce602d19e49549bb5cf45da4",
    "files": [
        {
            "sha": "1e780f49201c144d958d7692f68392937fdac3cb",
            "filename": "third_party/xla/xla/ffi/BUILD",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2FBUILD?ref=c851c888f25518efce602d19e49549bb5cf45da4",
            "patch": "@@ -166,6 +166,8 @@ cc_library(\n         \":call_frame\",\n         \":execution_context\",\n         \":execution_state\",\n+        \":ffi_internal_api\",\n+        \":ffi_structs\",\n         \":type_registry\",\n         \"//xla:executable_run_options\",\n         \"//xla:util\",\n@@ -194,6 +196,42 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"ffi_internal_api\",\n+    srcs = [\"ffi_internal_api.cc\"],\n+    hdrs = [\"ffi_internal_api.h\"],\n+    visibility = [\"//visibility:private\"],\n+    deps = [\n+        \":execution_context\",\n+        \":execution_state\",\n+        \":ffi_structs\",\n+        \"//xla:util\",\n+        \"//xla/ffi/api:c_api\",\n+        \"//xla/ffi/api:c_api_internal\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/tsl/concurrency:async_value\",\n+        \"//xla/tsl/concurrency:ref_count\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"ffi_structs\",\n+    hdrs = [\"ffi_structs.h\"],\n+    visibility = [\"//visibility:private\"],\n+    deps = [\n+        \":execution_context\",\n+        \":execution_state\",\n+        \"//xla:executable_run_options\",\n+        \"//xla/ffi/api:c_api\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/tsl/concurrency:async_value\",\n+        \"@com_google_absl//absl/status\",\n+    ],\n+)\n+\n cc_library(\n     name = \"attribute_map\",\n     srcs = [\"attribute_map.cc\"],"
        },
        {
            "sha": "3aa0d1c5f13533bef3e8ce0e58be1720090663bb",
            "filename": "third_party/xla/xla/ffi/api/c_api.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api.h?ref=c851c888f25518efce602d19e49549bb5cf45da4",
            "patch": "@@ -751,7 +751,7 @@ struct XLA_FFI_Api {\n   XLA_FFI_Extension_Base* extension_start;\n \n   XLA_FFI_Api_Version api_version;\n-  XLA_FFI_InternalApi* internal_api;\n+  const XLA_FFI_InternalApi* internal_api;\n \n   _XLA_FFI_API_STRUCT_FIELD(XLA_FFI_Error_Create);\n   _XLA_FFI_API_STRUCT_FIELD(XLA_FFI_Error_GetMessage);"
        },
        {
            "sha": "a56b25a7c3ce3edf3f62940ea56d70f810b50402",
            "filename": "third_party/xla/xla/ffi/ffi_api.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 184,
            "changes": 221,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc?ref=c851c888f25518efce602d19e49549bb5cf45da4",
            "patch": "@@ -43,58 +43,22 @@ limitations under the License.\n #include \"xla/ffi/call_frame.h\"\n #include \"xla/ffi/execution_context.h\"\n #include \"xla/ffi/execution_state.h\"\n+#include \"xla/ffi/ffi_internal_api.h\"\n+#include \"xla/ffi/ffi_structs.h\"\n #include \"xla/ffi/type_registry.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/chain.h\"\n-#include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n #define EIGEN_USE_THREADS\n #include \"unsupported/Eigen/CXX11/Tensor\"\n \n-//===----------------------------------------------------------------------===//\n-// XLA FFI C structs definition\n-//===----------------------------------------------------------------------===//\n-\n-struct XLA_FFI_Error {\n-  absl::Status status;\n-};\n-\n-struct XLA_FFI_Future {\n-  tsl::AsyncValueRef<tsl::Chain> async_value;\n-};\n-\n-struct XLA_FFI_ExecutionContext {\n-  struct CpuContext {\n-    const Eigen::ThreadPoolDevice* intra_op_thread_pool = nullptr;\n-  };\n-\n-  struct GpuContext {\n-    stream_executor::Stream* stream = nullptr;\n-    stream_executor::DeviceMemoryAllocator* allocator = nullptr;\n-  };\n-\n-  using BackendContext = std::variant<std::monostate, CpuContext, GpuContext>;\n-\n-  xla::RunId run_id = {};\n-  int32_t device_ordinal = -1;\n-  BackendContext backend_context = {};\n-\n-  const xla::HloComputation* called_computation = nullptr;\n-  const xla::ffi::ExecutionContext* execution_context = nullptr;\n-  xla::ffi::ExecutionState* execution_state = nullptr;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-\n namespace xla::ffi {\n \n // The minimum XLA:FFI API version that XLA runtime supports.\n@@ -889,157 +853,46 @@ static XLA_FFI_Error* XLA_FFI_ThreadPool_NumThreads(\n }\n \n //===----------------------------------------------------------------------===//\n-// Generic XLA internal APIs available on all XLA backends.\n-//===----------------------------------------------------------------------===//\n-\n-static XLA_FFI_Error* XLA_FFI_INTERNAL_Error_Forward(void* status) {\n-  auto* absl_status = reinterpret_cast<absl::Status*>(status);\n-  if (ABSL_PREDICT_TRUE(absl_status->ok())) {\n-    return nullptr;\n-  }\n-  return new XLA_FFI_Error{std::move(*absl_status)};\n-}\n-\n-static XLA_FFI_Future* XLA_FFI_INTERNAL_Future_Forward(void* async_value) {\n-  auto* tsl_async_value = reinterpret_cast<tsl::AsyncValue*>(async_value);\n-  DCHECK(tsl_async_value) << \"Async value must not be null\";\n-\n-  return new XLA_FFI_Future{\n-      tsl::AsyncValueRef<tsl::Chain>(tsl::TakeRef(tsl_async_value))};\n-}\n-\n-static int32_t XLA_FFI_INTERNAL_DeviceOrdinal_Get(\n-    XLA_FFI_ExecutionContext* ctx) {\n-  return ctx->device_ordinal;\n-}\n-\n-static int64_t XLA_FFI_INTERNAL_RunId_Get(XLA_FFI_ExecutionContext* ctx) {\n-  return ctx->run_id.ToInt();\n-}\n-\n-static void* XLA_FFI_INTERNAL_CalledComputation_Get(\n-    XLA_FFI_ExecutionContext* ctx) {\n-  return const_cast<HloComputation*>(ctx->called_computation);  // NOLINT\n-}\n-\n-static void* XLA_FFI_INTERNAL_ExecutionContext_Get(\n-    XLA_FFI_ExecutionContext* ctx) {\n-  return const_cast<ffi::ExecutionContext*>(ctx->execution_context);  // NOLINT\n-}\n-\n-static void* XLA_FFI_INTERNAL_ExecutionState_Get(\n-    XLA_FFI_ExecutionContext* ctx) {\n-  return const_cast<ffi::ExecutionState*>(ctx->execution_state);  // NOLINT\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// XLA:CPU specific internal APIs.\n-//===----------------------------------------------------------------------===//\n-\n-static XLA_FFI_Error* XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n-    XLA_FFI_ExecutionContext* ctx, void** thread_pool) {\n-  if (auto* cpu = std::get_if<XLA_FFI_ExecutionContext::CpuContext>(\n-          &ctx->backend_context)) {\n-    *thread_pool = const_cast<Eigen::ThreadPoolDevice*>(  // NOLINT\n-        cpu->intra_op_thread_pool);\n-    return nullptr;\n-  }\n-\n-  // For GPU backend we don't have intra-op thread pool, but we didn't promise\n-  // to return one, so instead of an error we return a nullptr thread pool.\n-  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n-          &ctx->backend_context)) {\n-    return nullptr;\n-  }\n-\n-  return new XLA_FFI_Error{InvalidArgument(\"XLA FFI context is not available\")};\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// XLA:GPU specific internal APIs.\n+// XLA FFI Api access\n //===----------------------------------------------------------------------===//\n \n-static XLA_FFI_Error* XLA_FFI_INTERNAL_Stream_Get(XLA_FFI_ExecutionContext* ctx,\n-                                                  void** stream) {\n-  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n-          &ctx->backend_context)) {\n-    *stream = gpu->stream;\n-    return nullptr;\n-  }\n-\n-  return new XLA_FFI_Error{\n-      InvalidArgument(\"XLA FFI GPU context is not available\")};\n-}\n-\n-static XLA_FFI_Error* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n-    XLA_FFI_ExecutionContext* ctx, void** allocator) {\n-  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n-          &ctx->backend_context)) {\n-    *allocator = gpu->allocator;\n-    return nullptr;\n-  }\n+const XLA_FFI_Api* GetXlaFfiApi() {\n+  static XLA_FFI_Api api = {\n+      XLA_FFI_Api_STRUCT_SIZE,\n+      /*extension_start=*/nullptr,\n+\n+      XLA_FFI_Api_Version{\n+          XLA_FFI_Api_Version_STRUCT_SIZE,\n+          /*extension_start=*/nullptr,\n+          XLA_FFI_API_MAJOR,\n+          XLA_FFI_API_MINOR,\n+      },\n+\n+      internal::GetInternalApi(),\n+\n+      XLA_FFI_Error_Create,\n+      XLA_FFI_Error_GetMessage,\n+      XLA_FFI_Error_Destroy,\n+      XLA_FFI_Handler_Register,\n+      XLA_FFI_Stream_Get,\n+      XLA_FFI_Type_Register,\n+      XLA_FFI_ExecutionContext_Get,\n+      XLA_FFI_State_Set,\n+      XLA_FFI_State_Get,\n+      XLA_FFI_DeviceMemory_Allocate,\n+      XLA_FFI_DeviceMemory_Free,\n+      XLA_FFI_ThreadPool_Schedule,\n+      XLA_FFI_ThreadPool_NumThreads,\n+      XLA_FFI_Future_Create,\n+      XLA_FFI_Future_SetAvailable,\n+      XLA_FFI_Future_SetError,\n+      XLA_FFI_RunId_Get,\n+      XLA_FFI_DeviceOrdinal_Get,\n+  };\n \n-  return new XLA_FFI_Error{\n-      InvalidArgument(\"XLA FFI GPU context is not available\")};\n+  return &api;\n }\n \n-//===----------------------------------------------------------------------===//\n-// XLA FFI Api access\n-//===----------------------------------------------------------------------===//\n-\n extern \"C\" const XLA_FFI_Api* XLA_FFI_GetApi() { return GetXlaFfiApi(); }\n \n-static XLA_FFI_InternalApi internal_api = {\n-    // Generic XLA APIs available on all XLA backends.\n-    XLA_FFI_INTERNAL_Error_Forward,\n-    XLA_FFI_INTERNAL_Future_Forward,\n-    XLA_FFI_INTERNAL_DeviceOrdinal_Get,\n-    XLA_FFI_INTERNAL_RunId_Get,\n-    XLA_FFI_INTERNAL_CalledComputation_Get,\n-    XLA_FFI_INTERNAL_ExecutionContext_Get,\n-    XLA_FFI_INTERNAL_ExecutionState_Get,\n-\n-    // XLA:CPU specific APIs.\n-    XLA_FFI_INTERNAL_IntraOpThreadPool_Get,\n-\n-    // XLA:GPU specific APIs.\n-    XLA_FFI_INTERNAL_Stream_Get,\n-    XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get,\n-};\n-\n-static XLA_FFI_Api api = {\n-    XLA_FFI_Api_STRUCT_SIZE,\n-    /*extension_start=*/nullptr,\n-\n-    XLA_FFI_Api_Version{\n-        XLA_FFI_Api_Version_STRUCT_SIZE,\n-        /*extension_start=*/nullptr,\n-        XLA_FFI_API_MAJOR,\n-        XLA_FFI_API_MINOR,\n-    },\n-\n-    &internal_api,\n-\n-    XLA_FFI_Error_Create,\n-    XLA_FFI_Error_GetMessage,\n-    XLA_FFI_Error_Destroy,\n-    XLA_FFI_Handler_Register,\n-    XLA_FFI_Stream_Get,\n-    XLA_FFI_Type_Register,\n-    XLA_FFI_ExecutionContext_Get,\n-    XLA_FFI_State_Set,\n-    XLA_FFI_State_Get,\n-    XLA_FFI_DeviceMemory_Allocate,\n-    XLA_FFI_DeviceMemory_Free,\n-    XLA_FFI_ThreadPool_Schedule,\n-    XLA_FFI_ThreadPool_NumThreads,\n-    XLA_FFI_Future_Create,\n-    XLA_FFI_Future_SetAvailable,\n-    XLA_FFI_Future_SetError,\n-    XLA_FFI_RunId_Get,\n-    XLA_FFI_DeviceOrdinal_Get,\n-};\n-\n-const XLA_FFI_Api* GetXlaFfiApi() { return &api; }\n-\n }  // namespace xla::ffi"
        },
        {
            "sha": "e18afb3fa2d13067d8b7903f0a236bd0933f7a4c",
            "filename": "third_party/xla/xla/ffi/ffi_internal_api.cc",
            "status": "added",
            "additions": 156,
            "deletions": 0,
            "changes": 156,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.cc?ref=c851c888f25518efce602d19e49549bb5cf45da4",
            "patch": "@@ -0,0 +1,156 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/ffi/ffi_internal_api.h\"\n+\n+#include <cstdint>\n+#include <utility>\n+#include <variant>\n+\n+#include \"absl/base/optimization.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/api/c_api_internal.h\"  // IWYU pragma: keep\n+#include \"xla/ffi/execution_context.h\"\n+#include \"xla/ffi/execution_state.h\"\n+#include \"xla/ffi/ffi_structs.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/tsl/concurrency/async_value.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+#include \"xla/tsl/concurrency/chain.h\"\n+#include \"xla/tsl/concurrency/ref_count.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::ffi::internal {\n+\n+//===----------------------------------------------------------------------===//\n+// Generic XLA internal APIs available on all XLA backends.\n+//===----------------------------------------------------------------------===//\n+\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_Error_Forward(void* status) {\n+  auto* absl_status = reinterpret_cast<absl::Status*>(status);\n+  if (ABSL_PREDICT_TRUE(absl_status->ok())) {\n+    return nullptr;\n+  }\n+  return new XLA_FFI_Error{std::move(*absl_status)};\n+}\n+\n+static XLA_FFI_Future* XLA_FFI_INTERNAL_Future_Forward(void* async_value) {\n+  auto* tsl_async_value = reinterpret_cast<tsl::AsyncValue*>(async_value);\n+  DCHECK(tsl_async_value) << \"Async value must not be null\";\n+\n+  return new XLA_FFI_Future{\n+      tsl::AsyncValueRef<tsl::Chain>(tsl::TakeRef(tsl_async_value))};\n+}\n+\n+static int32_t XLA_FFI_INTERNAL_DeviceOrdinal_Get(\n+    XLA_FFI_ExecutionContext* ctx) {\n+  return ctx->device_ordinal;\n+}\n+\n+static int64_t XLA_FFI_INTERNAL_RunId_Get(XLA_FFI_ExecutionContext* ctx) {\n+  return ctx->run_id.ToInt();\n+}\n+\n+static void* XLA_FFI_INTERNAL_CalledComputation_Get(\n+    XLA_FFI_ExecutionContext* ctx) {\n+  return const_cast<HloComputation*>(ctx->called_computation);  // NOLINT\n+}\n+\n+static void* XLA_FFI_INTERNAL_ExecutionContext_Get(\n+    XLA_FFI_ExecutionContext* ctx) {\n+  return const_cast<ExecutionContext*>(ctx->execution_context);  // NOLINT\n+}\n+\n+static void* XLA_FFI_INTERNAL_ExecutionState_Get(\n+    XLA_FFI_ExecutionContext* ctx) {\n+  return const_cast<ExecutionState*>(ctx->execution_state);  // NOLINT\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// XLA:CPU specific internal APIs.\n+//===----------------------------------------------------------------------===//\n+\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_IntraOpThreadPool_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** thread_pool) {\n+  if (auto* cpu = std::get_if<XLA_FFI_ExecutionContext::CpuContext>(\n+          &ctx->backend_context)) {\n+    *thread_pool = const_cast<Eigen::ThreadPoolDevice*>(  // NOLINT\n+        cpu->intra_op_thread_pool);\n+    return nullptr;\n+  }\n+\n+  // For GPU backend we don't have intra-op thread pool, but we didn't promise\n+  // to return one, so instead of an error we return a nullptr thread pool.\n+  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n+          &ctx->backend_context)) {\n+    return nullptr;\n+  }\n+\n+  return new XLA_FFI_Error{InvalidArgument(\"XLA FFI context is not available\")};\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// XLA:GPU specific internal APIs.\n+//===----------------------------------------------------------------------===//\n+\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_Stream_Get(XLA_FFI_ExecutionContext* ctx,\n+                                                  void** stream) {\n+  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n+          &ctx->backend_context)) {\n+    *stream = gpu->stream;\n+    return nullptr;\n+  }\n+\n+  return new XLA_FFI_Error{\n+      InvalidArgument(\"XLA FFI GPU context is not available\")};\n+}\n+\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** allocator) {\n+  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n+          &ctx->backend_context)) {\n+    *allocator = gpu->allocator;\n+    return nullptr;\n+  }\n+\n+  return new XLA_FFI_Error{\n+      InvalidArgument(\"XLA FFI GPU context is not available\")};\n+}\n+\n+const XLA_FFI_InternalApi* GetInternalApi() {\n+  static XLA_FFI_InternalApi internal_api = {\n+      // Generic XLA APIs available on all XLA backends.\n+      XLA_FFI_INTERNAL_Error_Forward,\n+      XLA_FFI_INTERNAL_Future_Forward,\n+      XLA_FFI_INTERNAL_DeviceOrdinal_Get,\n+      XLA_FFI_INTERNAL_RunId_Get,\n+      XLA_FFI_INTERNAL_CalledComputation_Get,\n+      XLA_FFI_INTERNAL_ExecutionContext_Get,\n+      XLA_FFI_INTERNAL_ExecutionState_Get,\n+\n+      // XLA:CPU specific APIs.\n+      XLA_FFI_INTERNAL_IntraOpThreadPool_Get,\n+\n+      // XLA:GPU specific APIs.\n+      XLA_FFI_INTERNAL_Stream_Get,\n+      XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get,\n+  };\n+\n+  return &internal_api;\n+}\n+\n+}  // namespace xla::ffi::internal"
        },
        {
            "sha": "135388224cca4b5e2ea461fba34f85ad9f14bdfa",
            "filename": "third_party/xla/xla/ffi/ffi_internal_api.h",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.h?ref=c851c888f25518efce602d19e49549bb5cf45da4",
            "patch": "@@ -0,0 +1,28 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_FFI_FFI_INTERNAL_API_H_\n+#define XLA_FFI_FFI_INTERNAL_API_H_\n+\n+#include \"xla/ffi/api/c_api.h\"\n+\n+namespace xla::ffi::internal {\n+\n+// Returns a pointer to the implementation of the internal XLA FFI API.\n+const XLA_FFI_InternalApi* GetInternalApi();\n+\n+}  // namespace xla::ffi::internal\n+\n+#endif  // XLA_FFI_FFI_INTERNAL_API_H_"
        },
        {
            "sha": "fd09f6f9b2b210c8274853c011c45c83d5e616c4",
            "filename": "third_party/xla/xla/ffi/ffi_structs.h",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c851c888f25518efce602d19e49549bb5cf45da4/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h?ref=c851c888f25518efce602d19e49549bb5cf45da4",
            "patch": "@@ -0,0 +1,76 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_FFI_FFI_STRUCTS_H_\n+#define XLA_FFI_FFI_STRUCTS_H_\n+\n+#include <cstdint>\n+#include <variant>\n+\n+#include \"absl/status/status.h\"\n+#include \"xla/executable_run_options.h\"\n+#include \"xla/ffi/execution_context.h\"\n+#include \"xla/ffi/execution_state.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+#include \"xla/tsl/concurrency/chain.h\"\n+\n+//===----------------------------------------------------------------------===//\n+// Forward declare backend-specific types.\n+//===----------------------------------------------------------------------===//\n+\n+namespace Eigen {\n+struct ThreadPoolDevice;\n+}  // namespace Eigen\n+\n+namespace stream_executor {\n+class Stream;\n+class DeviceMemoryAllocator;\n+}  // namespace stream_executor\n+\n+//===----------------------------------------------------------------------===//\n+// XLA FFI C structs definition\n+//===----------------------------------------------------------------------===//\n+\n+struct XLA_FFI_Error {\n+  absl::Status status;\n+};\n+\n+struct XLA_FFI_Future {\n+  tsl::AsyncValueRef<tsl::Chain> async_value;\n+};\n+\n+struct XLA_FFI_ExecutionContext {\n+  struct CpuContext {\n+    const Eigen::ThreadPoolDevice* intra_op_thread_pool = nullptr;\n+  };\n+\n+  struct GpuContext {\n+    stream_executor::Stream* stream = nullptr;\n+    stream_executor::DeviceMemoryAllocator* allocator = nullptr;\n+  };\n+\n+  using BackendContext = std::variant<std::monostate, CpuContext, GpuContext>;\n+\n+  xla::RunId run_id = {};\n+  int32_t device_ordinal = -1;\n+  BackendContext backend_context = {};\n+\n+  const xla::HloComputation* called_computation = nullptr;\n+  const xla::ffi::ExecutionContext* execution_context = nullptr;\n+  xla::ffi::ExecutionState* execution_state = nullptr;\n+};\n+\n+#endif  // XLA_FFI_FFI_STRUCTS_H_"
        }
    ],
    "stats": {
        "total": 521,
        "additions": 336,
        "deletions": 185
    }
}