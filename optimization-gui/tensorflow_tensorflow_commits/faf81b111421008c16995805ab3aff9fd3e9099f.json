{
    "author": "kevinbchen",
    "message": "Use add output type for fallback bias type in FuseFullyConnectedAndAdd\n\nPreviously, the filter element type was used as the fallback type if the bias is of NoneType. If the filter type does not match the add type, this can lead to an unbroadcastable type for `new_bias` and downstream segfaults from empty/invalid types. Instead, let's just use the add output type.\n\nPiperOrigin-RevId: 804550625",
    "sha": "faf81b111421008c16995805ab3aff9fd3e9099f",
    "files": [
        {
            "sha": "7b5370c90242b46e2855077d607a6b45c567f9e6",
            "filename": "tensorflow/compiler/mlir/lite/tests/optimize.mlir",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/faf81b111421008c16995805ab3aff9fd3e9099f/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/faf81b111421008c16995805ab3aff9fd3e9099f/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir?ref=faf81b111421008c16995805ab3aff9fd3e9099f",
            "patch": "@@ -677,6 +677,21 @@ func.func @FuseFullyConnectedAddWithNoBias(%arg0: tensor<40x37xf32>, %arg1: tens\n   // CHECK: return %[[fc]]\n }\n \n+// CHECK-LABEL: @FuseFullyConnectedAddWithNoBias\n+func.func @FuseFullyConnectedAddWithNoBiasAndDifferentInputFilterType(%arg0: tensor<40x37xbf16>, %arg1: tensor<40x37xbf16>) -> tensor<40x40xf32> {\n+  %cst = \"tfl.no_value\"() {value} : () -> none\n+  %cst2 = arith.constant dense<2.0> : tensor<40xf32>\n+\n+  %0 = \"tfl.fully_connected\" (%arg0, %arg1, %cst) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<40x37xbf16>, tensor<40x37xbf16>, none) -> (tensor<40x40xf32>)\n+  %1 = \"tfl.add\"(%0, %cst2) {fused_activation_function = \"NONE\"} : (tensor<40x40xf32>, tensor<40xf32>) -> tensor<40x40xf32>\n+\n+  func.return %1 : tensor<40x40xf32>\n+\n+  // CHECK-DAG: %cst = arith.constant dense<2.000000e+00> : tensor<40xf32>\n+  // CHECK: %[[fc:.*]] = \"tfl.fully_connected\"(%arg0, %arg1, %cst)\n+  // CHECK: return %[[fc]]\n+}\n+\n // CHECK-LABEL: @FuseFullyConnectedAddWithNoBiasWithQDQs\n func.func @FuseFullyConnectedAddWithNoBiasWithQDQs(%arg0: tensor<40x37xf32>, %arg1: tensor<40x37xf32>) -> tensor<40x40xf32> {\n   %cst = \"tfl.no_value\"() {value} : () -> none"
        },
        {
            "sha": "6a116197e67043b388488e0716d2c2a218b3b2d4",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_pass.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 51,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/faf81b111421008c16995805ab3aff9fd3e9099f/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/faf81b111421008c16995805ab3aff9fd3e9099f/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc?ref=faf81b111421008c16995805ab3aff9fd3e9099f",
            "patch": "@@ -1247,16 +1247,17 @@ static std::optional<Value> GetAs1DValue(PatternRewriter& rewriter, Value value,\n }\n \n // Tries to get the given `bias` as a 1D tensor of `num_channels` elements.\n-// If `bias` is a `NoneType`, a 1D tensor of zeros is created.\n+// If `bias` is a `NoneType`, a 1D tensor of zeros is created with the given\n+// `fallback_element_type`.\n // Otherwise, it uses `GetAs1DValue` to handle scalar constants and other\n // broadcastable shapes.\n static std::optional<Value> GetBiasIn1D(PatternRewriter& rewriter, Value bias,\n                                         int num_channels,\n-                                        Type filter_element_type) {\n+                                        Type fallback_element_type) {\n   // If it's none, create a zero tensor with shape {num_channels}.\n   if (mlir::isa<NoneType>(bias.getType())) {\n     RankedTensorType type =\n-        RankedTensorType::get({num_channels}, filter_element_type);\n+        RankedTensorType::get({num_channels}, fallback_element_type);\n     auto attr = rewriter.getZeroAttr(type);\n     return rewriter.create<arith::ConstantOp>(bias.getLoc(), type, attr);\n   }\n@@ -1294,38 +1295,6 @@ static RankedTensorType GetRankedTensorType(Value value) {\n   return nullptr;\n }\n \n-// Gets the number of channels and filter element type for a FullyConnected op.\n-// This is used to determine the shape of the bias tensor when fusing an Add op.\n-// It first tries to get this information from the filter tensor. If the filter\n-// is unranked, it falls back to using the output tensor of the FullyConnected\n-// op.\n-static std::optional<std::pair<int, Type>> GetFcNumChannelsAndFilterType(\n-    TFL::FullyConnectedOp fc_op) {\n-  Value filter = fc_op.getFilter();\n-  if (auto filter_type = GetRankedTensorType(filter);\n-      filter_type && filter_type.getRank() == 2 &&\n-      !mlir::isa<quant::QuantizedType>(filter_type.getElementType())) {\n-    // Get the number of channels from the filter's shape if it's a ranked\n-    // 2D tensor. Filter must be a `2D` tensor with `{num_channels,\n-    // num_features}` shape.\n-    int num_channels = filter_type.getShape()[0];\n-    Type filter_element_type = filter_type.getElementType();\n-    return {{num_channels, filter_element_type}};\n-  }\n-\n-  // Fallback to using the FC op's output shape to determine the number of\n-  // channels. This is useful when the filter is unranked.\n-  auto fc_output_type =\n-      mlir::dyn_cast<RankedTensorType>(fc_op.getOutput()[0].getType());\n-  if (!fc_output_type || !fc_output_type.hasStaticShape() ||\n-      fc_output_type.getRank() == 0) {\n-    return std::nullopt;\n-  }\n-  int num_channels = fc_output_type.getShape().back();\n-  Type filter_element_type = fc_output_type.getElementType();\n-  return {{num_channels, filter_element_type}};\n-}\n-\n // Fuse Add with proceeding FullyConnected.\n // TODO(b/136285429): Move to tablegen when variadic is supported\n struct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n@@ -1369,22 +1338,6 @@ struct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n     ElementsAttr bias_value;\n     if (fc_op.getFusedActivationFunction() != \"NONE\") return failure();\n \n-    // Get the number of channels if possible.\n-    auto fc_info = GetFcNumChannelsAndFilterType(fc_op);\n-    if (!fc_info) {\n-      return failure();\n-    }\n-    const auto& [num_channels, filter_element_type] = *fc_info;\n-\n-    auto bias_1d =\n-        GetBiasIn1D(rewriter, bias, num_channels, filter_element_type);\n-    // Get the added value as a 1D tensor.\n-    auto add_rhs_1d = GetAs1DValue(rewriter, add_rhs, num_channels);\n-\n-    if (!bias_1d.has_value() || !add_rhs_1d.has_value()) {\n-      return failure();\n-    }\n-\n     auto fc_output_type =\n         mlir::dyn_cast<RankedTensorType>(fc_op.getOutput()[0].getType());\n     auto add_output_type =\n@@ -1398,6 +1351,31 @@ struct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n       return failure();\n     }\n \n+    // Get the number of output channels.\n+    if (fc_output_type.getShape().size() == 0) {\n+      return failure();\n+    }\n+    const int64_t num_channels = fc_output_type.getShape().back();\n+    if (::mlir::ShapedType::isDynamic(num_channels)) {\n+      return failure();\n+    }\n+\n+    auto bias_1d = GetBiasIn1D(rewriter, bias, num_channels,\n+                               add_output_type.getElementType());\n+    // Get the added value as a 1D tensor.\n+    auto add_rhs_1d = GetAs1DValue(rewriter, add_rhs, num_channels);\n+\n+    if (!bias_1d.has_value() || !add_rhs_1d.has_value()) {\n+      return failure();\n+    }\n+    // Sanity check that bias and add_rhs can be broadcasted together (shapes\n+    // should be broadcastable and element types must match).\n+    if (!IsBroadcastableElementsAttrAndType(bias_1d->getType(),\n+                                            add_rhs_1d->getType())) {\n+      return rewriter.notifyMatchFailure(\n+          add_op, \"Bias and add_rhs are not broadcastable\");\n+    }\n+\n     auto new_bias =\n         rewriter\n             .create<AddOp>(add_op.getLoc(), bias_1d.value(), add_rhs_1d.value(),"
        }
    ],
    "stats": {
        "total": 95,
        "additions": 44,
        "deletions": 51
    }
}