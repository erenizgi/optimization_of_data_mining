{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 846644862",
    "sha": "d68f58209eb731c3e298c2d487a3f3df5cd9ce6b",
    "files": [
        {
            "sha": "591bdb8a6ff33c2c677e5af43290e355848a7f5c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d68f58209eb731c3e298c2d487a3f3df5cd9ce6b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d68f58209eb731c3e298c2d487a3f3df5cd9ce6b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc?ref=d68f58209eb731c3e298c2d487a3f3df5cd9ce6b",
            "patch": "@@ -86,28 +86,30 @@ static void MakeTTGIR(mlir::OpPassManager* pm,\n     pm->addPass(mlir::createTritonAMDGPUScheduleLoops({num_stages}));\n     pm->addPass(mlir::createTritonAMDGPUPipeline(\n         {/*useAsyncCopy=*/false, /*usePingpong=*/false}));\n-    if (/*use_async_copy=*/false) {  // Not enabled by default.\n+    if (/*use_async_copy=*//* DISABLES CODE */ (\n+        false)) {  // Not enabled by default.\n       pm->addPass(mlir::createTritonAMDGPUCoalesceAsyncCopy());\n     }\n     pm->addPass(mlir::createCanonicalizerPass());\n   }\n-  if (/*(instruction_sched_variant==\"none\") == */ false) {\n+  if (/*(instruction_sched_variant==\"none\") == */ /* DISABLES CODE */ (false)) {\n     pm->addPass(mt::createTritonAMDGPUInsertInstructionSchedHintsPass(\"none\"));\n   }\n   pm->addPass(mt::gpu::createTritonGPUOptimizeDotOperands({true}));\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   pm->addPass(mt::gpu::createTritonGPUReduceDataDuplication());\n-  if (/*(instruction_sched_variant==\"none\") == */ false) {\n+  if (/*(instruction_sched_variant==\"none\") == */ /* DISABLES CODE */ (false)) {\n     pm->addPass(mlir::createTritonAMDGPUInThreadTranspose());\n     pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   }\n   if (rocm_cc.has_amd_matrix_instr()) {\n     pm->addPass(mt::gpu::createTritonGPUReorderInstructions());\n   }\n-  if (/*use_block_pingpong=*/false) {\n+  if (/*use_block_pingpong=*//* DISABLES CODE */ (false)) {\n     pm->addPass(mlir::createTritonAMDGPUBlockPingpong({num_stages}));\n   }\n-  if (/*use_buffer_ops=*/false) {  // Not enabled by default.\n+  if (/*use_buffer_ops=*//* DISABLES CODE */ (\n+      false)) {  // Not enabled by default.\n     pm->addPass(mlir::createTritonAMDGPUCanonicalizePointers());\n     pm->addPass(mlir::createCanonicalizerPass());\n     pm->addPass(mlir::createTritonAMDGPUConvertToBufferOps({arch_name}));\n@@ -140,7 +142,7 @@ static void MakeLLIR(mlir::OpPassManager* pm,\n   pm->addPass(mlir::createCanonicalizerPass());\n   pm->addPass(mlir::createCSEPass());\n   pm->addPass(mlir::createSymbolDCEPass());\n-  if (/*(instruction_sched_variant==\"none\") == */ false) {\n+  if (/*(instruction_sched_variant==\"none\") == */ /* DISABLES CODE */ (false)) {\n     pm->addPass(mt::createTritonAMDGPULowerInstructionSchedHintsPass(\n         rocm_cc.gfx_version(), num_stages));\n   }"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 8,
        "deletions": 6
    }
}