{
    "author": "ermilovmaxim",
    "message": "Add proto serialization for AllReduceStartThunk\n\nPiperOrigin-RevId: 845967844",
    "sha": "3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
    "files": [
        {
            "sha": "c9221c3ffb8ce358923146f4f3bb95c57ddf4a7f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
            "patch": "@@ -1400,7 +1400,9 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/backends/gpu/collectives:gpu_communicator\",\n         \"//xla/core/collectives:communicator\",\n+        \"//xla/core/collectives:reduction_kind\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n@@ -1409,15 +1411,32 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n         \"@local_tsl//tsl/platform:casts\",\n     ],\n )\n \n+xla_cc_test(\n+    name = \"all_reduce_thunk_test\",\n+    srcs = [\"all_reduce_thunk_test.cc\"],\n+    deps = [\n+        \":all_reduce_thunk\",\n+        \":collective_thunk\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"all_to_all_thunk\",\n     srcs = [\"all_to_all_thunk.cc\"],\n@@ -2864,6 +2883,7 @@ cc_library(\n     hdrs = [\"thunk_proto_deserialization.h\"],\n     deps = [\n         \":all_gather_thunk\",\n+        \":all_reduce_thunk\",\n         \":collective_thunk\",\n         \":conditional_thunk\",\n         \":convolution_reorder_thunk\","
        },
        {
            "sha": "4347670507e22078f86691adf10fb21e3bfc981a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 128,
            "deletions": 3,
            "changes": 131,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
            "patch": "@@ -22,17 +22,21 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n #include \"xla/backends/gpu/runtime/collective_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n@@ -46,11 +50,42 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla {\n namespace gpu {\n namespace {\n \n+ReductionKindProto ToReductionKindProto(ReductionKind kind) {\n+  switch (kind) {\n+    case ReductionKind::SUM:\n+      return REDUCTION_KIND_SUM;\n+    case ReductionKind::PRODUCT:\n+      return REDUCTION_KIND_PRODUCT;\n+    case ReductionKind::MIN:\n+      return REDUCTION_KIND_MIN;\n+    case ReductionKind::MAX:\n+      return REDUCTION_KIND_MAX;\n+  }\n+}\n+\n+absl::StatusOr<ReductionKind> FromReductionKindProto(\n+    const ReductionKindProto& proto) {\n+  switch (proto) {\n+    case REDUCTION_KIND_SUM:\n+      return ReductionKind::SUM;\n+    case REDUCTION_KIND_PRODUCT:\n+      return ReductionKind::PRODUCT;\n+    case REDUCTION_KIND_MIN:\n+      return ReductionKind::MIN;\n+    case REDUCTION_KIND_MAX:\n+      return ReductionKind::MAX;\n+    default:\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Unknown ReductionKindProto: \", proto));\n+  }\n+}\n+\n absl::Status CheckImplementableInst(const HloInstruction* inst,\n                                     Thunk::Kind reduction_op) {\n   for (HloInstruction* operand : inst->operands()) {\n@@ -120,14 +155,36 @@ AllReduceReduceScatterThunkBase::AllReduceReduceScatterThunkBase(\n   CHECK_EQ(config_.config.operand_element_type.size(), buffers_.size());\n }\n \n+AllReduceReduceScatterThunkBase::AllReduceReduceScatterThunkBase(\n+    Thunk::Kind kind, ThunkInfo thunk_info, AllReduceConfig config,\n+    std::vector<Buffer> buffers,\n+    std::shared_ptr<CollectiveThunk::AsyncEvents> async_events)\n+    : CollectiveThunk(kind, thunk_info, async_events,\n+                      AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE),\n+      config_(std::move(config)),\n+      buffers_(std::move(buffers)) {\n+  CHECK_EQ(config_.config.operand_element_type.size(), buffers_.size());\n+}\n+\n AllReduceStartThunk::AllReduceStartThunk(\n     ThunkInfo thunk_info, const HloAllReduceInstruction* inst,\n     std::vector<Buffer> buffers,\n     std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n     bool p2p_memcpy_enabled)\n-    : AllReduceReduceScatterThunkBase(\n-          Thunk::kAllReduceStart, thunk_info, GetAllReduceConfigInst(inst),\n-          std::move(buffers), IsGPUSyncCollective(*inst)),\n+    : AllReduceStartThunk(\n+          thunk_info, GetAllReduceConfigInst(inst), std::move(buffers),\n+          std::move(collective_kernel_thunk),\n+          IsGPUSyncCollective(*inst)\n+              ? nullptr\n+              : std::make_shared<CollectiveThunk::AsyncEvents>()) {}\n+\n+AllReduceStartThunk::AllReduceStartThunk(\n+    ThunkInfo thunk_info, const AllReduceConfig& config,\n+    std::vector<Buffer> buffers,\n+    std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n+    std::shared_ptr<CollectiveThunk::AsyncEvents> async_events)\n+    : AllReduceReduceScatterThunkBase(Thunk::kAllReduceStart, thunk_info,\n+                                      config, std::move(buffers), async_events),\n       collective_kernel_thunk_(std::move(collective_kernel_thunk)) {}\n \n absl::Status AllReduceStartThunk::CheckImplementable(\n@@ -188,6 +245,74 @@ absl::StatusOr<bool> AllReduceStartThunk::RunCollective(\n   return true;\n }\n \n+absl::StatusOr<std::unique_ptr<AllReduceStartThunk>>\n+AllReduceStartThunk::FromProto(\n+    ThunkInfo thunk_info, const AllReduceStartThunkProto& thunk_proto,\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    CollectiveThunk::AsyncEventsMap& async_events_map) {\n+  std::vector<CollectiveThunk::Buffer> buffers;\n+  buffers.reserve(thunk_proto.buffers_size());\n+  for (const CollectiveBufferProto& proto : thunk_proto.buffers()) {\n+    ASSIGN_OR_RETURN(\n+        CollectiveThunk::Buffer buffer,\n+        CollectiveThunk::Buffer::FromProto(proto, buffer_allocations));\n+    buffers.push_back(buffer);\n+  }\n+\n+  std::shared_ptr<CollectiveThunk::AsyncEvents>& async_events =\n+      async_events_map[AsyncEventsUniqueId{\n+          thunk_proto.async_events_unique_id()}];\n+  if (!async_events) {\n+    async_events = std::make_shared<CollectiveThunk::AsyncEvents>();\n+  }\n+\n+  CollectiveConfig config =\n+      CollectiveConfig::FromProto(thunk_proto.collective_config());\n+\n+  ASSIGN_OR_RETURN(ReductionKind reduction_kind,\n+                   FromReductionKindProto(thunk_proto.reduction_kind()));\n+\n+  auto kernel_thunk = std::make_unique<CollectiveKernelThunk>(\n+      thunk_info, config, reduction_kind, thunk_proto.is_async(), buffers,\n+      thunk_proto.collective_kernel_enabled(), thunk_proto.kernel_name(),\n+      thunk_proto.shmem_bytes(), thunk_proto.is_multimem_enabled());\n+\n+  return std::make_unique<AllReduceStartThunk>(\n+      std::move(thunk_info), AllReduceConfig{config, reduction_kind},\n+      std::move(buffers), std::move(kernel_thunk), async_events);\n+}\n+\n+absl::StatusOr<ThunkProto> AllReduceStartThunk::ToProto() const {\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+\n+  AllReduceStartThunkProto* thunk_proto =\n+      proto.mutable_all_reduce_start_thunk();\n+\n+  std::optional<AsyncEventsUniqueId> async_events_id = GetAsyncEventsUniqueId();\n+  if (!async_events_id.has_value()) {\n+    return absl::FailedPreconditionError(\"AsyncEvents is not set.\");\n+  }\n+  thunk_proto->set_async_events_unique_id(async_events_id->value());\n+\n+  for (const Buffer& buffer : buffers_) {\n+    ASSIGN_OR_RETURN(*thunk_proto->add_buffers(), buffer.ToProto());\n+  }\n+\n+  *thunk_proto->mutable_collective_config() = config_.config.ToProto();\n+  thunk_proto->set_reduction_kind(ToReductionKindProto(config_.reduction_kind));\n+\n+  thunk_proto->set_is_multimem_enabled(\n+      collective_kernel_thunk_->is_multimem_enabled());\n+  thunk_proto->set_shmem_bytes(collective_kernel_thunk_->shmem_bytes());\n+  thunk_proto->set_kernel_name(collective_kernel_thunk_->kernel_name());\n+  thunk_proto->set_collective_kernel_enabled(\n+      collective_kernel_thunk_->collective_kernel_enabled());\n+  thunk_proto->set_is_async(collective_kernel_thunk_->is_async());\n+\n+  return proto;\n+}\n+\n ReduceScatterStartThunk::ReduceScatterStartThunk(\n     ThunkInfo thunk_info, const HloReduceScatterInstruction* inst,\n     std::vector<Buffer> buffers, bool p2p_memcpy_enabled)"
        },
        {
            "sha": "88856dd6e608d5a3c3497cb2173922365fda8660",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.h",
            "status": "modified",
            "additions": 21,
            "deletions": 2,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h?ref=3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
            "patch": "@@ -22,12 +22,15 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/collective_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/stream.h\"\n \n@@ -48,6 +51,10 @@ class AllReduceReduceScatterThunkBase : public CollectiveThunk {\n   AllReduceReduceScatterThunkBase(Kind kind, ThunkInfo thunk_info,\n                                   AllReduceConfig config,\n                                   std::vector<Buffer> buffers, bool is_sync);\n+  AllReduceReduceScatterThunkBase(\n+      Kind kind, ThunkInfo thunk_info, AllReduceConfig config,\n+      std::vector<Buffer> buffers,\n+      std::shared_ptr<CollectiveThunk::AsyncEvents> async_events);\n \n   const CollectiveConfig& config() const override { return config_.config; }\n   ReductionKind reduction_kind() const { return config_.reduction_kind; }\n@@ -70,8 +77,13 @@ class AllReduceStartThunk : public AllReduceReduceScatterThunkBase {\n       std::vector<Buffer> buffers,\n       std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n       bool p2p_memcpy_enabled = false);\n+  AllReduceStartThunk(\n+      ThunkInfo thunk_info, const AllReduceConfig& config,\n+      std::vector<Buffer> buffers,\n+      std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n+      std::shared_ptr<CollectiveThunk::AsyncEvents> async_events);\n \n-  static const char* GetHloOpName() { return \"all-reduce-start\"; }\n+  static absl::string_view GetHloOpName() { return \"all-reduce-start\"; }\n \n   static absl::Status CheckImplementable(const HloAllReduceInstruction* inst,\n                                          int64_t replica_count,\n@@ -83,6 +95,13 @@ class AllReduceStartThunk : public AllReduceReduceScatterThunkBase {\n   absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n \n+  static absl::StatusOr<std::unique_ptr<AllReduceStartThunk>> FromProto(\n+      ThunkInfo thunk_info, const AllReduceStartThunkProto& thunk_proto,\n+      absl::Span<const BufferAllocation> buffer_allocations,\n+      CollectiveThunk::AsyncEventsMap& async_events_map);\n+\n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n  protected:\n   absl::StatusOr<bool> RunCollective(const ExecuteParams& params,\n                                      const GpuCliqueKey& clique_key,\n@@ -104,7 +123,7 @@ class ReduceScatterStartThunk : public AllReduceReduceScatterThunkBase {\n                           std::vector<Buffer> buffers,\n                           bool p2p_memcpy_enabled = false);\n \n-  static const char* GetHloOpName() { return \"reduce-scatter-start\"; }\n+  static absl::string_view GetHloOpName() { return \"reduce-scatter-start\"; }\n \n   static absl::Status CheckImplementable(\n       const HloReduceScatterInstruction* inst, int64_t replica_count,"
        },
        {
            "sha": "96c258b30d762359dc4ec86fc4ba34201048ed5b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk_test.cc",
            "status": "added",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk_test.cc?ref=3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
            "patch": "@@ -0,0 +1,74 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::tsl::proto_testing::EqualsProto;\n+\n+TEST(CollectiveThunkTest, ProtoRoundTrip) {\n+  ThunkProto proto = tsl::proto_testing::ParseTextProtoOrDie<ThunkProto>(\n+      R\"pb(\n+        thunk_info {\n+          profile_annotation: \"partition_id_profile_annotation\"\n+          execution_stream_id: 2\n+        }\n+        all_reduce_start_thunk {\n+          async_events_unique_id: 3\n+          collective_config {}\n+          reduction_kind: 1\n+        }\n+      )pb\");\n+\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.profile_annotation = proto.thunk_info().profile_annotation();\n+  thunk_info.execution_stream_id = xla::gpu::ExecutionStreamId{\n+      static_cast<xla::gpu::ExecutionStreamId::ValueType>(\n+          proto.thunk_info().execution_stream_id())};\n+\n+  CollectiveThunk::AsyncEventsMap async_events_map;\n+  std::vector<BufferAllocation> buffer_allocations = {\n+      BufferAllocation(/*index=*/0, /*size=*/4, /*color=*/0)};\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<AllReduceStartThunk> thunk,\n+      AllReduceStartThunk::FromProto(thunk_info, proto.all_reduce_start_thunk(),\n+                                     buffer_allocations, async_events_map));\n+  ASSERT_NE(thunk->async_events(), nullptr);\n+\n+  ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n+\n+  // Ids are unique and expected to differ.\n+  proto.mutable_all_reduce_start_thunk()->set_async_events_unique_id(\n+      round_trip_proto.all_reduce_start_thunk().async_events_unique_id());\n+  EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "2d7de400a4cb1df88c79f52762a401dbe2e175d0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
            "patch": "@@ -78,6 +78,15 @@ class CollectiveKernelThunk : public Thunk {\n     per_stream_state_.reserve(kMaxNumExecutors);\n   }\n \n+  bool is_multimem_enabled() const { return is_multimem_enabled_; }\n+\n+  int32_t shmem_bytes() const { return shmem_bytes_; }\n+\n+  absl::string_view kernel_name() const { return kernel_name_; }\n+\n+  bool collective_kernel_enabled() const { return collective_kernel_enabled_; }\n+  bool is_async() const { return is_async_; }\n+\n   // Returns true if the collective kernel is supported for the given clique.\n   absl::StatusOr<bool> IsSupported(\n       const GpuCliqueKey& clique_key, se::StreamExecutor& executor,"
        },
        {
            "sha": "eb564b5e4eb3762f96599cf34711cba49a72daed",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
            "patch": "@@ -405,6 +405,27 @@ message AllGatherStartThunkProto {\n   repeated CollectiveBufferProto buffers = 3;\n }\n \n+enum ReductionKindProto {\n+  REDUCTION_KIND_UNSPECIFIED = 0;\n+  REDUCTION_KIND_SUM = 1;\n+  REDUCTION_KIND_PRODUCT = 2;\n+  REDUCTION_KIND_MIN = 3;\n+  REDUCTION_KIND_MAX = 4;\n+}\n+\n+message AllReduceStartThunkProto {\n+  uint64 async_events_unique_id = 1;\n+  CollectiveConfigProto collective_config = 2;\n+  repeated CollectiveBufferProto buffers = 3;\n+\n+  ReductionKindProto reduction_kind = 4;\n+  bool is_multimem_enabled = 5;\n+  int32 shmem_bytes = 6;\n+  string kernel_name = 7;\n+  bool collective_kernel_enabled = 8;\n+  bool is_async = 9;\n+}\n+\n message CollectiveDoneThunkProto {\n   ThunkKindProto thunk_kind = 1;\n   AsyncStreamKind async_stream_kind = 2;\n@@ -451,6 +472,7 @@ message ThunkProto {\n     CustomKernelThunkProto custom_kernel_thunk = 36;\n     CollectiveDoneThunkProto collective_done_thunk = 37;\n     AllGatherStartThunkProto all_gather_start_thunk = 38;\n+    AllReduceStartThunkProto all_reduce_start_thunk = 39;\n   }\n }\n "
        },
        {
            "sha": "a4c8bbdd7e9e51b4423f137b05cbcf2591c3aca9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3015ca53fe15b4ccb9b61db19ff7895533bfcb88/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc?ref=3015ca53fe15b4ccb9b61db19ff7895533bfcb88",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"google/protobuf/descriptor.h\"\n #include \"google/protobuf/message.h\"\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n+#include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n #include \"xla/backends/gpu/runtime/convolution_reorder_thunk.h\"\n@@ -247,6 +248,10 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProtoImpl(\n       return AllGatherStartThunk::FromProto(\n           std::move(thunk_info), thunk_proto.all_gather_start_thunk(),\n           buffer_allocations, collective_async_events_map);\n+    case ThunkProto::kAllReduceStartThunk:\n+      return AllReduceStartThunk::FromProto(\n+          std::move(thunk_info), thunk_proto.all_reduce_start_thunk(),\n+          buffer_allocations, collective_async_events_map);\n     default:\n       std::optional<absl::string_view> unsupported_thunk_type =\n           GetStoredThunkTypeName(thunk_proto);"
        }
    ],
    "stats": {
        "total": 284,
        "additions": 279,
        "deletions": 5
    }
}