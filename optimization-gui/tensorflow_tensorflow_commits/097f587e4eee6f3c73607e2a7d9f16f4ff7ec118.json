{
    "author": "Moerafaat",
    "message": "[XLA:GPU/WS] Adding test coverage for auto warp specialization via Triton.\n\nPiperOrigin-RevId: 820637611",
    "sha": "097f587e4eee6f3c73607e2a7d9f16f4ff7ec118",
    "files": [
        {
            "sha": "1c3efc425122bd2dd70269f2bf156f39bbc24668",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/097f587e4eee6f3c73607e2a7d9f16f4ff7ec118/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/097f587e4eee6f3c73607e2a7d9f16f4ff7ec118/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=097f587e4eee6f3c73607e2a7d9f16f4ff7ec118",
            "patch": "@@ -499,6 +499,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -857,10 +858,10 @@ xla_test(\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n-        \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/service:algorithm_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\","
        },
        {
            "sha": "7f671c90a8a598dd20519c5b25a93228c17c08cb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 4,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/097f587e4eee6f3c73607e2a7d9f16f4ff7ec118/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/097f587e4eee6f3c73607e2a7d9f16f4ff7ec118/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=097f587e4eee6f3c73607e2a7d9f16f4ff7ec118",
            "patch": "@@ -15,13 +15,10 @@ limitations under the License.\n \n #include <array>\n #include <cstdint>\n-#include <memory>\n #include <ostream>\n #include <string>\n-#include <tuple>\n #include <utility>\n #include <variant>\n-#include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n@@ -47,13 +44,13 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n-#include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/algorithm_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/shape.h\"\n@@ -116,6 +113,17 @@ INSTANTIATE_TEST_SUITE_P(TmaParameterizedTritonEmitterTestSuite,\n                            return info.param ? \"tma_allowed\" : \"tma_disabled\";\n                          });\n \n+class WarpSpecializationTritonEmitterTest : public TritonEmitterTest {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = TritonEmitterTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_experimental_enable_triton_tma(true);\n+    debug_options.set_xla_gpu_experimental_enable_triton_warp_specialization(\n+        true);\n+    return debug_options;\n+  }\n+};\n+\n struct TmaAndDotLayoutTestParams {\n   std::vector<int64_t> lhs_layout;\n   std::vector<int64_t> rhs_layout;\n@@ -3244,6 +3252,72 @@ CHECK-COUNT-1: triton_xla.insert\n       hlo_text, ErrorSpec{/*aabs=*/1e-4, /*arel=*/1e-6}));\n }\n \n+TEST_F(WarpSpecializationTritonEmitterTest,\n+       DotAccumulationLoopUsesWarpSpecialization) {\n+  if (!GetCudaComputeCapability().IsAtLeastBlackwell()) {\n+    GTEST_SKIP() << \"Currently only supported on Blackwell and newer.\";\n+  }\n+\n+  const std::string hlo_text = R\"(\n+flhs {\n+  ROOT flhs.p0 = f16[256,256] parameter(0)\n+}\n+\n+frhs {\n+  ROOT frhs.p0 = f16[256,256] parameter(0)\n+}\n+\n+fdot {\n+  fdot.p0 = f16[256,256] parameter(0)\n+  fdot.p1 = f16[256,256] parameter(1)\n+  fdot.lhs = f16[256,256] fusion(fdot.p0), kind=kCustom, calls=flhs, backend_config={\n+    \"fusion_backend_config\":{\n+      \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n+        \"output_tiles\":[{\"sizes\":[\"128\", \"64\"]}],\n+        \"is_tma_allowed\":\"1\"\n+      }\n+    }\n+  }\n+  fdot.rhs = f16[256,256]{1,0} fusion(fdot.p1), kind=kCustom, calls=frhs, backend_config={\n+    \"fusion_backend_config\":{\n+      \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n+        \"output_tiles\":[{\"sizes\":[\"64\", \"128\"]}],\n+        \"is_tma_allowed\":\"1\"\n+      }\n+    }\n+  }\n+  ROOT fdot.root = f16[256,256]{1,0} dot(fdot.lhs, fdot.rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    algorithm=dot_f16_f16_f32\n+}\n+\n+ENTRY entry {\n+  entry.p0 = f16[256,256] parameter(0)\n+  entry.p1 = f16[256,256] parameter(1)\n+  ROOT fusion = f16[256,256] fusion(entry.p0, entry.p1),\n+    kind=kCustom, calls=fdot, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"128\"]}],\n+          \"num_warps\":\"8\",\n+          \"num_ctas\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"is_tma_allowed\":\"1\"}}}\n+})\";\n+\n+  // Check that the IR attribute is set correctly.\n+  TF_EXPECT_OK(CreateTritonIrAndFileCheck(this, hlo_text, \"fdot\", R\"(\n+  // CHECK:       scf.for\n+  // CHECK:       scf.yield\n+  // CHECK-NEXT:  tt.warp_specialize\n+  // )\"));\n+\n+  // Make sure it runs correctly.\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      hlo_text, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n TEST_F(TritonEmitterTest, MaskedDotIsEmittedCorrectly) {\n   const std::string kHloText = R\"(\n flhs {"
        },
        {
            "sha": "65f34ad79ecd7f9aec3e46f44ece0962a4221e5b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 99,
            "deletions": 2,
            "changes": 101,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/097f587e4eee6f3c73607e2a7d9f16f4ff7ec118/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/097f587e4eee6f3c73607e2a7d9f16f4ff7ec118/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=097f587e4eee6f3c73607e2a7d9f16f4ff7ec118",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -43,6 +44,8 @@ namespace {\n using ::tsl::testing::IsOkAndHolds;\n using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n \n+using TritonEmitterDevicelessTest = HloHardwareIndependentTestBase;\n+\n class AnnotationsTest : public HloHardwareIndependentTestBase {\n  public:\n   DebugOptions GetDebugOptionsForTest() const override {\n@@ -53,6 +56,18 @@ class AnnotationsTest : public HloHardwareIndependentTestBase {\n   }\n };\n \n+class WarpSpecializationTritonEmitterTest : public TritonEmitterDevicelessTest {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options =\n+        TritonEmitterDevicelessTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_experimental_enable_triton_tma(true);\n+    debug_options.set_xla_gpu_experimental_enable_triton_warp_specialization(\n+        true);\n+    return debug_options;\n+  }\n+};\n+\n TEST_F(AnnotationsTest, Annotations) {\n   static constexpr absl::string_view kHloText = R\"(\n HloModule Annotations\n@@ -111,8 +126,6 @@ ENTRY e {\n   }\n }\n \n-using TritonEmitterDevicelessTest = HloHardwareIndependentTestBase;\n-\n TEST_F(TritonEmitterDevicelessTest, FailsGracefullyIfNumWarpsIsMissing) {\n   constexpr absl::string_view kHloText = R\"(\n triton_computation {\n@@ -155,5 +168,89 @@ ENTRY entry {\n                       \"(num_warps, num_ctas, num_stages) must be positive\")));\n }\n \n+TEST_F(WarpSpecializationTritonEmitterTest,\n+       ExtraWarpsAreRequestedForWarpSpecialization) {\n+  const std::string hlo_text = R\"(\n+flhs {\n+  ROOT flhs.p0 = f16[256,256] parameter(0)\n+}\n+\n+frhs {\n+  ROOT frhs.p0 = f16[256,256] parameter(0)\n+}\n+\n+fdot {\n+  fdot.p0 = f16[256,256] parameter(0)\n+  fdot.p1 = f16[256,256] parameter(1)\n+  fdot.lhs = f16[256,256] fusion(fdot.p0), kind=kCustom, calls=flhs, backend_config={\n+    \"fusion_backend_config\":{\n+      \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n+        \"output_tiles\":[{\"sizes\":[\"128\", \"64\"]}],\n+        \"is_tma_allowed\":\"1\"\n+      }\n+    }\n+  }\n+  fdot.rhs = f16[256,256]{1,0} fusion(fdot.p1), kind=kCustom, calls=frhs, backend_config={\n+    \"fusion_backend_config\":{\n+      \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n+        \"output_tiles\":[{\"sizes\":[\"64\", \"128\"]}],\n+        \"is_tma_allowed\":\"1\"\n+      }\n+    }\n+  }\n+  ROOT fdot.root = f16[256,256]{1,0} dot(fdot.lhs, fdot.rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    algorithm=dot_f16_f16_f32\n+}\n+\n+ENTRY entry {\n+  entry.p0 = f16[256,256] parameter(0)\n+  entry.p1 = f16[256,256] parameter(1)\n+  ROOT fusion = f16[256,256] fusion(entry.p0, entry.p1),\n+    kind=kCustom, calls=fdot, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"128\"]}],\n+          \"num_warps\":\"8\",\n+          \"num_ctas\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"is_tma_allowed\":\"1\"}}}\n+})\";\n+\n+  // Check that we extract the launch configuration correctly when warp\n+  // specialization is used.\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo_text));\n+  auto* fusion = Cast<HloFusionInstruction>(\n+      module->entry_computation()->root_instruction());\n+  const se::DeviceDescription dev_info =\n+      TestGpuDeviceInfo::RTXB200SXMDeviceInfo();\n+  llvm::LLVMContext llvm_ctx;\n+  llvm::Module llvm_module(\"module\", llvm_ctx);\n+  mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      TritonWrapperResult result,\n+      TritonWrapper(\"test_fn\", fusion, se::CudaComputeCapability::Blackwell(),\n+                    dev_info,\n+                    BlockLevelParameters::FromBlockLevelFusionConfig(\n+                        fusion->backend_config<GpuBackendConfig>()\n+                            ->fusion_backend_config()\n+                            .block_level_fusion_config()),\n+                    &llvm_module, symbolic_expr_context));\n+\n+  // Warp specialization influences the total number of threads we end up\n+  // using. Usually we would expect num_warps * warp_size threads per block, but\n+  // Triton allocates extra \"worker warps\" when WS is used.\n+  //\n+  // NOTE: The value used here is based on inspecting the value in the IR.\n+  // Hopefully this is stable across different Triton versions. If it starts\n+  // failing, we could modify the value here to match and try to understand why\n+  // it changed.\n+  EXPECT_EQ(result.thread_dims.x, 384);\n+  EXPECT_EQ(result.thread_dims.y, 1);\n+  EXPECT_EQ(result.thread_dims.z, 1);\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 186,
        "additions": 179,
        "deletions": 7
    }
}