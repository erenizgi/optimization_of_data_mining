{
    "author": "loislo",
    "message": "[XLA:GPU] fine tune the xla part of the scaled-dot op implementation in order to support the case when we omit one of the scales and pass the bf16 argument instead.\n\nWe adjusted the emitter for the case when the scale is missing.\nAlso we relaxed the hlo verifier a bit and tweaked the composite rewriter that should accept the dim indexes passed by jax.\n\nPiperOrigin-RevId: 822036474",
    "sha": "0836518bc56e7514cf78f76975a4ad06002bec1d",
    "files": [
        {
            "sha": "3a52e65c8adc8b53e4a7f3b1a585106c9ad959ff",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -409,11 +409,9 @@ cc_library(\n     hdrs = [\"dot_algorithms.h\"],\n     deps = [\n         \":emitter_helpers\",\n-        \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:algorithm_util\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "ba28872670817a1deec83d3d76c22dfac8809fee",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -40,9 +40,6 @@ limitations under the License.\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/utils/hlo_traversal.h\"\n-#include \"xla/primitive_util.h\"\n #include \"xla/service/algorithm_util.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -142,6 +139,9 @@ absl::StatusOr<ttir::ScaleDotElemType> GetScaleDotElemType(Type value) {\n   if (type == mlir::Float4E2M1FNType::get(value.getContext())) {\n     return ttir::ScaleDotElemType::E2M1;\n   }\n+  if (type == mlir::BFloat16Type::get(value.getContext())) {\n+    return ttir::ScaleDotElemType::BF16;\n+  }\n   return absl::InvalidArgumentError(\n       absl::StrCat(\"Unsupported type: \", llvm_ir::DumpToString(type)));\n }\n@@ -153,12 +153,16 @@ absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n   TF_ASSIGN_OR_RETURN(auto rhs_dot_elem_type,\n                       GetScaleDotElemType(operands.rhs.getType()));\n \n-  auto lhs_scale = Bitcast(b, operands.lhs_scale, b.getI8Type());\n-  auto rhs_scale = Bitcast(b, operands.rhs_scale, b.getI8Type());\n-\n-  // TODO(b/436988479): Remove this once we have a fix for the scaled dot\n-  // rewrite on the Triton side. With this transpose we have matching numerics.\n-  rhs_scale = b.create<ttir::TransOp>(rhs_scale, mlir::ArrayRef<int32_t>{1, 0});\n+  Value lhs_scale;\n+  if (lhs_dot_elem_type != ttir::ScaleDotElemType::BF16) {\n+    lhs_scale = Bitcast(b, operands.lhs_scale, b.getI8Type());\n+  }\n+  Value rhs_scale;\n+  if (rhs_dot_elem_type != ttir::ScaleDotElemType::BF16) {\n+    rhs_scale = Bitcast(b, operands.rhs_scale, b.getI8Type());\n+    rhs_scale =\n+        b.create<ttir::TransOp>(rhs_scale, mlir::ArrayRef<int32_t>{1, 0});\n+  }\n \n   // make type with the same shape as the scale but with i8 type\n   return b.create<ttir::DotScaledOp>("
        },
        {
            "sha": "e5771cc81db1e732ced605cda614dad89a5769e4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 15,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -865,11 +865,11 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n // Returns `shape` without all its unit dimensions, as well as the index of the\n // remaining dimensions in the original `shape`.\n std::pair<SmallVector<int64_t>, SmallVector<int64_t>> CollapseUnitDims(\n-    llvm::ArrayRef<int64_t> shape) {\n+    llvm::ArrayRef<int64_t> shape, llvm::ArrayRef<int64_t> counterpart_shape) {\n   SmallVector<int64_t> shape_without_unit_dims;\n   SmallVector<int64_t> non_unit_dims_indices;\n   for (auto [i, size] : llvm::enumerate(shape)) {\n-    if (size != 1) {\n+    if (size != 1 || size != counterpart_shape[i]) {\n       shape_without_unit_dims.push_back(size);\n       non_unit_dims_indices.push_back(i);\n     }\n@@ -884,15 +884,28 @@ enum class DotOperandSide { kLhs, kRhs };\n // the given side (the second dimension for LHS, the first dimension for the\n // RHS).\n //\n+// If it is a scaled-dot scale operand then we drop the extra dims only\n+// when they equal to 1  and are matching with the corresponding operand.\n+// Example:\n+//   when lhs_scale operand with shape [1,128, 1] (passed as operand parameter)\n+//   and lhs operand with shape [1,128, 32] (passed as counterpart_operand\n+//   parameter)\n+//   the function will drop only the first dim and will keep the last one\n+//   because the last one of the lhs operand is not equal to 1.\n+//\n // Returns an error if canonicalization is not possible.\n-absl::StatusOr<Value> CanonicalizeDotOperand(EmitterLocOpBuilder b,\n-                                             Value operand,\n-                                             int64_t contracting_dim_idx,\n-                                             DotOperandSide side) {\n+absl::StatusOr<Value> CanonicalizeDotOperand(\n+    EmitterLocOpBuilder b, Value operand, int64_t contracting_dim_idx,\n+    DotOperandSide side, Value counterpart_operand = nullptr) {\n   llvm::ArrayRef<int64_t> shape =\n       mlir::cast<ShapedType>(operand.getType()).getShape();\n+  llvm::ArrayRef<int64_t> counterpart_shape =\n+      counterpart_operand == nullptr\n+          ? shape\n+          : mlir::cast<ShapedType>(counterpart_operand.getType()).getShape();\n+\n   auto [shape_without_unit_dims, non_unit_dims_indices] =\n-      CollapseUnitDims(shape);\n+      CollapseUnitDims(shape, counterpart_shape);\n \n   if (shape_without_unit_dims.size() != 2) {\n     return absl::FailedPreconditionError(\n@@ -970,7 +983,7 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n       GetPaddedTileSizes(tiled_hlo_dot.tile_sizes());\n \n   SmallVector<int64_t, 2> padded_tile_sizes_no_unit_dims =\n-      CollapseUnitDims(padded_tile_sizes).first;\n+      CollapseUnitDims(padded_tile_sizes, padded_tile_sizes).first;\n \n   // Sanity check: Triton historically did not support non-2D dots (and still\n   // doesn't support arbitrary nD dots), so we require that the dot is tiled\n@@ -1113,7 +1126,7 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n       GetPaddedTileSizes(tiled_hlo_dot.tile_sizes());\n \n   SmallVector<int64_t, 2> padded_tile_sizes_no_unit_dims =\n-      CollapseUnitDims(padded_tile_sizes).first;\n+      CollapseUnitDims(padded_tile_sizes, padded_tile_sizes).first;\n \n   // Sanity check: Triton historically did not support non-2D dots (and still\n   // doesn't support arbitrary nD dots), so we require that the dot is tiled\n@@ -1202,18 +1215,19 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n \n     // Canonicalize the dot operands to match Triton's/the hardware's\n     // expectations.\n+\n+    TF_ASSIGN_OR_RETURN(\n+        lhs_scale, CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n+                                          DotOperandSide::kLhs, lhs));\n+    TF_ASSIGN_OR_RETURN(\n+        rhs_scale, CanonicalizeDotOperand(b, rhs_scale, rhs_contracting_dim_idx,\n+                                          DotOperandSide::kRhs, rhs));\n     TF_ASSIGN_OR_RETURN(lhs,\n                         CanonicalizeDotOperand(b, lhs, lhs_contracting_dim_idx,\n                                                DotOperandSide::kLhs));\n     TF_ASSIGN_OR_RETURN(rhs,\n                         CanonicalizeDotOperand(b, rhs, rhs_contracting_dim_idx,\n                                                DotOperandSide::kRhs));\n-    TF_ASSIGN_OR_RETURN(\n-        lhs_scale, CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n-                                          DotOperandSide::kLhs));\n-    TF_ASSIGN_OR_RETURN(\n-        rhs_scale, CanonicalizeDotOperand(b, rhs_scale, rhs_contracting_dim_idx,\n-                                          DotOperandSide::kRhs));\n \n     TF_ASSIGN_OR_RETURN(\n         Value acc_next,"
        },
        {
            "sha": "8f2347660368204e8548f5137916f22bf7d3b3b2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 7,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -4761,19 +4761,20 @@ class TritonScaledDotTest : public TritonEmitterTest {\n   }\n };\n \n-TEST_F(TritonScaledDotTest, ScaledDotWithBatchGetFusedAndExecutedCorrectly) {\n+TEST_F(TritonScaledDotTest,\n+       ScaledDotWithOmmittedLhsScaleGetFusedAndExecutedCorrectly) {\n   if (!GetCudaComputeCapability().IsAtLeastHopper()) {\n     GTEST_SKIP() << \"Skipping test for pre-Hopper GPUs.\";\n   }\n   constexpr absl::string_view kHloTextTemplate = R\"hlo(\n-HloModule ScaledDotWithBatchGetFusedAndExecutedCorrectly\n+HloModule ScaledDotWithOmmittedLhsScaleGetFusedAndExecutedCorrectly\n \n ENTRY e {\n-  lhs = f8e4m3fn[3,128,128] parameter(0)\n+  lhs = bf16[3,128,128] parameter(0)\n   rhs = f8e4m3fn[3,128,128] parameter(1)\n-  lhs_scale = f8e8m0fnu[3,128,4] parameter(2)\n-  rhs_scale = f8e8m0fnu[3,128,4 ] parameter(3)\n-  ROOT _ = bf16[3,128,128] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n+  constant = bf16[1,1,1] constant(1.0)\n+  rhs_scale = f8e8m0fnu[3,128,4] parameter(2)\n+  ROOT _ = bf16[3,128,128] scaled-dot(lhs, rhs, constant, rhs_scale),\n     lhs_batch_dims={0},\n     rhs_batch_dims={0},\n     lhs_contracting_dims={2},\n@@ -4805,7 +4806,7 @@ ENTRY e {\n       *optimized_module, HloOpcode::kScaledDot);\n   constexpr absl::string_view kExpectedTritonIr = R\"(\n       CHECK: tt.dot_scaled\n-      CHECK: tensor<16x128xf8E4M3FN>, tensor<16x4xi8>\n+      CHECK: tensor<16x128xbf16>\n       CHECK: tensor<128x16xf8E4M3FN>, tensor<16x4xi8>\n       CHECK: -> tensor<16x16xf32>\n   )\";\n@@ -4825,6 +4826,62 @@ ENTRY e {\n       std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n+TEST_F(TritonScaledDotTest, ScaledDotWithBatchGetFusedAndExecutedCorrectly) {\n+  if (!GetCudaComputeCapability().IsAtLeastHopper()) {\n+    GTEST_SKIP() << \"Skipping test for pre-Hopper GPUs.\";\n+  }\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule ScaledDotWithBatchGetFusedAndExecutedCorrectly\n+\n+ENTRY e {\n+  lhs = f8e4m3fn[3,128,128] parameter(0)\n+  rhs = f8e4m3fn[3,128,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[3,128,4] parameter(2)\n+  rhs_scale = f8e8m0fnu[3,128,4 ] parameter(3)\n+  ROOT _ = bf16[3,128,128] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n+    lhs_batch_dims={0},\n+    rhs_batch_dims={0},\n+    lhs_contracting_dims={2},\n+    rhs_contracting_dims={2}\n+}\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloTextTemplate));\n+  TF_ASSERT_OK_AND_ASSIGN(auto optimized_module,\n+                          GetOptimizedModule(std::move(module)));\n+  constexpr absl::string_view kExpectedOptimizedHLO = R\"(\n+    CHECK: fusion\n+    CHECK: ROOT {{.*}} scaled-dot\n+    CHECK: ENTRY\n+    CHECK: __triton_nested_gemm_fusion\n+  )\";\n+  EXPECT_THAT(RunFileCheck(optimized_module->ToString(), kExpectedOptimizedHLO),\n+              true);\n+\n+  HloComputation* scaled_dot_computation = GetFirstComputationWithInstruction(\n+      *optimized_module, HloOpcode::kScaledDot);\n+  constexpr absl::string_view kExpectedTritonIr = R\"(\n+      CHECK: tt.dot_scaled\n+      CHECK: tensor<16x128xf8E4M3FN>, tensor<16x4xi8>\n+      CHECK: tensor<128x16xf8E4M3FN>, tensor<16x4xi8>\n+      CHECK: -> tensor<16x16xf32>\n+  )\";\n+  EXPECT_THAT(CreateTritonIrAndFileCheck(*scaled_dot_computation,\n+                                         /*block_level_parameters=*/\n+                                         {\n+                                             {{1, 16, 16}},\n+                                             4,\n+                                             1,\n+                                             1,\n+                                             false,\n+                                         },\n+                                         kExpectedTritonIr),\n+              absl_testing::IsOk());\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "d84bc02f4f87b97b9707b6f62028315351e56313",
            "filename": "third_party/xla/xla/service/gpu/split_k_gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -894,22 +894,24 @@ ENTRY %entry_computation {\n                               m::Op().WithShape(F8E8M0FNU, {32, 3, 32}))));\n }\n \n-TEST_F(SplitKTest, ScaledDot_LhsOnly) {\n+// TODO(b/436988479): Re-enable when split-K is fixed for scaled dots.\n+TEST_F(SplitKTest, DISABLED_ScaledDot_LhsOnly) {\n   const std::string hlo_text = R\"(\n triton_gemm_dot {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  rhs = f8e5m2[32,128] parameter(1)\n+  rhs = bf16[32,128] parameter(1)\n   lhs_scale = f8e8m0fnu[16,4] parameter(2)\n-  rhs_scale = f8e5m2[] constant(1.0)\n+  rhs_scale = bf16[1,1] parameter(3)\n   ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n ENTRY %entry_computation {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  rhs = f8e5m2[32,128] parameter(1)\n+  rhs = bf16[32,128] parameter(1)\n   lhs_scale = f8e8m0fnu[16,4] parameter(2)\n-  ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale),\n+  rhs_scale = bf16[1,1] constant(1.0)\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale, rhs_scale),\n       kind=kCustom, calls=triton_gemm_dot\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n@@ -924,27 +926,29 @@ ENTRY %entry_computation {\n       GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n   EXPECT_THAT(dot_fusion->called_computations()[0]->root_instruction(),\n               GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 512}),\n-                                      m::Op().WithShape(F8E5M2, {32, 3, 512}),\n+                                      m::Op().WithShape(BF16, {32, 3, 512}),\n                                       m::Op().WithShape(F8E8M0FNU, {16, 3, 16}),\n-                                      m::Op().WithShape(F8E5M2, {}))));\n+                                      m::Op().WithShape(BF16, {1, 1, 1}))));\n }\n \n-TEST_F(SplitKTest, ScaledDot_RhsOnly) {\n+// TODO(b/436988479): Re-enable once the split-K is fixed for scaled dots.\n+TEST_F(SplitKTest, DISABLED_ScaledDot_RhsOnly) {\n   const std::string hlo_text = R\"(\n triton_gemm_dot {\n-  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs = bf16[16,128] parameter(0)\n   rhs = f8e5m2[32,128] parameter(1)\n-  lhs_scale = f8e4m3fn[] constant(1.0)\n-  rhs_scale = f8e8m0fnu[32,4] parameter(2)\n+  lhs_scale = bf16[1,1] parameter(2)\n+  rhs_scale = f8e8m0fnu[32,4] parameter(3)\n   ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n ENTRY %entry_computation {\n-  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs = bf16[16,128] parameter(0)\n   rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = bf16[1,1] constant(1.0)\n   rhs_scale = f8e8m0fnu[32,4] parameter(2)\n-  ROOT fusion = f32[16,32] fusion(lhs, rhs, rhs_scale),\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale, rhs_scale),\n       kind=kCustom, calls=triton_gemm_dot\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n@@ -959,9 +963,9 @@ ENTRY %entry_computation {\n       GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n   EXPECT_THAT(\n       dot_fusion->called_computations()[0]->root_instruction(),\n-      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 512}),\n+      GmockMatch(m::ScaledDot(m::Op().WithShape(BF16, {16, 3, 512}),\n                               m::Op().WithShape(F8E5M2, {32, 3, 512}),\n-                              m::Op().WithShape(F8E4M3FN, {}),\n+                              m::Op().WithShape(BF16, {1, 1, 1}),\n                               m::Op().WithShape(F8E8M0FNU, {32, 3, 16}))));\n }\n "
        },
        {
            "sha": "f1dae687a10aade79a1b09c931aa552dd37d0524",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -2441,8 +2441,14 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:AsmParser\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Parser\",\n+        \"@llvm-project//mlir:Support\",\n     ],\n )\n "
        },
        {
            "sha": "653470b6b097b6b0dda24fd0aaa9b5b98740abc6",
            "filename": "third_party/xla/xla/service/gpu/transforms/composite_rewriter.cc",
            "status": "modified",
            "additions": 69,
            "deletions": 9,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -15,10 +15,20 @@ limitations under the License.\n \n #include \"xla/service/gpu/transforms/composite_rewriter.h\"\n \n+#include <cstdint>\n+\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/AsmParser/AsmParser.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/Parser/Parser.h\"\n+#include \"mlir/Support/LLVM.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -33,6 +43,54 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+namespace {\n+\n+absl::StatusOr<DotDimensionNumbers> ParseDimensionNumbers(\n+    absl::string_view composite_attributes) {\n+  mlir::MLIRContext context;\n+  mlir::Attribute attr = mlir::parseAttribute(composite_attributes, &context);\n+  mlir::DictionaryAttr dict_attrs = mlir::dyn_cast<mlir::DictionaryAttr>(attr);\n+  auto get_int = [&](absl::string_view key) -> absl::StatusOr<int32_t> {\n+    if (!dict_attrs.contains(key)) {\n+      return absl::InvalidArgumentError(absl::StrCat(key, \" is not set\"));\n+    }\n+    mlir::Attribute attr = dict_attrs.get(key);\n+    if (!mlir::isa<mlir::IntegerAttr>(attr)) {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(key, \" is not an integer\"));\n+    }\n+    return mlir::cast<mlir::IntegerAttr>(attr).getInt();\n+  };\n+  DotDimensionNumbers dot_dimension_numbers;\n+  TF_ASSIGN_OR_RETURN(int32_t lhs_contracting_dim_index,\n+                      get_int(\"lhs_contracting_dim_index\"));\n+  dot_dimension_numbers.add_lhs_contracting_dimensions(\n+      lhs_contracting_dim_index);\n+  TF_ASSIGN_OR_RETURN(int32_t rhs_contracting_dim_index,\n+                      get_int(\"rhs_contracting_dim_index\"));\n+  dot_dimension_numbers.add_rhs_contracting_dimensions(\n+      rhs_contracting_dim_index);\n+\n+  if (dict_attrs.contains(\"lhs_batch_dim_index\")) {\n+    TF_ASSIGN_OR_RETURN(int32_t lhs_batch_dim_index,\n+                        get_int(\"lhs_batch_dim_index\"));\n+    dot_dimension_numbers.add_lhs_batch_dimensions(lhs_batch_dim_index);\n+  }\n+  if (dict_attrs.contains(\"rhs_batch_dim_index\")) {\n+    TF_ASSIGN_OR_RETURN(int32_t rhs_batch_dim_index,\n+                        get_int(\"rhs_batch_dim_index\"));\n+    dot_dimension_numbers.add_rhs_batch_dimensions(rhs_batch_dim_index);\n+  }\n+  if (dot_dimension_numbers.lhs_batch_dimensions_size() !=\n+      dot_dimension_numbers.rhs_batch_dimensions_size()) {\n+    return absl::InvalidArgumentError(\n+        \"batch dimension should be specified for both lhs and rhs.\");\n+  }\n+  return dot_dimension_numbers;\n+}\n+\n+}  // namespace\n+\n absl::StatusOr<bool> CompositeRewriter::RewriteComputation(\n     HloComputation* computation) {\n   bool changed = false;\n@@ -48,18 +106,20 @@ absl::StatusOr<bool> CompositeRewriter::RewriteComputation(\n       VLOG(3) << \"No frontend attributes\";\n       continue;\n     }\n-    auto attrs = call->frontend_attributes().map();\n+    auto frontend_attrs = call->frontend_attributes().map();\n     auto key = \"composite.name\";\n-    if (!attrs.contains(key) || attrs.at(key) != \"xla.scaled_dot\") {\n-      VLOG(3) << key << \" is not xla.scaled_dot: \" << attrs.at(key);\n+    if (!frontend_attrs.contains(key) ||\n+        frontend_attrs.at(key) != \"xla.scaled_dot\") {\n+      VLOG(3) << key << \" is not xla.scaled_dot: \" << frontend_attrs.at(key);\n       continue;\n     }\n-    DotDimensionNumbers dot_dimension_numbers;\n-    dot_dimension_numbers.add_lhs_contracting_dimensions(2);\n-    dot_dimension_numbers.add_rhs_contracting_dimensions(2);\n-    dot_dimension_numbers.add_lhs_batch_dimensions(0);\n-    dot_dimension_numbers.add_rhs_batch_dimensions(0);\n-\n+    if (!frontend_attrs.contains(\"composite.attributes\")) {\n+      return absl::InvalidArgumentError(\n+          \"composite.attributes is not set for xla.scaled_dot\");\n+    }\n+    TF_ASSIGN_OR_RETURN(\n+        DotDimensionNumbers dot_dimension_numbers,\n+        ParseDimensionNumbers(frontend_attrs.at(\"composite.attributes\")));\n     auto* scaled_dot =\n         computation->AddInstruction(HloInstruction::CreateScaledDot(\n             call->shape(), call->mutable_operand(0), call->mutable_operand(1),"
        },
        {
            "sha": "bc5953bb3837d28bb7d062ed9f9fa94d5563d017",
            "filename": "third_party/xla/xla/service/gpu/transforms/composite_rewriter_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -63,7 +63,13 @@ TEST(CompositeRewriterTest, ScaledDotCompositeRewrite) {\n           to_apply=%xla.scaled_dot.1,\n           is_composite=true,\n           frontend_attributes={\n-            composite.attributes={preferred_element_type = bf16},\n+            composite.attributes={\n+              preferred_element_type = bf16,\n+              lhs_contracting_dim_index = 2 : i64,\n+              rhs_contracting_dim_index = 1 : i64,\n+              lhs_batch_dim_index = 0 : i64,\n+              rhs_batch_dim_index = 0 : i64\n+            },\n             composite.name=\"xla.scaled_dot\",\n             composite.version=\"1\"\n           }"
        },
        {
            "sha": "b37c6c820fa51af9247be4cb0996b909e4e3e896",
            "filename": "third_party/xla/xla/service/hlo_verifier.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 35,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -218,41 +218,22 @@ absl::Status ShapeVerifier::HandleRaggedDot(HloInstruction* ragged_dot) {\n absl::StatusOr<bool> IsNoOpScale(const HloInstruction* dot,\n                                  const HloInstruction* operand,\n                                  const HloInstruction* scale_operand) {\n-  // It should be a constant scalar.\n-  if (!ShapeUtil::IsScalar(scale_operand->shape()) ||\n-      scale_operand->opcode() != HloOpcode::kConstant) {\n+  // It should have the same type as the operand, and the shape should have the\n+  // same rank as the operand but with dim sizes equal to 1.\n+  const Shape& shape = scale_operand->shape();\n+  if (shape.element_type() != operand->shape().element_type()) {\n     return false;\n   }\n-  // If the scale operand is a constant, it must be a scalar of the same type\n-  // as the operand.\n-  if (scale_operand->shape().element_type() !=\n-      operand->shape().element_type()) {\n-    return absl::FailedPreconditionError(absl::StrFormat(\n-        \"Dummy scale operand '%s' has a different type than operand '%s'. %s \"\n-        \"vs %s in %s\",\n-        scale_operand->name(), operand->name(),\n-        PrimitiveType_Name(scale_operand->shape().element_type()),\n-        PrimitiveType_Name(operand->shape().element_type()), dot->ToString()));\n-  }\n-  auto constant = Cast<HloConstantInstruction>(scale_operand);\n-\n-  // If the element type is float, the scale must be 1.0.\n-  if (primitive_util::IsFloatingPointType(operand->shape().element_type())) {\n-    if (!constant->literal().IsAllFloat(1.0)) {\n-      return absl::FailedPreconditionError(absl::StrFormat(\n-          \"Dummy scale operand %s of %s is not a scalar with value 1.0\",\n-          scale_operand->name(), dot->ToString()));\n-    }\n-    return true;  // Dummy constant scale equal to 1.0 with float type found.\n-  }\n-\n-  // If the element type is not float, the scale must be 1.\n-  if (!constant->literal().IsAll(1)) {\n-    return absl::FailedPreconditionError(absl::StrFormat(\n-        \"Dummy scale operand %s of %s is not a constant with value 1\",\n-        scale_operand->name(), dot->ToString()));\n+  if (operand->shape().element_type() != BF16) {\n+    return false;\n   }\n-  return true;  // Dummy constant scale equal to 1 with integer type found.\n+  // It might be enough to check the types only but for now let's check the\n+  // shape as well.\n+  return std::all_of(shape.dimensions().begin(), shape.dimensions().end(),\n+                     [](int64_t dim) { return dim == 1; }) &&\n+         std::any_of(operand->shape().dimensions().begin(),\n+                     operand->shape().dimensions().end(),\n+                     [](int64_t dim) { return dim != 1; });\n }\n \n absl::Status ScalesShapeVerifier(\n@@ -290,9 +271,10 @@ absl::Status ScalesShapeVerifier(\n   for (int i = 0; i < operand_dims.size(); ++i) {\n     if (operand_dims[i] % scale_operand_dims[i]) {\n       return absl::FailedPreconditionError(absl::StrFormat(\n-          \"Dimension %d of operand %s should be a multiple of dimension \"\n-          \"%d of scale operand %s in %s\",\n-          i, operand->name(), i, scale_operand->name(), dot->ToString()));\n+          \"Dimension %d of operand \\n%s\\n should be a multiple of dimension \"\n+          \"%d of scale operand \\n%s\\n in %s\",\n+          i, operand->ToString(), i, scale_operand->ToString(),\n+          dot->ToString()));\n     }\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "a92b8a098f55cae8cf83c8292753ec25535085e1",
            "filename": "third_party/xla/xla/service/hlo_verifier_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -4941,10 +4941,10 @@ TEST_F(HloVerifierTest, ScaledDotWithNoScalesFails) {\n   static constexpr absl::string_view kScaledDotHloString = R\"(\n     HloModule module\n     ENTRY entry_computation {\n-      a = f32[2,10] parameter(0)\n-      b = f32[10,2] parameter(1)\n-      a_scale = f32[] constant(1)\n-      b_scale = f32[] constant(1)\n+      a = bf16[2,10] parameter(0)\n+      b = bf16[10,2] parameter(1)\n+      a_scale = bf16[] constant(1)\n+      b_scale = bf16[] constant(1)\n       ROOT dot = f32[2,2] scaled-dot(a, b, a_scale, b_scale),\n         lhs_contracting_dims={1},\n         rhs_contracting_dims={0}"
        },
        {
            "sha": "da55a029f45840a462a710282f7cf1758b5eb298",
            "filename": "third_party/xla/xla/service/matmul_indexing_utils.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fmatmul_indexing_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0836518bc56e7514cf78f76975a4ad06002bec1d/third_party%2Fxla%2Fxla%2Fservice%2Fmatmul_indexing_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmatmul_indexing_utils.cc?ref=0836518bc56e7514cf78f76975a4ad06002bec1d",
            "patch": "@@ -60,7 +60,11 @@ absl::StatusOr<std::vector<int64_t>> GetNonContractingDims(\n                                          contracting_dims, batch_dims);\n \n   TF_RET_CHECK(batch_dims.size() + contracting_dims.size() + nc.size() ==\n-               shape.dimensions().size());\n+               shape.dimensions().size())\n+      << \"batch_dims: \" << batch_dims.size()\n+      << \" contracting_dims: \" << contracting_dims.size()\n+      << \" nc: \" << nc.size()\n+      << \" vs shape dims size: \" << shape.dimensions().size();\n   return std::vector<int64_t>(nc.begin(), nc.end());\n }\n \n@@ -130,15 +134,13 @@ absl::StatusOr<std::array<DotOperandDims, 4>> DotOperandDims::FromScaledDot(\n     const HloInstruction* scaled_dot) {\n   TF_ASSIGN_OR_RETURN(auto lhs_dims, FromDotOperand(scaled_dot, 0));\n   DotOperandDims lhs_scale_dims;\n-  if (scaled_dot->operand(2)->opcode() != HloOpcode::kConstant ||\n-      !scaled_dot->operand(2)->shape().dimensions().empty()) {\n+  if (!ShapeUtil::IsScalar(scaled_dot->operand(2)->shape())) {\n     TF_ASSIGN_OR_RETURN(lhs_scale_dims, FromDotOperand(scaled_dot, 2));\n   }\n \n   TF_ASSIGN_OR_RETURN(auto rhs_dims, FromDotOperand(scaled_dot, 1));\n   DotOperandDims rhs_scale_dims;\n-  if (scaled_dot->operand(3)->opcode() != HloOpcode::kConstant ||\n-      !scaled_dot->operand(3)->shape().dimensions().empty()) {\n+  if (!ShapeUtil::IsScalar(scaled_dot->operand(3)->shape())) {\n     TF_ASSIGN_OR_RETURN(rhs_scale_dims, FromDotOperand(scaled_dot, 3));\n   }\n "
        }
    ],
    "stats": {
        "total": 337,
        "additions": 235,
        "deletions": 102
    }
}