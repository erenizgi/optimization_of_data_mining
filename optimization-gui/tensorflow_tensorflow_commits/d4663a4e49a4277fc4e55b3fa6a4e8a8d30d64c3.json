{
    "author": "othakkar",
    "message": "PR #30997: [XLA:CPU][oneDNN] Enable oneDNN MatMul Custom Calls in Thunk Runtime\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30997\n\nThis PR enables support for oneDNN MatMul operations in the XLA:CPU Thunk runtime, building upon the foundational implementation of `OneDnnOpThunk`.\n\nKey changes:\n- Added `onednn_op_thunk.cc` and `onednn_op_thunk.h` implementing the base `OneDnnOpThunk`.\n- Extended `onednn_memory_util.h` with `OneDnnResources` to manage primitives and memory objects across async execution.\n- Updated thunk emitter to emit `OneDnnOpThunk` for supported oneDNN custom call ops during compilation.\n- Enabled oneDNN custom call rewrite for MatMul in `cpu_compiler.cc` and added support for oneDNN MatMul op via `ExecuteOneDnnMatMul(...)`.\n- Updated Bazel build rules to register new library and test targets.\nCopybara import of the project:\n\n--\na58ea8fdb017e02fd6143d926d4b38126a397a81 by Om Thakkar <om.thakkar@intel.com>:\n\nenable oneDNN matmul custom call in thunk runtime\n\n--\nd3ae43b1b1b79267f1fbd0e749492ad9192b850c by Om Thakkar <om.thakkar@intel.com>:\n\ndefine and use OpBuffers struct inside OneDnnOpThunk\n\n--\nefe3773787ea8c95eae227f729596efc7849502a by Om Thakkar <om.thakkar@intel.com>:\n\nsimplify is_onednn_compatible\n\n--\na6885dce5679c2b872bad9884237717b90540c42 by Om Thakkar <om.thakkar@intel.com>:\n\naddressing nit review comments\n\n--\n78df281880be29b2f0a4b54cd6669057255a77f9 by Om Thakkar <om.thakkar@intel.com>:\n\nremove serialization and de-serialization of backend configs\n\n--\n3a6115e023522f75be248a1dc1183f75660d1535 by Om Thakkar <om.thakkar@intel.com>:\n\ncoding style checks\n\nMerging this change closes #30997\n\nPiperOrigin-RevId: 806906547",
    "sha": "d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
    "files": [
        {
            "sha": "673440d5ff7d81b78c5f4d3bb8282a114846bbe2",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/BUILD",
            "status": "modified",
            "additions": 52,
            "deletions": 1,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -1,10 +1,12 @@\n+load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\"//xla/tsl:tsl.bzl\", \"tsl_copts\")\n load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n load(\n     \"//xla/tsl/mkl:graph.bzl\",\n     \"onednn_graph_cc_library\",\n     \"onednn_graph_cc_test\",\n )\n+load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -76,9 +78,10 @@ onednn_graph_cc_library(\n     ],\n )\n \n-onednn_graph_cc_library(\n+cc_library(\n     name = \"onednn_threadpool\",\n     hdrs = [\"onednn_threadpool.h\"],\n+    # copybara:uncomment compatible_with = [\"//buildenv/target:non_prod\"],\n     deps = [\n         \":onednn_interop\",\n         \"//xla/backends/cpu/runtime:work_queue\",\n@@ -110,3 +113,51 @@ onednn_graph_cc_test(\n         \"@eigen_archive//:eigen3\",\n     ],\n )\n+\n+cc_library(\n+    name = \"onednn_op_thunk\",\n+    srcs = [\"onednn_op_thunk.cc\"],\n+    hdrs = [\"onednn_op_thunk.h\"],\n+    copts = tsl_copts(),\n+    visibility = [\"//visibility:public\"],\n+    deps = [\n+        \":onednn_threadpool\",\n+        \"//xla:status_macros\",\n+        \"//xla/backends/cpu/runtime:thunk\",\n+        \"//xla/runtime:buffer_use\",\n+        \"//xla/runtime:object_pool\",\n+        \"//xla/service/cpu:onednn_matmul\",\n+        \"//xla/service/cpu:onednn_memory_util\",\n+        \"//xla/service/cpu:onednn_util\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/tsl/concurrency:async_value\",\n+        \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/functional:function_ref\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"onednn_op_thunk_test\",\n+    srcs = [\"onednn_op_thunk_test.cc\"],\n+    copts = tsl_copts(),\n+    fail_if_no_test_linked = False,  # NOLINT=No tests if we don't build with oneDNN.\n+    fail_if_no_test_selected = False,  # NOLINT=No tests to select if we don't build with oneDNN.\n+    deps = [\n+        \":onednn_op_thunk\",\n+        \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n+        \"//xla/backends/cpu/runtime:thunk_testlib\",\n+        \"//xla/tsl/concurrency:async_value\",\n+        \"//xla/tsl/platform:env\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@eigen_archive//:eigen3\",\n+    ],\n+)"
        },
        {
            "sha": "bea843b17d12476160d5d11b7a8e500eab0f18bb",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.cc",
            "status": "added",
            "additions": 196,
            "deletions": 0,
            "changes": 196,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -0,0 +1,196 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifdef INTEL_MKL\n+\n+#include \"xla/backends/cpu/runtime/onednn/onednn_op_thunk.h\"\n+\n+#include <cstddef>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/base/call_once.h\"\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/functional/function_ref.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/memory/memory.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/cpu/runtime/onednn/onednn_threadpool.h\"\n+#include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/runtime/buffer_use.h\"\n+#include \"xla/service/cpu/onednn_matmul.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+#include \"xla/tsl/platform/logging.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::cpu {\n+\n+// oneDNN runtime instantiated for the oneDNN operation.\n+struct OneDnnOpThunk::OneDnnRuntime {\n+  OneDnnRuntime(Eigen::ThreadPoolInterface* thread_pool);\n+\n+  OneDnnRuntime(OneDnnRuntime&&) = default;\n+  OneDnnRuntime& operator=(OneDnnRuntime&&) = default;\n+\n+  tsl::AsyncValueRef<OneDnnOpThunk::ExecuteEvent> Invoke(\n+      Eigen::ThreadPoolInterface* thread_pool,\n+      absl::Span<MemrefInfoHandler> arguments,\n+      absl::Span<MemrefInfoHandler> results,\n+      const OneDnnOpThunk::OneDnnOpConfig& config, const std::string& target);\n+\n+  std::unique_ptr<OneDnnThreadPool> threadpool;\n+\n+  dnnl::engine cpu_engine;\n+  dnnl::stream onednn_stream;\n+  // We initialize the resources struct here to default values, so that we can\n+  // keep the primitive and memory objects alive for the duration of the\n+  // runtime. Otherwise, they would be destroyed as soon as we exit the\n+  // ExecuteOneDnn<primitive> method. This is a requirement of\n+  // oneDNN library's asynchronous execution model.\n+  OneDnnResources resources;\n+};\n+\n+OneDnnOpThunk::OneDnnRuntime::OneDnnRuntime(\n+    Eigen::ThreadPoolInterface* thread_pool)\n+    : threadpool(\n+          std::make_unique<OneDnnThreadPool>(thread_pool, /*is_async=*/true)),\n+      cpu_engine(dnnl::engine::kind::cpu, 0),\n+      onednn_stream(\n+          dnnl::threadpool_interop::make_stream(cpu_engine, threadpool.get())),\n+      resources() {}\n+\n+tsl::AsyncValueRef<OneDnnOpThunk::ExecuteEvent>\n+OneDnnOpThunk::OneDnnRuntime::Invoke(\n+    Eigen::ThreadPoolInterface* thread_pool,\n+    absl::Span<MemrefInfoHandler> arguments,\n+    absl::Span<MemrefInfoHandler> results,\n+    const OneDnnOpThunk::OneDnnOpConfig& config, const std::string& target) {\n+  // Update threadpool\n+  threadpool->set_thread_pool(thread_pool);\n+\n+  // TODO(intel-tf): Add support for more oneDNN operations as needed.\n+  static absl::once_flag log_once_flag;\n+  absl::call_once(log_once_flag, [&] {\n+    VLOG(0) << absl::StreamFormat(\n+        \"Executing oneDNN thunk with target `%s`: num_args=%d, num_results=%d\",\n+        target, arguments.size(), results.size());\n+  });\n+\n+  if (target == \"__onednn$matmul\") {\n+    const auto& matmul_config = std::get<OneDnnMatMulConfig>(config);\n+    ExecuteOneDnnMatMul(arguments, results, matmul_config, cpu_engine,\n+                        onednn_stream, resources);\n+  } else {\n+    return absl::InvalidArgumentError(\n+        absl::StrFormat(\"Unsupported oneDNN operation target: `%s`\", target));\n+  }\n+\n+  return threadpool->done_event();\n+}\n+\n+absl::StatusOr<std::unique_ptr<OneDnnOpThunk>> OneDnnOpThunk::Create(\n+    const std::string& custom_call_target, Info info, OpBuffers buffers,\n+    OneDnnOpConfig config) {\n+  return absl::WrapUnique(new OneDnnOpThunk(std::move(custom_call_target),\n+                                            std::move(info), std::move(buffers),\n+                                            std::move(config)));\n+}\n+\n+OneDnnOpThunk::OneDnnOpThunk(const std::string& custom_call_target, Info info,\n+                             OpBuffers buffers, OneDnnOpConfig config)\n+    : Thunk(Thunk::Kind::kCustomCall, std::move(info)),\n+      op_buffers_(std::move(buffers)),\n+      config_(std::move(config)),\n+      target_(custom_call_target) {}\n+\n+OneDnnOpThunk::~OneDnnOpThunk() = default;\n+\n+OneDnnOpThunk::BufferUses OneDnnOpThunk::buffer_uses() const {\n+  BufferUses buffer_uses;\n+  for (const auto& argument : op_buffers_.arguments_buffers) {\n+    buffer_uses.emplace_back(argument, BufferUse::kRead);\n+  }\n+  for (const auto& result : op_buffers_.results_buffers) {\n+    buffer_uses.emplace_back(result, BufferUse::kWrite);\n+  }\n+  return buffer_uses;\n+}\n+\n+tsl::AsyncValueRef<OneDnnOpThunk::ExecuteEvent> OneDnnOpThunk::Execute(\n+    const ExecuteParams& params) {\n+  Eigen::ThreadPoolInterface* thread_pool =\n+      params.intra_op_threadpool->getPool();\n+  DCHECK(thread_pool != nullptr) << \"Thread pool must not be null\";\n+\n+  // Create oneDNN runtime for the operation.\n+  auto runtime = std::make_unique<OneDnnRuntime>(thread_pool);\n+\n+  // Resolve device memory for arguments.\n+  int64_t num_operands = op_buffers_.arguments_shapes.size();\n+  runtime->resources.arg_memrefs.reserve(num_operands);\n+  for (size_t i = 0; i < num_operands; ++i) {\n+    const auto& shape = op_buffers_.arguments_shapes[i];\n+    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase arg,\n+                        params.buffer_allocations->GetDeviceAddress(\n+                            op_buffers_.arguments_buffers[i]));\n+\n+    ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(arg.opaque(), arg.size());\n+    VLOG(3) << absl::StreamFormat(\n+        \"  arg: %s (%p)\", op_buffers_.arguments_shapes[i].ToString(true),\n+        arg.opaque());\n+\n+    auto memref = CreateMemrefFromShape(shape, arg.opaque());\n+    runtime->resources.arg_memrefs.push_back(std::move(memref));\n+  }\n+\n+  // Resolve device memory for results.\n+  int64_t num_results = op_buffers_.results_shapes.size();\n+  runtime->resources.result_memrefs.reserve(num_results);\n+  for (size_t i = 0; i < num_results; ++i) {\n+    const auto& shape = op_buffers_.results_shapes[i];\n+    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase res,\n+                        params.buffer_allocations->GetDeviceAddress(\n+                            op_buffers_.results_buffers[i]));\n+\n+    ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(res.opaque(), res.size());\n+    VLOG(3) << absl::StreamFormat(\"  res: %s (%p)\",\n+                                  op_buffers_.results_shapes[i].ToString(true),\n+                                  res.opaque());\n+\n+    auto memref = CreateMemrefFromShape(shape, res.opaque());\n+    runtime->resources.result_memrefs.push_back(std::move(memref));\n+  }\n+\n+  auto executed = runtime->Invoke(\n+      thread_pool, absl::MakeSpan(runtime->resources.arg_memrefs),\n+      absl::MakeSpan(runtime->resources.result_memrefs), config_, target_);\n+\n+  // Do not return runtime to the pool until the execution is done.\n+  executed.AndThen([runtime = std::move(runtime)]() {\n+    // runtime will be destroyed here when going out of scope.\n+    VLOG(3) << \"OneDnnOpThunk execution completed and destroying runtime now.\";\n+  });\n+\n+  return executed;\n+}\n+\n+}  // namespace xla::cpu\n+\n+#endif  // INTEL_MKL"
        },
        {
            "sha": "ec666c02de776ea5169f78812519d9db11924305",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.h",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -0,0 +1,76 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_ONEDNN_ONEDNN_OP_THUNK_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_ONEDNN_ONEDNN_OP_THUNK_H_\n+\n+#ifdef INTEL_MKL\n+\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/runtime/object_pool.h\"\n+#include \"xla/service/cpu/onednn_memory_util.h\"\n+#include \"xla/service/cpu/onednn_util.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+\n+namespace xla::cpu {\n+\n+class OneDnnOpThunk : public Thunk {\n+ public:\n+  ~OneDnnOpThunk() override;\n+\n+  // Buffer allocation slices and shapes.\n+  struct OpBuffers {\n+    std::vector<BufferAllocation::Slice> arguments_buffers;\n+    std::vector<Shape> arguments_shapes;\n+\n+    std::vector<BufferAllocation::Slice> results_buffers;\n+    std::vector<Shape> results_shapes;\n+    bool is_tuple_result;\n+  };\n+\n+  // Variant config for supported oneDNN ops.\n+  // TODO(intel-tf): Add more oneDNN operation configs as needed.\n+  using OneDnnOpConfig = std::variant<OneDnnMatMulConfig>;\n+\n+  static absl::StatusOr<std::unique_ptr<OneDnnOpThunk>> Create(\n+      const std::string& custom_call_target, Info info, OpBuffers buffers,\n+      OneDnnOpConfig config);\n+\n+  tsl::AsyncValueRef<ExecuteEvent> Execute(const ExecuteParams& params) final;\n+\n+  BufferUses buffer_uses() const final;\n+\n+ private:\n+  OneDnnOpThunk(const std::string& custom_call_target, Info info,\n+                OpBuffers buffers, OneDnnOpConfig config);\n+\n+  // oneDNN runtime instantiated for the oneDNN operation.\n+  struct OneDnnRuntime;\n+\n+  OpBuffers op_buffers_;\n+  OneDnnOpConfig config_;\n+  std::string target_;\n+};\n+\n+}  // namespace xla::cpu\n+\n+#endif  // INTEL_MKL\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_ONEDNN_ONEDNN_OP_THUNK_H_"
        },
        {
            "sha": "17ca132322a27123dfc9807e1d988178a243dbd9",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk_test.cc",
            "status": "added",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -0,0 +1,112 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifdef INTEL_MKL\n+\n+#include \"xla/backends/cpu/runtime/onednn/onednn_op_thunk.h\"\n+\n+#include \"gtest/gtest.h\"\n+#include \"xla/backends/cpu/runtime/thunk_testlib.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/concurrency/async_value_ref.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n+\n+#define EIGEN_USE_THREADS\n+#include \"unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace xla::cpu {\n+namespace {\n+using dnnl::engine;\n+using dnnl::stream;\n+\n+TEST(OneDnnOpThunkTest, SimpleOneDnnMatMulThunk) {\n+  // Set up a thread pool for parallel execution\n+  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 8);\n+  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n+                                 threads.NumThreads());\n+\n+  // Define shapes for lhs (2x3), rhs (3x2), and output (2x2)\n+  Shape lhs_shape = ShapeUtil::MakeShape(F32, {2, 3});\n+  Shape rhs_shape = ShapeUtil::MakeShape(F32, {3, 2});\n+  Shape out_shape = ShapeUtil::MakeShape(F32, {2, 2});\n+\n+  // Prepare dummy data (row-major):\n+  // A = [ [1, 2, 3],\n+  //       [4, 5, 6] ]\n+  // B = [ [7,  8],\n+  //       [9, 10],\n+  //       [11,12] ]\n+  // C = A * B =\n+  //     [ [58, 64],\n+  //       [139,154] ]\n+  std::vector<float> input_a = {1.f, 2.f, 3.f, 4.f, 5.f, 6.f};\n+  std::vector<float> input_b = {7.f, 8.f, 9.f, 10.f, 11.f, 12.f};\n+  std::vector<float> output(4, 0.f);\n+\n+  // Create Literals from data\n+  Literal lhs_literal = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{1.f, 2.f, 3.f}, {4.f, 5.f, 6.f}}));\n+  Literal rhs_literal = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{7.f, 8.f}, {9.f, 10.f}, {11.f, 12.f}}));\n+  Literal out_literal = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{0.f, 0.f}, {0.f, 0.f}}));\n+\n+  // Create buffer allocations and slices\n+  auto lhs_alloc = CreateBufferAllocation(0, lhs_literal);\n+  auto rhs_alloc = CreateBufferAllocation(1, rhs_literal);\n+  auto out_alloc = CreateBufferAllocation(2, out_literal);\n+\n+  auto lhs_slice = CreateBufferAllocationSlice(lhs_alloc);\n+  auto rhs_slice = CreateBufferAllocationSlice(rhs_alloc);\n+  auto out_slice = CreateBufferAllocationSlice(out_alloc);\n+\n+  BufferAllocations allocations =\n+      CreateBufferAllocations(lhs_literal, rhs_literal, out_literal);\n+\n+  // Set up op_buffers\n+  OneDnnOpThunk::OpBuffers op_buffers;\n+  op_buffers.arguments_buffers = {lhs_slice, rhs_slice};\n+  op_buffers.arguments_shapes = {lhs_shape, rhs_shape};\n+  op_buffers.results_buffers = {out_slice};\n+  op_buffers.results_shapes = {out_shape};\n+\n+  // Create thunk (matmul)\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto thunk,\n+      OneDnnOpThunk::Create(\"__onednn$matmul\", Thunk::Info(), op_buffers, {}));\n+\n+  // Set up execute params\n+  Thunk::ExecuteParams params;\n+  params.buffer_allocations = &allocations;\n+  params.intra_op_threadpool = &device;\n+\n+  // Execute the thunk\n+  auto exec_event = thunk->Execute(params);\n+  tsl::BlockUntilReady(exec_event);\n+  ASSERT_FALSE(exec_event.IsError()) << \"OneDnnOpThunk execution failed\";\n+\n+  // Expected output literal\n+  Literal expected = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{58.f, 64.f}, {139.f, 154.f}}));\n+\n+  // Load output and verify\n+  EXPECT_EQ(out_literal, expected);\n+}\n+\n+}  // namespace\n+}  // namespace xla::cpu\n+\n+#endif  // INTEL_MKL"
        },
        {
            "sha": "cedc1abbe92b52ff03ca532d630ca0ba369c6364",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -928,6 +928,7 @@ cc_library(\n     name = \"thunk_emitter\",\n     srcs = [\"thunk_emitter.cc\"],\n     hdrs = [\"thunk_emitter.h\"],\n+    copts = tsl_copts(),\n     local_defines = if_graph_api([\"XLA_ONEDNN_USE_GRAPH_API=1\"]),\n     deps = [\n         \":backend_config_proto_cc\",\n@@ -979,6 +980,7 @@ cc_library(\n         \"//xla/backends/cpu/runtime:topk_thunk\",\n         \"//xla/backends/cpu/runtime:while_thunk\",\n         \"//xla/backends/cpu/runtime/onednn:onednn_fusion_thunk\",\n+        \"//xla/backends/cpu/runtime/onednn:onednn_op_thunk\",\n         \"//xla/backends/cpu/runtime/xnnpack:xnn_dot_thunk\",\n         \"//xla/backends/cpu/runtime/xnnpack:xnn_fusion_thunk\",\n         \"//xla/codegen:kernel_definition\","
        },
        {
            "sha": "030abdd64f205956e8ef185ac3b437d91cbb3813",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -336,6 +336,13 @@ ModuleComputationsTransitivelyContainCustomCall(const HloModule& module) {\n \n namespace cpu {\n \n+inline bool IsOneDnnCompatible(bool is_aot_compile) {\n+#ifdef ENABLE_ONEDNN_ASYNC\n+  return !is_aot_compile;\n+#endif\n+  return false;\n+}\n+\n CpuCompiler::CpuCompiler() {\n   // Initialize LLVM the first time the CpuCompiler is initialized.\n   static bool llvm_initialized = []() {\n@@ -535,7 +542,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   const bool is_fusion_emitters =\n       module->config().debug_options().xla_cpu_use_fusion_emitters();\n   bool use_shardy_partitioner = module->config().use_shardy_partitioner();\n-  bool is_onednn_compatible = false;\n+  bool is_onednn_compatible = IsOneDnnCompatible(is_aot_compile);\n   bool flatten_before_fusion = !options::FlattenAfterFusion(module->config());\n \n   if (num_partitions > 1) {\n@@ -674,8 +681,10 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n #if defined(INTEL_MKL)\n   // AOT compiled code runs in single thread.\n   bool is_thunk_runtime = true;\n-  is_onednn_compatible = !is_aot_compile && !is_thunk_runtime;\n-  if (is_onednn_compatible) {\n+  // TODO(intel-tf): Use IsOneDnnCompatible function to determine whether to\n+  // enable OneDnnOpsRewriter after enabling the OneDnnOpsRewriter with thunk\n+  // runtime.\n+  if (!is_thunk_runtime) {\n     // Placing OneDnnOpsRewriter here to match the flax patterns\n     // TODO: Decide where would be the appropriate place for this pass to make\n     // it more generic\n@@ -867,7 +876,7 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n     const CompileOptions& compile_options) {\n   const auto& debug_options = module->config().debug_options();\n   const bool is_fusion_emitters = debug_options.xla_cpu_use_fusion_emitters();\n-  bool is_onednn_compatible = false;\n+  bool is_onednn_compatible = IsOneDnnCompatible(is_aot_compile);\n   bool flatten_after_fusion = options::FlattenAfterFusion(module->config());\n   HloPassPipeline pipeline(\"HLO passes after layout assignment\");\n \n@@ -893,8 +902,6 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n \n #if defined(INTEL_MKL)\n   // AOT compiled code runs in single thread.\n-  bool is_thunk_runtime = true;\n-  is_onednn_compatible = !is_aot_compile && !is_thunk_runtime;\n   if (is_onednn_compatible) {\n     // Run SimplifyFPConversions pass to simplify the BF16 pattern and make it\n     // easier to match."
        },
        {
            "sha": "47bbdb5238baf83ae9568b6ed50d4e863fabbcd9",
            "filename": "third_party/xla/xla/service/cpu/onednn_contraction_rewriter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -513,6 +513,9 @@ bool OneDnnContractionRewriter::ShouldRewriteDot(\n \n bool OneDnnContractionRewriter::ShouldRewriteConv(\n     const HloInstruction* conv_instr) {\n+  // TODO(intel-tf): remove this restriction after enabling oneDNN convolution\n+  // support in thunk runtime.\n+  return false;\n   if (conv_instr->opcode() != HloOpcode::kConvolution) return false;\n   if (conv_instr->HasControlDependencies()) return false;\n   if (!IsSupportedType(conv_instr->shape().element_type())) return false;"
        },
        {
            "sha": "f96e09d14819535612d788546d952543a2607c95",
            "filename": "third_party/xla/xla/service/cpu/onednn_matmul.cc",
            "status": "modified",
            "additions": 81,
            "deletions": 1,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.cc?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -31,7 +31,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n #include \"xla/service/cpu/onednn_config.pb.h\"\n-#include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/service/cpu/runtime_lightweight_check.h\"\n #include \"xla/shape.h\"\n@@ -48,6 +47,7 @@ namespace {\n using dnnl::engine;\n using dnnl::matmul;\n using dnnl::memory;\n+using dnnl::primitive;\n using dnnl::stream;\n \n void TransposeIfNecessary(\n@@ -212,6 +212,86 @@ CreateOneDnnPrimDesc<dnnl::matmul::primitive_desc>(HloInstruction* instr) {\n                               fused_shapes, matmul_config);\n }\n \n+void ExecuteOneDnnMatMul(absl::Span<MemrefInfoHandler> arguments,\n+                         absl::Span<MemrefInfoHandler> results,\n+                         OneDnnMatMulConfig matmul_config,\n+                         const dnnl::engine& cpu_engine,\n+                         dnnl::stream& onednn_stream,\n+                         OneDnnResources& resources) {\n+  MemrefInfo input_minfo(arguments[0].get());\n+  MemrefInfo weights_minfo(arguments[1].get());\n+  MemrefInfo output_minfo(results[0].get());\n+\n+  auto input_md = input_minfo.GetOneDnnMemDesc();\n+  auto weights_md = weights_minfo.GetOneDnnMemDesc();\n+  auto output_md = output_minfo.GetOneDnnMemDesc();\n+\n+  // Input and weights memory::desc need to be in correct layout before matmul\n+  // primitive descriptor is created.\n+  TransposeIfNecessary(matmul_config.lhs().tensor().dimensions(),\n+                       matmul_config.transpose_a(), input_md);\n+  TransposeIfNecessary(matmul_config.rhs().tensor().dimensions(),\n+                       matmul_config.transpose_b(), weights_md);\n+  TransposeIfNecessary(matmul_config.result().tensor().dimensions(), false,\n+                       output_md);\n+\n+  auto weight_format = memory::format_tag::ab;\n+  if (matmul_config.optimization_config().weights_prepacked()) {\n+    // Weight pre-packing is supported for 2D weights only.\n+    // Since prepacked weights array is flattened, try to infer the dims from\n+    // input and output.\n+    // TODO(intel-tf): Add support for prepacked weights for higher than 2D\n+    // array.\n+    weights_md =\n+        memory::desc({input_md.get_dims().back(), output_md.get_dims().back()},\n+                     weights_md.get_data_type(), weight_format);\n+  }\n+\n+  // Excluding input and weight operands.\n+  const int64_t num_fused_operands = arguments.size() - 2;\n+  std::vector<memory::desc> fused_mds;\n+  std::vector<void*> fused_bufs;\n+  for (int64_t i = 0; i < num_fused_operands; ++i) {\n+    MemrefInfo operand_minfo(arguments[i + 2].get());\n+    fused_mds.push_back(operand_minfo.GetOneDnnMemDesc());\n+    fused_bufs.push_back(operand_minfo.Data());\n+  }\n+\n+  FusedOperandsRef fused_operands_ref{fused_bufs, resources.postop_args};\n+  auto matmul_pd =\n+      CreateMatMulPrimDesc(cpu_engine, input_md, weights_md, output_md,\n+                           fused_mds, matmul_config, &fused_operands_ref);\n+\n+  resources.src_mem = memory(input_md, cpu_engine, input_minfo.Data());\n+  resources.wei_mem =\n+      memory(matmul_pd->weights_desc(), cpu_engine, weights_minfo.Data());\n+  resources.dst_mem = memory(output_md, cpu_engine, output_minfo.Data());\n+\n+  if (std::strstr(matmul_pd->impl_info_str(), \"ref\") != nullptr) {\n+    LOG(WARNING) << \"[Perf]: MatMul reference implementation being executed\";\n+  }\n+\n+  resources.primitive = primitive(*matmul_pd);\n+\n+  std::unordered_map<int, memory> matmul_args{\n+      {DNNL_ARG_SRC, resources.src_mem},\n+      {DNNL_ARG_WEIGHTS, resources.wei_mem},\n+      {DNNL_ARG_DST, resources.dst_mem}};\n+\n+  if (matmul_config.optimization_config().user_scratchpad()) {\n+    MemrefInfo scratch_minfo(results[1].get());\n+    auto scratchpad_md = matmul_pd->scratchpad_desc();\n+    resources.scratch_mem =\n+        memory(scratchpad_md, cpu_engine, scratch_minfo.Data());\n+    matmul_args.insert({DNNL_ARG_SCRATCHPAD, resources.scratch_mem});\n+  }\n+\n+  matmul_args.insert(resources.postop_args.begin(),\n+                     resources.postop_args.end());\n+\n+  resources.primitive.execute(onednn_stream, matmul_args);\n+}\n+\n ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnMatMul(\n     void* result, void* scratch, void** args) {\n   // args[0]: ptr to nargs"
        },
        {
            "sha": "e724c72b58b83f76e685a065cef8b2112979f810",
            "filename": "third_party/xla/xla/service/cpu/onednn_matmul.h",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_matmul.h?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n \n #include \"dnnl.hpp\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n+#include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/shape.h\"\n \n@@ -34,6 +35,13 @@ Shape OneDnnMatMulOptWeightsShape(const Shape& input_shape,\n                                   const Shape& output_shape,\n                                   const OneDnnMatMulConfig* matmul_config);\n \n+void ExecuteOneDnnMatMul(absl::Span<MemrefInfoHandler> arguments,\n+                         absl::Span<MemrefInfoHandler> results,\n+                         OneDnnMatMulConfig matmul_config,\n+                         const dnnl::engine& cpu_engine,\n+                         dnnl::stream& onednn_stream,\n+                         OneDnnResources& resources);\n+\n extern \"C\" {\n extern void __xla_cpu_runtime_OneDnnMatMul(void* result, void* scratch,\n                                            void** args);"
        },
        {
            "sha": "b274d26f331f3c263110bfb328e1b7e7e8a3663f",
            "filename": "third_party/xla/xla/service/cpu/onednn_memory_util.h",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_memory_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_memory_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_memory_util.h?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -132,6 +132,36 @@ dnnl::memory::desc ShapeToMemDesc(const Shape& shape);\n \n Shape MemDescToXlaShapeFlattened(const dnnl::memory::desc& md);\n \n+// Define a struct to encapsulate oneDNN memory and primitive objects.\n+struct OneDnnResources {\n+  // Primitive object\n+  dnnl::primitive primitive;\n+\n+  // Memory objects\n+  dnnl::memory src_mem;\n+  dnnl::memory wei_mem;\n+  dnnl::memory dst_mem;\n+  dnnl::memory scratch_mem;\n+\n+  // Post-operation arguments\n+  std::vector<std::pair<int, dnnl::memory>> postop_args;\n+\n+  // Memory reference handlers for arguments and results.\n+  std::vector<MemrefInfoHandler> arg_memrefs;\n+  std::vector<MemrefInfoHandler> result_memrefs;\n+\n+  // Constructor to initialize all members to default values.\n+  OneDnnResources()\n+      : primitive(dnnl::primitive()),\n+        src_mem(dnnl::memory()),\n+        wei_mem(dnnl::memory()),\n+        dst_mem(dnnl::memory()),\n+        scratch_mem(dnnl::memory()),\n+        postop_args(),\n+        arg_memrefs(),\n+        result_memrefs() {}\n+};\n+\n }  // namespace cpu\n }  // namespace xla\n "
        },
        {
            "sha": "6a4553f19d68f3d76cf7e317a120d7faa488ee08",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 7,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -116,6 +116,10 @@ limitations under the License.\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n \n+#ifdef INTEL_MKL\n+#include \"xla/backends/cpu/runtime/onednn/onednn_op_thunk.h\"\n+#endif  // INTEL_MKL\n+\n #if XLA_ONEDNN_USE_GRAPH_API\n #include \"xla/backends/cpu/onednn_emitter.h\"\n #include \"xla/backends/cpu/onednn_support.h\"\n@@ -1257,7 +1261,9 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitFftThunk(\n       /*output_shape=*/instruction->shape());\n }\n \n-static absl::StatusOr<CustomCallThunk::OpBuffers> GetCustomCallOpBuffers(\n+// Generic helper to collect argument/result slices for different OpBuffers\n+template <typename OpBuffers>\n+static absl::StatusOr<OpBuffers> GetOpBuffers(\n     const HloInstruction* instruction,\n     const BufferAssignment& buffer_assignment) {\n   // Collect buffer slices for all operands.\n@@ -1282,7 +1288,7 @@ static absl::StatusOr<CustomCallThunk::OpBuffers> GetCustomCallOpBuffers(\n     results_shapes.push_back(indexed.shape);\n   }\n \n-  return CustomCallThunk::OpBuffers{\n+  return OpBuffers{\n       /*arguments_buffers=*/std::move(arguments_buffers),\n       /*arguments_shapes=*/std::move(arguments_shapes),\n       /*results_buffers=*/std::move(results_buffers),\n@@ -1291,6 +1297,29 @@ static absl::StatusOr<CustomCallThunk::OpBuffers> GetCustomCallOpBuffers(\n   };\n }\n \n+#ifdef INTEL_MKL\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOneDnnOpThunk(\n+    const HloInstruction* instruction) {\n+  auto custom_call = Cast<HloCustomCallInstruction>(instruction);\n+  auto custom_call_target = custom_call->custom_call_target();\n+  auto backend_config = custom_call->backend_config<BackendConfig>();\n+\n+  OneDnnOpThunk::OneDnnOpConfig config;\n+  if (custom_call_target == \"__onednn$matmul\") {\n+    config = backend_config->onednn_matmul_config();\n+  } else {\n+    return Unimplemented(\n+        \"Custom call target %s is not supported in thunk runtime\",\n+        custom_call_target);\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(auto op_buffers, GetOpBuffers<OneDnnOpThunk::OpBuffers>(\n+                                           instruction, buffer_assignment_));\n+  return ThunkSequence::Of<OneDnnOpThunk>(\n+      custom_call_target, ThunkInfo(custom_call), op_buffers, config);\n+}\n+#endif  // INTEL_MKL\n+\n static bool IsValidCustomCallApiVersion(CustomCallApiVersion api_version) {\n   switch (api_version) {\n     case CustomCallApiVersion::API_VERSION_ORIGINAL:\n@@ -1310,17 +1339,22 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCustomCallThunk(\n   // TODO(penporn): Support these existing targets.\n   auto custom_call_target = custom_call->custom_call_target();\n   if (custom_call_target == \"PadToStatic\" ||\n-      custom_call_target == \"__onednn$matmul\" ||\n+      custom_call_target == \"__onednn$convolution\" ||\n       custom_call_target == \"__onednn$softmax\" ||\n-      custom_call_target == \"__onednn$layernorm\" ||\n-      custom_call_target == \"__onednn$matmul_reorder\") {\n+      custom_call_target == \"__onednn$layernorm\") {\n     return Unimplemented(\"Custom call target %s is not implemented.\",\n                          custom_call_target);\n   }\n   if (custom_call_target == \"TopK\") {\n     return EmitTopKThunk(custom_call);\n   } else if (custom_call_target == \"SliceToDynamic\") {\n     return EmitSliceToDynamicThunk(instruction);\n+  } else if (absl::StartsWith(custom_call->custom_call_target(), \"__onednn$\")) {\n+#ifdef INTEL_MKL\n+    return EmitOneDnnOpThunk(instruction);\n+#else\n+    return Unimplemented(\"XLA is not built with oneDNN.\");\n+#endif  // INTEL_MKL\n   }\n \n   // Check the API version.\n@@ -1344,8 +1378,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCustomCallThunk(\n           : ((version == API_VERSION_TYPED_FFI)\n                  ? backend_config->custom_call_config().attributes()\n                  : backend_config->custom_call_config().opaque());\n-  TF_ASSIGN_OR_RETURN(auto op_buffers,\n-                      GetCustomCallOpBuffers(instruction, buffer_assignment_));\n+  TF_ASSIGN_OR_RETURN(auto op_buffers, GetOpBuffers<CustomCallThunk::OpBuffers>(\n+                                           instruction, buffer_assignment_));\n \n   return ThunkSequence::Of<CustomCallThunk>(ThunkInfo(instruction),\n                                             custom_call_target, op_buffers,"
        },
        {
            "sha": "28cbe30c5c337fce76bea4dafe860e0a40375248",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h?ref=d4663a4e49a4277fc4e55b3fa6a4e8a8d30d64c3",
            "patch": "@@ -201,6 +201,9 @@ class ThunkEmitter {\n   absl::StatusOr<ThunkSequence> EmitTopKThunk(\n       const HloCustomCallInstruction* custom_call);\n \n+  absl::StatusOr<ThunkSequence> EmitOneDnnOpThunk(\n+      const HloInstruction* instruction);\n+\n   absl::StatusOr<ThunkSequence> EmitSliceThunk(\n       const HloInstruction* instruction);\n "
        }
    ],
    "stats": {
        "total": 632,
        "additions": 617,
        "deletions": 15
    }
}