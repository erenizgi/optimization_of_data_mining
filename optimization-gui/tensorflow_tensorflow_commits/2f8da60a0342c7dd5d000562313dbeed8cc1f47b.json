{
    "author": "basioli-k",
    "message": "[XLA:GPU][host offloading] Emit host execute thunks if present in the HLO.\n\nPiperOrigin-RevId: 797774136",
    "sha": "2f8da60a0342c7dd5d000562313dbeed8cc1f47b",
    "files": [
        {
            "sha": "76664ea6d68012f96c9463e8b19ac053cfb9ff34",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client_test.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc?ref=2f8da60a0342c7dd5d000562313dbeed8cc1f47b",
            "patch": "@@ -1837,5 +1837,53 @@ TEST(TfrtGpuClientTest, MultipleDeviceShareDmaMapping) {\n   TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr.get()));\n }\n \n+TEST(TfrtGpuClientTest, HostExecuteRuntimeTest) {\n+  static constexpr char const* kProgram = R\"(\n+    HloModule module\n+\n+    add_inplace {\n+      p0 = f32[] parameter(0)\n+      ROOT add = f32[] add(p0, p0)\n+    }\n+\n+    ENTRY entry {\n+      %p0 = f32[] parameter(0)\n+      %start =\n+        ((f32[]), f32[], s32[]) custom-call-start(%p0),\n+          custom_call_target=\"HostExecute\",\n+          async_execution_thread=\"host\",\n+          to_apply=%add_inplace,\n+          output_to_operand_aliasing={{}: (0, {})}\n+      ROOT %done = f32[] custom-call-done(%start)\n+    })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, GetTfrtGpuClient(GpuClientOptions()));\n+  TF_ASSERT_OK_AND_ASSIGN(auto executable,\n+                          CompileExecutable(kProgram, *client));\n+\n+  auto device = client->addressable_devices()[0];\n+  TF_EXPECT_OK(device->default_memory_space());\n+\n+  Shape shape = ShapeUtil::MakeShape(F32, {});\n+  constexpr float data[] = {0.1f};\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto input,\n+      client->BufferFromHostBuffer(\n+          data, shape.element_type(), shape.dimensions(),\n+          /*byte_strides=*/std::nullopt,\n+          PjRtClient::HostBufferSemantics::kImmutableOnlyDuringCall,\n+          /*on_done_with_host_buffer=*/nullptr, *device->default_memory_space(),\n+          /*device_layout=*/nullptr));\n+  EXPECT_EQ(input->memory_space()->kind(), \"device\");\n+\n+  ExecuteOptions opts;\n+  auto result = executable->Execute(/*argument_handles=*/{{input.get()}}, opts);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::shared_ptr<xla::Literal> result_literal,\n+                          ExtractSingleResult(result));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(LiteralUtil::CreateR0<float>(0.2f),\n+                                     *result_literal));\n+}\n+\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "6e66a52de0699fb61110795486c3b2ccf5327957",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=2f8da60a0342c7dd5d000562313dbeed8cc1f47b",
            "patch": "@@ -322,6 +322,7 @@ cc_library(\n         \":ir_emission_utils\",\n         \":kernel_reuse_cache\",\n         \"//xla/backends/gpu/runtime:collective_thunk\",\n+        \"//xla/backends/gpu/runtime:host_execute_thunk\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:name_uniquer\",\n@@ -389,6 +390,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:fft_thunk\",\n         \"//xla/backends/gpu/runtime:gemm_thunk\",\n         \"//xla/backends/gpu/runtime:gpublas_lt_matmul_thunk\",\n+        \"//xla/backends/gpu/runtime:host_execute_thunk\",\n         \"//xla/backends/gpu/runtime:host_send_recv_thunk\",\n         \"//xla/backends/gpu/runtime:infeed_thunk\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n@@ -443,6 +445,7 @@ cc_library(\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n+        \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:dnn_proto_cc\","
        },
        {
            "sha": "76835f573dbf2627d59dc990c0f4e39e4532e50c",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=2f8da60a0342c7dd5d000562313dbeed8cc1f47b",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Operation.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/host_execute_thunk.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -49,6 +50,12 @@ using CollectivesAsyncEvents =\n     absl::flat_hash_map<std::variant<mlir::Operation*, const HloInstruction*>,\n                         std::shared_ptr<CollectiveThunk::AsyncEvents>>;\n \n+// Maps host offloading start ops to their async events so we can emit done\n+// thunk sharing events with corresponding start thunk.\n+using InstructionToHostExecuteAsyncEvents =\n+    absl::flat_hash_map<const HloInstruction*,\n+                        std::shared_ptr<HostExecuteAsyncEvents>>;\n+\n // IrEmitterContext encapsulates common (mutable and immutable) data structures\n // used by both IrEmitterNested and IrEmitterUnnested, such as the buffer\n // assignment and the name uniquer.\n@@ -125,6 +132,11 @@ class IrEmitterContext {\n     return collectives_async_events_;\n   }\n \n+  InstructionToHostExecuteAsyncEvents&\n+  instruction_to_host_execute_async_events() {\n+    return instruction_to_host_execute_async_events_;\n+  }\n+\n   bool emit_kernels() const { return emit_kernels_; }\n \n  private:\n@@ -141,6 +153,7 @@ class IrEmitterContext {\n   KernelReuseCache kernel_cache_;\n \n   CollectivesAsyncEvents collectives_async_events_;\n+  InstructionToHostExecuteAsyncEvents instruction_to_host_execute_async_events_;\n \n   // We should not emit kernels when loading thunks from a compilation result.\n   const bool emit_kernels_;"
        },
        {
            "sha": "6e10a227c7c4e87299f0ed7a806fc1e9081b5efc",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=2f8da60a0342c7dd5d000562313dbeed8cc1f47b",
            "patch": "@@ -95,6 +95,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/fft_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\"\n+#include \"xla/backends/gpu/runtime/host_execute_thunk.h\"\n #include \"xla/backends/gpu/runtime/host_send_recv_thunk.h\"\n #include \"xla/backends/gpu/runtime/infeed_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n@@ -178,6 +179,7 @@ limitations under the License.\n #include \"xla/stream_executor/platform/platform_object_registry.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/protobuf/dnn.pb.h\"\n@@ -191,6 +193,17 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+namespace {\n+// TODO: move into a host_execute specific file.\n+bool IsHostExecuteCustomCall(const HloInstruction& hlo) {\n+  return hlo.opcode() == HloOpcode::kCustomCall &&\n+         hlo.custom_call_target() ==\n+             \"HostExecute\";  // TODO: this constant string should be shared with\n+                             // the TPU one\n+}\n+\n+}  // namespace\n+\n IrEmitterUnnested::IrEmitterUnnested(IrEmitterContext* ir_emitter_context)\n     : IrEmitter(ir_emitter_context, /*is_nested=*/false),\n       send_recv_events_(std::make_shared<HostSendRecvAsyncEvents>()),\n@@ -3015,6 +3028,16 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n         }\n         case HloOpcode::kCall:\n         case HloOpcode::kCustomCall: {\n+          if (IsHostExecuteCustomCall(*wrapped)) {\n+            auto custom_call = Cast<HloCustomCallInstruction>(wrapped);\n+\n+            auto async_events =\n+                GetInstructionToHostExecuteAsyncEvents().at(custom_call);\n+\n+            AddThunkToThunkSequence(std::make_unique<HostExecuteDoneThunk>(\n+                Thunk::ThunkInfo::WithProfileAnnotation(instr), async_events));\n+            return absl::OkStatus();\n+          }\n           // Wait until the concurrent stream has finished.\n           auto* async_done = Cast<HloAsyncInstruction>(instr);\n           const ExecutionStreamAssignment& stream_assignment =\n@@ -3088,6 +3111,66 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n           return EmitAsyncComputation(instr);\n         }\n         case HloOpcode::kCustomCall: {\n+          if (IsHostExecuteCustomCall(*wrapped)) {\n+            auto custom_call = Cast<HloCustomCallInstruction>(wrapped);\n+\n+            std::unique_ptr<HloModule> hlo_module =\n+                ExtractComputationIntoNewModule(\n+                    *custom_call->called_computation());\n+\n+            // All offloaded computations are marked as host computations from\n+            // the perspective of the GPU backend. Since these will execute on\n+            // the main thread from the CPU backend perspective, we need to mark\n+            // them as such.\n+            for (auto* computation : hlo_module->computations()) {\n+              computation->SetExecutionThread(\n+                  HloInstruction::kMainExecutionThread);\n+            }\n+\n+            absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4>\n+                operand_slices;\n+            for (HloInstruction* operand : wrapped->operands()) {\n+              for (auto& indexed : ShapeUtil::GetLeafShapes(operand->shape())) {\n+                TF_ASSIGN_OR_RETURN(\n+                    auto slice,\n+                    ir_emitter_context_->buffer_assignment().GetUniqueSlice(\n+                        operand, indexed.index));\n+                operand_slices.push_back({slice, indexed.shape});\n+              }\n+            }\n+\n+            // Collect buffer slices for all results.\n+            absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4>\n+                result_slices;\n+            for (auto& indexed : ShapeUtil::GetLeafShapes(wrapped->shape())) {\n+              TF_ASSIGN_OR_RETURN(\n+                  auto slice,\n+                  ir_emitter_context_->buffer_assignment().GetUniqueSlice(\n+                      wrapped, indexed.index));\n+              result_slices.push_back({slice, indexed.shape});\n+            }\n+\n+            auto thunk = std::make_unique<HostExecuteStartThunk>(\n+                Thunk::ThunkInfo::WithProfileAnnotation(instr), *hlo_module,\n+                std::move(operand_slices), std::move(result_slices));\n+\n+            auto async_events = thunk->async_events();\n+\n+            auto [it, inserted] =\n+                GetInstructionToHostExecuteAsyncEvents().emplace(custom_call,\n+                                                                 async_events);\n+\n+            if (!inserted) {\n+              return Internal(\n+                  \"Async events already exist for host offloading custom call \"\n+                  \"%s.\",\n+                  custom_call->ToString());\n+            }\n+\n+            AddThunkToThunkSequence(std::move(thunk));\n+\n+            return absl::OkStatus();\n+          }\n           return EmitAsyncCustomCallStart(instr);\n         }\n         default:"
        },
        {
            "sha": "6be999d2b34337ab08dcb33e6194bf3470407e9f",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2f8da60a0342c7dd5d000562313dbeed8cc1f47b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h?ref=2f8da60a0342c7dd5d000562313dbeed8cc1f47b",
            "patch": "@@ -334,6 +334,11 @@ class IrEmitterUnnested : public IrEmitter {\n     return ir_emitter_context_->collectives_async_events();\n   }\n \n+  InstructionToHostExecuteAsyncEvents&\n+  GetInstructionToHostExecuteAsyncEvents() {\n+    return ir_emitter_context_->instruction_to_host_execute_async_events();\n+  }\n+\n   // The thunk sequence this IrEmitter generates for the input computation.\n   ThunkSequence thunk_sequence_;\n   ThunkSequence scoped_thunk_sequence_;"
        }
    ],
    "stats": {
        "total": 152,
        "additions": 152,
        "deletions": 0
    }
}