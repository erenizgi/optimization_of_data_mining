{
    "author": "ezhulenev",
    "message": "[xla:collectives] Migrate from tsl::AsyncValueRef to xla::Future\n\nThis change is mostly NFC, simply migrating from one concurrency primitive to another.\n\nPiperOrigin-RevId: 814433625",
    "sha": "cb5fcbed6c7729998217225d248e6c0bb01e8897",
    "files": [
        {
            "sha": "a519cafddb1089fa4814c74edb4048f523537279",
            "filename": "third_party/xla/xla/backends/cpu/collectives/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2FBUILD?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -133,6 +133,7 @@ cc_library(\n     hdrs = [\"in_process_communicator.h\"],\n     deps = [\n         \":cpu_collectives\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n@@ -259,6 +260,7 @@ cc_library(\n     features = [\"-use_header_modules\"],\n     deps = [\n         \":cpu_collectives\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:types\",\n@@ -325,6 +327,7 @@ cc_library(\n     ],\n     features = [\"-use_header_modules\"],\n     deps = [\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n@@ -336,12 +339,12 @@ cc_library(\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:statusor\",\n         \"@mpitrampoline\",\n     ],\n )"
        },
        {
            "sha": "91d28ac43551c10744e90a295f3a00fb10b5fbef",
            "filename": "third_party/xla/xla/backends/cpu/collectives/gloo_collectives_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_collectives_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_collectives_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_collectives_test.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -38,7 +38,6 @@ limitations under the License.\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/global_device_id.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -115,11 +114,7 @@ absl::StatusOr<std::vector<uint8_t>> AllReduce(\n       AsDeviceMemory(input_buffer), AsDeviceMemory(output_buffer),\n       xla::PrimitiveType::U8, kBufferSize, xla::ReductionKind::SUM, executor);\n \n-  tsl::BlockUntilReady(event);\n-\n-  if (event.IsError()) {\n-    return event.GetError();\n-  }\n+  TF_RETURN_IF_ERROR(event.Await());\n \n   return output_buffer;\n }"
        },
        {
            "sha": "61a7725e5d07843c98a639bbdd8054243acb15f0",
            "filename": "third_party/xla/xla/backends/cpu/collectives/gloo_communicator.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 19,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -43,11 +43,11 @@ limitations under the License.\n #include \"gloo/types.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/types.h\"\n@@ -103,10 +103,11 @@ static absl::Status SetAllReduceOptions(ReductionKind reduction_kind,\n   return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::AllReduce(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n-    const Executor& executor) {\n+Future<> GlooCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n+                                     se::DeviceMemoryBase recv_buffer,\n+                                     PrimitiveType dtype, size_t count,\n+                                     ReductionKind reduction_kind,\n+                                     const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n \n   gloo::AllreduceOptions options(context_);\n@@ -182,12 +183,12 @@ tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::AllReduce(\n     return absl::UnknownError(\n         absl::StrCat(\"Gloo all-reduce failed: \", e.what()));\n   }\n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n static constexpr uint8_t kCollectivePermuteSlotPrefix = 0x40;\n \n-tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::CollectivePermute(\n+Future<> GlooCommunicator::CollectivePermute(\n     se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n@@ -235,10 +236,10 @@ tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::CollectivePermute(\n     return absl::UnknownError(\n         absl::StrCat(\"Gloo collective permute failed: \", e.what()));\n   }\n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::AllToAll(\n+Future<> GlooCommunicator::AllToAll(\n     absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n     absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n@@ -291,12 +292,13 @@ tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::AllToAll(\n     return absl::UnknownError(\n         absl::StrCat(\"Gloo all-to-all failed: \", e.what()));\n   }\n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::AllGather(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, const Executor& executor) {\n+Future<> GlooCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n+                                     se::DeviceMemoryBase recv_buffer,\n+                                     PrimitiveType dtype, size_t count,\n+                                     const Executor& executor) {\n   uint32_t tag = 0;  // TODO(phawkins): use better tags.\n \n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n@@ -315,7 +317,7 @@ tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::AllGather(\n     return absl::UnknownError(\n         absl::StrCat(\"Gloo AllGather failed: \", e.what()));\n   }\n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n template <typename T>\n@@ -367,10 +369,11 @@ absl::Status ReduceScatterHelper(std::shared_ptr<gloo::Context> context,\n   return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::ReduceScatter(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n-    const Executor& executor) {\n+Future<> GlooCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                                         se::DeviceMemoryBase recv_buffer,\n+                                         PrimitiveType dtype, size_t count,\n+                                         ReductionKind reduction_kind,\n+                                         const Executor& executor) {\n   size_t chunk_bytes = count * primitive_util::ByteWidth(dtype);\n   std::unique_ptr<char[]> temp(new char[chunk_bytes * context_->size]);\n   std::memcpy(temp.get(), send_buffer.opaque(), chunk_bytes * context_->size);\n@@ -436,7 +439,7 @@ tsl::AsyncValueRef<GlooCommunicator::Event> GlooCommunicator::ReduceScatter(\n       return absl::InvalidArgumentError(\"Unknown datatype in reducescatter\");\n   }\n   std::memcpy(recv_buffer.opaque(), temp.get(), chunk_bytes);\n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "8f631ed692e97dc1316fa61edd9d6d466d05fa3f",
            "filename": "third_party/xla/xla/backends/cpu/collectives/gloo_communicator.h",
            "status": "modified",
            "additions": 33,
            "deletions": 37,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fgloo_communicator.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -22,16 +22,15 @@ limitations under the License.\n #include <string>\n \n #include \"absl/container/inlined_vector.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n #include \"gloo/context.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -44,47 +43,44 @@ class GlooCommunicator : public Communicator {\n                    size_t num_ranks);\n   ~GlooCommunicator() override;\n \n-  tsl::AsyncValueRef<Event> AllReduce(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      ReductionKind reduction_kind,\n-                                      const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> CollectivePermute(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-      PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n-      absl::Span<const RankId> target_ranks, const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> AllToAll(\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n-      PrimitiveType dtype, size_t count, const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> AllGather(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                          se::DeviceMemoryBase recv_buffer,\n-                                          PrimitiveType dtype, size_t count,\n-                                          ReductionKind reduction_kind,\n-                                          const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> Broadcast(se::DeviceMemoryBase,\n-                                      se::DeviceMemoryBase, PrimitiveType,\n-                                      size_t, RankId,\n-                                      const Executor&) override {\n+  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, ReductionKind reduction_kind,\n+                     const Executor& executor) override;\n+\n+  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n+                             se::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count,\n+                             std::optional<RankId> source_rank,\n+                             absl::Span<const RankId> target_ranks,\n+                             const Executor& executor) override;\n+\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+                    PrimitiveType dtype, size_t count,\n+                    const Executor& executor) override;\n+\n+  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, const Executor& executor) override;\n+\n+  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                         size_t count, ReductionKind reduction_kind,\n+                         const Executor& executor) override;\n+\n+  Future<> Broadcast(se::DeviceMemoryBase, se::DeviceMemoryBase, PrimitiveType,\n+                     size_t, RankId, const Executor&) override {\n     return Unimplemented(\"Broadcast is not implemented\");\n   }\n \n-  tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase, PrimitiveType, size_t,\n-                                 RankId, const Executor&) override {\n+  Future<> Send(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+                const Executor&) override {\n     return Unimplemented(\"Send is not implemented\");\n   }\n \n-  tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase, PrimitiveType, size_t,\n-                                 RankId, const Executor&) override {\n+  Future<> Recv(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+                const Executor&) override {\n     return Unimplemented(\"Recv is not implemented\");\n   }\n "
        },
        {
            "sha": "021384c70b5b5c481c327ed177527cf5c96183b7",
            "filename": "third_party/xla/xla/backends/cpu/collectives/in_process_communicator.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 32,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -37,11 +37,11 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/lib/math/math_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -399,12 +399,11 @@ static absl::Status CollectivePermuteOp(\n InProcessCommunicator::InProcessCommunicator(size_t rank, size_t num_ranks)\n     : rank_(rank), num_ranks_(num_ranks) {}\n \n-tsl::AsyncValueRef<InProcessCommunicator::Event>\n-InProcessCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n-                                 se::DeviceMemoryBase recv_buffer,\n-                                 PrimitiveType dtype, size_t count,\n-                                 ReductionKind reduction_kind,\n-                                 const Executor& executor) {\n+Future<> InProcessCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n+                                          se::DeviceMemoryBase recv_buffer,\n+                                          PrimitiveType dtype, size_t count,\n+                                          ReductionKind reduction_kind,\n+                                          const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n   const RendezvousKey& key = cpu_executor->rendezvous_key();\n \n@@ -420,15 +419,14 @@ InProcessCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n   TF_RETURN_IF_ERROR(\n       op->Invoke(AllReduceOp, rank_, dtype, count, reduction_kind));\n \n-  return OkEvent();\n+  return Future<>(absl::OkStatus());\n }\n \n-tsl::AsyncValueRef<InProcessCommunicator::Event>\n-InProcessCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                     se::DeviceMemoryBase recv_buffer,\n-                                     PrimitiveType dtype, size_t count,\n-                                     ReductionKind reduction_kind,\n-                                     const Executor& executor) {\n+Future<> InProcessCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                                              se::DeviceMemoryBase recv_buffer,\n+                                              PrimitiveType dtype, size_t count,\n+                                              ReductionKind reduction_kind,\n+                                              const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n   const RendezvousKey& key = cpu_executor->rendezvous_key();\n \n@@ -444,16 +442,13 @@ InProcessCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n   TF_RETURN_IF_ERROR(\n       op->Invoke(ReduceScatterOp, rank_, dtype, count, reduction_kind));\n \n-  return OkEvent();\n+  return Future<>(absl::OkStatus());\n }\n \n-tsl::AsyncValueRef<InProcessCommunicator::Event>\n-InProcessCommunicator::CollectivePermute(se::DeviceMemoryBase send_buffer,\n-                                         se::DeviceMemoryBase recv_buffer,\n-                                         PrimitiveType dtype, size_t count,\n-                                         std::optional<RankId> source_rank,\n-                                         absl::Span<const RankId> target_ranks,\n-                                         const Executor& executor) {\n+Future<> InProcessCommunicator::CollectivePermute(\n+    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n+    PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n+    absl::Span<const RankId> target_ranks, const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n   const RendezvousKey& key = cpu_executor->rendezvous_key();\n \n@@ -471,11 +466,10 @@ InProcessCommunicator::CollectivePermute(se::DeviceMemoryBase send_buffer,\n \n   TF_RETURN_IF_ERROR(op->Invoke(CollectivePermuteOp, rank_, num_bytes));\n \n-  return OkEvent();\n+  return Future<>(absl::OkStatus());\n }\n \n-tsl::AsyncValueRef<InProcessCommunicator::Event>\n-InProcessCommunicator::AllToAll(\n+Future<> InProcessCommunicator::AllToAll(\n     absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n     absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n@@ -497,14 +491,13 @@ InProcessCommunicator::AllToAll(\n \n   TF_RETURN_IF_ERROR(op->Invoke(AllToAllOp, rank_, num_bytes));\n \n-  return OkEvent();\n+  return Future<>(absl::OkStatus());\n }\n \n-tsl::AsyncValueRef<InProcessCommunicator::Event>\n-InProcessCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n-                                 se::DeviceMemoryBase recv_buffer,\n-                                 PrimitiveType dtype, size_t count,\n-                                 const Executor& executor) {\n+Future<> InProcessCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n+                                          se::DeviceMemoryBase recv_buffer,\n+                                          PrimitiveType dtype, size_t count,\n+                                          const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(auto cpu_executor, CpuCollectives::TryCast(&executor));\n   const RendezvousKey& key = cpu_executor->rendezvous_key();\n \n@@ -521,7 +514,7 @@ InProcessCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n \n   TF_RETURN_IF_ERROR(op->Invoke(AllGatherOp, rank_, num_bytes));\n \n-  return OkEvent();\n+  return Future<>(absl::OkStatus());\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "d930b0361d58607b197cc5281de8bace098a2b5d",
            "filename": "third_party/xla/xla/backends/cpu/collectives/in_process_communicator.h",
            "status": "modified",
            "additions": 36,
            "deletions": 40,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fin_process_communicator.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -21,15 +21,14 @@ limitations under the License.\n #include <string>\n \n #include \"absl/container/inlined_vector.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -41,48 +40,45 @@ class InProcessCommunicator : public Communicator {\n  public:\n   InProcessCommunicator(size_t rank, size_t num_ranks);\n \n-  tsl::AsyncValueRef<Event> AllReduce(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      ReductionKind reduction_kind,\n-                                      const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> CollectivePermute(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-      PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n-      absl::Span<const RankId> target_ranks, const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> AllToAll(\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n-      PrimitiveType dtype, size_t count, const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> AllGather(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                          se::DeviceMemoryBase recv_buffer,\n-                                          PrimitiveType dtype, size_t count,\n-                                          ReductionKind reduction_kind,\n-                                          const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> Broadcast(se::DeviceMemoryBase,\n-                                      se::DeviceMemoryBase, PrimitiveType,\n-                                      size_t, RankId,\n-                                      const Executor&) override {\n-    return Unimplemented(\"Broadcast is not implemented\");\n+  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, ReductionKind reduction_kind,\n+                     const Executor& executor) override;\n+\n+  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n+                             se::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count,\n+                             std::optional<RankId> source_rank,\n+                             absl::Span<const RankId> target_ranks,\n+                             const Executor& executor) override;\n+\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+                    PrimitiveType dtype, size_t count,\n+                    const Executor& executor) override;\n+\n+  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, const Executor& executor) override;\n+\n+  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                         size_t count, ReductionKind reduction_kind,\n+                         const Executor& executor) override;\n+\n+  Future<> Broadcast(se::DeviceMemoryBase, se::DeviceMemoryBase, PrimitiveType,\n+                     size_t, RankId, const Executor&) override {\n+    return Future<>(Unimplemented(\"Broadcast is not implemented\"));\n   }\n \n-  tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase, PrimitiveType, size_t,\n-                                 RankId, const Executor&) override {\n-    return Unimplemented(\"Send is not implemented\");\n+  Future<> Send(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+                const Executor&) override {\n+    return Future<>(Unimplemented(\"Send is not implemented\"));\n   }\n \n-  tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase, PrimitiveType, size_t,\n-                                 RankId, const Executor&) override {\n-    return Unimplemented(\"Recv is not implemented\");\n+  Future<> Recv(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+                const Executor&) override {\n+    return Future<>(Unimplemented(\"Recv is not implemented\"));\n   }\n \n   absl::StatusOr<size_t> NumRanks() const override { return num_ranks_; }"
        },
        {
            "sha": "abcb8aa11b2a89fa0854b9a0a461e204e4dc47d6",
            "filename": "third_party/xla/xla/backends/cpu/collectives/mpi_communicator.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 20,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -27,15 +27,15 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"mpi.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla::cpu {\n \n@@ -125,18 +125,19 @@ MpiCommunicator::MpiCommunicator(int color, int key) {\n \n MpiCommunicator::~MpiCommunicator() { MPI_Comm_free(&comm_); };\n \n-tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::AllReduce(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n-    const Executor& executor) {\n+Future<> MpiCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n+                                    se::DeviceMemoryBase recv_buffer,\n+                                    PrimitiveType dtype, size_t count,\n+                                    ReductionKind reduction_kind,\n+                                    const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(MPI_Datatype type, PrimitiveTypeToMpiType(dtype));\n   TF_ASSIGN_OR_RETURN(MPI_Op op, ReductionKindToMpiOp(reduction_kind, type));\n   TF_RETURN_IF_ERROR(MpiErrorToAbslStatus(MPI_Allreduce(\n       send_buffer.opaque(), recv_buffer.opaque(), count, type, op, comm_)));\n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::CollectivePermute(\n+Future<> MpiCommunicator::CollectivePermute(\n     se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n@@ -177,10 +178,10 @@ tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::CollectivePermute(\n         MpiErrorToAbslStatus(MPI_Wait(&request, MPI_STATUS_IGNORE)));\n   }\n \n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::AllToAll(\n+Future<> MpiCommunicator::AllToAll(\n     absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n     absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n@@ -214,24 +215,26 @@ tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::AllToAll(\n                      recv_rank, tag, comm_, MPI_STATUS_IGNORE)));\n   }\n \n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::AllGather(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, const Executor& executor) {\n+Future<> MpiCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n+                                    se::DeviceMemoryBase recv_buffer,\n+                                    PrimitiveType dtype, size_t count,\n+                                    const Executor& executor) {\n   TF_ASSIGN_OR_RETURN(MPI_Datatype type, PrimitiveTypeToMpiType(dtype));\n   TF_RETURN_IF_ERROR(MpiErrorToAbslStatus(\n       MPI_Allgather(send_buffer.opaque(), count, type, recv_buffer.opaque(),\n                     count, type, comm_)));\n \n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::ReduceScatter(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n-    const Executor& executor) {\n+Future<> MpiCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                                        se::DeviceMemoryBase recv_buffer,\n+                                        PrimitiveType dtype, size_t count,\n+                                        ReductionKind reduction_kind,\n+                                        const Executor& executor) {\n   const int size = mpi_size_;\n   std::vector<int> recvcounts(size, count);\n   TF_ASSIGN_OR_RETURN(MPI_Datatype type, PrimitiveTypeToMpiType(dtype));\n@@ -240,7 +243,7 @@ tsl::AsyncValueRef<MpiCommunicator::Event> MpiCommunicator::ReduceScatter(\n       MPI_Reduce_scatter(send_buffer.opaque(), recv_buffer.opaque(),\n                          recvcounts.data(), type, op, comm_)));\n \n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "f7864c8fbd1682339f045cbb8e2aeac69f1da8e2",
            "filename": "third_party/xla/xla/backends/cpu/collectives/mpi_communicator.h",
            "status": "modified",
            "additions": 31,
            "deletions": 35,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcollectives%2Fmpi_communicator.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -21,16 +21,15 @@ limitations under the License.\n #include <string>\n \n #include \"absl/container/inlined_vector.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n #include \"mpi.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -41,45 +40,42 @@ class MpiCommunicator : public Communicator {\n   explicit MpiCommunicator(int color, int key);\n   ~MpiCommunicator() override;\n \n-  tsl::AsyncValueRef<Event> AllReduce(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      ReductionKind reduction_kind,\n-                                      const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> CollectivePermute(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-      PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n-      absl::Span<const RankId> target_ranks, const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> AllToAll(\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n-      PrimitiveType dtype, size_t count, const Executor& executor) override;\n-  tsl::AsyncValueRef<Event> AllGather(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      const Executor& executor) override;\n-  tsl::AsyncValueRef<Event> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                          se::DeviceMemoryBase recv_buffer,\n-                                          PrimitiveType dtype, size_t count,\n-                                          ReductionKind reduction_kind,\n-                                          const Executor& executor) override;\n-\n-  tsl::AsyncValueRef<Event> Broadcast(se::DeviceMemoryBase,\n-                                      se::DeviceMemoryBase, PrimitiveType,\n-                                      size_t, RankId,\n-                                      const Executor&) override {\n+  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, ReductionKind reduction_kind,\n+                     const Executor& executor) override;\n+\n+  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n+                             se::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count,\n+                             std::optional<RankId> source_rank,\n+                             absl::Span<const RankId> target_ranks,\n+                             const Executor& executor) override;\n+\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+                    PrimitiveType dtype, size_t count,\n+                    const Executor& executor) override;\n+  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, const Executor& executor) override;\n+  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                         size_t count, ReductionKind reduction_kind,\n+                         const Executor& executor) override;\n+\n+  Future<> Broadcast(se::DeviceMemoryBase, se::DeviceMemoryBase, PrimitiveType,\n+                     size_t, RankId, const Executor&) override {\n     return Unimplemented(\"Broadcast is not implemented\");\n   }\n \n-  tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase, PrimitiveType, size_t,\n-                                 RankId, const Executor&) override {\n+  Future<> Send(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+                const Executor&) override {\n     return Unimplemented(\"Send is not implemented\");\n   }\n \n-  tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase, PrimitiveType, size_t,\n-                                 RankId, const Executor&) override {\n+  Future<> Recv(se::DeviceMemoryBase, PrimitiveType, size_t, RankId,\n+                const Executor&) override {\n     return Unimplemented(\"Recv is not implemented\");\n   }\n "
        },
        {
            "sha": "6cb7885b76860b2198e8e43848fede52b58f9d66",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -346,6 +346,7 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla/backends/cpu/collectives:cpu_collectives\",\n         \"//xla/core/collectives:communicator\",\n@@ -468,6 +469,7 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/backends/cpu/collectives:cpu_collectives\",\n@@ -495,6 +497,7 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/cpu/collectives:cpu_collectives\",\n@@ -520,6 +523,7 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n@@ -546,6 +550,7 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:xla_data_proto_cc\",\n@@ -576,6 +581,7 @@ cc_library(\n     hdrs = [\"collective_thunk.h\"],\n     deps = [\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n@@ -585,6 +591,7 @@ cc_library(\n         \"//xla/backends/cpu/collectives:cpu_collectives\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/service:buffer_assignment\","
        },
        {
            "sha": "48f791c366f6645a7e633179011931ec8109ee4d",
            "filename": "third_party/xla/xla/backends/cpu/runtime/all_gather_thunk.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_gather_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_gather_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_gather_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -18,23 +18,22 @@ limitations under the License.\n #include <cstdint>\n #include <memory>\n #include <utility>\n+#include <vector>\n \n-#include \"absl/base/optimization.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/log.h\"\n #include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/backends/cpu/runtime/collective_thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::cpu {\n@@ -73,32 +72,23 @@ tsl::AsyncValueRef<AllGatherThunk::ExecuteEvent> AllGatherThunk::Execute(\n         destination_buffer(i).ToString(), data.destination[i].opaque());\n   }\n \n-  return ExecuteWithCommunicator(\n+  Future<> future = ExecuteWithCommunicator(\n       params.collective_params,\n-      [&](const RendezvousKey& key,\n-          Communicator& comm) -> tsl::AsyncValueRef<Communicator::Event> {\n+      [&](const RendezvousKey& key, Communicator& comm) {\n         CpuCollectives::Executor executor(key, DefaultCollectiveTimeout());\n \n-        tsl::CountDownAsyncValueRef<Communicator::Event> state(\n-            data.source.size());\n-\n+        std::vector<Future<>> futures(data.source.size());\n         for (int32_t i = 0; i < data.source.size(); ++i) {\n           const Shape& shape = source_shape(i);\n-          auto communicator_event = comm.AllGather(\n-              data.source[i], data.destination[i], shape.element_type(),\n-              ShapeUtil::ElementsIn(shape), executor);\n-\n-          communicator_event.AndThen([state, communicator_event]() mutable {\n-            if (ABSL_PREDICT_FALSE(communicator_event.IsError())) {\n-              state.CountDown(communicator_event.GetError());\n-            } else {\n-              state.CountDown();\n-            }\n-          });\n+          futures[i] = comm.AllGather(data.source[i], data.destination[i],\n+                                      shape.element_type(),\n+                                      ShapeUtil::ElementsIn(shape), executor);\n         }\n \n-        return state.AsRef();\n+        return JoinFutures(futures);\n       });\n+\n+  return ToExecuteEvent(future);\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "094b1db86953324502a368f09f9883f57c06b8f2",
            "filename": "third_party/xla/xla/backends/cpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 19,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_reduce_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -19,26 +19,25 @@ limitations under the License.\n #include <cstring>\n #include <memory>\n #include <utility>\n+#include <vector>\n \n-#include \"absl/base/optimization.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/backends/cpu/runtime/collective_thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n@@ -99,31 +98,25 @@ tsl::AsyncValueRef<AllReduceThunk::ExecuteEvent> AllReduceThunk::Execute(\n     return OkExecuteEvent();\n   }\n \n-  return ExecuteWithCommunicator(\n+  Future<> future = ExecuteWithCommunicator(\n       params.collective_params,\n-      [&, data = std::move(data)](const RendezvousKey& key, Communicator& comm)\n-          -> tsl::AsyncValueRef<Communicator::Event> {\n-        tsl::CountDownAsyncValueRef<Communicator::Event> state(\n-            data.source.size());\n-\n+      [&, data = std::move(data)](const RendezvousKey& key,\n+                                  Communicator& comm) {\n         CpuCollectives::Executor executor(key, DefaultCollectiveTimeout());\n+        std::vector<Future<>> futures(data.source.size());\n+\n         for (int32_t i = 0; i < data.source.size(); ++i) {\n           const Shape& shape = destination_shape(i);\n \n-          auto communicator_event = comm.AllReduce(\n+          futures[i] = comm.AllReduce(\n               data.source[i], data.destination[i], shape.element_type(),\n               ShapeUtil::ElementsIn(shape), reduction_kind_, executor);\n-\n-          communicator_event.AndThen([state, communicator_event]() mutable {\n-            if (ABSL_PREDICT_FALSE(communicator_event.IsError())) {\n-              state.CountDown(communicator_event.GetError());\n-            } else {\n-              state.CountDown();\n-            }\n-          });\n         }\n-        return state.AsRef();\n+\n+        return JoinFutures(futures);\n       });\n+\n+  return ToExecuteEvent(future);\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "bce495c222cc48fe5441f7063e659202a5eee421",
            "filename": "third_party/xla/xla/backends/cpu/runtime/all_to_all_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_to_all_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -26,12 +26,12 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/collective_thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -71,7 +71,7 @@ tsl::AsyncValueRef<AllToAllThunk::ExecuteEvent> AllToAllThunk::Execute(\n         destination_buffer(i).ToString(), data.destination[i].opaque());\n   }\n \n-  return ExecuteWithCommunicator(\n+  Future<> future = ExecuteWithCommunicator(\n       params.collective_params,\n       [&](const RendezvousKey& key, Communicator& comm) {\n         CpuCollectives::Executor executor(key, DefaultCollectiveTimeout());\n@@ -81,6 +81,8 @@ tsl::AsyncValueRef<AllToAllThunk::ExecuteEvent> AllToAllThunk::Execute(\n                              std::move(data.destination), shape.element_type(),\n                              ShapeUtil::ElementsIn(shape), executor);\n       });\n+\n+  return ToExecuteEvent(future);\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "8b3f6dd537fbed71895dadd92bbedcde3f3d5273",
            "filename": "third_party/xla/xla/backends/cpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 16,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -22,11 +22,9 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/base/optimization.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/log.h\"\n #include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n@@ -37,14 +35,14 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::cpu {\n@@ -127,31 +125,24 @@ CollectivePermuteThunk::Execute(const ExecuteParams& params) {\n         destination_buffer(i).ToString(), data.destination[i].opaque());\n   }\n \n-  return ExecuteWithCommunicator(\n+  Future<> future = ExecuteWithCommunicator(\n       params.collective_params,\n       [&](const RendezvousKey& key, Communicator& comm) {\n         CpuCollectives::Executor executor(key, DefaultCollectiveTimeout());\n-        tsl::CountDownAsyncValueRef<Communicator::Event> state(\n-            data.source.size());\n+        std::vector<Future<>> futures(data.source.size());\n         for (int32_t i = 0; i < data.source.size(); ++i) {\n           const Shape& shape = source_shape(i);\n \n-          auto communicator_event = comm.CollectivePermute(\n+          futures[i] = comm.CollectivePermute(\n               data.source[i], data.destination[i], shape.element_type(),\n               ShapeUtil::ElementsIn(shape), source_replica_id, copy_to,\n               executor);\n-\n-          communicator_event.AndThen([state, communicator_event]() mutable {\n-            if (ABSL_PREDICT_FALSE(communicator_event.IsError())) {\n-              state.CountDown(communicator_event.GetError());\n-            } else {\n-              state.CountDown();\n-            }\n-          });\n         }\n \n-        return state.AsRef();\n+        return JoinFutures(futures);\n       });\n+\n+  return ToExecuteEvent(future);\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "bed45d3d4491b6daebc5de349f33c30ad16c1980",
            "filename": "third_party/xla/xla/backends/cpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -40,6 +40,8 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -50,7 +52,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -191,8 +192,7 @@ absl::StatusOr<int32_t> CollectiveThunk::RankInGlobalDevices(\n   return std::distance(key.global_devices.begin(), it);\n }\n \n-tsl::AsyncValueRef<CollectiveThunk::ExecuteEvent>\n-CollectiveThunk::ExecuteWithCommunicator(\n+Future<> CollectiveThunk::ExecuteWithCommunicator(\n     const Thunk::CollectiveExecuteParams* params, Callback callback) {\n   // Check that we have access to collectives interface implementation and\n   // parameters that define our \"position\" in a collective clique."
        },
        {
            "sha": "f4fc932161df7d5ec6fcf0852b316e3c61781749",
            "filename": "third_party/xla/xla/backends/cpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 19,
            "deletions": 5,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -30,13 +30,14 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n+#include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n #include \"xla/runtime/resource_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/global_device_id.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -105,8 +106,8 @@ class CollectiveThunk : public Thunk {\n   ResourceUses resource_uses() const final;\n \n   // Callback for collective thunk implementations.\n-  using Callback = absl::AnyInvocable<tsl::AsyncValueRef<Communicator::Event>(\n-      const RendezvousKey& key, Communicator& comm)>;\n+  using Callback = absl::AnyInvocable<Future<>(const RendezvousKey& key,\n+                                               Communicator& comm)>;\n \n   static bool IsDataTypeSupportedByCollectiveReduce(PrimitiveType datatype);\n \n@@ -120,8 +121,8 @@ class CollectiveThunk : public Thunk {\n \n   // Acquires collective communicator for the given parameters and executes the\n   // user provided callback with acquired rendezvous key, rank and communicator.\n-  tsl::AsyncValueRef<ExecuteEvent> ExecuteWithCommunicator(\n-      const Thunk::CollectiveExecuteParams* params, Callback callback);\n+  Future<> ExecuteWithCommunicator(const Thunk::CollectiveExecuteParams* params,\n+                                   Callback callback);\n \n   const BufferAllocation::Slice& source_buffer(int64_t index) const;\n   absl::Span<const BufferAllocation::Slice> source_buffers() const;\n@@ -137,6 +138,19 @@ class CollectiveThunk : public Thunk {\n   // thread pool, owned by the underlying collective implementation.\n   bool ExecutesOnExternalThreadPool() const final { return true; }\n \n+ protected:\n+  tsl::AsyncValueRef<ExecuteEvent> ToExecuteEvent(Future<> future) {\n+    auto event = tsl::MakeConstructedAsyncValueRef<ExecuteEvent>();\n+    future.OnReady([event](absl::Status status) {\n+      if (!status.ok()) {\n+        event.SetError(status);\n+      } else {\n+        event.SetStateConcrete();\n+      }\n+    });\n+    return event;\n+  }\n+\n  private:\n   OpParams op_params_;\n   OpBuffers op_buffers_;"
        },
        {
            "sha": "67b9929ef6f02d9c4abd8058840da60c50de547b",
            "filename": "third_party/xla/xla/backends/cpu/runtime/reduce_scatter_thunk.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 17,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Freduce_scatter_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Freduce_scatter_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Freduce_scatter_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -18,24 +18,23 @@ limitations under the License.\n #include <cstdint>\n #include <memory>\n #include <utility>\n+#include <vector>\n \n-#include \"absl/base/optimization.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/log.h\"\n #include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/backends/cpu/runtime/collective_thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n@@ -85,31 +84,23 @@ ReduceScatterThunk::Execute(const ExecuteParams& params) {\n         destination_buffer(i).ToString(), data.destination[i].opaque());\n   }\n \n-  return ExecuteWithCommunicator(\n+  Future<> future = ExecuteWithCommunicator(\n       params.collective_params,\n       [&](const RendezvousKey& key, Communicator& comm) {\n         CpuCollectives::Executor executor(key, DefaultCollectiveTimeout());\n-\n-        tsl::CountDownAsyncValueRef<Communicator::Event> state(\n-            data.source.size());\n+        std::vector<Future<>> futures(data.source.size());\n \n         for (int32_t i = 0; i < data.source.size(); ++i) {\n           const Shape& shape = destination_shape(i);\n-          auto communicator_event = comm.ReduceScatter(\n+          futures[i] = comm.ReduceScatter(\n               data.source[i], data.destination[i], shape.element_type(),\n               ShapeUtil::ElementsIn(shape), reduction_kind_, executor);\n-\n-          communicator_event.AndThen([state, communicator_event]() mutable {\n-            if (ABSL_PREDICT_FALSE(communicator_event.IsError())) {\n-              state.CountDown(communicator_event.GetError());\n-            } else {\n-              state.CountDown();\n-            }\n-          });\n         }\n \n-        return state.AsRef();\n+        return JoinFutures(futures);\n       });\n+\n+  return ToExecuteEvent(future);\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "1e41a8bfabf6d3f143faa1a39e261e40b25bbe41",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -215,6 +215,7 @@ cc_library(\n     name = \"gpu_communicator\",\n     hdrs = [\"gpu_communicator.h\"],\n     deps = [\n+        \"//xla:future\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n@@ -359,6 +360,7 @@ cc_library(\n         \":gpu_communicator\",\n         \":nccl_errors\",\n         \":single_threaded_executor\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n@@ -426,7 +428,9 @@ cc_library(\n     visibility = [\"//visibility:private\"],\n     deps = [\n         \":gpu_collectives\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/core/collectives\",\n         \"//xla/core/collectives:clique_id\",\n@@ -485,6 +489,7 @@ xla_test(\n         \":nccl_collectives\",\n         \":nccl_communicator\",\n         \":nccl_errors\",\n+        \"//xla:future\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/service:collective_ops_utils\",\n@@ -495,6 +500,7 @@ xla_test(\n         \"//xla/tsl/platform:status_matchers\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/utility\","
        },
        {
            "sha": "dfad79314470a947cadd37c9e3a82c69533565dd",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_communicator.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_communicator.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -25,9 +25,9 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n \n namespace xla::gpu {\n \n@@ -44,7 +44,7 @@ class GpuCommunicator : public Communicator {\n \n   // Executes f in a group. f should invoke synchronous collective methods like\n   // LaunchAllReduce and not asynchronous collective methods like AllReduce.\n-  virtual tsl::AsyncValueRef<Communicator::Event> GroupExecute(\n+  virtual Future<> GroupExecute(\n       absl::AnyInvocable<absl::Status(GpuCommunicator*)> f) = 0;\n \n   virtual absl::Status LaunchAllReduce(se::DeviceMemoryBase send_buffer,"
        },
        {
            "sha": "a9784d06adb39f6d09ede5e6124bb893c017cca4",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 73,
            "deletions": 122,
            "changes": 195,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n-#include <tuple>\n #include <utility>\n #include <vector>\n \n@@ -40,14 +39,13 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/single_threaded_executor.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_stream.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/concurrency/async_value.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/executor.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -70,34 +68,6 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n-// Blocks until ref is ready and returns its value (or error).\n-template <typename T>\n-absl::StatusOr<T> BlockAndGet(tsl::AsyncValueRef<T> ref) {\n-  tsl::BlockUntilReady(ref);\n-  if (ref.IsError()) {\n-    return ref.GetError();\n-  }\n-  return std::move(std::move(ref).get());\n-}\n-\n-// Blocks until ref is ready and returns its value (or error).\n-absl::Status BlockAndGet(tsl::AsyncValueRef<absl::Status> ref) {\n-  tsl::BlockUntilReady(ref);\n-  if (ref.IsError()) {\n-    return ref.GetError();\n-  }\n-  return ref.get();\n-}\n-\n-// Blocks until ref is ready and returns absl::OkStatus() (or error).\n-absl::Status BlockAndGet(tsl::AsyncValueRef<NcclCommunicator::Event> ref) {\n-  tsl::BlockUntilReady(ref);\n-  if (ref.IsError()) {\n-    return ref.GetError();\n-  }\n-  return absl::OkStatus();\n-}\n-\n se::Stream* ToStream(const Communicator::Executor& executor) {\n   return tsl::down_cast<const GpuCollectives::Executor&>(executor).stream();\n }\n@@ -211,10 +181,7 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n         XLA_NCCL_RETURN_IF_ERROR(ncclCommDeregister(comm_.comm(), handle_));\n         return comm_.PollUntilDone();\n       };\n-      if (!executor_) {\n-        return f();\n-      }\n-      return BlockAndGet(tsl::MakeAsyncValueRef(*executor_, f));\n+      return executor_ ? Future<>::MakeOn(*executor_, f).Await() : f();\n #else\n       return Unimplemented(\n           \"[%d] NCCL version does not support ncclCommDeregister\",\n@@ -235,10 +202,7 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n             ncclCommWindowDeregister(comm_.comm(), *(ncclWindow_t*)(handle_)));\n         return comm_.PollUntilDone();\n       };\n-      if (!executor_) {\n-        return f();\n-      }\n-      return BlockAndGet(tsl::MakeAsyncValueRef(*executor_, f));\n+      return executor_ ? Future<>::MakeOn(*executor_, f).Await() : f();\n #else\n       return Unimplemented(\n           \"[%d] NCCL version does not support ncclCommWindowDeregister\",\n@@ -281,7 +245,7 @@ absl::StatusOr<std::unique_ptr<NcclCommunicator>> NcclCommunicator::Create(\n   // single threaded executor.\n   auto executor = std::make_unique<SingleThreadedExecutor>(env);\n   TF_ASSIGN_OR_RETURN(ncclComm_t comm,\n-                      BlockAndGet(tsl::TryMakeAsyncValueRef(*executor, f)));\n+                      Future<ncclComm_t>::MakeOn(*executor, f).Await());\n   return absl::WrapUnique(new NcclCommunicator(comm, std::move(executor)));\n }\n \n@@ -303,7 +267,7 @@ NcclCommunicator::~NcclCommunicator() {\n     return XLA_NCCL_STATUS(ncclCommDestroy(comm_));\n   };\n \n-  if (absl::Status s = BlockAndGet(Execute(f)); !s.ok()) {\n+  if (absl::Status s = Execute(f).Await(); !s.ok()) {\n     LOG(ERROR) << \"NcclCommunicator::~NcclCommunicator: \" << s;\n   }\n }\n@@ -313,20 +277,20 @@ absl::Status NcclCommunicator::Abort() {\n   // executor_ will cancel. This will allow the aborting lambda below to run.\n   canceling_.store(true);\n \n-  return BlockAndGet(Execute([this]() -> absl::Status {\n+  return ExecuteAwait([this]() -> absl::Status {\n     VLOG(1) << \"Abort NCCL communicator: \" << *this;\n     if (aborted_) {\n       return FailedPrecondition(\"NcclCommunicator already aborted\");\n     }\n     aborted_ = true;\n-    // Note that we intentionally don't call PollUntilDone. Once comm_ has been\n-    // aborted, we can no longer safely touch it.\n+    // Note that we intentionally don't call PollUntilDone. Once comm_\n+    // has been aborted, we can no longer safely touch it.\n     return XLA_NCCL_STATUS(ncclCommAbort(comm_));\n-  }));\n+  });\n }\n \n absl::Status NcclCommunicator::HealthCheck() const {\n-  return BlockAndGet(Execute([this]() -> absl::Status {\n+  return ExecuteAwait([this]() -> absl::Status {\n     VLOG(5) << \"Get last async error for NCCL communicator: \" << *this;\n     if (canceling_.load()) {\n       return absl::FailedPreconditionError(\"NcclCommunicator aborted\");\n@@ -340,21 +304,22 @@ absl::Status NcclCommunicator::HealthCheck() const {\n \n     return Internal(\"%s. Last NCCL error (maybe unrelated): %s\",\n                     ncclGetLastError(comm_), ncclGetErrorString(async_err));\n-  }));\n+  });\n }\n \n absl::StatusOr<size_t> NcclCommunicator::NumRanks() const {\n-  return BlockAndGet(Execute<size_t>([this]() -> absl::StatusOr<size_t> {\n+  return ExecuteAwait<size_t>([this]() -> absl::StatusOr<size_t> {\n     VLOG(5) << \"Get the number of ranks in NCCL communicator: \" << *this;\n     if (canceling_.load()) {\n       return absl::FailedPreconditionError(\"NcclCommunicator aborted\");\n     }\n \n-    // We intentionally don't call PollUntilDone. ncclCommCount is blocking.\n+    // We intentionally don't call PollUntilDone. ncclCommCount is\n+    // blocking.\n     int32_t count = 0;\n     XLA_NCCL_RETURN_IF_ERROR(ncclCommCount(comm_, &count));\n     return count;\n-  }));\n+  });\n }\n \n absl::Status NcclCommunicator::RegisterBufferOnce(\n@@ -396,10 +361,11 @@ NcclCommunicator::RegisterBuffer(stream_executor::DeviceMemoryBase buffer,\n   using Handle = std::unique_ptr<Communicator::RegisteredBufferHandle>;\n \n   if (!use_symmetric_buffer) {\n-    return BlockAndGet(Execute<Handle>(\n+    return ExecuteAwait<Handle>(\n         [&buffer, device_ordinal, this]() -> absl::StatusOr<Handle> {\n           VLOG(3) << absl::StreamFormat(\n-              \"[%d] Register buffer for NCCL communicator; buffer=%p; size=%d; \"\n+              \"[%d] Register buffer for NCCL communicator; buffer=%p; \"\n+              \"size=%d; \"\n               \"comm=%p\",\n               device_ordinal, buffer.opaque(), buffer.size(), comm_);\n           if (canceling_.load()) {\n@@ -414,33 +380,32 @@ NcclCommunicator::RegisterBuffer(stream_executor::DeviceMemoryBase buffer,\n           return std::make_unique<NcclRegisteredBufferHandle>(\n               *this, handle, executor_.get(), /*symmetric_buffer= */ false,\n               device_ordinal);\n-        }));\n+        });\n #else\n   return Unimplemented(\"[%d] NCCL version does not support ncclCommRegister\",\n                        device_ordinal);\n #endif  // NCCL_VERSION_CODE >= 21901\n   } else {\n #if (NCCL_VERSION_CODE >= 22700)\n-    return BlockAndGet(Execute<Handle>([&buffer, device_ordinal,\n-                                        this]() -> absl::StatusOr<Handle> {\n-      VLOG(3) << absl::StreamFormat(\n-          \"[%d] Register symmetric buffer for NCCL communicator; buffer=%p; \"\n-          \"size=%d; \"\n-          \"comm=%p\",\n-          device_ordinal, buffer.opaque(), buffer.size(), comm_);\n-      void* handle = nullptr;\n-      XLA_NCCL_RETURN_IF_ERROR(ncclGroupStart());\n-      XLA_NCCL_RETURN_IF_ERROR(ncclCommWindowRegister(\n-          comm_, buffer.opaque(), buffer.size(), (ncclWindow_t*)&handle,\n-          NCCL_WIN_COLL_SYMMETRIC));\n-      XLA_NCCL_RETURN_IF_ERROR(ncclGroupEnd());\n-      if (group_nesting_level_ == 0) {\n-        TF_RETURN_IF_ERROR(PollUntilDone());\n-      }\n-      return std::make_unique<NcclRegisteredBufferHandle>(\n-          *this, handle, executor_.get(), /*symmetric_buffer= */ true,\n-          device_ordinal);\n-    }));\n+    return ExecuteAwait<Handle>(\n+        [&buffer, device_ordinal, this]() -> absl::StatusOr<Handle> {\n+          VLOG(3) << absl::StreamFormat(\n+              \"[%d] Register symmetric buffer for NCCL communicator; \"\n+              \"buffer=%p; size=%d; comm=%p\",\n+              device_ordinal, buffer.opaque(), buffer.size(), comm_);\n+          void* handle = nullptr;\n+          XLA_NCCL_RETURN_IF_ERROR(ncclGroupStart());\n+          XLA_NCCL_RETURN_IF_ERROR(ncclCommWindowRegister(\n+              comm_, buffer.opaque(), buffer.size(), (ncclWindow_t*)&handle,\n+              NCCL_WIN_COLL_SYMMETRIC));\n+          XLA_NCCL_RETURN_IF_ERROR(ncclGroupEnd());\n+          if (group_nesting_level_ == 0) {\n+            TF_RETURN_IF_ERROR(PollUntilDone());\n+          }\n+          return std::make_unique<NcclRegisteredBufferHandle>(\n+              *this, handle, executor_.get(),\n+              /*symmetric_buffer= */ true, device_ordinal);\n+        });\n #else\n   return Unimplemented(\n       \"[%d] NCCL version does not support ncclCommWindowRegister\",\n@@ -449,7 +414,7 @@ NcclCommunicator::RegisterBuffer(stream_executor::DeviceMemoryBase buffer,\n   }\n }\n \n-tsl::AsyncValueRef<Communicator::Event> NcclCommunicator::GroupExecute(\n+Future<> NcclCommunicator::GroupExecute(\n     absl::AnyInvocable<absl::Status(GpuCommunicator*)> f) {\n   return Execute([f = std::move(f), this]() mutable -> absl::Status {\n     TF_RETURN_IF_ERROR(GroupStart());\n@@ -459,47 +424,51 @@ tsl::AsyncValueRef<Communicator::Event> NcclCommunicator::GroupExecute(\n   });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::AllReduce(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n-    const Communicator::Executor& executor) {\n+Future<> NcclCommunicator::AllReduce(se::DeviceMemoryBase send_buffer,\n+                                     se::DeviceMemoryBase recv_buffer,\n+                                     PrimitiveType dtype, size_t count,\n+                                     ReductionKind reduction_kind,\n+                                     const Communicator::Executor& executor) {\n   return Execute([send_buffer, recv_buffer, dtype, count, reduction_kind,\n                   &executor, this]() -> absl::Status {\n     return LaunchAllReduce(send_buffer, recv_buffer, dtype, count,\n                            reduction_kind, executor);\n   });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::Broadcast(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, RankId root, const Executor& executor) {\n+Future<> NcclCommunicator::Broadcast(se::DeviceMemoryBase send_buffer,\n+                                     se::DeviceMemoryBase recv_buffer,\n+                                     PrimitiveType dtype, size_t count,\n+                                     RankId root, const Executor& executor) {\n   return Execute(\n       [send_buffer, recv_buffer, dtype, count, root, &executor, this]() {\n         return LaunchBroadcast(send_buffer, recv_buffer, dtype, count, root,\n                                executor);\n       });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::ReduceScatter(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n-    const Executor& executor) {\n+Future<> NcclCommunicator::ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                                         se::DeviceMemoryBase recv_buffer,\n+                                         PrimitiveType dtype, size_t count,\n+                                         ReductionKind reduction_kind,\n+                                         const Executor& executor) {\n   return Execute([send_buffer, recv_buffer, dtype, count, reduction_kind,\n                   &executor, this]() {\n     return LaunchReduceScatter(send_buffer, recv_buffer, dtype, count,\n                                reduction_kind, executor);\n   });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::AllGather(\n-    se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-    PrimitiveType dtype, size_t count, const Executor& executor) {\n+Future<> NcclCommunicator::AllGather(se::DeviceMemoryBase send_buffer,\n+                                     se::DeviceMemoryBase recv_buffer,\n+                                     PrimitiveType dtype, size_t count,\n+                                     const Executor& executor) {\n   return Execute([send_buffer, recv_buffer, dtype, count, &executor, this]() {\n     return LaunchAllGather(send_buffer, recv_buffer, dtype, count, executor);\n   });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::AllToAll(\n+Future<> NcclCommunicator::AllToAll(\n     absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n     absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n     PrimitiveType dtype, size_t count, const Executor& executor) {\n@@ -508,7 +477,7 @@ tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::AllToAll(\n   });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::CollectivePermute(\n+Future<> NcclCommunicator::CollectivePermute(\n     se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n     PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n     absl::Span<const RankId> target_ranks, const Executor& executor) {\n@@ -522,17 +491,17 @@ tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::CollectivePermute(\n   });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::Send(\n-    se::DeviceMemoryBase send_buffer, PrimitiveType dtype, size_t count,\n-    RankId peer, const Executor& executor) {\n+Future<> NcclCommunicator::Send(se::DeviceMemoryBase send_buffer,\n+                                PrimitiveType dtype, size_t count, RankId peer,\n+                                const Executor& executor) {\n   return Execute([send_buffer, dtype, count, peer, &executor, this]() {\n     return LaunchSend(send_buffer, dtype, count, peer, executor);\n   });\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::Recv(\n-    se::DeviceMemoryBase recv_buffer, PrimitiveType dtype, size_t count,\n-    RankId peer, const Executor& executor) {\n+Future<> NcclCommunicator::Recv(se::DeviceMemoryBase recv_buffer,\n+                                PrimitiveType dtype, size_t count, RankId peer,\n+                                const Executor& executor) {\n   return Execute([recv_buffer, dtype, count, peer, &executor, this]() {\n     return LaunchRecv(recv_buffer, dtype, count, peer, executor);\n   });\n@@ -846,35 +815,17 @@ absl::Status NcclCommunicator::PollUntilDone() const {\n   return ::xla::gpu::PollUntilDone(comm_, canceling_);\n }\n \n-tsl::AsyncValueRef<NcclCommunicator::Event> NcclCommunicator::Execute(\n-    absl::AnyInvocable<absl::Status()> f) const {\n-  if (!executor_) {\n-    // Execute on the calling thread.\n-    TF_RETURN_IF_ERROR(std::move(f)());\n-    return OkEvent();\n-  }\n-\n-  // Execute on executor_.\n-  return tsl::TryMakeAsyncValueRef(\n-      *executor_,\n-      [f = std::move(f)]() mutable -> absl::StatusOr<NcclCommunicator::Event> {\n-        TF_RETURN_IF_ERROR(std::move(f)());\n-        return NcclCommunicator::Event{};\n-      });\n+Future<> NcclCommunicator::Execute(\n+    absl::AnyInvocable<absl::Status() &&> f) const {\n+  return executor_ ? Future<>::MakeOn(*executor_, std::move(f))\n+                   : Future<>(std::move(f)());\n }\n \n template <typename T>\n-tsl::AsyncValueRef<T> NcclCommunicator::Execute(\n-    absl::AnyInvocable<absl::StatusOr<T>()> f) const {\n-  if (!executor_) {\n-    // Execute on the calling thread.\n-    auto ref = tsl::MakeUnconstructedAsyncValueRef<T>();\n-    ref.emplace(std::move(f)());\n-    return ref;\n-  }\n-\n-  // Execute on executor_.\n-  return tsl::TryMakeAsyncValueRef(*executor_, std::move(f));\n+Future<T> NcclCommunicator::Execute(\n+    absl::AnyInvocable<absl::StatusOr<T>() &&> f) const {\n+  return executor_ ? Future<T>::MakeOn(*executor_, std::move(f))\n+                   : Future<T>(std::move(f)());\n }\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "379f02ced597b332bcdc1b4a6b53fde0becbe065",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.h",
            "status": "modified",
            "additions": 46,
            "deletions": 42,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n-#include <tuple>\n #include <utility>\n \n #include \"absl/base/thread_annotations.h\"\n@@ -36,6 +35,7 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n@@ -90,49 +90,44 @@ class NcclCommunicator : public GpuCommunicator {\n                                   int device_ordinal,\n                                   bool use_symmetric_buffer) final;\n \n-  tsl::AsyncValueRef<Communicator::Event> GroupExecute(\n+  Future<> GroupExecute(\n       absl::AnyInvocable<absl::Status(GpuCommunicator*)> f) final;\n \n-  tsl::AsyncValueRef<Event> AllReduce(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      ReductionKind reduction_kind,\n-                                      const Executor& executor) final;\n-\n-  tsl::AsyncValueRef<Event> Broadcast(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      RankId root,\n-                                      const Executor& executor) final;\n-\n-  tsl::AsyncValueRef<Event> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                          se::DeviceMemoryBase recv_buffer,\n-                                          PrimitiveType dtype, size_t count,\n-                                          ReductionKind reduction_kind,\n-                                          const Executor& executor) final;\n-\n-  tsl::AsyncValueRef<Event> AllGather(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      const Executor& executor) final;\n-\n-  tsl::AsyncValueRef<Event> AllToAll(\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n-      PrimitiveType dtype, size_t count, const Executor& executor) final;\n+  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, ReductionKind reduction_kind,\n+                     const Executor& executor) final;\n+\n+  Future<> Broadcast(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, RankId root, const Executor& executor) final;\n+\n+  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                         size_t count, ReductionKind reduction_kind,\n+                         const Executor& executor) final;\n+\n+  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, const Executor& executor) final;\n \n-  tsl::AsyncValueRef<Event> CollectivePermute(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-      PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n-      absl::Span<const RankId> target_ranks, const Executor& executor) final;\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+                    PrimitiveType dtype, size_t count,\n+                    const Executor& executor) final;\n \n-  tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase send_buffer,\n-                                 PrimitiveType dtype, size_t count, RankId peer,\n-                                 const Executor& executor) final;\n+  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n+                             se::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count,\n+                             std::optional<RankId> source_rank,\n+                             absl::Span<const RankId> target_ranks,\n+                             const Executor& executor) final;\n \n-  tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase recv_buffer,\n-                                 PrimitiveType dtype, size_t count, RankId peer,\n-                                 const Executor& executor) final;\n+  Future<> Send(se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+                size_t count, RankId peer, const Executor& executor) final;\n+\n+  Future<> Recv(se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                size_t count, RankId peer, const Executor& executor) final;\n \n   std::string ToString() const final;\n \n@@ -201,12 +196,21 @@ class NcclCommunicator : public GpuCommunicator {\n   absl::Status PollUntilDone() const;\n \n   // Executes f on executor_, or calls f directly if executor_ is null.\n-  tsl::AsyncValueRef<Event> Execute(absl::AnyInvocable<absl::Status()> f) const;\n+  Future<> Execute(absl::AnyInvocable<absl::Status() &&> f) const;\n \n   // Executes f on executor_, or calls f directly if executor_ is null.\n   template <typename T>\n-  tsl::AsyncValueRef<T> Execute(\n-      absl::AnyInvocable<absl::StatusOr<T>()> f) const;\n+  Future<T> Execute(absl::AnyInvocable<absl::StatusOr<T>() &&> f) const;\n+\n+  absl::Status ExecuteAwait(absl::AnyInvocable<absl::Status() &&> f) const {\n+    return Execute(std::move(f)).Await();\n+  }\n+\n+  template <typename T>\n+  absl::StatusOr<T> ExecuteAwait(\n+      absl::AnyInvocable<absl::StatusOr<T>() &&> f) const {\n+    return Execute<T>(std::move(f)).Await();\n+  }\n \n   // Underlying NCCL communicator.\n   ncclComm_t comm_;"
        },
        {
            "sha": "2251303670f28fdfdefdb9c25a4291dd48382065",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator_test.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -23,17 +23,16 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/collectives/nccl_errors.h\"\n-#include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n \n #if TENSORFLOW_USE_ROCM\n #include \"rocm/rocm_config.h\"\n@@ -50,8 +49,6 @@ namespace xla::gpu {\n namespace {\n \n using ::testing::HasSubstr;\n-using ::tsl::testing::IsOk;\n-using ::tsl::testing::StatusIs;\n \n constexpr absl::string_view kCudaError = \"unhandled cuda error\";\n \n@@ -60,10 +57,8 @@ void AssertAborted(absl::Status s) {\n                                         HasSubstr(\"aborted\")));\n };\n \n-void AssertEventAborted(tsl::AsyncValueRef<Communicator::Event> event) {\n-  tsl::BlockUntilReady(event);\n-  ASSERT_TRUE(event.IsError());\n-  ASSERT_THAT(event.GetError(),\n+void AssertEventAborted(Future<> future) {\n+  ASSERT_THAT(future.Await(),\n               absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition,\n                                      HasSubstr(\"aborted\")));\n };"
        },
        {
            "sha": "7e62541ba0ede8c9a8f809339f2b75e3197685a1",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_communicator.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -27,15 +27,19 @@ limitations under the License.\n #include \"third_party/nvshmem/nvshmemx.h\"  // IWYU pragma: keep\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/collectives/nvshmem_collectives.h\"\n+#include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_stream.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/casts.h\"\n \n namespace xla::gpu {\n \n@@ -208,7 +212,7 @@ absl::StatusOr<size_t> NvshmemCommunicator::CurrentRank() {\n   return rank;\n }\n \n-tsl::AsyncValueRef<NvshmemCommunicator::Event> NvshmemCommunicator::AllReduce(\n+Future<> NvshmemCommunicator::AllReduce(\n     se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n     PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n     const Communicator::Executor& executor) {\n@@ -292,7 +296,7 @@ tsl::AsyncValueRef<NvshmemCommunicator::Event> NvshmemCommunicator::AllReduce(\n     default:\n       return absl::InternalError(\"Invalid Nvshmem reduction type.\");\n   }\n-  return OkEvent();\n+  return absl::OkStatus();\n }\n \n std::string NvshmemCommunicator::ToString() const {\n@@ -430,9 +434,10 @@ absl::Status NvshmemCommunicator::P2P(absl::string_view op_name,\n   return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<NvshmemCommunicator::Event> NvshmemCommunicator::Send(\n-    se::DeviceMemoryBase recv_buffer, se::DeviceMemoryBase send_buffer,\n-    PrimitiveType dtype, size_t count, RankId peer, const Executor& executor) {\n+Future<> NvshmemCommunicator::Send(se::DeviceMemoryBase recv_buffer,\n+                                   se::DeviceMemoryBase send_buffer,\n+                                   PrimitiveType dtype, size_t count,\n+                                   RankId peer, const Executor& executor) {\n   VLOG(1) << \"Send NVSHMEM communicator: \" << ToString();\n   if (aborted_) {\n     return absl::FailedPreconditionError(\"NvshmemCommunicator aborted\");\n@@ -444,12 +449,13 @@ tsl::AsyncValueRef<NvshmemCommunicator::Event> NvshmemCommunicator::Send(\n   count = ToRealCount(dtype, count);\n   TF_RETURN_IF_ERROR(\n       P2P(\"send\", dtype, recv_buffer, send_buffer, count, peer, executor));\n-  return tsl::MakeAvailableAsyncValueRef<Event>();\n+  return absl::OkStatus();\n }\n \n-tsl::AsyncValueRef<NvshmemCommunicator::Event> NvshmemCommunicator::Recv(\n-    se::DeviceMemoryBase recv_buffer, se::DeviceMemoryBase send_buffer,\n-    PrimitiveType dtype, size_t count, RankId peer, const Executor& executor) {\n+Future<> NvshmemCommunicator::Recv(se::DeviceMemoryBase recv_buffer,\n+                                   se::DeviceMemoryBase send_buffer,\n+                                   PrimitiveType dtype, size_t count,\n+                                   RankId peer, const Executor& executor) {\n   VLOG(1) << \"Recv NVSHMEM communicator: \" << ToString();\n   if (aborted_) {\n     return absl::FailedPreconditionError(\"NvshmemCommunicator aborted\");\n@@ -461,7 +467,7 @@ tsl::AsyncValueRef<NvshmemCommunicator::Event> NvshmemCommunicator::Recv(\n   count = ToRealCount(dtype, count);\n   TF_RETURN_IF_ERROR(\n       P2P(\"recv\", dtype, recv_buffer, send_buffer, count, peer, executor));\n-  return tsl::MakeAvailableAsyncValueRef<Event>();\n+  return absl::OkStatus();\n }\n \n absl::Status NvshmemCommunicator::Quiet(const Executor& executor) {"
        },
        {
            "sha": "8e48e8ff19676f7b607045a09355ac715fdbdfb5",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_communicator.h",
            "status": "modified",
            "additions": 37,
            "deletions": 42,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -55,70 +56,64 @@ class NvshmemCommunicator : public Communicator {\n \n   absl::Status Barrier(const Executor& executor) final;\n \n-  tsl::AsyncValueRef<Event> AllReduce(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      ReductionKind reduction_kind,\n-                                      const Executor& executor) final;\n-\n-  tsl::AsyncValueRef<Event> Broadcast(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      RankId root,\n-                                      const Executor& executor) final {\n+  Future<> AllReduce(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, ReductionKind reduction_kind,\n+                     const Executor& executor) final;\n+\n+  Future<> Broadcast(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, RankId root,\n+                     const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  tsl::AsyncValueRef<Event> ReduceScatter(se::DeviceMemoryBase send_buffer,\n-                                          se::DeviceMemoryBase recv_buffer,\n-                                          PrimitiveType dtype, size_t count,\n-                                          ReductionKind reduction_kind,\n-                                          const Executor& executor) final {\n+  Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                         size_t count, ReductionKind reduction_kind,\n+                         const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  tsl::AsyncValueRef<Event> AllGather(se::DeviceMemoryBase send_buffer,\n-                                      se::DeviceMemoryBase recv_buffer,\n-                                      PrimitiveType dtype, size_t count,\n-                                      const Executor& executor) final {\n+  Future<> AllGather(se::DeviceMemoryBase send_buffer,\n+                     se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                     size_t count, const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  tsl::AsyncValueRef<Event> AllToAll(\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n-      absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n-      PrimitiveType dtype, size_t count, const Executor& executor) final {\n+  Future<> AllToAll(absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n+                    absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n+                    PrimitiveType dtype, size_t count,\n+                    const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  tsl::AsyncValueRef<Event> CollectivePermute(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-      PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n-      absl::Span<const RankId> target_ranks, const Executor& executor) final {\n+  Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n+                             se::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count,\n+                             std::optional<RankId> source_rank,\n+                             absl::Span<const RankId> target_ranks,\n+                             const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase send_buffer,\n-                                 PrimitiveType dtype, size_t count, RankId peer,\n-                                 const Executor& executor) final {\n+  Future<> Send(se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+                size_t count, RankId peer, const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase recv_buffer,\n-                                 PrimitiveType dtype, size_t count, RankId peer,\n-                                 const Executor& executor) final {\n+  Future<> Recv(se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                size_t count, RankId peer, const Executor& executor) final {\n     return absl::UnimplementedError(\"Not implemented.\");\n   };\n \n-  tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase recv_buffer,\n-                                 se::DeviceMemoryBase send_buffer,\n-                                 PrimitiveType dtype, size_t count, RankId peer,\n-                                 const Executor& executor) final;\n+  Future<> Send(se::DeviceMemoryBase recv_buffer,\n+                se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+                size_t count, RankId peer, const Executor& executor) final;\n \n-  tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase recv_buffer,\n-                                 se::DeviceMemoryBase send_buffer,\n-                                 PrimitiveType dtype, size_t count, RankId peer,\n-                                 const Executor& executor) final;\n+  Future<> Recv(se::DeviceMemoryBase recv_buffer,\n+                se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+                size_t count, RankId peer, const Executor& executor) final;\n \n   absl::Status Quiet(const Executor& executor) final;\n "
        },
        {
            "sha": "d8def8aa3a11ad61692346aa44c66b9468517b2d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -1067,11 +1067,13 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/backends/gpu/collectives:gpu_communicator\",\n         \"//xla/core/collectives:communicator\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:backend_configs_cc\",\n@@ -1174,6 +1176,7 @@ cc_library(\n         \":collective_kernel_thunk\",\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:status_macros\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n@@ -1248,6 +1251,7 @@ cc_library(\n         \":collective_thunk\",\n         \":ragged_all_to_all\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n@@ -1291,6 +1295,7 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \"//xla:future\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n@@ -1498,6 +1503,7 @@ cc_library(\n         \":collective_thunk\",\n         \":thunk\",\n         \":thunk_id\",\n+        \"//xla:future\",\n         \"//xla:util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_communicator\",\n@@ -2131,6 +2137,7 @@ cc_library(\n     srcs = [\"ragged_all_to_all.cc\"],\n     hdrs = [\"ragged_all_to_all.h\"],\n     deps = [\n+        \"//xla:future\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n         \"//xla:util\",\n@@ -2361,6 +2368,7 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:computation_placer\",\n@@ -2456,13 +2464,15 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/core/collectives:communicator\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:computation_placer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/concurrency:async_value\",\n+        \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "bc9c761705fadb6346906b335b30d7fc29fe1427",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_gather_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -27,15 +27,15 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -113,7 +113,7 @@ absl::Status RunAllGather(std::vector<DeviceBufferPair>& buffers,\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm,\n                                           use_symmetric_buffer));\n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-  tsl::AsyncValueRef<Communicator::Event> event = gpu_comm->GroupExecute(\n+  Future<> future = gpu_comm->GroupExecute(\n       [&buffers, &stream](GpuCommunicator* comm) -> absl::Status {\n         for (DeviceBufferPair& buffer : buffers) {\n           TF_RETURN_IF_ERROR(comm->LaunchAllGather(\n@@ -124,12 +124,9 @@ absl::Status RunAllGather(std::vector<DeviceBufferPair>& buffers,\n         return absl::OkStatus();\n       });\n \n-  tsl::BlockUntilReady(event);\n+  TF_RETURN_IF_ERROR(future.Await());\n   VLOG(3) << \"[\" << device_ordinal\n           << \"] Done performing all-gather for ordinal: \" << device_ordinal;\n-  if (event.IsError()) {\n-    return event.GetError();\n-  }\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "48afdb40223a9e5c2fad72c4e33f61e983f3737f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/collective_ops_utils.h\"\n@@ -92,7 +93,7 @@ absl::Status RunAllReduce(ReductionKind reduction_kind,\n                                           use_symmetric_buffer));\n \n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-  tsl::AsyncValueRef<Communicator::Event> event =\n+  Future<> future =\n       gpu_comm->GroupExecute([reduction_kind, &buffers,\n                               &stream](GpuCommunicator* comm) -> absl::Status {\n         for (DeviceBufferPair& buffer : buffers) {\n@@ -103,11 +104,8 @@ absl::Status RunAllReduce(ReductionKind reduction_kind,\n         }\n         return absl::OkStatus();\n       });\n-  tsl::BlockUntilReady(event);\n+  TF_RETURN_IF_ERROR(future.Await());\n   VLOG(3) << \"[\" << device_ordinal << \"] Done performing all-reduce\";\n-  if (event.IsError()) {\n-    return event.GetError();\n-  }\n   return absl::OkStatus();\n }\n \n@@ -242,7 +240,7 @@ absl::Status RunReduceScatter(ReductionKind reduction_kind,\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm->NumRanks());\n \n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-  tsl::AsyncValueRef<Communicator::Event> event =\n+  Future<> future =\n       gpu_comm->GroupExecute([num_ranks, reduction_kind, &buffers,\n                               &stream](GpuCommunicator* comm) -> absl::Status {\n         for (DeviceBufferPair& buffer : buffers) {\n@@ -259,11 +257,8 @@ absl::Status RunReduceScatter(ReductionKind reduction_kind,\n         }\n         return absl::OkStatus();\n       });\n-  tsl::BlockUntilReady(event);\n+  TF_RETURN_IF_ERROR(future.Await());\n   VLOG(3) << \"[\" << device_ordinal << \"] Done performing reduce-scatter\";\n-  if (event.IsError()) {\n-    return event.GetError();\n-  }\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "2581b1606c56d2a4432ca57da331e0248f4f8235",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 32,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -317,53 +317,25 @@ absl::Status RunAllToAll(bool has_split_dimension,\n       }\n     }\n \n-    auto event = comm->AllToAll(\n+    auto future = comm->AllToAll(\n         std::move(send_buffers), std::move(recv_buffers), element_type,\n         chunk_element_count, GpuCollectives::On(stream));\n-\n-    tsl::BlockUntilReady(event);\n-    if (event.IsError()) {\n-      return event.GetError();\n-    }\n+    TF_RETURN_IF_ERROR(future.Await());\n   } else {\n     for (const DeviceBufferPair& buffer : buffers) {\n       send_buffers.push_back(buffer.source_buffer);\n       recv_buffers.push_back(buffer.destination_buffer);\n     }\n \n-    auto event =\n+    auto future =\n         comm->AllToAll(std::move(send_buffers), std::move(recv_buffers),\n                        element_type, element_count, GpuCollectives::On(stream));\n-\n-    tsl::BlockUntilReady(event);\n-    if (event.IsError()) {\n-      return event.GetError();\n-    }\n+    TF_RETURN_IF_ERROR(future.Await());\n   }\n \n   return absl::OkStatus();\n }\n \n-static absl::Status SendPtrToPeer(void* ptr, RankId peer, GpuCommunicator* comm,\n-                                  se::Stream& stream) {\n-  VLOG(3) << absl::StreamFormat(\n-      \"RecvPtrFromPeer on device #%d; peer=%d; comm=%p; stream=%p\",\n-      stream.parent()->device_ordinal(), peer.value(), comm, &stream);\n-\n-  return comm->LaunchSend(se::DeviceMemoryBase(ptr, sizeof(void*)), U64, 1,\n-                          peer, GpuCollectives::On(stream));\n-}\n-\n-static absl::Status RecvPtrFromPeer(void* ptr, RankId peer,\n-                                    GpuCommunicator* comm, se::Stream& stream) {\n-  VLOG(3) << absl::StreamFormat(\n-      \"RecvPtrFromPeer on device #%d; peer=%d; comm=%p; stream=%p\",\n-      stream.parent()->device_ordinal(), peer.value(), comm, &stream);\n-\n-  return comm->LaunchRecv(se::DeviceMemoryBase(ptr, sizeof(void*)), U64, 1,\n-                          peer, GpuCollectives::On(stream));\n-}\n-\n // Syncs the execution progress across all devices.\n absl::Status SyncProgress(absl::string_view name,\n                           const GpuCliqueKey& clique_key, RankId rank,"
        },
        {
            "sha": "38e122e52ce615c0b91905f3ec38967d90505216",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_broadcast_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_broadcast_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n@@ -76,7 +77,7 @@ absl::StatusOr<bool> CollectiveBroadcastStartThunk::RunCollective(\n absl::Status RunCollectiveBroadcast(std::vector<DeviceBufferPair>& buffers,\n                                     se::Stream& stream, Communicator* comm) {\n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-  tsl::AsyncValueRef<Communicator::Event> event = gpu_comm->GroupExecute(\n+  Future<> future = gpu_comm->GroupExecute(\n       [&buffers, &stream](GpuCommunicator* comm) -> absl::Status {\n         for (auto buffer : buffers) {\n           se::DeviceMemoryBase src_addr = buffer.source_buffer;\n@@ -89,11 +90,7 @@ absl::Status RunCollectiveBroadcast(std::vector<DeviceBufferPair>& buffers,\n         }\n         return absl::OkStatus();\n       });\n-  tsl::BlockUntilReady(event);\n-  if (event.IsError()) {\n-    return event.GetError();\n-  }\n-  return absl::OkStatus();\n+  return future.Await();\n }\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "14d132091ea61c5e0a967d79e76574fe751bb2bf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_group_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -30,10 +30,10 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -104,17 +104,14 @@ absl::Status CollectiveGroupThunk::ExecuteOnStream(\n \n   Communicator* comm = *communicator_set.begin();\n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-  tsl::AsyncValueRef<Communicator::Event> group_event = gpu_comm->GroupExecute(\n+  Future<> group_future = gpu_comm->GroupExecute(\n       [this, &params](GpuCommunicator* comm) -> absl::Status {\n         for (const std::unique_ptr<Thunk>& thunk : thunks_) {\n           TF_RETURN_IF_ERROR(thunk->ExecuteOnStream(params));\n         }\n         return absl::OkStatus();\n       });\n-  tsl::BlockUntilReady(group_event);\n-  if (group_event.IsError()) {\n-    return group_event.GetError();\n-  }\n+  TF_RETURN_IF_ERROR(group_future.Await());\n \n   TF_ASSIGN_OR_RETURN(se::Event * event,\n                       async_events_->GetEvent(params.stream->parent()));"
        },
        {
            "sha": "50c807c3881fad45a8fb0ecdc39d58f98d3550c5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -413,19 +413,16 @@ absl::Status RunCollectivePermute(\n         const auto src_addr = src_addrs.at(idx);\n         const auto dest_addr = dest_addrs.at(idx);\n         const auto buffer = buffers.at(idx);\n-        auto event = comm->CollectivePermute(\n+        auto future = comm->CollectivePermute(\n             src_addr, dest_addr, buffer.element_type, buffer.element_count,\n             source_rank, target_ranks, GpuCollectives::On(stream));\n-        tsl::BlockUntilReady(event);\n-        if (event.IsError()) {\n-          return event.GetError();\n-        }\n+        TF_RETURN_IF_ERROR(future.Await());\n       }\n     } else {\n       TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, comm,\n                                               use_symmetric_buffer));\n       auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-      tsl::AsyncValueRef<Communicator::Event> event = gpu_comm->GroupExecute(\n+      auto future = gpu_comm->GroupExecute(\n           [source_rank, &buffers, &src_addrs, &dest_addrs, &target_ranks,\n            &stream](GpuCommunicator* comm) -> absl::Status {\n             for (uint64_t idx = 0; idx < buffers.size(); ++idx) {\n@@ -439,10 +436,7 @@ absl::Status RunCollectivePermute(\n             }\n             return absl::OkStatus();\n           });\n-      tsl::BlockUntilReady(event);\n-      if (event.IsError()) {\n-        return event.GetError();\n-      }\n+      TF_RETURN_IF_ERROR(future.Await());\n     }\n   }\n "
        },
        {
            "sha": "4b6f285cb12f8bfe9d722db72ace67ee9f6beee5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_all_reduce_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_all_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_all_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_all_reduce_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -30,13 +30,14 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/nvshmem_collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -53,13 +54,10 @@ absl::Status RunNvshmemAllReduce(ReductionKind reduction_kind,\n   VLOG(3) << \"Performing nvshmem all-reduce from device ordinal: \"\n           << *nvshmem_comm->CurrentRank();\n   for (DeviceBufferPair& buffer : buffers) {\n-    auto event = nvshmem_comm->AllReduce(\n+    auto future = nvshmem_comm->AllReduce(\n         buffer.source_buffer, buffer.destination_buffer, buffer.element_type,\n         buffer.element_count, reduction_kind, GpuCollectives::On(stream));\n-    tsl::BlockUntilReady(event);\n-    if (event.IsError()) {\n-      return event.GetError();\n-    }\n+    TF_RETURN_IF_ERROR(future.Await());\n   }\n \n   return absl::OkStatus();"
        },
        {
            "sha": "686c7b5f832e12262b50f5ed702389c19a1334b4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_permute_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -41,16 +41,15 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/global_device_id.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -203,13 +202,10 @@ absl::Status RunCollectivePermute(P2PConfig::SourceTargetMapEntry source_target,\n       VLOG(1) << \"CollectivePermute: rank \" << device_ordinal\n               << \" sending data to target \" << *target_id;\n \n-      auto send_event = nvshmem_comm->Send(\n+      auto send_future = nvshmem_comm->Send(\n           dest_addr, src_addr, buffer.element_type, buffer.element_count,\n           RankId(*target_id), GpuCollectives::On(stream));\n-      tsl::BlockUntilReady(send_event);\n-      if (send_event.IsError()) {\n-        return send_event.GetError();\n-      }\n+      TF_RETURN_IF_ERROR(send_future.Await());\n     }\n \n     if (source_id) {"
        },
        {
            "sha": "2bfba5d86bcbf5859bc3b65a89863e60329e93a5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_recv_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_recv_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_recv_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_recv_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -157,13 +157,10 @@ absl::Status NvshmemRecvThunk::RunNvshmemCollective(const ExecuteParams& params,\n           << \" source_buffer=\" << buffer.source_buffer.opaque()\n           << \" element_count=\" << buffer.element_count\n           << \" source_id=\" << *source_id;\n-  auto recv_event = nvshmem_comm->Recv(\n+  auto recv_future = nvshmem_comm->Recv(\n       buffer.destination_buffer, buffer.source_buffer, buffer.element_type,\n       buffer.element_count, RankId(*source_id), GpuCollectives::On(stream));\n-  tsl::BlockUntilReady(recv_event);\n-  if (recv_event.IsError()) {\n-    return recv_event.GetError();\n-  }\n+  TF_RETURN_IF_ERROR(recv_future.Await());\n   TF_RETURN_IF_ERROR(nvshmem_comm->Quiet(GpuCollectives::On(stream)));\n \n   return absl::OkStatus();"
        },
        {
            "sha": "c5b0615a6255b838897156815047fedb35e3428a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_send_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_send_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_send_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_send_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -166,13 +166,10 @@ absl::Status NvshmemSendThunk::RunNvshmemCollective(const ExecuteParams& params,\n           << \" source_buffer=\" << buffer.source_buffer.opaque()\n           << \" element_count=\" << buffer.element_count\n           << \" target_id=\" << *target_id;\n-  auto send_event = nvshmem_comm->Send(\n+  auto send_future = nvshmem_comm->Send(\n       buffer.destination_buffer, buffer.source_buffer, buffer.element_type,\n       buffer.element_count, RankId(*target_id), GpuCollectives::On(stream));\n-  tsl::BlockUntilReady(send_event);\n-  if (send_event.IsError()) {\n-    return send_event.GetError();\n-  }\n+  TF_RETURN_IF_ERROR(send_future.Await());\n   TF_RETURN_IF_ERROR(nvshmem_comm->Quiet(GpuCollectives::On(stream)));\n \n   return absl::OkStatus();"
        },
        {
            "sha": "e0fb0ee13cc09cbbd21cfd961d99587890638a90",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -114,7 +115,7 @@ absl::Status RunAllToAllOnIndexBuffer(\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm->NumRanks());\n \n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-  tsl::AsyncValueRef<Communicator::Event> event = gpu_comm->GroupExecute(\n+  Future<> future = gpu_comm->GroupExecute(\n       [num_ranks, num_updates_per_replica, element_type, &source_buffer,\n        &destination_buffer, &stream](GpuCommunicator* comm) -> absl::Status {\n         for (int peer = 0; peer < num_ranks; ++peer) {\n@@ -136,10 +137,7 @@ absl::Status RunAllToAllOnIndexBuffer(\n         }\n         return absl::OkStatus();\n       });\n-  tsl::BlockUntilReady(event);\n-  if (event.IsError()) {\n-    return event.GetError();\n-  }\n+  TF_RETURN_IF_ERROR(future.Await());\n   return stream.BlockHostUntilDone();\n }\n \n@@ -180,7 +178,7 @@ absl::Status RunRaggedAllToAll(\n   const int64_t* recv_sizes = ragged_metadata_allocs[3];\n \n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(comm);\n-  tsl::AsyncValueRef<Communicator::Event> event = gpu_comm->GroupExecute(\n+  Future<> future = gpu_comm->GroupExecute(\n       [num_updates_per_replica, num_ranks, input_offsets, send_sizes,\n        output_offsets, recv_sizes, ragged_row_element_size, &buffers,\n        &stream](GpuCommunicator* comm) -> absl::Status {\n@@ -216,11 +214,7 @@ absl::Status RunRaggedAllToAll(\n \n         return absl::OkStatus();\n       });\n-  tsl::BlockUntilReady(event);\n-  if (event.IsError()) {\n-    return event.GetError();\n-  }\n-  return absl::OkStatus();\n+  return future.Await();\n }\n \n // Contains the values that are passed between host threads with rendezvous."
        },
        {
            "sha": "d032306060dc4841bff446a469d2b6012f5b6df6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/recv_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Frecv_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -30,13 +30,11 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/global_device_id.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -131,14 +129,10 @@ absl::StatusOr<bool> RecvThunk::RunCollective(const ExecuteParams& params,\n     if (should_run) {\n       TF_RETURN_IF_ERROR(\n           MaybeRegisterBuffers(stream.parent(), {buffer}, comm_handle.comm));\n-      auto event = comm_handle.comm->Recv(\n+      auto future = comm_handle.comm->Recv(\n           dest_addr, buffer.element_type, buffer.element_count,\n           RankId(*source_id), GpuCollectives::On(stream));\n-\n-      tsl::BlockUntilReady(event);\n-      if (event.IsError()) {\n-        return event.GetError();\n-      }\n+      TF_RETURN_IF_ERROR(future.Await());\n     } else {\n       VLOG(3) << \"[\" << device_ordinal << \"] Skipping Recv\";\n     }"
        },
        {
            "sha": "d4199c21aed48ab7219cf4e004950d36bbd438d7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/send_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsend_thunk.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -132,14 +132,10 @@ absl::StatusOr<bool> SendThunk::RunCollective(const ExecuteParams& params,\n     if (should_run) {\n       TF_RETURN_IF_ERROR(\n           MaybeRegisterBuffers(stream.parent(), {buffer}, comm_handle.comm));\n-      auto event = comm_handle.comm->Send(\n+      auto future = comm_handle.comm->Send(\n           src_addr, buffer.element_type, buffer.element_count,\n           RankId(*target_id), GpuCollectives::On(stream));\n-\n-      tsl::BlockUntilReady(event);\n-      if (event.IsError()) {\n-        return event.GetError();\n-      }\n+      TF_RETURN_IF_ERROR(future.Await());\n     } else {\n       VLOG(3) << \"[\" << device_ordinal << \"] Skipping Send\";\n     }"
        },
        {
            "sha": "250dd93c050c3f8a05f7759695c1127bd1882476",
            "filename": "third_party/xla/xla/core/collectives/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -69,6 +69,7 @@ cc_library(\n     hdrs = [\"communicator.h\"],\n     deps = [\n         \":rank_id\",\n+        \"//xla:future\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:collective_ops_utils\","
        },
        {
            "sha": "6b6cd091f19f139cfd76a2bd483479373680f101",
            "filename": "third_party/xla/xla/core/collectives/communicator.h",
            "status": "modified",
            "additions": 38,
            "deletions": 52,
            "changes": 90,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #define XLA_CORE_COLLECTIVES_COMMUNICATOR_H_\n \n #include <cstddef>\n-#include <memory>\n #include <optional>\n #include <ostream>\n #include <string>\n@@ -27,11 +26,9 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/future.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/platform.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/concurrency/chain.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -47,8 +44,6 @@ namespace xla {\n // completed.\n class Communicator {\n  public:\n-  using Event = tsl::Chain;\n-\n   virtual ~Communicator() = default;\n \n   // An executor is an abstraction for the underlying resource where collective\n@@ -96,76 +91,73 @@ class Communicator {\n \n   // Reduce buffers of length `count` in `send_buff` using `reduction_kind`\n   // reduction and leaves identical copies of the result on each `recv_buff`.\n-  virtual tsl::AsyncValueRef<Event> AllReduce(\n-      stream_executor::DeviceMemoryBase send_buffer,\n-      stream_executor::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n-      size_t count, ReductionKind reduction_kind, const Executor& executor) = 0;\n+  virtual Future<> AllReduce(stream_executor::DeviceMemoryBase send_buffer,\n+                             stream_executor::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count,\n+                             ReductionKind reduction_kind,\n+                             const Executor& executor) = 0;\n \n   // Copy data in `send_buff` from the root device to the `recv_buff` on\n   // all other devices.\n-  virtual tsl::AsyncValueRef<Event> Broadcast(se::DeviceMemoryBase send_buffer,\n-                                              se::DeviceMemoryBase recv_buffer,\n-                                              PrimitiveType dtype, size_t count,\n-                                              RankId root,\n-                                              const Executor& executor) = 0;\n+  virtual Future<> Broadcast(se::DeviceMemoryBase send_buffer,\n+                             se::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count, RankId root,\n+                             const Executor& executor) = 0;\n \n   // Reduce data in `send_buff` from all devices using the `reduction_kind`\n   // operation and leave the reduced result scattered over the devices so that\n   // the `recv_buff` on rank `i` will contain the i-th block of the result.\n-  virtual tsl::AsyncValueRef<Event> ReduceScatter(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-      PrimitiveType dtype, size_t count, ReductionKind reduction_kind,\n-      const Executor& executor) = 0;\n+  virtual Future<> ReduceScatter(se::DeviceMemoryBase send_buffer,\n+                                 se::DeviceMemoryBase recv_buffer,\n+                                 PrimitiveType dtype, size_t count,\n+                                 ReductionKind reduction_kind,\n+                                 const Executor& executor) = 0;\n \n   // Gather `count` values from all devices into `recv_buffer`, receiving data\n   // from rank `i` at offset `i * sendcount`.\n-  virtual tsl::AsyncValueRef<Event> AllGather(se::DeviceMemoryBase send_buffer,\n-                                              se::DeviceMemoryBase recv_buffer,\n-                                              PrimitiveType dtype, size_t count,\n-                                              const Executor& executor) = 0;\n+  virtual Future<> AllGather(se::DeviceMemoryBase send_buffer,\n+                             se::DeviceMemoryBase recv_buffer,\n+                             PrimitiveType dtype, size_t count,\n+                             const Executor& executor) = 0;\n \n   // Sends data from `send_buffer` to `target_ranks` and receives data from\n   // `source_rank` into `recv_buffer`. If `source_rank` is not specified, the\n   // output is filled with zeros.\n-  virtual tsl::AsyncValueRef<Event> CollectivePermute(\n-      se::DeviceMemoryBase send_buffer, se::DeviceMemoryBase recv_buffer,\n-      PrimitiveType dtype, size_t count, std::optional<RankId> source_rank,\n-      absl::Span<const RankId> target_ranks, const Executor& executor) = 0;\n+  virtual Future<> CollectivePermute(se::DeviceMemoryBase send_buffer,\n+                                     se::DeviceMemoryBase recv_buffer,\n+                                     PrimitiveType dtype, size_t count,\n+                                     std::optional<RankId> source_rank,\n+                                     absl::Span<const RankId> target_ranks,\n+                                     const Executor& executor) = 0;\n \n   // Sends `count` values from `send_buffers` to other ranks and receives data\n   // from other ranks into `recv_buffers`.\n-  virtual tsl::AsyncValueRef<Event> AllToAll(\n+  virtual Future<> AllToAll(\n       absl::InlinedVector<se::DeviceMemoryBase, 4> send_buffers,\n       absl::InlinedVector<se::DeviceMemoryBase, 4> recv_buffers,\n       PrimitiveType dtype, size_t count, const Executor& executor) = 0;\n \n   // Send data from `send_buff` to rank `peer`.\n-  virtual tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase send_buffer,\n-                                         PrimitiveType dtype, size_t count,\n-                                         RankId peer,\n-                                         const Executor& executor) = 0;\n+  virtual Future<> Send(se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+                        size_t count, RankId peer,\n+                        const Executor& executor) = 0;\n \n   // Receive data from rank `peer` into `recv_buff`.\n-  virtual tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase recv_buffer,\n-                                         PrimitiveType dtype, size_t count,\n-                                         RankId peer,\n-                                         const Executor& executor) = 0;\n+  virtual Future<> Recv(se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,\n+                        size_t count, RankId peer,\n+                        const Executor& executor) = 0;\n \n   // Send data from `send_buff` to rank `recv_buff` (one-way send).\n-  virtual tsl::AsyncValueRef<Event> Send(se::DeviceMemoryBase recv_buffer,\n-                                         se::DeviceMemoryBase send_buffer,\n-                                         PrimitiveType dtype, size_t count,\n-                                         RankId peer,\n-                                         const Executor& executor) {\n+  virtual Future<> Send(se::DeviceMemoryBase recv_buffer,\n+                        se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+                        size_t count, RankId peer, const Executor& executor) {\n     return Unimplemented(\"One-way send is not implemented\");\n   }\n \n   // Receive data from rank `peer` into `recv_buff` (one-way recv).\n-  virtual tsl::AsyncValueRef<Event> Recv(se::DeviceMemoryBase recv_buffer,\n-                                         se::DeviceMemoryBase send_buffer,\n-                                         PrimitiveType dtype, size_t count,\n-                                         RankId peer,\n-                                         const Executor& executor) {\n+  virtual Future<> Recv(se::DeviceMemoryBase recv_buffer,\n+                        se::DeviceMemoryBase send_buffer, PrimitiveType dtype,\n+                        size_t count, RankId peer, const Executor& executor) {\n     return Unimplemented(\"One-way recv is not implemented\");\n   }\n \n@@ -191,12 +183,6 @@ class Communicator {\n   virtual absl::Status Fence() {\n     return Unimplemented(\"Fence is not implemented\");\n   }\n-\n- protected:\n-  // Returns an `Event` that is always available.\n-  static tsl::AsyncValueRef<Event> OkEvent() {\n-    return tsl::MakeAvailableAsyncValueRef<Event>();\n-  }\n };\n \n inline std::ostream& operator<<(std::ostream& os, const Communicator& comm) {"
        },
        {
            "sha": "0d34ac7485c233599c5aab0e172870522bd9114d",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -954,15 +954,10 @@ void StreamExecutorGpuClient::CopyToRemoteDevice(\n       std::unique_ptr<Communicator> communicator = std::move(communicators[0]);\n \n       // Send data to the receiver.\n-      tsl::AsyncValueRef<Communicator::Event> send_event = communicator->Send(\n+      Future<> send_future = communicator->Send(\n           mem->mem(), shape.element_type(), ShapeUtil::ElementsIn(shape),\n           RankId(0), gpu::GpuCollectives::On(*stream));\n-\n-      // Wait for the send to finish.\n-      tsl::BlockUntilReady(send_event);\n-      if (send_event.IsError()) {\n-        return send_event.GetError();\n-      }\n+      TF_RETURN_IF_ERROR(send_future.Await());\n \n       // Keep mem alive until the Send has finished executing. Note that\n       // send_event is fulfilled when the send is enqueued, but not necessarily\n@@ -1067,15 +1062,10 @@ StreamExecutorGpuClient::MakeCrossHostReceiveBuffers(\n       std::unique_ptr<Communicator> communicator = std::move(communicators[0]);\n \n       // Receive data from the sender.\n-      tsl::AsyncValueRef<Communicator::Event> recv_event = communicator->Recv(\n+      Future<> recv_future = communicator->Recv(\n           mem->mem(), shape.element_type(), ShapeUtil::ElementsIn(shape),\n           RankId(1), gpu::GpuCollectives::On(*stream));\n-\n-      // Wait for the receive to finish.\n-      tsl::BlockUntilReady(recv_event);\n-      if (recv_event.IsError()) {\n-        return recv_event.GetError();\n-      }\n+      TF_RETURN_IF_ERROR(recv_future.Await());\n \n       // Keep mem alive until the Recv has finished executing. Note that\n       // recv_event is fulfilled when the receive is enqueued, but not"
        },
        {
            "sha": "a74d79e86a2a1469d73d8d4c9fca4a91687dca04",
            "filename": "third_party/xla/xla/tsl/concurrency/future.h",
            "status": "modified",
            "additions": 17,
            "deletions": 5,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Ftsl%2Fconcurrency%2Ffuture.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cb5fcbed6c7729998217225d248e6c0bb01e8897/third_party%2Fxla%2Fxla%2Ftsl%2Fconcurrency%2Ffuture.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fconcurrency%2Ffuture.h?ref=cb5fcbed6c7729998217225d248e6c0bb01e8897",
            "patch": "@@ -496,8 +496,10 @@ class Future : public internal::FutureBase<absl::StatusOr<T>> {\n                 nullptr>\n   static Future<T> MakeOn(Executor& executor, F&& f) {\n     auto [promise, future] = MakePromise();\n-    executor.Execute([promise = std::move(promise),\n-                      f = std::forward<F>(f)]() mutable { promise.Set(f()); });\n+    executor.Execute(\n+        [promise = std::move(promise), f = std::forward<F>(f)]() mutable {\n+          promise.Set(std::move(f)());\n+        });\n     return std::move(future);\n   }\n \n@@ -696,13 +698,21 @@ class Future<void> : public internal::FutureBase<absl::Status> {\n   // For futures that are immediately ready with OK status, we use a global non\n   // reference-counted async value that avoids heap allocation and reference\n   // counting operations on a hot path.\n-  explicit Future(absl::Status status)\n+  Future(absl::Status status)  // NOLINT\n       : Base(ABSL_PREDICT_TRUE(status.ok())\n                  ? ready_promise_->AsRef()\n                  : tsl::MakeAvailableAsyncValueRef<absl::Status>(\n                        std::move(status)),\n              /*on_block_start=*/nullptr, /*on_block_end=*/nullptr) {}\n \n+  // Support implicit construction from immediate `Status` convertible to\n+  // `absl::Status`.\n+  template <\n+      typename Status,\n+      std::enable_if_t<std::is_convertible_v<Status, absl::Status>>* = nullptr>\n+  Future(Status&& status)  // NOLINT\n+      : Future(absl::Status(std::forward<Status>(status))) {}\n+\n   class Promise : public Base::Promise {\n    public:\n     using Base::Promise::Promise;\n@@ -750,8 +760,10 @@ class Future<void> : public internal::FutureBase<absl::Status> {\n             std::enable_if_t<internal::is_status_v<R>>* = nullptr>\n   static Future<> MakeOn(Executor& executor, F&& f) {\n     auto [promise, future] = MakePromise();\n-    executor.Execute([promise = std::move(promise),\n-                      f = std::forward<F>(f)]() mutable { promise.Set(f()); });\n+    executor.Execute(\n+        [promise = std::move(promise), f = std::forward<F>(f)]() mutable {\n+          promise.Set(std::move(f)());\n+        });\n     return std::move(future);\n   }\n "
        }
    ],
    "stats": {
        "total": 1235,
        "additions": 544,
        "deletions": 691
    }
}