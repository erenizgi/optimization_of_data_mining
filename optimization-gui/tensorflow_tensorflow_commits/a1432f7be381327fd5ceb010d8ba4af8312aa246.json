{
    "author": "majiddadashi",
    "message": "Generalize `UnpackDenseInt4IntoInt8` to `UnpackPackedIntToInt8`.\n\nThis change generalizes the function to support unpacking packed integers of different bit widths (currently 4-bit and 2-bit) into 8-bit integers. All call sites have been updated to pass `/*bit_width=*/4`.\n\nPiperOrigin-RevId: 817397833",
    "sha": "a1432f7be381327fd5ceb010d8ba4af8312aa246",
    "files": [
        {
            "sha": "7c8108b2ff53736536898f50aeaeec70c997e91f",
            "filename": "tensorflow/lite/kernels/conv.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fconv.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fconv.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fconv.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -714,9 +714,9 @@ void EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n   if (filter->type == kTfLiteInt4) {\n     const size_t bytes_unpacked = filter->bytes * 2;\n     unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),\n-        unpacked_filter_data.get());\n+        /*bit_width=*/4, unpacked_filter_data.get());\n     filter_data = reinterpret_cast<const uint8_t*>(unpacked_filter_data.get());\n   } else {\n     filter_data = GetTensorData<uint8_t>(filter);\n@@ -801,9 +801,9 @@ void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,\n   if (filter->type == kTfLiteInt4) {\n     const size_t bytes_unpacked = filter->bytes * 2;\n     unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),\n-        unpacked_filter_data.get());\n+        /*bit_width=*/4, unpacked_filter_data.get());\n     filter_data = unpacked_filter_data.get();\n   } else {\n     filter_data = GetTensorData<int8_t>(filter);\n@@ -901,9 +901,9 @@ void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,\n   if (filter->type == kTfLiteInt4) {\n     const size_t bytes_unpacked = filter->bytes * 2;\n     unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),\n-        unpacked_filter_data.get());\n+        /*bit_width=*/4, unpacked_filter_data.get());\n     filter_data = unpacked_filter_data.get();\n   } else {\n     filter_data = GetTensorData<int8_t>(filter);\n@@ -1101,9 +1101,9 @@ TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n   if (filter->type == kTfLiteInt4) {\n     const size_t bytes_unpacked = filter->bytes * 2;\n     unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),\n-        unpacked_filter_data.get());\n+        /*bit_width=*/4, unpacked_filter_data.get());\n     filter_data = unpacked_filter_data.get();\n   } else {\n     filter_data = GetTensorData<int8_t>(filter);\n@@ -1212,9 +1212,9 @@ TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,\n   if (filter->type == kTfLiteInt4) {\n     const size_t bytes_unpacked = filter->bytes * 2;\n     unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),\n-        unpacked_filter_data.get());\n+        /*bit_width=*/4, unpacked_filter_data.get());\n     filter_data = unpacked_filter_data.get();\n   } else {\n     filter_data = GetTensorData<int8_t>(filter);"
        },
        {
            "sha": "e0fa2ea0ae3f45ca1ff7921fabfddb76b0921b95",
            "filename": "tensorflow/lite/kernels/depthwise_conv.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fdepthwise_conv.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fdepthwise_conv.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fdepthwise_conv.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h\"\n #include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h\"\n #include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n #include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h\"\n #include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h\"\n@@ -416,9 +417,9 @@ TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,\n   auto unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n \n   if (filter->type == kTfLiteInt4) {\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),\n-        unpacked_filter_data.get());\n+        /*bit_width=*/4, unpacked_filter_data.get());\n     filter_data = unpacked_filter_data.get();\n   } else {\n     filter_data = GetTensorData<int8>(filter);"
        },
        {
            "sha": "77668d4770498261a2dab38dbd026e601ca08c2b",
            "filename": "tensorflow/lite/kernels/dequantize.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fdequantize.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fdequantize.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fdequantize.h?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -76,9 +76,9 @@ inline TfLiteStatus PerChannelDequantizeImpl(TfLiteContext* context,\n   auto unpacked_input_data = std::make_unique<int8_t[]>(bytes_unpacked);\n \n   if (input->type == kTfLiteInt4) {\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(input), GetTensorShape(input).FlatSize(),\n-        unpacked_input_data.get());\n+        /*bit_width=*/4, unpacked_input_data.get());\n     input_data = unpacked_input_data.get();\n   } else {\n     input_data = GetTensorData<int8_t>(input);\n@@ -120,9 +120,9 @@ TfLiteStatus DequantizeImpl(TfLiteContext* context, TfLiteNode* node,\n \n   if (input->type == kTfLiteInt4) {\n     // Use GetTensorShape(input).FlatSize() for num_elements.\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(input), GetTensorShape(input).FlatSize(),\n-        unpacked_input_data.get());\n+        /*bit_width=*/4, unpacked_input_data.get());\n     input_data = unpacked_input_data.get();\n   } else {\n     input_data = GetTensorData<int8_t>(input);"
        },
        {
            "sha": "da06ee3c3e892e19ccfd583bf6700ce9c3aef681",
            "filename": "tensorflow/lite/kernels/fully_connected.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"tensorflow/lite/kernels/internal/optimized/fully_connected_4bit.h\"\n #include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n #include \"tensorflow/lite/kernels/internal/optimized/sparse_ops/fully_connected.h\"\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n #include \"tensorflow/lite/kernels/internal/reference/fully_connected.h\"\n #include \"tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h\"\n@@ -754,9 +755,9 @@ TfLiteStatus EvalHybridDense(\n   if (filter->type == kTfLiteInt4) {\n     const size_t bytes_unpacked = filter->bytes * 2;\n     unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-    tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(filter), GetTensorShape(filter).FlatSize(),\n-        unpacked_filter_data.get());\n+        /*bit_width=*/4, unpacked_filter_data.get());\n     filter_data = unpacked_filter_data.get();\n   } else {\n     filter_data = GetTensorData<int8_t>(filter);\n@@ -1479,9 +1480,10 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n           if (filter->type == kTfLiteInt4) {\n             const size_t bytes_unpacked = filter->bytes * 2;\n             unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-            tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+            tflite::tensor_utils::UnpackPackedIntToInt8(\n                 GetTensorData<int8_t>(filter),\n-                GetTensorShape(filter).FlatSize(), unpacked_filter_data.get());\n+                GetTensorShape(filter).FlatSize(), /*bit_width=*/4,\n+                unpacked_filter_data.get());\n             filter_data = unpacked_filter_data.get();\n           } else {\n             filter_data = GetTensorData<int8_t>(filter);\n@@ -1507,9 +1509,10 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n           if (filter->type == kTfLiteInt4) {\n             const size_t bytes_unpacked = filter->bytes * 2;\n             unpacked_filter_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-            tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+            tflite::tensor_utils::UnpackPackedIntToInt8(\n                 GetTensorData<int8_t>(filter),\n-                GetTensorShape(filter).FlatSize(), unpacked_filter_data.get());\n+                GetTensorShape(filter).FlatSize(), /*bit_width=*/4,\n+                unpacked_filter_data.get());\n             filter_data = unpacked_filter_data.get();\n           } else {\n             filter_data = GetTensorData<int8_t>(filter);"
        },
        {
            "sha": "23e30eb78677743b1335db3ad0ea02702beb0825",
            "filename": "tensorflow/lite/kernels/gather_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fgather_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fgather_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fgather_test.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -105,8 +105,9 @@ class GatherOpModel : public SingleOpModel {\n       num_elements *= shape[i];\n     }\n     std::vector<int8_t> inflated_output(num_elements);\n-    tensor_utils::UnpackDenseInt4IntoInt8(data_int8.data(), num_elements,\n-                                          inflated_output.data());\n+    tensor_utils::UnpackPackedIntToInt8(data_int8.data(), num_elements,\n+                                        /*bit_width=*/4,\n+                                        inflated_output.data());\n     return inflated_output;\n   }\n "
        },
        {
            "sha": "e902d885c192bc5c080b4cc9a86445bbe802212e",
            "filename": "tensorflow/lite/kernels/internal/per_channel_dequantize_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Fper_channel_dequantize_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Fper_channel_dequantize_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Finternal%2Fper_channel_dequantize_test.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -132,8 +132,9 @@ TEST(PerChannelDequantize, TestInt4ToFloat_2D) {\n   std::vector<float> output(8, -1);\n   const size_t bytes_unpacked = packed_int4_input.size() * 2;\n   auto unpacked_input_data = std::make_unique<int8_t[]>(bytes_unpacked);\n-  tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n-      packed_int4_input.data(), bytes_unpacked, unpacked_input_data.get());\n+  tflite::tensor_utils::UnpackPackedIntToInt8(packed_int4_input.data(),\n+                                              bytes_unpacked, /*bit_width=*/4,\n+                                              unpacked_input_data.get());\n   EXPECT_THAT(std::vector<int8_t>(unpacked_input_data.get(),\n                                   unpacked_input_data.get() + bytes_unpacked),\n               ElementsAreArray(ArrayFloatNear({-1, -1, 0, 0, 1, 4, 1, -8})));"
        },
        {
            "sha": "fe419074a24d05c3dbbdab36cdc3c57b03ea4cb9",
            "filename": "tensorflow/lite/kernels/internal/portable_tensor_utils.cc",
            "status": "modified",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n \n #include <algorithm>\n+#include <cassert>\n #include <cmath>\n #include <cstdint>\n \n@@ -92,6 +93,56 @@ void UnpackDenseInt4IntoInt8(const int8_t* src_buffer, int num_elements,\n   }\n }\n \n+void UnpackPackedIntToInt8(const int8_t* src_buffer, int num_elements,\n+                           int bit_width, int8_t* dst_buffer) {\n+  assert(bit_width == 2 || bit_width == 4);\n+  if (bit_width == 4) {\n+    // num_elements means the number of elements regardless of packed or\n+    // unpacked. For example, 3 elements means both\n+    //   1) Packed: 3 int4's = 12 bit -> 16 bits (padded) = 2 bytes.\n+    //      stored in src_buffer[0] and src_buffer[1] (i = 0..1)\n+    //   2) Unpacked: 3 int8's = 3 bytes.\n+    //.     stored in dst_buffer[0], dst_buffer[1] and dst_buffer[2] (j = 0..2)\n+    for (int i = 0; i < num_elements / 2; i++) {\n+      int8_t byte = src_buffer[i];\n+      // Shift left first so that sign is properly extended when shifted right\n+      int8_t lower = static_cast<int8_t>(byte << 4) >> 4;\n+      int8_t higher = byte >> 4;\n+      dst_buffer[2 * i] = lower;\n+      dst_buffer[2 * i + 1] = higher;\n+    }\n+\n+    // If the buffer size is odd, extract the final lower nibble.\n+    if (num_elements % 2 != 0) {\n+      dst_buffer[num_elements - 1] =\n+          static_cast<int8_t>(src_buffer[num_elements / 2] << 4) >> 4;\n+    }\n+  } else if (bit_width == 2) {\n+    for (int i = 0; i < num_elements / 4; i++) {\n+      int8_t byte = src_buffer[i];\n+      // Shift left first so that sign is properly extended when shifted right\n+      int8_t val1 = static_cast<int8_t>(byte << 6) >> 6;\n+      int8_t val2 = static_cast<int8_t>((byte << 4) & 0xFF) >> 6;\n+      int8_t val3 = static_cast<int8_t>((byte << 2) & 0xFF) >> 6;\n+      int8_t val4 = byte >> 6;\n+      dst_buffer[4 * i] = val1;\n+      dst_buffer[4 * i + 1] = val2;\n+      dst_buffer[4 * i + 2] = val3;\n+      dst_buffer[4 * i + 3] = val4;\n+    }\n+\n+    // Handle the remaining elements.\n+    int remaining_elements = num_elements % 4;\n+    if (remaining_elements > 0) {\n+      int8_t byte = src_buffer[num_elements / 4];\n+      for (int i = 0; i < remaining_elements; i++) {\n+        dst_buffer[num_elements - remaining_elements + i] =\n+            static_cast<int8_t>((byte << (6 - 2 * i)) & 0xFF) >> 6;\n+      }\n+    }\n+  }\n+}\n+\n void PackInt8IntoDenseInt4(const int8_t* src_buffer, int num_elements,\n                            int8_t* dst_buffer) {\n   // num_elements means the number of elements regardless of packed or unpacked."
        },
        {
            "sha": "ee5063b0203997ea148dbb161a660f4cf549f9ac",
            "filename": "tensorflow/lite/kernels/internal/portable_tensor_utils.h",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.h?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -618,6 +618,23 @@ void ApplySignbitToVector(const float* __restrict__ vector, int v_size,\n void UnpackDenseInt4IntoInt8(const int8_t* src_buffer, int num_elements,\n                              int8_t* dst_buffer);\n \n+// Unpack or inflate `src_buffer` by taking each byte and splitting it into\n+// multiple elements into `dst_buffer`. Supports 2-bit and 4-bit packed integers\n+// Parameters:\n+//   src_buffer   : Densely packed buffer containing int2 or int4 values.\n+//   num_elements : Number of unpacked elements to be read from the buffer.\n+//                  This should be equal to the size of `dst_buffer`.\n+//   bit_width    : The bit width of the packed elements (either 2 or 4).\n+//   dst_buffer   : Buffer to unpack into. Should be allocated by the caller.\n+//                  Size should be at least `num_elements`.\n+// Notes:\n+//   For 4-bit unpacking: e.g., `src_buffer = {0x12, 0x34};` (num_elements = 4)\n+//   will return `dst_buffer = {0x02, 0x01, 0x04, 0x03}`.\n+//   For 2-bit unpacking: e.g., `src_buffer = {0x12};` (num_elements = 4)\n+//   will return `dst_buffer = {0x02, 0x00, 0x01, 0x00}` (sign extended).\n+void UnpackPackedIntToInt8(const int8_t* src_buffer, int num_elements,\n+                           int bit_width, int8_t* dst_buffer);\n+\n // Pack `src_buffer` into a densely packed buffer of int4 values.\n // Parameters:\n //   src_buffer   : Buffer containing int4 values stored in int8 memory."
        },
        {
            "sha": "f3634223533b8d77aadb14271575a0d6d6cef897",
            "filename": "tensorflow/lite/kernels/internal/tensor_utils_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Ftensor_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Finternal%2Ftensor_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Finternal%2Ftensor_utils_test.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"tensorflow/lite/core/c/builtin_op_data.h\"\n #include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n #include \"tensorflow/lite/kernels/internal/common.h\"\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n #include \"tensorflow/lite/kernels/test_util.h\"\n \n@@ -2112,7 +2113,7 @@ TEST(uKernels, UnpackInt4Basic) {\n   const int8_t input[2] = {0x38, static_cast<int8_t>(0xBE)};\n   const int8_t expected_output[4] = {-8, 3, -2, -5};\n   int8_t actual_output[4];\n-  UnpackDenseInt4IntoInt8(input, 4, actual_output);\n+  UnpackPackedIntToInt8(input, 4, 4, actual_output);\n   EXPECT_THAT(actual_output,\n               testing::Pointwise(testing::Eq(), expected_output));\n }\n@@ -2122,7 +2123,28 @@ TEST(uKernels, UnpackInt4OddLength) {\n   const int8_t input[2] = {0x21, 0x43};\n   const int8_t expected_output[3] = {1, 2, 3};\n   int8_t actual_output[3];\n-  UnpackDenseInt4IntoInt8(input, 3, actual_output);\n+  UnpackPackedIntToInt8(input, 3, 4, actual_output);\n+  EXPECT_THAT(actual_output,\n+              testing::Pointwise(testing::Eq(), expected_output));\n+}\n+\n+TEST(uKernels, UnpackInt2Basic) {\n+  // INT2 ranges from [-2,1], so 0x2 or b'10' is mapped to -2 in two's\n+  // complement. 0x3 or b'11' is mapped to -1.\n+  const int8_t input[1] = {0x6C};\n+  const int8_t expected_output[4] = {0, -1, -2, 1};\n+  int8_t actual_output[4];\n+  UnpackPackedIntToInt8(input, 4, 2, actual_output);\n+  EXPECT_THAT(actual_output,\n+              testing::Pointwise(testing::Eq(), expected_output));\n+}\n+\n+TEST(uKernels, UnpackInt2OddLength) {\n+  // `num_elements` is odd, so the last element 0x3 should be ignored\n+  const int8_t input[1] = {static_cast<int8_t>(0xD8)};\n+  const int8_t expected_output[3] = {0, -2, 1};\n+  int8_t actual_output[3];\n+  UnpackPackedIntToInt8(input, 3, 2, actual_output);\n   EXPECT_THAT(actual_output,\n               testing::Pointwise(testing::Eq(), expected_output));\n }"
        },
        {
            "sha": "80aac806761a2efda690048e84a14aa7e7f17487",
            "filename": "tensorflow/lite/kernels/quantize_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fquantize_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Fquantize_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fquantize_test.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n \n #include <gtest/gtest.h>\n #include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/types.h\"\n #include \"tensorflow/lite/kernels/kernel_util.h\"\n@@ -61,8 +62,9 @@ class QuantizeOpModel : public SingleOpModel {\n     TfLiteTensor* t = interpreter_->tensor(output_);\n     int num_elements = NumElements(t);\n     std::vector<int8_t> unpacked_output(num_elements);\n-    tensor_utils::UnpackDenseInt4IntoInt8(t->data.int8, num_elements,\n-                                          unpacked_output.data());\n+    tensor_utils::UnpackPackedIntToInt8(t->data.int8, num_elements,\n+                                        /*bit_width=*/4,\n+                                        unpacked_output.data());\n     return unpacked_output;\n   }\n "
        },
        {
            "sha": "5f08f8083fa1b78e2080e39eb5b040ea7ec087e5",
            "filename": "tensorflow/lite/kernels/transpose.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ftranspose.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -134,10 +134,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n       auto unpacked_input_data = std::make_unique<int8_t[]>(bytes_unpacked);\n       auto unpacked_output_data = std::make_unique<int8_t[]>(bytes_unpacked);\n \n-      tflite::tensor_utils::UnpackDenseInt4IntoInt8(\n+      tflite::tensor_utils::UnpackPackedIntToInt8(\n           GetTensorData<int8_t>(op_context.input),\n           GetTensorShape(op_context.input).FlatSize(),\n-          unpacked_input_data.get());\n+          /*bit_width=*/4, unpacked_input_data.get());\n       reference_ops::Transpose(\n           params, GetTensorShape(op_context.input), unpacked_input_data.get(),\n           GetTensorShape(op_context.output), unpacked_output_data.get());"
        },
        {
            "sha": "96813b10aeef7c055e1a7d66c6e427ed613c210c",
            "filename": "tensorflow/lite/kernels/transpose_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Ftranspose_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1432f7be381327fd5ceb010d8ba4af8312aa246/tensorflow%2Flite%2Fkernels%2Ftranspose_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ftranspose_test.cc?ref=a1432f7be381327fd5ceb010d8ba4af8312aa246",
            "patch": "@@ -72,8 +72,9 @@ class TransposeOpInt4Model : public SingleOpModel {\n       num_elements *= shape[i];\n     }\n     std::vector<int8_t> inflated_output(num_elements);\n-    tensor_utils::UnpackDenseInt4IntoInt8(data_int8.data(), num_elements,\n-                                          inflated_output.data());\n+    tensor_utils::UnpackPackedIntToInt8(data_int8.data(), num_elements,\n+                                        /*bit_width=*/4,\n+                                        inflated_output.data());\n     return inflated_output;\n   }\n "
        }
    ],
    "stats": {
        "total": 167,
        "additions": 133,
        "deletions": 34
    }
}