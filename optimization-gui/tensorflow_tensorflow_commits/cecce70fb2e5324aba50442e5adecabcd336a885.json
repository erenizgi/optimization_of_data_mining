{
    "author": "felixwqp",
    "message": "Rename `rail-aligned` into `world-level` in collective_ops_utils.h\n\nNetwork rail usually refers to a set of  NICs connected by the same fabric/switch, e.g. [Rail-optimized topology](https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/).\n\nPiperOrigin-RevId: 825696577",
    "sha": "cecce70fb2e5324aba50442e5adecabcd336a885",
    "files": [
        {
            "sha": "8a4348ecf316cac82269f38939fb2b62ca6c48ea",
            "filename": "third_party/xla/xla/service/gpu/model/collective_interpolator_test.cc",
            "status": "modified",
            "additions": 60,
            "deletions": 60,
            "changes": 120,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc?ref=cecce70fb2e5324aba50442e5adecabcd336a885",
            "patch": "@@ -151,10 +151,10 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n       case GPUCommunicationType::SINGLE_HOST:\n         iota = IotaReplicaGroupList(num_hosts, kNumGpusPerHost);\n         break;\n-      case GPUCommunicationType::RAIL_ALIGNED:\n+      case GPUCommunicationType::MULTI_HOST_WORLD_LEVEL:\n         iota = IotaReplicaGroupList(1, num_hosts * kNumGpusPerHost);\n         break;\n-      case GPUCommunicationType::NON_RAIL_ALIGNED:\n+      case GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL:\n         iota = IotaReplicaGroupList(kNumGpusPerHost, num_hosts,\n                                     {num_hosts, kNumGpusPerHost}, {1, 0});\n         break;\n@@ -169,56 +169,56 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n   std::vector<SpaceSpec> test_space_ = {\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/4 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/5 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/512,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 512,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/4 * 512,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/5 * 512,\n@@ -253,56 +253,56 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/4 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/5 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/512,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 512,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/4 * 512,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/5 * 512,\n@@ -337,56 +337,56 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/4 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/5 * 1024,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/512,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 512,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/4 * 512,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/5 * 512,\n@@ -428,14 +428,14 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n       },\n       {\n           /*opcode=*/HloOpcode::kAllToAll,\n-          /*comm=*/GPUCommunicationType::RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllToAll,\n-          /*comm=*/GPUCommunicationType::NON_RAIL_ALIGNED,\n+          /*comm=*/GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/4096,\n@@ -459,7 +459,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },\n@@ -471,7 +471,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n@@ -483,7 +483,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n@@ -495,7 +495,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/3,\n             },\n@@ -507,7 +507,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -519,7 +519,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },\n@@ -531,7 +531,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n@@ -543,7 +543,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n@@ -555,7 +555,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/3,\n             },\n@@ -567,7 +567,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -639,7 +639,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },\n@@ -651,7 +651,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n@@ -663,7 +663,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n@@ -675,7 +675,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/3,\n             },\n@@ -687,7 +687,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -699,7 +699,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },\n@@ -711,7 +711,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n@@ -723,7 +723,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n@@ -735,7 +735,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1032,\n                 /*num_nodes=*/3,\n             },\n@@ -747,7 +747,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -807,7 +807,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },\n@@ -819,7 +819,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n@@ -831,7 +831,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n@@ -843,7 +843,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1056,\n                 /*num_nodes=*/3,\n             },\n@@ -855,7 +855,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -867,7 +867,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },\n@@ -879,7 +879,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n@@ -891,7 +891,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n@@ -903,7 +903,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1032,\n                 /*num_nodes=*/3,\n             },\n@@ -915,7 +915,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -986,7 +986,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllToAll,\n                 /*comm=*/\n-                GPUCommunicationType::RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },\n@@ -997,7 +997,7 @@ INSTANTIATE_TEST_SUITE_P(\n             {\n                 /*opcode=*/HloOpcode::kAllToAll,\n                 /*comm=*/\n-                GPUCommunicationType::NON_RAIL_ALIGNED,\n+                GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/2,\n             },"
        },
        {
            "sha": "ad9a657549cace37bcea47d053eec4dfb57d45a1",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=cecce70fb2e5324aba50442e5adecabcd336a885",
            "patch": "@@ -204,13 +204,13 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n                       GetReplicaGroupCountAndSize(&instr));\n \n   switch (comm) {\n-    case GPUCommunicationType::RAIL_ALIGNED: {\n+    case GPUCommunicationType::MULTI_HOST_WORLD_LEVEL: {\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second / sol_flags.gpus_per_node,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n           gpu_device_info, sol_flags, analysis, symbolic_expr_context);\n     }\n-    case GPUCommunicationType::NON_RAIL_ALIGNED: {\n+    case GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL: {\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second,\n           /*num_communicators=*/num_groups_and_devices->first, instr,"
        },
        {
            "sha": "cd4b5e1ef573bcda4f4d95c60dac2cc2a39ff102",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=cecce70fb2e5324aba50442e5adecabcd336a885",
            "patch": "@@ -115,7 +115,7 @@ bool IsSingleHost(const CommunicationMetadata& pattern) {\n          pattern.replica_count <= pattern.num_devices_per_host;\n }\n \n-bool IsRailAligned(const CommunicationMetadata& pattern) {\n+bool IsWorldLevelCommunication(const CommunicationMetadata& pattern) {\n   if (!IsSingleHost(pattern) && pattern.node_to_participant_count.empty()) {\n     return true;\n   }\n@@ -126,8 +126,8 @@ bool IsRailAligned(const CommunicationMetadata& pattern) {\n       });\n }\n \n-bool IsNonRailAligned(const CommunicationMetadata& pattern) {\n-  return !IsSingleHost(pattern) && !IsRailAligned(pattern);\n+bool IsNonWorldLevelCommunication(const CommunicationMetadata& pattern) {\n+  return !IsSingleHost(pattern) && !IsWorldLevelCommunication(pattern);\n }\n \n }  // namespace\n@@ -152,11 +152,11 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n   if (IsSingleHost(comm)) {\n     return GPUCommunicationType::SINGLE_HOST;\n   }\n-  if (IsRailAligned(comm)) {\n-    return GPUCommunicationType::RAIL_ALIGNED;\n+  if (IsWorldLevelCommunication(comm)) {\n+    return GPUCommunicationType::MULTI_HOST_WORLD_LEVEL;\n   }\n-  if (IsNonRailAligned(comm)) {\n-    return GPUCommunicationType::NON_RAIL_ALIGNED;\n+  if (IsNonWorldLevelCommunication(comm)) {\n+    return GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL;\n   }\n \n   return GPUCommunicationType::UNDEFINED;"
        },
        {
            "sha": "719940c9e4a8503dc07896be3d219b994d6bd1be",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=cecce70fb2e5324aba50442e5adecabcd336a885",
            "patch": "@@ -29,10 +29,10 @@ enum class GPUCommunicationType {\n   UNDEFINED = 0,\n   // Communication involves devices from multiple hosts, and every host\n   // involved in the communication pattern has all of its devices participating.\n-  RAIL_ALIGNED = 1,\n+  MULTI_HOST_WORLD_LEVEL = 1,\n   // Communication involves devices from multiple hosts, but at least one of\n   // the involved hosts has only a subset of its devices participating.\n-  NON_RAIL_ALIGNED = 2,\n+  MULTI_HOST_NON_WORLD_LEVEL = 2,\n   // All devices participating in the collective operation reside on the same\n   // host machine.\n   SINGLE_HOST = 3"
        },
        {
            "sha": "35b9f1f5fc6246a67cd7930d0fdff4525645da7a",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cecce70fb2e5324aba50442e5adecabcd336a885/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc?ref=cecce70fb2e5324aba50442e5adecabcd336a885",
            "patch": "@@ -111,7 +111,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost16Devices) {\n               IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n-TEST_F(CommunicationTypeTest, DetectRailAlignedAllDevices) {\n+TEST_F(CommunicationTypeTest, DetectWorldLevelAllDevices) {\n   absl::string_view kHlo = R\"(\n     HloModule m, num_partitions=16\n \n@@ -131,10 +131,10 @@ TEST_F(CommunicationTypeTest, DetectRailAlignedAllDevices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_WORLD_LEVEL));\n }\n \n-TEST_F(CommunicationTypeTest, DetectRailAlignedHalfMesh) {\n+TEST_F(CommunicationTypeTest, DetectWorldLevelHalfMesh) {\n   absl::string_view kHlo = R\"(\n     HloModule m, num_partitions=32\n \n@@ -157,10 +157,10 @@ TEST_F(CommunicationTypeTest, DetectRailAlignedHalfMesh) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_WORLD_LEVEL));\n }\n \n-TEST_F(CommunicationTypeTest, DetectNonRailAligned) {\n+TEST_F(CommunicationTypeTest, DetectNonWorldLevel) {\n   absl::string_view kHlo = R\"(\n     HloModule m, num_partitions=16\n \n@@ -180,7 +180,7 @@ TEST_F(CommunicationTypeTest, DetectNonRailAligned) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHost16DevicesForEmptyReplicaGroups) {\n@@ -204,7 +204,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost16DevicesForEmptyReplicaGroups) {\n               IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n-TEST_F(CommunicationTypeTest, DetectsRailAligned8DevicesForEmptyReplicaGroups) {\n+TEST_F(CommunicationTypeTest, DetectWorldLevel8DevicesForEmptyReplicaGroups) {\n   absl::string_view kHlo = R\"(\n     HloModule m, replica_count=16\n \n@@ -222,10 +222,10 @@ TEST_F(CommunicationTypeTest, DetectsRailAligned8DevicesForEmptyReplicaGroups) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_WORLD_LEVEL));\n }\n \n-TEST_F(CommunicationTypeTest, DetectsNonRailAligned16Devices) {\n+TEST_F(CommunicationTypeTest, DetectNonWorldLevel16Devices) {\n   absl::string_view kHlo = R\"(\n     HloModule m, replica_count=16\n \n@@ -243,7 +243,7 @@ TEST_F(CommunicationTypeTest, DetectsNonRailAligned16Devices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermute) {\n@@ -266,7 +266,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermute) {\n               IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n }\n \n-TEST_F(CommunicationTypeTest, DetectsNonRailAlignedCollectivePermute) {\n+TEST_F(CommunicationTypeTest, DetectNonWorldLevelCollectivePermute) {\n   absl::string_view kHlo = R\"(\n     HloModule m, num_partitions=16\n \n@@ -284,10 +284,10 @@ TEST_F(CommunicationTypeTest, DetectsNonRailAlignedCollectivePermute) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::NON_RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL));\n }\n \n-TEST_F(CommunicationTypeTest, DetectsRailAlignedCollectivePermute) {\n+TEST_F(CommunicationTypeTest, DetectWorldLevelCollectivePermute) {\n   absl::string_view kHlo = R\"(\n     HloModule m, num_partitions=16\n \n@@ -304,7 +304,7 @@ TEST_F(CommunicationTypeTest, DetectsRailAlignedCollectivePermute) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::RAIL_ALIGNED));\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_WORLD_LEVEL));\n }\n \n }  // namespace"
        }
    ],
    "stats": {
        "total": 170,
        "additions": 85,
        "deletions": 85
    }
}