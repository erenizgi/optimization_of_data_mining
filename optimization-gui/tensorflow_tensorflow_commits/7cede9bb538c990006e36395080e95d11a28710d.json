{
    "author": "pifon2a",
    "message": "[XLA:GPU] Add GpuTopology to CompileOptions.\n\nPiperOrigin-RevId: 850340029",
    "sha": "7cede9bb538c990006e36395080e95d11a28710d",
    "files": [
        {
            "sha": "1e0b3d45cf96f406baa6c6a263aa3847be58a6d6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -26,6 +26,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n+        \"//xla/service:gpu_topology\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "fa754da8315d88bca03d6e091324d5ec25edfe38",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/gpu_topology.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -77,7 +78,7 @@ class GpuCodegenBackend : public CodegenBackend {\n         allow_register_spills_);\n \n     Compiler::CompileOptions options;\n-    options.gpu_target_config = target_config_;\n+    options.gpu_topology = GetSingleDeviceGpuTopology(\"\", target_config_);\n     options.embed_hlo_module = false;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));"
        },
        {
            "sha": "3752cd7f4e28b89f034042f3beca28bd306fa20d",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -450,6 +450,7 @@ cc_library(\n         \"//xla/pjrt:utils\",\n         \"//xla/service:compiler\",\n         \"//xla/service:dump\",\n+        \"//xla/service:gpu_topology\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_module_util\",\n         \"//xla/service:hlo_proto_cc\","
        },
        {
            "sha": "93579128605f2731f55a3a323bd91df39992e796",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/pjrt/utils.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/dump.h\"\n+#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_module_util.h\"\n@@ -179,7 +180,9 @@ StreamExecutorGpuCompiler::Compile(CompileOptions options,\n   DumpHloModuleIfEnabled(*hlo_module, kBeforeOptimizationsDumpName);\n \n   AotCompilationOptions aot_options(gpu_compiler->PlatformId());\n-  aot_options.set_gpu_target_config(*options.gpu_target_config);\n+  GpuTopology xla_gpu_topology = GetSingleDeviceGpuTopology(\n+      /*platform_version=*/\"\", *options.gpu_target_config);\n+  aot_options.set_gpu_topology(xla_gpu_topology);\n   aot_options.set_run_backend_only(\n       options.executable_build_options.run_backend_only());\n "
        },
        {
            "sha": "a66f51cd1e16b474472900dbcffe38b0db1eed43",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -1612,14 +1612,14 @@ cc_library(\n         \":compiled_module\",\n         \":computation_placer\",\n         \":executable\",\n+        \":gpu_topology\",\n         \":hlo_cost_analysis\",\n         \":hlo_module_config\",\n         \":metrics_hook_interface\",\n         \"//xla:debug_options_flags\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/backends/cpu:target_machine_options\",\n-        \"//xla/backends/gpu/target_config\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/stream_executor:device_address_allocator\",\n@@ -6275,6 +6275,8 @@ cc_library(\n         \"//learning/pathways/compilation_service:__subpackages__\",\n         \"//tensorflow/core/common_runtime/eager:__subpackages__\",\n         \":__subpackages__\",\n+        \"//xla/tools:__subpackages__\",\n+        \"//xla/backends/gpu/autotuner:__subpackages__\",\n     ]),\n     deps = [\n         \":gpu_topology_proto_cc\","
        },
        {
            "sha": "42d89cd0bf118c04cc02b54545876855ed36552f",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -37,7 +37,6 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"google/protobuf/message.h\"\n #include \"xla/backends/cpu/target_machine_options.h\"\n-#include \"xla/backends/gpu/target_config/target_config.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -46,6 +45,7 @@ limitations under the License.\n #include \"xla/service/compiled_module.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/metrics_hook_interface.h\"\n@@ -130,9 +130,9 @@ class Compiler {\n         const HloModule& module)>\n         layout_canonicalization_callback = {};\n \n-    // AOT device description. If provided, used instead of querying the device\n-    // on which compilation is performed.\n-    std::optional<GpuTargetConfig> gpu_target_config;\n+    // GPU topology. If provided, used instead of querying the device on which\n+    // compilation is performed.\n+    std::optional<GpuTopology> gpu_topology;\n \n     // CPU specific target information.\n     std::optional<CpuTargetConfig> cpu_target_config;\n@@ -440,11 +440,11 @@ class AotCompilationOptions {\n     sanitize_abilists_dataflow_ = abilists;\n   }\n \n-  const std::optional<gpu::GpuTargetConfig>& gpu_target_config() const {\n-    return gpu_target_config_;\n+  const std::optional<GpuTopology>& gpu_topology() const {\n+    return gpu_topology_;\n   }\n-  void set_gpu_target_config(const gpu::GpuTargetConfig& gpu_target_config) {\n-    gpu_target_config_ = gpu_target_config;\n+  void set_gpu_topology(const GpuTopology& gpu_topology) {\n+    gpu_topology_ = gpu_topology;\n   }\n \n   // Provides a way to end compilation early and get partial outputs.\n@@ -477,7 +477,7 @@ class AotCompilationOptions {\n   bool sanitize_dataflow_ = false;\n   std::vector<std::string> sanitize_abilists_dataflow_;\n   // Contains target-specific information required by AOT compilation.\n-  std::optional<gpu::GpuTargetConfig> gpu_target_config_;\n+  std::optional<GpuTopology> gpu_topology_;\n   EarlyExitPoint early_exit_point_ = EarlyExitPoint::kNone;\n };\n "
        },
        {
            "sha": "b449fb66271d67731f60778f6a6f962b4200bc27",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -2341,6 +2341,7 @@ xla_cc_test(\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:gpu_plugin\",\n+        \"//xla/service:gpu_topology\",\n         \"//xla/service:hlo_runner_interface\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:platform\","
        },
        {
            "sha": "30e35c4c9ec6f402dd89e6c3d828e0bc3d72bd5d",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_test.cc?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -35,6 +35,7 @@ limitations under the License.\n #include \"xla/literal_util.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo_runner_interface.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -129,7 +130,8 @@ TEST_P(GpuAotCompilationTest, AotCompilationWithoutGpuDevice) {\n   // Stream executor is not passed as an option.\n   Compiler::GpuTargetConfig gpu_target_config(stream_exec);\n   AotCompilationOptions aot_options(compiler->PlatformId());\n-  aot_options.set_gpu_target_config(gpu_target_config);\n+  aot_options.set_gpu_topology(\n+      GetSingleDeviceGpuTopology(\"\", gpu_target_config));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::vector<std::unique_ptr<AotCompilationResult>> aot_results,"
        },
        {
            "sha": "a6754f109fe8b8859a5a6d56f49e3bb1f4af0f4d",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 88,
            "deletions": 69,
            "changes": 157,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -383,6 +383,21 @@ se::GpuComputeCapability GetGpuVersion(const se::StreamExecutor* stream_exec) {\n   return stream_exec->GetDeviceDescription().gpu_compute_capability();\n }\n \n+bool IsDevicelessCompilation(const Compiler::CompileOptions& options,\n+                             const se::StreamExecutor* stream_exec) {\n+  return options.early_exit_with_layouts || stream_exec == nullptr;\n+}\n+\n+int GetNumVisibleDevices(const Compiler::CompileOptions& options,\n+                         const se::StreamExecutor* stream_exec,\n+                         const se::Platform* platform) {\n+  if (IsDevicelessCompilation(options, stream_exec) &&\n+      options.gpu_topology.has_value()) {\n+    return options.gpu_topology->num_devices_per_host();\n+  }\n+  return platform->VisibleDeviceCount();\n+}\n+\n }  // namespace\n \n GpuCompiler::GpuCompiler(se::Platform::Id platform_id,\n@@ -532,16 +547,16 @@ absl::Status RunSPMDPasses(\n namespace {\n \n absl::Status SetHostDeviceType(HloInstruction* instr) {\n-  TF_ASSIGN_OR_RETURN(auto backend_config,\n-                      instr->backend_config<GpuBackendConfig>());\n+  ASSIGN_OR_RETURN(auto backend_config,\n+                   instr->backend_config<GpuBackendConfig>());\n   backend_config.set_device_type(DEVICE_TYPE_HOST);\n   RETURN_IF_ERROR(instr->set_backend_config(backend_config));\n   return absl::OkStatus();\n }\n \n absl::Status ClearBackendConfigDeviceType(HloInstruction* instr) {\n-  TF_ASSIGN_OR_RETURN(auto backend_config,\n-                      instr->backend_config<GpuBackendConfig>());\n+  ASSIGN_OR_RETURN(auto backend_config,\n+                   instr->backend_config<GpuBackendConfig>());\n   backend_config.clear_device_type();\n   return instr->set_backend_config(backend_config);\n }\n@@ -1350,8 +1365,8 @@ absl::Status RunDynamicSliceFusionPasses(HloModule* hlo_module,\n   const DebugOptions& opts = hlo_module->config().debug_options();\n   if (opts.xla_gpu_enable_dynamic_slice_fusion()) {\n     HloPassPipeline pipeline(\"dynamic-slice\");\n-    TF_ASSIGN_OR_RETURN(se::Platform * platform,\n-                        se::PlatformManager::PlatformWithId(platform_id));\n+    ASSIGN_OR_RETURN(se::Platform * platform,\n+                     se::PlatformManager::PlatformWithId(platform_id));\n     pipeline.AddPass<GpuReduceScatterCombiner>(\n         kDefaultReduceScatterCombineThreshold,\n         opts.xla_gpu_reduce_scatter_combine_threshold_bytes(),\n@@ -1505,7 +1520,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   // Dump the HLO module after SPMD partitioning. There should be no more Python\n   // callbacks at this point.\n   DumpHloModuleIfEnabled(*hlo_module, \"after_spmd_partitioner\");\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       const stream_executor::Platform* platform,\n       stream_executor::PlatformManager::PlatformWithId(PlatformId()));\n \n@@ -1524,16 +1539,17 @@ absl::Status GpuCompiler::OptimizeHloModule(\n       platform->Name(), enable_sort_rewriter));\n   se::GpuComputeCapability gpu_version =\n       device_description.gpu_compute_capability();\n+  int device_count = GetNumVisibleDevices(options, stream_exec, platform);\n   RETURN_IF_ERROR(RunCollectiveOptimizationPasses(\n       hlo_module, options, layout_insensitive_algsimp_opts, gpu_version,\n-      platform->VisibleDeviceCount(), pointer_size_));\n+      device_count, pointer_size_));\n \n   // Run target-specific HLO optimization passes for convolution\n   // canonicalization.\n   se::dnn::VersionInfo dnn_version = gpu_target_config.dnn_version_info;\n   if (stream_exec != nullptr) {\n     gpu_version = GetGpuVersion(stream_exec);\n-    TF_ASSIGN_OR_RETURN(dnn_version, GetDnnVersionInfo(stream_exec));\n+    ASSIGN_OR_RETURN(dnn_version, GetDnnVersionInfo(stream_exec));\n   }\n \n   RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(\n@@ -1663,7 +1679,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n           gpu_target_config.platform_name == \"ROCM\");\n   DeviceOrDevicelessConfig device_config =\n       GetDeviceConfig(stream_exec, options, gpu_target_config);\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       AutotuneConfig autotune_config,\n       AutotuneConfig::FromDebugOptions(device_config, debug_options));\n   // Lambdas and related constants:\n@@ -1938,8 +1954,10 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n /*static*/ absl::StatusOr<GpuTargetConfig> GpuCompiler::GetTargetConfig(\n     const Compiler::CompileOptions& options, const DebugOptions& debug_opts,\n     se::StreamExecutor* executor) {\n-  if (options.gpu_target_config.has_value()) {\n-    return *options.gpu_target_config;\n+  if (options.gpu_topology.has_value()) {\n+    if (options.gpu_topology->has_gpu_target_config()) {\n+      return options.gpu_topology->gpu_target_config();\n+    }\n   }\n   if (!debug_opts.xla_gpu_target_config_filename().empty()) {\n     std::string gpu_target_config_string;\n@@ -1986,11 +2004,11 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n \n   const DebugOptions debug_opts = module->config().debug_options();\n   RETURN_IF_ERROR(LoadAutotuneResultsFromFile(debug_opts));\n-  bool is_deviceless = options.gpu_target_config.has_value() ||\n+  bool is_deviceless = options.gpu_topology.has_value() ||\n                        !debug_opts.xla_gpu_target_config_filename().empty();\n \n-  TF_ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n-                      GetTargetConfig(options, debug_opts, stream_exec));\n+  ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n+                   GetTargetConfig(options, debug_opts, stream_exec));\n   const std::optional<std::string> unoptimized_fingerprint =\n       MaybeUploadUnoptimizedGpuSymbols(module.get(),\n                                        gpu_target_config.ToProto());\n@@ -2028,9 +2046,8 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   AutotuneResults autotune_results;\n   DeviceOrDevicelessConfig device_config =\n       GetDeviceConfig(stream_exec, options, gpu_target_config);\n-  TF_ASSIGN_OR_RETURN(\n-      AutotuneConfig autotune_config,\n-      AutotuneConfig::FromDebugOptions(device_config, debug_opts));\n+  ASSIGN_OR_RETURN(AutotuneConfig autotune_config,\n+                   AutotuneConfig::FromDebugOptions(device_config, debug_opts));\n   if (!is_deviceless) {\n     RETURN_IF_ERROR(AutotunerUtil::SerializeAutotuneResults(&autotune_results));\n     RETURN_IF_ERROR(SerializeAutotuneResultsToFile(debug_opts));\n@@ -2044,7 +2061,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   }\n \n   if (DumpingEnabledForHloModule(*module)) {\n-    TF_ASSIGN_OR_RETURN(\n+    ASSIGN_OR_RETURN(\n         std::string autotune_results,\n         AutotunerUtil::SerializeAutotuneResults(/*as_textproto=*/true));\n     DumpToFileInDirOrStdout(*module, \"\", \"autotune_results.pbtxt\",\n@@ -2197,7 +2214,7 @@ GpuCompiler::CompileSingleModule(\n                 : \".\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       BackendCompileResult result,\n       CompileTargetBinary(module_config, llvm_module, device_description,\n                           relocatable, debug_module, options, shard_number));\n@@ -2414,7 +2431,7 @@ absl::StatusOr<GpuCompiler::BackendCompileResult> GpuCompiler::CompileAndLink(\n   std::vector<KernelReuseCache::NamedBinary> binaries_to_cache;\n   binaries_to_cache.reserve(single_function_module_count);\n   for (const auto& [name, maybe_result] : compile_results) {\n-    TF_ASSIGN_OR_RETURN(auto result, maybe_result);\n+    ASSIGN_OR_RETURN(auto result, maybe_result);\n     if (result.binary.empty()) {\n       continue;\n     }\n@@ -2490,10 +2507,10 @@ absl::StatusOr<xla::cpu::CompilationResultProto> GetCpuCompilationResult(\n     const HloModuleProto& hlo_proto) {\n   xla::cpu::NanoRtClient client;\n   XlaComputation computation(hlo_proto);\n-  TF_ASSIGN_OR_RETURN(std::unique_ptr<xla::cpu::NanoRtExecutable> executable,\n-                      client.Compile(computation));\n-  TF_ASSIGN_OR_RETURN(std::unique_ptr<AotCompilationResult> result,\n-                      client.Export(executable.get()));\n+  ASSIGN_OR_RETURN(std::unique_ptr<xla::cpu::NanoRtExecutable> executable,\n+                   client.Compile(computation));\n+  ASSIGN_OR_RETURN(std::unique_ptr<AotCompilationResult> result,\n+                   client.Export(executable.get()));\n   xla::cpu::CpuAotCompilationResult* cpu_aot_compilation_result =\n       tsl::down_cast<xla::cpu::CpuAotCompilationResult*>(result.get());\n   return cpu_aot_compilation_result->proto();\n@@ -2509,9 +2526,9 @@ GpuCompiler::CompileToBackendResult(\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n   RETURN_IF_ERROR(\n       RunPreSchedulingPasses(module, gpu_device_info, alias_info.get()));\n-  TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n-                      ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n-                                        &mlir_context_, alias_info.get()));\n+  ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n+                   ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n+                                     &mlir_context_, alias_info.get()));\n   HloPassPipeline pipeline(\"scheduled-gpu-module\");\n   AddHloVerifier(&pipeline);\n   RETURN_IF_ERROR(pipeline.Run(module).status());\n@@ -2529,8 +2546,8 @@ GpuCompiler::CompileToBackendResult(\n             \". Are you missing gpu_plugin or stream_executor dependency?\"));\n   }\n \n-  TF_ASSIGN_OR_RETURN(bool can_use_link_modules,\n-                      CanUseLinkModules(module->config(), gpu_device_info));\n+  ASSIGN_OR_RETURN(bool can_use_link_modules,\n+                   CanUseLinkModules(module->config(), gpu_device_info));\n   const bool split_modules =\n       can_use_link_modules &&\n       module->config()\n@@ -2548,12 +2565,12 @@ GpuCompiler::CompileToBackendResult(\n     BufferValue::SizeFunction buffer_size_bytes_function =\n         BufferSizeBytesFunction();\n     // Compile the module to thnks and llvm IR.\n-    TF_ASSIGN_OR_RETURN(compile_module_results,\n-                        CompileModuleToLlvmIr(\n-                            module, llvm_context, target_triple_, data_layout_,\n-                            *platform, gpu_device_info, alias_info.get(),\n-                            std::move(buffer_size_bytes_function),\n-                            /*split_constants_module=*/use_cache));\n+    ASSIGN_OR_RETURN(compile_module_results,\n+                     CompileModuleToLlvmIr(\n+                         module, llvm_context, target_triple_, data_layout_,\n+                         *platform, gpu_device_info, alias_info.get(),\n+                         std::move(buffer_size_bytes_function),\n+                         /*split_constants_module=*/use_cache));\n   }\n \n   if (user_pre_optimization_hook_) {\n@@ -2577,12 +2594,12 @@ GpuCompiler::CompileToBackendResult(\n   // TODO(anlunx): Enable multi-threading once deviceless AOT compilation is\n   // enabled.\n   if (split_modules) {\n-    TF_ASSIGN_OR_RETURN(backend_result,\n-                        CompileAndLink(module->config(), compile_module_results,\n-                                       gpu_device_info, options, module));\n+    ASSIGN_OR_RETURN(backend_result,\n+                     CompileAndLink(module->config(), compile_module_results,\n+                                    gpu_device_info, options, module));\n   } else {\n     CHECK(compile_module_results.llvm_module_constants == nullptr);\n-    TF_ASSIGN_OR_RETURN(\n+    ASSIGN_OR_RETURN(\n         backend_result,\n         CompileSingleModule(module->config(), gpu_device_info, module,\n                             &*compile_module_results.llvm_module,\n@@ -2639,8 +2656,8 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   }\n \n   const DebugOptions& debug_opts = module->config().debug_options();\n-  TF_ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n-                      GetTargetConfig(options, debug_opts, stream_exec));\n+  ASSIGN_OR_RETURN(GpuTargetConfig gpu_target_config,\n+                   GetTargetConfig(options, debug_opts, stream_exec));\n \n   if (DumpingEnabledForHloModule(*module)) {\n     std::string textproto;\n@@ -2672,9 +2689,9 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n         tsl::strings::HumanReadableNumBytes(cost_analysis.bytes_accessed()));\n   }\n \n-  TF_ASSIGN_OR_RETURN(CompileResultWithMetadata res,\n-                      CompileToBackendResult(module.get(), &llvm_context,\n-                                             options, gpu_device_info));\n+  ASSIGN_OR_RETURN(CompileResultWithMetadata res,\n+                   CompileToBackendResult(module.get(), &llvm_context, options,\n+                                          gpu_device_info));\n   ModuleStats module_stats = res.backend_result.module_stats;\n \n   if (DumpingEnabledForHloModule(*module)) {\n@@ -2694,7 +2711,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   });\n \n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       std::unique_ptr<GpuExecutable> gpu_executable,\n       GpuExecutable::Create(GpuExecutable::Params{\n           /*asm_text=*/embed_debug_info ? std::move(res.backend_result.asm_text)\n@@ -2771,14 +2788,14 @@ GpuCompiler::NewCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n                                    const AotCompilationOptions& options) {\n   CompileOptions compile_options;\n   compile_options.device_allocator = options.device_allocator();\n-  compile_options.gpu_target_config = options.gpu_target_config();\n+  compile_options.gpu_topology = options.gpu_topology();\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       std::unique_ptr<Executable> executable,\n       RunBackend(std::move(hlo_module), options.executor(), compile_options));\n \n   std::vector<std::unique_ptr<AotCompilationResult>> results;\n-  TF_ASSIGN_OR_RETURN(results.emplace_back(), Export(executable.get()));\n+  ASSIGN_OR_RETURN(results.emplace_back(), Export(executable.get()));\n   return results;\n }\n \n@@ -2790,11 +2807,11 @@ GpuCompiler::EarlyExitCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n       AotCompilationOptions::EarlyExitPoint::kAfterLayoutAssignment;\n   CompileOptions compile_options;\n   compile_options.device_allocator = options.device_allocator();\n-  compile_options.gpu_target_config = options.gpu_target_config();\n+  compile_options.gpu_topology = options.gpu_topology();\n   compile_options.early_exit_with_layouts = early_exit_with_layouts;\n \n   std::vector<std::unique_ptr<AotCompilationResult>> results;\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       auto optimized_module,\n       RunHloPasses(std::move(hlo_module), options.executor(), compile_options));\n   results.push_back(std::make_unique<EarlyExitCompilationResult>(\n@@ -2814,30 +2831,32 @@ GpuCompiler::LegacyCompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n     }};\n     CompileOptions compile_options;\n     compile_options.device_allocator = options.device_allocator();\n-    compile_options.gpu_target_config = options.gpu_target_config();\n-    TF_ASSIGN_OR_RETURN(optimized_module,\n-                        RunHloPasses(std::move(hlo_module), options.executor(),\n-                                     compile_options));\n+    compile_options.gpu_topology = options.gpu_topology();\n+    ASSIGN_OR_RETURN(optimized_module,\n+                     RunHloPasses(std::move(hlo_module), options.executor(),\n+                                  compile_options));\n   } else {\n     optimized_module = std::move(hlo_module);\n   }\n \n-\n-  const std::optional<Compiler::GpuTargetConfig>& target_config =\n-      options.gpu_target_config();\n+  std::optional<Compiler::GpuTargetConfig> target_config;\n+  if (options.gpu_topology().has_value() &&\n+      options.gpu_topology()->has_gpu_target_config()) {\n+    target_config = options.gpu_topology()->gpu_target_config();\n+  }\n   CHECK(target_config.has_value() || options.executor() != nullptr);\n   const se::DeviceDescription& gpu_device_info =\n       target_config.has_value() ? target_config->device_description\n                                 : options.executor()->GetDeviceDescription();\n   llvm::LLVMContext llvm_context;\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       CompileResultWithMetadata res,\n       CompileToBackendResult(optimized_module.get(), &llvm_context,\n                              {options.device_allocator()}, gpu_device_info));\n \n   // Create GpuThunkAotCompilationResult if thunk runtime is enabled.\n   std::vector<std::unique_ptr<AotCompilationResult>> results;\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       results.emplace_back(),\n       LegacyGpuAotCompilationResult::FromModule(\n           optimized_module.get(),\n@@ -2864,7 +2883,7 @@ absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(\n           .config()\n           .debug_options()\n           .xla_gpu_experimental_aot_compiled_thunks()) {\n-    TF_ASSIGN_OR_RETURN(GpuExecutableProto proto, gpu_executable->ToProto());\n+    ASSIGN_OR_RETURN(GpuExecutableProto proto, gpu_executable->ToProto());\n     return GpuAotCompilationResult::FromProto(std::move(proto));\n   }\n \n@@ -3125,7 +3144,7 @@ GpuCompiler::LoadExecutableFromAotResult(\n   const GpuExecutableProto& proto = gpu_aot_result->GetGpuExecutableProto();\n \n   // Recreate HloModule+HloModuleConfig from proto.\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       std::unique_ptr<HloModule> hlo_module,\n       HloModule::CreateFromProtoWithConfig(proto.hlo_module_with_config()));\n \n@@ -3134,8 +3153,8 @@ GpuCompiler::LoadExecutableFromAotResult(\n   std::vector<uint8_t> binary(proto.binary().begin(), proto.binary().end());\n \n   // Build the executable, which should be a thunk sequence.\n-  TF_ASSIGN_OR_RETURN(se::Platform * platform,\n-                      se::PlatformManager::PlatformWithId(PlatformId()));\n+  ASSIGN_OR_RETURN(se::Platform * platform,\n+                   se::PlatformManager::PlatformWithId(PlatformId()));\n   std::string platform_name = platform->Name();\n \n   const se::DeviceDescription& gpu_device_info =\n@@ -3144,7 +3163,7 @@ GpuCompiler::LoadExecutableFromAotResult(\n \n   // Recreate BufferAssignment from proto.\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       std::unique_ptr<BufferAssignment> buffer_assignment,\n       BufferAssignment::FromProto(proto.buffer_assignment(), hlo_module.get(),\n                                   BufferSizeBytesFunction(), alias_info.get()));\n@@ -3164,14 +3183,14 @@ GpuCompiler::LoadExecutableFromAotResult(\n   }\n \n   ThunkEmitter thunk_emitter(&ir_emitter_context);\n-  TF_ASSIGN_OR_RETURN(auto sequential_thunk,\n-                      thunk_emitter.EmitHloEntryComputation(hlo_module.get()));\n+  ASSIGN_OR_RETURN(auto sequential_thunk,\n+                   thunk_emitter.EmitHloEntryComputation(hlo_module.get()));\n \n   // Get all other fields required by GpuExecutable.\n   std::vector<GpuExecutable::ConstantInfo> constants =\n       std::move(ir_emitter_context.constants());\n-  TF_ASSIGN_OR_RETURN(auto output_info,\n-                      GetOutputInfo(*hlo_module, *buffer_assignment));\n+  ASSIGN_OR_RETURN(auto output_info,\n+                   GetOutputInfo(*hlo_module, *buffer_assignment));\n   ProgramShape program_shape =\n       hlo_module->entry_computation_layout().ComputeProgramShape();\n   *program_shape.mutable_result() = hlo_module->result_shape();"
        },
        {
            "sha": "0d6a357818daf241ec20b59fdda2df5af4ebbaae",
            "filename": "third_party/xla/xla/service/gpu_topology.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.cc?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -88,4 +88,10 @@ absl::StatusOr<GpuTopology> GetGpuTopologyForPlatform(\n                      num_devices_per_host, std::move(gpu_target_config));\n }\n \n+GpuTopology GetSingleDeviceGpuTopology(\n+    absl::string_view platform_version,\n+    const gpu::GpuTargetConfig& gpu_target_config) {\n+  return GpuTopology(platform_version, 1, 1, 1, gpu_target_config);\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "5aa383ec42945d7bc865817ec9b8877ead304926",
            "filename": "third_party/xla/xla/service/gpu_topology.h",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu_topology.h?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -76,11 +76,11 @@ class GpuTopology {\n   }\n \n  private:\n-  const std::string platform_version_;\n-  const int32_t num_partitions_;\n-  const int32_t num_hosts_per_partition_;\n-  const int32_t num_devices_per_host_;\n-  const std::optional<gpu::GpuTargetConfig> gpu_target_config_;\n+  std::string platform_version_;\n+  int32_t num_partitions_;\n+  int32_t num_hosts_per_partition_;\n+  int32_t num_devices_per_host_;\n+  std::optional<gpu::GpuTargetConfig> gpu_target_config_;\n \n   bool is_topology_symmetric() const {\n     return num_partitions_ != -1 && num_hosts_per_partition_ != -1 &&\n@@ -92,6 +92,10 @@ absl::StatusOr<GpuTopology> GetGpuTopologyForPlatform(\n     absl::string_view platform_version, int32_t num_partitions,\n     int32_t num_hosts_per_partition, int32_t num_devices_per_host);\n \n+GpuTopology GetSingleDeviceGpuTopology(\n+    absl::string_view platform_version,\n+    const gpu::GpuTargetConfig& gpu_target_config);\n+\n }  // namespace xla\n \n #endif  // XLA_SERVICE_GPU_TOPOLOGY_H_"
        },
        {
            "sha": "f9b4437a9aeb2175bfba02fad13ca7815274b401",
            "filename": "third_party/xla/xla/tools/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2FBUILD?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -1004,6 +1004,7 @@ tsl_gpu_library(\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:export_hlo\",\n+        \"//xla/service:gpu_topology\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:platform_util\","
        },
        {
            "sha": "0016243bde2586a803eaafd01a0090d84cb5757a",
            "filename": "third_party/xla/xla/tools/xla_compile_lib.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7cede9bb538c990006e36395080e95d11a28710d/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc?ref=7cede9bb538c990006e36395080e95d11a28710d",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/service/export_hlo.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/gpu_symbol_repository.h\"\n+#include \"xla/service/gpu_topology.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/platform_util.h\"\n@@ -94,7 +95,9 @@ static absl::StatusOr<std::string> CompileGpuExecutable(\n \n   if (aot) {\n     AotCompilationOptions aot_options(platform->id());\n-    aot_options.set_gpu_target_config(*target_config);\n+    GpuTopology topology =\n+        GetSingleDeviceGpuTopology(/*platform_version=*/\"\", *target_config);\n+    aot_options.set_gpu_topology(topology);\n     // We need the optimized module, so we call RunHloPasses ourselves above.\n     aot_options.set_run_backend_only(true);\n "
        }
    ],
    "stats": {
        "total": 220,
        "additions": 132,
        "deletions": 88
    }
}