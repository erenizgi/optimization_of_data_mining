{
    "author": "vwbaker",
    "message": "Propagate kernel statistics from NVPTX compilation to XLA Executable.\n\nThis will allow the autotuner or any other the caller to make the decision on whether to accept the compilation based on how many register bytes were spilled rather than a binary choice of cancelling on register spilling. We can then replace cancel_if_reg_spill by using this information.\n\nPiperOrigin-RevId: 840185750",
    "sha": "2339ad732bc24eb03ab3e2572f76e445a638b14f",
    "files": [
        {
            "sha": "767af23d6679ae4c45efbd6c91352cf7da2f67d9",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -1546,12 +1546,14 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/base:nullability\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "c3a136dc1251c91de337febf5a3171290dd0e232",
            "filename": "third_party/xla/xla/service/executable.h",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n \n #include \"absl/base/nullability.h\"\n #include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n@@ -45,6 +46,7 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -426,6 +428,14 @@ class Executable {\n                : nullptr;\n   }\n \n+  // Returns a map of kernel name to relevant kernel stats.\n+  const ModuleStats& module_stats() { return module_stats_; }\n+\n+  // Sets a module_stats map of kernel name to relevant kernel stats.\n+  void set_module_stats(ModuleStats module_stats) {\n+    module_stats_ = std::move(module_stats);\n+  }\n+\n   // Gather unused but donated buffers, return them to the caller of this API.\n   // We don't free buffers inside this function since the caller could have\n   // different preferences for buffer deallocation. For example, in TensorFlow,\n@@ -468,6 +478,9 @@ class Executable {\n   std::unique_ptr<HloProfilePrinterData> hlo_profile_printer_data_;\n   std::unique_ptr<HloProfileIndexMap> hlo_profile_index_map_;\n \n+  // A map from kernel name to relevant kernel stats.\n+  ModuleStats module_stats_;\n+\n   // The serialized HLO proto. Non-null only if dumping snapshots is enabled.\n   // This field may also be only partially set: if only\n   // hlo_proto_->buffer_assignment is set and hlo_proto_->hlo_module isn't, the"
        },
        {
            "sha": "6ddc45a6f0f3147aab2d2dc0a721e4ebb76d9d06",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -725,6 +725,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:event_based_timer\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:module_spec\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:scoped_module_handle\",\n@@ -1834,6 +1835,7 @@ cc_library(\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:semantic_version\","
        },
        {
            "sha": "2844dd61d01861c8b27fe2d3bf9d29db246310a9",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -298,6 +298,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/dnn.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n@@ -2554,6 +2555,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   TF_ASSIGN_OR_RETURN(CompileResultWithMetadata res,\n                       CompileToBackendResult(module.get(), &llvm_context,\n                                              options, gpu_device_info));\n+  ModuleStats module_stats = res.backend_result.module_stats;\n \n   if (DumpingEnabledForHloModule(*module)) {\n     DumpToFileInDirOrStdout(\n@@ -2598,7 +2600,8 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n           /*debug_module=*/options.embed_hlo_module\n               ? std::move(module)\n               : std::unique_ptr<HloModule>(),\n-          /*enable_debug_info_manager=*/embed_debug_info}));\n+          /*enable_debug_info_manager=*/embed_debug_info,\n+          /*module_stats=*/std::move(module_stats)}));\n \n   if (embed_ir_in_executable) {\n     std::string ir_module_string_before_opt ="
        },
        {
            "sha": "235c184ecf22c601c8a4b3e1a79c536dcc305281",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -45,6 +45,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/dnn.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -144,6 +145,7 @@ class GpuCompiler : public LLVMCompiler {\n     std::string asm_text;\n     std::vector<uint8_t> binary;\n     BinaryMap dnn_compiled_graphs;\n+    ModuleStats module_stats;\n   };\n \n   // During compilation with device, stream_exec != null and autotune_results"
        },
        {
            "sha": "ae22f4041ede10a6eb7bb7e1f29f3dc498835359",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -251,7 +251,8 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(\n       std::move(params.mlir_allocations), std::move(params.buffer_assignment),\n       std::move(allocator.MutableAllocations()), std::move(params.alias_info),\n       std::move(params.debug_options), std::move(params.constants),\n-      std::move(params.output_info), params.enable_debug_info_manager));\n+      std::move(params.output_info), params.enable_debug_info_manager,\n+      std::move(params.module_stats)));\n }\n \n // Implementation note: HLO profiling is always enabled for GPU executables,\n@@ -268,7 +269,7 @@ GpuExecutable::GpuExecutable(\n     std::unique_ptr<GpuAliasInfo> alias_info, DebugOptions debug_options,\n     std::vector<ConstantInfo> constants,\n     absl::flat_hash_map<ShapeIndex, OutputInfo> output_info,\n-    bool enable_debug_info_manager)\n+    bool enable_debug_info_manager, ModuleStats module_stats)\n     : Executable(std::move(debug_module)),\n       text_(std::move(asm_text)),\n       binary_(std::move(binary)),\n@@ -300,6 +301,7 @@ GpuExecutable::GpuExecutable(\n     XlaDebugInfoManager::Get()->RegisterModule(shared_module(),\n                                                buffer_assignment_);\n   }\n+  set_module_stats(std::move(module_stats));\n }\n \n GpuExecutable::~GpuExecutable() {"
        },
        {
            "sha": "671c572e230c46ff8595ff6e4d4ef939d5571751",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -54,6 +54,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/scoped_module_handle.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n@@ -120,6 +121,7 @@ class GpuExecutable : public Executable {\n     se::DeviceDescription device_description;\n     std::unique_ptr<HloModule> debug_module = nullptr;\n     bool enable_debug_info_manager = true;\n+    ModuleStats module_stats;\n   };\n \n   static absl::StatusOr<std::unique_ptr<GpuExecutable>> Create(Params params);\n@@ -242,7 +244,7 @@ class GpuExecutable : public Executable {\n       std::unique_ptr<GpuAliasInfo> alias_info, DebugOptions debug_options,\n       std::vector<ConstantInfo> constants,\n       absl::flat_hash_map<ShapeIndex, OutputInfo> output_info,\n-      bool enable_debug_info_manager);\n+      bool enable_debug_info_manager, ModuleStats module_stats);\n \n   // GpuExecutable check with either AMD's ISA version, or Nvidia's major minor\n   // version for compute capability, depending on the hardware."
        },
        {
            "sha": "d48523abc3da12801e141500b3450eaab1219665",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -717,15 +717,18 @@ NVPTXCompiler::CompileTargetBinary(\n                         compilation_provider->CompileToRelocatableModule(\n                             cc, ptx, compilation_options));\n     record_ptx_to_cubin_metric();\n-    return BackendCompileResult{std::move(ptx),\n-                                std::move(relocatable_module.cubin)};\n+    return BackendCompileResult{\n+        std::move(ptx), std::move(relocatable_module.cubin),\n+        /*dnn_compiled_graphs=*/{}, std::move(relocatable_module.module_stats)};\n   }\n \n   TF_ASSIGN_OR_RETURN(\n       se::cuda::Assembly assembly,\n       compilation_provider->Compile(cc, ptx, compilation_options));\n   record_ptx_to_cubin_metric();\n-  return BackendCompileResult{std::move(ptx), std::move(assembly.cubin)};\n+  return BackendCompileResult{std::move(ptx), std::move(assembly.cubin),\n+                              /*dnn_compiled_graphs=*/{},\n+                              std::move(assembly.module_stats)};\n }\n \n absl::StatusOr<bool> NVPTXCompiler::CanUseLinkModules("
        },
        {
            "sha": "9d69d97b21ee2511ab8a3335a0a9f6621fe255a6",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -850,6 +850,7 @@ cc_library(\n         \":compilation_provider\",\n         \":cuda_compute_capability\",\n         \":ptx_compiler_helpers\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/gpu:gpu_asm_opts\",\n         \"//xla/tsl/platform:statusor\",\n@@ -913,6 +914,7 @@ xla_cc_test(\n         \":cuda_compute_capability\",\n         \":ptx_compiler\",\n         \":ptx_compiler_support\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/gpu:gpu_asm_opts\",\n         \"//xla/tsl/platform:statusor\",\n@@ -969,6 +971,7 @@ cc_library(\n         \":compilation_provider\",\n         \":cuda_compute_capability\",\n         \":ptx_compiler_helpers\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor/gpu:gpu_asm_opts\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1033,10 +1036,13 @@ xla_cc_test(\n         \"notsan\",\n     ],\n     deps = [\n+        \":compilation_provider\",\n         \":cuda_compute_capability\",\n         \":nvjitlink\",\n         \":nvjitlink_support\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor/gpu:gpu_asm_opts\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n@@ -1597,6 +1603,7 @@ cc_library(\n         \":ptx_compiler_helpers\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/gpu:gpu_asm_opts\",\n         \"//xla/tsl/platform:env\",\n@@ -1746,6 +1753,7 @@ cc_library(\n     deps = [\n         \":compilation_options\",\n         \":cuda_compute_capability\",\n+        \"//xla/stream_executor:kernel_stats\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\","
        },
        {
            "sha": "d6755a6960863c9050317ba2a4d2b4eded0879ce",
            "filename": "third_party/xla/xla/stream_executor/cuda/compilation_provider.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcompilation_provider.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcompilation_provider.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcompilation_provider.h?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/cuda/compilation_options.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n \n namespace stream_executor::cuda {\n \n@@ -48,6 +49,9 @@ struct RelocatableModule {\n   // An optional error/informational log of the compilation process that\n   // produced this CUBIN.\n   std::optional<std::string> compilation_log;\n+\n+  // Stats about each of the kernels from the compiler.\n+  ModuleStats module_stats;\n };\n \n // A compiled and linked CUDA program in CUBIN format.\n@@ -65,6 +69,9 @@ struct Assembly {\n   // An optional error/informational log of the compilation process that\n   // produced this CUBIN.\n   std::optional<std::string> compilation_log;\n+\n+  // Stats about each of the kernels from the compiler.\n+  ModuleStats module_stats;\n };\n \n // A PTX module in textual assembly format."
        },
        {
            "sha": "eec2ef7829a08868a5717b031b45a3be462ea5ef",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvjitlink_impl.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_impl.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/nvjitlink.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler_helpers.h\"\n #include \"xla/stream_executor/gpu/gpu_asm_opts.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -242,6 +243,7 @@ absl::StatusOr<cuda::Assembly> CompileAndLinkUsingLibNvJitLink(\n \n   TF_RETURN_IF_ERROR(\n       CreateErrorFromPTXASLog(info_log, architecture, cancel_if_reg_spill));\n+  ModuleStats module_stats = ExtractModuleStatsFromLog(info_log);\n \n   size_t cubin_size{};\n   RETURN_IF_NVJITLINK_ERROR(\n@@ -255,7 +257,8 @@ absl::StatusOr<cuda::Assembly> CompileAndLinkUsingLibNvJitLink(\n         absl::StrCat(*maybe_compilation_log, \"\\n\", std::move(info_log));\n   }\n \n-  return cuda::Assembly{std::move(cubin), maybe_compilation_log};\n+  return cuda::Assembly{std::move(cubin), maybe_compilation_log,\n+                        std::move(module_stats)};\n }\n \n absl::StatusOr<int> GetLatestPtxIsaVersionForLibNvJitLink() {"
        },
        {
            "sha": "38fe3904946e3957c19bbe94ac58681e3f8b4724",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvjitlink_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_test.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -31,6 +31,8 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/nvjitlink_support.h\"\n #include \"xla/stream_executor/gpu/gpu_asm_opts.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace {\n \n@@ -197,11 +199,16 @@ TEST_F(NvJitLinkTest, CancelsOnRegSpill) {\n               absl_testing::StatusIs(absl::StatusCode::kCancelled));\n \n   // We also test the converse to ensure our test case isn't broken.\n-  EXPECT_THAT(CompileAndLinkHelper(kDefaultComputeCapability,\n-                                   {dependent_ptx.c_str(), kDependeePtx},\n-                                   /*disable_gpuasm_optimizations=*/true,\n-                                   /*cancel_if_reg_spill=*/false),\n-              absl_testing::IsOk());\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::cuda::Assembly assembly,\n+      CompileAndLinkHelper(kDefaultComputeCapability,\n+                           {dependent_ptx.c_str(), kDependeePtx},\n+                           /*disable_gpuasm_optimizations=*/true,\n+                           /*cancel_if_reg_spill=*/false));\n+  ASSERT_EQ(assembly.module_stats.size(), 1);\n+  KernelStats kernel_stats = assembly.module_stats.begin()->second;\n+  EXPECT_GT(kernel_stats.store_bytes_spilled, 0);\n+  EXPECT_GT(kernel_stats.load_bytes_spilled, 0);\n }\n \n }  // namespace"
        },
        {
            "sha": "f544efa99ebdcdcf65015c3c8a5b1d225cdb8732",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvptxcompiler_compilation_provider.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvptxcompiler_compilation_provider.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvptxcompiler_compilation_provider.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvptxcompiler_compilation_provider.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -68,7 +68,8 @@ NvptxcompilerCompilationProvider::CompileToRelocatableModule(\n                       CompileHelper(cc, ptx, options,\n                                     /*compile_to_relocatable_module=*/true));\n   return RelocatableModule{std::move(assembly.cubin),\n-                           std::move(assembly.compilation_log)};\n+                           std::move(assembly.compilation_log),\n+                           std::move(assembly.module_stats)};\n }\n \n absl::StatusOr<Assembly> NvptxcompilerCompilationProvider::CompileAndLink("
        },
        {
            "sha": "2d56a0ad201467ddb515f2c91e183dc99e3f63b8",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler_helpers.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -104,7 +104,7 @@ ModuleStats ExtractModuleStatsFromLog(absl::string_view log) {\n \n   int spill_stores = 0;\n   int spill_loads = 0;\n-  absl::string_view function_name;\n+  std::string function_name;\n   absl::string_view search_log = log;\n   while (RE2::FindAndConsume(&search_log, *kSpillRegex, &function_name,\n                              &spill_stores, &spill_loads)) {"
        },
        {
            "sha": "24a716e8e62b8b1054f0291d086e05a50c6d2900",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler_impl.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_impl.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/ptx_compiler.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler_helpers.h\"\n #include \"xla/stream_executor/gpu/gpu_asm_opts.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -176,6 +177,7 @@ absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingLibNvPtxCompiler(\n       VLOG(2) << info_log;\n     }\n   }\n+  ModuleStats module_stats = ExtractModuleStatsFromLog(info_log);\n \n   size_t cubinSize{};\n   RETURN_IF_NVPTXCOMPILER_ERROR(\n@@ -191,7 +193,8 @@ absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingLibNvPtxCompiler(\n         absl::StrCat(std::move(*error_log), \"\\n\", std::move(info_log));\n   }\n \n-  return cuda::Assembly{cubin, std::move(maybe_compilation_log)};\n+  return cuda::Assembly{cubin, std::move(maybe_compilation_log),\n+                        std::move(module_stats)};\n }\n \n absl::StatusOr<SemanticVersion> GetLibNvPtxCompilerVersion() {"
        },
        {
            "sha": "f792fa30dbb6bdb354642b3b151caaf58b43033f",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_test.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler_support.h\"\n #include \"xla/stream_executor/gpu/gpu_asm_opts.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -203,6 +204,19 @@ TEST_F(PtxCompilerTest, CancelsOnRegSpill) {\n               absl_testing::IsOk());\n }\n \n+TEST_F(PtxCompilerTest, RecordsRegisterSpillStats) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::cuda::Assembly assembly,\n+      CompileGpuAsmUsingLibNvPtxCompiler(\n+          kDefaultComputeCapability, kSpillingPtx,\n+          stream_executor::GpuAsmOpts(/*disable_gpuasm_optimizations=*/true),\n+          /*cancel_if_reg_spill=*/false, /*dump_compilation_log=*/false));\n+  ASSERT_EQ(assembly.module_stats.size(), 1);\n+  KernelStats kernel_stats = assembly.module_stats.begin()->second;\n+  EXPECT_GT(kernel_stats.store_bytes_spilled, 0);\n+  EXPECT_GT(kernel_stats.load_bytes_spilled, 0);\n+}\n+\n TEST_F(PtxCompilerTest, AcceptsExtraArguments) {\n   // It's tricky to test whether `extra_arguments` works without depending on\n   // too much nvptx internals. So we pass the `--generate-line-info` flags and"
        },
        {
            "sha": "b5c36a145a853be37cbe123c4a12fa943128a9ff",
            "filename": "third_party/xla/xla/stream_executor/cuda/subprocess_compilation.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2339ad732bc24eb03ab3e2572f76e445a638b14f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc?ref=2339ad732bc24eb03ab3e2572f76e445a638b14f",
            "patch": "@@ -50,6 +50,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler_helpers.h\"\n #include \"xla/stream_executor/gpu/gpu_asm_opts.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -354,6 +355,7 @@ absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingPtxAs(\n       VLOG(2) << stderr_output;\n     }\n   }\n+  ModuleStats module_stats = ExtractModuleStatsFromLog(stderr_output);\n \n   // Read in the result of compilation and return it as a byte vector.\n   std::string cubin;\n@@ -364,7 +366,8 @@ absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingPtxAs(\n   if (dump_compilation_log) {\n     maybe_compilation_error_log = std::move(stderr_output);\n   }\n-  return cuda::Assembly{cubin_vector, maybe_compilation_error_log};\n+  return cuda::Assembly{cubin_vector, maybe_compilation_error_log,\n+                        std::move(module_stats)};\n }\n \n absl::StatusOr<SemanticVersion> GetAsmCompilerVersion("
        }
    ],
    "stats": {
        "total": 109,
        "additions": 92,
        "deletions": 17
    }
}