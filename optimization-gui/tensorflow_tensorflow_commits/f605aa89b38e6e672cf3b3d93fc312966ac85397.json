{
    "author": "tensorflower-gardener",
    "message": "Reverts ea2d4362090e1bfd9da21e56ce9aac0b5dc2c194\n\nPiperOrigin-RevId: 839259976",
    "sha": "f605aa89b38e6e672cf3b3d93fc312966ac85397",
    "files": [
        {
            "sha": "22017a733945a224333814731c425a999c2fc115",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=f605aa89b38e6e672cf3b3d93fc312966ac85397",
            "patch": "@@ -1096,14 +1096,6 @@ absl::StatusOr<TensorValue> EmitPad(\n           .getResult());\n }\n \n-absl::StatusOr<TensorValue> EmitTiledDynamicSlice(\n-    mlir::ImplicitLocOpBuilder& b,\n-    const TiledHloInstruction& tiled_dynamic_slice,\n-    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n-  // Slicing happens in `ComputeOffsetsForTile` when this value is emitted.\n-  return values[tiled_dynamic_slice.operand(0)];\n-}\n-\n absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n     mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo,\n@@ -1236,7 +1228,9 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kDynamicSlice) {\n-    return EmitTiledDynamicSlice(b, tiled_hlo, values);\n+    // Dynamic slice is implemented as a load and does not require any further\n+    // processing.\n+    return values[tiled_hlo.operand(0)];\n   }\n \n   return absl::UnimplementedError("
        },
        {
            "sha": "57c38baca46cde3a3f997db8fc46e7dea6bd9e19",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 22,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=f605aa89b38e6e672cf3b3d93fc312966ac85397",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n \n #include <string>\n+#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -652,8 +653,10 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n     case HloOpcode::kParameter:\n       return CodegenDecision::Allow();\n     case HloOpcode::kDynamicSlice:\n-      return IsTritonSupportedDynamicSlice(\n-          *Cast<HloDynamicSliceInstruction>(&instr));\n+      // TODO(b/417172838): enable this once we confirm that no benchmarks were\n+      // regressed.\n+      return CodegenDecision::Forbid(\n+          \"dynamic slice is supported but not enabled yet\");\n     case HloOpcode::kBitcast:\n       if (ShapeUtil::ElementsIn(instr.operand(0)->shape()) !=\n           ShapeUtil::ElementsIn(instr.shape())) {\n@@ -701,6 +704,7 @@ namespace internal {\n bool IsTritonUnsupportedOpcode(HloOpcode opcode) {\n   switch (opcode) {\n     case HloOpcode::kDynamicReshape:\n+    case HloOpcode::kDynamicSlice:\n     case HloOpcode::kDynamicUpdateSlice:\n     case HloOpcode::kGather:\n     case HloOpcode::kRaggedDot:\n@@ -739,26 +743,6 @@ absl::Status EnsureTritonSupportsComputeCapability(\n   return absl::OkStatus();\n }\n \n-CodegenDecision IsTritonSupportedDynamicSlice(\n-    const HloDynamicSliceInstruction& instr) {\n-  for (const HloInstruction* index_operand : instr.index_operands()) {\n-    switch (index_operand->shape().element_type()) {\n-      case S8:\n-      case S16:\n-      case S32:\n-      case S64:\n-        break;  // supported\n-      default:\n-        return CodegenDecision::Forbid(\n-            \"Dynamic slice is only supported S8, S16, S32, or S64 offsets.\");\n-    }\n-  }\n-  if (instr.shape().element_type() == PrimitiveType::S4) {\n-    return CodegenDecision::Forbid(\"S4 is not supported.\");\n-  }\n-  return CodegenDecision::Allow();\n-}\n-\n CodegenDecision IsTritonSupportedInstruction(\n     const HloInstruction& instr, const se::GpuComputeCapability& gpu_version) {\n   CodegenDecision decision ="
        },
        {
            "sha": "de2c15c6c470118438748cd944d901c410c6caa0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.h",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h?ref=f605aa89b38e6e672cf3b3d93fc312966ac85397",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/shape.h\"\n@@ -67,11 +66,6 @@ CodegenDecision IsTritonSupportedComputation(\n // `kTritonGemmFusionKind`.\n bool IsTritonFusedComputation(const HloComputation& computation);\n \n-// TODO(b/393299275): this function is only exposed for\n-// triton_tiling_propagation.cc. If possible it should be removed.\n-CodegenDecision IsTritonSupportedDynamicSlice(\n-    const HloDynamicSliceInstruction& instr);\n-\n namespace internal {\n // TODO(b/363981282): Remove the function below once all ops are tested via\n // HLOs. This is exposed for testing purposes only and will be removed in the"
        },
        {
            "sha": "c5bc005541c389352b42f7b832c28093021a70a8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=f605aa89b38e6e672cf3b3d93fc312966ac85397",
            "patch": "@@ -156,15 +156,6 @@ std::vector<xla::PrimitiveType> AllOpSupportedTypes(HloOpcode opcode) {\n   return result;\n }\n \n-std::vector<xla::PrimitiveType> AllIntegralDataTypes() {\n-  std::vector<xla::PrimitiveType> result;\n-  absl::c_copy_if(AllXlaDataTypes(), std::back_inserter(result),\n-                  [&](PrimitiveType data_type) {\n-                    return primitive_util::IsIntegralType(data_type);\n-                  });\n-  return result;\n-}\n-\n std::vector<PrecisionConfig::Algorithm> AllPrecisionAlgorithms() {\n   std::vector<PrecisionConfig::Algorithm> algorithms;\n   const tsl::protobuf::EnumDescriptor* algorithm_descriptor =\n@@ -3099,54 +3090,6 @@ INSTANTIATE_TEST_SUITE_P(SortSuite, SortTest,\n                          AllTestCombinationsForOpcodes({HloOpcode::kSort}),\n                          TritonSupportTestTypeAndOpcodeAndDeviceToString);\n \n-using DynamicSliceTest = TritonSupportTestWithTypeAndDeviceParam;\n-\n-TEST_P(DynamicSliceTest, OperandTypes) {\n-  auto [data_type, cc] = GetParam();\n-  const std::string kHloTestTemplate = R\"(\n-ENTRY triton_computation {\n-  operand = $0[256,256] parameter(0)\n-  start_1 = s32[] parameter(1)\n-  start_2 = s32[] constant(0)\n-  ROOT dynamic_slice_op = $0[32,256] dynamic-slice(operand, start_1, start_2),\n-                          dynamic_slice_sizes={32,256}\n-})\";\n-  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti, ParseTemplateAndGetInstruction(\n-                                                    kHloTestTemplate, data_type,\n-                                                    HloOpcode::kDynamicSlice));\n-  RunSupportTest(std::move(ti), /*output_tile_sizes=*/{2, 4}, cc);\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(\n-    DynamicSliceSuite, DynamicSliceTest,\n-    ::testing::Combine(::testing::ValuesIn(AllXlaDataTypes()),\n-                       ::testing::ValuesIn(AllDevicesToTest())),\n-    TritonSupportTestTypeAndDeviceToString);\n-\n-using DynamicSliceOffsetTypesTest = TritonSupportTestWithTypeAndDeviceParam;\n-\n-TEST_P(DynamicSliceOffsetTypesTest, DynamicSlice2D) {\n-  auto [data_type, cc] = GetParam();\n-  const std::string kHloTestTemplate = R\"(\n-ENTRY triton_computation {\n-  operand = f32[256,256] parameter(0)\n-  start_1 = $0[] parameter(1)\n-  start_2 = $0[] parameter(2)\n-  ROOT dynamic_slice_op = f32[32,64] dynamic-slice(operand, start_1, start_2),\n-                          dynamic_slice_sizes={32,64}\n-})\";\n-  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti, ParseTemplateAndGetInstruction(\n-                                                    kHloTestTemplate, data_type,\n-                                                    HloOpcode::kDynamicSlice));\n-  RunSupportTest(std::move(ti), /*output_tile_sizes=*/{2, 4}, cc);\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(\n-    DynamicSliceOffsetTypesSuite, DynamicSliceOffsetTypesTest,\n-    ::testing::Combine(::testing::ValuesIn(AllIntegralDataTypes()),\n-                       ::testing::ValuesIn(AllDevicesToTest())),\n-    TritonSupportTestTypeAndDeviceToString);\n-\n using RecvOpsTest = TritonSupportTestWithTypeAndDeviceParam;\n \n TEST_P(RecvOpsTest, RecvAndRecvDone) {\n@@ -3534,6 +3477,7 @@ constexpr std::array kUnsupportedOps = {\n     // clang-format off\n     // go/keep-sorted start\n     HloOpcode::kDynamicReshape,\n+    HloOpcode::kDynamicSlice,\n     HloOpcode::kDynamicUpdateSlice,\n     HloOpcode::kGather,\n     HloOpcode::kRaggedDot,\n@@ -3593,7 +3537,6 @@ absl::flat_hash_set<HloOpcode> AllTestedOpcodes() {\n   ret.emplace(HloOpcode::kCustomCall);\n   ret.emplace(HloOpcode::kDomain);\n   ret.emplace(HloOpcode::kDot);\n-  ret.emplace(HloOpcode::kDynamicSlice);\n   ret.emplace(HloOpcode::kFft);\n   ret.emplace(HloOpcode::kFusion);\n   ret.emplace(HloOpcode::kGetDimensionSize);"
        },
        {
            "sha": "1b36d029adeb816b8d70d1d8a4b202eba6573ee3",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 37,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc?ref=f605aa89b38e6e672cf3b3d93fc312966ac85397",
            "patch": "@@ -44,7 +44,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n-#include \"xla/layout.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_padding_requirements.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n@@ -274,36 +273,6 @@ std::optional<DimOrdersAndReqs> GetUserDimOrdersAndCombinedReqsIfProfitable(\n       std::get<DotRequirements>(combined_reqs)};\n }\n \n-// Checks if a dynamic slice can be fused.\n-bool CanFuseDynamicSlice(const HloDynamicSliceInstruction& dynamic_slice,\n-                         const se::GpuComputeCapability& gpu_version) {\n-  if (CodegenDecision decision =\n-          IsTritonSupportedInstruction(dynamic_slice, gpu_version);\n-      !decision.CanFuse()) {\n-    VLOG(5) << \"Not fusing \" << dynamic_slice.ToString()\n-            << \" to the output due to the decision: \" << decision.Explain();\n-    return false;\n-  }\n-  // TODO(b/417172838): this check replicates the legacy emitter behavior.\n-  // New emitter might support all dimensions but we should verify that.\n-  const HloInstruction* input = dynamic_slice.operand(0);\n-  Layout in_layout = input->shape().layout();\n-  int64_t majormost_dim_id =\n-      in_layout.minor_to_major(in_layout.minor_to_major().size() - 1);\n-  for (int i = 0; i < input->shape().dimensions().size(); ++i) {\n-    if (i == majormost_dim_id) {\n-      continue;\n-    }\n-    if (input->shape().dimensions(i) != dynamic_slice.slice_sizes(i)) {\n-      VLOG(5) << \"Not fusing \" << dynamic_slice.ToString()\n-              << \" to the output due to the unsupported dynamic slice on \"\n-                 \"non-major-most dimension.\";\n-      return false;\n-    }\n-  }\n-  return true;\n-}\n-\n class FusionPlanBuilder {\n  public:\n   // Builds and returns the FusionPlan. Clears internal state.\n@@ -445,12 +414,10 @@ FusionPlanAndRequirements BuildFusionPlanTowardOperands(\n     // replaces unsupported F8E8M0FNU with u8. We should have a more principled\n     // way check if we will be able to emit the triton code for the fusion.\n     if (original_hlo.opcode() == HloOpcode::kDynamicSlice) {\n-      const HloDynamicSliceInstruction& dynamic_slice =\n-          *Cast<HloDynamicSliceInstruction>(&original_hlo);\n-      if (!CanFuseDynamicSlice(dynamic_slice, gpu_version)) {\n-        fusion_builder.SetShouldFuseNode(node_id, false);\n-        continue;\n-      }\n+      // TODO(b/417172838): support dynamic slice op.\n+      fusion_builder.SetShouldFuseNode(node_id, false);\n+      LOG(INFO) << \"Not fusing dynamic slice: \" << original_hlo.ToString();\n+      continue;\n     }\n \n     auto opt_result = GetOperandDimOrdersAndCombinedReqsIfProfitable("
        },
        {
            "sha": "c77d76d6954aed56941694885583da7f0e96886e",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=f605aa89b38e6e672cf3b3d93fc312966ac85397",
            "patch": "@@ -264,7 +264,8 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-TEST_F(GemmFusionTest, DynamicSliceIsFused) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest, DISABLED_DynamicSliceIsFused) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -288,7 +289,8 @@ ENTRY e {\n                                     m::Parameter(), m::Constant()))));\n }\n \n-TEST_F(GemmFusionTest, DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest, DISABLED_DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -319,7 +321,8 @@ ENTRY e {\n                             m::Parameter(), m::Parameter()))));\n }\n \n-TEST_F(GemmFusionTest, DoNotFuseDynamicSliceOfNonMajorFragments) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest, DISABLED_DoNotFuseDynamicSliceOfNonMajorFragments) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -338,7 +341,9 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-TEST_F(GemmFusionTest, CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest,\n+       DISABLED_CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {"
        },
        {
            "sha": "437a2269739cf7d04c34bf408b17b9ae9332266d",
            "filename": "third_party/xla/xla/service/gpu/triton_tiling_propagation.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f605aa89b38e6e672cf3b3d93fc312966ac85397/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_tiling_propagation.cc?ref=f605aa89b38e6e672cf3b3d93fc312966ac85397",
            "patch": "@@ -914,7 +914,7 @@ DimOrderMapOrError GetPropagatedDimOrders(const HloInstruction& hlo,\n                                                   properties);\n   } else if (hlo.opcode() == HloOpcode::kDynamicSlice &&\n              direction == TransformDirection::kOutputToInput) {\n-    if (CodegenDecision decision = IsTritonSupportedDynamicSlice(\n+    if (CodegenDecision decision = legacy_triton::IsTritonSupportedDynamicSlice(\n             *Cast<HloDynamicSliceInstruction>(&hlo));\n         !decision.CanFuse()) {\n       // CodegenDecision is actually the same type as FusionDecision."
        }
    ],
    "stats": {
        "total": 161,
        "additions": 24,
        "deletions": 137
    }
}