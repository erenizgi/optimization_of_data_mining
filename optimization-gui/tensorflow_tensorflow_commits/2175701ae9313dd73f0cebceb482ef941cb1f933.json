{
    "author": "sahas3",
    "message": "[tosa] : Use QuantizedType signed info for legalization. (#105376)",
    "sha": "2175701ae9313dd73f0cebceb482ef941cb1f933",
    "files": [
        {
            "sha": "7e4573aa5a09e449bd5b14d443443e04cbd507a3",
            "filename": "tensorflow/compiler/mlir/tosa/tests/tfl-to-tosa-pipeline.mlir",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftests%2Ftfl-to-tosa-pipeline.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftests%2Ftfl-to-tosa-pipeline.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftests%2Ftfl-to-tosa-pipeline.mlir?ref=2175701ae9313dd73f0cebceb482ef941cb1f933",
            "patch": "@@ -2930,6 +2930,21 @@ func.func @test_relu_qi8(%arg0: tensor<13x21x3x!quant.uniform<i8:f32, 0.01568594\n \n // -----\n \n+// CHECK-LABEL:   func.func @test_relu_qu16(\n+// CHECK-SAME:      %[[ARG0:.*]]: tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802>>) -> tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802>> {\n+// CHECK:           %[[VAL_0:.*]] = \"tosa.const\"() <{values = dense<1073741824> : tensor<1xi32>}> : () -> tensor<1xi32>\n+// CHECK:           %[[VAL_1:.*]] = \"tosa.const\"() <{values = dense<30> : tensor<1xi8>}> : () -> tensor<1xi8>\n+// CHECK:           %[[VAL_2:.*]] = \"tosa.const\"() <{values = dense<0> : tensor<1xui16>}> : () -> tensor<1xui16>\n+// CHECK:           %[[RESCALE_0:.*]] = tosa.rescale %[[ARG0]], %[[VAL_0]], %[[VAL_1]], %[[VAL_2]], %[[VAL_2]] {input_unsigned = true, output_unsigned = true, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true} : (tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802>>, tensor<1xi32>, tensor<1xi8>, tensor<1xui16>, tensor<1xui16>) -> tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802>>\n+// CHECK:           %[[CLAMP_0:.*]] = tosa.clamp %[[RESCALE_0]] {max_val = 65535 : ui16, min_val = 0 : ui16} : (tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802>>) -> tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802>>\n+// CHECK:           return %[[CLAMP_0]]\n+func.func @test_relu_qu16(%arg0:tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802:0>>) -> (tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802:0>>) {\n+    %0 = \"tfl.relu\"(%arg0) : (tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802:0>>) -> tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802:0>>\n+    return %0 : tensor<?x112x112x32x!quant.uniform<u16:f32, 0.023529412224888802:0>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: test_relu0To1_qi8\n // CHECK-DAG: %[[VAL_0:.*]]: tensor<13x21x3x!quant.uniform<i8:f32, 0.015686025843024254:-1>>\n // CHECK-DAG: %[[VAL_1:.*]] = \"tosa.const\"() <{values = dense<2147449478> : tensor<1xi32>}>\n@@ -3684,6 +3699,24 @@ func.func @test_conv2d_int8_input_variable_bias(%input: tensor<1x32x32x8x!quant.\n \n // -----\n \n+// CHECK-LABEL:   func.func @test_conv2d_qu16(\n+// CHECK-SAME:      %[[ARG0:.*]]: tensor<1x32x32x8x!quant.uniform<u16:f32, 1.000000e+00>>,\n+// CHECK-SAME:      %[[ARG1:.*]]: tensor<3x3x8x16x!quant.uniform<i8:f32, 1.000000e+00>>) -> tensor<1x32x32x3x!quant.uniform<u16:f32, 1.000000e+00>> {\n+// CHECK:           %[[VAL_0:.*]] = \"tosa.const\"() <{values = dense<14> : tensor<1xi8>}> : () -> tensor<1xi8>\n+// CHECK:           %[[VAL_1:.*]] = \"tosa.const\"() <{values = dense<16384> : tensor<1xi16>}> : () -> tensor<1xi16>\n+// CHECK:           %[[VAL_2:.*]] = \"tosa.const\"() <{values = dense<0> : tensor<1xi48>}> : () -> tensor<1xi48>\n+// CHECK:           %[[VAL_3:.*]] = \"tosa.const\"() <{values = dense<0> : tensor<1xui16>}> : () -> tensor<1xui16>\n+// CHECK:           %[[VAL_4:.*]] = \"tosa.const\"() <{values = dense<0> : tensor<1xi8>}> : () -> tensor<1xi8>\n+// CHECK:           %[[VAL_5:.*]] = tosa.conv2d %[[ARG0]], %[[ARG1]], %[[VAL_2]], %[[VAL_3]], %[[VAL_4]] {acc_type = i48, dilation = array<i64: 1, 1>, pad = array<i64: 1, 1, 3, 4>, stride = array<i64: 1, 1>} : (tensor<1x32x32x8x!quant.uniform<u16:f32, 1.000000e+00>>, tensor<3x3x8x16x!quant.uniform<i8:f32, 1.000000e+00>>, tensor<1xi48>, tensor<1xui16>, tensor<1xi8>) -> tensor<1x32x32x3xi48>\n+// CHECK:           %[[RESCALE_0:.*]] = tosa.rescale %[[VAL_5]], %[[VAL_1]], %[[VAL_0]], %[[VAL_2]], %[[VAL_3]] {input_unsigned = true, output_unsigned = true, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = false} : (tensor<1x32x32x3xi48>, tensor<1xi16>, tensor<1xi8>, tensor<1xi48>, tensor<1xui16>) -> tensor<1x32x32x3x!quant.uniform<u16:f32, 1.000000e+00>>\n+// CHECK:           return %[[RESCALE_0]]\n+func.func @test_conv2d_qu16(%input: tensor<1x32x32x8x!quant.uniform<u16:f32, 1.0>>, %filter: tensor<3x3x8x16x!quant.uniform<i8:f32, 1.0>>) -> tensor<1x32x32x3x!quant.uniform<u16:f32, 1.0>> {\n+  %bias = \"tfl.no_value\"() {value} : () -> none\n+  %0 = \"tfl.conv_2d\"(%input, %filter, %bias) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x32x32x8x!quant.uniform<u16:f32, 1.0>>, tensor<3x3x8x16x!quant.uniform<i8:f32, 1.0>>, none) -> tensor<1x32x32x3x!quant.uniform<u16:f32, 1.0>>\n+  return %0 : tensor<1x32x32x3x!quant.uniform<u16:f32, 1.0>>\n+}\n+// -----\n+\n // CHECK-LABEL: @test_squeeze\n func.func @test_squeeze(%arg0: tensor<2x1x3x1xf32>) -> tensor<2x3x1xf32> {\n   // CHECK: tosa.reshape"
        },
        {
            "sha": "7805fdd9742f11d61c067fd8d4199b4d67b97a3f",
            "filename": "tensorflow/compiler/mlir/tosa/tests/tfl-unequal-ranks.mlir",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftests%2Ftfl-unequal-ranks.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftests%2Ftfl-unequal-ranks.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftests%2Ftfl-unequal-ranks.mlir?ref=2175701ae9313dd73f0cebceb482ef941cb1f933",
            "patch": "@@ -13,6 +13,15 @@ func.func @test_add(%arg0: tensor<192x192x3xf32>, %arg1: tensor<16x192x192x3xf32\n \n // -----\n \n+// CHECK-LABEL: test_add_dynamic\n+func.func @test_add_dynamic(%arg0: tensor<?x?x?xf32>, %arg1: tensor<5xf32>) -> tensor<?x?x5xf32> {\n+    // CHECK: tosa.add\n+    %1 = tfl.add(%arg0, %arg1) {fused_activation_function = \"NONE\"} : (tensor<?x?x?xf32>, tensor<5xf32>) -> tensor<?x?x5xf32>\n+    func.return %1 : tensor<?x?x5xf32>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: test_add_qi8\n func.func @test_add_qi8(%arg0: tensor<13x21x1x!quant.uniform<i8:f32, 0.01568480022251606:-1>>, %arg1: tensor<1x13x21x3x!quant.uniform<i8:f32, 0.015686055645346642:-1>>) -> tensor<1x13x21x3x!quant.uniform<i8:f32, 0.031318482011556625:-1>> {\n   // CHECK: tosa.add"
        },
        {
            "sha": "c7e49c5703fcd7343ce13d9bf1bcd751876733ca",
            "filename": "tensorflow/compiler/mlir/tosa/transforms/legalize_tfl.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 22,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftransforms%2Flegalize_tfl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftransforms%2Flegalize_tfl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftransforms%2Flegalize_tfl.cc?ref=2175701ae9313dd73f0cebceb482ef941cb1f933",
            "patch": "@@ -31,25 +31,26 @@ limitations under the License.\n #include <unordered_set>\n \n #include \"llvm/ADT/ArrayRef.h\"\n-#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n-#include \"mlir/Dialect/Quant/IR/QuantTypes.h\"  // from @llvm-project\n-#include \"mlir/Dialect/Tosa/IR/TosaOps.h\"  // from @llvm-project\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"             // from @llvm-project\n+#include \"mlir/Dialect/Quant/IR/QuantTypes.h\"         // from @llvm-project\n+#include \"mlir/Dialect/Tosa/IR/TosaOps.h\"             // from @llvm-project\n #include \"mlir/Dialect/Tosa/Utils/ConversionUtils.h\"  // from @llvm-project\n-#include \"mlir/IR/Block.h\"  // from @llvm-project\n+#include \"mlir/Dialect/Tosa/Utils/QuantUtils.h\"\n+#include \"mlir/IR/Block.h\"                       // from @llvm-project\n #include \"mlir/IR/BuiltinAttributeInterfaces.h\"  // from @llvm-project\n-#include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n-#include \"mlir/IR/BuiltinTypeInterfaces.h\"  // from @llvm-project\n-#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n-#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n-#include \"mlir/IR/Matchers.h\"  // from @llvm-project\n-#include \"mlir/IR/PatternMatch.h\"  // from @llvm-project\n-#include \"mlir/IR/Region.h\"  // from @llvm-project\n-#include \"mlir/IR/TypeUtilities.h\"  // from @llvm-project\n-#include \"mlir/IR/Types.h\"  // from @llvm-project\n-#include \"mlir/IR/Value.h\"  // from @llvm-project\n-#include \"mlir/IR/ValueRange.h\"  // from @llvm-project\n-#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n-#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n+#include \"mlir/IR/BuiltinAttributes.h\"           // from @llvm-project\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"       // from @llvm-project\n+#include \"mlir/IR/BuiltinTypes.h\"                // from @llvm-project\n+#include \"mlir/IR/MLIRContext.h\"                 // from @llvm-project\n+#include \"mlir/IR/Matchers.h\"                    // from @llvm-project\n+#include \"mlir/IR/PatternMatch.h\"                // from @llvm-project\n+#include \"mlir/IR/Region.h\"                      // from @llvm-project\n+#include \"mlir/IR/TypeUtilities.h\"               // from @llvm-project\n+#include \"mlir/IR/Types.h\"                       // from @llvm-project\n+#include \"mlir/IR/Value.h\"                       // from @llvm-project\n+#include \"mlir/IR/ValueRange.h\"                  // from @llvm-project\n+#include \"mlir/Support/LLVM.h\"                   // from @llvm-project\n+#include \"mlir/Support/LogicalResult.h\"          // from @llvm-project\n #include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n #include \"tensorflow/compiler/mlir/lite/quantization/ir/QuantOps.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/utils/dynamic_shape_utils.h\"\n@@ -359,7 +360,8 @@ LogicalResult ConvertTFLReluOp::matchAndRewrite(\n   auto element_type = input_type.getElementType();\n   if (auto quant_type =\n           dyn_cast<mlir::quant::UniformQuantizedType>(element_type)) {\n-    element_type = quant_type.getStorageType();\n+    element_type =\n+        tosa::getStorageElementTypeFromQuantized(quant_type);\n   }\n \n   mlir::Attribute min_val, max_val;\n@@ -429,7 +431,7 @@ LogicalResult ConvertTFLRelu1Op::matchAndRewrite(\n   auto element_type = input_type.getElementType();\n   if (auto quant_type =\n           dyn_cast<mlir::quant::UniformQuantizedType>(element_type)) {\n-    element_type = quant_type.getStorageType();\n+    element_type = tosa::getStorageElementTypeFromQuantized(quant_type);\n   }\n \n   mlir::Attribute min_val, max_val;\n@@ -496,7 +498,7 @@ LogicalResult ConvertTFLRelu0To1Op::matchAndRewrite(\n   auto element_type = input_type.getElementType();\n   if (auto quant_type =\n           dyn_cast<mlir::quant::UniformQuantizedType>(element_type)) {\n-    element_type = quant_type.getStorageType();\n+    element_type = tosa::getStorageElementTypeFromQuantized(quant_type);\n   }\n \n   mlir::Attribute min_val, max_val;\n@@ -563,7 +565,7 @@ LogicalResult ConvertTFLRelu6Op::matchAndRewrite(\n   auto element_type = input_type.getElementType();\n   if (auto quant_type =\n           dyn_cast<mlir::quant::UniformQuantizedType>(element_type)) {\n-    element_type = quant_type.getStorageType();\n+    element_type = tosa::getStorageElementTypeFromQuantized(quant_type);\n   }\n \n   mlir::Attribute min_val, max_val;\n@@ -1405,7 +1407,8 @@ RankedTensorType getTypeForSlice(RankedTensorType type, int64_t slice_dim,\n         per_channel_qtype.getZeroPoints().begin() + offset,\n         per_channel_qtype.getZeroPoints().begin() + offset + slice_size);\n     auto output_per_channel_qtype = quant::UniformQuantizedPerAxisType::get(\n-        per_channel_qtype.getFlags(), per_channel_qtype.getStorageType(),\n+        per_channel_qtype.getFlags(),\n+        tosa::getStorageElementTypeFromQuantized(per_channel_qtype),\n         per_channel_qtype.getExpressedType(), output_scale_arr, output_zp_arr,\n         per_channel_qtype.getQuantizedDimension(),\n         per_channel_qtype.getStorageTypeMin(),"
        },
        {
            "sha": "b1bde08cf929ebe273f5bfc1af4c156d95454178",
            "filename": "tensorflow/compiler/mlir/tosa/transforms/legalize_utils.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 11,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftransforms%2Flegalize_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2175701ae9313dd73f0cebceb482ef941cb1f933/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftransforms%2Flegalize_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftosa%2Ftransforms%2Flegalize_utils.cc?ref=2175701ae9313dd73f0cebceb482ef941cb1f933",
            "patch": "@@ -54,11 +54,11 @@ mlir::TypeAttr getConvAccTypeAttr(PatternRewriter& rewriter,\n   // in case of quantized types: get base element types\n   if (auto qtype =\n           llvm::dyn_cast<mlir::quant::UniformQuantizedType>(input_etype))\n-    input_etype = qtype.getStorageType();\n+    input_etype = tosa::getStorageElementTypeFromQuantized(qtype);\n \n   if (auto qtype =\n           llvm::dyn_cast<mlir::quant::UniformQuantizedType>(output_etype))\n-    output_etype = qtype.getStorageType();\n+    output_etype = tosa::getStorageElementTypeFromQuantized(qtype);\n \n   // special cases: input_etype and output_etype are both f16 or bf16: use\n   // acc_type=f32\n@@ -355,8 +355,19 @@ Value buildRescale(PatternRewriter& rewriter, Operation* op,\n                    int32_t scale_multiplier, int32_t scale_shift,\n                    int64_t input_zp, int64_t output_zp, tosa::RoundingMode rounding_mode,\n                    bool scale32) {\n-  bool input_unsigned = input_val.getType().isUnsignedInteger();\n-  bool output_unsigned = output_type.isUnsignedInteger();\n+  bool input_unsigned, output_unsigned;\n+  if (auto qtype = dyn_cast<mlir::quant::QuantizedType>(\n+          cast<ShapedType>(input_val.getType()).getElementType())) {\n+    input_unsigned = !qtype.isSigned();\n+  } else {\n+    input_unsigned = input_val.getType().isUnsignedInteger();\n+  }\n+  if (auto qtype =\n+          dyn_cast<mlir::quant::QuantizedType>(output_type.getElementType())) {\n+    output_unsigned = !qtype.isSigned();\n+  } else {\n+    output_unsigned = output_type.isUnsignedInteger();\n+  }\n   auto loc = op->getLoc();\n   Value multiplier_val =\n       buildRescaleMultiplier(scale32, rewriter, loc, {scale_multiplier});\n@@ -486,8 +497,8 @@ Value buildRescaleOpConvOutput(PatternRewriter& rewriter, Operation* op,\n   const auto rounding_mode_attr = tosa::RoundingModeAttr::get(\n       rewriter.getContext(), rounding_mode);\n \n-  bool input_unsigned = input_qtype.isUnsignedInteger();\n-  bool output_unsigned = output_qtype.isUnsignedInteger();\n+  bool input_unsigned = !input_qtype.isSigned();\n+  bool output_unsigned = !output_qtype.isSigned();\n \n   auto loc = op->getLoc();\n   const Value empty_output_val = rewriter.create<tensor::EmptyOp>(\n@@ -664,7 +675,7 @@ Value getTosaConstHardSwish8bitTable(PatternRewriter& rewriter, Operation* op,\n                                 rewriter.getF32Type(), 1.0f, 0, -128, 127);\n   auto const_type = tensorflow::GetTypeFromTFTensorShape({256}, element_qtype);\n   auto storage_type = tensorflow::GetTypeFromTFTensorShape(\n-      {256}, element_qtype.getStorageType());\n+      {256}, getStorageElementTypeFromQuantized(element_qtype));\n   auto const_attr = DenseElementsAttr::get(storage_type, llvm::ArrayRef(table));\n \n   auto const_op =\n@@ -718,7 +729,8 @@ Value getTosaConstRsqrt8bitTable(PatternRewriter& rewriter, Operation* op,\n                                 rewriter.getF32Type(), 1.0f, 0, -128, 127);\n   auto const_type = tensorflow::GetTypeFromTFTensorShape({256}, element_qtype);\n   auto storage_type = tensorflow::GetTypeFromTFTensorShape(\n-      {256}, element_qtype.getStorageType());\n+      {256},\n+      tosa::getStorageElementTypeFromQuantized(element_qtype));\n   auto const_attr = DenseElementsAttr::get(storage_type, llvm::ArrayRef(table));\n \n   auto const_op =\n@@ -756,7 +768,7 @@ Value getTosaConst8bitTable(PatternRewriter& rewriter, Operation* op,\n                                 rewriter.getF32Type(), 1.0f, 0, -128, 127);\n   auto const_type = tensorflow::GetTypeFromTFTensorShape({256}, element_qtype);\n   auto storage_type = tensorflow::GetTypeFromTFTensorShape(\n-      {256}, element_qtype.getStorageType());\n+      {256}, tosa::getStorageElementTypeFromQuantized(element_qtype));\n   auto const_attr = DenseElementsAttr::get(storage_type, llvm::ArrayRef(table));\n \n   auto const_op =\n@@ -880,7 +892,7 @@ void getTosaConst32bitSoftmaxExpTable(PatternRewriter& rewriter, Operation* op,\n                                 rewriter.getF32Type(), 1.0f, 0, -32768, 32767);\n   auto const_type = tensorflow::GetTypeFromTFTensorShape({513}, element_qtype);\n   auto storage_type = tensorflow::GetTypeFromTFTensorShape(\n-      {513}, element_qtype.getStorageType());\n+      {513}, tosa::getStorageElementTypeFromQuantized(element_qtype));\n \n   auto first_const_attr =\n       DenseElementsAttr::get(storage_type, llvm::ArrayRef(first_table));\n@@ -1409,7 +1421,7 @@ Value reshapeScalarTo1D(PatternRewriter& rewriter, Location loc, Value value) {\n     auto element_qtype = dyn_cast<quant::QuantizedType>(element_type);\n     if (element_qtype) {\n       storage_type = tensorflow::GetTypeFromTFTensorShape(\n-          {1}, element_qtype.getStorageType());\n+          {1}, tosa::getStorageElementTypeFromQuantized(element_qtype));\n     }\n \n     DenseElementsAttr const_attr;"
        }
    ],
    "stats": {
        "total": 123,
        "additions": 90,
        "deletions": 33
    }
}