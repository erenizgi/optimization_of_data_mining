{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 834251906",
    "sha": "a10b2e7caf46f32e5ffe15109fee43f7da93db45",
    "files": [
        {
            "sha": "1443cffc4c6e6a95cb64d53e63b7959a66fcf26f",
            "filename": "tensorflow/core/summary/loader.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Floader.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Floader.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Floader.cc?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -33,9 +33,9 @@ namespace tensorflow {\n namespace {\n \n template <typename T>\n-string AddCommas(T n) {\n+std::string AddCommas(T n) {\n   static_assert(std::is_integral<T>::value, \"is_integral\");\n-  string s = strings::StrCat(n);\n+  std::string s = strings::StrCat(n);\n   if (s.size() > 3) {\n     int extra = s.size() / 3 - (s.size() % 3 == 0 ? 1 : 0);\n     s.append(extra, 'X');\n@@ -52,19 +52,19 @@ string AddCommas(T n) {\n }\n \n int main(int argc, char* argv[]) {\n-  string path;\n-  string events;\n-  string experiment_name;\n-  string run_name;\n-  string user_name;\n+  std::string path;\n+  std::string events;\n+  std::string experiment_name;\n+  std::string run_name;\n+  std::string user_name;\n   std::vector<Flag> flag_list = {\n       Flag(\"db\", &path, \"Path of SQLite DB file\"),\n       Flag(\"events\", &events, \"TensorFlow record proto event log file\"),\n       Flag(\"experiment_name\", &experiment_name, \"The DB experiment_name value\"),\n       Flag(\"run_name\", &run_name, \"The DB run_name value\"),\n       Flag(\"user_name\", &user_name, \"The DB user_name value\"),\n   };\n-  string usage = Flags::Usage(argv[0], flag_list);\n+  std::string usage = Flags::Usage(argv[0], flag_list);\n   bool parse_result = Flags::Parse(&argc, argv, flag_list);\n   if (!parse_result || path.empty()) {\n     std::cerr << \"The loader tool imports tf.Event record files, created by\\n\"\n@@ -99,9 +99,9 @@ int main(int argc, char* argv[]) {\n   TF_CHECK_OK(env->NewRandomAccessFile(events, &file));\n   io::RecordReader reader(file.get());\n \n-  uint64 start = env->NowMicros();\n-  uint64 records = 0;\n-  uint64 offset = 0;\n+  uint64_t start = env->NowMicros();\n+  uint64_t records = 0;\n+  uint64_t offset = 0;\n   tstring record;\n   while (true) {\n     std::unique_ptr<Event> event = std::unique_ptr<Event>(new Event);\n@@ -116,9 +116,10 @@ int main(int argc, char* argv[]) {\n     TF_CHECK_OK(db_writer->WriteEvent(std::move(event)));\n     ++records;\n   }\n-  uint64 elapsed = env->NowMicros() - start;\n-  uint64 bps = (elapsed == 0 ? offset : static_cast<uint64>(\n-                                            offset / (elapsed / 1000000.0)));\n+  uint64_t elapsed = env->NowMicros() - start;\n+  uint64_t bps =\n+      (elapsed == 0 ? offset\n+                    : static_cast<uint64_t>(offset / (elapsed / 1000000.0)));\n   LOG(INFO) << \"Loaded \" << AddCommas(offset) << \" bytes with \"\n             << AddCommas(records) << \" records at \" << AddCommas(bps) << \" bps\";\n   return 0;"
        },
        {
            "sha": "d39fd74812491f25101026af1c16ea91ad770a35",
            "filename": "tensorflow/core/summary/schema.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fschema.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fschema.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fschema.h?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -21,7 +21,7 @@ limitations under the License.\n \n namespace tensorflow {\n \n-constexpr uint32 kTensorboardSqliteApplicationId = 0xfeedabee;\n+constexpr uint32_t kTensorboardSqliteApplicationId = 0xfeedabee;\n \n /// \\brief Creates TensorBoard SQLite tables and indexes.\n ///"
        },
        {
            "sha": "a5e3695e42010302e36f0699ebde6fe432c29510",
            "filename": "tensorflow/core/summary/summary_converter.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.cc?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -72,16 +72,16 @@ absl::Status TensorValueAt(Tensor t, int64_t i, T* out) {\n #undef COMPLEX_CASE\n }\n \n-typedef Eigen::Tensor<uint8, 2, Eigen::RowMajor> Uint8Image;\n+typedef Eigen::Tensor<uint8_t, 2, Eigen::RowMajor> Uint8Image;\n \n // Add the sequence of images specified by ith_image to the summary.\n //\n // Factoring this loop out into a helper function lets ith_image behave\n // differently in the float and uint8 cases: the float case needs a temporary\n // buffer which can be shared across calls to ith_image, but the uint8 case\n // does not.\n-absl::Status AddImages(const string& tag, int max_images, int batch_size, int w,\n-                       int h, int depth,\n+absl::Status AddImages(const std::string& tag, int max_images, int batch_size,\n+                       int w, int h, int depth,\n                        const std::function<Uint8Image(int)>& ith_image,\n                        Summary* s) {\n   const int N = std::min<int>(max_images, batch_size);\n@@ -118,7 +118,7 @@ absl::Status AddImages(const string& tag, int max_images, int batch_size, int w,\n template <class T>\n void NormalizeFloatImage(int hw, int depth,\n                          typename TTypes<T>::ConstMatrix values,\n-                         typename TTypes<uint8>::ConstVec bad_color,\n+                         typename TTypes<uint8_t>::ConstVec bad_color,\n                          Uint8Image* image) {\n   if (!image->size()) return;  // Nothing to do for empty images\n \n@@ -178,8 +178,8 @@ void NormalizeFloatImage(int hw, int depth,\n       }\n     }\n     if (finite) {\n-      image->chip<0>(i) =\n-          (values.template chip<0>(i) * scale + offset).template cast<uint8>();\n+      image->chip<0>(i) = (values.template chip<0>(i) * scale + offset)\n+                              .template cast<uint8_t>();\n     } else {\n       image->chip<0>(i) = bad_color;\n     }\n@@ -189,16 +189,16 @@ void NormalizeFloatImage(int hw, int depth,\n template <class T>\n absl::Status NormalizeAndAddImages(const Tensor& tensor, int max_images, int h,\n                                    int w, int hw, int depth, int batch_size,\n-                                   const string& base_tag,\n+                                   const std::string& base_tag,\n                                    Tensor bad_color_tensor, Summary* s) {\n   // For float and half images, nans and infs are replaced with bad_color.\n   if (bad_color_tensor.dim_size(0) < depth) {\n     return errors::InvalidArgument(\n         \"expected depth <= bad_color.size, got depth = \", depth,\n         \", bad_color.size = \", bad_color_tensor.dim_size(0));\n   }\n-  auto bad_color_full = bad_color_tensor.vec<uint8>();\n-  typename TTypes<uint8>::ConstVec bad_color(bad_color_full.data(), depth);\n+  auto bad_color_full = bad_color_tensor.vec<uint8_t>();\n+  typename TTypes<uint8_t>::ConstVec bad_color(bad_color_full.data(), depth);\n \n   // Float images must be scaled and translated.\n   Uint8Image image(hw, depth);\n@@ -214,7 +214,7 @@ absl::Status NormalizeAndAddImages(const Tensor& tensor, int max_images, int h,\n \n }  // namespace\n \n-absl::Status AddTensorAsScalarToSummary(const Tensor& t, const string& tag,\n+absl::Status AddTensorAsScalarToSummary(const Tensor& t, const std::string& tag,\n                                         Summary* s) {\n   Summary::Value* v = s->add_value();\n   v->set_tag(tag);\n@@ -224,8 +224,8 @@ absl::Status AddTensorAsScalarToSummary(const Tensor& t, const string& tag,\n   return absl::OkStatus();\n }\n \n-absl::Status AddTensorAsHistogramToSummary(const Tensor& t, const string& tag,\n-                                           Summary* s) {\n+absl::Status AddTensorAsHistogramToSummary(const Tensor& t,\n+                                           const std::string& tag, Summary* s) {\n   Summary::Value* v = s->add_value();\n   v->set_tag(tag);\n   histogram::Histogram histo;\n@@ -244,9 +244,9 @@ absl::Status AddTensorAsHistogramToSummary(const Tensor& t, const string& tag,\n   return absl::OkStatus();\n }\n \n-absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_images, const Tensor& bad_color,\n-                                       Summary* s) {\n+absl::Status AddTensorAsImageToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_images,\n+                                       const Tensor& bad_color, Summary* s) {\n   if (!(tensor.dims() == 4 &&\n         (tensor.dim_size(3) == 1 || tensor.dim_size(3) == 3 ||\n          tensor.dim_size(3) == 4))) {\n@@ -269,8 +269,8 @@ absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n   if (tensor.dtype() == DT_UINT8) {\n     // For uint8 input, no normalization is necessary\n     auto ith_image = [&tensor, batch_size, hw, depth](int i) {\n-      auto values = tensor.shaped<uint8, 3>({batch_size, hw, depth});\n-      return typename TTypes<uint8>::ConstMatrix(\n+      auto values = tensor.shaped<uint8_t, 3>({batch_size, hw, depth});\n+      return typename TTypes<uint8_t>::ConstMatrix(\n           &values(i, 0, 0), Eigen::DSizes<Eigen::DenseIndex, 2>(hw, depth));\n     };\n     TF_RETURN_IF_ERROR(\n@@ -293,9 +293,9 @@ absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n   return absl::OkStatus();\n }\n \n-absl::Status AddTensorAsAudioToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_outputs, float sample_rate,\n-                                       Summary* s) {\n+absl::Status AddTensorAsAudioToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_outputs,\n+                                       float sample_rate, Summary* s) {\n   if (sample_rate <= 0.0f) {\n     return errors::InvalidArgument(\"sample_rate must be > 0\");\n   }"
        },
        {
            "sha": "650958341e9d37098af163681a9ebc021200b19d",
            "filename": "tensorflow/core/summary/summary_converter.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_converter.h?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -23,16 +23,16 @@ limitations under the License.\n namespace tensorflow {\n \n // TODO(jart): Delete these methods in favor of new Python implementation.\n-absl::Status AddTensorAsScalarToSummary(const Tensor& t, const string& tag,\n+absl::Status AddTensorAsScalarToSummary(const Tensor& t, const std::string& tag,\n                                         Summary* s);\n-absl::Status AddTensorAsHistogramToSummary(const Tensor& t, const string& tag,\n-                                           Summary* s);\n-absl::Status AddTensorAsImageToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_images, const Tensor& bad_color,\n-                                       Summary* s);\n-absl::Status AddTensorAsAudioToSummary(const Tensor& tensor, const string& tag,\n-                                       int max_outputs, float sample_rate,\n-                                       Summary* s);\n+absl::Status AddTensorAsHistogramToSummary(const Tensor& t,\n+                                           const std::string& tag, Summary* s);\n+absl::Status AddTensorAsImageToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_images,\n+                                       const Tensor& bad_color, Summary* s);\n+absl::Status AddTensorAsAudioToSummary(const Tensor& tensor,\n+                                       const std::string& tag, int max_outputs,\n+                                       float sample_rate, Summary* s);\n \n }  // namespace tensorflow\n "
        },
        {
            "sha": "849fc9a6954c7e96ec77d5e7b8e3e91e104f6bc9",
            "filename": "tensorflow/core/summary/summary_db_writer.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 59,
            "changes": 121,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.cc?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -67,13 +67,13 @@ namespace tensorflow {\n namespace {\n \n // https://www.sqlite.org/fileformat.html#record_format\n-const uint64 kIdTiers[] = {\n+const uint64_t kIdTiers[] = {\n     0x7fffffULL,        // 23-bit (3 bytes on disk)\n     0x7fffffffULL,      // 31-bit (4 bytes on disk)\n     0x7fffffffffffULL,  // 47-bit (5 bytes on disk)\n                         // remaining bits for future use\n };\n-const int kMaxIdTier = sizeof(kIdTiers) / sizeof(uint64) - 1;\n+const int kMaxIdTier = sizeof(kIdTiers) / sizeof(uint64_t) - 1;\n const int kIdCollisionDelayMicros = 10;\n const int kMaxIdCollisions = 21;  // sum(2**i*10Âµs for i in range(21))~=21s\n const int64_t kAbsent = 0LL;\n@@ -92,16 +92,16 @@ const int64_t kPreallocateRows = 1000;\n // hundreds of megs but doesn't need the transaction to maintain its\n // invariants. This ensures the WAL read penalty is small and might\n // allow writers in other processes a chance to schedule.\n-const uint64 kFlushBytes = 1024 * 1024;\n+const uint64_t kFlushBytes = 1024 * 1024;\n \n-double DoubleTime(uint64 micros) {\n+double DoubleTime(uint64_t micros) {\n   // TODO(@jart): Follow precise definitions for time laid out in schema.\n   // TODO(@jart): Use monotonic clock from gRPC codebase.\n   return static_cast<double>(micros) / 1.0e6;\n }\n \n-string StringifyShape(const TensorShape& shape) {\n-  string result;\n+std::string StringifyShape(const TensorShape& shape) {\n+  std::string result;\n   bool first = true;\n   for (const auto& dim : shape) {\n     if (first) {\n@@ -233,7 +233,7 @@ class IdAllocator {\n class GraphWriter {\n  public:\n   static absl::Status Save(Sqlite* db, SqliteTransaction* txn, IdAllocator* ids,\n-                           GraphDef* graph, uint64 now, int64_t run_id,\n+                           GraphDef* graph, uint64_t now, int64_t run_id,\n                            int64_t* graph_id)\n       SQLITE_EXCLUSIVE_TRANSACTIONS_REQUIRED(*db) {\n     TF_RETURN_IF_ERROR(ids->CreateNewId(graph_id));\n@@ -246,7 +246,7 @@ class GraphWriter {\n   }\n \n  private:\n-  GraphWriter(Sqlite* db, SqliteTransaction* txn, GraphDef* graph, uint64 now,\n+  GraphWriter(Sqlite* db, SqliteTransaction* txn, GraphDef* graph, uint64_t now,\n               int64_t graph_id)\n       : db_(db), txn_(txn), graph_(graph), now_(now), graph_id_(graph_id) {}\n \n@@ -338,7 +338,7 @@ class GraphWriter {\n       node->clear_op();\n       node->clear_device();\n       node->clear_input();\n-      string node_def;\n+      std::string node_def;\n       if (node->SerializeToString(&node_def)) {\n         insert.BindBlobUnsafe(6, node_def);\n       }\n@@ -364,7 +364,7 @@ class GraphWriter {\n     insert.BindInt(2, graph_id_);\n     insert.BindDouble(3, DoubleTime(now_));\n     graph_->clear_node();\n-    string graph_def;\n+    std::string graph_def;\n     if (graph_->SerializeToString(&graph_def)) {\n       insert.BindBlobUnsafe(4, graph_def);\n     }\n@@ -382,11 +382,11 @@ class GraphWriter {\n \n   Sqlite* const db_;\n   SqliteTransaction* const txn_;\n-  uint64 unflushed_bytes_ = 0;\n+  uint64_t unflushed_bytes_ = 0;\n   GraphDef* const graph_;\n-  const uint64 now_;\n+  const uint64_t now_;\n   const int64_t graph_id_;\n-  std::vector<string> name_copies_;\n+  std::vector<std::string> name_copies_;\n   std::unordered_map<absl::string_view, int64_t, StringPieceHasher>\n       name_to_node_id_;\n \n@@ -403,25 +403,25 @@ class GraphWriter {\n /// This class is thread safe.\n class RunMetadata {\n  public:\n-  RunMetadata(IdAllocator* ids, const string& experiment_name,\n-              const string& run_name, const string& user_name)\n+  RunMetadata(IdAllocator* ids, const std::string& experiment_name,\n+              const std::string& run_name, const std::string& user_name)\n       : ids_{ids},\n         experiment_name_{experiment_name},\n         run_name_{run_name},\n         user_name_{user_name} {\n     DCHECK(ids_ != nullptr);\n   }\n \n-  const string& experiment_name() { return experiment_name_; }\n-  const string& run_name() { return run_name_; }\n-  const string& user_name() { return user_name_; }\n+  const std::string& experiment_name() { return experiment_name_; }\n+  const std::string& run_name() { return run_name_; }\n+  const std::string& user_name() { return user_name_; }\n \n   int64_t run_id() TF_LOCKS_EXCLUDED(mu_) {\n     mutex_lock lock(mu_);\n     return run_id_;\n   }\n \n-  absl::Status SetGraph(Sqlite* db, uint64 now, double computed_time,\n+  absl::Status SetGraph(Sqlite* db, uint64_t now, double computed_time,\n                         std::unique_ptr<GraphDef> g)\n       SQLITE_TRANSACTIONS_EXCLUDED(*db) TF_LOCKS_EXCLUDED(mu_) {\n     int64_t run_id;\n@@ -437,8 +437,8 @@ class RunMetadata {\n     return txn.Commit();\n   }\n \n-  absl::Status GetTagId(Sqlite* db, uint64 now, double computed_time,\n-                        const string& tag_name, int64_t* tag_id,\n+  absl::Status GetTagId(Sqlite* db, uint64_t now, double computed_time,\n+                        const std::string& tag_name, int64_t* tag_id,\n                         const SummaryMetadata& metadata)\n       TF_LOCKS_EXCLUDED(mu_) {\n     mutex_lock lock(mu_);\n@@ -484,7 +484,7 @@ class RunMetadata {\n   }\n \n  private:\n-  absl::Status InitializeUser(Sqlite* db, uint64 now)\n+  absl::Status InitializeUser(Sqlite* db, uint64_t now)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (user_id_ != kAbsent || user_name_.empty()) return absl::OkStatus();\n     const char* get_sql = R\"sql(\n@@ -516,7 +516,7 @@ class RunMetadata {\n     return absl::OkStatus();\n   }\n \n-  absl::Status InitializeExperiment(Sqlite* db, uint64 now,\n+  absl::Status InitializeExperiment(Sqlite* db, uint64_t now,\n                                     double computed_time)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (experiment_name_.empty()) return absl::OkStatus();\n@@ -584,7 +584,7 @@ class RunMetadata {\n     return absl::OkStatus();\n   }\n \n-  absl::Status InitializeRun(Sqlite* db, uint64 now, double computed_time)\n+  absl::Status InitializeRun(Sqlite* db, uint64_t now, double computed_time)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (run_name_.empty()) return absl::OkStatus();\n     TF_RETURN_IF_ERROR(InitializeExperiment(db, now, computed_time));\n@@ -630,15 +630,15 @@ class RunMetadata {\n \n   mutex mu_;\n   IdAllocator* const ids_;\n-  const string experiment_name_;\n-  const string run_name_;\n-  const string user_name_;\n+  const std::string experiment_name_;\n+  const std::string run_name_;\n+  const std::string user_name_;\n   int64_t experiment_id_ TF_GUARDED_BY(mu_) = kAbsent;\n   int64_t run_id_ TF_GUARDED_BY(mu_) = kAbsent;\n   int64_t user_id_ TF_GUARDED_BY(mu_) = kAbsent;\n   double experiment_started_time_ TF_GUARDED_BY(mu_) = 0.0;\n   double run_started_time_ TF_GUARDED_BY(mu_) = 0.0;\n-  std::unordered_map<string, int64_t> tag_ids_ TF_GUARDED_BY(mu_);\n+  std::unordered_map<std::string, int64_t> tag_ids_ TF_GUARDED_BY(mu_);\n \n   RunMetadata(const RunMetadata&) = delete;\n   void operator=(const RunMetadata&) = delete;\n@@ -654,7 +654,7 @@ class SeriesWriter {\n     DCHECK(series_ > 0);\n   }\n \n-  absl::Status Append(Sqlite* db, int64_t step, uint64 now,\n+  absl::Status Append(Sqlite* db, int64_t step, uint64_t now,\n                       double computed_time, const Tensor& t)\n       SQLITE_TRANSACTIONS_EXCLUDED(*db) TF_LOCKS_EXCLUDED(mu_) {\n     mutex_lock lock(mu_);\n@@ -837,9 +837,9 @@ class SeriesWriter {\n   mutex mu_;\n   const int64_t series_;\n   RunMetadata* const meta_;\n-  uint64 count_ TF_GUARDED_BY(mu_) = 0;\n+  uint64_t count_ TF_GUARDED_BY(mu_) = 0;\n   std::deque<int64_t> rowids_ TF_GUARDED_BY(mu_);\n-  uint64 unflushed_bytes_ TF_GUARDED_BY(mu_) = 0;\n+  uint64_t unflushed_bytes_ TF_GUARDED_BY(mu_) = 0;\n \n   SeriesWriter(const SeriesWriter&) = delete;\n   void operator=(const SeriesWriter&) = delete;\n@@ -856,7 +856,7 @@ class RunWriter {\n  public:\n   explicit RunWriter(RunMetadata* meta) : meta_{meta} {}\n \n-  absl::Status Append(Sqlite* db, int64_t tag_id, int64_t step, uint64 now,\n+  absl::Status Append(Sqlite* db, int64_t tag_id, int64_t step, uint64_t now,\n                       double computed_time, const Tensor& t)\n       SQLITE_TRANSACTIONS_EXCLUDED(*db) TF_LOCKS_EXCLUDED(mu_) {\n     SeriesWriter* writer = GetSeriesWriter(tag_id);\n@@ -903,8 +903,8 @@ class RunWriter {\n /// This class is thread safe.\n class SummaryDbWriter : public SummaryWriterInterface {\n  public:\n-  SummaryDbWriter(Env* env, Sqlite* db, const string& experiment_name,\n-                  const string& run_name, const string& user_name)\n+  SummaryDbWriter(Env* env, Sqlite* db, const std::string& experiment_name,\n+                  const std::string& run_name, const std::string& user_name)\n       : SummaryWriterInterface(),\n         env_{env},\n         db_{db},\n@@ -941,8 +941,9 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n   absl::Status Flush() override { return absl::OkStatus(); }\n \n-  absl::Status WriteTensor(int64_t global_step, Tensor t, const string& tag,\n-                           const string& serialized_metadata) override {\n+  absl::Status WriteTensor(int64_t global_step, Tensor t,\n+                           const std::string& tag,\n+                           const std::string& serialized_metadata) override {\n     TF_RETURN_IF_ERROR(CheckSupportedType(t));\n     SummaryMetadata metadata;\n     if (!metadata.ParseFromString(serialized_metadata)) {\n@@ -952,7 +953,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteScalar(int64_t global_step, Tensor t,\n-                           const string& tag) override {\n+                           const std::string& tag) override {\n     TF_RETURN_IF_ERROR(CheckSupportedType(t));\n     SummaryMetadata metadata;\n     PatchPluginName(&metadata, kScalarPluginName);\n@@ -961,7 +962,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n   absl::Status WriteGraph(int64_t global_step,\n                           std::unique_ptr<GraphDef> g) override {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     return meta_.SetGraph(db_, now, DoubleTime(now), std::move(g));\n   }\n \n@@ -970,8 +971,8 @@ class SummaryDbWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteHistogram(int64_t global_step, Tensor t,\n-                              const string& tag) override {\n-    uint64 now = env_->NowMicros();\n+                              const std::string& tag) override {\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(DoubleTime(now));\n@@ -980,9 +981,9 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return MigrateEvent(std::move(e));\n   }\n \n-  absl::Status WriteImage(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteImage(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_images, Tensor bad_color) override {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(DoubleTime(now));\n@@ -991,9 +992,9 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return MigrateEvent(std::move(e));\n   }\n \n-  absl::Status WriteAudio(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteAudio(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_outputs, float sample_rate) override {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(DoubleTime(now));\n@@ -1002,12 +1003,12 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return MigrateEvent(std::move(e));\n   }\n \n-  string DebugString() const override { return \"SummaryDbWriter\"; }\n+  std::string DebugString() const override { return \"SummaryDbWriter\"; }\n \n  private:\n-  absl::Status Write(int64_t step, const Tensor& t, const string& tag,\n+  absl::Status Write(int64_t step, const Tensor& t, const std::string& tag,\n                      const SummaryMetadata& metadata) {\n-    uint64 now = env_->NowMicros();\n+    uint64_t now = env_->NowMicros();\n     double computed_time = DoubleTime(now);\n     int64_t tag_id;\n     TF_RETURN_IF_ERROR(\n@@ -1022,7 +1023,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n   absl::Status MigrateEvent(std::unique_ptr<Event> e) {\n     switch (e->what_case()) {\n       case Event::WhatCase::kSummary: {\n-        uint64 now = env_->NowMicros();\n+        uint64_t now = env_->NowMicros();\n         auto summaries = e->mutable_summary();\n         for (int i = 0; i < summaries->value_size(); ++i) {\n           Summary::Value* value = summaries->mutable_value(i);\n@@ -1046,16 +1047,16 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return absl::OkStatus();\n   }\n \n-  absl::Status MigrateGraph(const Event* e, const string& graph_def) {\n-    uint64 now = env_->NowMicros();\n+  absl::Status MigrateGraph(const Event* e, const std::string& graph_def) {\n+    uint64_t now = env_->NowMicros();\n     std::unique_ptr<GraphDef> graph{new GraphDef};\n     if (!ParseProtoUnlimited(graph.get(), graph_def)) {\n       return errors::InvalidArgument(\"bad proto\");\n     }\n     return meta_.SetGraph(db_, now, e->wall_time(), std::move(graph));\n   }\n \n-  absl::Status MigrateSummary(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateSummary(const Event* e, Summary::Value* s, uint64_t now) {\n     switch (s->value_case()) {\n       case Summary::Value::ValueCase::kTensor:\n         TF_RETURN_WITH_CONTEXT_IF_ERROR(MigrateTensor(e, s, now), \"tensor\");\n@@ -1078,7 +1079,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return absl::OkStatus();\n   }\n \n-  absl::Status MigrateTensor(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateTensor(const Event* e, Summary::Value* s, uint64_t now) {\n     Tensor t;\n     if (!t.FromProto(s->tensor())) return errors::InvalidArgument(\"bad proto\");\n     TF_RETURN_IF_ERROR(CheckSupportedType(t));\n@@ -1090,7 +1091,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n   // TODO(jart): Refactor Summary -> Tensor logic into separate file.\n \n-  absl::Status MigrateScalar(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateScalar(const Event* e, Summary::Value* s, uint64_t now) {\n     // See tensorboard/plugins/scalar/summary.py and data_compat.py\n     Tensor t{DT_FLOAT, {}};\n     t.scalar<float>()() = s->simple_value();\n@@ -1101,7 +1102,8 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return run_.Append(db_, tag_id, e->step(), now, e->wall_time(), t);\n   }\n \n-  absl::Status MigrateHistogram(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateHistogram(const Event* e, Summary::Value* s,\n+                                uint64_t now) {\n     const HistogramProto& histo = s->histo();\n     int k = histo.bucket_size();\n     if (k != histo.bucket_limit_size()) {\n@@ -1132,7 +1134,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return run_.Append(db_, tag_id, e->step(), now, e->wall_time(), t);\n   }\n \n-  absl::Status MigrateImage(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateImage(const Event* e, Summary::Value* s, uint64_t now) {\n     // See tensorboard/plugins/image/summary.py and data_compat.py\n     Tensor t{DT_STRING, {3}};\n     auto img = s->mutable_image();\n@@ -1146,7 +1148,7 @@ class SummaryDbWriter : public SummaryWriterInterface {\n     return run_.Append(db_, tag_id, e->step(), now, e->wall_time(), t);\n   }\n \n-  absl::Status MigrateAudio(const Event* e, Summary::Value* s, uint64 now) {\n+  absl::Status MigrateAudio(const Event* e, Summary::Value* s, uint64_t now) {\n     // See tensorboard/plugins/audio/summary.py and data_compat.py\n     Tensor t{DT_STRING, {1, 2}};\n     auto wav = s->mutable_audio();\n@@ -1168,9 +1170,10 @@ class SummaryDbWriter : public SummaryWriterInterface {\n \n }  // namespace\n \n-absl::Status CreateSummaryDbWriter(Sqlite* db, const string& experiment_name,\n-                                   const string& run_name,\n-                                   const string& user_name, Env* env,\n+absl::Status CreateSummaryDbWriter(Sqlite* db,\n+                                   const std::string& experiment_name,\n+                                   const std::string& run_name,\n+                                   const std::string& user_name, Env* env,\n                                    SummaryWriterInterface** result) {\n   *result = new SummaryDbWriter(env, db, experiment_name, run_name, user_name);\n   return absl::OkStatus();"
        },
        {
            "sha": "05900fe8ce0ca6747ee7f5975d7683ec578b37cd",
            "filename": "tensorflow/core/summary/summary_db_writer.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer.h?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -34,9 +34,10 @@ namespace tensorflow {\n /// the future if support for other DBs is added to core.\n ///\n /// The result holds a new reference to db.\n-absl::Status CreateSummaryDbWriter(Sqlite* db, const string& experiment_name,\n-                                   const string& run_name,\n-                                   const string& user_name, Env* env,\n+absl::Status CreateSummaryDbWriter(Sqlite* db,\n+                                   const std::string& experiment_name,\n+                                   const std::string& run_name,\n+                                   const std::string& user_name, Env* env,\n                                    SummaryWriterInterface** result);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "8c25da1823f0576d47db58976c5eea03c817a695",
            "filename": "tensorflow/core/summary/summary_db_writer_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_db_writer_test.cc?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -47,12 +47,12 @@ Tensor MakeScalarInt64(int64_t x) {\n class FakeClockEnv : public EnvWrapper {\n  public:\n   FakeClockEnv() : EnvWrapper(Env::Default()), current_millis_(0) {}\n-  void AdvanceByMillis(const uint64 millis) { current_millis_ += millis; }\n-  uint64 NowMicros() const override { return current_millis_ * 1000; }\n-  uint64 NowSeconds() const override { return current_millis_ * 1000; }\n+  void AdvanceByMillis(const uint64_t millis) { current_millis_ += millis; }\n+  uint64_t NowMicros() const override { return current_millis_ * 1000; }\n+  uint64_t NowSeconds() const override { return current_millis_ * 1000; }\n \n  private:\n-  uint64 current_millis_;\n+  uint64_t current_millis_;\n };\n \n class SummaryDbWriterTest : public ::testing::Test {\n@@ -71,7 +71,7 @@ class SummaryDbWriterTest : public ::testing::Test {\n     db_ = nullptr;\n   }\n \n-  int64_t QueryInt(const string& sql) {\n+  int64_t QueryInt(const std::string& sql) {\n     SqliteStatement stmt = db_->PrepareOrDie(sql);\n     bool is_done;\n     absl::Status s = stmt.Step(&is_done);\n@@ -82,7 +82,7 @@ class SummaryDbWriterTest : public ::testing::Test {\n     return stmt.ColumnInt(0);\n   }\n \n-  double QueryDouble(const string& sql) {\n+  double QueryDouble(const std::string& sql) {\n     SqliteStatement stmt = db_->PrepareOrDie(sql);\n     bool is_done;\n     absl::Status s = stmt.Step(&is_done);\n@@ -93,7 +93,7 @@ class SummaryDbWriterTest : public ::testing::Test {\n     return stmt.ColumnDouble(0);\n   }\n \n-  string QueryString(const string& sql) {\n+  std::string QueryString(const std::string& sql) {\n     SqliteStatement stmt = db_->PrepareOrDie(sql);\n     bool is_done;\n     absl::Status s = stmt.Step(&is_done);\n@@ -142,7 +142,7 @@ TEST_F(SummaryDbWriterTest, WriteHistogram_VerifyTensorValues) {\n \n   // TODO(nickfelt): implement QueryTensor() to encapsulate this\n   // Verify the data\n-  string result = QueryString(\"SELECT data FROM Tensors\");\n+  std::string result = QueryString(\"SELECT data FROM Tensors\");\n   const double* val = reinterpret_cast<const double*>(result.data());\n   double histarray[] = {std::numeric_limits<double>::min(),\n                         -30.5,"
        },
        {
            "sha": "dfb1bba4aecbe59c6055e18a9487c83956efaf80",
            "filename": "tensorflow/core/summary/summary_file_writer.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.cc?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -47,7 +47,8 @@ class SummaryFileWriter : public SummaryWriterInterface {\n         flush_millis_(flush_millis),\n         env_(env) {}\n \n-  absl::Status Initialize(const string& logdir, const string& filename_suffix) {\n+  absl::Status Initialize(const std::string& logdir,\n+                          const std::string& filename_suffix) {\n     const absl::Status is_dir = env_->IsDirectory(logdir);\n     if (!is_dir.ok()) {\n       if (is_dir.code() != tensorflow::error::NOT_FOUND) {\n@@ -60,8 +61,8 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     int32_t pid = env_->GetProcessId();\n     static std::atomic<int64_t> file_id_counter(0);\n     // Precede filename_suffix with \".\" if it doesn't already start with one.\n-    string sep = absl::StartsWith(filename_suffix, \".\") ? \"\" : \".\";\n-    const string uniquified_filename_suffix = absl::StrCat(\n+    std::string sep = absl::StartsWith(filename_suffix, \".\") ? \"\" : \".\";\n+    const std::string uniquified_filename_suffix = absl::StrCat(\n         \".\", pid, \".\", file_id_counter.fetch_add(1), sep, filename_suffix);\n     mutex_lock ml(mu_);\n     events_writer_ =\n@@ -86,8 +87,9 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     (void)Flush();  // Ignore errors.\n   }\n \n-  absl::Status WriteTensor(int64_t global_step, Tensor t, const string& tag,\n-                           const string& serialized_metadata) override {\n+  absl::Status WriteTensor(int64_t global_step, Tensor t,\n+                           const std::string& tag,\n+                           const std::string& serialized_metadata) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(GetWallTime());\n@@ -110,7 +112,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteScalar(int64_t global_step, Tensor t,\n-                           const string& tag) override {\n+                           const std::string& tag) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(GetWallTime());\n@@ -120,7 +122,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n   }\n \n   absl::Status WriteHistogram(int64_t global_step, Tensor t,\n-                              const string& tag) override {\n+                              const std::string& tag) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n     e->set_wall_time(GetWallTime());\n@@ -129,7 +131,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     return WriteEvent(std::move(e));\n   }\n \n-  absl::Status WriteImage(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteImage(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_images, Tensor bad_color) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n@@ -139,7 +141,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     return WriteEvent(std::move(e));\n   }\n \n-  absl::Status WriteAudio(int64_t global_step, Tensor t, const string& tag,\n+  absl::Status WriteAudio(int64_t global_step, Tensor t, const std::string& tag,\n                           int max_outputs, float sample_rate) override {\n     std::unique_ptr<Event> e{new Event};\n     e->set_step(global_step);\n@@ -168,7 +170,7 @@ class SummaryFileWriter : public SummaryWriterInterface {\n     return absl::OkStatus();\n   }\n \n-  string DebugString() const override { return \"SummaryFileWriter\"; }\n+  std::string DebugString() const override { return \"SummaryFileWriter\"; }\n \n  private:\n   double GetWallTime() {\n@@ -189,21 +191,22 @@ class SummaryFileWriter : public SummaryWriterInterface {\n   bool is_initialized_;\n   const int max_queue_;\n   const int flush_millis_;\n-  uint64 last_flush_;\n+  uint64_t last_flush_;\n   Env* env_;\n   mutex mu_;\n   std::vector<std::unique_ptr<Event>> queue_ TF_GUARDED_BY(mu_);\n   // A pointer to allow deferred construction.\n   std::unique_ptr<EventsWriter> events_writer_ TF_GUARDED_BY(mu_);\n-  std::vector<std::pair<string, SummaryMetadata>> registered_summaries_\n+  std::vector<std::pair<std::string, SummaryMetadata>> registered_summaries_\n       TF_GUARDED_BY(mu_);\n };\n \n }  // namespace\n \n absl::Status CreateSummaryFileWriter(int max_queue, int flush_millis,\n-                                     const string& logdir,\n-                                     const string& filename_suffix, Env* env,\n+                                     const std::string& logdir,\n+                                     const std::string& filename_suffix,\n+                                     Env* env,\n                                      SummaryWriterInterface** result) {\n   SummaryFileWriter* w = new SummaryFileWriter(max_queue, flush_millis, env);\n   const absl::Status s = w->Initialize(logdir, filename_suffix);"
        },
        {
            "sha": "a3ba40bf8a4db38631f9d08e3d58d224f9dd9754",
            "filename": "tensorflow/core/summary/summary_file_writer.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer.h?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -35,9 +35,9 @@ namespace tensorflow {\n /// returned status is ok. The Env object must not be destroyed until\n /// after the returned writer.\n absl::Status CreateSummaryFileWriter(int max_queue, int flush_millis,\n-                                     const string& logdir,\n-                                     const string& filename_suffix, Env* env,\n-                                     SummaryWriterInterface** result);\n+                                     const std::string& logdir,\n+                                     const std::string& filename_suffix,\n+                                     Env* env, SummaryWriterInterface** result);\n \n }  // namespace tensorflow\n "
        },
        {
            "sha": "94ca029774f40d52a9b0c518e9113318423841d1",
            "filename": "tensorflow/core/summary/summary_file_writer_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fsummary_file_writer_test.cc?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -43,21 +43,21 @@ namespace {\n class FakeClockEnv : public EnvWrapper {\n  public:\n   FakeClockEnv() : EnvWrapper(Env::Default()), current_millis_(0) {}\n-  void AdvanceByMillis(const uint64 millis) { current_millis_ += millis; }\n-  uint64 NowMicros() const override { return current_millis_ * 1000; }\n-  uint64 NowSeconds() const override { return current_millis_ * 1000; }\n+  void AdvanceByMillis(const uint64_t millis) { current_millis_ += millis; }\n+  uint64_t NowMicros() const override { return current_millis_ * 1000; }\n+  uint64_t NowSeconds() const override { return current_millis_ * 1000; }\n \n  private:\n-  uint64 current_millis_;\n+  uint64_t current_millis_;\n };\n \n class SummaryFileWriterTest : public ::testing::Test {\n  protected:\n   absl::Status SummaryTestHelper(\n-      const string& test_name,\n+      const std::string& test_name,\n       const std::function<absl::Status(SummaryWriterInterface*)>& writer_fn,\n       const std::function<void(const Event&)>& test_fn) {\n-    static std::set<string>* tests = new std::set<string>();\n+    static std::set<std::string>* tests = new std::set<std::string>();\n     CHECK(tests->insert(test_name).second) << \": \" << test_name;\n \n     SummaryWriterInterface* writer;\n@@ -68,10 +68,10 @@ class SummaryFileWriterTest : public ::testing::Test {\n     TF_CHECK_OK(writer_fn(writer));\n     TF_CHECK_OK(writer->Flush());\n \n-    std::vector<string> files;\n+    std::vector<std::string> files;\n     TF_CHECK_OK(env_.GetChildren(testing::TmpDir(), &files));\n     bool found = false;\n-    for (const string& f : files) {\n+    for (const std::string& f : files) {\n       if (absl::StrContains(f, test_name)) {\n         if (found) {\n           return errors::Unknown(\"Found more than one file for \", test_name);\n@@ -82,7 +82,7 @@ class SummaryFileWriterTest : public ::testing::Test {\n                                              &read_file));\n         io::RecordReader reader(read_file.get(), io::RecordReaderOptions());\n         tstring record;\n-        uint64 offset = 0;\n+        uint64_t offset = 0;\n         TF_CHECK_OK(\n             reader.ReadRecord(&offset,\n                               &record));  // The first event is irrelevant\n@@ -179,7 +179,7 @@ namespace {\n template <typename T>\n static absl::Status CreateImage(SummaryWriterInterface* writer) {\n   Tensor bad_color(DT_UINT8, TensorShape({1}));\n-  bad_color.scalar<uint8>()() = 0;\n+  bad_color.scalar<uint8_t>()() = 0;\n   Tensor one(DataTypeToEnum<T>::v(), TensorShape({1, 1, 1, 1}));\n   one.scalar<T>()() = T(1);\n   TF_RETURN_IF_ERROR(writer->WriteImage(2, one, \"name\", 1, bad_color));\n@@ -202,7 +202,7 @@ static void CheckImage(const Event& e) {\n \n TEST_F(SummaryFileWriterTest, WriteImageUInt8) {\n   TF_CHECK_OK(\n-      SummaryTestHelper(\"image_test_uint8\", CreateImage<uint8>, CheckImage));\n+      SummaryTestHelper(\"image_test_uint8\", CreateImage<uint8_t>, CheckImage));\n }\n \n TEST_F(SummaryFileWriterTest, WriteImageFloat) {\n@@ -272,19 +272,19 @@ TEST_F(SummaryFileWriterTest, WallTime) {\n \n TEST_F(SummaryFileWriterTest, AvoidFilenameCollision) {\n   // Keep unique with all other test names in this file.\n-  string test_name = \"avoid_filename_collision_test\";\n+  std::string test_name = \"avoid_filename_collision_test\";\n   int num_files = 10;\n   for (int i = 0; i < num_files; i++) {\n     SummaryWriterInterface* writer;\n     TF_CHECK_OK(CreateSummaryFileWriter(1, 1, testing::TmpDir(), test_name,\n                                         &env_, &writer));\n     core::ScopedUnref deleter(writer);\n   }\n-  std::vector<string> files;\n+  std::vector<std::string> files;\n   TF_CHECK_OK(env_.GetChildren(testing::TmpDir(), &files));\n   // Filter `files` down to just those generated in this test.\n   files.erase(std::remove_if(files.begin(), files.end(),\n-                             [test_name](string f) {\n+                             [test_name](std::string f) {\n                                return !absl::StrContains(f, test_name);\n                              }),\n               files.end());"
        },
        {
            "sha": "29c459cca89f13b0e67e6b69dcde74cfb6fc2f69",
            "filename": "tensorflow/core/summary/vacuum.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fvacuum.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a10b2e7caf46f32e5ffe15109fee43f7da93db45/tensorflow%2Fcore%2Fsummary%2Fvacuum.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fsummary%2Fvacuum.cc?ref=a10b2e7caf46f32e5ffe15109fee43f7da93db45",
            "patch": "@@ -110,7 +110,7 @@ void Vacuum(const char* path) {\n }\n \n int main(int argc, char* argv[]) {\n-  string usage = Flags::Usage(argv[0], {});\n+  std::string usage = Flags::Usage(argv[0], {});\n   bool parse_result = Flags::Parse(&argc, argv, {});\n   if (!parse_result) {\n     std::cerr << \"The vacuum tool rebuilds SQLite database files created by\\n\""
        }
    ],
    "stats": {
        "total": 300,
        "additions": 154,
        "deletions": 146
    }
}