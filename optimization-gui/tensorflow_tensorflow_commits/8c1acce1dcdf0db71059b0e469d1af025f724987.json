{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Refactor RaggedAllToAll kernel to process multiple updates per block.\n\nThe new kernel implementation uses a group of N blocks to process N updates, so each block sends 1/Nth of each update. This helps to distribute the load in case when updates are unbalanced.\n\nThe choice of N requires careful consideration as larger number of updates per process result in the increase of indexing computation that can be more expensive than data transfer itself.\n\nPiperOrigin-RevId: 803456036",
    "sha": "8c1acce1dcdf0db71059b0e469d1af025f724987",
    "files": [
        {
            "sha": "a303194c252a81a28cf46289c13f5361bb931b0c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c1acce1dcdf0db71059b0e469d1af025f724987/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c1acce1dcdf0db71059b0e469d1af025f724987/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc?ref=8c1acce1dcdf0db71059b0e469d1af025f724987",
            "patch": "@@ -89,10 +89,6 @@ absl::Status RunRaggedAllToAllKernel(\n \n   se::StreamExecutor* executor = stream->parent();\n   static constexpr size_t kThreads = 128;\n-  static constexpr size_t kMaxBlocksPerUpdate = 1024;\n-\n-  // blockIdx.x is the index of the update.\n-  int64_t num_blocks_x = num_updates_per_output * num_outputs;\n \n   int64_t num_vectorized_row_elements = num_row_elements;\n   int64_t vector_size_bytes = xla::primitive_util::BitWidth(element_type) / 8;\n@@ -102,18 +98,25 @@ absl::Status RunRaggedAllToAllKernel(\n     vector_size_bytes *= 2;\n   }\n \n-  // blockIdx.y and threadIdx.x are used to iterate over the elements of the\n-  // update. Since the size of each update is not known at compile time, the\n-  // kernel assumes the worst case of `num_input_rows * num_row_elements`\n-  // elements per update and uses a loop up to `send_size * num_row_elements` to\n-  // terminate early.\n-  size_t num_blocks_y =\n-      std::min(CeilOfRatio<size_t>(num_input_rows * num_vectorized_row_elements,\n-                                   kThreads),\n-               kMaxBlocksPerUpdate);\n+  int64_t num_updates_per_block = 1;\n+  int64_t num_block_clusters = num_updates_per_output;\n+\n+  // Decide how many updates should each block process. In the kernel, N blocks\n+  // process N updates. This is done to reduce imbalance in data transfer per\n+  // block if updates happen to be unevenly distributed. The numbers were\n+  // chosen empirically in Sep 2025 and can change in the future.\n+  const int64_t max_num_updates_per_block =\n+      std::min<int64_t>(CeilOfRatio<int64_t>(num_input_rows, 16), 64);\n+\n+  while (num_updates_per_block < max_num_updates_per_block &&\n+         num_block_clusters % 2 == 0) {\n+    num_block_clusters /= 2;\n+    num_updates_per_block *= 2;\n+  }\n \n   se::ThreadDim thread_dims(kThreads, 1, 1);\n-  se::BlockDim block_dims(num_blocks_x, num_blocks_y, 1);\n+  se::BlockDim block_dims(num_outputs, num_block_clusters,\n+                          num_updates_per_block);\n \n   std::array<void*, stream_executor::gpu::kMaxNumRaggedAllToAllOutputPtrs>\n       output_ptrs;"
        },
        {
            "sha": "4717bda5385d05fcfeb39c13e35aaa2cd090531a",
            "filename": "third_party/xla/xla/stream_executor/gpu/ragged_all_to_all_kernel_lib.cu.h",
            "status": "modified",
            "additions": 27,
            "deletions": 15,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c1acce1dcdf0db71059b0e469d1af025f724987/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c1acce1dcdf0db71059b0e469d1af025f724987/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel_lib.cu.h?ref=8c1acce1dcdf0db71059b0e469d1af025f724987",
            "patch": "@@ -53,8 +53,11 @@ struct alignas(kSize) Vec {\n //  update_slice = input[input_offset: input_offset + send_size]\n //  output_ptrs[j][output_offset : output_offset + send_size] = update_slice\n //\n+// `num_updates_per_block` blocks cooperate to process `num_updates_per_block`\n+// updates. This is done to reduce imbalance in data transfer per block.\n+//\n // Launch parameters:\n-//  - Block grid: (N*num_updates_per_rank, num_blocks_per_update, 1)\n+//  - Block grid: (num_ranks, num_block_clusters, num_updates_per_block)\n //  - Thread grid: (num_threads_per_update, 1, 1)\n template <int64_t kVectorSize>\n __global__ void __launch_bounds__(128) RaggedAllToAllKernelImpl(\n@@ -66,25 +69,34 @@ __global__ void __launch_bounds__(128) RaggedAllToAllKernelImpl(\n     int64_t num_updates_per_replica, int64_t num_row_elements) {\n   using T = Vec<kVectorSize>;\n \n-  int64_t update_idx = blockIdx.x;\n-  int64_t output_idx = update_idx / num_updates_per_replica;\n-\n   const T* typed_input_ptr = static_cast<const T* __restrict__>(input_ptr);\n-  T* output_ptr = static_cast<T* __restrict__>(output_ptrs[output_idx]);\n+  T* output_ptr = static_cast<T* __restrict__>(output_ptrs[blockIdx.x]);\n+\n+  int64_t num_updates_to_process = gridDim.z;\n+\n+  for (int64_t i = 0; i < num_updates_to_process; ++i) {\n+    const int64_t update_idx =\n+        blockIdx.x * num_updates_per_replica + blockIdx.y * gridDim.z + i;\n+\n+    const int64_t input_offset = input_offsets_ptr[update_idx];\n+    const int64_t send_size = send_sizes_ptr[update_idx];\n+    const int64_t output_offset = output_offsets_ptr[update_idx];\n \n-  int64_t input_offset = input_offsets_ptr[update_idx];\n-  int64_t send_size = send_sizes_ptr[update_idx];\n-  int64_t output_offset = output_offsets_ptr[update_idx];\n+    const int64_t input_offset_start = input_offset * num_row_elements;\n+    const int64_t output_offset_start = output_offset * num_row_elements;\n \n-  int64_t input_offset_start = input_offset * num_row_elements;\n-  int64_t output_offset_start = output_offset * num_row_elements;\n+    const int64_t update_size = send_size * num_row_elements;\n \n-  int64_t update_size = send_size * num_row_elements;\n+    int64_t offset_update_batch_idx = blockIdx.z + i;\n+    if (offset_update_batch_idx >= num_updates_to_process) {\n+      offset_update_batch_idx -= num_updates_to_process;\n+    }\n \n-  for (int64_t i = threadIdx.x + blockIdx.y * blockDim.x; i < update_size;\n-       i += blockDim.x * gridDim.y) {\n-    output_ptr[output_offset_start + i] =\n-        typed_input_ptr[input_offset_start + i];\n+    for (int64_t j = threadIdx.x + offset_update_batch_idx * blockDim.x;\n+         j < update_size; j += num_updates_to_process * blockDim.x) {\n+      output_ptr[output_offset_start + j] =\n+          typed_input_ptr[input_offset_start + j];\n+    }\n   }\n }\n }  // namespace stream_executor::gpu"
        }
    ],
    "stats": {
        "total": 73,
        "additions": 44,
        "deletions": 29
    }
}