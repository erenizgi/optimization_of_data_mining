{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 829719938",
    "sha": "be54b30fb35353acbcd2b148f6541cf8c8608e4d",
    "files": [
        {
            "sha": "0f14acbac3b841ec684359c3819e343f7232eaf5",
            "filename": "tensorflow/core/ops/array_grad.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Farray_grad.cc?ref=be54b30fb35353acbcd2b148f6541cf8c8608e4d",
            "patch": "@@ -149,13 +149,13 @@ absl::Status ConcatGradHelper(const AttrSlice& attrs, FunctionDef* g,\n   DataType T;\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"T\", &T));\n \n-  std::vector<string> shape_i;\n-  std::vector<string> offset_i;\n-  std::vector<string> dx_i;\n+  std::vector<std::string> shape_i;\n+  std::vector<std::string> offset_i;\n+  std::vector<std::string> dx_i;\n   for (int i = 0; i < N; ++i) {\n-    shape_i.push_back(strings::StrCat(\"shapes:output:\", i));\n-    offset_i.push_back(strings::StrCat(\"offset:offset:\", i));\n-    dx_i.push_back(strings::StrCat(\"dx_\", i, \":output:0\"));\n+    shape_i.push_back(absl::StrCat(\"shapes:output:\", i));\n+    offset_i.push_back(absl::StrCat(\"offset:offset:\", i));\n+    dx_i.push_back(absl::StrCat(\"dx_\", i, \":output:0\"));\n   }\n \n   // ConcatGrad(dim, x, dy):\n@@ -175,7 +175,7 @@ absl::Status ConcatGradHelper(const AttrSlice& attrs, FunctionDef* g,\n   // For each dx[i], we take a slice of dy. The offset and size of the\n   // slice is given by offset[i] and shape[i].\n   for (int i = 0; i < N; ++i) {\n-    nodes.push_back({{strings::StrCat(\"dx_\", i)},\n+    nodes.push_back({{absl::StrCat(\"dx_\", i)},\n                      \"Slice\",\n                      {\"dy\", offset_i[i], shape_i[i]},\n                      {{\"T\", \"$T\"}, {\"Index\", DT_INT32}}});\n@@ -270,10 +270,10 @@ REGISTER_OP_GRADIENT(\"SplitV\", SplitVGrad);\n absl::Status ArrayToListGrad(const AttrSlice& attrs, FunctionDef* g) {\n   int N;\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"N\", &N));\n-  std::vector<string> dys;\n+  std::vector<std::string> dys;\n   dys.reserve(N);\n   for (int i = 0; i < N; ++i) {\n-    dys.push_back(strings::StrCat(\"dy:\", i));\n+    dys.push_back(absl::StrCat(\"dy:\", i));\n   }\n   // clang-format off\n   *g = FDH::Define("
        },
        {
            "sha": "b68d4e7da8cbddd3a5645206b60f3e6539c92aaf",
            "filename": "tensorflow/core/ops/array_grad_test.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 40,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_grad_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_grad_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Farray_grad_test.cc?ref=be54b30fb35353acbcd2b148f6541cf8c8608e4d",
            "patch": "@@ -172,7 +172,7 @@ TEST(ArrayGradTest, ConcatGrad) {\n \n   // Test Concat.\n   auto dx = ConcatGrad(1, x0, x1, dy);\n-  test::ExpectTensorEqual<int32>(dx[0], test::AsScalar(0));\n+  test::ExpectTensorEqual<int32_t>(dx[0], test::AsScalar(0));\n   test::ExpectClose(\n       dx[1],\n       test::AsTensor<float>({0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,\n@@ -185,7 +185,7 @@ TEST(ArrayGradTest, ConcatGrad) {\n \n   // Test ConcatV2 with positive concat axis.\n   dx = ConcatGradV2(1, x0, x1, dy);\n-  test::ExpectTensorEqual<int32>(dx[dx.size() - 1], test::AsScalar(0));\n+  test::ExpectTensorEqual<int32_t>(dx[dx.size() - 1], test::AsScalar(0));\n   test::ExpectClose(\n       dx[0],\n       test::AsTensor<float>({0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,\n@@ -198,7 +198,7 @@ TEST(ArrayGradTest, ConcatGrad) {\n \n   // Test ConcatV2 with negative concat axis.\n   dx = ConcatGradV2(-2, x0, x1, dy);\n-  test::ExpectTensorEqual<int32>(dx[dx.size() - 1], test::AsScalar(0));\n+  test::ExpectTensorEqual<int32_t>(dx[dx.size() - 1], test::AsScalar(0));\n   test::ExpectClose(\n       dx[0],\n       test::AsTensor<float>({0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,\n@@ -289,7 +289,7 @@ TEST(ArrayGradTest, SplitGrad) {\n   // SplitGrad\n   {\n     auto dx = SplitGrad(1, x, dy0, dy1);\n-    test::ExpectTensorEqual<int32>(dx[0], expected_d_dim);\n+    test::ExpectTensorEqual<int32_t>(dx[0], expected_d_dim);\n     test::ExpectClose(dx[1], expected_dx);\n   }\n   // SplitVGrad\n@@ -300,7 +300,7 @@ TEST(ArrayGradTest, SplitGrad) {\n     auto dx = SplitVGrad(x, size_splits, 1, dy0, dy1);\n     test::ExpectClose(dx[0], expected_dx);\n     test::ExpectTensorEqual<int64_t>(dx[1], expected_d_size_splits);\n-    test::ExpectTensorEqual<int32>(dx[2], expected_d_dim);\n+    test::ExpectTensorEqual<int32_t>(dx[2], expected_d_dim);\n   }\n }\n \n@@ -329,7 +329,7 @@ std::vector<Tensor> ReshapeGrad(const Tensor& x, const Tensor& s,\n TEST(ArrayGradTest, ReshapeGrad) {\n   Tensor x(DT_FLOAT, {2, 4, 5});\n   x.flat<float>().setZero();\n-  auto s = test::AsTensor<int32>({8, 5});\n+  auto s = test::AsTensor<int32_t>({8, 5});\n   Tensor dy(DT_FLOAT, {8, 5});\n   test::FillIota<float>(&dy, 73);\n   auto dx = ReshapeGrad(x, s, dy);\n@@ -340,7 +340,7 @@ TEST(ArrayGradTest, ReshapeGrad) {\n                   93.,  94.,  95.,  96.,  97.,  98.,  99.,  100., 101., 102.,\n                   103., 104., 105., 106., 107., 108., 109., 110., 111., 112.},\n                  {2, 4, 5}));\n-  test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0}));\n+  test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0, 0}));\n }\n \n std::vector<Tensor> ExpandDimsGrad(const Tensor& x, const Tensor& s,\n@@ -368,7 +368,7 @@ std::vector<Tensor> ExpandDimsGrad(const Tensor& x, const Tensor& s,\n TEST(ArrayGradTest, ExpandDimsGrad) {\n   Tensor x(DT_FLOAT, {2, 4, 5});\n   x.flat<float>().setZero();\n-  auto s = test::AsTensor<int32>({1});\n+  auto s = test::AsTensor<int32_t>({1});\n   Tensor dy(DT_FLOAT, {2, 1, 4, 5});\n   test::FillIota<float>(&dy, 73);\n   auto dx = ExpandDimsGrad(x, s, dy);\n@@ -379,7 +379,7 @@ TEST(ArrayGradTest, ExpandDimsGrad) {\n                   93.,  94.,  95.,  96.,  97.,  98.,  99.,  100., 101., 102.,\n                   103., 104., 105., 106., 107., 108., 109., 110., 111., 112.},\n                  {2, 4, 5}));\n-  test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0}));\n+  test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0}));\n }\n \n std::vector<Tensor> SqueezeGrad(const Tensor& x, const Tensor& dy) {\n@@ -436,7 +436,7 @@ std::vector<Tensor> TransposeGrad(const Tensor& x, const Tensor& p,\n TEST(ArrayGradTest, TransposeGrad) {\n   Tensor x(DT_FLOAT, {2, 4, 5});\n   x.flat<float>().setZero();\n-  auto p = test::AsTensor<int32>({2, 0, 1});\n+  auto p = test::AsTensor<int32_t>({2, 0, 1});\n   Tensor dy(DT_FLOAT, {5, 2, 4});\n   test::FillIota<float>(&dy, 0);\n   auto dx = TransposeGrad(x, p, dy);\n@@ -446,7 +446,7 @@ TEST(ArrayGradTest, TransposeGrad) {\n                                 4., 12., 20., 28., 36., 5., 13., 21., 29., 37.,\n                                 6., 14., 22., 30., 38., 7., 15., 23., 31., 39.},\n                                {2, 4, 5}));\n-  test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0, 0}));\n+  test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0, 0, 0}));\n }\n \n std::vector<Tensor> ReverseGrad(const Tensor& x, const Tensor& dims,\n@@ -510,13 +510,13 @@ std::vector<Tensor> ReverseV2Grad(const Tensor& x, const Tensor& axis,\n TEST(ArrayGradTest, ReverseV2Grad) {\n   Tensor x(DT_FLOAT, {2, 3});\n   x.flat<float>().setZero();\n-  auto axis = test::AsTensor<int32>({1});\n+  auto axis = test::AsTensor<int32_t>({1});\n   Tensor dy(DT_FLOAT, {2, 3});\n   test::FillIota<float>(&dy, 1);\n   auto dx = ReverseV2Grad(x, axis, dy);\n   test::ExpectTensorEqual<float>(\n       dx[0], test::AsTensor<float>({3., 2., 1., 6., 5., 4.}, {2, 3}));\n-  test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0}));\n+  test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0}));\n }\n \n std::vector<Tensor> SliceGrad(const Tensor& x, const Tensor& b, const Tensor& s,\n@@ -546,8 +546,8 @@ std::vector<Tensor> SliceGrad(const Tensor& x, const Tensor& b, const Tensor& s,\n TEST(ArrayGradTest, SliceGrad) {\n   Tensor x(DT_FLOAT, {2, 3, 4});\n   x.flat<float>().setZero();\n-  auto begin = test::AsTensor<int32>({1, 1, 1});\n-  auto size = test::AsTensor<int32>({1, 2, 2});\n+  auto begin = test::AsTensor<int32_t>({1, 1, 1});\n+  auto size = test::AsTensor<int32_t>({1, 2, 2});\n   Tensor dy(DT_FLOAT, {1, 2, 2});\n   test::FillIota<float>(&dy, 1);\n   auto dx = SliceGrad(x, begin, size, dy);\n@@ -558,8 +558,8 @@ TEST(ArrayGradTest, SliceGrad) {\n                             0., 0., 0., 0., 0., 1., 2., 0., 0., 3., 4., 0.,\n                         },\n                         {2, 3, 4}));\n-  test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0, 0}));\n-  test::ExpectTensorEqual<int32>(dx[2], test::AsTensor<int32>({0, 0, 0}));\n+  test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0, 0, 0}));\n+  test::ExpectTensorEqual<int32_t>(dx[2], test::AsTensor<int32_t>({0, 0, 0}));\n }\n \n std::vector<Tensor> StridedSliceGrad(const Tensor& x, const Tensor& begin,\n@@ -653,12 +653,12 @@ std::vector<Tensor> StridedSliceGradGrad(\n TEST(ArrayGradTest, StridedSliceGrad) {\n   Tensor x(DT_FLOAT, {2, 3, 4});\n   x.flat<float>().setZero();\n-  Tensor x_shape = test::AsTensor<int32>({2, 3, 4}, {3});\n+  Tensor x_shape = test::AsTensor<int32_t>({2, 3, 4}, {3});\n \n   {\n-    auto start = test::AsTensor<int32>({1, 1, 1});\n-    auto stop = test::AsTensor<int32>({2, 3, 3});\n-    auto strides = test::AsTensor<int32>({1, 1, 1});\n+    auto start = test::AsTensor<int32_t>({1, 1, 1});\n+    auto stop = test::AsTensor<int32_t>({2, 3, 3});\n+    auto strides = test::AsTensor<int32_t>({1, 1, 1});\n     Tensor dy(DT_FLOAT, {1, 2, 2});\n     test::FillIota<float>(&dy, 1);\n     int begin_mask = 0, end_mask = 0, new_axis_mask = 0, shrink_axis_mask = 0,\n@@ -673,8 +673,8 @@ TEST(ArrayGradTest, StridedSliceGrad) {\n                               0., 0., 0., 0., 0., 1., 2., 0., 0., 3., 4., 0.,\n                           },\n                           {2, 3, 4}));\n-    test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0, 0}));\n-    test::ExpectTensorEqual<int32>(dx[2], test::AsTensor<int32>({0, 0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0, 0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[2], test::AsTensor<int32_t>({0, 0, 0}));\n     auto ddx = StridedSliceGradGrad(x_shape, start, stop, strides, dy, dx[0],\n                                     begin_mask, end_mask, ellipsis_mask,\n                                     new_axis_mask, shrink_axis_mask);\n@@ -683,9 +683,9 @@ TEST(ArrayGradTest, StridedSliceGrad) {\n \n   // test equivalent of python tf.gradients(foo[1:2, 1:3, 1:3])\n   {\n-    auto start = test::AsTensor<int32>({1, 1, 1});\n-    auto stop = test::AsTensor<int32>({2, 3, 3});\n-    auto strides = test::AsTensor<int32>({1, 1, 1});\n+    auto start = test::AsTensor<int32_t>({1, 1, 1});\n+    auto stop = test::AsTensor<int32_t>({2, 3, 3});\n+    auto strides = test::AsTensor<int32_t>({1, 1, 1});\n     Tensor dy(DT_FLOAT, {1, 2, 2});\n     test::FillIota<float>(&dy, 1);\n     int begin_mask = 0, end_mask = 0, new_axis_mask = 0, shrink_axis_mask = 0,\n@@ -700,8 +700,8 @@ TEST(ArrayGradTest, StridedSliceGrad) {\n                               0., 0., 0., 0., 0., 1., 2., 0., 0., 3., 4., 0.,\n                           },\n                           {2, 3, 4}));\n-    test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0, 0}));\n-    test::ExpectTensorEqual<int32>(dx[2], test::AsTensor<int32>({0, 0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0, 0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[2], test::AsTensor<int32_t>({0, 0, 0}));\n     auto ddx = StridedSliceGradGrad(x_shape, start, stop, strides, dy, dx[0],\n                                     begin_mask, end_mask, ellipsis_mask,\n                                     new_axis_mask, shrink_axis_mask);\n@@ -711,9 +711,9 @@ TEST(ArrayGradTest, StridedSliceGrad) {\n   // test equivalent of python tf.gradients(foo[1, 1:, :-2, None])\n   {\n     int dontcare = 66;\n-    auto start = test::AsTensor<int32>({1, 1, dontcare, dontcare});\n-    auto stop = test::AsTensor<int32>({2, dontcare, -2, dontcare});\n-    auto strides = test::AsTensor<int32>({1, 1, 1, dontcare});\n+    auto start = test::AsTensor<int32_t>({1, 1, dontcare, dontcare});\n+    auto stop = test::AsTensor<int32_t>({2, dontcare, -2, dontcare});\n+    auto strides = test::AsTensor<int32_t>({1, 1, 1, dontcare});\n     Tensor dy(DT_FLOAT, {2, 2, 1});\n     test::FillIota<float>(&dy, 1);\n     int begin_mask = 4, end_mask = 2, new_axis_mask = 8, shrink_axis_mask = 1,\n@@ -728,8 +728,10 @@ TEST(ArrayGradTest, StridedSliceGrad) {\n                               0., 0., 0., 0., 1., 2., 0., 0., 3., 4., 0., 0.,\n                           },\n                           {2, 3, 4}));\n-    test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0, 0, 0}));\n-    test::ExpectTensorEqual<int32>(dx[2], test::AsTensor<int32>({0, 0, 0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[1],\n+                                     test::AsTensor<int32_t>({0, 0, 0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[2],\n+                                     test::AsTensor<int32_t>({0, 0, 0, 0}));\n     auto ddx = StridedSliceGradGrad(x_shape, start, stop, strides, dy, dx[0],\n                                     begin_mask, end_mask, ellipsis_mask,\n                                     new_axis_mask, shrink_axis_mask);\n@@ -739,9 +741,9 @@ TEST(ArrayGradTest, StridedSliceGrad) {\n   // test equivalent of tf.gradients(foo[1, ...]) i.e. foo[1, 0:3, 0:4]\n   {\n     int dontcare = 66;\n-    auto start = test::AsTensor<int32>({1, dontcare});\n-    auto stop = test::AsTensor<int32>({2, dontcare});\n-    auto strides = test::AsTensor<int32>({1, 1});\n+    auto start = test::AsTensor<int32_t>({1, dontcare});\n+    auto stop = test::AsTensor<int32_t>({2, dontcare});\n+    auto strides = test::AsTensor<int32_t>({1, 1});\n     Tensor dy(DT_FLOAT, {3, 4});\n     test::FillIota<float>(&dy, 1);\n     int begin_mask = 0, end_mask = 0, new_axis_mask = 0, shrink_axis_mask = 1,\n@@ -756,8 +758,8 @@ TEST(ArrayGradTest, StridedSliceGrad) {\n                               1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12.,\n                           },\n                           {2, 3, 4}));\n-    test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0}));\n-    test::ExpectTensorEqual<int32>(dx[2], test::AsTensor<int32>({0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[1], test::AsTensor<int32_t>({0, 0}));\n+    test::ExpectTensorEqual<int32_t>(dx[2], test::AsTensor<int32_t>({0, 0}));\n     auto ddx = StridedSliceGradGrad(x_shape, start, stop, strides, dy, dx[0],\n                                     begin_mask, end_mask, ellipsis_mask,\n                                     new_axis_mask, shrink_axis_mask);\n@@ -793,12 +795,13 @@ TEST(ArrayGradTest, BroadcastToGrad) {\n   Tensor x(DT_FLOAT, {2, 2});\n   x.flat<float>().setZero();\n   Tensor shape(DT_INT32, {3});\n-  test::FillValues<int32>(&shape, {2, 2, 2});\n+  test::FillValues<int32_t>(&shape, {2, 2, 2});\n   Tensor dy(DT_FLOAT, {2, 2, 2});\n   test::FillIota<float>(&dy, 0);\n   auto dx = BroadcastToGrad(x, shape, dy);\n   test::ExpectClose(dx[0], test::AsTensor<float>({4., 6., 8., 10.}, {2, 2}));\n-  test::ExpectTensorEqual<int32>(dx[1], test::AsTensor<int32>({0, 0, 0}, {3}));\n+  test::ExpectTensorEqual<int32_t>(dx[1],\n+                                   test::AsTensor<int32_t>({0, 0, 0}, {3}));\n }\n }  // namespace\n }  // namespace tensorflow"
        },
        {
            "sha": "2d4a9937adbe50c62a178220f5aee897cffb8aaa",
            "filename": "tensorflow/core/ops/array_ops.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 27,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Farray_ops.cc?ref=be54b30fb35353acbcd2b148f6541cf8c8608e4d",
            "patch": "@@ -43,7 +43,7 @@ using shape_inference::UnchangedShape;\n namespace {\n \n absl::Status GetAxisForPackAndUnpack(InferenceContext* c,\n-                                     int32_t rank_after_pack, int32* axis) {\n+                                     int32_t rank_after_pack, int32_t* axis) {\n   TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", axis));\n   if (*axis < -1 * rank_after_pack || *axis >= rank_after_pack) {\n     return errors::InvalidArgument(\"Invalid axis: \", *axis, \"; must be in [\",\n@@ -116,7 +116,7 @@ absl::Status PadShapeFn(InferenceContext* c) {\n   TF_RETURN_IF_ERROR(c->WithValue(n_dim, num_dims, &n_dim));\n \n   if (paddings_t->dtype() == DT_INT32) {\n-    return PadKnown<int32>(c, input, paddings_t, num_dims);\n+    return PadKnown<int32_t>(c, input, paddings_t, num_dims);\n   } else {\n     return PadKnown<int64_t>(c, input, paddings_t, num_dims);\n   }\n@@ -165,7 +165,7 @@ absl::Status TransposeShapeFn(InferenceContext* c) {\n   if (perm != nullptr) {\n     std::vector<int64_t> data;\n     if (perm->dtype() == DT_INT32) {\n-      data = AsInt64<int32>(perm, rank);\n+      data = AsInt64<int32_t>(perm, rank);\n     } else {\n       data = AsInt64<int64_t>(perm, rank);\n     }\n@@ -660,7 +660,8 @@ REGISTER_OP(\"SplitV\")\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(input, split_dim + 1, &input));\n         std::vector<int64_t> data;\n         if (size_splits->dtype() == DT_INT32) {\n-          data = AsInt64<int32>(size_splits, size_splits->shape().dim_size(0));\n+          data =\n+              AsInt64<int32_t>(size_splits, size_splits->shape().dim_size(0));\n         } else {\n           data =\n               AsInt64<int64_t>(size_splits, size_splits->shape().dim_size(0));\n@@ -1033,7 +1034,8 @@ REGISTER_OP(\"ReverseV2\")\n         int32_t rank = c->Rank(input);\n         std::vector<int64_t> axis_value;\n         if (axis_tensor->dtype() == DT_INT32) {\n-          axis_value = AsInt64<int32>(axis_tensor, axis_tensor->NumElements());\n+          axis_value =\n+              AsInt64<int32_t>(axis_tensor, axis_tensor->NumElements());\n         } else {\n           axis_value =\n               AsInt64<int64_t>(axis_tensor, axis_tensor->NumElements());\n@@ -1131,7 +1133,7 @@ REGISTER_OP(\"Fill\")\n       const Tensor* t = c->input_tensor(0);\n       if (t != nullptr) {\n         for (int i = 0; i < t->NumElements(); ++i) {\n-          if ((index_type == DT_INT32 && t->vec<int32>()(i) < 0) ||\n+          if ((index_type == DT_INT32 && t->vec<int32_t>()(i) < 0) ||\n               (index_type == DT_INT64 && t->vec<int64_t>()(i) < 0)) {\n             return errors::InvalidArgument(\"Fill dimensions must be >= 0\");\n           }\n@@ -1249,7 +1251,7 @@ REGISTER_OP(\"GatherV2\")\n       // Note, axis can be negative.\n       int64_t axis = 0;\n       if (axis_t->dtype() == DT_INT32) {\n-        axis = axis_t->scalar<int32>()();\n+        axis = axis_t->scalar<int32_t>()();\n       } else {\n         axis = axis_t->scalar<int64_t>()();\n       }\n@@ -1482,7 +1484,7 @@ absl::Status UniqueIdxShapeFn(InferenceContext* c) {\n   } else if (n == 1) {\n     int64_t axis;\n     if (axis_t->dtype() == DT_INT32) {\n-      axis = static_cast<int64_t>(axis_t->flat<int32>()(0));\n+      axis = static_cast<int64_t>(axis_t->flat<int32_t>()(0));\n     } else {\n       axis = axis_t->flat<int64_t>()(0);\n     }\n@@ -1753,7 +1755,7 @@ REGISTER_OP(\"StridedSlice\")\n \n       PartialTensorShape processing_shape, final_shape;\n       bool is_identity, is_simple_slice, slice_dim0;\n-      absl::InlinedVector<int64, 4UL> begin, end, strides;\n+      absl::InlinedVector<int64_t, 4UL> begin, end, strides;\n       TF_RETURN_IF_ERROR(ValidateStridedSliceOp(\n           begin_value, end_value, *strides_value, input_shape, begin_mask,\n           end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask,\n@@ -2026,7 +2028,7 @@ REGISTER_OP(\"MirrorPadGrad\")\n       }\n \n       if (paddings_t->dtype() == DT_INT32) {\n-        return MirrorPadKnown<int32>(c, input, paddings_t, input_rank);\n+        return MirrorPadKnown<int32_t>(c, input, paddings_t, input_rank);\n       } else {\n         return MirrorPadKnown<int64_t>(c, input, paddings_t, input_rank);\n       }\n@@ -2107,7 +2109,7 @@ REGISTER_OP(\"ExpandDims\")\n \n       int64_t dim;\n       if (dim_t->dtype() == DT_INT32) {\n-        dim = static_cast<int64_t>(dim_t->flat<int32>()(0));\n+        dim = static_cast<int64_t>(dim_t->flat<int32_t>()(0));\n       } else {\n         dim = dim_t->flat<int64_t>()(0);\n       }\n@@ -2151,7 +2153,7 @@ REGISTER_OP(\"Squeeze\")\n       const int32_t input_rank = c->Rank(input);\n \n       // Validate and wrap squeeze dimensions.\n-      std::vector<int32> squeeze_dims;\n+      std::vector<int32_t> squeeze_dims;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"squeeze_dims\", &squeeze_dims));\n       for (int i = 0; i < squeeze_dims.size(); ++i) {\n         if (squeeze_dims[i] < -input_rank || squeeze_dims[i] >= input_rank) {\n@@ -2238,7 +2240,7 @@ std::vector<int64_t> GetFlatInt64(const Tensor& t) {\n // Converts int32 or int64 Tensor to flat std::vector<int64_t>.\n std::vector<int64_t> GetFlatInt64(const Tensor& t) {\n   if (t.dtype() == DT_INT32) {\n-    return GetFlatInt64<int32>(t);\n+    return GetFlatInt64<int32_t>(t);\n   } else {\n     return GetFlatInt64<int64_t>(t);\n   }\n@@ -2489,7 +2491,7 @@ REGISTER_OP(\"SpaceToDepth\")\n     .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n     // TODO(pauldonnelly): Implement GPU kernels for NCHW_VECT_C.\n     .SetShapeFn([](InferenceContext* c) {\n-      string data_format_str;\n+      std::string data_format_str;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n       TensorFormat data_format;\n       FormatFromString(data_format_str, &data_format);\n@@ -2543,7 +2545,7 @@ REGISTER_OP(\"DepthToSpace\")\n     .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n     // TODO(pauldonnelly): Implement GPU kernels for NCHW and NCHW_VECT_C.\n     .SetShapeFn([](InferenceContext* c) {\n-      string data_format_str;\n+      std::string data_format_str;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n       TensorFormat data_format;\n       FormatFromString(data_format_str, &data_format);\n@@ -2602,7 +2604,7 @@ REGISTER_OP(\"ExtractImagePatches\")\n       ShapeHandle input_shape;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n \n-      std::vector<int32> ksizes;\n+      std::vector<int32_t> ksizes;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"ksizes\", &ksizes));\n       if (ksizes.size() != 4) {\n         return errors::InvalidArgument(\n@@ -2611,7 +2613,7 @@ REGISTER_OP(\"ExtractImagePatches\")\n             ksizes.size());\n       }\n \n-      std::vector<int32> strides;\n+      std::vector<int32_t> strides;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n       if (strides.size() != 4) {\n         return errors::InvalidArgument(\n@@ -2620,7 +2622,7 @@ REGISTER_OP(\"ExtractImagePatches\")\n             strides.size());\n       }\n \n-      std::vector<int32> rates;\n+      std::vector<int32_t> rates;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n       if (rates.size() != 4) {\n         return errors::InvalidArgument(\n@@ -2692,7 +2694,7 @@ REGISTER_OP(\"ExtractVolumePatches\")\n       ShapeHandle input_shape;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 5, &input_shape));\n \n-      std::vector<int32> ksizes;\n+      std::vector<int32_t> ksizes;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"ksizes\", &ksizes));\n       if (ksizes.size() != 5) {\n         return errors::InvalidArgument(\n@@ -2701,7 +2703,7 @@ REGISTER_OP(\"ExtractVolumePatches\")\n             ksizes.size());\n       }\n \n-      std::vector<int32> strides;\n+      std::vector<int32_t> strides;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n       if (strides.size() != 5) {\n         return errors::InvalidArgument(\n@@ -2863,7 +2865,7 @@ REGISTER_OP(\"QuantizeAndDequantizeV2\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n-        if (axis >= kint32max) {\n+        if (axis >= std::numeric_limits<int32_t>::max()) {\n           return errors::InvalidArgument(\n               \"Axis cannot be >= kint32max value, got \", axis);\n         }\n@@ -2902,7 +2904,7 @@ REGISTER_OP(\"QuantizeAndDequantizeV4\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n-        if (axis >= kint32max) {\n+        if (axis >= std::numeric_limits<int32_t>::max()) {\n           return errors::InvalidArgument(\n               \"Axis cannot be >= kint32max value, got \", axis);\n         }\n@@ -2937,7 +2939,7 @@ REGISTER_OP(\"QuantizeAndDequantizeV4Grad\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n-        if (axis >= kint32max) {\n+        if (axis >= std::numeric_limits<int32_t>::max()) {\n           return errors::InvalidArgument(\n               \"Axis cannot be >= kint32max value, got \", axis);\n         }\n@@ -2977,7 +2979,7 @@ REGISTER_OP(\"QuantizeAndDequantizeV3\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n-        if (axis >= kint32max) {\n+        if (axis >= std::numeric_limits<int32_t>::max()) {\n           return errors::InvalidArgument(\n               \"Axis cannot be >= kint32max value, got \", axis);\n         }\n@@ -3042,7 +3044,7 @@ REGISTER_OP(\"Dequantize\")\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(2), minmax_rank, &minmax));\n       if (axis != -1) {\n         ShapeHandle input;\n-        if (axis >= kint32max) {\n+        if (axis >= std::numeric_limits<int32_t>::max()) {\n           // Check int32 max bound for a corner case to prevent integer flow\n           // when input actually has kint32max rank and above bound check is not\n           // triggered.\n@@ -3379,11 +3381,11 @@ REGISTER_OP(\"Fingerprint\")\n           return errors::InvalidArgument(\"`method` must be rank 0: \",\n                                          method->shape());\n         }\n-        const string& method_string = method->scalar<tstring>()();\n+        const std::string& method_string = method->scalar<tstring>()();\n         if (method_string != \"farmhash64\") {\n           return errors::InvalidArgument(\"Unsupported method: \", method_string);\n         }\n-        fingerprint_size = c->MakeDim(sizeof(uint64));\n+        fingerprint_size = c->MakeDim(sizeof(uint64_t));\n       }\n \n       DimensionHandle batch = c->Dim(c->input(0), 0);"
        },
        {
            "sha": "ddbf13d893b44eab6a10d972a4fe9c9b1d060008",
            "filename": "tensorflow/core/ops/array_ops_test.cc",
            "status": "modified",
            "additions": 75,
            "deletions": 74,
            "changes": 149,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be54b30fb35353acbcd2b148f6541cf8c8608e4d/tensorflow%2Fcore%2Fops%2Farray_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Farray_ops_test.cc?ref=be54b30fb35353acbcd2b148f6541cf8c8608e4d",
            "patch": "@@ -315,7 +315,7 @@ TEST(ArrayOpsTest, Fill_ShapeFn) {\n   INFER_OK(op, \"[?];?\", \"?\");\n   INFER_OK(op, \"[4];?\", \"[?,?,?,?]\");\n \n-  Tensor in_t = test::AsTensor<int32>({1, 2, 3, 4});\n+  Tensor in_t = test::AsTensor<int32_t>({1, 2, 3, 4});\n   op.input_tensors[0] = &in_t;\n   INFER_OK(op, \"[4];?\", \"[1,2,3,4]\");\n }\n@@ -695,14 +695,14 @@ TEST(ArrayOpsTest, ExpandDims_ShapeFn) {\n \n   // Expand at front of tensor.\n   for (int32_t idx : {0, -4}) {\n-    dim_t = test::AsScalar<int32>(idx);\n+    dim_t = test::AsScalar<int32_t>(idx);\n     INFER_OK(op, \"?;?\", \"?\");\n     INFER_OK(op, \"[5,?,7];?\", \"[1,d0_0,d0_1,d0_2]\");\n   }\n \n   // Expand at middle of tensor.\n   for (int32_t idx : {1, -3}) {\n-    dim_t = test::AsScalar<int32>(idx);\n+    dim_t = test::AsScalar<int32_t>(idx);\n     INFER_OK(op, \"?;?\", \"?\");\n     INFER_OK(op, \"[5,?,7];?\", \"[d0_0,1,d0_1,d0_2]\");\n \n@@ -712,7 +712,7 @@ TEST(ArrayOpsTest, ExpandDims_ShapeFn) {\n     INFER_OK(op, \"[5,?,7];?\", \"[d0_0,1,d0_1,d0_2]\");\n   }\n   for (int32_t idx : {2, -2}) {\n-    dim_t = test::AsScalar<int32>(idx);\n+    dim_t = test::AsScalar<int32_t>(idx);\n     INFER_OK(op, \"?;?\", \"?\");\n     INFER_OK(op, \"[5,?,7];?\", \"[d0_0,d0_1,1,d0_2]\");\n \n@@ -724,7 +724,7 @@ TEST(ArrayOpsTest, ExpandDims_ShapeFn) {\n \n   for (int32_t idx : {3, -1}) {\n     // Expand at the end.\n-    dim_t = test::AsScalar<int32>(idx);\n+    dim_t = test::AsScalar<int32_t>(idx);\n     INFER_OK(op, \"?;?\", \"?\");\n     INFER_OK(op, \"[5,?,7];?\", \"[d0_0,d0_1,d0_2,1]\");\n \n@@ -735,31 +735,31 @@ TEST(ArrayOpsTest, ExpandDims_ShapeFn) {\n   }\n   for (int32_t idx : {4, -5}) {\n     // Invalid idx.\n-    dim_t = test::AsScalar<int32>(idx);\n+    dim_t = test::AsScalar<int32_t>(idx);\n     INFER_ERROR(\"not in the interval [-4, 3]\", op, \"[5,?,7];?\");\n     dim_t = test::AsScalar<int64_t>(idx);\n     INFER_ERROR(\"not in the interval [-4, 3]\", op, \"[5,?,7];?\");\n   }\n \n   // Expand using an input vector tensor.\n-  std::vector<int32> dims;\n+  std::vector<int32_t> dims;\n   dims.push_back(0);\n-  dim_t = test::AsTensor<int32>(dims);\n+  dim_t = test::AsTensor<int32_t>(dims);\n   INFER_OK(op, \"?;?\", \"?\");\n   INFER_OK(op, \"[5,?,7];?\", \"[1,d0_0,d0_1,d0_2]\");\n \n   // Expand using too many input elements.\n   dims.push_back(1);\n-  dim_t = test::AsTensor<int32>(dims);\n+  dim_t = test::AsTensor<int32_t>(dims);\n   INFER_ERROR(\"'dim' input must be a tensor with a single\", op, \"?;?\");\n   INFER_ERROR(\"'dim' input must be a tensor with a single\", op, \"[5,6,7];?\");\n \n   // Examples from ExpandDims doc.\n-  dim_t = test::AsScalar<int32>(0);\n+  dim_t = test::AsScalar<int32_t>(0);\n   INFER_OK(op, \"[2];[]\", \"[1,d0_0]\");\n-  dim_t = test::AsScalar<int32>(1);\n+  dim_t = test::AsScalar<int32_t>(1);\n   INFER_OK(op, \"[2];[]\", \"[d0_0,1]\");\n-  dim_t = test::AsScalar<int32>(-1);\n+  dim_t = test::AsScalar<int32_t>(-1);\n   INFER_OK(op, \"[2];[]\", \"[d0_0,1]\");\n }\n \n@@ -966,7 +966,7 @@ TEST(ArrayOpsTest, Reshape_ShapeFn) {\n   INFER_OK(op, \"[4];[?]\", \"?\");\n \n   // All dimensions provided.\n-  Tensor new_shape = test::AsTensor<int32>({1, 2, 3});\n+  Tensor new_shape = test::AsTensor<int32_t>({1, 2, 3});\n   op.input_tensors[1] = &new_shape;\n   INFER_OK(op, \"?;[3]\", \"[1,2,3]\");\n   INFER_OK(op, \"[?];[3]\", \"[1,2,3]\");\n@@ -978,39 +978,39 @@ TEST(ArrayOpsTest, Reshape_ShapeFn) {\n \n   // Unknown dimensions.\n   // Flatten:\n-  new_shape = test::AsTensor<int32>({-1});\n+  new_shape = test::AsTensor<int32_t>({-1});\n   INFER_OK(op, \"?;[1]\", \"[?]\");\n   INFER_OK(op, \"[?];[1]\", \"[d0_0]\");\n   INFER_OK(op, \"[2,2];[1]\", \"[4]\");\n   // The first dimension is inferred:\n-  new_shape = test::AsTensor<int32>({2, -1});\n+  new_shape = test::AsTensor<int32_t>({2, -1});\n   INFER_OK(op, \"[3,4];[2]\", \"[2,6]\");\n   // The total number of elements must be evenly divisible by the known\n   // dimensions.\n   INFER_ERROR(\"Dimension size must be evenly divisible by 2 but is 7\", op,\n               \"[7];[2]\");\n   // Multiple missing dimensions cannot be inferred.\n-  new_shape = test::AsTensor<int32>({-1, -1, 2});\n+  new_shape = test::AsTensor<int32_t>({-1, -1, 2});\n   INFER_OK(op, \"[8];[3]\", \"[?,?,2]\");\n   INFER_OK(op, \"?;[3]\", \"[?,?,2]\");\n \n   // Symbolic shape propagation\n-  new_shape = test::AsTensor<int32>({-1, 2, 3});\n+  new_shape = test::AsTensor<int32_t>({-1, 2, 3});\n   INFER_OK(op, \"[?,2,3];[3]\", \"[d0_0,2,3]\");\n \n   // Reshaping to a scalar.\n-  new_shape = test::AsTensor<int32>({});\n+  new_shape = test::AsTensor<int32_t>({});\n   INFER_OK(op, \"[1];[0]\", \"[]\");\n   INFER_ERROR(\n       \"Cannot reshape a tensor with 2 elements to shape [] (1 elements)\", op,\n       \"[1,2];[0]\");\n \n   // Reshaping a tensor with no elements.\n-  new_shape = test::AsTensor<int32>({-1});\n+  new_shape = test::AsTensor<int32_t>({-1});\n   INFER_OK(op, \"[0];[1]\", \"[0]\");\n-  new_shape = test::AsTensor<int32>({-1, 6});\n+  new_shape = test::AsTensor<int32_t>({-1, 6});\n   INFER_OK(op, \"[0,2];[1]\", \"[0,6]\");\n-  new_shape = test::AsTensor<int32>({0, -1});\n+  new_shape = test::AsTensor<int32_t>({0, -1});\n   INFER_OK(op, \"[0,2];[1]\", \"[0,?]\");\n }\n \n@@ -1024,7 +1024,7 @@ TEST(ArrayOpsTest, QuantizedReshape_ShapeFn) {\n   INFER_OK(op, \"[?];?;?;?\", \"?;[];[]\");\n   INFER_OK(op, \"[?];[?];?;?\", \"?;[];[]\");\n   INFER_OK(op, \"[4];[?];?;?\", \"?;[];[]\");\n-  Tensor new_shape = test::AsTensor<int32>({1, 2, 3});\n+  Tensor new_shape = test::AsTensor<int32_t>({1, 2, 3});\n   op.input_tensors[1] = &new_shape;\n   INFER_OK(op, \"[?];[3];?;?\", \"[1,2,3];[];[]\");\n   INFER_OK(op, \"[6];[3];?;?\", \"[1,2,3];[];[]\");\n@@ -1096,23 +1096,23 @@ TEST(ArrayOpsTest, Transpose_ShapeFn) {\n   INFER_OK(op, \"[?];?\", \"[?]\");\n   INFER_OK(op, \"[?,?];[2]\", \"[?,?]\");\n   INFER_ERROR(\"Dimension must be 3 but is 2\", op, \"[1,2,3];[2]\");\n-  Tensor perm = test::AsTensor<int32>({0});\n+  Tensor perm = test::AsTensor<int32_t>({0});\n   op.input_tensors[1] = &perm;\n   INFER_OK(op, \"[?];[?]\", \"[d0_0]\");\n-  perm = test::AsTensor<int32>({1, 0});\n+  perm = test::AsTensor<int32_t>({1, 0});\n   INFER_OK(op, \"?;[2]\", \"[?,?]\");\n   INFER_OK(op, \"[?,?];[2]\", \"[d0_1,d0_0]\");\n   INFER_OK(op, \"[1,?];[2]\", \"[d0_1,d0_0]\");\n   INFER_OK(op, \"?;[0]\", \"in0\");\n \n   // Invalid arguments.\n-  perm = test::AsTensor<int32>({1, 2});\n+  perm = test::AsTensor<int32_t>({1, 2});\n   INFER_ERROR(\"perm dim 2 is out of range of input rank 2\", op, \"[1,2];[2]\");\n-  perm = test::AsTensor<int32>({0});\n+  perm = test::AsTensor<int32_t>({0});\n   INFER_ERROR(\"Dimension must be 2 but is 1\", op, \"[1,2];[1]\");\n \n   // Larger valid cases.\n-  perm = test::AsTensor<int32>({1, 0, 3, 4, 2});\n+  perm = test::AsTensor<int32_t>({1, 0, 3, 4, 2});\n   INFER_OK(op, \"[0,1,2,3,4];[5]\", \"[d0_1,d0_0,d0_3,d0_4,d0_2]\");\n   INFER_OK(op, \"[0,?,2,3,4];[5]\", \"[d0_1,d0_0,d0_3,d0_4,d0_2]\");\n }\n@@ -1163,7 +1163,7 @@ TEST(ArrayOpsTest, Bitcast_ShapeFn) {\n TEST(ArrayOpsTest, Squeeze_ShapeFn) {\n   ShapeInferenceTestOp op(\"Squeeze\");\n \n-  auto rebuild_node_def = [&op](const std::vector<int32>& squeeze_dims) {\n+  auto rebuild_node_def = [&op](const std::vector<int32_t>& squeeze_dims) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"Squeeze\")\n                      .Input(\"input\", 0, DT_FLOAT)\n                      .Attr(\"squeeze_dims\", squeeze_dims)\n@@ -1257,10 +1257,10 @@ TEST(ArrayOpsTest, Split_ShapeFn) {\n   INFER_OK(op, \"?;[1,4]\", \"[?,?];[?,?]\");\n \n   // split_dim is known.\n-  Tensor split_dim = test::AsTensor<int32>({1, 2});\n+  Tensor split_dim = test::AsTensor<int32_t>({1, 2});\n   op.input_tensors[0] = &split_dim;\n   INFER_ERROR(\"Input must be scalar but has rank 1\", op, \"[?];[?,?]\");\n-  split_dim = test::AsScalar<int32>(1);\n+  split_dim = test::AsScalar<int32_t>(1);\n   INFER_OK(op, \"?;?\", \"?;?\");\n   INFER_OK(op, \"?;[?,?]\", \"[d1_0,?];[d1_0,?]\");\n   INFER_OK(op, \"?;[1,4]\", \"[d1_0,2];[d1_0,2]\");\n@@ -1269,21 +1269,21 @@ TEST(ArrayOpsTest, Split_ShapeFn) {\n               \"?;[1,5]\");\n \n   // split_dim too large.\n-  split_dim = test::AsScalar<int32>(3);\n+  split_dim = test::AsScalar<int32_t>(3);\n   INFER_ERROR(\n       \"Dimension size, given by scalar input 3 must be in range [-3, 3)\", op,\n       \"?;[1,4,8]\");\n \n   // Negative split_dim.\n-  split_dim = test::AsScalar<int32>(-1);\n+  split_dim = test::AsScalar<int32_t>(-1);\n   INFER_OK(op, \"?;?\", \"?;?\");\n   INFER_OK(op, \"?;[?,?]\", \"[d1_0,?];[d1_0,?]\");\n   INFER_OK(op, \"?;[1,?]\", \"[d1_0,?];[d1_0,?]\");\n   INFER_OK(op, \"?;[1,4]\", \"[d1_0,2];[d1_0,2]\");\n   INFER_OK(op, \"?;[1,4,8]\", \"[d1_0,d1_1,4];[d1_0,d1_1,4]\");\n-  split_dim = test::AsScalar<int32>(-2);\n+  split_dim = test::AsScalar<int32_t>(-2);\n   INFER_OK(op, \"?;[1,4,8]\", \"[d1_0,2,d1_2];[d1_0,2,d1_2]\");\n-  split_dim = test::AsScalar<int32>(-4);\n+  split_dim = test::AsScalar<int32_t>(-4);\n   INFER_ERROR(\n       \"Dimension size, given by scalar input -4 must be in range [-3, 3)\", op,\n       \"?;[1,4,8]\");\n@@ -1312,7 +1312,7 @@ TEST(ArrayOpsTest, Tile_ShapeFn) {\n   INFER_OK(op, \"?;[4]\", \"[?,?,?,?]\");\n \n   // Test a tile of a 4D input.\n-  Tensor multiples = test::AsTensor<int32>({2, 3, 4, 5});\n+  Tensor multiples = test::AsTensor<int32_t>({2, 3, 4, 5});\n   op.input_tensors[1] = &multiples;\n   INFER_OK(op, \"[2,3,1,4];[4]\", \"[4,9,4,20]\");\n   // Test 64-bit tensor type\n@@ -1362,22 +1362,23 @@ TEST(ArrayOpsTest, OneHot_ShapeFn) {\n   INFER_OK(op, \"?;[];?;?\", \"?\");\n \n   // Depth must be scalar.\n-  Tensor depth = test::AsTensor<int32>({1, 2});\n+  Tensor depth = test::AsTensor<int32_t>({1, 2});\n   op.input_tensors[1] = &depth;\n   INFER_ERROR(\"Input must be scalar but has rank 1\", op, \"?;[2];?;?\");\n \n   // Full information is available.\n-  depth = test::AsScalar<int32>(2);\n+  depth = test::AsScalar<int32_t>(2);\n   INFER_OK(op, \"[1,3,4];[];?;?\", \"[d0_0,2,d0_1,d0_2]\");\n   set_axis(-1);\n   INFER_OK(op, \"[1,3,4];[];?;?\", \"[d0_0,d0_1,d0_2,2]\");\n }\n \n TEST(ArrayOpsTest, ExtractImagePatchesShapeTest) {\n   ShapeInferenceTestOp op(\"ExtractImagePatches\");\n-  auto set_op = [&op](const std::vector<int32>& ksizes,\n-                      const std::vector<int32>& strides,\n-                      const std::vector<int32>& rates, const string& padding) {\n+  auto set_op = [&op](const std::vector<int32_t>& ksizes,\n+                      const std::vector<int32_t>& strides,\n+                      const std::vector<int32_t>& rates,\n+                      const std::string& padding) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"ExtractImagePatches\")\n                      .Input(\"input\", 0, DT_FLOAT)\n                      .Attr(\"ksizes\", ksizes)\n@@ -1453,20 +1454,20 @@ TEST(ArrayOpsTest, SpaceToBatch_ShapeFn) {\n   INFER_ERROR(\"rank\", op, \"[1,10,10,3];[4]\");\n   INFER_ERROR(\"3 and 2\", op, \"[1,10,10,3];[2,3]\");\n \n-  Tensor paddings = test::AsTensor<int32>({4, 2, 2, 4}, {{2, 2}});\n+  Tensor paddings = test::AsTensor<int32_t>({4, 2, 2, 4}, {{2, 2}});\n   op.input_tensors[1] = &paddings;\n   INFER_OK(op, \"[1,10,10,3];[2,2]\", \"[4,8,8,d0_3]\");\n   paddings = test::AsTensor<int64_t>({4, 2, 2, 4}, {{2, 2}});\n   INFER_OK(op, \"[1,10,10,3];[2,2]\", \"[4,8,8,d0_3]\");\n \n   // Bad paddings values\n-  paddings = test::AsTensor<int32>({1, 2, 3, 4}, {{2, 2}});\n+  paddings = test::AsTensor<int32_t>({1, 2, 3, 4}, {{2, 2}});\n   op.input_tensors[1] = &paddings;\n   INFER_ERROR(\"Dimension size must be evenly divisible by 2 but is 13\", op,\n               \"[1,10,10,3];[2,2]\");\n \n   // Negative paddings\n-  paddings = test::AsTensor<int32>({1, -2, 3, 4}, {{2, 2}});\n+  paddings = test::AsTensor<int32_t>({1, -2, 3, 4}, {{2, 2}});\n   op.input_tensors[1] = &paddings;\n   INFER_ERROR(\"cannot be negative\", op, \"[1,10,10,3];[2,2]\");\n }\n@@ -1491,21 +1492,21 @@ TEST(ArrayOpsTest, SpaceToBatchND_ShapeFn) {\n \n   {\n     // Dimensions are partially known, block_shape known.\n-    Tensor block_shape = test::AsTensor<int32>({2, 3});\n+    Tensor block_shape = test::AsTensor<int32_t>({2, 3});\n     op.input_tensors[1] = &block_shape;\n     INFER_OK(op, \"[3,?,?,2];[2];?\", \"[18,?,?,d0_3]\");\n \n     // Dimensions are partially known, block_shape and paddings known.\n     {\n-      Tensor paddings = test::AsTensor<int32>({1, 1, 0, 1}, {{2, 2}});\n+      Tensor paddings = test::AsTensor<int32_t>({1, 1, 0, 1}, {{2, 2}});\n       op.input_tensors[2] = &paddings;\n       INFER_OK(op, \"[3,?,2,2];[2];[2,2]\", \"[18,?,1,d0_3]\");\n       op.input_tensors[2] = nullptr;\n     }\n \n     // Dimensions are fully known, block_shape and paddings are known.\n     {\n-      Tensor paddings = test::AsTensor<int32>({1, 1, 0, 0}, {{2, 2}});\n+      Tensor paddings = test::AsTensor<int32_t>({1, 1, 0, 0}, {{2, 2}});\n       op.input_tensors[2] = &paddings;\n       INFER_OK(op, \"[3,2,3,2];[2];[2,2]\", \"[18,2,1,d0_3]\");\n       op.input_tensors[2] = nullptr;\n@@ -1518,36 +1519,36 @@ TEST(ArrayOpsTest, SpaceToBatchND_ShapeFn) {\n   INFER_ERROR(\"block_shape must have known size\", op, \"?;[?];?\");\n \n   {\n-    Tensor block_shape = test::AsTensor<int32>({0, 2});\n+    Tensor block_shape = test::AsTensor<int32_t>({0, 2});\n     op.input_tensors[1] = &block_shape;\n     INFER_ERROR(\"block_shape must be positive\", op, \"[1,2,2];[2];[2,2]\");\n     op.input_tensors[1] = nullptr;\n   }\n \n   {\n-    Tensor block_shape = test::AsTensor<int32>({1, 1});\n+    Tensor block_shape = test::AsTensor<int32_t>({1, 1});\n     op.input_tensors[1] = &block_shape;\n-    Tensor paddings = test::AsTensor<int32>({0, -1, 0, 0}, {{2, 2}});\n+    Tensor paddings = test::AsTensor<int32_t>({0, -1, 0, 0}, {{2, 2}});\n     op.input_tensors[2] = &paddings;\n     INFER_ERROR(\"paddings cannot be negative\", op, \"[1,2,2];[2];[2,2]\");\n     op.input_tensors[1] = nullptr;\n     op.input_tensors[2] = nullptr;\n   }\n \n   {\n-    Tensor block_shape = test::AsTensor<int32>({3, 3});\n+    Tensor block_shape = test::AsTensor<int32_t>({3, 3});\n     op.input_tensors[1] = &block_shape;\n-    Tensor paddings = test::AsTensor<int32>({0, 0, 0, 0}, {{2, 2}});\n+    Tensor paddings = test::AsTensor<int32_t>({0, 0, 0, 0}, {{2, 2}});\n     op.input_tensors[2] = &paddings;\n     INFER_ERROR(\"divisible\", op, \"[1,2,3,1];[2];[2,2]\");\n     op.input_tensors[1] = nullptr;\n     op.input_tensors[2] = nullptr;\n   }\n \n   {\n-    Tensor block_shape = test::AsTensor<int32>({});\n+    Tensor block_shape = test::AsTensor<int32_t>({});\n     op.input_tensors[1] = &block_shape;\n-    Tensor paddings = test::AsTensor<int32>({});\n+    Tensor paddings = test::AsTensor<int32_t>({});\n     op.input_tensors[2] = &paddings;\n     INFER_OK(op, \"?;[0];[0,2]\", \"?\");\n     op.input_tensors[1] = nullptr;\n@@ -1586,17 +1587,17 @@ TEST(ArrayOpsTest, BatchToSpace_ShapeFn) {\n   INFER_OK(op, \"[4,8,8,3];[2,2]\", \"[1,10,10,d0_3]\");\n \n   // Bad croppings values\n-  croppings = test::AsTensor<int32>({100, 2, 3, 4}, {{2, 2}});\n+  croppings = test::AsTensor<int32_t>({100, 2, 3, 4}, {{2, 2}});\n   op.input_tensors[1] = &croppings;\n   INFER_ERROR(\"Negative dimension size caused by subtracting\", op,\n               \"[4,8,8,3];[2,2]\");\n-  croppings = test::AsTensor<int32>({1, 2, 3, 400}, {{2, 2}});\n+  croppings = test::AsTensor<int32_t>({1, 2, 3, 400}, {{2, 2}});\n   op.input_tensors[1] = &croppings;\n   INFER_ERROR(\"Negative dimension size caused by subtracting\", op,\n               \"[4,8,8,3];[2,2]\");\n \n   // Negative paddings\n-  croppings = test::AsTensor<int32>({1, -2, 3, 4}, {{2, 2}});\n+  croppings = test::AsTensor<int32_t>({1, -2, 3, 4}, {{2, 2}});\n   op.input_tensors[1] = &croppings;\n   INFER_ERROR(\"cannot be negative\", op, \"[4,8,8,3];[2,2]\");\n }\n@@ -1618,23 +1619,23 @@ TEST(ArrayOpsTest, BatchToSpaceND_ShapeFn) {\n \n   {\n     // Dimensions are partially known, block_shape known.\n-    Tensor block_shape = test::AsTensor<int32>({2, 3});\n+    Tensor block_shape = test::AsTensor<int32_t>({2, 3});\n     op.input_tensors[1] = &block_shape;\n     INFER_OK(op, \"[?,?,?,2];[2];?\", \"[?,?,?,d0_3]\");\n \n     INFER_OK(op, \"[18,?,?,2];[2];?\", \"[3,?,?,d0_3]\");\n \n     // Dimensions are partially known, block_shape and crops known.\n     {\n-      Tensor crops = test::AsTensor<int32>({1, 1, 0, 1}, {{2, 2}});\n+      Tensor crops = test::AsTensor<int32_t>({1, 1, 0, 1}, {{2, 2}});\n       op.input_tensors[2] = &crops;\n       INFER_OK(op, \"[18,?,2,2];[2];[2,2]\", \"[3,?,5,d0_3]\");\n       op.input_tensors[2] = nullptr;\n     }\n \n     // Dimensions are fully known, block_shape and crops are known.\n     {\n-      Tensor crops = test::AsTensor<int32>({1, 1, 0, 0}, {{2, 2}});\n+      Tensor crops = test::AsTensor<int32_t>({1, 1, 0, 0}, {{2, 2}});\n       op.input_tensors[2] = &crops;\n       INFER_OK(op, \"[18,2,1,2];[2];[2,2]\", \"[3,2,3,d0_3]\");\n       op.input_tensors[2] = nullptr;\n@@ -1649,16 +1650,16 @@ TEST(ArrayOpsTest, BatchToSpaceND_ShapeFn) {\n   INFER_ERROR(\"rank\", op, \"[2,2,3];[3];[3,2]\");\n \n   {\n-    Tensor block_shape = test::AsTensor<int32>({0, 2});\n+    Tensor block_shape = test::AsTensor<int32_t>({0, 2});\n     op.input_tensors[1] = &block_shape;\n     INFER_ERROR(\"block_shape must be positive\", op, \"[1,2,2];[2];[2,2]\");\n     op.input_tensors[1] = nullptr;\n   }\n \n   {\n-    Tensor block_shape = test::AsTensor<int32>({1, 1});\n+    Tensor block_shape = test::AsTensor<int32_t>({1, 1});\n     op.input_tensors[1] = &block_shape;\n-    Tensor paddings = test::AsTensor<int32>({0, -1, 0, 0}, {{2, 2}});\n+    Tensor paddings = test::AsTensor<int32_t>({0, -1, 0, 0}, {{2, 2}});\n     op.input_tensors[2] = &paddings;\n     INFER_ERROR(\"crops cannot be negative\", op, \"[1,2,2];[2];[2,2]\");\n     op.input_tensors[1] = nullptr;\n@@ -1667,9 +1668,9 @@ TEST(ArrayOpsTest, BatchToSpaceND_ShapeFn) {\n \n   // The amount to crop exceeds the padded size.\n   {\n-    Tensor block_shape = test::AsTensor<int32>({2, 2});\n+    Tensor block_shape = test::AsTensor<int32_t>({2, 2});\n     op.input_tensors[1] = &block_shape;\n-    Tensor crops = test::AsTensor<int32>({3, 2, 0, 0}, {{2, 2}});\n+    Tensor crops = test::AsTensor<int32_t>({3, 2, 0, 0}, {{2, 2}});\n     op.input_tensors[2] = &crops;\n     INFER_ERROR(\"Negative\", op, \"[4,2,3,1];[2];[2,2]\");\n     op.input_tensors[1] = nullptr;\n@@ -1678,7 +1679,7 @@ TEST(ArrayOpsTest, BatchToSpaceND_ShapeFn) {\n \n   // The batch size is not divisible by the product of the block_shape.\n   {\n-    Tensor block_shape = test::AsTensor<int32>({2, 3});\n+    Tensor block_shape = test::AsTensor<int32_t>({2, 3});\n     op.input_tensors[1] = &block_shape;\n     INFER_ERROR(\"divisible\", op, \"[3,1,1,1];[2];[2,2]\");\n     op.input_tensors[1] = nullptr;\n@@ -1755,22 +1756,22 @@ TEST(ArrayOpsTest, Slice_ShapeFn) {\n \n   // Tests with known values.\n   op.input_tensors.resize(3);\n-  Tensor begin = test::AsTensor<int32>({0, 1, 2, 1});\n-  Tensor sizes = test::AsTensor<int32>({1, 2, 1, 3});\n+  Tensor begin = test::AsTensor<int32_t>({0, 1, 2, 1});\n+  Tensor sizes = test::AsTensor<int32_t>({1, 2, 1, 3});\n   op.input_tensors[1] = &begin;\n   op.input_tensors[2] = &sizes;\n   INFER_OK(op, \"[2,3,4,5];[4];[4]\", \"[1,2,1,3]\");\n \n   // -1 in sizes means \"get the rest\"\n-  sizes = test::AsTensor<int32>({-1, -1, 1, -1});\n+  sizes = test::AsTensor<int32_t>({-1, -1, 1, -1});\n   INFER_OK(op, \"[2,3,4,5];[4];[4]\", \"[d0_0,2,1,4]\");\n \n-  begin = test::AsTensor<int32>({0, 1, 2, 6});\n-  sizes = test::AsTensor<int32>({-1, -1, -1, -1});\n+  begin = test::AsTensor<int32_t>({0, 1, 2, 6});\n+  sizes = test::AsTensor<int32_t>({-1, -1, -1, -1});\n   INFER_ERROR(\"Negative dimension size\", op, \"[2,3,4,5];[4];[4]\");\n \n-  begin = test::AsTensor<int32>({0, 1, 2, 5});\n-  sizes = test::AsTensor<int32>({-1, -1, -1, -2});\n+  begin = test::AsTensor<int32_t>({0, 1, 2, 5});\n+  sizes = test::AsTensor<int32_t>({-1, -1, -1, -2});\n   INFER_ERROR(\"cannot be < -1\", op, \"[2,3,4,5];[4];[4]\");\n }\n \n@@ -1784,7 +1785,7 @@ TEST(ArrayOpsTest, StridedSlice_ShapeFn) {\n                    .Attr(\"shrink_axis_mask\", 1)\n                    .Finalize(&op.node_def));\n   op.input_tensors.resize(4);\n-  Tensor strides = test::AsTensor<int32>({1});\n+  Tensor strides = test::AsTensor<int32_t>({1});\n   op.input_tensors[3] = &strides;\n   // Slicing on the 0-th dimension.\n   INFER_OK(op, \"[2,3,4,5];[1];[1];[1]\", \"[3,4,5]\");\n@@ -1799,7 +1800,7 @@ TEST(ArrayOpsTest, StridedSliceGrad_ShapeFn) {\n   INFER_OK(op, \"[?];?;?;?;?\", \"?\");\n   INFER_OK(op, \"[4];?;?;?;?\", \"[?,?,?,?]\");\n \n-  Tensor in_t = test::AsTensor<int32>({1, 2, 3, 4});\n+  Tensor in_t = test::AsTensor<int32_t>({1, 2, 3, 4});\n   op.input_tensors[0] = &in_t;\n   INFER_OK(op, \"[4];?;?;?;?\", \"[1,2,3,4]\");\n }"
        }
    ],
    "stats": {
        "total": 306,
        "additions": 156,
        "deletions": 150
    }
}