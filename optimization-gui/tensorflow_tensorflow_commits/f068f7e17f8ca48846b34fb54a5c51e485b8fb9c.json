{
    "author": "mtsokol",
    "message": "PR #33201: Docs: Error 0102\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33201\n\nAfter #32628\n\nðŸš€ Kind of Contribution\n\nðŸ“š Documentation\nCopybara import of the project:\n\n--\nf7eaa2740a83d33aed95ef8fe93ea1fa74766b0c by Mateusz SokÃ³Å‚ <mat646@gmail.com>:\n\nDoc page for Error 0102\n\nMerging this change closes #33201\n\nPiperOrigin-RevId: 831487905",
    "sha": "f068f7e17f8ca48846b34fb54a5c51e485b8fb9c",
    "files": [
        {
            "sha": "212b4c7efb3167a32f70899bb948dcb142e91112",
            "filename": "third_party/xla/docs/error_codes.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f068f7e17f8ca48846b34fb54a5c51e485b8fb9c/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f068f7e17f8ca48846b34fb54a5c51e485b8fb9c/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferror_codes.md?ref=f068f7e17f8ca48846b34fb54a5c51e485b8fb9c",
            "patch": "@@ -1,3 +1,5 @@\n # XLA Error codes\n \n This page is a list of all error codes emitted by the XLA compiler.\n+\n+-   [E0102](./errors/error_0102.md)"
        },
        {
            "sha": "5f58474c5b04a752d788c467dabcb7ae5e3b1e44",
            "filename": "third_party/xla/docs/errors/error_0102.md",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f068f7e17f8ca48846b34fb54a5c51e485b8fb9c/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0102.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f068f7e17f8ca48846b34fb54a5c51e485b8fb9c/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0102.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0102.md?ref=f068f7e17f8ca48846b34fb54a5c51e485b8fb9c",
            "patch": "@@ -0,0 +1,53 @@\n+# Error code: 0102\n+\n+**Category:** Program input buffer mismatch\n+\n+**Type:** Runtime\n+\n+## Error log example\n+\n+```\n+XlaRuntimeError: INVALID_ARGUMENT: Executable(jit_embedding_pipeline_step_fn) expected parameter 2482 of size 5242880 (bf16[16,1280,40]{2,1,0:T(8,128)(2,1)}) but got buffer with incompatible size 1638400 (bf16[16,1280,40]{1,2,0:T(8,128)(2,1)}): while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).\n+```\n+\n+## Why do these happen?\n+\n+This error occurs when the XLA runtime detects a mismatch between the size of a\n+memory buffer expected by a compiled program and the size of the buffer that is\n+actually provided at execution time. The error message indicates both the\n+expected and actual sizes, as well as the tensor shapes and layouts.\n+\n+Note that these errors might occur even if two tensors have the same shape but\n+their size in memory can be different if their physical layout (how the data is\n+tiled and arranged on the hardware) is different.\n+\n+These errors are predominantly caused by: - **Checkpoint and XLA configuration\n+mismatch** - A model is trained and a checkpoint is saved. The physical layout\n+of the weights in that checkpoint is determined by the exact XLA version and\n+configuration (e.g. XLA flags) at that time. Later, this checkpoint is loaded in\n+a different environment where the configuration has changed. A new flag, a\n+different default value, or a change in the model/XLA code can cause the runtime\n+to expect a different physical layout for the weights. When the old buffer from\n+the checkpoint is passed to the new compiled XLA program, the runtime throws an\n+error. - **Hardware/Topology-Specific Layouts** - The XLA compiler is free to\n+choose different physical layouts for tensors to optimize performance on\n+different hardware. A layout that is optimal for v4 TPU might be different from\n+a v5 TPU, or even for different pod slices of the same chip (e.g., 4x4x4 vs\n+4x8). The error occurs when a model is compiled with an assumption about one\n+topology's layout, but at runtime it is scheduled on a different topology, or\n+there is a bug in the compiler's layout logic for a specific piece of hardware.\n+\n+## How can a user fix their program when they do happen?\n+\n+-   Ensure configuration consistency between model export and re-runs from\n+    checkpoints:\n+    -   Avoid using old checkpoints with new code unless you are certain that no\n+        layout-affecting changes have been made.\n+    -   Re-export the Saved Model: If you suspect a checkpoint/configuration\n+        mismatch, the most reliable solution is to re-export the saved model\n+        using the exact same (and current) codebase and configuration that you\n+        are using for inference or fine-tuning.\n+    -   Check for configuration changes (e.g. XLA flags) between the two runs.\n+-   Hardware/Topology-Specific layouts:\n+    -   Check for hardware version and topology mismatches if switching hardware\n+        or topologies."
        }
    ],
    "stats": {
        "total": 55,
        "additions": 55,
        "deletions": 0
    }
}