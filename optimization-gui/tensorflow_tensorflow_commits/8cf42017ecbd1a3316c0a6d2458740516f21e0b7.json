{
    "author": "majnemer",
    "message": "[TSL] Consolidate NUMA code across different platforms.\n\nNo functional change is intended.\n\nPiperOrigin-RevId: 821216963",
    "sha": "8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
    "files": [
        {
            "sha": "85d16a4b398ad66b332d234d47b4c99f0acf9d5d",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/numa.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fnuma.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fnuma.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fnuma.h?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -16,8 +16,7 @@ limitations under the License.\n #ifndef TENSORFLOW_TSL_PLATFORM_NUMA_H_\n #define TENSORFLOW_TSL_PLATFORM_NUMA_H_\n \n-#include \"xla/tsl/platform/types.h\"\n-#include \"tsl/platform/platform.h\"\n+#include <cstddef>\n \n namespace tsl {\n namespace port {"
        },
        {
            "sha": "a5be271747f42806acbba34cf4e165b75cc64163",
            "filename": "third_party/xla/xla/tsl/platform/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2FBUILD?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -58,6 +58,8 @@ exports_files(\n         \"threadpool.cc\",\n         \"threadpool.h\",\n         \"env.h\",\n+        \"numa_hwloc.cc\",\n+        \"numa_noop.cc\",\n     ],\n     visibility = internal_visibility([\n         \"//tensorflow/core/platform:__subpackages__\","
        },
        {
            "sha": "46c0c3069e0fec6d5521d61e83aef2ff55c12dde",
            "filename": "third_party/xla/xla/tsl/platform/default/BUILD",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2FBUILD?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -324,7 +324,14 @@ cc_library(\n     srcs = [\n         \"port.cc\",\n         \"@local_tsl//tsl/platform:cpu_info.cc\",\n-    ],\n+    ] + select({\n+        \"//xla/tsl:with_numa_support\": [\n+            \"//xla/tsl/platform:numa_hwloc.cc\",\n+        ],\n+        \"//conditions:default\": [\n+            \"//xla/tsl/platform:numa_noop.cc\",\n+        ],\n+    }),\n     hdrs = [\n         \"//xla/tsl/platform/profile_utils:cpu_utils.h\",\n         \"@local_tsl//tsl/platform:cpu_info.h\",\n@@ -336,11 +343,7 @@ cc_library(\n         \"@local_tsl//tsl/platform:snappy.h\",\n     ],\n     copts = tsl_copts(),\n-    defines = [\"TF_USE_SNAPPY\"] + select({\n-        # TF Additional NUMA defines\n-        \"//xla/tsl:with_numa_support\": [\"TENSORFLOW_USE_NUMA\"],\n-        \"//conditions:default\": [],\n-    }),\n+    defines = [\"TF_USE_SNAPPY\"],\n     tags = [\n         \"manual\",\n         \"no_oss\",\n@@ -357,12 +360,12 @@ cc_library(\n         \"@local_tsl//tsl/platform\",\n         \"@snappy\",\n     ] + select({\n-        # TF Additional NUMA dependencies\n         \"//xla/tsl:with_numa_support\": [\n-            # Don't merge in a single line\n+            \"@com_google_absl//absl/log\",\n             \"@hwloc\",\n         ],\n-        \"//conditions:default\": [],\n+        \"//conditions:default\": [\n+        ],\n     }),\n )\n \n@@ -608,6 +611,7 @@ filegroup(\n         \"status.h\",\n         \"statusor.h\",\n         \"tracing_impl.h\",\n+        \"//xla/tsl/platform:numa_noop.cc\",\n         \"//xla/tsl/platform/profile_utils:cpu_utils.h\",\n         \"//xla/tsl/platform/profile_utils:i_cpu_utils_helper.h\",\n     ],"
        },
        {
            "sha": "a406d79f42d01495150ff4aec0dbd230e6c302f5",
            "filename": "third_party/xla/xla/tsl/platform/default/port.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 143,
            "changes": 143,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fport.cc?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -48,10 +48,6 @@ limitations under the License.\n #include <thread>\n #endif\n \n-#if TENSORFLOW_USE_NUMA\n-#include \"hwloc.h\"\n-#endif\n-\n #if defined(__ANDROID__) && (defined(__i386__) || defined(__x86_64__))\n #define TENSORFLOW_HAS_CXA_DEMANGLE 0\n #elif (__GNUC__ >= 4 || (__GNUC__ >= 3 && __GNUC_MINOR__ >= 4)) && \\\n@@ -170,145 +166,6 @@ int NumHyperthreadsPerCore() {\n   return (ht_per_core > 0) ? ht_per_core : 1;\n }\n \n-#ifdef TENSORFLOW_USE_NUMA\n-namespace {\n-static hwloc_topology_t hwloc_topology_handle;\n-\n-bool HaveHWLocTopology() {\n-  // One time initialization\n-  static bool init = []() {\n-    if (hwloc_topology_init(&hwloc_topology_handle)) {\n-      LOG(ERROR) << \"Call to hwloc_topology_init() failed\";\n-      return false;\n-    }\n-    if (hwloc_topology_load(hwloc_topology_handle)) {\n-      LOG(ERROR) << \"Call to hwloc_topology_load() failed\";\n-      return false;\n-    }\n-    return true;\n-  }();\n-  return init;\n-}\n-\n-// Return the first hwloc object of the given type whose os_index\n-// matches 'index'.\n-hwloc_obj_t GetHWLocTypeIndex(hwloc_obj_type_t tp, int index) {\n-  hwloc_obj_t obj = nullptr;\n-  if (index >= 0) {\n-    while ((obj = hwloc_get_next_obj_by_type(hwloc_topology_handle, tp, obj)) !=\n-           nullptr) {\n-      if (obj->os_index == index) break;\n-    }\n-  }\n-  return obj;\n-}\n-}  // namespace\n-#endif  // TENSORFLOW_USE_NUMA\n-\n-bool NUMAEnabled() { return (NUMANumNodes() > 1); }\n-\n-int NUMANumNodes() {\n-#ifdef TENSORFLOW_USE_NUMA\n-  if (HaveHWLocTopology()) {\n-    int num_numanodes =\n-        hwloc_get_nbobjs_by_type(hwloc_topology_handle, HWLOC_OBJ_NUMANODE);\n-    return std::max(1, num_numanodes);\n-  } else {\n-    return 1;\n-  }\n-#else\n-  return 1;\n-#endif  // TENSORFLOW_USE_NUMA\n-}\n-\n-void NUMASetThreadNodeAffinity(int node) {\n-#ifdef TENSORFLOW_USE_NUMA\n-  if (HaveHWLocTopology()) {\n-    // Find the corresponding NUMA node topology object.\n-    hwloc_obj_t obj = GetHWLocTypeIndex(HWLOC_OBJ_NUMANODE, node);\n-    if (obj) {\n-      hwloc_set_cpubind(hwloc_topology_handle, obj->cpuset,\n-                        HWLOC_CPUBIND_THREAD | HWLOC_CPUBIND_STRICT);\n-    } else {\n-      LOG(ERROR) << \"Could not find hwloc NUMA node \" << node;\n-    }\n-  }\n-#endif  // TENSORFLOW_USE_NUMA\n-}\n-\n-int NUMAGetThreadNodeAffinity() {\n-  int node_index = kNUMANoAffinity;\n-#ifdef TENSORFLOW_USE_NUMA\n-  if (HaveHWLocTopology()) {\n-    hwloc_cpuset_t thread_cpuset = hwloc_bitmap_alloc();\n-    hwloc_get_cpubind(hwloc_topology_handle, thread_cpuset,\n-                      HWLOC_CPUBIND_THREAD);\n-    hwloc_obj_t obj = nullptr;\n-    // Return the first NUMA node whose cpuset is a (non-proper) superset of\n-    // that of the current thread.\n-    while ((obj = hwloc_get_next_obj_by_type(\n-                hwloc_topology_handle, HWLOC_OBJ_NUMANODE, obj)) != nullptr) {\n-      if (hwloc_bitmap_isincluded(thread_cpuset, obj->cpuset)) {\n-        node_index = obj->os_index;\n-        break;\n-      }\n-    }\n-    hwloc_bitmap_free(thread_cpuset);\n-  }\n-#endif  // TENSORFLOW_USE_NUMA\n-  return node_index;\n-}\n-\n-void* NUMAMalloc(int node, size_t size, int minimum_alignment) {\n-#ifdef TENSORFLOW_USE_NUMA\n-  if (HaveHWLocTopology()) {\n-    hwloc_obj_t numa_node = GetHWLocTypeIndex(HWLOC_OBJ_NUMANODE, node);\n-    if (numa_node) {\n-      return hwloc_alloc_membind(hwloc_topology_handle, size,\n-                                 numa_node->nodeset, HWLOC_MEMBIND_BIND,\n-                                 HWLOC_MEMBIND_BYNODESET);\n-    } else {\n-      LOG(ERROR) << \"Failed to find hwloc NUMA node \" << node;\n-    }\n-  }\n-#endif  // TENSORFLOW_USE_NUMA\n-  return tsl::port::AlignedMalloc(size, minimum_alignment);\n-}\n-\n-void NUMAFree(void* ptr, size_t size) {\n-#ifdef TENSORFLOW_USE_NUMA\n-  if (HaveHWLocTopology()) {\n-    hwloc_free(hwloc_topology_handle, ptr, size);\n-    return;\n-  }\n-#endif  // TENSORFLOW_USE_NUMA\n-  tsl::port::Free(ptr);\n-}\n-\n-int NUMAGetMemAffinity(const void* addr) {\n-  int node = kNUMANoAffinity;\n-#ifdef TENSORFLOW_USE_NUMA\n-  if (HaveHWLocTopology() && addr) {\n-    hwloc_nodeset_t nodeset = hwloc_bitmap_alloc();\n-    if (!hwloc_get_area_memlocation(hwloc_topology_handle, addr, 4, nodeset,\n-                                    HWLOC_MEMBIND_BYNODESET)) {\n-      hwloc_obj_t obj = nullptr;\n-      while ((obj = hwloc_get_next_obj_by_type(\n-                  hwloc_topology_handle, HWLOC_OBJ_NUMANODE, obj)) != nullptr) {\n-        if (hwloc_bitmap_isincluded(nodeset, obj->nodeset)) {\n-          node = obj->os_index;\n-          break;\n-        }\n-      }\n-      hwloc_bitmap_free(nodeset);\n-    } else {\n-      LOG(ERROR) << \"Failed call to hwloc_get_area_memlocation.\";\n-    }\n-  }\n-#endif  // TENSORFLOW_USE_NUMA\n-  return node;\n-}\n-\n bool Snappy_Compress(const char* input, size_t length, string* output) {\n #ifdef TF_USE_SNAPPY\n   output->resize(snappy::MaxCompressedLength(length));"
        },
        {
            "sha": "50ba2c6a664fd106b52340687cf910b696ed930e",
            "filename": "third_party/xla/xla/tsl/platform/numa_hwloc.cc",
            "status": "added",
            "additions": 205,
            "deletions": 0,
            "changes": 205,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fnuma_hwloc.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fnuma_hwloc.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fnuma_hwloc.cc?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -0,0 +1,205 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstddef>\n+#include <memory>\n+#include <type_traits>\n+\n+#include \"absl/base/call_once.h\"\n+#include \"absl/log/log.h\"\n+#include \"hwloc.h\"\n+#include \"tsl/platform/mem.h\"\n+#include \"tsl/platform/numa.h\"\n+\n+namespace tsl {\n+namespace port {\n+\n+namespace {\n+hwloc_topology_t GetHWLocTopology() {\n+  static absl::once_flag init_once;\n+  static hwloc_topology_t hwloc_topology_handle = nullptr;\n+  absl::call_once(init_once, [] {\n+    if (hwloc_topology_init(&hwloc_topology_handle)) {\n+      LOG(ERROR) << \"Call to hwloc_topology_init() failed\";\n+      return;\n+    }\n+    if (hwloc_topology_load(hwloc_topology_handle)) {\n+      LOG(ERROR) << \"Call to hwloc_topology_load() failed\";\n+      return;\n+    }\n+  });\n+  return hwloc_topology_handle;\n+}\n+\n+// Return the first hwloc object of the given type whose os_index\n+// matches 'index'.\n+hwloc_obj_t GetHWLocTypeIndex(hwloc_obj_type_t tp, int index) {\n+  auto* topology = GetHWLocTopology();\n+  if (!topology) {\n+    return nullptr;\n+  }\n+\n+  if (index < 0) {\n+    return nullptr;\n+  }\n+\n+  hwloc_obj_t obj = nullptr;\n+  while ((obj = hwloc_get_next_obj_by_type(topology, tp, obj)) != nullptr) {\n+    if (obj->os_index == index) {\n+      break;\n+    }\n+  }\n+  return obj;\n+}\n+\n+struct HWLocBitmapDeleter {\n+  void operator()(hwloc_bitmap_t bitmap) const { hwloc_bitmap_free(bitmap); }\n+};\n+\n+auto AllocateBitmap() {\n+  return std::unique_ptr<std::remove_pointer_t<hwloc_bitmap_t>,\n+                         HWLocBitmapDeleter>(hwloc_bitmap_alloc());\n+}\n+}  // namespace\n+\n+bool NUMAEnabled() { return NUMANumNodes() > 1; }\n+\n+int NUMANumNodes() {\n+  static int num_numanodes = 1;\n+  static absl::once_flag init_once;\n+  absl::call_once(init_once, [] {\n+    auto* topology = GetHWLocTopology();\n+    if (!topology) {\n+      return;\n+    }\n+    num_numanodes = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_NUMANODE);\n+    if (num_numanodes < 1) {\n+      LOG(ERROR) << \"Unknown number of NUMA nodes (got \" << num_numanodes\n+                 << \"), assuming 1.\";\n+      num_numanodes = 1;\n+    }\n+  });\n+  return num_numanodes;\n+}\n+\n+void NUMASetThreadNodeAffinity(int node) {\n+  if (node == kNUMANoAffinity) {\n+    return;\n+  }\n+\n+  auto* topology = GetHWLocTopology();\n+  if (!topology) {\n+    return;\n+  }\n+\n+  // Find the corresponding NUMA node topology object.\n+  hwloc_obj_t obj = GetHWLocTypeIndex(HWLOC_OBJ_NUMANODE, node);\n+  if (!obj) {\n+    LOG(ERROR) << \"Could not find hwloc NUMA node \" << node;\n+    return;\n+  }\n+\n+  if (hwloc_set_cpubind(topology, obj->cpuset,\n+                        HWLOC_CPUBIND_THREAD | HWLOC_CPUBIND_STRICT)) {\n+    LOG(ERROR).WithPerror() << \"Call to hwloc_set_cpubind() failed\";\n+  }\n+}\n+\n+int NUMAGetThreadNodeAffinity() {\n+  auto* topology = GetHWLocTopology();\n+  if (!topology) {\n+    return kNUMANoAffinity;\n+  }\n+\n+  auto thread_cpuset = AllocateBitmap();\n+  if (!thread_cpuset) {\n+    LOG(ERROR) << \"Call to hwloc_bitmap_alloc() failed\";\n+    return kNUMANoAffinity;\n+  }\n+\n+  if (hwloc_get_cpubind(topology, thread_cpuset.get(), HWLOC_CPUBIND_THREAD)) {\n+    LOG(ERROR).WithPerror() << \"Call to hwloc_get_cpubind() failed\";\n+    return kNUMANoAffinity;\n+  }\n+\n+  hwloc_obj_t obj = nullptr;\n+  // Return the first NUMA node whose cpuset is a (non-proper) superset of\n+  // that of the current thread.\n+  while ((obj = hwloc_get_next_obj_by_type(topology, HWLOC_OBJ_NUMANODE,\n+                                           obj)) != nullptr) {\n+    if (hwloc_bitmap_isincluded(thread_cpuset.get(), obj->cpuset)) {\n+      break;\n+    }\n+  }\n+  return obj ? obj->os_index : kNUMANoAffinity;\n+}\n+\n+void* NUMAMalloc(int node, size_t size, int minimum_alignment) {\n+  if (node != kNUMANoAffinity) {\n+    if (auto* topology = GetHWLocTopology()) {\n+      hwloc_obj_t numa_node = GetHWLocTypeIndex(HWLOC_OBJ_NUMANODE, node);\n+      if (numa_node) {\n+        return hwloc_alloc_membind(topology, size, numa_node->nodeset,\n+                                   HWLOC_MEMBIND_BIND, HWLOC_MEMBIND_BYNODESET);\n+      }\n+      LOG(ERROR) << \"Failed to find hwloc NUMA node \" << node;\n+    }\n+  }\n+  return ::tsl::port::AlignedMalloc(size, minimum_alignment);\n+}\n+\n+void NUMAFree(void* ptr, size_t size) {\n+  auto* topology = GetHWLocTopology();\n+  if (!topology) {\n+    ::tsl::port::Free(ptr);\n+    return;\n+  }\n+  hwloc_free(topology, ptr, size);\n+}\n+\n+int NUMAGetMemAffinity(const void* ptr) {\n+  if (!ptr) {\n+    return kNUMANoAffinity;\n+  }\n+\n+  auto* topology = GetHWLocTopology();\n+  if (!topology) {\n+    return kNUMANoAffinity;\n+  }\n+\n+  auto nodeset = AllocateBitmap();\n+  if (!nodeset) {\n+    LOG(ERROR) << \"Call to hwloc_bitmap_alloc() failed\";\n+    return kNUMANoAffinity;\n+  }\n+\n+  if (hwloc_get_area_memlocation(topology, ptr, 4, nodeset.get(),\n+                                 HWLOC_MEMBIND_BYNODESET)) {\n+    LOG(ERROR) << \"Failed call to hwloc_get_area_memlocation.\";\n+    return kNUMANoAffinity;\n+  }\n+\n+  hwloc_obj_t obj = nullptr;\n+  while ((obj = hwloc_get_next_obj_by_type(topology, HWLOC_OBJ_NUMANODE,\n+                                           obj)) != nullptr) {\n+    if (hwloc_bitmap_isincluded(nodeset.get(), obj->nodeset)) {\n+      break;\n+    }\n+  }\n+  return obj ? obj->os_index : kNUMANoAffinity;\n+}\n+\n+}  // namespace port\n+}  // namespace tsl"
        },
        {
            "sha": "616c3ae57c5ded733eca03c90c2a89d67230e47a",
            "filename": "third_party/xla/xla/tsl/platform/numa_noop.cc",
            "status": "added",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fnuma_noop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fnuma_noop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fnuma_noop.cc?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -0,0 +1,41 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstddef>\n+\n+#include \"tsl/platform/mem.h\"\n+#include \"tsl/platform/numa.h\"\n+\n+namespace tsl {\n+namespace port {\n+\n+bool NUMAEnabled() { return false; }\n+\n+int NUMANumNodes() { return 1; }\n+\n+void NUMASetThreadNodeAffinity(int node) {}\n+\n+int NUMAGetThreadNodeAffinity() { return kNUMANoAffinity; }\n+\n+void* NUMAMalloc(int node, size_t size, int minimum_alignment) {\n+  return ::tsl::port::AlignedMalloc(size, minimum_alignment);\n+}\n+\n+void NUMAFree(void* ptr, size_t size) { ::tsl::port::Free(ptr); }\n+\n+int NUMAGetMemAffinity(const void* ptr) { return kNUMANoAffinity; }\n+\n+}  // namespace port\n+}  // namespace tsl"
        },
        {
            "sha": "41dee8607da97059a6744341311f47d1e9425b56",
            "filename": "third_party/xla/xla/tsl/platform/windows/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fwindows%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fwindows%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fwindows%2FBUILD?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -174,6 +174,7 @@ cc_library(\n     name = \"platform_port\",\n     srcs = [\n         \"port.cc\",\n+        \"//xla/tsl/platform:numa_noop.cc\",\n         \"@local_tsl//tsl/platform:cpu_info.cc\",\n     ],\n     hdrs = ["
        },
        {
            "sha": "d75627d1069f72dd1e2e9126e61a7bcecc83a48d",
            "filename": "third_party/xla/xla/tsl/platform/windows/port.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fwindows%2Fport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8cf42017ecbd1a3316c0a6d2458740516f21e0b7/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fwindows%2Fport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fwindows%2Fport.cc?ref=8cf42017ecbd1a3316c0a6d2458740516f21e0b7",
            "patch": "@@ -105,25 +105,6 @@ int GetCurrentCPU() {\n   return GetCurrentProcessorNumber();\n }\n \n-bool NUMAEnabled() {\n-  // Not yet implemented: coming soon.\n-  return false;\n-}\n-\n-int NUMANumNodes() { return 1; }\n-\n-void NUMASetThreadNodeAffinity(int node) {}\n-\n-int NUMAGetThreadNodeAffinity() { return kNUMANoAffinity; }\n-\n-void* NUMAMalloc(int node, size_t size, int minimum_alignment) {\n-  return tsl::port::AlignedMalloc(size, minimum_alignment);\n-}\n-\n-void NUMAFree(void* ptr, size_t size) { tsl::port::Free(ptr); }\n-\n-int NUMAGetMemAffinity(const void* addr) { return kNUMANoAffinity; }\n-\n bool Snappy_Compress(const char* input, size_t length, string* output) {\n #ifdef TF_USE_SNAPPY\n   output->resize(snappy::MaxCompressedLength(length));"
        }
    ],
    "stats": {
        "total": 436,
        "additions": 263,
        "deletions": 173
    }
}