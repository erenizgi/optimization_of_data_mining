{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Add simple multimem one-shot example.\n\nPiperOrigin-RevId: 824508652",
    "sha": "0e809d4bc80b613481eceb74e08371f1423c2610",
    "files": [
        {
            "sha": "4bc56f73840af8fee3d6d5870971a9447c4f11e8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_remote_access.mlir",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_remote_access.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_remote_access.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_remote_access.mlir?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -15,20 +15,17 @@ tt.func @get_peer_ptr(\n   %arg0: !tt.ptr<i64>, %peer_id: i64, %metadata: !tt.ptr<i64>\n ) -> !tt.ptr<i64> {\n   // CHECK-NOT: triton_xla.get_peer_ptr\n-  // Offset to local_buffer_root_ptrs.\n-  // CHECK: %c72_i64 = arith.constant 72 : i64\n-\n   // Byte size of a pointer.\n-  // CHECK-NEXT: %c8_i64 = arith.constant 8 : i64\n+  // CHECK: %c8_i64 = arith.constant 8 : i64\n \n   // Load metadata->rank\n   // CHECK-NEXT: %0 = tt.load %arg2 : !tt.ptr<i64>\n \n   // Calculate offset to current base pointer.\n   // CHECK-NEXT: %1 = arith.muli %0, %c8_i64 : i64\n \n-  // Load metadata->local_buffer_root_ptrs[metadata->rank].\n-  // CHECK-NEXT: %2 = arith.addi %1, %c72_i64 : i64\n+  // Load metadata->buffer_root_ptrs[metadata->rank].\n+  // CHECK-NEXT: %2 = arith.addi %1, %c8_i64 : i64\n   // CHECK-NEXT: %3 = tt.addptr %arg2, %2 : !tt.ptr<i64>, i64\n   // CHECK-NEXT: %4 = tt.load %3 : !tt.ptr<i64>\n \n@@ -38,13 +35,13 @@ tt.func @get_peer_ptr(\n \n   // Calculate offset to peer base pointer.\n   // CHECK-NEXT: %7 = arith.muli %arg1, %c8_i64 : i64\n-  // CHECK-NEXT: %8 = arith.addi %7, %c72_i64 : i64\n+  // CHECK-NEXT: %8 = arith.addi %7, %c8_i64 : i64\n \n-  // Load metadata->local_buffer_root_ptrs[peer_id].\n+  // Load metadata->buffer_root_ptrs[peer_id].\n   // CHECK-NEXT: %9 = tt.addptr %arg2, %8 : !tt.ptr<i64>, i64\n   // CHECK-NEXT: %10 = tt.load %9 : !tt.ptr<i64>\n \n-  // Load metadata->local_buffer_root_ptrs[peer_id] + offset.\n+  // Load metadata->buffer_root_ptrs[peer_id] + offset.\n   // CHECK-NEXT: %11 = arith.addi %10, %6 : i64\n   // CHECK-NEXT: %12 = tt.int_to_ptr %11 : i64 -> !tt.ptr<i64>\n   // CHECK-NEXT: tt.return %12 : !tt.ptr<i64>"
        },
        {
            "sha": "6a56ffa0fb475611f4ac30dc9ba875141fe83594",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_remote_access_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -94,9 +94,9 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n   // 1. Load metadata->rank.\n   Value current_rank_load_op = builder.create<GetRankOp>(metadata);\n \n-  // 2. Load metadata->local_buffer_root_ptrs[metadata->rank].\n+  // 2. Load metadata->buffer_root_ptrs[metadata->rank].\n   Value local_buffers_ptrs_offset = builder.create<arith::ConstantIntOp>(\n-      type_i64, offsetof(CollectiveKernelMetadata, local_buffer_root_ptrs));\n+      type_i64, offsetof(CollectiveKernelMetadata, buffer_root_ptrs));\n \n   Value rank_offset =\n       builder.create<arith::ExtUIOp>(type_i64, current_rank_load_op);\n@@ -116,13 +116,13 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n       /*isVolatile=*/rewriter.getBoolAttr(false));\n \n   // 3. Calculate offset =\n-  //      address - metadata->local_buffer_root_ptrs[metadata->rank].\n+  //      address - metadata->buffer_root_ptrs[metadata->rank].\n   Value current_range_address_int =\n       builder.create<PtrToIntOp>(type_i64, address);\n   Value offsetInt = builder.create<arith::SubIOp>(current_range_address_int,\n                                                   current_range_address_value);\n \n-  // 4. Load metadata->local_buffer_root_ptrs[peer_id].\n+  // 4. Load metadata->buffer_root_ptrs[peer_id].\n   Value peer_index = builder.create<arith::ExtUIOp>(type_i64, peer_id);\n   Value peer_index_offset_bytes =\n       builder.create<arith::MulIOp>(peer_index, pointer_size_bytes_const);"
        },
        {
            "sha": "c222417ba54d70220c313bd424b83d1400024746",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -2320,8 +2320,10 @@ xla_test(\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n+        \"//xla/stream_executor/gpu:gpu_executor_header\",\n         \"//xla/stream_executor/gpu:gpu_init\",\n         \"//xla/stream_executor/host:host_platform\",\n         \"//xla/tests:literal_test_util\","
        },
        {
            "sha": "287288ca676a0f92c46a376ce74c07b628b10832",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -58,6 +58,7 @@ class TagRegistry {\n  public:\n   static constexpr auto kOneShot = Impl<AllReduceStrategy::kOneShot>{};\n   static constexpr auto kTwoShot = Impl<AllReduceStrategy::kTwoShot>{};\n+  static constexpr auto kMultimem = Impl<AllReduceStrategy::kMultimem>{};\n };\n \n // Static set of supported kernel tags.\n@@ -182,7 +183,8 @@ bool IsAllReduceKernelSupported(int64_t num_ranks, int64_t num_elements,\n     return false;\n   }\n   const int64_t alignment_requirement =\n-      all_reduce_strategy == AllReduceStrategy::kOneShot\n+      all_reduce_strategy == AllReduceStrategy::kOneShot ||\n+              all_reduce_strategy == AllReduceStrategy::kMultimem\n           ? se::gpu::kNumElementsPerThread\n           : se::gpu::kNumElementsPerThread * num_ranks;\n \n@@ -232,6 +234,8 @@ absl::Status RunAllReduceKernel(\n         return launch_kernel_impl(tag_registry.kOneShot);\n       case AllReduceStrategy::kTwoShot:\n         return launch_kernel_impl(tag_registry.kTwoShot);\n+      case AllReduceStrategy::kMultimem:\n+        return launch_kernel_impl(tag_registry.kMultimem);\n     }\n   };\n "
        },
        {
            "sha": "f8c5cc4d5daa997729f961f4caf4a1e8dd622937",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_test.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 7,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -42,13 +42,14 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n+#include \"xla/stream_executor/gpu/gpu_executor.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tests/literal_test_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n@@ -94,6 +95,19 @@ class AllReduceKernelTest : public ::testing::Test,\n     TF_RETURN_IF_ERROR(executors[0]->EnablePeerAccessTo(executors[1]));\n     TF_RETURN_IF_ERROR(executors[1]->EnablePeerAccessTo(executors[0]));\n \n+    std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>\n+        multicast_memory;\n+    if (params_.all_reduce_strategy == AllReduceStrategy::kMultimem) {\n+      TF_ASSIGN_OR_RETURN(\n+          multicast_memory,\n+          dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[0])\n+              ->CreateMulticastMemory(num_elements * sizeof(T), num_ranks));\n+\n+      for (int i = 0; i < num_ranks; ++i) {\n+        TF_RETURN_IF_ERROR(multicast_memory->SubscribeDevice(i));\n+      }\n+    }\n+\n     std::vector<std::unique_ptr<se::Stream>> streams;\n     std::vector<se::DeviceMemoryBase> allocated_buffers;\n     std::vector<se::DeviceMemoryBase> local_input_buffers;\n@@ -115,7 +129,8 @@ class AllReduceKernelTest : public ::testing::Test,\n           /*local_input_buffer_size=*/aligned_input_size +\n           /*data_buffer_size=*/aligned_input_size +\n           /*signal_buffer_size=*/aligned_signal_size;\n-      allocated_buffers.emplace_back(executor->AllocateArray<T>(total_size));\n+      allocated_buffers.emplace_back(executor->AllocateArray<T>(\n+          total_size, static_cast<int64_t>(stream_executor::MemoryType::kP2P)));\n       local_input_buffers.emplace_back(\n           allocated_buffers[i].GetByteSlice(0, aligned_input_size));\n       TF_RET_CHECK(!local_input_buffers[i].is_null());\n@@ -140,10 +155,19 @@ class AllReduceKernelTest : public ::testing::Test,\n       metadata.rank = i;\n \n       for (int j = 0; j < num_ranks; ++j) {\n-        // One-Shot all-reduce doesn't use an input buffer from the peers.\n-        metadata.buffer_root_ptrs[j] = 0;\n-        metadata.local_buffer_root_ptrs[j] =\n-            (uint64_t)allocated_buffers[j].opaque();\n+        metadata.buffer_root_ptrs[j] = (uint64_t)allocated_buffers[j].opaque();\n+      }\n+\n+      if (params_.all_reduce_strategy == AllReduceStrategy::kMultimem) {\n+        stream_executor::gpu::GpuExecutor* gpu_executor =\n+            dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[i]);\n+        TF_RET_CHECK(gpu_executor != nullptr);\n+        TF_ASSIGN_OR_RETURN(void* mapped_memory,\n+                            multicast_memory->MapMemory(\n+                                allocated_buffers[i].opaque(), gpu_executor));\n+        metadata.multicast_buffer_ptr = (uint64_t)mapped_memory;\n+      } else {\n+        metadata.multicast_buffer_ptr = 0;\n       }\n \n       metadata_buffers.emplace_back(executors[i]->AllocateArray<uint64_t>(\n@@ -196,6 +220,8 @@ class AllReduceKernelTest : public ::testing::Test,\n \n   int64_t num_elements() const { return params_.num_elements; }\n \n+  AllReduceStrategy strategy() const { return params_.all_reduce_strategy; }\n+\n  private:\n   TestParams params_;\n };\n@@ -205,6 +231,11 @@ TEST_P(AllReduceKernelTest, KernelTestAddF32) {\n \n   std::vector<se::StreamExecutor*> executors = {GetGpuExecutor(0),\n                                                 GetGpuExecutor(1)};\n+  if (strategy() == AllReduceStrategy::kMultimem &&\n+      !dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[0])\n+           ->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Multimem not supported on this device.\";\n+  }\n \n   if (!executors[0]->CanEnablePeerAccessTo(executors[1])) {\n     GTEST_SKIP() << \"Test requires direct peer memory access between devices.\";\n@@ -238,6 +269,9 @@ TEST_P(AllReduceKernelTest, KernelTestAddF32) {\n }\n \n TEST_P(AllReduceKernelTest, KernelTestAddBF16) {\n+  if (strategy() == AllReduceStrategy::kMultimem) {\n+    GTEST_SKIP() << \"Multimem does not support BF16.\";\n+  }\n   constexpr int64_t kNumRanks = 2;\n \n   std::vector<se::StreamExecutor*> executors = {GetGpuExecutor(0),\n@@ -271,6 +305,9 @@ TEST_P(AllReduceKernelTest, KernelTestAddBF16) {\n }\n \n TEST_P(AllReduceKernelTest, KernelTestOrPred) {\n+  if (strategy() == AllReduceStrategy::kMultimem) {\n+    GTEST_SKIP() << \"Multimem does not support predicates.\";\n+  }\n   constexpr int64_t kNumRanks = 2;\n \n   std::vector<se::StreamExecutor*> executors = {GetGpuExecutor(0),\n@@ -305,6 +342,9 @@ TEST_P(AllReduceKernelTest, KernelTestOrPred) {\n }\n \n TEST_P(AllReduceKernelTest, KernelTestAddPred_Unsupported) {\n+  if (strategy() == AllReduceStrategy::kMultimem) {\n+    GTEST_SKIP() << \"Multimem does not support predicates.\";\n+  }\n   constexpr int64_t kNumRanks = 2;\n   std::vector<se::StreamExecutor*> executors = {GetGpuExecutor(0),\n                                                 GetGpuExecutor(1)};\n@@ -327,7 +367,8 @@ INSTANTIATE_TEST_SUITE_P(\n     AllReduceKernelTest, AllReduceKernelTest,\n     ::testing::ConvertGenerator(\n         ::testing::Combine(::testing::Values(AllReduceStrategy::kOneShot,\n-                                             AllReduceStrategy::kTwoShot),\n+                                             AllReduceStrategy::kTwoShot,\n+                                             AllReduceStrategy::kMultimem),\n                            ::testing::Values(128000, 124000)),\n         [](const std::tuple<AllReduceStrategy, int64_t>& params) {\n           return TestParams{std::get<0>(params), std::get<1>(params)};"
        },
        {
            "sha": "89a7c84cc6ccc1985a4ff07c4b400670182c6c59",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -97,6 +97,8 @@ int64_t GetMaxSupportedAllReduceSizeBytes(AllReduceStrategy strategy) {\n       return kMaxOneShotAllReduceSizeBytes;\n     case AllReduceStrategy::kTwoShot:\n       return kMaxTwoShotAllReduceSizeBytes;\n+    case AllReduceStrategy::kMultimem:\n+      return kMaxTwoShotAllReduceSizeBytes;\n   }\n }\n \n@@ -152,7 +154,6 @@ int64_t CollectiveKernelThunk::GetInputSizeBytes() const {\n \n struct BaseRangePtrRendezvousValue {\n   RankId rank;\n-  se::DeviceMemoryBase locally_allocated_buffer_ptr;\n   se::DeviceMemoryBase buffer_ptr;\n \n   bool operator<(const BaseRangePtrRendezvousValue& other) const {\n@@ -170,11 +171,7 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n       << \"Device \" << params.collective_params->global_device_id\n       << \"is not in the clique.\";\n   rendezvous_value.rank = rank.value();\n-  rendezvous_value.locally_allocated_buffer_ptr = state.local_buffer.memory();\n-  TF_ASSIGN_OR_RETURN(rendezvous_value.buffer_ptr,\n-                      params.executor->GetMemoryRange(\n-                          params.buffer_allocations->GetDeviceAddress(\n-                              buffers_[0].source_buffer)));\n+  rendezvous_value.buffer_ptr = state.local_buffer.memory();\n \n   auto rendezvous_fn =\n       [](absl::Span<const BaseRangePtrRendezvousValue* const> values) {\n@@ -207,13 +204,13 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n   CollectiveKernelMetadata metadata;\n   metadata.rank = rank.value().value();\n   for (int i = 0; i < rendezvous_values->size(); ++i) {\n-    metadata.local_buffer_root_ptrs[i] =\n-        (uint64_t)rendezvous_values->at(i)\n-            .locally_allocated_buffer_ptr.opaque();\n     metadata.buffer_root_ptrs[i] =\n         (uint64_t)rendezvous_values->at(i).buffer_ptr.opaque();\n   }\n \n+  // TODO(patrios): Add multicast setup.\n+  metadata.multicast_buffer_ptr = 0;\n+\n   se::DeviceMemoryBase metadata_ptr =\n       params.executor->Allocate(sizeof(CollectiveKernelMetadata), 0);\n   TF_RETURN_IF_ERROR(params.stream->Memcpy(&metadata_ptr, (void*)&metadata,"
        },
        {
            "sha": "539ef191707a10b4c48a1a2702629ebd083c16d9",
            "filename": "third_party/xla/xla/stream_executor/cuda/all_reduce_kernel_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fall_reduce_kernel_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fall_reduce_kernel_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fall_reduce_kernel_cuda.cc?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -91,6 +91,9 @@ __device__ __forceinline__ void WaitSignalFlag<PlatformType::CUDA>(\n REGISTER_ALL_REDUCE_KERNEL(AddBF16, xla::bfloat16, __nv_bfloat16, SUM);\n REGISTER_ALL_REDUCE_KERNEL(AddF32, float, float, SUM);\n \n+// Multimem so far supported only for f32.\n+REGISTER_ALL_REDUCE_KERNEL_IMPL(AddF32, float, float, SUM, kMultimem);\n+\n // AllReduce doesn't have a corresponding reduction kind for logical operations.\n // NCCL uses MAX and MIN on uint8_t for logical operations.\n REGISTER_ALL_REDUCE_KERNEL(OrPRED, bool, uint8_t, MAX);"
        },
        {
            "sha": "2bc23b4d713f7d73b401d7c84ca6377631718f06",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -190,7 +190,9 @@ class CudaExecutor : public GpuExecutor {\n   // Returns a handle to the given memory if it was allocated with VMM API.\n   absl::StatusOr<VmmMemoryHandle> RetainVmmMemoryHandle(void* ptr);\n \n-  bool is_multicast_supported() const { return is_multicast_supported_; }\n+  bool is_multicast_supported() const override {\n+    return is_multicast_supported_;\n+  }\n \n  private:\n   absl::Status VmmDeallocateMemory(void* ptr);"
        },
        {
            "sha": "da3065502d51d243df64eb2d69da7774de51b4c3",
            "filename": "third_party/xla/xla/stream_executor/gpu/all_reduce_kernel.h",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel.h?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -25,9 +25,18 @@ limitations under the License.\n \n namespace stream_executor::gpu {\n \n+// Strategy for performing an all-reduce.\n enum class AllReduceStrategy : uint32_t {\n+  // With one-shot strategy all GPUs gathers and reduces data from all peer\n+  // GPUs.\n   kOneShot,\n+  // With two-shot strategy each GPU gathers and reduces only a part of the\n+  // data in the first shot, as a second shot it gathers peer GPUs results to\n+  // construct a final result.\n   kTwoShot,\n+  // With multimem strategy single GPU uses multimem instructions to perform\n+  // reduce+broadcast in one-shot.\n+  kMultimem,\n };\n \n template <typename Sink>\n@@ -39,6 +48,9 @@ void AbslStringify(Sink& sink, AllReduceStrategy strategy) {\n     case AllReduceStrategy::kTwoShot:\n       sink.Append(\"kTwoShot\");\n       break;\n+    case AllReduceStrategy::kMultimem:\n+      sink.Append(\"kMultimem\");\n+      break;\n   }\n }\n "
        },
        {
            "sha": "ce3ee061a8754681f53f17711cf79dceb01c5472",
            "filename": "third_party/xla/xla/stream_executor/gpu/all_reduce_kernel_lib.cu.h",
            "status": "modified",
            "additions": 92,
            "deletions": 3,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -87,11 +87,19 @@ __device__ __forceinline__ void VecOp(Vec<T>& res, const Vec<T>& vec) {\n template <typename T>\n __device__ __forceinline__ RestrictedPtr<T> GetPeerPtr(\n     void* ptr, int64_t peer_rank, const CollectiveKernelMetadata& metadata) {\n-  uint64_t current_base = metadata.local_buffer_root_ptrs[metadata.rank];\n+  uint64_t current_base = metadata.buffer_root_ptrs[metadata.rank];\n   uint64_t offset = (uint64_t)ptr - current_base;\n \n-  return (RestrictedPtr<T>)(metadata.local_buffer_root_ptrs[peer_rank] +\n-                            offset);\n+  return (RestrictedPtr<T>)(metadata.buffer_root_ptrs[peer_rank] + offset);\n+}\n+\n+template <typename T>\n+__device__ __forceinline__ RestrictedPtr<T> GetMultimemPtr(\n+    void* ptr, const CollectiveKernelMetadata& metadata) {\n+  uint64_t current_base = metadata.buffer_root_ptrs[metadata.rank];\n+  uint64_t offset = (uint64_t)ptr - current_base;\n+\n+  return (RestrictedPtr<T>)(metadata.multicast_buffer_ptr + offset);\n }\n \n template <PlatformType T = PlatformType::NOGPU>\n@@ -164,6 +172,79 @@ __device__ __forceinline__ void OneShotAllReduceKernelImpl(\n   }\n }\n \n+#if __CUDA_ARCH__ >= 900\n+\n+// This is the simplest implementation of all-reduce with multimem instructions.\n+// Right now all devices are copying their data to the remote buffer after\n+// which, the first device performs the reduce and broadcast operations using\n+// multimem instructions.\n+template <typename T, xla::ReductionKind ReductionKindT,\n+          PlatformType PlatformT = PlatformType::NOGPU>\n+__device__ __forceinline__ void MultimemAllReduceKernelImpl(\n+    const AllReduceKernelParams<T>& args) {\n+  if (!std::is_same_v<T, float>) {\n+    assert(false &&\n+           \"Multimem all-reduce strategy is only supported for float.\");\n+  }\n+\n+  __shared__ std::array<RestrictedPtr<uint32_t>, kMaxNumAllReduceInputPtrs>\n+      signal_flags_buffers;\n+\n+  if (threadIdx.x < kMaxNumAllReduceInputPtrs) {\n+    signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n+        args.symmetric_signal_ptrs, threadIdx.x, *args.metadata);\n+  }\n+\n+  int64_t offset =\n+      kNumElementsPerThread * (blockIdx.x * blockDim.x + threadIdx.x);\n+  int64_t stride = kNumElementsPerThread * blockDim.x * gridDim.x;\n+\n+  // Copy data from local input buffer to remote input buffer.\n+  for (int i = offset; i < args.num_elements; i += stride) {\n+    VecStore(args.symmetric_input_ptrs + i, VecLoad(args.input_buffer + i));\n+  }\n+\n+  SyncRemoteBlocks<PlatformT>(signal_flags_buffers, args.rank, args.num_ranks,\n+                              args.signal_value);\n+  __syncthreads();\n+\n+  RestrictedPtr<T> multimem_ptr =\n+      GetMultimemPtr<T>(args.symmetric_input_ptrs, *args.metadata);\n+  if (args.metadata->rank == 0) {\n+    for (int i = offset; i < args.num_elements; i += stride) {\n+      T* multimem_element_ptr = multimem_ptr + i;\n+\n+      // Reduce\n+      Vec<T> vec;\n+      asm volatile(\n+          \"multimem.ld_reduce.relaxed.sys.global.add.v4.f32 {%0,%1,%2,%3}, \"\n+          \"[%4];\"\n+          : \"=f\"(vec.data[0]), \"=f\"(vec.data[1]), \"=f\"(vec.data[2]),\n+            \"=f\"(vec.data[3])\n+          : \"l\"(multimem_element_ptr)\n+          : \"memory\");\n+\n+      // Broadcast\n+      asm volatile(\n+          \"multimem.st.relaxed.sys.global.v4.f32 [%0], {%1,%2,%3,%4};\" ::\"l\"(\n+              multimem_element_ptr),\n+          \"f\"(vec.data[0]), \"f\"(vec.data[1]), \"f\"(vec.data[2]), \"f\"(vec.data[3])\n+          : \"memory\");\n+    }\n+  }\n+\n+  __syncthreads();\n+  // Wait for all participants to receive the data.\n+  SyncRemoteBlocks<PlatformT>(signal_flags_buffers, args.rank, args.num_ranks,\n+                              args.signal_value + 1);\n+  __syncthreads();\n+\n+  for (int i = offset; i < args.num_elements; i += stride) {\n+    VecStore(args.output_buffer + i, VecLoad(args.symmetric_input_ptrs + i));\n+  }\n+}\n+#endif  // __CUDA_ARCH__ >= 900\n+\n template <typename T, xla::ReductionKind ReductionKindT,\n           PlatformType PlatformT = PlatformType::NOGPU>\n __device__ __forceinline__ void TwoShotAllReduceKernelImpl(\n@@ -278,6 +359,14 @@ __global__ void AllReduceKernelImpl(AllReduceKernelParams<T> args) {\n     OneShotAllReduceKernelImpl<T, ReductionKindT, PlatformT>(args);\n   } else if constexpr (kAllReduceStrategy == AllReduceStrategy::kTwoShot) {\n     TwoShotAllReduceKernelImpl<T, ReductionKindT, PlatformT>(args);\n+  } else if constexpr (kAllReduceStrategy == AllReduceStrategy::kMultimem) {\n+#if __CUDA_ARCH__ >= 900\n+    MultimemAllReduceKernelImpl<T, ReductionKindT, PlatformT>(args);\n+#else\n+    assert(false &&\n+           \"Multimem all-reduce strategy is not supported on this \"\n+           \"architecture.\");\n+#endif\n   } else {\n     assert(false && \"Unsupported all-reduce strategy\");\n   }"
        },
        {
            "sha": "b629c47e88624fca7fa9366394e79a25eecd863c",
            "filename": "third_party/xla/xla/stream_executor/gpu/collective_kernel_metadata.h",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -25,16 +25,14 @@ limitations under the License.\n // allocated by the buffer assignment and allows kernel to address input and\n // output buffers. The second one is used for buffers allocated within the\n // collective kernel thunk.\n-// TODO(patrios): Unify two root pointers once symmetric memory allocator will\n-// be implemented.\n struct CollectiveKernelMetadata {\n   constexpr static int kMaxNumDevices = 8;\n   int64_t rank;\n-  // Root pointer for buffers allocated by the buffer assignment.\n+  // Root pointer for buffers.\n   int64_t buffer_root_ptrs[kMaxNumDevices];\n \n-  // Root pointer for buffers allocated by the collective kernel thunk.\n-  int64_t local_buffer_root_ptrs[kMaxNumDevices];\n+  // Root pointer for multicast buffer for current device.\n+  int64_t multicast_buffer_ptr;\n };\n \n #endif  // XLA_STREAM_EXECUTOR_GPU_COLLECTIVE_KERNEL_METADATA_H_"
        },
        {
            "sha": "d83cb60b414f9fb067ce6a2288d3389519982fd3",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_executor.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e809d4bc80b613481eceb74e08371f1423c2610/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h?ref=0e809d4bc80b613481eceb74e08371f1423c2610",
            "patch": "@@ -90,6 +90,8 @@ class GpuExecutor : public StreamExecutorCommon {\n         \"CreateMulticastMemory is not implemented.\");\n   };\n \n+  virtual bool is_multicast_supported() const { return false; }\n+\n  private:\n   // The device ordinal value that this executor was initialized with; recorded\n   // for use in getting device metadata. Immutable post-initialization."
        }
    ],
    "stats": {
        "total": 225,
        "additions": 186,
        "deletions": 39
    }
}