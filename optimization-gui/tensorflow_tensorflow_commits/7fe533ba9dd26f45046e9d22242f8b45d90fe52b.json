{
    "author": "GleasonK",
    "message": "[CHLO] Simplify bounded dynamism support in CHLO\n\nPiperOrigin-RevId: 840865690",
    "sha": "7fe533ba9dd26f45046e9d22242f8b45d90fe52b",
    "files": [
        {
            "sha": "adc1a07d777e1f84b4add1acfb072edb1e45f175",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 934,
            "deletions": 23,
            "changes": 957,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fe533ba9dd26f45046e9d22242f8b45d90fe52b/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fe533ba9dd26f45046e9d22242f8b45d90fe52b/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=7fe533ba9dd26f45046e9d22242f8b45d90fe52b",
            "patch": "@@ -1,3 +1,39 @@\n+diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel\n+--- stablehlo/BUILD.bazel\n++++ stablehlo/BUILD.bazel\n+@@ -1183,6 +1183,7 @@\n+         \":chlo_ops\",\n+         \":chlo_rewriters_inc_gen\",\n+         \":stablehlo_aggressive_simplification_inc_gen\",\n++        \":stablehlo_broadcast_lowering\",\n+         \":stablehlo_create_compatibility_expander_inc_gen\",\n+         \":stablehlo_create_complex_math_expander_inc_gen\",\n+         \":stablehlo_legalize_deprecated_ops_inc_gen\",\n+@@ -1922,6 +1923,24 @@\n+     ],\n+ )\n+ \n++cc_test(\n++    name = \"chlo_builder_test\",\n++    srcs = [\"stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\"],\n++    deps = [\n++        \":attr_type_builder_util\",\n++        \":chlo_builder\",\n++        \":func_builder\",\n++        \":mlir_builder\",\n++        \":register\",\n++        \":stablehlo_builder\",\n++        \":stablehlo_ops\",\n++        \"@llvm-project//mlir:IR\",\n++        \"@llvm-project//mlir:Support\",\n++        \"@llvm-project//third-party/unittest:gmock\",\n++        \"@llvm-project//third-party/unittest:gtest\",\n++    ],\n++)\n++\n+ gentbl_cc_library(\n+     name = \"func_builder_inc\",\n+     tbl_outs = {\n diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n --- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n +++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n@@ -33,6 +69,67 @@ diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalize\n      // If any of the output dimensions is 0, the tensor has no elements. In that\n      // case, we can just replace the reshape with an empty op.\n      if (llvm::is_contained(resultType.getShape(), 0)) {\n+diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp\n+--- stablehlo/stablehlo/dialect/Base.cpp\n++++ stablehlo/stablehlo/dialect/Base.cpp\n+@@ -29,6 +29,7 @@\n+ #include \"llvm/ADT/STLExtras.h\"\n+ #include \"llvm/ADT/Sequence.h\"\n+ #include \"llvm/ADT/SmallVector.h\"\n++#include \"llvm/Support/Casting.h\"\n+ #include \"llvm/Support/Debug.h\"\n+ #include \"llvm/Support/ErrorHandling.h\"\n+ #include \"mlir/Dialect/Quant/IR/QuantTypes.h\"\n+@@ -781,6 +782,14 @@\n+           numScales == rankedType.getDimSize(quantDim));\n+ }\n+ \n++bool isBoundedDynamic(Type type) {\n++  RankedTensorType rankedType = dyn_cast<RankedTensorType>(type);\n++  if (!rankedType) return false;\n++  auto boundedAttr =\n++      mlir::dyn_cast_if_present<BoundedAttrInterface>(rankedType.getEncoding());\n++  return boundedAttr != nullptr;\n++}\n++\n+ bool hasSingleBoundedDimension(Type type) {\n+   RankedTensorType rankedType = dyn_cast<RankedTensorType>(type);\n+   auto boundedAttr =\n+diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h\n+--- stablehlo/stablehlo/dialect/Base.h\n++++ stablehlo/stablehlo/dialect/Base.h\n+@@ -101,6 +101,9 @@\n+ // mentioned in the StableHLO specification.\n+ bool isValidQuantizedDimension(Type type);\n+ \n++// Returns true if the given type is a bounded dynamic tensor.\n++bool isBoundedDynamic(Type type);\n++\n+ // Returns true if the given type has a single bounded dimension.\n+ bool hasSingleBoundedDimension(Type type);\n+ \n+diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.cpp b/stablehlo/stablehlo/dialect/ChloOps.cpp\n+--- stablehlo/stablehlo/dialect/ChloOps.cpp\n++++ stablehlo/stablehlo/dialect/ChloOps.cpp\n+@@ -365,11 +365,14 @@\n+   Type elementType = op.getValue().getType();\n+   Type operandType = op.getOperand().getType();\n+   if (isa<UnrankedTensorType>(operandType)) {\n++    // TODO(b/326463552): Remove unranked dynamism from CHLO.\n+     inferredReturnShapes.emplace_back(elementType);\n+-  } else {\n+-    const auto& shape = cast<RankedTensorType>(operandType).getShape();\n+-    inferredReturnShapes.emplace_back(shape, elementType);\n+-  }\n++    return success();\n++  }\n++  auto rankedType = cast<RankedTensorType>(operandType);\n++  const auto& shape = rankedType.getShape();\n++  Attribute encoding = rankedType.getEncoding();\n++  inferredReturnShapes.emplace_back(shape, elementType, encoding);\n+   return success();\n+ }\n+ \n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp\n --- stablehlo/stablehlo/dialect/StablehloOps.cpp\n +++ stablehlo/stablehlo/dialect/StablehloOps.cpp\n@@ -118,6 +215,205 @@ diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/di\n  // PrecisionConfigAttr is a constraint attribute on ArrayAttrs.\n  // Create this class to allow for building this attr similar to other\n  // attributes.\n+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt b/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n+--- stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n++++ stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n+@@ -137,6 +137,7 @@\n+     set_target_properties(check-stablehlo-ci PROPERTIES FOLDER \"Tests\")\n+     add_unittest(check-stablehlo-ci \"unittests\"\n+       MlirBuilderTest.cpp\n++      ChloBuilderTest.cpp\n+       StablehloBuilderTest.cpp\n+       AttrTypeBuilderUtilTest.cpp\n+     )\n+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp\n+--- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp\n++++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp\n+@@ -31,5 +31,15 @@\n+ \n+ #include \"stablehlo/integrations/cpp/builder/ChloBuilder.cpp.inc\"\n+ \n++/////////////////\n++// MANUAL APIs\n++/////////////////\n++\n++MlirOp ConstantLike(MlirOp input, DenseElementsAttr val) {\n++  MlirBuilder& builder = input.getBuilder();\n++  auto splat_val = val.getSplatValue<TypedAttr>();\n++  return builder.create<chlo::ConstantLikeOp>(splat_val, input.getValue());\n++}\n++\n+ }  // namespace chlo\n+ }  // namespace mlir\n+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h\n+--- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h\n++++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h\n+@@ -19,6 +19,7 @@\n+ #include <cstdint>\n+ \n+ #include \"llvm/ADT/SmallVector.h\"\n++#include \"mlir/IR/BuiltinAttributes.h\"\n+ #include \"stablehlo/dialect/ChloOps.h\"\n+ #include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n+ \n+@@ -31,6 +32,12 @@\n+ \n+ #include \"stablehlo/integrations/cpp/builder/ChloBuilder.h.inc\"\n+ \n++/////////////////\n++// MANUAL APIs\n++/////////////////\n++\n++MlirOp ConstantLike(MlirOp input, DenseElementsAttr val);\n++\n+ }  // namespace chlo\n+ }  // namespace mlir\n+ \n+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\n+--- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\n++++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\n+@@ -0,0 +1,141 @@\n++/* Copyright 2025 The OpenXLA Authors.\n++\n++Licensed under the Apache License, Version 2.0 (the \"License\");\n++you may not use this file except in compliance with the License.\n++You may obtain a copy of the License at\n++\n++    http://www.apache.org/licenses/LICENSE-2.0\n++\n++Unless required by applicable law or agreed to in writing, software\n++distributed under the License is distributed on an \"AS IS\" BASIS,\n++WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n++See the License for the specific language governing permissions and\n++limitations under the License.\n++==============================================================================*/\n++\n++#include <string>\n++\n++#include \"mlir/IR/BuiltinAttributes.h\"\n++#include \"mlir/IR/BuiltinOps.h\"\n++#include \"mlir/IR/DialectRegistry.h\"\n++#include \"mlir/IR/MLIRContext.h\"\n++#include \"mlir/IR/OwningOpRef.h\"\n++#include \"mlir/IR/Types.h\"\n++#include \"mlir/IR/Verifier.h\"\n++#include \"mlir/Support/DebugStringHelper.h\"\n++#include \"mlir/Support/LLVM.h\"\n++#include \"stablehlo/dialect/Register.h\"\n++#include \"stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h\"\n++#include \"stablehlo/integrations/cpp/builder/ChloBuilder.h\"\n++#include \"stablehlo/integrations/cpp/builder/FuncBuilder.h\"\n++#include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n++#include \"testing/base/public/gunit.h\"\n++#include \"stablehlo/integrations/cpp/builder/StablehloBuilder.h\"\n++\n++namespace mlir {\n++namespace chlo {\n++\n++namespace {\n++\n++// Wrap a module builder and register the classes needed\n++class ChloModuleBuilder {\n++ public:\n++  ChloModuleBuilder()\n++      : context_(), module_builder_(context_, mlir::unknownLoc(context_)) {\n++    DialectRegistry registry;\n++    stablehlo::registerAllDialects(registry);\n++    context_.appendDialectRegistry(registry);\n++    context_.loadAllAvailableDialects();\n++  }\n++\n++  ModuleBuilder& get() { return module_builder_; }\n++  ModuleBuilder* operator->() { return &module_builder_; }\n++\n++ private:\n++  MLIRContext context_;\n++  ModuleBuilder module_builder_;\n++};\n++\n++// TODO: Make a FileCheck matcher\n++\n++}  // namespace\n++\n++TEST(ChloBuilderTest, SmokeTest) {\n++  std::string expected = R\"mlir(module {\n++  func.func @main(%arg0: tensor<2xi64>) -> tensor<2xi64> {\n++    %0 = chlo.constant dense<1> : tensor<i64>\n++    %1 = chlo.broadcast_add %arg0, %0 : (tensor<2xi64>, tensor<i64>) -> tensor<2xi64>\n++    return %1 : tensor<2xi64>\n++  }\n++})mlir\";\n++\n++  ChloModuleBuilder mb;\n++  {  // Build Main Func\n++    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n++    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n++    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);\n++    auto typeScalari64 = makeTensorType(mb->getContext(), {}, ElementType::I64);\n++    auto arg0 = func::Argument(fb, type2xi64);\n++    auto cst = Constant(fb, mlir::makeConstant(1L, typeScalari64));\n++    auto add = BroadcastAdd(arg0, cst);\n++    func::Return(fb, {add});\n++  }\n++\n++  OwningOpRef<ModuleOp> module = mb->build();\n++  EXPECT_TRUE(succeeded(mlir::verify(*module)));\n++  EXPECT_EQ(expected, debugString(*module));\n++}\n++\n++TEST(MlirBuilderTest, ConstantLike) {\n++  std::string expected = R\"mlir(module {\n++  func.func @main(%arg0: tensor<2xi64>) -> tensor<2xi64> {\n++    %0 = \"chlo.constant_like\"(%arg0) <{value = 1 : i64}> : (tensor<2xi64>) -> tensor<2xi64>\n++    return %0 : tensor<2xi64>\n++  }\n++})mlir\";\n++\n++  ChloModuleBuilder mb;\n++  {  // Build Main Func\n++    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n++    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n++    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);\n++    auto typeScalari64 = makeTensorType(mb->getContext(), {}, ElementType::I64);\n++    auto arg0 = func::Argument(fb, type2xi64);\n++    auto cst = ConstantLike(arg0, mlir::makeConstant(1L, typeScalari64));\n++    func::Return(fb, {cst});\n++  }\n++\n++  OwningOpRef<ModuleOp> module = mb->build();\n++  EXPECT_TRUE(succeeded(mlir::verify(*module)));\n++  EXPECT_EQ(expected, debugString(*module));\n++}\n++\n++TEST(MlirBuilderTest, ConstantLikeBounded) {\n++  std::string expected = R\"mlir(module {\n++  func.func @main(%arg0: tensor<2xi64>, %arg1: tensor<i32>) -> tensor<?xi32, #stablehlo.bounds<2>> {\n++    %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<2xi64>, tensor<i32>) -> tensor<?xi64, #stablehlo.bounds<2>>\n++    %1 = \"chlo.constant_like\"(%0) <{value = 1 : i32}> : (tensor<?xi64, #stablehlo.bounds<2>>) -> tensor<?xi32, #stablehlo.bounds<2>>\n++    return %1 : tensor<?xi32, #stablehlo.bounds<2>>\n++  }\n++})mlir\";\n++\n++  ChloModuleBuilder mb;\n++  {  // Build Main Func\n++    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n++    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n++    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);\n++    auto typei32 = makeTensorType(mb->getContext(), {}, ElementType::I32);\n++    auto arg0 = func::Argument(fb, type2xi64);\n++    auto arg1 = func::Argument(fb, typei32);\n++    auto sds = stablehlo::SetDimensionSize(arg0, arg1, 0);\n++    auto cst = ConstantLike(sds, mlir::makeConstant(1L, typei32));\n++    func::Return(fb, {cst});\n++  }\n++\n++  OwningOpRef<ModuleOp> module = mb->build();\n++  EXPECT_TRUE(succeeded(mlir::verify(*module)));\n++  EXPECT_EQ(expected, debugString(*module));\n++}\n++\n++}  // namespace chlo\n++}  // namespace mlir\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n --- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n +++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n@@ -140,6 +436,17 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp\n      }\n      for (auto& operand : params.operands) {\n        parameters.push_back(\n+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n+--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n++++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n+@@ -67,6 +67,7 @@\n+   MlirOp operand = input;\n+   auto inputType = mlir::cast<RankedTensorType>(input.getType());\n+   auto resultType = inputType.clone(resultElementType);\n++  if (inputType == resultType) return input;  // skip no-op convert\n+   if (isa<ComplexType>(inputType.getElementType()) &&\n+       !isa<ComplexType>(resultElementType)) {\n+     operand = stablehlo::Real(operand);\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n --- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n +++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n@@ -195,6 +502,392 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c\n  ////////\n  // Custom Attribute Tests\n  ////////\n+diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n+--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n++++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n+@@ -622,6 +622,10 @@\n+   func.return %result : tensor<complex<f32>>\n+ }\n+ \n++//////\n++// Broadcast binary elementwise ops tests are located in\n++// chlo_legalize_to_stablehlo_broadcast.mlir\n++\n+ // -----\n+ \n+ // Lower statically shaped `constant_like` to constant.\n+@@ -632,6 +636,24 @@\n+   %result = \"chlo.constant_like\"(%arg) { value = 3.2 : f32 }\n+       : (tensor<1x2xi64>) -> tensor<1x2xf32>\n+   func.return %result : tensor<1x2xf32>\n++}\n++\n++// -----\n++\n++// Lower dynamically shaped `constant_like` to broadcasted constant.\n++// CHECK-LABEL: constant_like_bounded_dynamic_shape\n++// CHECK-SAME: (%[[ARG0:.*]]: tensor<2xi64>, %[[ARG1:.*]]: tensor<i32>)\n++func.func @constant_like_bounded_dynamic_shape(%arg0: tensor<2xi64>, %arg1: tensor<i32>) -> tensor<?xi32, #stablehlo.bounds<2>> {\n++  %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<2xi64>, tensor<i32>) -> tensor<?xi64, #stablehlo.bounds<2>>\n++  // CHECK-NOT: chlo.constant_like\n++  // CHECK: %[[ARG0_DYN:.*]] = stablehlo.set_dimension_size %[[ARG0]], %[[ARG1]], dim = 0 : (tensor<2xi64>, tensor<i32>) -> tensor<?xi64, #stablehlo.bounds<2>>\n++  // CHECK: %[[CST:.*]] = stablehlo.constant dense<1> : tensor<i32>\n++  // CHECK-NEXT: %[[BCAST:.*]] = stablehlo.broadcast_in_dim %[[CST]], dims = [] : (tensor<i32>) -> tensor<2xi32>\n++  // CHECK-NEXT: %[[GDS:.*]] = stablehlo.get_dimension_size %[[ARG0_DYN]], dim = 0 : (tensor<?xi64, #stablehlo.bounds<2>>) -> tensor<i32>\n++  // CHECK-NEXT: %[[SDS:.*]] = stablehlo.set_dimension_size %[[BCAST]], %[[GDS]], dim = 0 : (tensor<2xi32>, tensor<i32>) -> tensor<?xi32, #stablehlo.bounds<2>>\n++  // CHECK-NEXT: return %[[SDS]] : tensor<?xi32, #stablehlo.bounds<2>>\n++  %1 = \"chlo.constant_like\"(%0) <{value = 1 : i32}> : (tensor<?xi64, #stablehlo.bounds<2>>) -> tensor<?xi32, #stablehlo.bounds<2>>\n++  return %1 : tensor<?xi32, #stablehlo.bounds<2>>\n+ }\n+ \n+ // -----\n+diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n+--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n++++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n+@@ -3,8 +3,8 @@\n+ // Check the non-broadcast case for each registered op, then just check a\n+ // representative op for detailed broadcast semantics.\n+ \n+-// CHECK-LABEL: @addWithoutBroadcast\n+-func.func @addWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @add_no_broadcast\n++func.func @add_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.add %arg0, %arg1\n+   %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -12,8 +12,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @addStaticBroadcastExpanding\n+-func.func @addStaticBroadcastExpanding(%arg0: tensor<4xf32>, %arg1: tensor<f32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @add_static_broadcast_expanding\n++func.func @add_static_broadcast_expanding(%arg0: tensor<4xf32>, %arg1: tensor<f32>) -> tensor<4xf32> {\n+   // CHECK:      %[[BROADCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n+   // CHECK-NEXT: stablehlo.add %arg0, %[[BROADCAST]]\n+   // CHECK-NOT: shape\n+@@ -23,8 +23,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @addStaticBroadcastSameRank\n+-func.func @addStaticBroadcastSameRank(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {\n++// CHECK-LABEL: @add_static_broadcast_same_rank\n++func.func @add_static_broadcast_same_rank(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {\n+   // CHECK:      %[[ARG0_B:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<1x4xf32>) -> tensor<4x4xf32>\n+   // CHECK-NEXT: %[[ARG1_B:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<4x1xf32>) -> tensor<4x4xf32>\n+   // CHECK-NEXT: stablehlo.add %[[ARG0_B]], %[[ARG1_B]] : tensor<4x4xf32>\n+@@ -35,11 +35,33 @@\n+ \n+ // -----\n+ \n+-\n+-// CHECK-LABEL: @dynamicBroadcast\n++// [<=10] x [<=10] => [<=10]\n++// CHECK-LABEL: func @add_bounded_dynamic_no_broadcast\n++func.func @add_bounded_dynamic_no_broadcast(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> tensor<?xf64, #stablehlo.bounds<10>> {\n++  // CHECK-NEXT: stablehlo.add %arg0, %arg1\n++  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<?xf64, #stablehlo.bounds<10>>) -> tensor<?xf64, #stablehlo.bounds<10>>\n++  return %0 : tensor<?xf64, #stablehlo.bounds<10>>\n++}\n++\n++// -----\n++\n++// [<=10] x [] => [<=10]\n++// CHECK-LABEL: func @add_bounded_dynamic_expanding\n++func.func @add_bounded_dynamic_expanding(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<f64>) -> tensor<?xf64, #stablehlo.bounds<10>> {\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.add %arg0, %[[RHS_BCAST_DYN]]\n++  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<f64>) -> tensor<?xf64, #stablehlo.bounds<10>>\n++  return %0 : tensor<?xf64, #stablehlo.bounds<10>>\n++}\n++\n++// -----\n++\n++// CHECK-LABEL: @add_dynamic_broadcast\n+ // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>\n+ // CHECK-SAME: %[[ARG1:.+]]: tensor<?x?xf32>\n+-func.func @dynamicBroadcast(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xf32> {\n++func.func @add_dynamic_broadcast(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xf32> {\n+   // CHECK-DAG:  %[[ARG0_S:.+]] = shape.shape_of %[[ARG0]]\n+   // CHECK-DAG:  %[[ARG1_S:.+]] = shape.shape_of %[[ARG1]]\n+   // CHECK-NEXT: %[[WITNESS:.+]] = shape.cstr_broadcastable %[[ARG0_S]], %[[ARG1_S]]\n+@@ -57,10 +79,10 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @dynamicBroadcastComplex\n++// CHECK-LABEL: @dynamic_broadcast_complex\n+ // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>\n+ // CHECK-SAME: %[[ARG1:.+]]: tensor<?x?xf32>\n+-func.func @dynamicBroadcastComplex(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xcomplex<f32>> {\n++func.func @dynamic_broadcast_complex(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xcomplex<f32>> {\n+   // CHECK-DAG:  %[[ARG0_S:.+]] = shape.shape_of %[[ARG0]]\n+   // CHECK-DAG:  %[[ARG1_S:.+]] = shape.shape_of %[[ARG1]]\n+   // CHECK-NEXT: %[[WITNESS:.+]] = shape.cstr_broadcastable %[[ARG0_S]], %[[ARG1_S]]\n+@@ -78,10 +100,10 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @dynamicBroadcastCompare\n++// CHECK-LABEL: @compare_dynamic_broadcast\n+ // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>\n+ // CHECK-SAME: %[[ARG1:.+]]: tensor<?x?xf32>\n+-func.func @dynamicBroadcastCompare(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xi1> {\n++func.func @compare_dynamic_broadcast(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xi1> {\n+   // CHECK-DAG: %[[ARG0_S:.+]] = shape.shape_of %[[ARG0]]\n+   // CHECK-DAG: %[[ARG1_S:.+]] = shape.shape_of %[[ARG1]]\n+   // CHECK: %[[WITNESS:.+]] = shape.cstr_broadcastable %[[ARG0_S]], %[[ARG1_S]]\n+@@ -191,8 +213,8 @@\n+ // -----\n+ \n+ // Verifies that broadcast_dimensions validity checks are valid.\n+-// CHECK-LABEL: @dynamicNonScalarBroadcastDimensions\n+-func.func @dynamicNonScalarBroadcastDimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n++// CHECK-LABEL: @dynamic_non_scalar_broadcast_dimensions\n++func.func @dynamic_non_scalar_broadcast_dimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n+   // CHECK: stablehlo.add\n+   %0 = chlo.broadcast_add %arg0, %arg1 {broadcast_dimensions =  array<i64: 1> } : (tensor<1x4xf32>, tensor<4xf32>) -> tensor<1x4xf32>\n+   func.return %0 : tensor<1x4xf32>\n+@@ -201,8 +223,8 @@\n+ // -----\n+ \n+ // Verifies that broadcast_dimensions validity checks are valid.\n+-// CHECK-LABEL: @dynamicNonScalarByScalarBroadcastDimensions\n+-func.func @dynamicNonScalarByScalarBroadcastDimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<f32>) -> tensor<1x4xf32> {\n++// CHECK-LABEL: @dynamic_non_scalar_by_scalar_broadcast_dimensions\n++func.func @dynamic_non_scalar_by_scalar_broadcast_dimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<f32>) -> tensor<1x4xf32> {\n+   // CHECK: stablehlo.add\n+   %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<1x4xf32>, tensor<f32>) -> tensor<1x4xf32>\n+   func.return %0 : tensor<1x4xf32>\n+@@ -211,7 +233,7 @@\n+ // -----\n+ \n+ // Verifies that invalid broadcast dimensions are rejected.\n+-func.func @dynamicNonScalarBroadcastDimensionsSizeMismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n++func.func @dynamic_non_scalar_broadcast_dimensions_size_mismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n+   // expected-warning @+2 {{unsupported non prefix-padded dynamic rank broadcast_dimensions}}\n+   // expected-error @+1 {{failed to legalize operation}}\n+   %0 = chlo.broadcast_add %arg0, %arg1 {broadcast_dimensions = array<i64: 1, 2>} : (tensor<1x4xf32>, tensor<4xf32>) -> tensor<1x4xf32>\n+@@ -221,7 +243,7 @@\n+ // -----\n+ \n+ // Verifies that invalid broadcast dimensions are rejected.\n+-func.func @dynamicNonScalarBroadcastDimensionsMismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n++func.func @dynamic_non_scalar_broadcast_dimensions_mismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n+   // expected-warning @+2 {{unsupported non prefix-padded dynamic rank broadcast_dimensions}}\n+   // expected-error @+1 {{failed to legalize operation}}\n+   %0 = chlo.broadcast_add %arg0, %arg1 {broadcast_dimensions = array<i64: 2>} : (tensor<1x4xf32>, tensor<4xf32>) -> tensor<1x4xf32>\n+@@ -232,8 +254,8 @@\n+ // Note that broadcast_add is used as a proxy for all of the template\n+ // expansions. Tests below merely verify that the op has an expansion.\n+ \n+-// CHECK-LABEL: @andWithoutBroadcast\n+-func.func @andWithoutBroadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n++// CHECK-LABEL: @and_no_broadcast\n++func.func @and_no_broadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n+   // CHECK: stablehlo.and %arg0, %arg1\n+   %0 = chlo.broadcast_and %arg0, %arg1 : (tensor<4xi1>, tensor<4xi1>) -> tensor<4xi1>\n+   func.return %0 : tensor<4xi1>\n+@@ -241,8 +263,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @atan2WithoutBroadcast\n+-func.func @atan2WithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @atan2_no_broadcast\n++func.func @atan2_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.atan2 %arg0, %arg1\n+   %0 = chlo.broadcast_atan2 %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -250,8 +272,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @compareWithoutBroadcast\n+-func.func @compareWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xi1> {\n++// CHECK-LABEL: @compare_no_broadcast\n++func.func @compare_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xi1> {\n+   // CHECK: stablehlo.compare EQ, %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n+   %0 = chlo.broadcast_compare %arg0, %arg1 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n+   func.return %0 : tensor<4xi1>\n+@@ -259,8 +281,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @complexWithoutBroadcast\n+-func.func @complexWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xcomplex<f32>> {\n++// CHECK-LABEL: @complex_no_broadcast\n++func.func @complex_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xcomplex<f32>> {\n+   // CHECK: stablehlo.complex %arg0, %arg1 : tensor<4xcomplex<f32>>\n+   %0 = chlo.broadcast_complex %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xcomplex<f32>>\n+   func.return %0 : tensor<4xcomplex<f32>>\n+@@ -268,8 +290,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @divideWithoutBroadcast\n+-func.func @divideWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @divide_no_broadcast\n++func.func @divide_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.divide %arg0, %arg1\n+   %0 = chlo.broadcast_divide %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -277,8 +299,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @maximumWithoutBroadcast\n+-func.func @maximumWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @maximum_no_broadcast\n++func.func @maximum_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.maximum %arg0, %arg1\n+   %0 = chlo.broadcast_maximum %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -286,8 +308,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @minimumWithoutBroadcast\n+-func.func @minimumWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @minimum_no_broadcast\n++func.func @minimum_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.minimum %arg0, %arg1\n+   %0 = chlo.broadcast_minimum %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -295,8 +317,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @multiplyWithoutBroadcast\n+-func.func @multiplyWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @multiply_no_broadcast\n++func.func @multiply_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.multiply %arg0, %arg1\n+   %0 = chlo.broadcast_multiply %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -304,8 +326,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @orWithoutBroadcast\n+-func.func @orWithoutBroadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n++// CHECK-LABEL: @or_no_broadcast\n++func.func @or_no_broadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n+   // CHECK: stablehlo.or %arg0, %arg1\n+   %0 = chlo.broadcast_or %arg0, %arg1 : (tensor<4xi1>, tensor<4xi1>) -> tensor<4xi1>\n+   func.return %0 : tensor<4xi1>\n+@@ -313,8 +335,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @powerWithoutBroadcast\n+-func.func @powerWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @power_no_broadcast\n++func.func @power_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.power %arg0, %arg1\n+   %0 = chlo.broadcast_power %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -322,8 +344,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @remainderWithoutBroadcast\n+-func.func @remainderWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @remainder_no_broadcast\n++func.func @remainder_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.remainder %arg0, %arg1\n+   %0 = chlo.broadcast_remainder %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -331,8 +353,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @shift_leftWithoutBroadcast\n+-func.func @shift_leftWithoutBroadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n++// CHECK-LABEL: @shift_left_no_broadcast\n++func.func @shift_left_no_broadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n+   // CHECK: stablehlo.shift_left %arg0, %arg1\n+   %0 = chlo.broadcast_shift_left %arg0, %arg1 : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>\n+   func.return %0 : tensor<4xi32>\n+@@ -340,8 +362,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @shift_right_arithmeticWithoutBroadcast\n+-func.func @shift_right_arithmeticWithoutBroadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n++// CHECK-LABEL: @shift_right_arithmetic_no_broadcast\n++func.func @shift_right_arithmetic_no_broadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n+   // CHECK: stablehlo.shift_right_arithmetic %arg0, %arg1\n+   %0 = chlo.broadcast_shift_right_arithmetic %arg0, %arg1 : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>\n+   func.return %0 : tensor<4xi32>\n+@@ -349,8 +371,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @shift_right_logicalWithoutBroadcast\n+-func.func @shift_right_logicalWithoutBroadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n++// CHECK-LABEL: @shift_right_logical_no_broadcast\n++func.func @shift_right_logical_no_broadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n+   // CHECK: stablehlo.shift_right_logical %arg0, %arg1\n+   %0 = chlo.broadcast_shift_right_logical %arg0, %arg1 : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>\n+   func.return %0 : tensor<4xi32>\n+@@ -358,8 +380,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @subWithoutBroadcast\n+-func.func @subWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n++// CHECK-LABEL: @sub_no_broadcast\n++func.func @sub_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n+   // CHECK: stablehlo.subtract %arg0, %arg1\n+   %0 = chlo.broadcast_subtract %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n+   func.return %0 : tensor<4xf32>\n+@@ -367,16 +389,16 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @xorWithoutBroadcast\n+-func.func @xorWithoutBroadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n++// CHECK-LABEL: @xor_no_broadcast\n++func.func @xor_no_broadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n+   // CHECK: stablehlo.xor %arg0, %arg1\n+   %0 = chlo.broadcast_xor %arg0, %arg1 : (tensor<4xi1>, tensor<4xi1>) -> tensor<4xi1>\n+   func.return %0 : tensor<4xi1>\n+ }\n+ \n+ // -----\n+-// CHECK-LABEL: @NextAfterWithoutBroadcast\n+-func.func @NextAfterWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n++// CHECK-LABEL: @next_after_no_broadcast\n++func.func @next_after_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n+     -> tensor<4xf32> {\n+   // CHECK-NOT: chlo.broadcast_next_after\n+   %0 = chlo.broadcast_next_after %arg0, %arg1\n+@@ -386,8 +408,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @PolygammaWithoutBroadcast\n+-func.func @PolygammaWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n++// CHECK-LABEL: @Polygamma_no_broadcast\n++func.func @Polygamma_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n+     -> tensor<4xf32> {\n+   // CHECK-NOT: chlo.broadcast_polygamma\n+   // CHECK-NOT: chlo.polygamma\n+@@ -398,8 +420,8 @@\n+ \n+ // -----\n+ \n+-// CHECK-LABEL: @ZetaWithoutBroadcast\n+-func.func @ZetaWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n++// CHECK-LABEL: @Zeta_no_broadcast\n++func.func @Zeta_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n+     -> tensor<4xf32> {\n+   // CHECK-NOT: chlo.broadcast_zeta\n+   // CHECK-NOT: chlo.zeta\n diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stablehlo/tests/ops_broadcasting.mlir\n --- stablehlo/stablehlo/tests/ops_broadcasting.mlir\n +++ stablehlo/stablehlo/tests/ops_broadcasting.mlir\n@@ -387,23 +1080,205 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplific\n  }\n  \n  // CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape\n+diff --ruN a/stablehlo/stablehlo/transforms/CMakeLists.txt b/stablehlo/stablehlo/transforms/CMakeLists.txt\n+--- stablehlo/stablehlo/transforms/CMakeLists.txt\n++++ stablehlo/stablehlo/transforms/CMakeLists.txt\n+@@ -113,6 +113,7 @@\n+   MLIRTransformUtils\n+   StablehloBase\n+   StablehloBroadcastUtils\n++  StablehloBroadcastLowering\n+   StablehloLinalgTransforms\n+   StablehloOps\n+   StablehloOptimizationPasses\n+diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n+--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n++++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n+@@ -35,7 +35,6 @@\n+ #include \"mlir/IR/BuiltinAttributes.h\"\n+ #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+ #include \"mlir/IR/BuiltinTypes.h\"\n+-#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n+ #include \"mlir/IR/MLIRContext.h\"\n+ #include \"mlir/IR/PatternMatch.h\"\n+ #include \"mlir/IR/TypeUtilities.h\"\n+@@ -51,6 +50,7 @@\n+ #include \"stablehlo/transforms/ChloDecompositionUtils.h\"\n+ #include \"stablehlo/transforms/PassUtils.h\"\n+ #include \"stablehlo/transforms/Passes.h\"\n++#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n+ \n+ // This must precede all other headers, otherwise during Windows cross\n+ // compilation, M_PI will not be defined.\n+@@ -201,34 +201,13 @@\n+       val);\n+ }\n+ \n+-// Broadcast using numpy-style broadcasting semantics.\n+-// This is only valid if the CHLO op has static shaped operands, and no\n+-// explicitly specified broadcast_dimensions.\n+-//\n+-// Asserts that input is ranked tensor type.\n+-Value numpyBroadcastIfNeeded(Value op, RankedTensorType opResultType,\n+-                             PatternRewriter& rewriter) {\n+-  RankedTensorType inputType = cast<RankedTensorType>(op.getType());\n+-  RankedTensorType broadcastedResultType =\n+-      opResultType.clone(inputType.getElementType());\n+-\n+-  // No broadcasting needed if input type matches broadcasted result type.\n+-  if (inputType == broadcastedResultType) return op;\n+-\n+-  // broadcast dims are the last dims for numpy style broadcasting.\n+-  int64_t inputRank = inputType.getRank();\n+-  int64_t resultRank = opResultType.getRank();\n+-  auto broadcastDimensions =\n+-      llvm::to_vector(llvm::seq<int64_t>(resultRank - inputRank, resultRank));\n+-  return stablehlo::BroadcastInDimOp::create(rewriter, op.getLoc(),\n+-                                             broadcastedResultType, op,\n+-                                             broadcastDimensions)\n+-      .getResult();\n+-}\n+-\n+ //===----------------------------------------------------------------------===//\n+ // Broadcasting Patterns.\n+ //===----------------------------------------------------------------------===//\n++\n++bool isStaticOrBoundedDynamicTensor(RankedTensorType type) {\n++  return type.hasStaticShape() || hlo::isBoundedDynamic(type);\n++}\n+ \n+ // Converts binary ops that statically are determined to not broadcast directly\n+ // to the corresponding stablehlo non-broadcasting op.\n+@@ -243,12 +222,14 @@\n+     // Only rewrite for statically determinable non-broadcasting cases.\n+     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());\n+     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());\n+-    if (!lhsType || !rhsType || lhsType.getShape() != rhsType.getShape() ||\n+-        !lhsType.hasStaticShape() || !rhsType.hasStaticShape())\n++    if (!lhsType || !rhsType || !isStaticOrBoundedDynamicTensor(lhsType) ||\n++        !isStaticOrBoundedDynamicTensor(rhsType) ||\n++        lhsType.getShape() != rhsType.getShape() ||\n++        lhsType.getEncoding() != rhsType.getEncoding())\n+       return rewriter.notifyMatchFailure(\n+           op,\n+           \"expected LHS and RHS to be ranked tensors with matching shapes that \"\n+-          \"are all static\");\n++          \"are all static or bounded dynamic\");\n+ \n+     rewriter.replaceOp(\n+         op, ValueRange{Adaptor::createOp(op, op.getType(),\n+@@ -270,41 +251,46 @@\n+     // Only rewrite for statically determinable non-broadcasting cases.\n+     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());\n+     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());\n+-    if (!lhsType || !rhsType || !lhsType.hasStaticShape() ||\n+-        !rhsType.hasStaticShape())\n++    if (!lhsType || !rhsType || !isStaticOrBoundedDynamicTensor(lhsType) ||\n++        !isStaticOrBoundedDynamicTensor(rhsType))\n+       return rewriter.notifyMatchFailure(\n+           op,\n+-          \"expected LHS and RHS to be ranked tensor types with static \"\n+-          \"shape\");\n++          \"expected LHS and RHS to be ranked tensor types with static or \"\n++          \"bounded dynamic shape\");\n+ \n+     // Rely on CHLO type inference to figure out the proper broadcasted shape.\n+     auto resultType = dyn_cast<RankedTensorType>(op.getResult().getType());\n+-    if (!resultType || !resultType.hasStaticShape())\n++    if (!resultType || !isStaticOrBoundedDynamicTensor(resultType))\n+       return rewriter.notifyMatchFailure(\n+-          op, \"expected result to be a ranked tensor type with static shape\");\n++          op,\n++          \"expected result to be a ranked tensor type with static or bounded \"\n++          \"dynamic shape\");\n+ \n+     auto lhs = adaptor.getLhs();\n+     auto rhs = adaptor.getRhs();\n+     auto broadcastDimensions = adaptor.getBroadcastDimensions();\n+     if (broadcastDimensions &&\n+-        !hlo::isLegalNumpyRankedBroadcast(lhs, rhs, *broadcastDimensions))\n++        !hlo::isLegalNumpyRankedBroadcast(lhs, rhs, *broadcastDimensions)) {\n+       return rewriter.notifyMatchFailure(\n+           op,\n+           \"expected implicit broadcast_dimensions or numpy-style broadcasting\");\n++    }\n+ \n+     LLVM_DEBUG(llvm::dbgs()\n+                << \"CHLO Decomposing \" << op->getName() << \" with broadcast \"\n+                << lhsType << \" x \" << rhsType << \" -> \" << resultType << \"\\n\");\n+ \n+-    // If operands are static directly create stablehlo broadcasting ops.\n+-    // Use numpy-style broadcasting with using StableHLO broadcast ops,\n+-    // when user didn't specify broadcast_dimensions.\n+-    auto lhsBroadcast =\n+-        numpyBroadcastIfNeeded(adaptor.getLhs(), resultType, rewriter);\n+-    auto rhsBroadcast =\n+-        numpyBroadcastIfNeeded(adaptor.getRhs(), resultType, rewriter);\n+-    auto result = Adaptor::createOp(op, resultType,\n+-                                    {lhsBroadcast, rhsBroadcast}, rewriter);\n++    // If operands are static or bounded dynamic, directly create stablehlo\n++    // broadcasting ops. Use numpy-style broadcasting with using StableHLO\n++    // broadcast ops. Can leave off broadcast_dimensions since the above\n++    // logic verifies that they are the default for numpy-style broadcasting.\n++    mlir::SmallVector<Value> broadcastOperands = {lhs, rhs};\n++    auto broadcasted_values =\n++        stablehlo::numpyBroadcastIfNeeded(rewriter, broadcastOperands);\n++    if (failed(broadcasted_values)) return failure();\n++\n++    auto result =\n++        Adaptor::createOp(op, resultType, *broadcasted_values, rewriter);\n+     rewriter.replaceOp(op, {result.getResult()});\n+     return success();\n+   }\n+@@ -425,7 +411,21 @@\n+       return success();\n+     }\n+ \n+-    // Lower to broadcasted constant.\n++    // Lower to cst -> broadcast -> set_dimension_size if bounded dynamic.\n++    if (hlo::isBoundedDynamic(resultTy)) {\n++      Value constant = mlir::stablehlo::ConstantOp::create(\n++          rewriter, op.getLoc(), op.getValue());\n++      mlir::FailureOr<stablehlo::Dimensions> operandDims =\n++          getDimensions(adaptor.getOperand());\n++      if (failed(operandDims)) return failure();\n++      mlir::FailureOr<Value> broadcast =\n++          stablehlo::numpyBroadcastIfNeeded(rewriter, constant, *operandDims);\n++      if (failed(broadcast)) return failure();\n++      rewriter.replaceOp(op, *broadcast);\n++      return success();\n++    }\n++\n++    // Lower unbounded dynamic to broadcasted constant.\n+     Location loc = op.getLoc();\n+     Value constant =\n+         mlir::stablehlo::ConstantOp::create(rewriter, loc, op.getValue());\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-@@ -63,7 +63,8 @@\n-   // Get tensor type\n-   mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n-   if (!tensor_type)\n--    return emitError(op.getLoc(), \"expected ranked tensor type\");\n-+    return emitError(op.getLoc(),\n-+                     \"expected ranked tensor type for broadcast inputs\");\n- \n-   auto encoding =\n-       mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n-@@ -78,10 +79,11 @@\n-   return dimensions;\n+@@ -59,29 +59,11 @@\n+   };\n  }\n  \n+-FailureOr<Dimensions> getDimensions(Value op) {\n+-  // Get tensor type\n+-  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n+-  if (!tensor_type)\n+-    return emitError(op.getLoc(), \"expected ranked tensor type\");\n+-\n+-  auto encoding =\n+-      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n+-          tensor_type.getEncoding());\n+-\n+-  Dimensions dimensions;\n+-  dimensions.reserve(tensor_type.getRank());\n+-  for (int64_t idx = 0; idx < tensor_type.getRank(); ++idx) {\n+-    auto dimInfo = getDimensionInfo(op, tensor_type, encoding, idx);\n+-    dimensions.push_back(dimInfo);\n+-  }\n+-  return dimensions;\n+-}\n+-\n -FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(const Dimensions& a,\n +FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(Value op,\n +                                                       const Dimensions& a,\n@@ -414,7 +1289,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n    size_t max_rank = std::max(a.size(), b.size());\n    Dimensions result(max_rank);\n  \n-@@ -110,14 +112,14 @@\n+@@ -110,14 +92,14 @@\n  \n      // If both LHS and RHS are not 1, dim size must match.\n      if (dim_a.size != dim_b.size) {\n@@ -432,21 +1307,47 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n                         \"cannot mix bounded and static dimensions in broadcast\");\n      }\n  \n-@@ -126,7 +128,7 @@\n+@@ -126,8 +108,30 @@\n    }\n  \n    LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n -                          << toString(result));\n +                          << toString(result) << \"\\n\");\n    return result;\n++}\n++\n++}  // namespace\n++\n++FailureOr<Dimensions> getDimensions(Value op) {\n++  // Get tensor type\n++  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n++  if (!tensor_type)\n++    return emitError(op.getLoc(),\n++                     \"expected ranked tensor type for broadcast inputs\");\n++\n++  auto encoding =\n++      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n++          tensor_type.getEncoding());\n++\n++  Dimensions dimensions;\n++  dimensions.reserve(tensor_type.getRank());\n++  for (int64_t idx = 0; idx < tensor_type.getRank(); ++idx) {\n++    auto dimInfo = getDimensionInfo(op, tensor_type, encoding, idx);\n++    dimensions.push_back(dimInfo);\n++  }\n++  return dimensions;\n  }\n  \n-@@ -155,8 +157,11 @@\n- \n- }  // namespace\n+ mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n+@@ -153,10 +157,12 @@\n+   return mlir::RankedTensorType::get(shape, element_type, encoding);\n+ }\n  \n+-}  // namespace\n+-\n -FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops) {\n -  if (ops.empty()) return failure();\n++\n +FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n +                                             ArrayRef<Value> ops) {\n +  if (ops.empty())\n@@ -455,7 +1356,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n  \n    Value first = ops[0];\n    auto bcastShapeOrFail = getDimensions(first);\n-@@ -168,7 +173,7 @@\n+@@ -168,7 +174,7 @@\n      auto dims = getDimensions(currOp);\n      if (failed(dims)) return failure();\n      auto currBcastShapeOrFail =\n@@ -464,7 +1365,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n      if (failed(currBcastShapeOrFail)) return failure();\n      bcastShape = std::move(*currBcastShapeOrFail);\n    }\n-@@ -192,7 +197,7 @@\n+@@ -192,7 +198,7 @@\n  FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n                                                       ArrayRef<Value> operands) {\n    // Figure out the broadcast shape\n@@ -473,7 +1374,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n    if (failed(bcastShapeOrFail)) return failure();\n    Dimensions bcastShape = std::move(*bcastShapeOrFail);\n  \n-@@ -208,35 +213,34 @@\n+@@ -208,35 +214,34 @@\n  \n  FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n                                          const Dimensions& shape) {\n@@ -526,7 +1427,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n  \n    // Construct the result type of the broadcast\n    //  - If input is static and target shape is static, use static shape.\n-@@ -244,33 +248,35 @@\n+@@ -244,33 +249,35 @@\n    //  - If input is not bounded, but target shape is bounded, broadcast to\n    //    the padded shape then call SetDimensionSize to make dynamic.\n    auto bcastShape = shape;\n@@ -578,7 +1479,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n  \n    // Mark the padded broadcast as dynamic where the result is bounded.\n    // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n-@@ -278,13 +284,13 @@\n+@@ -278,13 +285,13 @@\n    for (size_t i = 0; i < shape.size(); ++i) {\n      if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n        Value boundOp = shape[i].boundOp.value();\n@@ -601,8 +1502,18 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n-@@ -49,7 +49,8 @@\n+@@ -47,9 +47,18 @@\n+ using Dimensions = SmallVector<DimensionInfo>;\n+ std::string toString(const Dimensions& dims);\n  \n++// Returns the dimensions of the given op, or failure if the op's type is not a\n++// ranked tensor.\n++FailureOr<Dimensions> getDimensions(Value op);\n++\n++// Returns the ranked tensor type with the given dimensions and element type.\n++mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n++                                           mlir::Type element_type);\n++\n  // Returns the common shape these ops would broadcast to, or an error if the\n  // ops are not broadcastable.\n -FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops);"
        }
    ],
    "stats": {
        "total": 957,
        "additions": 934,
        "deletions": 23
    }
}