{
    "author": "Moerafaat",
    "message": "[XLA:GPU/TMA] Add heuristic for Triton TMA autotuning. This prunes down the configuration search space while maintaining the impactful configurations for TMA.\n\nPiperOrigin-RevId: 836598095",
    "sha": "79553abf29630629ae79ff03fd99432e74af46e8",
    "files": [
        {
            "sha": "d72b96958ad36105ae79642726e9c6be45b8a400",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=79553abf29630629ae79ff03fd99432e74af46e8",
            "patch": "@@ -925,6 +925,7 @@ cc_library(\n     hdrs = [\"tma_utils.h\"],\n     deps = [\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n+        \"//xla/service/gpu:matmul_utils\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:tma_metadata\","
        },
        {
            "sha": "7159afc56a98a0f6f44fdf7f987680348c16e9d6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/tma_utils.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc?ref=79553abf29630629ae79ff03fd99432e74af46e8",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -138,4 +139,16 @@ absl::StatusOr<TmaDescriptor> CreateTmaDescriptor(\n                              GetTmaSwizzleMode(swizzle_mode));\n }\n \n+bool IsTmaRecommended(const TritonGemmConfig& config) {\n+  // The current recommendation is based on analyzing the E2E \"Nucleo\" group\n+  // data. It might make sense to re-evaluate this recommendation later if we\n+  // believe there are missed opportunities.\n+  return (config.split_k == 1 || config.split_k == 16) &&\n+         config.num_warps <= 8 &&\n+         (config.num_stages == 1 || config.num_stages == 3 ||\n+          config.num_stages == 4) &&\n+         config.block_m <= 256 && config.block_n <= 256 &&\n+         config.block_k <= 256;\n+}\n+\n }  // namespace xla::gpu"
        },
        {
            "sha": "658af12d90c22caec3f4decafb3ed4612832db2c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/tma_utils.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.h?ref=79553abf29630629ae79ff03fd99432e74af46e8",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n \n namespace xla::gpu {\n@@ -30,6 +31,11 @@ absl::StatusOr<stream_executor::gpu::TmaDescriptor> CreateTmaDescriptor(\n     llvm::ArrayRef<int64_t> global_shape, llvm::ArrayRef<int64_t> tile_shape,\n     llvm::ArrayRef<int64_t> tile_strides, llvm::ArrayRef<int64_t> layout,\n     int element_byte_size, mlir::triton::xla::SwizzleMode swizzle_mode);\n+\n+// Recommends whether to attempt using TMA for a given configuration. This helps\n+// prune the search space and avoid compile-time regressions from trying out all\n+// configurations.\n+bool IsTmaRecommended(const TritonGemmConfig& config);\n }  // namespace xla::gpu\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_TMA_UTILS_H_"
        },
        {
            "sha": "a13b22e4cb0f35757c0e0d49ba2ee36bfd7c26ba",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=79553abf29630629ae79ff03fd99432e74af46e8",
            "patch": "@@ -48,6 +48,7 @@ cc_library(\n         \":triton_configs\",\n         \"//xla:autotuning_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/codegen/triton:tma_utils\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n@@ -299,6 +300,7 @@ cc_library(\n     deps = [\n         \"//xla:shape_util\",\n         \"//xla:util\",\n+        \"//xla/backends/gpu/codegen/triton:tma_utils\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service/gpu:matmul_utils\","
        },
        {
            "sha": "43a2806d38e1f96e250d15a47c53c3c599060a2f",
            "filename": "third_party/xla/xla/service/gpu/autotuning/dot_search_space.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc?ref=79553abf29630629ae79ff03fd99432e74af46e8",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"google/protobuf/repeated_field.h\"\n+#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -645,8 +646,11 @@ void TritonDotFusionSearchSpace::AddTmaParameter(\n   ConfigWithNotes new_config = config;\n   new_config.config.is_tma_allowed = false;\n   updated_configs.push_back(new_config);\n-  new_config.config.is_tma_allowed = true;\n-  updated_configs.push_back(new_config);\n+\n+  if (IsTmaRecommended(config.config)) {\n+    new_config.config.is_tma_allowed = true;\n+    updated_configs.push_back(new_config);\n+  }\n }\n \n void TritonDotFusionSearchSpace::AddWarpSpecializationParameter("
        },
        {
            "sha": "afcd300d933502e2e213de0c9fa53fcda3acf625",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=79553abf29630629ae79ff03fd99432e74af46e8",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #include <vector>\n \n #include \"third_party/gpus/cuda/include/cublas_v2.h\"\n+#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -116,8 +117,10 @@ std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     config.is_tma_allowed = false;\n     tma_parameterized_configs.push_back(config);\n \n-    config.is_tma_allowed = true;\n-    tma_parameterized_configs.push_back(config);\n+    if (IsTmaRecommended(config)) {\n+      config.is_tma_allowed = true;\n+      tma_parameterized_configs.push_back(config);\n+    }\n   }\n \n   // TODO(b/449668102): Currently only supporting warp specialization on"
        },
        {
            "sha": "84c2391190db42c716b43cfd15d4ec29812c9ff6",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79553abf29630629ae79ff03fd99432e74af46e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=79553abf29630629ae79ff03fd99432e74af46e8",
            "patch": "@@ -1878,19 +1878,18 @@ TEST_F(GemmFusionAutotunerEnableTma,\n   std::set<TritonGemmConfig> hopper_configs_set(hopper_configs.begin(),\n                                                 hopper_configs.end());\n \n-  // Expect that both configs are greater than zero and that the number of\n-  // configs for Hopper is twice the number of configs for Ampere. This is\n-  // because Hopper expects the same configs with and without TMA.\n+  // Expect that both configs sets are non-empty, that Hopper configs include\n+  // TMA options, and Ampere configs do not.\n   EXPECT_GT(ampere_configs_set.size(), 0);\n   EXPECT_GT(hopper_configs_set.size(), 0);\n-  EXPECT_EQ(ampere_configs_set.size() * 2, hopper_configs_set.size());\n \n-  auto count_tma_allowed = [](const std::vector<TritonGemmConfig>& configs) {\n-    return std::count_if(\n+  auto any_tma_allowed = [](const std::vector<TritonGemmConfig>& configs) {\n+    return std::any_of(\n         configs.begin(), configs.end(),\n         [](const TritonGemmConfig& config) { return config.is_tma_allowed; });\n   };\n-  EXPECT_EQ(count_tma_allowed(hopper_configs), hopper_configs.size() / 2);\n+  EXPECT_FALSE(any_tma_allowed(ampere_configs));\n+  EXPECT_TRUE(any_tma_allowed(hopper_configs));\n \n   EXPECT_TRUE(RunAndCompare(std::move(module),\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 39,
        "deletions": 11
    }
}