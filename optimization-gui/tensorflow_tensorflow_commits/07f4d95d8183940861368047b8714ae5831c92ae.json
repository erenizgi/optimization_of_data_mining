{
    "author": "vamsimanchala",
    "message": "Propagate delegate compiler stack version to TAC subgraph functions\n\nThis change introduces a new attribute `tac.delegate_compiler_stack_version` to track the compiler stack version used to target ops for delegation. This attribute is propagated from individual ops to the `FuncOp` created by `RaiseTargetSubgraphs`, and subsequently copied by `RaiseDialectOpsToFunc`. This allows downstream consumers to identify which delegate compiler stack version was associated with a given subgraph.\n\n*   **New Attribute:** Introduce `kDelegateCompilerStackVersion` (\"tac.delegate_compiler_stack_version\") in `tac/common/targets.h`.\n*   **RaiseTargetSubgraphs:** Update `RaiseTargetSubgraphs` to compute and attach `tac.delegate_compiler_stack_version` to outlined `FuncOps`. The pass reads the attribute from ops within the subgraph and ensures they are consistent before attaching the attribute to the function. If ops have inconsistent versions, subgraph creation will fail.\n*   **RaiseDialectOpsToFunc:** Update `RaiseDialectOpsToFunc` to copy `tac.delegate_compiler_stack_version` when creating delegate sub-graph functions.\n\n#darwinn_tac_multi_signature_param_sharing\n\nPiperOrigin-RevId: 840301517",
    "sha": "07f4d95d8183940861368047b8714ae5831c92ae",
    "files": [
        {
            "sha": "1d34c7c428dc24a4f0f3b59752ef375238ae22c9",
            "filename": "tensorflow/compiler/mlir/lite/experimental/tac/common/targets.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07f4d95d8183940861368047b8714ae5831c92ae/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Fcommon%2Ftargets.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07f4d95d8183940861368047b8714ae5831c92ae/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Fcommon%2Ftargets.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Fcommon%2Ftargets.h?ref=07f4d95d8183940861368047b8714ae5831c92ae",
            "patch": "@@ -45,6 +45,9 @@ constexpr char kSkipTargetAnnotation[] = \"tac.skip_target_annotation\";\n // Custom options fingerprint to apply different options for different filters.\n constexpr char kCustomOptionsFingerprint[] = \"tac.custom_options_fingerprint\";\n \n+// Delegate compiler stack version.\n+constexpr char kDelegateCompilerVersion[] = \"tac.delegate_compiler_version\";\n+\n // TODO(renjieliu): Add more inference types.\n enum InferenceType {\n   UNKNOWN = 0,"
        },
        {
            "sha": "a558d674fba83deb250e66447fbb8c83ada8d671",
            "filename": "tensorflow/compiler/mlir/lite/experimental/tac/tests/raise-target-subgraphs.mlir",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07f4d95d8183940861368047b8714ae5831c92ae/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftests%2Fraise-target-subgraphs.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07f4d95d8183940861368047b8714ae5831c92ae/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftests%2Fraise-target-subgraphs.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftests%2Fraise-target-subgraphs.mlir?ref=07f4d95d8183940861368047b8714ae5831c92ae",
            "patch": "@@ -570,3 +570,21 @@ func.func @testIgnoreInferenceType(%arg0: tensor<1x384x384xf32>, %arg1: tensor<1\n // CHECK-IGNORE-INFERENCE-TYPE:   %[[RES2:.*]] = tfl.mul %arg1, %arg1 {fused_activation_function = \"NONE\", tac.device = \"GPU\", tac.inference_type = \"QUANTIZED_INT8\"} : tensor<1x1x384x!quant.uniform<i8:f32, 3.000000e-03:-128>>\n // CHECK-IGNORE-INFERENCE-TYPE:   return %[[RES1]], %[[RES2]] : tensor<1x384x384xf32>, tensor<1x1x384x!quant.uniform<i8:f32, 3.000000e-03:-128>>\n // CHECK-IGNORE-INFERENCE-TYPE: }\n+\n+// -----\n+\n+func.func @testStackVersionPropagation(%arg0: tensor<1x4x4x1920xf32>) -> tensor<1x4x4x64xf32> {\n+  %cst = arith.constant dense<1.200000e+00> : tensor<64xf32>\n+  %0 = \"tfl.pseudo_qconst\"() <{qtype = tensor<64x1x1x1920x!quant.uniform<i8<-127:127>:f32, 0.002>>, value = dense<1> : tensor<64x1x1x1920xi8>}> : () -> tensor<64x1x1x1920x!quant.uniform<i8<-127:127>:f32, 0.002>>\n+  %1 = \"tfl.conv_2d\"(%arg0, %0, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"VALID\", stride_h = 1 : i32, stride_w = 1 : i32}> {tac.delegate_compiler_version = \"V_1_0\", tac.device = \"DARWINN\", tac.inference_type = \"HYBRID\"} : (tensor<1x4x4x1920xf32>, tensor<64x1x1x1920x!quant.uniform<i8<-127:127>:f32, 0.002>>, tensor<64xf32>) -> tensor<1x4x4x64xf32>\n+  %2 = tfl.add %1, %1 {fused_activation_function = \"NONE\", tac.delegate_compiler_version = \"V_1_5\", tac.device = \"DARWINN\", tac.inference_type = \"FLOAT\"} : tensor<1x4x4x64xf32>\n+  func.return %2 : tensor<1x4x4x64xf32>\n+}\n+\n+// CHECK-LABEL: func @testStackVersionPropagation\n+// CHECK: %[[RES0:.*]] = call @func_0_DARWINN_HYBRID(%arg0, %{{.*}}, %{{.*}}) {tac.device = \"DARWINN\", tac.inference_type = \"HYBRID\", tac.interface_name = \"func_0\"} : (tensor<1x4x4x1920xf32>, tensor<64x1x1x1920x!quant.uniform<i8<-127:127>:f32, 2.000000e-03>>, tensor<64xf32>) -> tensor<1x4x4x64xf32>\n+// CHECK: %[[RES1:.*]] = call @func_1_DARWINN_FLOAT(%[[RES0]]) {tac.device = \"DARWINN\", tac.inference_type = \"FLOAT\", tac.interface_name = \"func_1\"} : (tensor<1x4x4x64xf32>) -> tensor<1x4x4x64xf32>\n+// CHECK: return %[[RES1]]\n+// CHECK: }\n+// CHECK: func.func private @func_0_DARWINN_HYBRID(%arg0: tensor<1x4x4x1920xf32>, %arg1: tensor<64x1x1x1920x!quant.uniform<i8<-127:127>:f32, 2.000000e-03>>, %arg2: tensor<64xf32>) -> tensor<1x4x4x64xf32> attributes {tac.delegate_compiler_version = \"V_1_0\", tac.device = \"DARWINN\", tac.inference_type = \"HYBRID\", tac.interface_name = \"func_0\"}\n+// CHECK: func.func private @func_1_DARWINN_FLOAT(%arg0: tensor<1x4x4x64xf32>) -> tensor<1x4x4x64xf32> attributes {tac.delegate_compiler_version = \"V_1_5\", tac.device = \"DARWINN\", tac.inference_type = \"FLOAT\", tac.interface_name = \"func_1\"}"
        },
        {
            "sha": "4939d033efca8ed0a16416ca0571a654283c4ed7",
            "filename": "tensorflow/compiler/mlir/lite/experimental/tac/transforms/raise_target_subgraphs.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 15,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07f4d95d8183940861368047b8714ae5831c92ae/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fraise_target_subgraphs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07f4d95d8183940861368047b8714ae5831c92ae/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fraise_target_subgraphs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fexperimental%2Ftac%2Ftransforms%2Fraise_target_subgraphs.cc?ref=07f4d95d8183940861368047b8714ae5831c92ae",
            "patch": "@@ -103,30 +103,30 @@ class RaiseTargetSubgraphsPass\n       int& func_count, const TF::SideEffectAnalysis::Info& side_effect_info);\n };\n \n-// Returns the custom options fingerprint for an added function op, by walking\n-// through the ops in the function and collating all the custom options\n-// fingerprints.\n-std::optional<StringAttr> GetCustomOptionsFingerprint(func::FuncOp func) {\n-  StringAttr custom_options_fingerprint;\n+// Returns the value of the string attribute `attr_name` for an added function\n+// op, by walking through the ops in the function and collating all the\n+// attribute values. If the attribute values are not all the same, returns\n+// std::nullopt.\n+std::optional<StringAttr> GetConsolidatedStringAttr(func::FuncOp func,\n+                                                    StringRef attr_name) {\n+  StringAttr attr_value;\n   WalkResult result = func.walk([&](Operation* op) {\n-    if (op->hasAttr(kCustomOptionsFingerprint)) {\n-      if (custom_options_fingerprint &&\n-          custom_options_fingerprint !=\n-              op->getAttr(kCustomOptionsFingerprint)) {\n-        // If the custom options fingerprint is not null, and it is different\n-        // from the current op's custom options fingerprint, then it is an\n+    if (op->hasAttr(attr_name)) {\n+      StringAttr current_attr = mlir::cast<StringAttr>(op->getAttr(attr_name));\n+      if (attr_value && attr_value != current_attr) {\n+        // If the attribute value is not null, and it is different\n+        // from the current op's attribute value, then it is an\n         // error.\n         return WalkResult::interrupt();\n       }\n-      custom_options_fingerprint =\n-          mlir::cast<StringAttr>(op->getAttr(kCustomOptionsFingerprint));\n+      attr_value = current_attr;\n     }\n     return WalkResult::advance();\n   });\n   if (result.wasInterrupted()) {\n     return std::nullopt;\n   }\n-  return custom_options_fingerprint;\n+  return attr_value;\n }\n \n // After raising ops and adding the Func & Call op, call this function\n@@ -139,7 +139,8 @@ bool AddAttrs(OpsAdded& ops_added, OpBuilder& builder, int func_count) {\n \n   added_func_op->setAttr(kInterfaceNameAttr, interface_name);\n   added_call_op->setAttr(kInterfaceNameAttr, interface_name);\n-  auto custom_options_fingerprint = GetCustomOptionsFingerprint(added_func_op);\n+  auto custom_options_fingerprint =\n+      GetConsolidatedStringAttr(added_func_op, kCustomOptionsFingerprint);\n   if (!custom_options_fingerprint.has_value()) {\n     return false;\n   }\n@@ -148,6 +149,16 @@ bool AddAttrs(OpsAdded& ops_added, OpBuilder& builder, int func_count) {\n                            custom_options_fingerprint.value());\n   }\n \n+  auto delegate_compiler_version =\n+      GetConsolidatedStringAttr(added_func_op, kDelegateCompilerVersion);\n+  if (!delegate_compiler_version.has_value()) {\n+    return false;\n+  }\n+  if (delegate_compiler_version.value() != nullptr) {\n+    added_func_op->setAttr(kDelegateCompilerVersion,\n+                           delegate_compiler_version.value());\n+  }\n+\n   StringAttr device = mlir::cast<StringAttr>(\n       added_func_op->getRegion(0).getBlocks().front().front().getAttr(kDevice));\n   StringAttr inference_type = mlir::cast<StringAttr>("
        }
    ],
    "stats": {
        "total": 62,
        "additions": 47,
        "deletions": 15
    }
}