{
    "author": "loislo",
    "message": "[XLA:GPU] Disable float normalization for the scaled-dot op.\n\nWe rewrite the scaled-dot with multiply and broadcast when the operands are not compatible. As a result if there is scaled dot then it has compatible operand and does not require the float normalization.\n\nPiperOrigin-RevId: 799502145",
    "sha": "5ef6377b59cab30b43a7cd2eb7fe2d3975274c89",
    "files": [
        {
            "sha": "94f98ed4a3e71998f8283a93991ccb58464af166",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ef6377b59cab30b43a7cd2eb7fe2d3975274c89/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ef6377b59cab30b43a7cd2eb7fe2d3975274c89/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc?ref=5ef6377b59cab30b43a7cd2eb7fe2d3975274c89",
            "patch": "@@ -40,6 +40,11 @@ bool GpuFloatSupport::SupportsMixedPrecisions(const HloInstruction& hlo) const {\n \n   switch (hlo.opcode()) {\n     // Handled by Triton GEMM or cuBLAS.\n+    case HloOpcode::kScaledDot:\n+      // We accept any scaled dot, because there is a rewrite pass that will\n+      // lower it to a dot + multiply for unsupported types.\n+      return true;\n+    // Handled by Triton GEMM or cuBLAS.\n     case HloOpcode::kDot: {\n       const PrimitiveType lhs_type = hlo.operand(0)->shape().element_type();\n       const PrimitiveType rhs_type = hlo.operand(1)->shape().element_type();\n@@ -63,8 +68,11 @@ bool GpuFloatSupport::IsSupported(const HloInstruction& hlo) const {\n     case HloOpcode::kAllReduceStart:\n     case HloOpcode::kAllReduceDone:\n     case HloOpcode::kReduceScatter:\n-    // Handled by Triton GEMM.\n-    case HloOpcode::kDot:\n+    case HloOpcode::kScaledDot:\n+      // We accept any scaled dot, because there is a rewrite pass that will\n+      // lower it to a dot + multiply for unsupported types.\n+      return true;\n+    case HloOpcode::kDot:  // Handled by Triton GEMM.\n       using TypeAndCC =\n           std::pair<PrimitiveType, stream_executor::CudaComputeCapability>;\n       for (auto [type, cc] :"
        },
        {
            "sha": "052db1fefeb56ba00f2b3ee6586df8fe800b6102",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ef6377b59cab30b43a7cd2eb7fe2d3975274c89/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ef6377b59cab30b43a7cd2eb7fe2d3975274c89/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc?ref=5ef6377b59cab30b43a7cd2eb7fe2d3975274c89",
            "patch": "@@ -430,5 +430,26 @@ ENTRY main {\n   EXPECT_TRUE(Normalize(module_exp.get(), cc, BF16, F32));\n }\n \n+TEST_F(FloatSupportTest, ScaledDotIsIgnored) {\n+  auto cc = se::CudaComputeCapability::Hopper();\n+  constexpr absl::string_view kHloModule = R\"(\n+    HloModule ScaledDotIsIgnored\n+\n+    ENTRY main {\n+      lhs = bf16[1024, 1024] parameter(0)\n+      lhs_scale = bf16[1, 1] parameter(1)\n+      rhs = bf16[1024, 1024] parameter(2)\n+      rhs_scale = bf16[1, 1] parameter(3)\n+      ROOT r = bf16[1024, 1024] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+         lhs_contracting_dims={1},\n+         rhs_contracting_dims={1}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule));\n+  EXPECT_FALSE(Normalize(module.get(), cc, BF16, F32));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 31,
        "deletions": 2
    }
}