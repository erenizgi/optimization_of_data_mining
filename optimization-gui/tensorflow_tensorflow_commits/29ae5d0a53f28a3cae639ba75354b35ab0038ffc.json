{
    "author": "KanishAnand",
    "message": "(1/N) Add support for `NamedSharding` in existing `HloSharding` methods. Remaining methods will be updated in follow up cl's.\n\nPiperOrigin-RevId: 831046450",
    "sha": "29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
    "files": [
        {
            "sha": "b4a8d8e9b35acc5c24a9c6e65a824edfb3dc78b8",
            "filename": "third_party/xla/xla/hlo/ir/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -204,7 +204,11 @@ cc_library(\n     hdrs = [\"named_sharding.h\"],\n     deps = [\n         \":mesh_and_axis\",\n+        \":tile_assignment\",\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/types:span\",\n     ],\n )"
        },
        {
            "sha": "9a5f8d681b488a679f38469aa09185394f207e16",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 10,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/array.h\"\n #include \"xla/hlo/ir/hlo_op_metadata.h\"\n+#include \"xla/hlo/ir/named_sharding.h\"\n #include \"xla/overflow_util.h\"\n #include \"xla/printer.h\"\n #include \"xla/shape.h\"\n@@ -139,7 +140,11 @@ bool NextIndex(absl::InlinedVector<int64_t, 6>* index,\n }  // namespace\n \n HloSharding HloSharding::AssignDevice(int64_t device_id,\n-                                      absl::Span<const OpMetadata> metadata) {\n+                                      absl::Span<const OpMetadata> metadata,\n+                                      bool use_named_sharding) {\n+  if (use_named_sharding) {\n+    return HloSharding(NamedSharding::MaximalSharding(device_id, metadata));\n+  }\n   return HloSharding(device_id, metadata);\n }\n \n@@ -522,7 +527,9 @@ bool HloSharding::UsesDevice(int64_t device) const {\n       return s.UsesDevice(device);\n     });\n   }\n-  return replicated_ || manual_ || tile_assignment_.UsesDevice(device);\n+\n+  return IsReplicatedLeaf() || IsManualLeaf() ||\n+         TileAgnosticDeviceAssignment().UsesDevice(device);\n }\n \n std::map<int64_t, int64_t> HloSharding::UsedDevices(int64_t* count) const {\n@@ -761,8 +768,10 @@ std::optional<int64_t> HloSharding::UniqueDevice() const {\n     }\n     return unique_device;\n   }\n-  if (!replicated_ && maximal_) {\n-    return static_cast<int64_t>(*tile_assignment_.array().begin());\n+\n+  if (!IsReplicatedLeaf() && IsTileMaximalLeaf()) {\n+    return static_cast<int64_t>(\n+        *TileAgnosticDeviceAssignment().array().begin());\n   }\n   return std::nullopt;\n }\n@@ -838,21 +847,22 @@ absl::Status HloSharding::ValidateNonTuple(\n     return absl::InvalidArgumentError(\n         \"Validation shape is a tuple but sharding is not.\");\n   }\n-  if (replicated_ || manual_ || unreduced_ || unknown_) {\n+  if (IsReplicatedLeaf() || IsManualLeaf() || IsUnreducedLeaf() ||\n+      IsUnknownLeaf()) {\n     return absl::OkStatus();\n   }\n \n-  if (maximal_) {\n-    CHECK(!tile_assignment_.iota_);\n-    if (tile_assignment_.array().num_elements() != 1) {\n+  if (IsTileMaximalLeaf()) {\n+    CHECK(!TileAgnosticDeviceAssignment().iota_);\n+    if (TileAgnosticDeviceAssignment().array().num_elements() != 1) {\n       return absl::InvalidArgumentError(\n           \"Tile maximal sharding must have a single device assignment.\");\n     }\n-    return DeviceInRange(tile_assignment_.first(), num_devices);\n+    return DeviceInRange(TileAgnosticDeviceAssignment().first(), num_devices);\n   }\n \n   // The correct constructor has to be used to create tile maximal shardings.\n-  if (tile_assignment_.num_elements() == 1) {\n+  if (TileAgnosticDeviceAssignment().num_elements() == 1) {\n     return absl::InvalidArgumentError(\n         \"Tile assignment only contains a single device. If a replicated \"\n         \"sharding was intended, use HloSharding::Replicated(). If a device \"\n@@ -898,6 +908,21 @@ absl::Status HloSharding::ValidateNonTuple(\n   return absl::OkStatus();\n }\n \n+const TileAssignment& HloSharding::TileAgnosticDeviceAssignment() const {\n+  // Returns device assignment regardless of sharding tiling.\n+  //  - named_sharding_->device_assignment() only contains the information of\n+  //    the mesh without information of how axes are used.\n+  //  - tile_assignment_ keeps the information of mesh and how axes are used.\n+  //\n+  // For example, a NamedSharding [mesh= ['a'=2, 'b'=2] {'a'}, {}] and\n+  // HloSharding [2,1,2]<=4 last_tile_dim_replicate would have the same\n+  // underlying device order as: {{0, 1}, {2, 3}}.\n+  if (UseNamedShardingLeaf()) {\n+    return named_sharding_->device_assignment();\n+  }\n+  return tile_assignment_;\n+}\n+\n /*static*/ absl::StatusOr<HloSharding> HloSharding::FromProto(\n     const OpSharding& proto) {\n   std::vector<OpMetadata> metadata(proto.metadata().begin(),"
        },
        {
            "sha": "92b1695feee4ec12df4c68ef06689edd3cbb773f",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 70,
            "deletions": 12,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -56,7 +56,11 @@ class HloSharding {\n \n   // Creates a trivial sharding that replicates a maximal tile across all\n   // devices.\n-  static HloSharding Replicate(absl::Span<const OpMetadata> metadata = {}) {\n+  static HloSharding Replicate(absl::Span<const OpMetadata> metadata = {},\n+                               bool use_named_sharding = false) {\n+    if (use_named_sharding) {\n+      return HloSharding(NamedSharding::Replicate(metadata));\n+    }\n     return HloSharding(/*manual=*/false, /*replicated=*/true, /*unknown=*/false,\n                        /*unreduced=*/false, metadata);\n   }\n@@ -81,7 +85,8 @@ class HloSharding {\n   // Creates a sharding that emulates device placement; a tile shape equal to\n   // the input shape (one tile) assigned to a single device.\n   static HloSharding AssignDevice(int64_t device_id,\n-                                  absl::Span<const OpMetadata> metadata = {});\n+                                  absl::Span<const OpMetadata> metadata = {},\n+                                  bool use_named_sharding = false);\n \n   // Creates a new sharding which splits a shape into tiles amongst the devices\n   // specified by `tile_assignment`.\n@@ -188,59 +193,80 @@ class HloSharding {\n   absl::Status Validate(const Shape& shape,\n                         std::optional<int64_t> num_devices = {}) const;\n \n+  // Returns true if the sharding is represented using `NamedSharding` format.\n+  bool UseNamedShardingLeaf() const {\n+    DCHECK(!IsTuple());\n+    return named_sharding_.has_value();\n+  }\n+\n   // Returns true if the sharding has tuple type.\n   bool IsTuple() const { return tuple_; }\n \n   // Returns true if the sharding is trivial: replicate on all devices.\n   bool IsReplicated() const {\n     if (!IsTuple()) {\n-      return replicated_;\n+      return IsReplicatedLeaf();\n     }\n     return absl::c_all_of(\n         tuple_elements_, [](const HloSharding& s) { return s.IsReplicated(); });\n   }\n   bool IsReplicatedLeaf() const {\n     DCHECK(!IsTuple());\n+    if (UseNamedShardingLeaf()) {\n+      return named_sharding_->IsReplicated();\n+    }\n     return replicated_;\n   }\n \n   // Returns true if the tile size is the same as the input size.\n   bool IsTileMaximal() const {\n     if (!IsTuple()) {\n-      return maximal_;\n+      return IsTileMaximalLeaf();\n     }\n     return absl::c_all_of(tuple_elements_, [](const HloSharding& s) {\n       return s.IsTileMaximal();\n     });\n   }\n   bool IsTileMaximalLeaf() const {\n     DCHECK(!IsTuple());\n+    if (UseNamedShardingLeaf()) {\n+      return named_sharding_->IsTileMaximal();\n+    }\n     return maximal_;\n   }\n \n   // Returns whether the sharding represents manual partitioning.\n   bool IsManual() const {\n     if (!IsTuple()) {\n-      return manual_;\n+      return IsManualLeaf();\n     }\n     return absl::c_all_of(tuple_elements_,\n                           [](const HloSharding& s) { return s.IsManual(); });\n   }\n   bool IsManualLeaf() const {\n     DCHECK(!IsTuple());\n+    if (UseNamedShardingLeaf()) {\n+      // ManualSharding is represented by separate ManualComputationOp in named\n+      // sharding format.\n+      return false;\n+    }\n     return manual_;\n   }\n \n   // Returns whether the sharding represents a placeholder sharding.\n   bool IsUnknown() const {\n     if (!IsTuple()) {\n-      return unknown_;\n+      return IsUnknownLeaf();\n     }\n     return absl::c_all_of(tuple_elements_,\n                           [](const HloSharding& s) { return s.IsUnknown(); });\n   }\n   bool IsUnknownLeaf() const {\n     DCHECK(!IsTuple());\n+    if (UseNamedShardingLeaf()) {\n+      // There is no Unknown sharding type in named sharding format.\n+      return false;\n+    }\n     return unknown_;\n   }\n \n@@ -258,6 +284,11 @@ class HloSharding {\n \n   bool IsShardGroup() const {\n     if (!IsTuple()) {\n+      if (UseNamedShardingLeaf()) {\n+        // Sharding groups are represented by separate ShardingGroup op in named\n+        // sharding format.\n+        return false;\n+      }\n       return shard_group_.shard_group_id != -1 &&\n              (shard_group_.shard_like || shard_group_.shard_as);\n     }\n@@ -269,6 +300,11 @@ class HloSharding {\n \n   bool IsShardAs() const {\n     if (!IsTuple()) {\n+      if (UseNamedShardingLeaf()) {\n+        // Sharding groups are represented by separate ShardingGroup op in named\n+        // sharding format.\n+        return false;\n+      }\n       return shard_group_.shard_group_id != -1 && shard_group_.shard_as;\n     }\n     return !tuple_elements_.empty() &&\n@@ -278,6 +314,11 @@ class HloSharding {\n \n   bool IsShardLike() const {\n     if (!IsTuple()) {\n+      if (UseNamedShardingLeaf()) {\n+        // Sharding groups are represented by separate ShardingGroup op in named\n+        // sharding format.\n+        return false;\n+      }\n       return shard_group_.shard_group_id != -1 && shard_group_.shard_like;\n     }\n     return !tuple_elements_.empty() &&\n@@ -636,7 +677,8 @@ class HloSharding {\n         manual_(manual),\n         unknown_(unknown),\n         unreduced_(unreduced),\n-        replicate_on_last_tile_dim_(false) {}\n+        replicate_on_last_tile_dim_(false),\n+        named_sharding_(std::nullopt) {}\n   // device_id values:\n   // -2: magic number to mean unassigned device, used by spatial partitioning\n   // -1: the id of the host\n@@ -652,7 +694,8 @@ class HloSharding {\n         manual_(false),\n         unknown_(false),\n         unreduced_(false),\n-        replicate_on_last_tile_dim_(false) {}\n+        replicate_on_last_tile_dim_(false),\n+        named_sharding_(std::nullopt) {}\n   explicit HloSharding(TileAssignment tile_assignment,\n                        bool replicate_on_last_tile_dim,\n                        absl::Span<const OpMetadata> metadata = {})\n@@ -664,7 +707,8 @@ class HloSharding {\n         manual_(false),\n         unknown_(false),\n         unreduced_(false),\n-        replicate_on_last_tile_dim_(replicate_on_last_tile_dim) {}\n+        replicate_on_last_tile_dim_(replicate_on_last_tile_dim),\n+        named_sharding_(std::nullopt) {}\n   explicit HloSharding(TileAssignment tile_assignment,\n                        absl::Span<const OpSharding::Type> subgroup_types,\n                        absl::Span<const OpMetadata> metadata = {})\n@@ -677,7 +721,8 @@ class HloSharding {\n         manual_(false),\n         unknown_(false),\n         unreduced_(false),\n-        replicate_on_last_tile_dim_(false) {}\n+        replicate_on_last_tile_dim_(false),\n+        named_sharding_(std::nullopt) {}\n   explicit HloSharding(std::vector<HloSharding> tuple_shardings)\n       : tuple_elements_(std::move(tuple_shardings)),\n         replicated_(false),\n@@ -686,7 +731,17 @@ class HloSharding {\n         manual_(false),\n         unknown_(false),\n         unreduced_(false),\n-        replicate_on_last_tile_dim_(false) {}\n+        replicate_on_last_tile_dim_(false),\n+        named_sharding_(std::nullopt) {}\n+  explicit HloSharding(NamedSharding named_sharding)\n+      : replicated_(false),\n+        maximal_(false),\n+        tuple_(false),\n+        manual_(false),\n+        unknown_(false),\n+        unreduced_(false),\n+        replicate_on_last_tile_dim_(false),\n+        named_sharding_(std::move(named_sharding)) {}\n \n   // Test-only constructor for sharding format code coverage. Copies the\n   // original sharding with provided tile assignment.\n@@ -701,7 +756,8 @@ class HloSharding {\n         manual_(other.manual_),\n         unknown_(other.unknown_),\n         unreduced_(other.unreduced_),\n-        replicate_on_last_tile_dim_(other.replicate_on_last_tile_dim_) {\n+        replicate_on_last_tile_dim_(other.replicate_on_last_tile_dim_),\n+        named_sharding_(std::nullopt) {\n     CHECK(tile_assignment_ == other.tile_assignment_)\n         << tile_assignment_.ToString() << \" v.s. \"\n         << other.tile_assignment_.ToString();\n@@ -720,6 +776,8 @@ class HloSharding {\n   absl::Status ValidateNonTuple(const Shape& shape,\n                                 std::optional<int64_t> num_devices) const;\n \n+  const TileAssignment& TileAgnosticDeviceAssignment() const;\n+\n   // This field is only used if replicated_ is false. If maximal_ is true, then\n   // the field contains a rank 1 array with a single element, which is the\n   // device the HLO is assigned to. If maximal_ is false, the field contains an"
        },
        {
            "sha": "6a7807f51d698ea734c11c39b200f6bd615ece75",
            "filename": "third_party/xla/xla/hlo/ir/mesh_and_axis.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis.cc?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -96,8 +96,20 @@ Mesh::Mesh(TileAssignment device_assignment,\n \n MeshProto Mesh::ToProto() const {\n   MeshProto proto;\n+  int64_t num_axes = axes_names_.size();\n+\n+  if (num_axes == 0) {\n+    if (device_assignment_.num_elements() == 0) {\n+      return MeshProto();\n+    }\n+    // Maximal mesh\n+    // TODO(b/454008727): Validate device_ids_size is 1.\n+    proto.add_device_ids(*device_assignment_.array().begin());\n+    return proto;\n+  }\n+\n   std::vector<MeshProto::MeshAxis> axes;\n-  axes.reserve(axes_names_.size());\n+  axes.reserve(num_axes);\n \n   for (auto [name, size] :\n        llvm::zip_equal(axes_names_, device_assignment_.dimensions())) {\n@@ -118,6 +130,16 @@ MeshProto Mesh::ToProto() const {\n }\n \n Mesh Mesh::FromProto(const MeshProto& proto) {\n+  // TODO(b/454008727): Add validators for Mesh and AxisRef FromProto methods.\n+  if (proto.axes_size() == 0) {\n+    if (proto.device_ids_size() == 0) {\n+      return Mesh();\n+    }\n+    // Maximal mesh\n+    // TODO(b/454008727): Validate device_ids_size is 1.\n+    return Mesh(proto.device_ids(0));\n+  }\n+\n   std::vector<int64_t> mesh_axis_sizes;\n   std::vector<absl::string_view> mesh_axis_names;\n   mesh_axis_sizes.reserve(proto.axes_size());"
        },
        {
            "sha": "2b913f7638dad0e7c43be554b6820b966fa18d75",
            "filename": "third_party/xla/xla/hlo/ir/mesh_and_axis.h",
            "status": "modified",
            "additions": 25,
            "deletions": 7,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis.h?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -36,17 +36,28 @@ namespace xla {\n \n class AxisRef;\n \n-// C++ representation for corresponding `OpSharding::Mesh` proto so same\n+// C++ representation for corresponding OpSharding::Mesh proto so same\n // documentation applies, except device assignment is represented in the array\n // format instead of list of device ids to align with various array specific\n-// queries. Note that `TileAssignment` is used instead of `xla::Array` for\n-// optimized array representation in iota based cases which is the most common\n-// case.\n+// queries. `TileAssignment` is used instead of `xla::Array` for optimized array\n+// representation in the most common iota-based cases.\n //\n-// Example: device_assignment {{3, 0, 2}, {1, 4, 5}} with axes names {\"data\",\n-// \"model\"} represents the mesh [\"data\"=2, \"model\"=3].\n+// - device_assignment_.dimensions() represents the axis sizes.\n+// - device_assignment_.array() represents the list of device IDs.\n+//\n+// For maximal mesh, axes_names is empty and device_assignment_ contains the\n+// single device id.\n+//\n+// Example: device_assignment {{3, 0, 2}, {1, 4, 5}} with axes names\n+// {\"data\", \"model\"} represents the mesh [\"data\"=2, \"model\"=3].\n class Mesh {\n  public:\n+  // Empty mesh\n+  explicit Mesh() = default;\n+\n+  // Maximal Mesh\n+  explicit Mesh(int64_t device_id) : device_assignment_(device_id) {}\n+\n   // Constructs an iota device assignment mesh with given axes sizes and names.\n   //\n   // Example: axes_sizes {2, 3} and axes_names {\"data\", \"model\"} represent the\n@@ -67,6 +78,13 @@ class Mesh {\n   explicit Mesh(TileAssignment device_assignment,\n                 absl::Span<const absl::string_view> axes_names);\n \n+  // Returns whether this mesh is a maximal-sharding mesh.\n+  //\n+  // A maximal-sharding mesh contains an empty axis list and a single device id.\n+  bool IsMaximal() const {\n+    return axes_names_.empty() && device_assignment_.num_elements() == 1;\n+  }\n+\n   bool operator==(const Mesh& other) const {\n     return device_assignment_ == other.device_assignment_ &&\n            axes_names_ == other.axes_names_;\n@@ -104,7 +122,7 @@ class Mesh {\n \n   static Mesh FromProto(const MeshProto& proto);\n \n-  TileAssignment device_assignment() const { return device_assignment_; }\n+  const TileAssignment& device_assignment() const { return device_assignment_; }\n   std::vector<std::string> axis_names() const { return axes_names_; }\n   absl::Span<const int64_t> axis_sizes() const {\n     return device_assignment_.dimensions();"
        },
        {
            "sha": "3dbcc53db7724682b47ae5bf97efbea674a8171e",
            "filename": "third_party/xla/xla/hlo/ir/mesh_and_axis_test.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fmesh_and_axis_test.cc?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -317,4 +317,37 @@ TEST(MeshAndAxisTest, AxisRefCanCoexistWithoutOverlap) {\n   coexistWithoutOverlap(AxisRef(0, {2, 8}), AxisRef(0, {4, 2}), false);\n }\n \n+TEST(MeshAndAxisTest, EmptyMesh) {\n+  Mesh empty_mesh;\n+  EXPECT_EQ(empty_mesh, Mesh());\n+  EXPECT_NE(empty_mesh, Mesh(5));\n+  EXPECT_NE(empty_mesh, Mesh({1}, {\"a\"}));\n+  EXPECT_FALSE(empty_mesh.IsMaximal());\n+  EXPECT_THAT(empty_mesh.ToProto(), EqualsProto(MeshProto()));\n+  EXPECT_EQ(empty_mesh, Mesh::FromProto(MeshProto()));\n+  EXPECT_EQ(empty_mesh, Mesh::FromProto(empty_mesh.ToProto()));\n+}\n+\n+TEST(MeshAndAxisTest, MaximalMesh) {\n+  Mesh maximal_mesh(5);\n+  EXPECT_TRUE(maximal_mesh.IsMaximal());\n+  Mesh non_maximal_mesh({2, 3}, {\"a\", \"b\"});\n+  EXPECT_FALSE(non_maximal_mesh.IsMaximal());\n+  Mesh mesh_single_axis({1}, {\"a\"});\n+  EXPECT_FALSE(mesh_single_axis.IsMaximal());\n+\n+  EXPECT_EQ(maximal_mesh, Mesh(5));\n+  EXPECT_NE(maximal_mesh, Mesh(6));\n+\n+  MeshProto expected_proto;\n+  expected_proto.add_device_ids(5);\n+  EXPECT_THAT(maximal_mesh.ToProto(), EqualsProto(expected_proto));\n+\n+  MeshProto from_proto;\n+  from_proto.add_device_ids(7);\n+  EXPECT_EQ(Mesh(7), Mesh::FromProto(from_proto));\n+\n+  EXPECT_EQ(maximal_mesh, Mesh::FromProto(maximal_mesh.ToProto()));\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "7763086e2b4b88d6226842e41b5ac9d3512c0942",
            "filename": "third_party/xla/xla/hlo/ir/named_sharding.h",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -16,11 +16,14 @@ limitations under the License.\n #ifndef XLA_HLO_IR_NAMED_SHARDING_H_\n #define XLA_HLO_IR_NAMED_SHARDING_H_\n \n+#include <cstdint>\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/types/span.h\"\n #include \"xla/hlo/ir/mesh_and_axis.h\"\n+#include \"xla/hlo/ir/tile_assignment.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -38,6 +41,8 @@ class NamedSharding {\n     explicit DimensionSharding(std::vector<AxisRef> axes, bool is_closed)\n         : axes_(std::move(axes)), is_closed_(is_closed) {}\n \n+    absl::Span<const AxisRef> axes() const { return axes_; }\n+\n    private:\n     std::vector<AxisRef> axes_;\n     bool is_closed_;\n@@ -55,7 +60,7 @@ class NamedSharding {\n     return !(*this == other);\n   }\n \n-  // TODO(b/456212087): Add some validation checks\n+  // TODO(b/456212087): Add validation checks\n   explicit NamedSharding(Mesh mesh,\n                          absl::Span<const DimensionSharding> dim_shardings = {},\n                          absl::Span<const AxisRef> replicated_axes = {},\n@@ -68,6 +73,42 @@ class NamedSharding {\n         metadata_(metadata.begin(), metadata.end()) {}\n \n  private:\n+  friend class HloSharding;\n+\n+  // Creates a sharding with empty mesh and no sharding axes depicting it is\n+  // replicated across all devices.\n+  static NamedSharding Replicate(absl::Span<const OpMetadata> metadata = {}) {\n+    return NamedSharding(/*mesh=*/Mesh(), /*dim_shardings=*/{},\n+                         /*replicated_axes=*/{},\n+                         /*unreduced_axes=*/{}, metadata);\n+  }\n+\n+  static NamedSharding MaximalSharding(\n+      int64_t device_id, absl::Span<const OpMetadata> metadata = {}) {\n+    return NamedSharding(Mesh(device_id), /*dim_shardings=*/{},\n+                         /*replicated_axes=*/{},\n+                         /*unreduced_axes=*/{}, metadata);\n+  }\n+\n+  bool IsReplicated() const {\n+    return !IsMaximal() &&\n+           absl::c_all_of(dim_shardings_, [](const DimensionSharding& s) {\n+             return s.axes().empty();\n+           });\n+  }\n+\n+  bool IsMaximal() const { return mesh_.IsMaximal(); }\n+\n+  // Returns true if the tile size is the same as the input size.\n+  //\n+  // This checks for both replicated and maximal sharding, as in both cases tile\n+  // size is same as input size.\n+  bool IsTileMaximal() const { return IsReplicated() || IsMaximal(); }\n+\n+  const TileAssignment& device_assignment() const {\n+    return mesh_.device_assignment();\n+  }\n+\n   Mesh mesh_;\n   std::vector<DimensionSharding> dim_shardings_;\n   std::vector<AxisRef> replicated_axes_;"
        },
        {
            "sha": "589d1c7b2e40a96cfdf36417d52919fb8ac1db68",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -913,6 +913,7 @@ xla_cc_test(\n     name = \"hlo_sharding_test\",\n     srcs = [\"hlo_sharding_test.cc\"],\n     deps = [\n+        \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:tile_assignment\","
        },
        {
            "sha": "a00be7f860f013989f742d949e18c7c6207aada9",
            "filename": "third_party/xla/xla/service/hlo_sharding_test.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 12,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_sharding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/29ae5d0a53f28a3cae639ba75354b35ab0038ffc/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_sharding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_sharding_test.cc?ref=29ae5d0a53f28a3cae639ba75354b35ab0038ffc",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/test.h\"\n #include \"xla/hlo/testlib/test_helpers.h\"\n+#include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n@@ -59,31 +60,45 @@ std::vector<OpMetadata> ListMetadata() {\n \n class HloShardingTest : public HloHardwareIndependentTestBase {};\n \n-TEST_F(HloShardingTest, Replicate) {\n-  HloSharding sharding = HloSharding::Replicate();\n+// TODO(b/456418464): Parameterize `HloShardingTest` itself after supporting\n+// NamedSharding in all methods.\n+class HloShardingRepresentationTest\n+    : public HloShardingTest,\n+      public ::testing::WithParamInterface<bool> {};\n+\n+TEST_P(HloShardingRepresentationTest, Replicate) {\n+  bool use_named_sharding = GetParam();\n+  HloSharding sharding = HloSharding::Replicate({}, use_named_sharding);\n+  EXPECT_EQ(sharding.UseNamedShardingLeaf(), use_named_sharding);\n   EXPECT_TRUE(sharding.IsReplicated());\n   EXPECT_TRUE(sharding.IsTileMaximal());\n   EXPECT_TRUE(sharding.UsesDevice(0));\n   EXPECT_TRUE(sharding.UsesDevice(65535));\n \n-  HloSharding other = HloSharding::Replicate();\n+  HloSharding other = HloSharding::Replicate({}, use_named_sharding);\n   EXPECT_EQ(other, sharding);\n+  EXPECT_NE(HloSharding::Replicate(),\n+            HloSharding::Replicate({}, /*use_named_sharding=*/true));\n \n   EXPECT_IS_OK(sharding.Validate(ShapeUtil::MakeShape(U32, {4}),\n                                  /*num_devices=*/2));\n   EXPECT_FALSE(sharding.HasUniqueDevice());\n }\n \n-TEST_F(HloShardingTest, DevicePlacement) {\n-  HloSharding sharding = HloSharding::AssignDevice(5);\n+TEST_P(HloShardingRepresentationTest, DevicePlacement) {\n+  bool use_named_sharding = GetParam();\n+  HloSharding sharding = HloSharding::AssignDevice(5, {}, use_named_sharding);\n+  EXPECT_EQ(sharding.UseNamedShardingLeaf(), use_named_sharding);\n   EXPECT_FALSE(sharding.IsReplicated());\n   EXPECT_TRUE(sharding.IsTileMaximal());\n   EXPECT_FALSE(sharding.UsesDevice(0));\n   EXPECT_TRUE(sharding.UsesDevice(5));\n   EXPECT_EQ(5, sharding.GetUniqueDevice());\n \n-  HloSharding other = HloSharding::Replicate();\n+  HloSharding other = HloSharding::Replicate({}, use_named_sharding);\n   EXPECT_NE(other, sharding);\n+  EXPECT_NE(HloSharding::AssignDevice(5),\n+            HloSharding::AssignDevice(5, {}, /*use_named_sharding=*/true));\n \n   EXPECT_IS_OK(sharding.Validate(ShapeUtil::MakeShape(U32, {4}),\n                                  /*num_devices=*/6));\n@@ -337,21 +352,31 @@ TEST_F(HloShardingTest, V1V2SubgroupEquivalence) {\n }\n \n // Tests that empty tuple is supported.\n-TEST_F(HloShardingTest, EmptySingleTuple) {\n-  HloSharding sharding = HloSharding::SingleTuple(ShapeUtil::MakeTupleShape({}),\n-                                                  HloSharding::AssignDevice(0));\n+TEST_P(HloShardingRepresentationTest, EmptySingleTuple) {\n+  bool use_named_sharding = GetParam();\n+  HloSharding sharding = HloSharding::SingleTuple(\n+      ShapeUtil::MakeTupleShape({}),\n+      HloSharding::AssignDevice(0, {}, use_named_sharding));\n   EXPECT_TRUE(sharding.ExtractSingleSharding());\n+  EXPECT_EQ(sharding.ExtractSingleSharding()->UseNamedShardingLeaf(),\n+            use_named_sharding);\n }\n \n // Tests that empty tuple is not a shard group.\n-TEST_F(HloShardingTest, EmptySingleTupleIsNotShardGroup) {\n-  HloSharding sharding = HloSharding::SingleTuple(ShapeUtil::MakeTupleShape({}),\n-                                                  HloSharding::AssignDevice(0));\n+TEST_P(HloShardingRepresentationTest, EmptySingleTupleIsNotShardGroup) {\n+  bool use_named_sharding = GetParam();\n+  HloSharding sharding = HloSharding::SingleTuple(\n+      ShapeUtil::MakeTupleShape({}),\n+      HloSharding::AssignDevice(0, {}, use_named_sharding));\n   EXPECT_FALSE(sharding.IsShardGroup());\n   EXPECT_FALSE(sharding.IsShardAs());\n   EXPECT_FALSE(sharding.IsShardLike());\n }\n \n+INSTANTIATE_TEST_SUITE_P(HloShardingRepresentationTest,\n+                         HloShardingRepresentationTest,\n+                         ::testing::Values(false, true));\n+\n TEST_F(HloShardingTest, NestedTuple) {\n   // nested_tuple_shape = (f32[], (f32[3]), f32[4, 6])\n   Shape nested_tuple_shape = ShapeUtil::MakeTupleShape({"
        }
    ],
    "stats": {
        "total": 313,
        "additions": 270,
        "deletions": 43
    }
}