{
    "author": "pemeliya",
    "message": "PR #33819: [ROCM][NFC] gemm regression fp8 and bf16 tests, improved logging in rocm_executor\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33819\n\nüìù Summary of Changes\nThis PR adds a functional test checking for a particular failed gemm config on rocm platform. But this might also be useful on cuda. To detect this bug, the matrix C of a gemm must be placed to pinned host memory (to slowdown gpu-side access).\n\nAnother test - regression_dot_test.cc - is checking for bf16 gemm failure on rocm platform\nBesides, I have also improved logging in rocm_executor.cc when host memory is allocated.\n\nüéØ Justification\nExplain why this change is important and which workload benefits from this\nchange.\n\nüöÄ Kind of Contribution\n üß™ Tests\n\n@xla-rotation Can you have a look please?\nCopybara import of the project:\n\n--\nc9f98a8a165ce4c6fd5c5dbbdccee2ec6bfd7d64 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nadded gemm regression fp8 test\n\n--\n1fc3cd30340e447d5e45d07575b327ec321f943f by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nadded more gemm tests\n\n--\n72b8996e87aa61ca2cd30bf364776edc75407536 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nupdated the tests\n\nMerging this change closes #33819\n\nPiperOrigin-RevId: 839170082",
    "sha": "72d13ec6ef9bdef46f4a716762dd152eee4a79f0",
    "files": [
        {
            "sha": "7211633fd4e2cc3516bdf9640f2e1cbdf8b7d33d",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=72d13ec6ef9bdef46f4a716762dd152eee4a79f0",
            "patch": "@@ -605,6 +605,22 @@ xla_test(\n     ],\n )\n \n+xla_test(\n+    name = \"regression_dot_test\",\n+    srcs = [\"regression_dot_test.cc\"],\n+    backends = [\"gpu\"],\n+    tags = [\"test_migrated_to_hlo_runner_pjrt\"],\n+    deps = [\n+        \"//xla:error_spec\",\n+        \"//xla:literal_util\",\n+        \"//xla/tests:hlo_pjrt_interpreter_reference_mixin\",\n+        \"//xla/tests:hlo_pjrt_test_base\",\n+        \"//xla/tests:test_utils\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n xla_test(\n     name = \"sorting_test\",\n     srcs = [\"sorting_test.cc\"],"
        },
        {
            "sha": "d5bce4d4325985874f1f7aa8bc42b0baaa11ee35",
            "filename": "third_party/xla/xla/service/gpu/tests/regression_dot_test.cc",
            "status": "added",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fregression_dot_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fregression_dot_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fregression_dot_test.cc?ref=72d13ec6ef9bdef46f4a716762dd152eee4a79f0",
            "patch": "@@ -0,0 +1,62 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <utility>\n+\n+#include <gtest/gtest.h>\n+#include \"xla/error_spec.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/tests/hlo_pjrt_interpreter_reference_mixin.h\"\n+#include \"xla/tests/hlo_pjrt_test_base.h\"\n+#include \"xla/tests/test_utils.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+using RegressionDotTest = HloPjRtInterpreterReferenceMixin<HloPjRtTestBase>;\n+\n+TEST_F(RegressionDotTest, LargeBF16Gemm) {\n+  const char* hlo_text = R\"(\n+HloModule test\n+sum {\n+  a = bf16[] parameter(0)\n+  b = bf16[] parameter(1)\n+  ROOT add = bf16[] add(a, b)\n+}\n+ENTRY main {\n+  X = bf16[24480,64] parameter(0)\n+  Y = bf16[64,3072] parameter(1)\n+  zero = bf16[] constant(0)\n+  prod = bf16[24480,3072] dot(X, Y), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  ROOT R = bf16[3072] reduce(prod, zero), dimensions={0}, to_apply=sum\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo_text));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto fake_arguments,\n+      MakeFakeArguments(module.get(), /*pseudo_random=*/true,\n+                        /*use_large_range=*/false));\n+\n+  EXPECT_TRUE(RunAndCompare(std::move(module),\n+                            LiteralUtil::MakePointers(fake_arguments),\n+                            ErrorSpec{1e-3, 1e-2}));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "9af6e8e1adeff49c760cbd7949497dd5f39e3de8",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc?ref=72d13ec6ef9bdef46f4a716762dd152eee4a79f0",
            "patch": "@@ -491,14 +491,14 @@ absl::StatusOr<void*> HostAllocate(Context* context, uint64_t bytes) {\n   TF_RETURN_IF_ERROR(\n       ToStatus(wrap::hipHostMalloc(&host_mem, bytes, hipHostMallocPortable),\n                \"failed to allocate host memory\"));\n+  VLOG(2) << \"allocated \" << host_mem << \" for context \" << context << \" of \"\n+          << bytes << \" bytes of host memory\";\n   return host_mem;\n }\n \n absl::StatusOr<std::unique_ptr<MemoryAllocation>> AllocateHostMemory(\n     RocmContext* rocm_context, uint64_t size) {\n   TF_ASSIGN_OR_RETURN(void* ptr, HostAllocate(rocm_context, size));\n-  VLOG(2) << \"allocated \" << ptr << \" for context \" << rocm_context << \" of \"\n-          << size << \" bytes of host memory\";\n   return std::make_unique<GenericMemoryAllocation>(\n       ptr, size, [rocm_context](void* location, uint64_t size) {\n         hipError_t res = wrap::hipHostFree(location);"
        },
        {
            "sha": "fbb647a44855a680ba56025edf5b50bff895ce16",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD?ref=72d13ec6ef9bdef46f4a716762dd152eee4a79f0",
            "patch": "@@ -261,6 +261,9 @@ xla_test(\n         \"//xla/runtime/large_hlo_snapshot_serialization:serialization\",\n         \"//xla/service:computation_layout\",\n         \"//xla/service:hlo_proto_cc\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/tests:test_utils\",\n         \"//xla/tests:xla_test_backend_predicates\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "3d789f5dfbd2a1d5e79ff2842c4c07ae6c01ad4b",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/data/fp8_gemm_loop.hlo",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fdata%2Ffp8_gemm_loop.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fdata%2Ffp8_gemm_loop.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fdata%2Ffp8_gemm_loop.hlo?ref=72d13ec6ef9bdef46f4a716762dd152eee4a79f0",
            "patch": "@@ -0,0 +1,50 @@\n+HloModule test\n+\n+sum {\n+  a = f32[] parameter(0)\n+  b = f32[] parameter(1)\n+  ROOT add = f32[] add(a, b)\n+}\n+\n+while_cond {\n+  pp = (s32[], f8e5m2[32768,1024], f8e4m3fn[8192,1024], bf16[32768,8192], bf16[32768,8192]) parameter(0)\n+  idx = s32[] get-tuple-element(pp), index=0\n+  total = s32[] constant(20)\n+  ROOT cmp = pred[] compare(idx, total), direction=LT\n+}\n+\n+while_body {\n+  pp = (s32[], f8e5m2[32768,1024], f8e4m3fn[8192,1024], bf16[32768,8192], bf16[32768,8192]) parameter(0)\n+  idx = s32[] get-tuple-element(pp), index=0\n+\n+  X = f8e5m2[32768,1024] get-tuple-element(pp), index=1\n+  Y = f8e4m3fn[8192,1024] get-tuple-element(pp), index=2\n+  Z = bf16[32768,8192] get-tuple-element(pp), index=3\n+  Prev = bf16[32768,8192] get-tuple-element(pp), index=4\n+\n+  dot_XY = bf16[32768,8192] dot(X, Y), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+  add_XY = bf16[32768,8192] add(dot_XY, Z)\n+\n+  New = bf16[32768,8192] add(add_XY, Prev)\n+\n+  inc = s32[] constant(1)\n+  idx2 = s32[] add(idx, inc)\n+  \n+  ROOT TT = (s32[], f8e5m2[32768,1024], f8e4m3fn[8192,1024], bf16[32768,8192], bf16[32768,8192]) tuple(idx2, X, Y, Z, New)\n+} \n+\n+ENTRY main {\n+  X = f8e5m2[32768,1024] parameter(0)\n+  Y = f8e4m3fn[8192,1024] parameter(1)\n+  Z = bf16[32768,8192] parameter(2)\n+  c0 = s32[] constant(0)\n+  zero = bf16[] constant(0)\n+  W = bf16[32768,8192] broadcast(zero), dimensions={}\n+\n+  TT = (s32[], f8e5m2[32768,1024], f8e4m3fn[8192,1024], bf16[32768,8192], bf16[32768,8192]) tuple(c0, X, Y, Z, W)\n+  while = (s32[], f8e5m2[32768,1024], f8e4m3fn[8192,1024], bf16[32768,8192], bf16[32768,8192]) while(TT), condition=while_cond, body=while_body\n+  R = bf16[32768,8192] get-tuple-element(while), index=4\n+  R32 = f32[32768,8192] convert(R)\n+  zero32 = f32[] constant(0)\n+  ROOT red = f32[32768] reduce(R32, zero32), dimensions={1}, to_apply=sum\n+}"
        },
        {
            "sha": "078f3a2746f328b0027190313f974181d92d65a0",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/functional_hlo_runner_test.cc",
            "status": "modified",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Ffunctional_hlo_runner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72d13ec6ef9bdef46f4a716762dd152eee4a79f0/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Ffunctional_hlo_runner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Ffunctional_hlo_runner_test.cc?ref=72d13ec6ef9bdef46f4a716762dd152eee4a79f0",
            "patch": "@@ -44,7 +44,10 @@ limitations under the License.\n #include \"xla/runtime/large_hlo_snapshot_serialization/serialization.h\"\n #include \"xla/service/computation_layout.h\"\n #include \"xla/service/hlo.pb.h\"\n+#include \"xla/service/platform_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/tests/test_utils.h\"\n #include \"xla/tools/multihost_hlo_runner/create_client.h\"\n #include \"xla/tools/multihost_hlo_runner/hlo_input_output_format.h\"\n #include \"xla/tools/multihost_hlo_runner/profiler_interface.h\"\n@@ -104,6 +107,111 @@ TEST_F(FunctionalHloRunnerTest, SingleDeviceHlo) {\n       running_options, {GetHloPath(\"single_device.hlo\")}, InputFormat::kText));\n }\n \n+TEST_F(FunctionalHloRunnerTest, SingleDevicePinnedHostZeroInputs) {\n+  if (test::DeviceTypeIs(test::kCpu)) {\n+    GTEST_SKIP() << \"This test is specialized for GPU platform!\";\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::string platform_name,\n+                          PlatformUtil::CanonicalPlatformName(\"gpu\"));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(se::Platform * platform,\n+                          se::PlatformManager::PlatformWithName(\n+                              absl::AsciiStrToUpper(platform_name)));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto executors,\n+                          PlatformUtil::GetStreamExecutors(platform));\n+  EXPECT_TRUE(!executors.empty());\n+  const auto& desc = executors[0]->GetDeviceDescription();\n+  if (platform_name == \"rocm\") {\n+    if (!desc.rocm_compute_capability().has_fp8_support()) {\n+      GTEST_SKIP() << \"This test requires fp8 support!\";\n+    }\n+  }\n+\n+  GpuClientOptions gpu_opts;\n+  gpu_opts.allocator_config.kind = GpuAllocatorConfig::Kind::kPlatform;\n+  gpu_opts.should_stage_host_to_device_transfers = false;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::PjRtClient> client,\n+                          CreateGpuClient(gpu_opts));\n+\n+  xla::DebugOptions debug_options;\n+  FunctionalHloRunner::PreprocessingOptions preproc_options;\n+  FunctionalHloRunner::RawCompileOptions raw_compile_options;\n+  raw_compile_options.num_replicas = 1;\n+  raw_compile_options.num_partitions = 1;\n+\n+  auto set_host_memory_space = [](ShapeLayout* layout) {\n+    Shape shape = layout->shape();\n+    ShapeUtil::ForEachMutableSubshape(&shape, [](Shape* subshape,\n+                                                 const ShapeIndex& index) {\n+      if (subshape->IsArray()) {\n+        subshape->mutable_layout()->set_memory_space(Layout::kHostMemorySpace);\n+      }\n+    });\n+    *layout = ShapeLayout(shape);\n+  };\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto hlo_module_and_arguments,\n+      FunctionalHloRunner::LoadHloModuleAndArguments(\n+          {GetHloPath(\"fp8_gemm_loop.hlo\")}, InputFormat::kText));\n+  auto hlo_module = std::move(hlo_module_and_arguments.hlo_module);\n+\n+  auto& entry_layout =\n+      *hlo_module->mutable_config().mutable_entry_computation_layout();\n+\n+  FunctionalHloRunner::PerDeviceLiteralVecType arguments;\n+  auto& args_vector = arguments[0];\n+\n+  const auto& params =\n+      hlo_module->entry_computation()->parameter_instructions();\n+  args_vector.resize(params.size());\n+  for (size_t i = 0; i < params.size(); ++i) {\n+    const auto& shape = params[i]->shape();\n+    EXPECT_TRUE(shape.IsArray());\n+    Literal literal(shape);\n+    primitive_util::ArrayTypeSwitch(\n+        [&](auto prim_const) {\n+          using T = primitive_util::NativeTypeOf<prim_const>;\n+          int idx = 0;\n+          for (auto& val : literal.data<T>()) {\n+            // Zero-out all input arguments except for the matrix B\n+            val = static_cast<T>(i == 1 ? 0.05f + (idx + 1) / 100.f : 0.0f);\n+          }\n+          idx++;\n+        },\n+        shape.element_type());\n+    args_vector[i] = std::move(literal);\n+  }\n+\n+  EXPECT_EQ(entry_layout.parameter_count(), 3);\n+  // Allocate the matrix C in pinned mem that would make the loading slow\n+  set_host_memory_space(entry_layout.mutable_parameter_layout(2));\n+\n+  FunctionalHloRunner::RunningOptions running_options;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      CompileOptions compile_options,\n+      FunctionalHloRunner::CreateCompileOptions(*client, raw_compile_options));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto output, FunctionalHloRunner::CompileAndRun(\n+                       *client, debug_options, preproc_options, compile_options,\n+                       running_options, hlo_module.get(), arguments));\n+\n+  for (const auto& [dev, vec] : output) {\n+    for (const auto& item : vec) {\n+      bool res = item.EachCellUntilFailure<float>(\n+          [](absl::Span<const int64_t> indices, auto value) -> bool {\n+            return Eigen::numext::isfinite(value) &&\n+                   Eigen::numext::abs(value) < 1e-8;\n+          });\n+      EXPECT_TRUE(res);\n+    }\n+  }\n+}\n+\n TEST_F(FunctionalHloRunnerTest, SingleDeviceHloWithRandomEngine) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::PjRtClient> client,\n                           GetPjRtClient());"
        }
    ],
    "stats": {
        "total": 243,
        "additions": 241,
        "deletions": 2
    }
}