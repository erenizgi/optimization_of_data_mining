{
    "author": "apivovarov",
    "message": "Refactor GPU: Move GetCurrentId to p2p_thunk_common.\n\nThis change deduplicates the `GetCurrentId` function by moving it from `collective_permute_thunk.cc` and `nvshmem_collective_permute_thunk.cc` to `p2p_thunk_common.cc`, and renaming it to `GetCollectiveCurrentId`. The build dependencies are updated accordingly.\n\nPiperOrigin-RevId: 829506840",
    "sha": "9f7f4c7bc67a571887b29ba4af02666b1c6731d3",
    "files": [
        {
            "sha": "d60c1936f012f32d9a004e207b4dfa3062fd3b05",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=9f7f4c7bc67a571887b29ba4af02666b1c6731d3",
            "patch": "@@ -1550,7 +1550,6 @@ cc_library(\n         \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:computation_placer\",\n-        \"//xla/service:global_device_id\",\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n@@ -1630,6 +1629,7 @@ cc_library(\n     hdrs = [\"p2p_thunk_common.h\"],\n     deps = [\n         \":collective_thunk\",\n+        \":thunk\",\n         \"//xla:executable_run_options\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n@@ -1638,6 +1638,8 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/service:collective_ops_utils\",\n+        \"//xla/service:computation_placer_hdr\",\n+        \"//xla/service:global_device_id\",\n         \"//xla/service:source_target_pairs\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2686,7 +2688,6 @@ cc_library(\n         \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:computation_placer\",\n-        \"//xla/service:global_device_id\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/stream_executor:device_memory\","
        },
        {
            "sha": "4efcabc1d67d8d74af09ac2171ae4eb3edf32835",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 20,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=9f7f4c7bc67a571887b29ba4af02666b1c6731d3",
            "patch": "@@ -46,7 +46,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/computation_placer.h\"\n-#include \"xla/service/global_device_id.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/rendezvous.h\"\n@@ -62,21 +61,6 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-absl::StatusOr<const int64_t> GetCurrentId(\n-    Thunk::CollectiveExecuteParams* collective_params,\n-    const P2PConfig& config) {\n-  GlobalDeviceId global_device_id = collective_params->global_device_id;\n-  TF_ASSIGN_OR_RETURN(\n-      const DeviceAssignment::LogicalID current_logical_id,\n-      collective_params->device_assn->LogicalIdForDevice(global_device_id));\n-  const int64_t current_id =\n-      config.config.group_mode ==\n-              CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_CROSS_REPLICA\n-          ? current_logical_id.replica_id\n-          : current_logical_id.computation_id;\n-  return current_id;\n-}\n-\n bool IsLocalPeerTransfer(const P2PConfig::SourceTargetMapEntry& source_target,\n                          const int64_t current_id, const int64_t device_count) {\n   const std::optional<int64_t> source_id = source_target.source;\n@@ -188,8 +172,9 @@ absl::Status CollectivePermuteStartThunk::Initialize(\n   VLOG(5) << \"Local device count: \" << device_count_;\n \n   if (p2p_memcpy_enabled_) {\n-    TF_ASSIGN_OR_RETURN(const int64_t current_id,\n-                        GetCurrentId(params.collective_params, config_));\n+    TF_ASSIGN_OR_RETURN(\n+        const int64_t current_id,\n+        GetCollectiveCurrentId(params.collective_params, config_));\n     {\n       absl::MutexLock lock(barrier_mutex_);\n       if (receiver_barrier_events_.find(current_id) ==\n@@ -254,8 +239,9 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n       ConvertToDeviceBuffers(params,\n                              std::vector<CollectiveThunk::Buffer>(buffers_),\n                              config_.config.operand_element_type));\n-  TF_ASSIGN_OR_RETURN(const int64_t current_id,\n-                      GetCurrentId(params.collective_params, config_));\n+  TF_ASSIGN_OR_RETURN(\n+      const int64_t current_id,\n+      GetCollectiveCurrentId(params.collective_params, config_));\n   std::string device_string = GetDeviceString(*params.collective_params);\n \n   const P2PConfig::SourceTargetMapEntry source_target ="
        },
        {
            "sha": "455d03b08eb15efd234cca76bbf46e1609d3b136",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_permute_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 22,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc?ref=9f7f4c7bc67a571887b29ba4af02666b1c6731d3",
            "patch": "@@ -43,8 +43,6 @@ limitations under the License.\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/computation_placer.h\"\n-#include \"xla/service/global_device_id.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -55,24 +53,6 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n-namespace {\n-\n-absl::StatusOr<const int64_t> GetCurrentId(\n-    Thunk::CollectiveExecuteParams* collective_params,\n-    const P2PConfig& config) {\n-  GlobalDeviceId global_device_id = collective_params->global_device_id;\n-  TF_ASSIGN_OR_RETURN(\n-      const DeviceAssignment::LogicalID current_logical_id,\n-      collective_params->device_assn->LogicalIdForDevice(global_device_id));\n-  const int64_t current_id =\n-      config.config.group_mode ==\n-              CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_CROSS_REPLICA\n-          ? current_logical_id.replica_id\n-          : current_logical_id.computation_id;\n-  return current_id;\n-}\n-\n-}  // namespace\n \n NvshmemCollectivePermuteStartThunk::NvshmemCollectivePermuteStartThunk(\n     ThunkInfo thunk_info, const HloCollectivePermuteInstruction* instr,\n@@ -153,8 +133,9 @@ absl::Status NvshmemCollectivePermuteStartThunk::RunNvshmemCollective(\n       ConvertToDeviceBuffers(params,\n                              std::vector<CollectiveThunk::Buffer>(buffers_),\n                              config_.config.operand_element_type));\n-  TF_ASSIGN_OR_RETURN(const int64_t current_id,\n-                      GetCurrentId(params.collective_params, config_));\n+  TF_ASSIGN_OR_RETURN(\n+      const int64_t current_id,\n+      GetCollectiveCurrentId(params.collective_params, config_));\n   std::string device_string =\n       CollectiveThunk::GetDeviceString(*params.collective_params);\n "
        },
        {
            "sha": "17e98e101dd38d7ebc2fbc7c75afdcd29b6fc406",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc?ref=9f7f4c7bc67a571887b29ba4af02666b1c6731d3",
            "patch": "@@ -26,12 +26,15 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/service/global_device_id.h\"\n #include \"xla/service/source_target_pairs.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n@@ -189,5 +192,22 @@ AsyncStreamKind GetStreamKindForP2P(const HloInstruction* instr) {\n   return AsyncStreamKind::ASYNC_STREAM_KIND_P2P0;\n }\n \n+// Retrieves the current collective ID (replica or partition ID) for the\n+// executing device.\n+absl::StatusOr<const int64_t> GetCollectiveCurrentId(\n+    Thunk::CollectiveExecuteParams* collective_params,\n+    const P2PConfig& config) {\n+  GlobalDeviceId global_device_id = collective_params->global_device_id;\n+  TF_ASSIGN_OR_RETURN(\n+      const DeviceAssignment::LogicalID current_logical_id,\n+      collective_params->device_assn->LogicalIdForDevice(global_device_id));\n+  const int64_t current_id =\n+      config.config.group_mode ==\n+              CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_CROSS_REPLICA\n+          ? current_logical_id.replica_id\n+          : current_logical_id.computation_id;\n+  return current_id;\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "c7ecc95e81c2ae9413c7008b3d957d923f1fbfcd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f7f4c7bc67a571887b29ba4af02666b1c6731d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h?ref=9f7f4c7bc67a571887b29ba4af02666b1c6731d3",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -102,6 +103,9 @@ P2PConfig GetP2PConfigForSendRecv(const HloSendRecvInstruction* instr,\n // instruction.\n AsyncStreamKind GetStreamKindForP2P(const HloInstruction* instr);\n \n+absl::StatusOr<const int64_t> GetCollectiveCurrentId(\n+    Thunk::CollectiveExecuteParams* collective_params, const P2PConfig& config);\n+\n }  // namespace gpu\n }  // namespace xla\n "
        }
    ],
    "stats": {
        "total": 80,
        "additions": 36,
        "deletions": 44
    }
}