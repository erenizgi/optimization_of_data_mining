{
    "author": "bixia1",
    "message": "Fix the verification of special custom-call targets Pin/Unpin to consider layout sensitivity.\n\nPiperOrigin-RevId: 799200099",
    "sha": "a072ade243565ec542993ab9c40f50f008f2b827",
    "files": [
        {
            "sha": "d2ed6e760e0abbf959cc11e44765adacd26b9345",
            "filename": "third_party/xla/xla/service/hlo_verifier.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 13,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a072ade243565ec542993ab9c40f50f008f2b827/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a072ade243565ec542993ab9c40f50f008f2b827/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc?ref=a072ade243565ec542993ab9c40f50f008f2b827",
            "patch": "@@ -3470,7 +3470,8 @@ absl::Status CheckBufferHasUniqueWriters(const HloInstruction* inst) {\n       });\n }\n \n-absl::Status VerifyPin(const HloCustomCallInstruction* inst) {\n+absl::Status VerifyPin(const HloCustomCallInstruction* inst,\n+                       bool layout_sentitive) {\n   if (inst->operand_count() != 1 || !inst->operand(0)->shape().IsArray() ||\n       inst->operand(0)->shape().IsBuffer()) {\n     return InvalidArgument(\n@@ -3481,8 +3482,8 @@ absl::Status VerifyPin(const HloCustomCallInstruction* inst) {\n     return InvalidArgument(\"custom-call to Pin must have one buffer result\");\n   }\n \n-  if (!xla::Shape::Equal()(inst->operand(0)->shape(),\n-                           inst->shape().buffer_shape())) {\n+  if (!xla::Shape::Equal().IgnoreLayout(!layout_sentitive)(\n+          inst->operand(0)->shape(), inst->shape().buffer_shape())) {\n     return InvalidArgument(\n         \"custom-call to Pin must have the same shape as the operand\");\n   }\n@@ -3508,7 +3509,8 @@ absl::Status VerifyCreateBuffer(const HloInstruction* inst) {\n   return CheckBufferHasUniqueWriter(inst, {});\n }\n \n-absl::Status VerifyUnpin(const HloCustomCallInstruction* inst) {\n+absl::Status VerifyUnpin(const HloCustomCallInstruction* inst,\n+                         bool layout_sentitive) {\n   if (inst->operand_count() != 1 || !inst->operand(0)->shape().IsBuffer()) {\n     return InvalidArgument(\"custom-call to Unpin must have one buffer operand\");\n   }\n@@ -3518,8 +3520,8 @@ absl::Status VerifyUnpin(const HloCustomCallInstruction* inst) {\n         \"custom-call to Unpin must have one array non-buffer result\");\n   }\n \n-  if (!xla::Shape::Equal()(inst->operand(0)->shape().buffer_shape(),\n-                           inst->shape())) {\n+  if (!xla::Shape::Equal().IgnoreLayout(!layout_sentitive)(\n+          inst->operand(0)->shape().buffer_shape(), inst->shape())) {\n     return InvalidArgument(\n         \"custom-call to Unpin must have the same shape as the operand\");\n   }\n@@ -3642,15 +3644,16 @@ absl::Status VerifyBuffersInResults(\n //   output-to-operand-aliasing.\n // - An HLO buffer result can only be updated at most once.\n //\n-absl::Status VerifyCustomCall(const HloCustomCallInstruction* inst) {\n+absl::Status VerifyCustomCall(const HloCustomCallInstruction* inst,\n+                              bool layout_sentitive) {\n   if (inst->IsCustomCall(kPinCustomCallTarget)) {\n-    return VerifyPin(Cast<HloCustomCallInstruction>(inst));\n+    return VerifyPin(Cast<HloCustomCallInstruction>(inst), layout_sentitive);\n   }\n   if (inst->IsCustomCall(kCreateBufferCustomCallTarget)) {\n     return VerifyCreateBuffer(inst);\n   }\n   if (inst->IsCustomCall(kUnpinCustomCallTarget)) {\n-    return VerifyUnpin(Cast<HloCustomCallInstruction>(inst));\n+    return VerifyUnpin(Cast<HloCustomCallInstruction>(inst), layout_sentitive);\n   }\n \n   TF_RETURN_IF_ERROR(VerifyBuffersInOperands(inst));\n@@ -3674,12 +3677,12 @@ absl::Status VerifyNoBuffersInContext(const HloInstruction* inst) {\n   return absl::OkStatus();\n }\n \n-absl::Status VerifyBuffers(const HloModule& module) {\n+absl::Status VerifyBuffers(const HloModule& module, bool layout_sentitive) {\n   for (auto* comp : module.computations()) {\n     for (auto* inst : comp->instructions()) {\n       if (inst->opcode() == HloOpcode::kCustomCall) {\n-        TF_RETURN_IF_ERROR(\n-            VerifyCustomCall(Cast<HloCustomCallInstruction>(inst)));\n+        TF_RETURN_IF_ERROR(VerifyCustomCall(\n+            Cast<HloCustomCallInstruction>(inst), layout_sentitive));\n       } else if (inst->opcode() == HloOpcode::kWhile) {\n         TF_RETURN_IF_ERROR(CheckBufferHasUniqueWriters(inst));\n       } else if (inst->opcode() == HloOpcode::kParameter) {\n@@ -3738,7 +3741,8 @@ absl::StatusOr<bool> HloVerifier::Run(\n       }\n     }\n \n-    TF_RETURN_IF_ERROR(VerifyBuffers(*module));\n+    TF_RETURN_IF_ERROR(VerifyBuffers(\n+        *module, target_metadata_->GetVerifierOpts().IsLayoutSensitive()));\n \n     TF_RETURN_IF_ERROR(shape_verifier->VerifyEntryComputationLayout(*module));\n "
        },
        {
            "sha": "402b96f0f0853d13e494eabab08181b9ff5b9823",
            "filename": "third_party/xla/xla/service/hlo_verifier_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a072ade243565ec542993ab9c40f50f008f2b827/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a072ade243565ec542993ab9c40f50f008f2b827/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc?ref=a072ade243565ec542993ab9c40f50f008f2b827",
            "patch": "@@ -4775,5 +4775,41 @@ TEST_F(HloVerifierTest, ScaledDotWithScaleNonContractingDimSucceeds) {\n   EXPECT_THAT(verifier().Run(module.get()), absl_testing::IsOkAndHolds(false));\n }\n \n+TEST_F(HloVerifierTest, VerifyBuffersLayoutChangeInPinAllowed) {\n+  const char* const hlo = R\"(\n+  HloModule module\n+\n+  ENTRY computation {\n+    p0 = f32[4,2]{1,0} parameter(0)\n+    a = b(f32[4,2]{0,1}) custom-call(p0), custom_call_target=\"Pin\",\n+      output_to_operand_aliasing={{}:(0, {})}\n+    ROOT c = f32[4,2]{0,1} custom-call(a), custom_call_target=\"Unpin\",\n+      output_to_operand_aliasing={{}:(0, {})}\n+  })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(hlo));\n+  auto status = verifier().Run(module.get()).status();\n+  ASSERT_TRUE(status.ok());\n+}\n+\n+TEST_F(HloVerifierTestLayoutSensitive,\n+       VerifyBuffersLayoutChangeInPinNotAllowed) {\n+  const char* const hlo = R\"(\n+  HloModule module\n+\n+  ENTRY computation {\n+    p0 = f32[4,2]{1,0} parameter(0)\n+    a = b(f32[4,2]{0,1}) custom-call(p0), custom_call_target=\"Pin\",\n+      output_to_operand_aliasing={{}:(0, {})}\n+    ROOT c = f32[4,2]{0,1} custom-call(a), custom_call_target=\"Unpin\",\n+      output_to_operand_aliasing={{}:(0, {})}\n+  })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(hlo));\n+  auto status = verifier().Run(module.get()).status();\n+  ASSERT_FALSE(status.ok());\n+  EXPECT_THAT(status.message(), HasSubstr(\"Different aliasing shapes\"));\n+}\n+\n }  // namespace\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 53,
        "deletions": 13
    }
}