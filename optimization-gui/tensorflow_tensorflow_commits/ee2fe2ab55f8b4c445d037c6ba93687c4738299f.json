{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Cleanup memory space assignment and add tests\n\nPiperOrigin-RevId: 813754871",
    "sha": "ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
    "files": [
        {
            "sha": "0022b69e85b023b283ec32033f1f5107585bc9f1",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -111,6 +111,7 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:transfer_manager\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n+        \"//xla/service/gpu:gpu_memory_space_assignment\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n@@ -168,7 +169,6 @@ cc_library(\n         \"//xla/service/gpu:gpu_compiler\",\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:gpu_executable\",\n-        \"//xla/service/gpu:gpu_memory_space_assignment\",\n         \"//xla/service/gpu:stream_executor_util\",\n     ]) + if_cuda([\n         # keep sorted"
        },
        {
            "sha": "19660851937f98e6c704e3b25e49a6e1d60b4991",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 11,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -86,6 +86,7 @@ limitations under the License.\n #include \"xla/service/compiler.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/global_device_id.h\"\n+#include \"xla/service/gpu/gpu_memory_space_assignment.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/service/transfer_manager.h\"\n #include \"xla/shape.h\"\n@@ -123,7 +124,6 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_compiler.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/gpu_executable.h\"\n-#include \"xla/service/gpu/gpu_memory_space_assignment.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/xla.pb.h\"\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n@@ -1323,9 +1323,10 @@ GetStreamExecutorGpuDeviceAllocator(\n             CreateCudaAsyncAllocator(\n                 *(ordinal_and_device.second), allocator_config.memory_fraction,\n                 allocator_config.preallocate, false, false, true));\n-        allocators.emplace_back(std::move(async_allocator),\n-                                ordinal_and_device.second->compute_stream(),\n-                                /*memory_space=*/0);\n+        allocators.emplace_back(\n+            std::move(async_allocator),\n+            ordinal_and_device.second->compute_stream(),\n+            /*memory_space=*/(int)xla::gpu::MemorySpaceColor::kDefault);\n       }\n       break;\n     }\n@@ -1340,9 +1341,10 @@ GetStreamExecutorGpuDeviceAllocator(\n                                allocator_config.memory_fraction,\n                                allocator_config.preallocate,\n                                allocator_config.gpu_system_memory_size));\n-        allocators.emplace_back(std::move(bfc_allocator),\n-                                ordinal_and_device.second->compute_stream(),\n-                                /*memory_space=*/0);\n+        allocators.emplace_back(\n+            std::move(bfc_allocator),\n+            ordinal_and_device.second->compute_stream(),\n+            /*memory_space=*/(int)xla::gpu::MemorySpaceColor::kDefault);\n       }\n       break;\n     }\n@@ -1367,9 +1369,10 @@ GetStreamExecutorGpuDeviceAllocator(\n             ordinal_and_device.second->executor(),\n             /*memory_fraction=*/1.0 - allocator_config.memory_fraction,\n             allocator_config.collective_memory_size));\n-    allocators.emplace_back(std::move(collective_bfc_allocator),\n-                            ordinal_and_device.second->compute_stream(),\n-                            /*memory_space=*/1);\n+    allocators.emplace_back(\n+        std::move(collective_bfc_allocator),\n+        ordinal_and_device.second->compute_stream(),\n+        /*memory_space=*/(int)xla::gpu::MemorySpaceColor::kCollective);\n   }\n \n   for (const auto& ordinal_and_device : addressable_devices) {\n@@ -1395,7 +1398,7 @@ GetStreamExecutorGpuDeviceAllocator(\n       allocators.emplace_back(\n           std::move(async_allocator),\n           ordinal_and_device.second->compute_stream(),\n-          /*memory_space=*/gpu::kTempBufferMemorySpaceColor);\n+          /*memory_space=*/(int)xla::gpu::MemorySpaceColor::kTempBuffer);\n     }\n   }\n #endif"
        },
        {
            "sha": "30e9f5de5de02d5cbd57d15ed7c035f93065902c",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -1872,7 +1872,7 @@ TEST(StreamExecutorGpuClientTest, CollectiveMemorySpaceSmoke) {\n \n   // Override default memory space to collective memory space.\n   EXPECT_EQ(buf->on_device_shape().layout().memory_space(),\n-            gpu::kCollectiveMemorySpaceColor);\n+            (int)gpu::MemorySpaceColor::kCollective);\n }\n \n TEST(StreamExecutorGpuClientTest,"
        },
        {
            "sha": "fcb10db54b388c7cb38db4817f8fdb913252121c",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -103,9 +103,12 @@ cc_library(\n \n cc_library(\n     name = \"gpu_memory_space_assignment\",\n+    srcs = [\"gpu_memory_space_assignment.cc\"],\n     hdrs = [\"gpu_memory_space_assignment.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n         \":backend_configs_cc\",\n+        \"//xla:xla_proto_cc\",\n         \"//xla/hlo/analysis:hlo_alias_analysis\",\n         \"//xla/hlo/analysis:hlo_ordering\",\n         \"//xla/hlo/ir:hlo\",\n@@ -119,6 +122,23 @@ cc_library(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"gpu_memory_space_assignment_test\",\n+    srcs = [\"gpu_memory_space_assignment_test.cc\"],\n+    deps = [\n+        \":gpu_memory_space_assignment\",\n+        \"//xla/hlo/analysis:alias_info\",\n+        \"//xla/hlo/analysis:hlo_alias_analysis\",\n+        \"//xla/hlo/analysis:hlo_ordering\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"launch_dimensions\",\n     srcs = ["
        },
        {
            "sha": "3e87ceda922d96b29721f33f33132012fbf2537a",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -251,20 +251,11 @@ absl::StatusOr<std::unique_ptr<BufferAssignment>> RunBufferAssignment(\n   ScopedAnnotation annotation(Phase(\"XlaBufferAssignment\", module));\n \n   const DebugOptions& options = module->config().debug_options();\n-  BufferAssigner::Colorer colorer =\n-      (options.xla_gpu_enable_nccl_user_buffers() ||\n-       options.xla_gpu_experimental_enable_nvshmem() ||\n-       options.xla_gpu_experimental_enable_nccl_symmetric_buffers())\n-          ? CollectiveColorer(\n-                options.xla_gpu_enable_nccl_user_buffers() ||\n-                    options\n-                        .xla_gpu_experimental_enable_nccl_symmetric_buffers(),\n-                options.xla_gpu_experimental_enable_nvshmem())\n-          : BufferAssigner::DefaultColorer();\n \n   std::optional<BufferValue::Color> color =\n       options.xla_gpu_temp_buffer_use_separate_color()\n-          ? std::optional<BufferValue::Color>(kTempBufferMemorySpaceColor)\n+          ? std::optional<BufferValue::Color>(\n+                (int)MemorySpaceColor::kTempBuffer)\n           : std::nullopt;\n \n   TF_ASSIGN_OR_RETURN(\n@@ -275,7 +266,7 @@ absl::StatusOr<std::unique_ptr<BufferAssignment>> RunBufferAssignment(\n           /*color_alignment=*/\n           [](LogicalBuffer::Color) { return kXlaAllocatedBufferAlignBytes; },\n           /*allocate_buffers_for_constants=*/true,\n-          /*colorer=*/colorer,\n+          /*colorer=*/CreateColorer(options),\n           /*must_not_live_out=*/{},\n           /*preset_assignments*/ {},\n           /*private_stack*/ {}, /*heap_buffer_interval_compare*/ nullptr,"
        },
        {
            "sha": "19d511cdd9358c68aab2f9c77088ece41b50529b",
            "filename": "third_party/xla/xla/service/gpu/gpu_memory_space_assignment.cc",
            "status": "added",
            "additions": 152,
            "deletions": 0,
            "changes": 152,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.cc?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -0,0 +1,152 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/gpu_memory_space_assignment.h\"\n+\n+#include \"absl/base/no_destructor.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/match.h\"\n+#include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n+#include \"xla/hlo/analysis/hlo_ordering.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/buffer_value.h\"\n+#include \"xla/service/hlo_value.h\"\n+\n+namespace xla::gpu {\n+\n+// NOTE: The explicit internal constructor is needed as an explicitly typed\n+// variable to avoid a method ambiguity error when compiling for CUDA 12.4.\n+const absl::NoDestructor<absl::flat_hash_set<HloOpcode>>\n+    kSupportedCollectiveOpcodes(absl::flat_hash_set<HloOpcode>{\n+        HloOpcode::kAllReduce,\n+        HloOpcode::kAllReduceStart,\n+        HloOpcode::kAllReduceDone,\n+        HloOpcode::kAllGather,\n+        HloOpcode::kAllGatherStart,\n+        HloOpcode::kAllGatherDone,\n+        HloOpcode::kReduceScatter,\n+        HloOpcode::kCollectivePermute,\n+        HloOpcode::kCollectivePermuteStart,\n+        HloOpcode::kCollectivePermuteDone,\n+        HloOpcode::kAllToAll,\n+    });\n+\n+bool IsNvshmemInstruction(const HloInstruction* inst) {\n+  bool is_nvshmem_collective = false;\n+  if (inst->has_backend_config()) {\n+    auto gpu_config = inst->backend_config<GpuBackendConfig>();\n+    if (!gpu_config.ok()) {\n+      return false;\n+    }\n+    const CollectiveBackendConfig& backend_config =\n+        gpu_config.value().collective_backend_config();\n+    is_nvshmem_collective =\n+        backend_config.backend() == CollectiveBackendConfig::NVSHMEM;\n+  }\n+  return is_nvshmem_collective;\n+}\n+\n+bool IsCollectiveMosaicGpuInstruction(const HloInstruction* inst) {\n+  return inst->opcode() == HloOpcode::kCustomCall &&\n+         (inst->custom_call_target() == \"mosaic_gpu\" ||\n+          inst->custom_call_target() == \"mosaic_gpu_v2\") &&\n+         absl::StrContains(inst->raw_backend_config_string(), \"nvshmem\");\n+}\n+\n+bool IsCollectiveMemoryInstruction(const HloInstruction* inst) {\n+  return kSupportedCollectiveOpcodes->contains(inst->opcode()) ||\n+         // opcode or async wrapped opcode is in kSupportedCollectiveOpcodes.\n+         ((inst->opcode() == HloOpcode::kAsyncStart ||\n+           inst->opcode() == HloOpcode::kAsyncDone) &&\n+          kSupportedCollectiveOpcodes->contains(inst->async_wrapped_opcode()));\n+}\n+\n+bool HasCollectiveMemoryInstruction(const HloValue* input_alias,\n+                                    bool require_nvshmem = false) {\n+  // If any use is a collective instruction, we must color the value to use\n+  // collective memory space.\n+  for (auto& use : input_alias->GetUses()) {\n+    if (IsCollectiveMemoryInstruction(use.instruction) &&\n+        (!require_nvshmem || IsNvshmemInstruction(use.instruction))) {\n+      return true;\n+    }\n+  }\n+  return IsCollectiveMemoryInstruction(input_alias->instruction()) &&\n+         (!require_nvshmem || IsNvshmemInstruction(input_alias->instruction()));\n+}\n+\n+bool HasCollectiveMosaicInstruction(const HloValue* input_alias) {\n+  for (auto& use : input_alias->GetUses()) {\n+    if (IsCollectiveMosaicGpuInstruction(use.instruction)) {\n+      return true;\n+    }\n+  }\n+  return IsCollectiveMosaicGpuInstruction(input_alias->instruction());\n+}\n+\n+// Set memory space to MemorySpaceColor::kCollective for all allocations used by\n+// all-reduce, all-gather, and reduce-scatter. This memory space maps to\n+// collective memory using ncclMemAlloc in the runtime.\n+absl::Status AssignColors(bool use_collective_memory, bool use_nvshmem,\n+                          HloAliasAnalysis* alias_analysis) {\n+  for (HloValue* value : alias_analysis->dataflow_analysis().values()) {\n+    // If the value has a layout with non-default memory space, use the memory\n+    // space from the layout.\n+    const HloPosition& defining_position = value->defining_position();\n+    if (defining_position.shape().has_layout()) {\n+      auto memory_space = defining_position.shape().layout().memory_space();\n+      if (memory_space != 0) {\n+        value->set_color(BufferValue::Color(memory_space));\n+        continue;\n+      }\n+    }\n+\n+    for (const auto& alias :\n+         alias_analysis->GetBufferContainingValue(*value).values()) {\n+      if (HasCollectiveMosaicInstruction(alias) && use_nvshmem) {\n+        // This is a temporary solution until a separate BFC allocator will be\n+        // added for the symmetric memory space.\n+        value->set_color((int)MemorySpaceColor::kCollective);\n+      } else if (((use_collective_memory &&\n+                   HasCollectiveMemoryInstruction(alias)) ||\n+                  (use_nvshmem && HasCollectiveMemoryInstruction(\n+                                      alias, /*require_nvshmem=*/true)))) {\n+        value->set_color((int)MemorySpaceColor::kCollective);\n+      } else if (!value->has_color()) {\n+        value->set_color((int)MemorySpaceColor::kDefault);\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n+BufferAssigner::Colorer CreateColorer(const DebugOptions& option) {\n+  // NCCL old registered buffers.\n+  bool nccl_user_buffers = option.xla_gpu_enable_nccl_user_buffers();\n+  bool nccl_symmetric_buffers =\n+      option.xla_gpu_experimental_enable_nccl_symmetric_buffers();\n+  bool use_nvshmem = option.xla_gpu_experimental_enable_nvshmem();\n+\n+  bool use_collective_memory = nccl_user_buffers || nccl_symmetric_buffers;\n+\n+  return [use_collective_memory, use_nvshmem](HloAliasAnalysis* alias_analysis,\n+                                              const HloOrdering&) {\n+    return AssignColors(use_collective_memory, use_nvshmem, alias_analysis);\n+  };\n+}\n+}  // namespace xla::gpu"
        },
        {
            "sha": "fbcf7fd6568b5087b01057c4863b591b56bc4a87",
            "filename": "third_party/xla/xla/service/gpu/gpu_memory_space_assignment.h",
            "status": "modified",
            "additions": 14,
            "deletions": 107,
            "changes": 121,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment.h?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -16,122 +16,29 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_GPU_MEMORY_SPACE_ASSIGNMENT_H_\n #define XLA_SERVICE_GPU_GPU_MEMORY_SPACE_ASSIGNMENT_H_\n \n-#include <cstdint>\n-\n-#include \"absl/base/no_destructor.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/strings/match.h\"\n-#include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n-#include \"xla/hlo/analysis/hlo_ordering.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/buffer_value.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/hlo_value.h\"\n+#include \"xla/xla.pb.h\"\n \n namespace xla {\n namespace gpu {\n \n-inline constexpr int64_t kCollectiveMemorySpaceColor = 1;\n-inline constexpr int64_t kTempBufferMemorySpaceColor = 2;\n-\n-// Set memory space to kCollectiveMemorySpaceColor for all allocations used by\n-// all-reduce, all-gather, and reduce-scatter. This memory space maps to\n-// collective memory using ncclMemAlloc in the runtime.\n-inline BufferAssigner::Colorer CollectiveColorer(bool use_user_buffers,\n-                                                 bool use_nvshmem) {\n-  return [use_user_buffers, use_nvshmem](HloAliasAnalysis* alias_analysis,\n-                                         const HloOrdering&) {\n-    // NOTE: The explicit internal constructor is needed as an explicitly typed\n-    // variable to avoid a method ambiguity error when compiling for CUDA 12.4.\n-    static const absl::NoDestructor<absl::flat_hash_set<HloOpcode>>\n-        kSupportedOpcodes(absl::flat_hash_set<HloOpcode>{\n-            HloOpcode::kAllReduce,\n-            HloOpcode::kAllReduceStart,\n-            HloOpcode::kAllReduceDone,\n-            HloOpcode::kAllGather,\n-            HloOpcode::kAllGatherStart,\n-            HloOpcode::kAllGatherDone,\n-            HloOpcode::kReduceScatter,\n-            HloOpcode::kCollectivePermute,\n-            HloOpcode::kCollectivePermuteStart,\n-            HloOpcode::kCollectivePermuteDone,\n-            HloOpcode::kAllToAll,\n-        });\n-\n-    auto is_nvshmem_op = [](const HloInstruction* inst) {\n-      bool is_nvshmem_collective = false;\n-      if (inst->has_backend_config()) {\n-        auto gpu_config = inst->backend_config<GpuBackendConfig>();\n-        if (!gpu_config.ok()) {\n-          return false;\n-        }\n-        const CollectiveBackendConfig& backend_config =\n-            gpu_config.value().collective_backend_config();\n-        is_nvshmem_collective =\n-            backend_config.backend() == CollectiveBackendConfig::NVSHMEM;\n-      }\n-      return is_nvshmem_collective;\n-    };\n+enum class MemorySpaceColor {\n+  // Corresponds to stream_executor::MemoryTypes::kDefault or kUnified.\n+  // This memory can be allocated with any device allocation API.\n+  kDefault = 0,\n \n-    auto is_mosaic_gpu_nvshmem_instr = [](const HloInstruction* instr) {\n-      return instr->opcode() == HloOpcode::kCustomCall &&\n-             (instr->custom_call_target() == \"mosaic_gpu\" ||\n-              instr->custom_call_target() == \"mosaic_gpu_v2\") &&\n-             absl::StrContains(instr->raw_backend_config_string(), \"nvshmem\");\n-    };\n-    auto is_collective_memory_instr = [&](const HloInstruction* instr) {\n-      if (use_user_buffers) {\n-        return kSupportedOpcodes->contains(instr->opcode()) ||\n-               // opcode or async wrapped opcode is in kSupportedOpcodes.\n-               ((instr->opcode() == HloOpcode::kAsyncStart ||\n-                 instr->opcode() == HloOpcode::kAsyncDone) &&\n-                kSupportedOpcodes->contains(instr->async_wrapped_opcode()));\n-      }\n-      if (use_nvshmem) {\n-        return is_mosaic_gpu_nvshmem_instr(instr) || is_nvshmem_op(instr);\n-      }\n-      return false;\n-    };\n-    auto has_collective_memory_in_uses = [&](const HloValue* input_alias) {\n-      // If any use is a collective instruction, we must color the value to use\n-      // collective memory space.\n-      for (auto& use : input_alias->GetUses()) {\n-        if (is_collective_memory_instr(use.instruction)) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    };\n-    for (HloValue* value : alias_analysis->dataflow_analysis().values()) {\n-      // If the value has a layout with non-default memory space, use the memory\n-      // space from the layout.\n-      const HloPosition& defining_position = value->defining_position();\n-      if (defining_position.shape().has_layout()) {\n-        auto memory_space = defining_position.shape().layout().memory_space();\n-        if (memory_space != 0) {\n-          value->set_color(BufferValue::Color(memory_space));\n-          continue;\n-        }\n-      }\n+  // Corresponds to stream_executor::MemoryTypes::kCollective.\n+  // This memory should be allocated with ncclMemAlloc in the runtime.\n+  kCollective = 1,\n \n-      auto& buffer = alias_analysis->GetBufferContainingValue(*value);\n-      for (const auto& alias : buffer.values()) {\n-        if (is_collective_memory_instr(alias->instruction()) ||\n-            has_collective_memory_in_uses(alias)) {\n-          value->set_color(kCollectiveMemorySpaceColor);\n-        }\n-      }\n-      if (!value->has_color()) {\n-        value->set_color(0);\n-      }\n-    }\n-    return absl::OkStatus();\n-  };\n-}\n+  // Temp buffers can be allocated within separate memory space (if\n+  // xla_gpu_temp_buffer_use_separate_color is set). This improves cuda-graphs\n+  // performance. See more details in the corresponding flag description.\n+  kTempBuffer = 2,\n+};\n \n+BufferAssigner::Colorer CreateColorer(const DebugOptions& option);\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "e61c35c3fcc9540a68d1d43c6023994fb77ede83",
            "filename": "third_party/xla/xla/service/gpu/gpu_memory_space_assignment_test.cc",
            "status": "added",
            "additions": 283,
            "deletions": 0,
            "changes": 283,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee2fe2ab55f8b4c445d037c6ba93687c4738299f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_memory_space_assignment_test.cc?ref=ee2fe2ab55f8b4c445d037c6ba93687c4738299f",
            "patch": "@@ -0,0 +1,283 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/gpu_memory_space_assignment.h\"\n+\n+#include <memory>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n+#include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n+#include \"xla/hlo/analysis/hlo_ordering.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+class GpuMemorySpaceAssignmentTest : public HloHardwareIndependentTestBase {};\n+\n+TEST_F(GpuMemorySpaceAssignmentTest, TestDefaultColorAssignment) {\n+  absl::string_view kHloModule = R\"(\n+    HloModule m\n+\n+    ENTRY main {\n+      ROOT parameter0 = f32[] parameter(0)\n+    }\n+  )\";\n+\n+  HloModuleConfig config;\n+  auto colorer = CreateColorer(DebugOptions());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule, config));\n+  AliasInfo alias_info;\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloAliasAnalysis> alias_analysis,\n+                          HloAliasAnalysis::Run(module.get(), &alias_info));\n+  DependencyHloOrdering ordering(module.get());\n+  TF_EXPECT_OK(colorer(alias_analysis.get(), ordering));\n+\n+  EXPECT_EQ(alias_analysis->buffers().size(), 1);\n+  EXPECT_EQ(alias_analysis->buffers()[0].values().size(), 1);\n+  EXPECT_EQ(alias_analysis->buffers()[0].values()[0]->has_color(), true);\n+  EXPECT_EQ(alias_analysis->buffers()[0].values()[0]->color(),\n+            (int)MemorySpaceColor::kDefault);\n+}\n+\n+struct CollectiveMemorySpaceAssignmentTestParams {\n+  bool use_nccl_user_buffers;\n+  bool use_nccl_symmetric_buffers;\n+};\n+\n+class GpuCollectiveMemorySpaceAssignmentTest\n+    : public GpuMemorySpaceAssignmentTest,\n+      public ::testing::WithParamInterface<\n+          CollectiveMemorySpaceAssignmentTestParams> {\n+ public:\n+  bool UseNcclUserBuffers() const { return GetParam().use_nccl_user_buffers; }\n+  bool UseNcclSymmetricBuffers() const {\n+    return GetParam().use_nccl_symmetric_buffers;\n+  }\n+};\n+\n+TEST_P(GpuCollectiveMemorySpaceAssignmentTest,\n+       TestCollectiveMemorySpaceAssignment) {\n+  absl::string_view kHloModule = R\"(\n+    HloModule m, replica_count=2\n+\n+    region_0.2 {\n+      Arg_0.3 = f32[] parameter(0)\n+      Arg_1.4 = f32[] parameter(1)\n+      ROOT add.5 = f32[] add(Arg_0.3, Arg_1.4)\n+    }\n+\n+    ENTRY main {\n+      Arg_0.1 = f32[8]{0} parameter(0)\n+      ROOT all-reduce.6 = f32[8]{0} all-reduce(Arg_0.1), replica_groups={}, to_apply=region_0.2\n+    }\n+  )\";\n+\n+  HloModuleConfig config;\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_enable_nccl_user_buffers(UseNcclUserBuffers());\n+  debug_options.set_xla_gpu_experimental_enable_nccl_symmetric_buffers(\n+      UseNcclSymmetricBuffers());\n+  auto colorer = CreateColorer(debug_options);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule, config));\n+  AliasInfo alias_info;\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloAliasAnalysis> alias_analysis,\n+                          HloAliasAnalysis::Run(module.get(), &alias_info));\n+  DependencyHloOrdering ordering(module.get());\n+  TF_EXPECT_OK(colorer(alias_analysis.get(), ordering));\n+\n+  const int kExpectedBuffersCount = 5;\n+  const int kExpectedDefaultBuffersCount = 3;\n+  EXPECT_EQ(alias_analysis->buffers().size(), kExpectedBuffersCount);\n+\n+  int expected_number_default_buffers =\n+      (UseNcclUserBuffers() || UseNcclSymmetricBuffers())\n+          ? kExpectedDefaultBuffersCount\n+          : kExpectedBuffersCount;\n+\n+  // Temporary buffers.\n+  for (int i = 0; i < expected_number_default_buffers; ++i) {\n+    EXPECT_EQ(alias_analysis->buffers()[i].values().size(), 1);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->has_color(), true);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->color(),\n+              (int)MemorySpaceColor::kDefault);\n+  }\n+\n+  for (int i = expected_number_default_buffers; i < kExpectedBuffersCount;\n+       ++i) {\n+    EXPECT_EQ(alias_analysis->buffers()[i].values().size(), 1);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->has_color(), true);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->color(),\n+              (int)MemorySpaceColor::kCollective);\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    GpuCollectiveMemorySpaceAssignmentTestSuiteInstantiation,\n+    GpuCollectiveMemorySpaceAssignmentTest,\n+    ::testing::ValuesIn<CollectiveMemorySpaceAssignmentTestParams>(\n+        {{false, false}, {true, false}, {false, true}, {true, true}}),\n+    [](const ::testing::TestParamInfo<\n+        GpuCollectiveMemorySpaceAssignmentTest::ParamType>& info) {\n+      return absl::StrCat(info.param.use_nccl_user_buffers\n+                              ? \"with_nccl_user_buffers\"\n+                              : \"without_nccl_user_buffers\",\n+                          \"_\",\n+                          info.param.use_nccl_symmetric_buffers\n+                              ? \"with_nccl_symmetric_buffers\"\n+                              : \"without_nccl_symmetric_buffers\");\n+    });\n+\n+struct MosaicMemorySpaceAssignmentTestParams {\n+  bool use_nvshmem;\n+  bool mosaic_contains_nvshmem;\n+};\n+\n+class GpuMosaicMemorySpaceAssignmentTest\n+    : public GpuMemorySpaceAssignmentTest,\n+      public ::testing::WithParamInterface<\n+          MosaicMemorySpaceAssignmentTestParams> {\n+ public:\n+  bool UseNvshmem() const { return GetParam().use_nvshmem; }\n+\n+  bool MosaicContainsNvshmem() const {\n+    return GetParam().mosaic_contains_nvshmem;\n+  }\n+};\n+\n+TEST_P(GpuMosaicMemorySpaceAssignmentTest, TestMosaicMemorySpaceAssignment) {\n+  const absl::string_view kMosaicModule = R\"(\n+    HloModule m\n+\n+    ENTRY main {\n+      ROOT %custom-call.9 = (f16[8], f16[8]) custom-call(), custom_call_target=\"mosaic_gpu_v2\"\n+    }\n+  )\";\n+\n+  const absl::string_view kMosaicNvshmemModule = R\"(\n+    HloModule m\n+\n+    ENTRY main {\n+      ROOT %custom-call.9 = (f16[8], f16[8]) custom-call(), custom_call_target=\"mosaic_gpu_v2\", backend_config={module=\"nvshmem\"}\n+    }\n+  )\";\n+\n+  const absl::string_view kHloModule =\n+      MosaicContainsNvshmem() ? kMosaicNvshmemModule : kMosaicModule;\n+\n+  HloModuleConfig config;\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_experimental_enable_nvshmem(UseNvshmem());\n+  auto colorer = CreateColorer(debug_options);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule, config));\n+  AliasInfo alias_info;\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloAliasAnalysis> alias_analysis,\n+                          HloAliasAnalysis::Run(module.get(), &alias_info));\n+  DependencyHloOrdering ordering(module.get());\n+  TF_EXPECT_OK(colorer(alias_analysis.get(), ordering));\n+\n+  EXPECT_EQ(alias_analysis->buffers().size(), 3);\n+\n+  const int kExpectedBuffersCount = 3;\n+  for (int i = 0; i < kExpectedBuffersCount; ++i) {\n+    EXPECT_EQ(alias_analysis->buffers()[i].values().size(), 1);\n+    if (MosaicContainsNvshmem()) {\n+      EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->has_color(), true);\n+      EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->color(),\n+                (int)(MosaicContainsNvshmem()\n+                          ? (UseNvshmem() ? MemorySpaceColor::kCollective\n+                                          : MemorySpaceColor::kDefault)\n+                          : MemorySpaceColor::kDefault));\n+    }\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    GpuMosaicMemorySpaceAssignmentTestSuiteInstantiation,\n+    GpuMosaicMemorySpaceAssignmentTest,\n+    ::testing::ValuesIn<MosaicMemorySpaceAssignmentTestParams>(\n+        {{false, false}, {true, false}, {false, true}, {true, true}}),\n+    [](const ::testing::TestParamInfo<\n+        GpuMosaicMemorySpaceAssignmentTest::ParamType>& info) {\n+      return absl::StrCat(\n+          info.param.use_nvshmem ? \"with_nvshmem\" : \"without_nvshmem\", \"_\",\n+          info.param.mosaic_contains_nvshmem ? \"contains_nvshmem\"\n+                                             : \"does_not_contain_nvshmem\");\n+    });\n+\n+TEST_F(GpuMemorySpaceAssignmentTest, TestNvshmemMemorySpaceAssignment) {\n+  absl::string_view kHloModule = R\"(\n+    HloModule m\n+\n+    apply_op {\n+      x = f32[] parameter(0)\n+      y = f32[] parameter(1)\n+      ROOT apply_op = f32[] add(x, y)\n+    }\n+\n+    ENTRY main {\n+      parameter0 = f32[] parameter(0)\n+      all-reduce = f32[] all-reduce-start(parameter0), to_apply=apply_op, backend_config={\"collective_backend_config\":{\"backend\":\"NVSHMEM\"}}\n+      ROOT all-reduce-done = f32[] all-reduce-done(all-reduce)\n+    }\n+  )\";\n+\n+  HloModuleConfig config;\n+  auto debug_options = DebugOptions();\n+  debug_options.set_xla_gpu_experimental_enable_nvshmem(true);\n+  auto colorer = CreateColorer(debug_options);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule, config));\n+  AliasInfo alias_info;\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloAliasAnalysis> alias_analysis,\n+                          HloAliasAnalysis::Run(module.get(), &alias_info));\n+  DependencyHloOrdering ordering(module.get());\n+  TF_EXPECT_OK(colorer(alias_analysis.get(), ordering));\n+\n+  const int kExpectedBuffersCount = 5;\n+  EXPECT_EQ(alias_analysis->buffers().size(), kExpectedBuffersCount);\n+\n+  const int kExpectedDefaultBuffersCount = 3;\n+\n+  for (int i = 0; i < kExpectedDefaultBuffersCount; ++i) {\n+    EXPECT_EQ(alias_analysis->buffers()[i].values().size(), 1);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->has_color(), true);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->color(),\n+              (int)MemorySpaceColor::kDefault);\n+  }\n+\n+  for (int i = kExpectedDefaultBuffersCount; i < kExpectedBuffersCount; ++i) {\n+    EXPECT_EQ(alias_analysis->buffers()[i].values().size(), 1);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->has_color(), true);\n+    EXPECT_EQ(alias_analysis->buffers()[i].values()[0]->color(),\n+              (int)MemorySpaceColor::kCollective);\n+  }\n+}\n+}  // namespace\n+}  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 620,
        "additions": 488,
        "deletions": 132
    }
}