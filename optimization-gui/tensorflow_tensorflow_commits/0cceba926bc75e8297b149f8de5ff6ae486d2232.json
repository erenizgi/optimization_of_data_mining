{
    "author": "unknown",
    "message": "Extend ragged dot HloEvaluator to handle batch mode.\n\nPiperOrigin-RevId: 814348534",
    "sha": "0cceba926bc75e8297b149f8de5ff6ae486d2232",
    "files": [
        {
            "sha": "46624292fd782450b19602818d73ccbc08e80f98",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator_test.cc",
            "status": "modified",
            "additions": 279,
            "deletions": 0,
            "changes": 279,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cceba926bc75e8297b149f8de5ff6ae486d2232/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cceba926bc75e8297b149f8de5ff6ae486d2232/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc?ref=0cceba926bc75e8297b149f8de5ff6ae486d2232",
            "patch": "@@ -1674,6 +1674,285 @@ TEST_F(HloEvaluatorTest, RaggedDotContractingModeWithBatchDimensions) {\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n }\n \n+TEST_F(HloEvaluatorTest, RaggedDotBatchMode) {\n+  HloComputation::Builder b(TestName());\n+\n+  // lhs[b,m,k]:\n+  // f32[3,4,1] {\n+  //  { {1}, {2}, {3}, {4} },     // batch 0\n+  //  { {5}, {6}, {7}, {8} },     // batch 1\n+  //  { {9}, {10}, {11}, {12} },  // batch 2\n+  // }\n+  auto lhs_array = std::make_unique<Array3D<float>>(3, 4, 1);\n+  lhs_array->FillIota(1.f);\n+  auto lhs_literal = LiteralUtil::CreateR3FromArray3D<float>(*lhs_array);\n+  HloInstruction* lhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(lhs_literal)));\n+\n+  // rhs[b,k,n]:\n+  // f32[3,1,2]\n+  // {{{ 0, 1 }},  // batch 0\n+  //  {{ 2, 3 }},  // batch 1\n+  //  {{ 4, 5 }}}  // batch 2\n+  auto rhs_array = std::make_unique<Array3D<float>>(3, 1, 2);\n+  rhs_array->FillIota(0.f);\n+  auto rhs_literal = LiteralUtil::CreateR3FromArray3D<float>(*rhs_array);\n+\n+  HloInstruction* rhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(rhs_literal)));\n+\n+  // group_sizes s64[g]:\n+  // { 2, 1 }\n+  auto gs_literal = LiteralUtil::CreateR1<int64_t>({2, 1});\n+  HloInstruction* gs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(gs_literal)));\n+\n+  // f32[b,m,n]\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {3, 4, 2});\n+  DotDimensionNumbers dot_dnums;\n+  dot_dnums.add_lhs_contracting_dimensions(2);\n+  dot_dnums.add_rhs_contracting_dimensions(1);\n+  dot_dnums.add_lhs_batch_dimensions(0);\n+  dot_dnums.add_rhs_batch_dimensions(0);\n+  RaggedDotDimensionNumbers ragged_dot_dnums;\n+  ragged_dot_dnums.add_lhs_ragged_dimensions(0);\n+  *ragged_dot_dnums.mutable_dot_dimension_numbers() = dot_dnums;\n+  b.AddInstruction(HloInstruction::CreateRaggedDot(\n+      output_shape, lhs_instruction, rhs_instruction, gs_instruction,\n+      ragged_dot_dnums, DefaultPrecisionConfig(2)));\n+  m_->AddEntryComputation(b.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result, Evaluate());\n+\n+  // clang-format off\n+  auto expected_array = Array3D<float>({\n+      // m = 0        m = 1         m = 2         m = 3\n+      {{0.f, 1.f},   {0.f,  2.f},  {0.f,  3.f},  {0.f,  4.f}},   // batch 0\n+      {{10.f, 15.f}, {12.f, 18.f}, {14.f, 21.f}, {16.f, 24.f}},  // batch 1\n+      {{36.f, 45.f}, {40.f, 50.f}, {44.f, 55.f}, {48.f, 60.f}},  // batch 2\n+  });\n+  // clang-format on\n+  auto expected = LiteralUtil::CreateR3FromArray3D<float>(expected_array);\n+\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+}\n+\n+TEST_F(HloEvaluatorTest, RaggedDotBatchModeGroupSizeSumLessThanB) {\n+  HloComputation::Builder b(TestName());\n+\n+  // lhs[b,m,k]:\n+  // f32[3,4,1] {\n+  //  { {1}, {2}, {3}, {4} },     // batch 0\n+  //  { {5}, {6}, {7}, {8} },     // batch 1\n+  //  { {9}, {10}, {11}, {12} },  // batch 2\n+  // }\n+  auto lhs_array = std::make_unique<Array3D<float>>(3, 4, 1);\n+  lhs_array->FillIota(1.f);\n+  auto lhs_literal = LiteralUtil::CreateR3FromArray3D<float>(*lhs_array);\n+  HloInstruction* lhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(lhs_literal)));\n+\n+  // rhs[b,k,n]:\n+  // f32[3,1,2]\n+  // {{{ 0, 1 }},  // batch 0\n+  //  {{ 2, 3 }},  // batch 1\n+  //  {{ 4, 5 }}}  // batch 2\n+  auto rhs_array = std::make_unique<Array3D<float>>(3, 1, 2);\n+  rhs_array->FillIota(0.f);\n+  auto rhs_literal = LiteralUtil::CreateR3FromArray3D<float>(*rhs_array);\n+\n+  HloInstruction* rhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(rhs_literal)));\n+\n+  // group_sizes s64[g]:\n+  // { 1, 1 }\n+  auto gs_literal = LiteralUtil::CreateR1<int64_t>({1, 1});\n+  HloInstruction* gs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(gs_literal)));\n+\n+  // f32[b,m,n]\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {3, 4, 2});\n+  DotDimensionNumbers dot_dnums;\n+  dot_dnums.add_lhs_contracting_dimensions(2);\n+  dot_dnums.add_rhs_contracting_dimensions(1);\n+  dot_dnums.add_lhs_batch_dimensions(0);\n+  dot_dnums.add_rhs_batch_dimensions(0);\n+  RaggedDotDimensionNumbers ragged_dot_dnums;\n+  ragged_dot_dnums.add_lhs_ragged_dimensions(0);\n+  *ragged_dot_dnums.mutable_dot_dimension_numbers() = dot_dnums;\n+  b.AddInstruction(HloInstruction::CreateRaggedDot(\n+      output_shape, lhs_instruction, rhs_instruction, gs_instruction,\n+      ragged_dot_dnums, DefaultPrecisionConfig(2)));\n+  m_->AddEntryComputation(b.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result, Evaluate());\n+\n+  // clang-format off\n+  auto expected_array = Array3D<float>({\n+      // m = 0        m = 1         m = 2         m = 3\n+      {{0.f, 1.f},   {0.f,  2.f},  {0.f,  3.f},  {0.f,  4.f}},   // batch 0\n+      {{10.f, 15.f}, {12.f, 18.f}, {14.f, 21.f}, {16.f, 24.f}},  // batch 1\n+      {{0.f, 0.f},   {0.f, 0.f},   {0.f, 0.f},   {0.f, 0.f}},    // batch 2\n+  });\n+  // clang-format on\n+  auto expected = LiteralUtil::CreateR3FromArray3D<float>(expected_array);\n+\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+}\n+\n+TEST_F(HloEvaluatorTest, RaggedDotBatchModeWithRaggedOuterBatchDim) {\n+  HloComputation::Builder b(TestName());\n+\n+  // lhs[b1,b2,m,k]:\n+  // f32[2,2,2,2]\n+  // {{{{  1,  2,},\n+  //    {  3,  4,}},  // batch 0,0\n+  //   {{  5,  6,},\n+  //    {  7,  8,}}}, // batch 0,1\n+  //  {{{  9, 10,},\n+  //    { 11, 12,}},  // batch 1,0\n+  //   {{ 13, 14,},\n+  //    { 15, 16,}}}} // batch 1,1\n+  auto lhs_array = std::make_unique<Array4D<float>>(2, 2, 2, 2);\n+  lhs_array->FillIota(1.f);\n+  auto lhs_literal = LiteralUtil::CreateR4FromArray4D<float>(*lhs_array);\n+  HloInstruction* lhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(lhs_literal)));\n+\n+  // rhs[b1,b2,k,n]:\n+  // f32[2,2,2,2]\n+  // {{{{  0,  1,},\n+  //    {  2,  3,}},  // batch 0,0\n+  //   {{  4,  5,},\n+  //    {  6,  7,}}}, // batch 0,1\n+  //  {{{  8,  9,},\n+  //    { 10, 11,}},  // batch 1,0\n+  //   {{ 12, 13,},\n+  //    { 14, 15,}}}} // batch 1,1\n+  auto rhs_array = std::make_unique<Array4D<float>>(2, 2, 2, 2);\n+  rhs_array->FillIota(0.f);\n+  auto rhs_literal = LiteralUtil::CreateR4FromArray4D<float>(*rhs_array);\n+\n+  HloInstruction* rhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(rhs_literal)));\n+\n+  // group_sizes[g]\n+  // s64[3]:\n+  // { 0, 1, 0 }  // batches 0,* are calculated and 1,* are not.\n+  auto gs_literal = LiteralUtil::CreateR1<int64_t>({0, 1, 0});\n+  HloInstruction* gs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(gs_literal)));\n+\n+  // f32[b1,b2,m,n]\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {2, 2, 2, 2});\n+  DotDimensionNumbers dot_dnums;\n+  dot_dnums.add_lhs_contracting_dimensions(3);\n+  dot_dnums.add_rhs_contracting_dimensions(2);\n+  dot_dnums.add_lhs_batch_dimensions(0);\n+  dot_dnums.add_lhs_batch_dimensions(1);\n+  dot_dnums.add_rhs_batch_dimensions(0);\n+  dot_dnums.add_rhs_batch_dimensions(1);\n+  RaggedDotDimensionNumbers ragged_dot_dnums;\n+  ragged_dot_dnums.add_lhs_ragged_dimensions(0);\n+  *ragged_dot_dnums.mutable_dot_dimension_numbers() = dot_dnums;\n+  b.AddInstruction(HloInstruction::CreateRaggedDot(\n+      output_shape, lhs_instruction, rhs_instruction, gs_instruction,\n+      ragged_dot_dnums, DefaultPrecisionConfig(2)));\n+  m_->AddEntryComputation(b.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result, Evaluate());\n+\n+  // clang-format off\n+  auto expected_array = Array4D<float>({\n+      // m = 0         m = 1\n+      {{{ 4.f,  7.f}, { 8.f, 15.f}},   // batch 0,0\n+       {{56.f, 67.f}, {76.f, 91.f}}},  // batch 0,1\n+      {{{ 0.f,  0.f}, { 0.f,  0.f}},   // batch 1,0\n+       {{ 0.f,  0.f}, { 0.f,  0.f}}}   // batch 1,1\n+  });\n+  // clang-format on\n+  auto expected = LiteralUtil::CreateR4FromArray4D<float>(expected_array);\n+\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+}\n+\n+TEST_F(HloEvaluatorTest, RaggedDotBatchModeWithRaggedInnerBatchDim) {\n+  HloComputation::Builder b(TestName());\n+\n+  // lhs[b1,b2,m,k]:\n+  // f32[2,2,2,2]\n+  // {{{{  1,  2,},\n+  //    {  3,  4,}},  // batch 0,0\n+  //   {{  5,  6,},\n+  //    {  7,  8,}}}, // batch 0,1\n+  //  {{{  9, 10,},\n+  //    { 11, 12,}},  // batch 1,0\n+  //   {{ 13, 14,},\n+  //    { 15, 16,}}}} // batch 1,1\n+  auto lhs_array = std::make_unique<Array4D<float>>(2, 2, 2, 2);\n+  lhs_array->FillIota(1.f);\n+  auto lhs_literal = LiteralUtil::CreateR4FromArray4D<float>(*lhs_array);\n+  HloInstruction* lhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(lhs_literal)));\n+\n+  // rhs[b1,b2,k,n]:\n+  // f32[2,2,2,2]\n+  // {{{{  0,  1,},\n+  //    {  2,  3,}},  // batch 0,0\n+  //   {{  4,  5,},\n+  //    {  6,  7,}}}, // batch 0,1\n+  //  {{{  8,  9,},\n+  //    { 10, 11,}},  // batch 1,0\n+  //   {{ 12, 13,},\n+  //    { 14, 15,}}}} // batch 1,1\n+  auto rhs_array = std::make_unique<Array4D<float>>(2, 2, 2, 2);\n+  rhs_array->FillIota(0.f);\n+  auto rhs_literal = LiteralUtil::CreateR4FromArray4D<float>(*rhs_array);\n+\n+  HloInstruction* rhs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(rhs_literal)));\n+\n+  // group_sizes[b1, g]\n+  // s64[2,3]:\n+  // {{ 0, 1, 0 },  // batch 0,0 is calculated and 0,1 is not.\n+  //  { 0, 1, 1 }}  // batches 1,* are calculated.\n+  auto gs_literal = LiteralUtil::CreateR2<int64_t>({{0, 1, 0}, {0, 1, 1}});\n+  HloInstruction* gs_instruction =\n+      b.AddInstruction(HloInstruction::CreateConstant(std::move(gs_literal)));\n+\n+  // f32[b1,b2,m,n]\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {2, 2, 2, 2});\n+  DotDimensionNumbers dot_dnums;\n+  dot_dnums.add_lhs_contracting_dimensions(3);\n+  dot_dnums.add_rhs_contracting_dimensions(2);\n+  dot_dnums.add_lhs_batch_dimensions(0);\n+  dot_dnums.add_lhs_batch_dimensions(1);\n+  dot_dnums.add_rhs_batch_dimensions(0);\n+  dot_dnums.add_rhs_batch_dimensions(1);\n+  RaggedDotDimensionNumbers ragged_dot_dnums;\n+  ragged_dot_dnums.add_lhs_ragged_dimensions(1);\n+  *ragged_dot_dnums.mutable_dot_dimension_numbers() = dot_dnums;\n+  b.AddInstruction(HloInstruction::CreateRaggedDot(\n+      output_shape, lhs_instruction, rhs_instruction, gs_instruction,\n+      ragged_dot_dnums, DefaultPrecisionConfig(2)));\n+  m_->AddEntryComputation(b.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result, Evaluate());\n+\n+  // clang-format off\n+  auto expected_array = Array4D<float>({\n+      // m = 0         m = 1\n+      {{{  4.f,   7.f}, {  8.f,  15.f}},   // batch 0,0\n+       {{  0.f,   0.f}, {  0.f,   0.f}}},  // batch 0,1\n+      {{{172.f, 191.f}, {208.f, 231.f}},   // batch 1,0\n+       {{352.f, 379.f}, {404.f, 435.f}}}   // batch 1,1\n+  });\n+  // clang-format on\n+  auto expected = LiteralUtil::CreateR4FromArray4D<float>(expected_array);\n+\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+}\n+\n HloInstruction* BF16Array2D(HloComputation::Builder& b, int rows, int cols,\n                             float value) {\n   auto array = std::make_unique<Array2D<float>>(rows, cols);"
        },
        {
            "sha": "5a74b45b1746ec4559fcf5b07c2a7e9cab4b0a5c",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator_typed_visitor.h",
            "status": "modified",
            "additions": 118,
            "deletions": 34,
            "changes": 152,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cceba926bc75e8297b149f8de5ff6ae486d2232/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cceba926bc75e8297b149f8de5ff6ae486d2232/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h?ref=0cceba926bc75e8297b149f8de5ff6ae486d2232",
            "patch": "@@ -1212,6 +1212,25 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n     return HandleDotSlowPath(dot);\n   }\n \n+  void IncrementContractingIndexes(\n+      DimensionVector& contracting_indexes,\n+      absl::Span<const int64_t> contracting_dims,\n+      const DimensionVector& contracting_dim_sizes,\n+      std::optional<int64_t> contracting_dim_to_skip = std::nullopt) {\n+    for (int i = contracting_dim_sizes.size() - 1; i >= 0; --i) {\n+      if (contracting_dim_to_skip.has_value() &&\n+          contracting_dim_to_skip.value() == i) {\n+        continue;\n+      }\n+      ++contracting_indexes[contracting_dims[i]];\n+      if (contracting_indexes[contracting_dims[i]] !=\n+          contracting_dim_sizes[i]) {\n+        break;\n+      }\n+      contracting_indexes[contracting_dims[i]] = 0;\n+    }\n+  }\n+\n   absl::Status HandleDotSlowPathWithLiterals(const HloInstruction* dot,\n                                              const Literal& lhs_literal,\n                                              const Literal& rhs_literal) {\n@@ -1294,20 +1313,10 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n                                           rhs_linear_index);\n             }\n \n-            // If there are no contracting dimensions, do not try to count down\n-            // from -1 to 0; that's an infinite loop.\n-            if (!contracting_dim_sizes.empty()) {\n-              for (int64_t i = contracting_dim_sizes.size() - 1; i >= 0; --i) {\n-                lhs_index[lhs_contracting_dims[i]]++;\n-                rhs_index[rhs_contracting_dims[i]]++;\n-                if (lhs_index[lhs_contracting_dims[i]] !=\n-                    contracting_dim_sizes[i]) {\n-                  break;\n-                }\n-                lhs_index[lhs_contracting_dims[i]] = 0;\n-                rhs_index[rhs_contracting_dims[i]] = 0;\n-              }\n-            }\n+            IncrementContractingIndexes(lhs_index, lhs_contracting_dims,\n+                                        contracting_dim_sizes);\n+            IncrementContractingIndexes(rhs_index, rhs_contracting_dims,\n+                                        contracting_dim_sizes);\n           }\n \n           return static_cast<ReturnT>(result_val);\n@@ -1440,15 +1449,10 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n                 static_cast<ElementwiseT>(rhs_literal.Get<ReturnT>(rhs_index));\n             result_val += ToArithmeticSafeType(lhs) * ToArithmeticSafeType(rhs);\n \n-            for (int64_t j = contracting_dim_sizes.size() - 1; j >= 0; --j) {\n-              lhs_index[lhs_contracting[j]]++;\n-              rhs_index[rhs_contracting[j]]++;\n-              if (lhs_index[lhs_contracting[j]] != contracting_dim_sizes[j]) {\n-                break;\n-              }\n-              lhs_index[lhs_contracting[j]] = 0;\n-              rhs_index[rhs_contracting[j]] = 0;\n-            }\n+            IncrementContractingIndexes(lhs_index, lhs_contracting,\n+                                        contracting_dim_sizes);\n+            IncrementContractingIndexes(rhs_index, rhs_contracting,\n+                                        contracting_dim_sizes);\n           }\n           return static_cast<ReturnT>(result_val);\n         }));\n@@ -1461,7 +1465,91 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n                                                     const Literal& lhs_literal,\n                                                     const Literal& rhs_literal,\n                                                     const Literal& gs_literal) {\n-    return absl::UnimplementedError(\"Ragged Dot Batch Mode not implemented\");\n+    auto ragged_dims = dot->ragged_dot_dimension_numbers();\n+    auto dot_dims = ragged_dims.dot_dimension_numbers();\n+    int64_t lhs_rank = lhs_literal.shape().dimensions().size();\n+    int64_t rhs_rank = rhs_literal.shape().dimensions().size();\n+    int64_t gs_rank = gs_literal.shape().dimensions().size();\n+\n+    auto lhs_contracting = dot_dims.lhs_contracting_dimensions();\n+    auto lhs_non_contracting = GetNonContractingDims(\n+        lhs_rank, lhs_contracting, dot_dims.lhs_batch_dimensions());\n+\n+    auto rhs_contracting = dot_dims.rhs_contracting_dimensions();\n+    auto rhs_non_contracting = GetNonContractingDims(\n+        rhs_rank, rhs_contracting, dot_dims.rhs_batch_dimensions());\n+\n+    DimensionVector contracting_dim_sizes;\n+    contracting_dim_sizes.reserve(lhs_contracting.size());\n+    for (int64_t i = 0; i < lhs_contracting.size(); ++i) {\n+      int64_t dim_size = lhs_literal.shape().dimensions(lhs_contracting[i]);\n+      contracting_dim_sizes.push_back(dim_size);\n+    }\n+    const int64_t total_contracting_size = Product(contracting_dim_sizes);\n+    const int64_t group_dim_index = gs_rank - 1;\n+    const int64_t num_groups = gs_literal.shape().dimensions(group_dim_index);\n+\n+    Shape dot_shape = GetShapeWithLayout(dot->shape());\n+    Literal result(dot_shape);\n+    TF_RETURN_IF_ERROR(result.PopulateParallel<ReturnT>(\n+        [&](absl::Span<const int64_t> result_index, int /*thread_id*/) {\n+          // Locations in each operand that we read from to calculate the\n+          // result at result_index.\n+          DimensionVector lhs_index(lhs_rank);\n+          DimensionVector rhs_index(rhs_rank);\n+          DimensionVector group_index(gs_rank);\n+\n+          // The batch dimensions will always be first in the final product.\n+          int64_t gs_idx = 0;\n+          int64_t idx = 0;\n+          for (int64_t i = 0; i < dot_dims.lhs_batch_dimensions_size(); ++i) {\n+            lhs_index[dot_dims.lhs_batch_dimensions(i)] = result_index[idx];\n+            rhs_index[dot_dims.rhs_batch_dimensions(i)] = result_index[idx];\n+            // gs only contains the batch dimensions outer to the ragged dim.\n+            if (gs_idx < group_dim_index) {\n+              group_index[gs_idx++] = result_index[idx];\n+            }\n+            ++idx;\n+          }\n+\n+          // Check whether this batch is handled by some group.\n+          int64_t batches_handled = 0;\n+          for (int i = 0; i < num_groups; ++i) {\n+            group_index[group_dim_index] = i;\n+            batches_handled += gs_literal.Get<int64_t>(group_index);\n+          }\n+          if (lhs_index[dot_dims.lhs_batch_dimensions(group_dim_index)] >=\n+              batches_handled) {\n+            return static_cast<ReturnT>(0);\n+          }\n+\n+          // Non-contracting dimensions - lhs, then rhs.\n+          for (int64_t i = 0; i < lhs_non_contracting.size(); ++i) {\n+            lhs_index[lhs_non_contracting[i]] = result_index[idx++];\n+          }\n+          for (int64_t i = 0; i < rhs_non_contracting.size(); ++i) {\n+            rhs_index[rhs_non_contracting[i]] = result_index[idx++];\n+          }\n+\n+          // Accumulate resulting product along the contracting dimensions.\n+          ElementwiseT result_val = static_cast<ElementwiseT>(0);\n+          for (int64_t i = 0; i < total_contracting_size; ++i) {\n+            const auto lhs =\n+                static_cast<ElementwiseT>(lhs_literal.Get<ReturnT>(lhs_index));\n+            const auto rhs =\n+                static_cast<ElementwiseT>(rhs_literal.Get<ReturnT>(rhs_index));\n+            result_val += ToArithmeticSafeType(lhs) * ToArithmeticSafeType(rhs);\n+\n+            IncrementContractingIndexes(lhs_index, lhs_contracting,\n+                                        contracting_dim_sizes);\n+            IncrementContractingIndexes(rhs_index, rhs_contracting,\n+                                        contracting_dim_sizes);\n+          }\n+          return static_cast<ReturnT>(result_val);\n+        }));\n+\n+    parent_->SetEvaluatedLiteralFor(dot, std::move(result));\n+    return absl::OkStatus();\n   }\n \n   absl::Status HandleRaggedDotContractingWithLiterals(\n@@ -1565,16 +1653,12 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n           result_val += ToArithmeticSafeType(lhs) * ToArithmeticSafeType(rhs);\n         }\n \n-        for (int64_t j = contracting_dim_sizes.size() - 1; j >= 0; --j) {\n-          if (j == ragged_dim_as_contracting_dim.value()) continue;\n-          ++lhs_index[lhs_contracting[j]];\n-          ++rhs_index[rhs_contracting[j]];\n-          if (lhs_index[lhs_contracting[j]] != contracting_dim_sizes[j]) {\n-            break;\n-          }\n-          lhs_index[lhs_contracting[j]] = 0;\n-          rhs_index[rhs_contracting[j]] = 0;\n-        }\n+        IncrementContractingIndexes(lhs_index, lhs_contracting,\n+                                    contracting_dim_sizes,\n+                                    ragged_dim_as_contracting_dim);\n+        IncrementContractingIndexes(rhs_index, rhs_contracting,\n+                                    contracting_dim_sizes,\n+                                    ragged_dim_as_contracting_dim);\n       }\n       return static_cast<ReturnT>(result_val);\n     }));"
        }
    ],
    "stats": {
        "total": 431,
        "additions": 397,
        "deletions": 34
    }
}