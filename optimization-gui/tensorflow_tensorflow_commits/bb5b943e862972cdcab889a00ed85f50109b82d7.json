{
    "author": "and-ivanov",
    "message": "PR #34107: [XLA:GPU] Fix cublas fallback test on Thor GPU (sm_110)\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34107\n\nThis fixes the failing test StatelessAutotunerTest.CublasFallbackForBf16Bf16F32Algorithm on Jetson Thor GPUs with compute capability 11.0.\nCopybara import of the project:\n\n--\n2385bc08e0a8778d3acfa48caa4a885dc259b206 by Andrei Ivanov <anivanov@nvidia.com>:\n\n[XLA:GPU] Fix cublas fallback test on Thor GPU (sm_110)\n\nMerging this change closes #34107\n\nPiperOrigin-RevId: 834674818",
    "sha": "bb5b943e862972cdcab889a00ed85f50109b82d7",
    "files": [
        {
            "sha": "b746510b36ce7fb390b45990db1bc83aea5b9505",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bb5b943e862972cdcab889a00ed85f50109b82d7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bb5b943e862972cdcab889a00ed85f50109b82d7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=bb5b943e862972cdcab889a00ed85f50109b82d7",
            "patch": "@@ -330,6 +330,7 @@ TEST_F(StatelessAutotunerTest, CublasFallbackForBf16Bf16F32Algorithm) {\n                \"Hopper\";\n         break;\n       case se::CudaComputeCapability::kBlackwell:\n+      case se::CudaComputeCapability::kBlackwell_11:\n       case se::CudaComputeCapability::kBlackwell_12:\n         EXPECT_TRUE(hasCublasConfig(configs))\n             << \"There should be a cublas fallback for dot_bf16_bf16_f32 on \""
        },
        {
            "sha": "24ad288ede79cfa999a0743d9b87fe9cc3b0ea0c",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_compute_capability.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bb5b943e862972cdcab889a00ed85f50109b82d7/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bb5b943e862972cdcab889a00ed85f50109b82d7/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h?ref=bb5b943e862972cdcab889a00ed85f50109b82d7",
            "patch": "@@ -70,6 +70,7 @@ struct CudaComputeCapability {\n     kAmpere = 8,\n     kHopper = 9,\n     kBlackwell = 10,\n+    kBlackwell_11 = 11,\n     kBlackwell_12 = 12\n   };\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}