{
    "author": "loislo",
    "message": "[XLA:GPU] Add autotuning support for `HloScaledDotInstruction` in `GemmFusionAutotuner`.\n\nThe autotuner now recognizes and generates Triton configurations for fusions containing `HloScaledDotInstruction`. This involves adding new methods to handle scaled dot operations and updating the backend config kind checks.\n\nPiperOrigin-RevId: 809942152",
    "sha": "42900d1192d932cddc090edfcc1f7c7ff5fd8b8b",
    "files": [
        {
            "sha": "62dddac0eaf4f9e2c4ce73b2c6218a997c186a63",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 4,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/42900d1192d932cddc090edfcc1f7c7ff5fd8b8b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/42900d1192d932cddc090edfcc1f7c7ff5fd8b8b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=42900d1192d932cddc090edfcc1f7c7ff5fd8b8b",
            "patch": "@@ -175,7 +175,8 @@ class GemmFusionCollector : public ConstDfsHloVisitorWithDefault {\n         gpu_config.fusion_backend_config();\n     if (backend_config.kind() != kTritonGemmFusionKind &&\n         backend_config.kind() != kCuDnnFusionKind &&\n-        backend_config.kind() != kCustomFusionKind) {\n+        backend_config.kind() != kCustomFusionKind &&\n+        backend_config.kind() != kTritonScaledDotFusionKind) {\n       return absl::OkStatus();\n     }\n \n@@ -829,9 +830,26 @@ static std::vector<BackendConfig> GenerateCustomKernelFusionConfigs(\n absl::StatusOr<std::vector<BackendConfig>>\n GemmFusionAutotunerImpl::GenerateConfigs(const HloFusionInstruction& fusion) {\n   tsl::profiler::TraceMe traceme(\"GenerateConfigs\");\n-  const HloDotInstruction* dot =\n-      Cast<HloDotInstruction>(hlo_query::GetFirstInstructionWithOpcode(\n-          *fusion.called_computation(), HloOpcode::kDot));\n+  auto* computation = fusion.called_computation();\n+  auto* dot =\n+      hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n+  if (dot) {\n+    return GenerateDotConfigs(fusion, Cast<HloDotInstruction>(dot));\n+  }\n+  auto* scaled_dot = hlo_query::GetFirstInstructionWithOpcode(\n+      *computation, HloOpcode::kScaledDot);\n+  if (scaled_dot) {\n+    return GenerateScaledDotConfigs(\n+        fusion, DynCast<HloScaledDotInstruction>(scaled_dot));\n+  }\n+  return absl::InternalError(\n+      absl::StrCat(\"No dot or scaled dot instruction found in fusion: %s\",\n+                   fusion.ToString()));\n+}\n+\n+absl::StatusOr<std::vector<BackendConfig>>\n+GemmFusionAutotunerImpl::GenerateDotConfigs(const HloFusionInstruction& fusion,\n+                                            const HloDotInstruction* dot) {\n   std::vector<BackendConfig> configs;\n \n   if (!debug_options_.xla_gpu_experimental_disable_binary_libraries()) {\n@@ -871,6 +889,38 @@ GemmFusionAutotunerImpl::GenerateConfigs(const HloFusionInstruction& fusion) {\n   return configs;\n }\n \n+absl::StatusOr<std::vector<BackendConfig>>\n+GemmFusionAutotunerImpl::GenerateScaledDotConfigs(\n+    const HloFusionInstruction& fusion, const HloScaledDotInstruction* dot) {\n+  std::vector<BackendConfig> configs;\n+  // Add triton configs.\n+  TF_ASSIGN_OR_RETURN(std::vector<TritonGemmConfig> triton_configs,\n+                      GenerateTritonConfigs(*dot));\n+  configs.reserve(triton_configs.size());\n+  for (TritonGemmConfig& config : triton_configs) {\n+    configs.push_back(std::move(config));\n+  }\n+  return configs;\n+}\n+\n+absl::StatusOr<std::vector<TritonGemmConfig>>\n+GemmFusionAutotunerImpl::GenerateTritonConfigs(\n+    const HloScaledDotInstruction& dot) {\n+  tsl::profiler::TraceMe traceme(\"GenerateTritonConfigs\");\n+  // TODO(b/421858850): Restricting configs for dots from broadcasts is a\n+  // temporary solution. We should remove this once we have a fix for the error.\n+  auto configs = GetDefaultTritonConfigs();\n+  if (HasBroadcastProducer(dot)) {\n+    RestrictTmaConfigs(configs);\n+  }\n+\n+  if (!IsAutotuningEnabled()) {\n+    // Keep the first config, which likely does not spill registers.\n+    configs.resize(1);\n+  }\n+  return configs;\n+}\n+\n absl::StatusOr<std::vector<TritonGemmConfig>>\n GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n   tsl::profiler::TraceMe traceme(\"GenerateTritonConfigs\");"
        },
        {
            "sha": "7a82944b531b6e0c54949a66a06abf3aebed099e",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/42900d1192d932cddc090edfcc1f7c7ff5fd8b8b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/42900d1192d932cddc090edfcc1f7c7ff5fd8b8b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=42900d1192d932cddc090edfcc1f7c7ff5fd8b8b",
            "patch": "@@ -136,6 +136,8 @@ class GemmFusionAutotunerImpl {\n       const HloFusionInstruction& fusion);\n   absl::StatusOr<std::vector<TritonGemmConfig>> GenerateTritonConfigs(\n       const HloDotInstruction& dot);\n+  absl::StatusOr<std::vector<TritonGemmConfig>> GenerateTritonConfigs(\n+      const HloScaledDotInstruction& dot);\n \n   // Compile all executables for all fusions.\n   absl::StatusOr<absl::flat_hash_map<const HloFusionInstruction*,\n@@ -160,6 +162,11 @@ class GemmFusionAutotunerImpl {\n   static const int64_t BLAS_GEMM_DEFAULT;\n \n  private:\n+  absl::StatusOr<std::vector<BackendConfig>> GenerateDotConfigs(\n+      const HloFusionInstruction& fusion, const HloDotInstruction* dot);\n+  absl::StatusOr<std::vector<BackendConfig>> GenerateScaledDotConfigs(\n+      const HloFusionInstruction& fusion, const HloScaledDotInstruction* dot);\n+\n   // Measures the performance of a single executable candidate.\n   //\n   // If required and the candidate is cuBLAS, this will save the output to the"
        },
        {
            "sha": "23361256420f1c6f7d8a9929d23d237d4ff75c71",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 2,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/42900d1192d932cddc090edfcc1f7c7ff5fd8b8b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/42900d1192d932cddc090edfcc1f7c7ff5fd8b8b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=42900d1192d932cddc090edfcc1f7c7ff5fd8b8b",
            "patch": "@@ -406,10 +406,10 @@ class GemmFusionAutotunerTest : public StatelessAutotunerTest {\n   }\n };\n \n+template <typename D>\n absl::StatusOr<std::vector<TritonGemmConfig>>\n GetPossibleMatmulAutotuneTritonConfigs(\n-    const HloDotInstruction& dot,\n-    const se::CudaComputeCapability& compute_capability,\n+    const D& dot, const se::CudaComputeCapability& compute_capability,\n     const se::SemanticVersion& toolkit_version,\n     const DebugOptions& debug_options) {\n   TF_ASSIGN_OR_RETURN(se::DeviceDescription device_description,\n@@ -1720,6 +1720,34 @@ TEST_F(GemmFusionAutotunerTest, VerifyHopperConfigsAreDifferentFromBlackwell) {\n   EXPECT_NE(blackwell_configs_set, hopper_configs_set);\n }\n \n+TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsAreGenerated) {\n+  if (isRocm()) {\n+    GTEST_SKIP() << \"Not supported on ROCm.\";\n+  }\n+\n+  std::unique_ptr<VerifiedHloModule> module = ParseAndReturnVerifiedModule(R\"(\n+    ENTRY e {\n+      p0 = f32[1024,1024] parameter(0)\n+      p0_scale = f32[1024,8] parameter(1)\n+      p1 = f32[1024,1024] parameter(2)\n+      p1_scale = f32[8,1024] parameter(3)\n+      ROOT r = f32[1024,1024] scaled-dot(p0, p0_scale, p1, p1_scale),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    })\")\n+                                                  .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const std::vector<TritonGemmConfig> blackwell_configs,\n+      GetPossibleMatmulAutotuneTritonConfigs(\n+          *Cast<HloScaledDotInstruction>(\n+              module->entry_computation()->root_instruction()),\n+          se::CudaComputeCapability(se::CudaComputeCapability::kBlackwell, 0),\n+          GetToolkitVersion(), GetDebugOptionsForTest()));\n+  std::set<TritonGemmConfig> blackwell_configs_set(blackwell_configs.begin(),\n+                                                   blackwell_configs.end());\n+  EXPECT_GT(blackwell_configs_set.size(), 0);\n+}\n+\n // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n // default.\n class GemmFusionAutotunerEnableTma : public GemmFusionAutotunerTest {"
        }
    ],
    "stats": {
        "total": 97,
        "additions": 91,
        "deletions": 6
    }
}