{
    "author": "KanishAnand",
    "message": "Delete unused `DevicesForSharding`\n\nPiperOrigin-RevId: 839445697",
    "sha": "f6f1d3f5cd59ce1ee1dcfd3f7700ae12f6d9f8ff",
    "files": [
        {
            "sha": "14c3c164cb30718d68f6074ac2a6db69f0284135",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 51,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6f1d3f5cd59ce1ee1dcfd3f7700ae12f6d9f8ff/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6f1d3f5cd59ce1ee1dcfd3f7700ae12f6d9f8ff/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=f6f1d3f5cd59ce1ee1dcfd3f7700ae12f6d9f8ff",
            "patch": "@@ -1602,57 +1602,6 @@ IdentityValueAndHloOpcodeForScatterReduceComputation(\n                       \"add/or/multiply/add/min/max\");\n }\n \n-namespace {\n-\n-void DevicesForShardingInternal(\n-    const HloSharding& sharding,\n-    const absl::flat_hash_set<int64_t>& available_devices,\n-    absl::flat_hash_set<int64_t>* used) {\n-  if (sharding.IsTuple()) {\n-    for (const auto& subsharding : sharding.tuple_elements()) {\n-      DevicesForShardingInternal(subsharding, available_devices, used);\n-    }\n-    return;\n-  }\n-\n-  if (sharding.IsReplicated()) {\n-    for (int64_t device : available_devices) {\n-      if (!HloSharding::IsReservedDevice(device)) {\n-        used->insert(device);\n-      }\n-    }\n-    return;\n-  }\n-\n-  DCHECK(std::all_of(\n-      sharding.tile_assignment().array().begin(),\n-      sharding.tile_assignment().array().end(),\n-      [&](int64_t device) { return available_devices.contains(device); }));\n-  sharding.tile_assignment().Each(\n-      [&](absl::Span<const int64_t> /*indices*/, int64_t device) {\n-        used->insert(device);\n-      });\n-}\n-\n-}  // namespace\n-\n-std::vector<int64_t> DevicesForSharding(\n-    const HloSharding& sharding, absl::Span<const int64_t> available_devices) {\n-  absl::flat_hash_set<int64_t> available_set;\n-  for (int64_t device : available_devices) {\n-    available_set.insert(device);\n-  }\n-  absl::flat_hash_set<int64_t> used_set;\n-  DevicesForShardingInternal(sharding, available_set, &used_set);\n-  std::vector<int64_t> devices;\n-  for (int64_t device : available_devices) {\n-    if (used_set.contains(device)) {\n-      devices.push_back(device);\n-    }\n-  }\n-  return devices;\n-}\n-\n HloSharding PartiallyReplicateTiledShardingOnDims(\n     const HloSharding& sharding, absl::Span<const int64_t> dims_to_replicate) {\n   if (sharding.IsTileMaximal() || sharding.IsManual()) {"
        },
        {
            "sha": "4fc2a1924a6a47b31212c2294ac9ff9312f18c8e",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.h",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f6f1d3f5cd59ce1ee1dcfd3f7700ae12f6d9f8ff/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f6f1d3f5cd59ce1ee1dcfd3f7700ae12f6d9f8ff/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h?ref=f6f1d3f5cd59ce1ee1dcfd3f7700ae12f6d9f8ff",
            "patch": "@@ -272,11 +272,6 @@ absl::StatusOr<std::pair<std::unique_ptr<HloInstruction>, HloOpcode>>\n IdentityValueAndHloOpcodeForScatterReduceComputation(\n     const HloScatterInstruction& scatter);\n \n-// Given a sharding and a list of devices in the topology, return a\n-// list of the devices that `sharding` applies to.\n-std::vector<int64_t> DevicesForSharding(\n-    const HloSharding& sharding, absl::Span<const int64_t> available_devices);\n-\n // Returns a sharding that replicates data across devices along the given\n // dimensions in the original sharding.\n HloSharding PartiallyReplicateTiledShardingOnDims("
        }
    ],
    "stats": {
        "total": 56,
        "additions": 0,
        "deletions": 56
    }
}