{
    "author": "tensorflower-gardener",
    "message": "Make all files depending on IndexingMap to use SymbolicExprContext\n\nThis CL is a key step in integrating the new SymbolicExpr library into IndexingMap (b/433696544). The primary goal is to replace the existing `mlir::MLIRContext` with `gpu::SymbolicExprContext` throughout every class that depends on IndexingMap.\n\nGoal:\n  - Enables Symbolic Reasoning: `SymbolicExprContext` is designed to manage symbolic expressions, which will allow for more powerful analysis and optimization of indexing maps.\n  - Performance: We believe with pifon@ than by using a dedicated context, compilation time could be improved by the fact of not overusing the lock inside MLIRContext (used everywhere). This should be confirmed with real data after finishing the entire migration.\n  - Foundation: This refactoring is a prerequisite for fully replacing `AffineExpr` with `SymbolicExpr` in `IndexingMap`. This should unblock the replacement of the internal implementation (cl/802100018).\n\nChanges:\n   - Signature Updates: Function signatures across numerous files in `xla/service/gpu`, `xla/backends/gpu`, `xla/backends/cpu`, and `xla/hlo/analysis` have been updated to accept `gpu::SymbolicExprContext*` instead of `mlir::MLIRContext*`.\n   - Context Propagation: The `SymbolicExprContext` is now created and owned by `GpuCompiler` and propagated down to the various components, including fusion emitters, autotuners, and performance models.\n   - Test Updates: Unit tests and test utilities have been updated to use the new context.\n   - Some areas required temporary workarounds where the `SymbolicExprContext` is created on the fly from an existing `mlir::MLIRContext`. These are marked with TODOs (b/446856820, b/446856303) to be cleaned up in follow-up CLs as the integration progresses.\n\nIdeally no functional changes are intended, but the `mlir::MLIRContext` was inconsistently managed across the codebase, requiring careful tracing and updates to ensure the new `SymbolicExprContext` is correctly propagated everywhere. This made the refactoring process time-consuming and I would appreciate careful review because I could have made some mistakes as well. In this process I had 3 different and unrelated segmentation faults and a crash in StorageUniquer for not using the same context in different places.\n\nThis CL represents step 2 of the integration plan outlined in b/433696544#comment9.\n\nPiperOrigin-RevId: 819228363",
    "sha": "f23728131ea8fe7e602f9007b8cba0eaced22a2e",
    "files": [
        {
            "sha": "de524b12eb2436347f7b375599259d8c71f8b2ee",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -155,6 +155,7 @@ cc_library(\n         \"//xla/codegen/emitters/transforms:passes\",\n         \"//xla/mlir/tools/mlir_replay/public:compiler_trace_proto_cc\",\n         \"//xla/mlir_hlo\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/framework/mlir:status_scoped_diagnostic_handler\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -641,6 +642,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/cpu:backend_config_proto_cc\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:inlined_vector\","
        },
        {
            "sha": "3065ca8cf918bc2349f66e974bd651d38deac985",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -58,6 +58,7 @@ cc_library(\n         \"//xla/service:dump\",\n         \"//xla/service:scatter_simplifier\",\n         \"//xla/service/cpu:backend_config_proto_cc\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -112,6 +113,7 @@ xla_cc_test(\n         \"//xla/service:buffer_value\",\n         \"//xla/service:logical_buffer\",\n         \"//xla/service/cpu:cpu_executable\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status:statusor\","
        },
        {
            "sha": "bc7ed5e835d2699a2f70d1dee5348ff275b61cb0",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 10,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -45,7 +45,6 @@ limitations under the License.\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/Interfaces/DataLayoutInterfaces.h\"\n@@ -73,6 +72,7 @@ limitations under the License.\n #include \"xla/mlir_hlo/mhlo/transforms/passes.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/dump.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -131,9 +131,11 @@ bool Needs64BitIndices(const HloComputation* computation) {\n \n using mlir::AffineExpr;\n \n-IndexingMap GetDefaultIndexingMap(absl::Span<const int64_t> thread_tile_sizes,\n-                                  absl::Span<const int64_t> shape,\n-                                  mlir::MLIRContext* mlir_context) {\n+IndexingMap GetDefaultIndexingMap(\n+    absl::Span<const int64_t> thread_tile_sizes,\n+    absl::Span<const int64_t> shape,\n+    // TODO: b/451959933 - Use reference or absl_nullable pointer.\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   CHECK_EQ(thread_tile_sizes.size(), shape.size())\n       << \"thread_tile_sizes and shape must have the same size\";\n   SmallVector<int64_t> thread_tile_counts;\n@@ -142,12 +144,14 @@ IndexingMap GetDefaultIndexingMap(absl::Span<const int64_t> thread_tile_sizes,\n     thread_tile_counts.push_back(CeilDiv(dim_size, tile_size));\n   }\n   // Delinearize thread_expr w.r.t. number of thread tiles per dimension.\n-  auto thread_expr = mlir::getAffineDimExpr(0, mlir_context);\n+  auto thread_expr =\n+      mlir::getAffineDimExpr(0, symbolic_expr_context->GetMLIRContext());\n   SmallVector<AffineExpr, 4> thread_ids =\n       DelinearizeInBoundsIndex(thread_expr, thread_tile_counts);\n   SmallVector<AffineExpr, 4> result;\n   result.reserve(thread_ids.size());\n-  auto linear_index = mlir::getAffineSymbolExpr(0, mlir_context);\n+  auto linear_index =\n+      mlir::getAffineSymbolExpr(0, symbolic_expr_context->GetMLIRContext());\n   SmallVector<AffineExpr, 4> indices_in_tile =\n       DelinearizeInBoundsIndex(linear_index, thread_tile_sizes);\n   SmallVector<std::pair<AffineExpr, Interval>, 4> constraints;\n@@ -160,8 +164,9 @@ IndexingMap GetDefaultIndexingMap(absl::Span<const int64_t> thread_tile_sizes,\n   int64_t num_threads = Product(thread_tile_counts);\n   int64_t num_tile_elements = Product(thread_tile_sizes);\n \n-  auto affine_map = mlir::AffineMap::get(/*num_dims=*/1, /*num_symbols=*/1,\n-                                         result, mlir_context);\n+  auto affine_map =\n+      mlir::AffineMap::get(/*num_dims=*/1, /*num_symbols=*/1, result,\n+                           symbolic_expr_context->GetMLIRContext());\n   return IndexingMap(\n       affine_map, {IndexingMap::Variable({0, num_threads - 1, \"thread_id\"})},\n       {IndexingMap::Variable({0, num_tile_elements - 1, \"linear_index\"})}, {},\n@@ -272,7 +277,8 @@ absl::StatusOr<emitters::CallTargetProvider> EmitCallTargets(\n     for (const auto& subgraph : comp.subgraphs()) {\n       if (subgraph_to_mlir_fn.contains(&subgraph)) {\n         TF_RETURN_IF_ERROR(emitters::SubgraphToMlirFunction(\n-            comp, subgraph, subgraph_to_mlir_fn[&subgraph], call_targets));\n+            comp, subgraph, subgraph_to_mlir_fn[&subgraph], call_targets,\n+            computations.symbolic_expr_context()));\n       }\n     }\n   }\n@@ -281,7 +287,8 @@ absl::StatusOr<emitters::CallTargetProvider> EmitCallTargets(\n     TF_RETURN_IF_ERROR(emitters::SubgraphToMlirFunction(\n         computations.FindPartitionedComputation(\n             fusion.fused_instructions_computation()),\n-        epilogue, subgraph_to_mlir_fn[&epilogue], call_targets));\n+        epilogue, subgraph_to_mlir_fn[&epilogue], call_targets,\n+        computations.symbolic_expr_context()));\n   }\n \n   return call_targets;"
        },
        {
            "sha": "190ee802130184b7199380e3451b63347ba3e659",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter.h",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,21 +28,22 @@ limitations under the License.\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/mlir/tools/mlir_replay/public/compiler_trace.pb.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace cpu {\n \n-IndexingMap GetDefaultIndexingMap(absl::Span<const int64_t> thread_tile_sizes,\n-                                  absl::Span<const int64_t> shape,\n-                                  mlir::MLIRContext* mlir_context);\n+IndexingMap GetDefaultIndexingMap(\n+    absl::Span<const int64_t> thread_tile_sizes,\n+    absl::Span<const int64_t> shape,\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n absl::StatusOr<mlir::func::FuncOp> EmitEntryFunctionApi(\n     mlir::ModuleOp fusion_module, const HloFusionInstruction& fusion,\n@@ -72,10 +73,11 @@ class CpuFusionEmitterBase {\n   virtual int64_t num_threads() const = 0;\n \n   virtual std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t, mlir::MLIRContext*) const = 0;\n+      int64_t root_index, gpu::SymbolicExprContext* ctx) const = 0;\n \n   virtual std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n-      int64_t, int64_t, mlir::MLIRContext*) const = 0;\n+      int64_t root_index, int64_t hero_operand_index,\n+      gpu::SymbolicExprContext* ctx) const = 0;\n \n   virtual std::string BackendExtraOptions() { return {}; }\n "
        },
        {
            "sha": "ccbe54a009d2b0924c412c89836de6a26b363edd",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_value.h\"\n #include \"xla/service/cpu/cpu_executable.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/logical_buffer.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/platform/casts.h\"\n@@ -129,8 +130,11 @@ TEST_F(CpuFusionEmitterTest, ScatterMlir) {\n                           RunBufferAssignment(*hlo_module));\n   auto fusion = Cast<HloFusionInstruction>(\n       hlo_module->entry_computation()->root_instruction());\n-  auto context = FusionCompiler::CreateContext();\n-  CpuScatterFusion emitter(*buffer_assignment, fusion, context.get());\n+  auto mlir_context = FusionCompiler::CreateContext();\n+  auto symbolic_expr_context =\n+      std::make_unique<gpu::SymbolicExprContext>(mlir_context.get());\n+  CpuScatterFusion emitter(*buffer_assignment, fusion,\n+                           symbolic_expr_context.get());\n   TF_ASSERT_OK_AND_ASSIGN(KernelDefinition kernel_definition,\n                           emitter.EmitKernelDefinition());\n   const auto& mlir_source = kernel_definition.source();\n@@ -157,12 +161,16 @@ TEST_F(CpuFusionEmitterTest, ScatterLlvm) {\n                           RunBufferAssignment(*hlo_module));\n   auto fusion = Cast<HloFusionInstruction>(\n       hlo_module->entry_computation()->root_instruction());\n-  auto context = FusionCompiler::CreateContext();\n-  CpuScatterFusion emitter(*buffer_assignment, fusion, context.get());\n+  auto mlir_context = FusionCompiler::CreateContext();\n+  auto symbolic_expr_context =\n+      std::make_unique<gpu::SymbolicExprContext>(mlir_context.get());\n+  CpuScatterFusion emitter(*buffer_assignment, fusion,\n+                           symbolic_expr_context.get());\n   TF_ASSERT_OK_AND_ASSIGN(KernelDefinition kernel_definition,\n                           emitter.EmitKernelDefinition());\n   auto [spec, source] = std::move(kernel_definition).ReleaseStorage();\n-  FusionCompiler compiler(context.get(), FusionCompiler::Options{512, 1, true});\n+  FusionCompiler compiler(mlir_context.get(),\n+                          FusionCompiler::Options{512, 1, true});\n   TF_ASSERT_OK_AND_ASSIGN(LlvmIrKernelSource llvm_source,\n                           compiler.Compile(std::move(source)));\n   auto llvm_dump = llvm_source.ToString();"
        },
        {
            "sha": "f3c84fb219ddcef54e4b3dea34618b3c1dd9679d",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_scatter_emitter.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -71,6 +71,7 @@ limitations under the License.\n #include \"xla/runtime/work_group.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/scatter_simplifier.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -91,22 +92,23 @@ namespace ma = ::mlir::arith;\n namespace scf = ::mlir::scf;\n \n std::vector<emitters::EpilogueSpecification> CpuScatterFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion, mlir::MLIRContext* context) const {\n+    const HloFusionInstruction& fusion,\n+    gpu::SymbolicExprContext* symbolic_expr_context) const {\n   const auto* scatter = fusion_->fused_expression_root();\n   // We don't actually support epilogues for scatter, but this is how we tell\n   // the base class that we don't want it to generate code for the scatter.\n   return {emitters::EpilogueSpecification::FromIdentityIndexing(\n-      scatter, scatter, context)};\n+      scatter, scatter, symbolic_expr_context)};\n }\n \n std::optional<IndexingMap> CpuScatterFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, mlir::MLIRContext* ctx) const {\n+    int64_t root_index, gpu::SymbolicExprContext* ctx) const {\n   return std::nullopt;\n }\n \n std::optional<IndexingMap> CpuScatterFusion::ComputeThreadIdToInputIndexing(\n     int64_t root_index, int64_t hero_operand_index,\n-    mlir::MLIRContext* ctx) const {\n+    gpu::SymbolicExprContext* ctx) const {\n   const auto* scatter =\n       DynCast<HloScatterInstruction>(fusion_->fused_expression_root());\n   CHECK(ScatterSimplifier::IsSimplifiedScatter(scatter))\n@@ -181,12 +183,13 @@ SmallVector<Value> EmitScatterComputation(\n   return {atomic_rmw->getResult(0)};\n }\n \n-CpuScatterFusion::CpuScatterFusion(const BufferAssignment& buffer_assignment,\n-                                   const HloFusionInstruction* fusion,\n-                                   mlir::MLIRContext* context)\n+CpuScatterFusion::CpuScatterFusion(\n+    const BufferAssignment& buffer_assignment,\n+    const HloFusionInstruction* fusion,\n+    gpu::SymbolicExprContext* symbolic_expr_context)\n     : buffer_assignment_(buffer_assignment),\n       fusion_(fusion),\n-      context_(context) {\n+      symbolic_expr_context_(symbolic_expr_context) {\n   const auto* scatter = Cast<HloScatterInstruction>(\n       fusion->fused_instructions_computation()->root_instruction());\n   auto update_shape = scatter->scatter_updates().front()->shape();\n@@ -249,7 +252,7 @@ IndexingMap GetScatterIndexingMap(\n }\n \n absl::StatusOr<MlirKernelDefinition> CpuScatterFusion::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(context_);\n+  mlir::OpBuilder builder(symbolic_expr_context_->GetMLIRContext());\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> mlir_module,\n                       CreateNamedMlirModuleOp(*fusion_, builder));\n \n@@ -273,9 +276,10 @@ absl::StatusOr<MlirKernelDefinition> CpuScatterFusion::EmitKernelDefinition() {\n                            std::string(module_name), buffer_assignment_));\n \n   std::vector<emitters::EpilogueSpecification> epilogues =\n-      GetEpilogues(*fusion_, context_);\n+      GetEpilogues(*fusion_, symbolic_expr_context_);\n   emitters::PartitionedComputations computations(\n-      fusion_->fused_instructions_computation(), context_, epilogues);\n+      fusion_->fused_instructions_computation(), symbolic_expr_context_,\n+      epilogues);\n   TF_ASSIGN_OR_RETURN(\n       emitters::CallTargetProvider call_targets,\n       EmitCallTargets(mlir_module.get(), *fusion_, computations, epilogues));"
        },
        {
            "sha": "cea1943120c8ccc5c6d883dce0c48fd725e1e9b1",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_scatter_emitter.h",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Value.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/kernel_definition.h\"\n@@ -32,16 +31,17 @@ limitations under the License.\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace cpu {\n \n // Generic scatter fusion. Lowers to LLVM via MLIR.\n class CpuScatterFusion final : public MlirKernelEmitter {\n  public:\n-  explicit CpuScatterFusion(const BufferAssignment& buffer_assignment,\n-                            const HloFusionInstruction* fusion,\n-                            mlir::MLIRContext* context);\n+  CpuScatterFusion(const BufferAssignment& buffer_assignment,\n+                   const HloFusionInstruction* fusion,\n+                   gpu::SymbolicExprContext* symbolic_expr_context);\n \n   absl::StatusOr<MlirKernelDefinition> EmitKernelDefinition() final;\n \n@@ -55,21 +55,22 @@ class CpuScatterFusion final : public MlirKernelEmitter {\n       const HloFusionInstruction& fusion) const;\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n-      const HloFusionInstruction& fusion, mlir::MLIRContext* context) const;\n+      const HloFusionInstruction& fusion,\n+      gpu::SymbolicExprContext* symbolic_expr_context) const;\n \n   mlir::Value EmitThreadId(mlir::ImplicitLocOpBuilder& builder, int dim) const;\n \n   // These two methods do not seem to be used @ecg?\n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const;\n+      int64_t root_index, gpu::SymbolicExprContext* ctx) const;\n \n   std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n       int64_t root_index, int64_t hero_operand_index,\n-      mlir::MLIRContext* ctx) const;\n+      gpu::SymbolicExprContext* ctx) const;\n \n   const BufferAssignment& buffer_assignment_;\n   const HloFusionInstruction* fusion_;\n-  mlir::MLIRContext* context_;\n+  gpu::SymbolicExprContext* symbolic_expr_context_;\n \n   int64_t vector_size_;\n   int64_t num_threads_;"
        },
        {
            "sha": "9b5648a9a5e965561e25723f3e1396dabae34eef",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/cpu/alignment.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n #include \"xla/backends/cpu/codegen/symbol_name_util.h\"\n@@ -57,6 +56,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -208,7 +208,7 @@ static HloFusionSpec GetLoopFusionSpec(const HloFusionInstruction& fusion) {\n }\n \n static absl::StatusOr<MlirKernelDefinition> EmitLoopFusionKernel(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+    gpu::SymbolicExprContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, absl::string_view name) {\n   VLOG(2) << \"Emitting loop fusion kernel: \" << name;\n   HloFusionSpec fusion_spec = GetLoopFusionSpec(fusion);\n@@ -225,7 +225,7 @@ static absl::StatusOr<MlirKernelDefinition> EmitLoopFusionKernel(\n   auto [kernel_spec, kernel_source] =\n       std::move(mlir_kernel_definition).ReleaseStorage();\n \n-  mlir::OpBuilder builder(&context);\n+  mlir::OpBuilder builder(context.GetMLIRContext());\n   kernel_source.module().getOperation()->setAttr(\n       xla::CpuMemoryRegionNameAttr::name,\n       builder.getStringAttr(\n@@ -234,7 +234,7 @@ static absl::StatusOr<MlirKernelDefinition> EmitLoopFusionKernel(\n }\n \n static absl::StatusOr<MlirKernelDefinition> EmitConcatenateFusionKernel(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+    gpu::SymbolicExprContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, absl::string_view name) {\n   VLOG(2) << \"Emitting concatenate fusion kernel: \" << name;\n   HloFusionSpec fusion_spec = GetLoopFusionSpec(fusion);\n@@ -251,7 +251,7 @@ static absl::StatusOr<MlirKernelDefinition> EmitConcatenateFusionKernel(\n   auto [kernel_spec, kernel_source] =\n       std::move(mlir_kernel_definition).ReleaseStorage();\n \n-  mlir::OpBuilder builder(&context);\n+  mlir::OpBuilder builder(context.GetMLIRContext());\n   kernel_source.module().getOperation()->setAttr(\n       xla::CpuMemoryRegionNameAttr::name,\n       builder.getStringAttr(BuildModuleMemoryRegionName(\n@@ -260,7 +260,7 @@ static absl::StatusOr<MlirKernelDefinition> EmitConcatenateFusionKernel(\n }\n \n static absl::StatusOr<MlirKernelDefinition> EmitDynamicUpdateSliceFusionKernel(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+    gpu::SymbolicExprContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, absl::string_view name) {\n   VLOG(2) << \"Emitting dynamic update slice fusion kernel: \" << name;\n   HloFusionSpec fusion_spec = GetLoopFusionSpec(fusion);\n@@ -278,7 +278,7 @@ static absl::StatusOr<MlirKernelDefinition> EmitDynamicUpdateSliceFusionKernel(\n   auto [kernel_spec, kernel_source] =\n       std::move(mlir_kernel_definition).ReleaseStorage();\n \n-  mlir::OpBuilder builder(&context);\n+  mlir::OpBuilder builder(context.GetMLIRContext());\n   kernel_source.module().getOperation()->setAttr(\n       xla::CpuMemoryRegionNameAttr::name,\n       builder.getStringAttr(\n@@ -287,7 +287,7 @@ static absl::StatusOr<MlirKernelDefinition> EmitDynamicUpdateSliceFusionKernel(\n }\n \n absl::StatusOr<MlirKernelDefinition> EmitFusionKernel(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+    gpu::SymbolicExprContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, bool use_unique_c_name) {\n   if (fusion.fusion_kind() == HloFusionInstruction::FusionKind::kLoop) {\n     TF_ASSIGN_OR_RETURN(std::string name, GetName(fusion, use_unique_c_name));"
        },
        {
            "sha": "18087d72a7dc702c7df45dbc8d453a338e674e43",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -17,18 +17,18 @@ limitations under the License.\n #define XLA_BACKENDS_CPU_CODEGEN_FUSION_EMITTER_H_\n \n #include \"absl/status/statusor.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/codegen/mlir_kernel_definition.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla::cpu {\n \n emitters::KernelArguments::BufferAlignment GetDefaultBufferAlignment();\n \n absl::StatusOr<MlirKernelDefinition> EmitFusionKernel(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+    gpu::SymbolicExprContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, bool use_unique_c_name);\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "3f63d19c9d81e8aad5c5967dc2c21b5fb83422ee",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_emitter_test.py",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter_test.py?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -53,8 +53,11 @@ def test_basic_add_sub(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n+    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n-        mlir_context, hlo_module.get_root_instruction(), buffer_assignment\n+        symbolic_expr_context,\n+        hlo_module.get_root_instruction(),\n+        buffer_assignment,\n     )\n \n     kernel_runner = testlib_cpu.KernelRunner.create(\n@@ -111,8 +114,11 @@ def test_convert_f32_bf16_f32(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n+    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n-        mlir_context, hlo_module.get_root_instruction(), buffer_assignment\n+        symbolic_expr_context,\n+        hlo_module.get_root_instruction(),\n+        buffer_assignment,\n     )\n \n     kernel_runner = testlib_cpu.KernelRunner.create(\n@@ -164,8 +170,11 @@ def test_convert_f32_bf16_f32_nan(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n+    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n-        mlir_context, hlo_module.get_root_instruction(), buffer_assignment\n+        symbolic_expr_context,\n+        hlo_module.get_root_instruction(),\n+        buffer_assignment,\n     )\n \n     kernel_runner = testlib_cpu.KernelRunner.create(\n@@ -214,8 +223,11 @@ def test_constant_with_layout(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n+    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n-        mlir_context, hlo_module.get_root_instruction(), buffer_assignment\n+        symbolic_expr_context,\n+        hlo_module.get_root_instruction(),\n+        buffer_assignment,\n     )\n \n     kernel_runner = testlib_cpu.KernelRunner.create(\n@@ -261,8 +273,11 @@ def test_exp_nan_dce(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n+    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n-        mlir_context, hlo_module.get_root_instruction(), buffer_assignment\n+        symbolic_expr_context,\n+        hlo_module.get_root_instruction(),\n+        buffer_assignment,\n     )\n \n     kernel_runner = testlib_cpu.KernelRunner.create("
        },
        {
            "sha": "476f8f3ba825ecfc27bcb3f8c236c46580b08b5c",
            "filename": "third_party/xla/xla/backends/cpu/codegen/scatter_kernel_emitter_test.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fscatter_kernel_emitter_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fscatter_kernel_emitter_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fscatter_kernel_emitter_test.py?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -94,10 +94,13 @@ def create_scatter_runner(\n   hlo_module = testlib_cpu.run_fusion_wrapper_pass(hlo_module)\n   hlo_module, buffer_assignment = utilities.annotate_hlo_module(hlo_module)\n \n-  context = testlib_cpu.MLIRContext()\n+  mlir_context = testlib_cpu.MLIRContext()\n+  symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n \n   scatter_emitter = testlib_cpu.ScatterKernelEmitter(\n-      hlo_module.get_root_instruction(), buffer_assignment, context\n+      hlo_module.get_root_instruction(),\n+      buffer_assignment,\n+      symbolic_expr_context,\n   )\n   kernel_definition = scatter_emitter.emit_kernel_definition()\n "
        },
        {
            "sha": "c771b9c61f4eef5edcd36115566fd5410607f1b8",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tools/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -82,6 +82,7 @@ xla_cc_binary(\n         \"//xla/codegen:mlir_kernel_definition\",\n         \"//xla/codegen/tools:test_lib\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "c41605d6b78659e69f4759f01ad39490611aa5e3",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tools/fusion_to_mlir.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <memory>\n #include <string>\n \n #include \"absl/log/check.h\"\n@@ -24,19 +25,23 @@ limitations under the License.\n #include \"xla/codegen/tools/test_lib.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/platform/init_main.h\"\n \n namespace xla::cpu {\n \n absl::Status Run(const std::string& filename) {\n-  auto context = FusionCompiler::CreateContext();\n+  auto mlir_context = FusionCompiler::CreateContext();\n+  auto symbolic_expr_context =\n+      std::make_unique<gpu::SymbolicExprContext>(mlir_context.get());\n   TF_ASSIGN_OR_RETURN(auto module, LoadTestModule(filename));\n   auto fusion = DynCast<HloFusionInstruction>(\n       module->entry_computation()->root_instruction());\n   fusion->SetAndSanitizeName(\"main\");\n-  TF_ASSIGN_OR_RETURN(MlirKernelDefinition kernel_definition,\n-                      EmitFusionKernel(*context, *fusion, nullptr, false));\n+  TF_ASSIGN_OR_RETURN(\n+      MlirKernelDefinition kernel_definition,\n+      EmitFusionKernel(*symbolic_expr_context, *fusion, nullptr, false));\n   llvm::outs() << kernel_definition.source().ToString();\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "85c29a01922057500d70706c24834c6392467785",
            "filename": "third_party/xla/xla/backends/cpu/testlib/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -169,6 +169,7 @@ tsl_pybind_extension(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/cpu:cpu_compiler_pure\",\n         \"//xla/service/cpu:fusion_wrapper\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n     ],\n )\n "
        },
        {
            "sha": "67edb286da8f02fec6a038f1661e867b9c4d873c",
            "filename": "third_party/xla/xla/backends/cpu/testlib/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2F__init__.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2F__init__.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2F__init__.py?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -29,6 +29,7 @@\n MLIRContext = _extension.MLIRContext\n MlirTestKernelEmitter = _extension.MlirTestKernelEmitter\n ScatterKernelEmitter = _extension.ScatterKernelEmitter\n+SymbolicExprContext = _extension.SymbolicExprContext\n TargetMachineFeatures = _extension.TargetMachineFeatures\n # go/keep-sorted end\n "
        },
        {
            "sha": "91b3d4a0cc2a46fdf744ccfe42ffd85428366cb9",
            "filename": "third_party/xla/xla/backends/cpu/testlib/kernel_runner_extension.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner_extension.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner_extension.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner_extension.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -57,6 +57,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/cpu/cpu_compiler.h\"\n #include \"xla/service/cpu/fusion_wrapper.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n \n namespace xla::cpu {\n@@ -156,6 +157,10 @@ NB_MODULE(_extension, kernel_runner_module) {\n   nb::class_<mlir::MLIRContext>(kernel_runner_module, \"MLIRContext\")\n       .def(nb::new_([] { return FusionCompiler::CreateContext(); }));\n \n+  nb::class_<gpu::SymbolicExprContext>(kernel_runner_module,\n+                                       \"SymbolicExprContext\")\n+      .def(nb::init<mlir::MLIRContext*>(), nb::keep_alive<1, 2>());\n+\n   nb::class_<TargetMachineFeatures>(kernel_runner_module,\n                                     \"TargetMachineFeatures\")\n       .def(\"__str__\", &TargetMachineFeatures::get_target_feature_string);\n@@ -193,20 +198,22 @@ NB_MODULE(_extension, kernel_runner_module) {\n       .def(\n           \"__init__\",\n           [](CpuScatterFusion* self, const HloFusionInstruction* instruction,\n-             const BufferAssignment* bufffer_assignment,\n-             mlir::MLIRContext* context) {\n-            new (self)\n-                CpuScatterFusion(*bufffer_assignment, instruction, context);\n+             const BufferAssignment* buffer_assignment,\n+             gpu::SymbolicExprContext* symbolic_expr_context) {\n+            new (self) CpuScatterFusion(*buffer_assignment, instruction,\n+                                        symbolic_expr_context);\n           },\n           nb::keep_alive<1, 2>(), nb::keep_alive<1, 3>(),\n-          nb::keep_alive<1, 3>());\n+          nb::keep_alive<1, 4>());\n \n   kernel_runner_module.def(\n       \"emit_fusion_kernel\",\n-      [](mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+      [](gpu::SymbolicExprContext& symbolic_expr_context,\n+         const HloFusionInstruction& fusion,\n          const BufferAssignment* buffer_assignment) {\n         absl::StatusOr<MlirKernelDefinition> kernel_definition =\n-            EmitFusionKernel(context, fusion, buffer_assignment, false);\n+            EmitFusionKernel(symbolic_expr_context, fusion, buffer_assignment,\n+                             false);\n         if (!kernel_definition.ok()) {\n           throw std::runtime_error(kernel_definition.status().ToString());\n         }"
        },
        {
            "sha": "f2c9cf4a5fbb9e23e1928864722f7b0264a16492",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -66,6 +66,7 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu/model:fusion_analysis_cache\",\n         \"//xla/service/gpu/model:gpu_indexing_performance_model\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n@@ -437,6 +438,7 @@ cc_library(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:cublas_cudnn\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:custom_kernel_fusion_rewriter\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n@@ -451,7 +453,6 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -516,6 +517,7 @@ cc_library(\n         \"//xla/service/gpu:split_k_gemm_rewriter\",\n         \"//xla/service/gpu/autotuning:dot_search_space\",\n         \"//xla/service/gpu/autotuning:triton_configs\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:fusion_wrapper\",\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/service/gpu/transforms:priority_fusion\",\n@@ -529,7 +531,6 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -561,6 +562,7 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n@@ -683,10 +685,10 @@ cc_library(\n         \":triton\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/service:compiler\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n     alwayslink = True,\n )\n@@ -704,10 +706,10 @@ cc_library(\n         \":triton\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/service:compiler\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n         \"//xla/stream_executor/rocm:rocm_platform_id\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n     alwayslink = True,\n )\n@@ -718,8 +720,8 @@ cc_library(\n     deps = [\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/service:compiler\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -824,6 +826,7 @@ xla_cc_binary(\n         \"//xla/service:compiler\",\n         \"//xla/service:gpu_plugin\",\n         \"//xla/service:platform_util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\","
        },
        {
            "sha": "bdf7c0995a3ca55ebe6b61a72828ac35b4ee47e9",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -84,7 +85,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetModule(\n \n absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       const std::string& autotune_cache_mode_str,\n-                      mlir::MLIRContext* mlir_context) {\n+                      SymbolicExprContext* symbolic_expr_context) {\n   TF_ASSIGN_OR_RETURN(std::string platform_name,\n                       PlatformUtil::CanonicalPlatformName(\"gpu\"));\n \n@@ -106,7 +107,7 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       registry.FindObject<GetCodegenBackends>(platform->id()));\n   std::vector<std::unique_ptr<CodegenBackend>> backends =\n       get_codegen_backends(stream_executor, &debug_options, compiler.get(),\n-                           &target_config, mlir_context);\n+                           &target_config, symbolic_expr_context);\n \n   std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n       std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n@@ -181,8 +182,9 @@ int main(int argc, char* argv[]) {\n   auto module = xla::gpu::GetModule(hlo_file);\n   CHECK_OK(module.status());\n   mlir::MLIRContext mlir_context;\n+  xla::gpu::SymbolicExprContext symbolic_expr_context(&mlir_context);\n   CHECK_OK(xla::gpu::Autotune(*module.value(), cache_dir, autotune_cache_mode,\n-                              &mlir_context));\n+                              &symbolic_expr_context));\n   std::cout << module.value()->ToString() << std::endl;\n   return 0;\n }"
        },
        {
            "sha": "93041f10243e75a7f66963ad955b98e392acb26b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_indexing_performance_model.h\"\n #include \"xla/service/instruction_fusion.h\"\n@@ -305,8 +306,10 @@ BlockLevelEmitterBackend::GetCostModelConfig(\n   auto device_info = target_config().device_description;\n   HloFusionAnalysisCache fusion_analysis_cache(device_info);\n   mlir::MLIRContext ctx;\n+  SymbolicExprContext symbolic_expr_context(&ctx);\n   GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n-      &device_info, &fusion_analysis_cache, shape_size_fn_, &ctx);\n+      &device_info, &fusion_analysis_cache, shape_size_fn_,\n+      &symbolic_expr_context);\n \n   auto fusion_adaptor =\n       HloFusionAdaptor::ForInstruction(Cast<HloFusionInstruction>(&instr));"
        },
        {
            "sha": "c7e573e0787557defd6e5ebfdc802f63d37c9ae4",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -20,9 +20,9 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla {\n@@ -32,7 +32,8 @@ namespace gpu {\n struct GetCodegenBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      const Compiler::TargetConfig*, mlir::MLIRContext* mlir_context)>;\n+      const Compiler::TargetConfig*,\n+      SymbolicExprContext* symbolic_expr_context)>;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "9a91ad0fbc8e0e3f3b53f9734ce1a8edad6a69b4",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,14 +19,14 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n #include \"xla/backends/gpu/autotuner/cudnn.h\"\n #include \"xla/backends/gpu/autotuner/factory.h\"\n #include \"xla/backends/gpu/autotuner/triton.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n #include \"xla/stream_executor/platform/platform_object_registry.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -38,10 +38,10 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n     const Compiler::TargetConfig* target_config,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n-      debug_options, compiler, target_config, mlir_context));\n+      debug_options, compiler, target_config, symbolic_expr_context));\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_executor, debug_options, compiler, target_config));\n   backends.push_back(std::make_unique<CublasLtBackend>("
        },
        {
            "sha": "902b1c28f3b3f9eb82fb6ecab983633e44972da3",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_rocm.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,12 +19,12 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/factory.h\"\n #include \"xla/backends/gpu/autotuner/triton.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/platform/platform_object_registry.h\"\n #include \"xla/stream_executor/rocm/rocm_platform_id.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -36,10 +36,10 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n     const Compiler::TargetConfig* target_config,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n-      debug_options, compiler, target_config, mlir_context));\n+      debug_options, compiler, target_config, symbolic_expr_context));\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_executor, debug_options, compiler, target_config));\n   return backends;"
        },
        {
            "sha": "7f819b6170aa040ab5c869cae2510d36e06451d5",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 12,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n@@ -41,6 +40,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/transforms/custom_kernel_fusion_rewriter.h\"\n #include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n@@ -79,7 +79,7 @@ HloCostAnalysis::Options PriorityFusionOptions() {\n absl::Status FissionToCublas(HloModule* hlo_module,\n                              const se::DeviceDescription& device_description,\n                              bool rewrite_to_cublaslt,\n-                             mlir::MLIRContext* mlir_context) {\n+                             SymbolicExprContext* symbolic_expr_context) {\n   hlo_module->mutable_config()\n       .mutable_debug_options()\n       .set_xla_gpu_enable_cublaslt(rewrite_to_cublaslt);\n@@ -117,7 +117,7 @@ absl::Status FissionToCublas(HloModule* hlo_module,\n \n     PriorityFusion fusion_pass(\n         /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n-        mlir_context);\n+        symbolic_expr_context);\n     TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n   }\n \n@@ -130,11 +130,11 @@ absl::Status FissionToCublas(HloModule* hlo_module,\n \n absl::Status FissionToCustomKernel(\n     HloModule* hlo_module, const se::DeviceDescription& device_description,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   CustomKernelFusionRewriter custom_kernel_fusion_rewriter(&device_description);\n   PriorityFusion fusion_pass(\n       /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n-      mlir_context);\n+      symbolic_expr_context);\n   TF_ASSIGN_OR_RETURN(bool is_rewritten_to_custom_kernel,\n                       custom_kernel_fusion_rewriter.Run(hlo_module));\n   TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n@@ -238,7 +238,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   std::unique_ptr<HloModule> cublas_hlo_module = hlo_module->Clone();\n   if (FissionToCublas(cublas_hlo_module.get(),\n                       target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/false, mlir_context_)\n+                      /*rewrite_to_cublaslt=*/false, symbolic_expr_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> cublas_configs,\n@@ -253,7 +253,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   std::unique_ptr<HloModule> cublaslt_hlo_module = hlo_module->Clone();\n   if (FissionToCublas(cublaslt_hlo_module.get(),\n                       target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/true, mlir_context_)\n+                      /*rewrite_to_cublaslt=*/true, symbolic_expr_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> cublaslt_configs,\n@@ -267,7 +267,8 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n \n   std::unique_ptr<HloModule> custom_kernel_hlo_module = hlo_module->Clone();\n   if (FissionToCustomKernel(custom_kernel_hlo_module.get(),\n-                            target_config().device_description, mlir_context_)\n+                            target_config().device_description,\n+                            symbolic_expr_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> custom_kernel_configs,\n@@ -315,7 +316,7 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   if (!use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n     TF_RETURN_IF_ERROR(\n         FissionToCublas(hlo_module, target_config().device_description,\n-                        /*rewrite_to_cublaslt=*/false, mlir_context_));\n+                        /*rewrite_to_cublaslt=*/false, symbolic_expr_context_));\n     for (HloComputation* computation :\n          hlo_module->MakeNonfusionComputations()) {\n       for (HloInstruction* instruction : computation->instructions()) {\n@@ -331,7 +332,7 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   if (use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n     TF_RETURN_IF_ERROR(\n         FissionToCublas(hlo_module, target_config().device_description,\n-                        /*rewrite_to_cublaslt=*/true, mlir_context_));\n+                        /*rewrite_to_cublaslt=*/true, symbolic_expr_context_));\n     for (HloComputation* computation :\n          hlo_module->MakeNonfusionComputations()) {\n       for (HloInstruction* instruction : computation->instructions()) {\n@@ -347,8 +348,9 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   }\n \n   if (config.Is<CustomKernelBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(FissionToCustomKernel(\n-        hlo_module, target_config().device_description, mlir_context_));\n+    TF_RETURN_IF_ERROR(FissionToCustomKernel(hlo_module,\n+                                             target_config().device_description,\n+                                             symbolic_expr_context_));\n     for (HloComputation* computation : hlo_module->computations()) {\n       if (IsCustomKernel(computation)) {\n         TF_RETURN_IF_ERROR(custom_kernel_backend_.ApplyConfig("
        },
        {
            "sha": "60b95da29bfa59ea6066ae9bdd1f9396fc470386",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,14 +22,14 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n #include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla.pb.h\"\n \n@@ -46,15 +46,15 @@ class FissionBackend : public GpuCodegenBackend {\n   explicit FissionBackend(stream_executor::StreamExecutor* stream_executor,\n                           const DebugOptions* debug_options, Compiler* compiler,\n                           const Compiler::TargetConfig* target_config,\n-                          mlir::MLIRContext* mlir_context)\n+                          SymbolicExprContext* symbolic_expr_context)\n       : GpuCodegenBackend(\"Fission\", debug_options, compiler, target_config),\n         cublas_backend_(stream_executor, debug_options, compiler,\n                         target_config),\n         cublaslt_backend_(stream_executor, debug_options, compiler,\n                           target_config),\n         custom_kernel_backend_(stream_executor, debug_options, compiler,\n                                target_config),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;\n@@ -68,7 +68,7 @@ class FissionBackend : public GpuCodegenBackend {\n   CublasBackend cublas_backend_;\n   CublasLtBackend cublaslt_backend_;\n   CustomKernelBackend custom_kernel_backend_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "c94416b2fd147ea8e5225bad6dc45169743c633b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -76,6 +76,7 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n   Compiler::TargetConfig target_config_;\n   FissionBackend backend_;\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n \n   FissionBackendTest()\n       : stream_executor_(PlatformUtil::GetDefaultPlatform()\n@@ -84,7 +85,7 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n                              .value()),\n         target_config_(stream_executor_),\n         backend_(stream_executor_, &debug_options_, &compiler_, &target_config_,\n-                 &mlir_context_) {}\n+                 &symbolic_expr_context_) {}\n };\n \n TEST_F(FissionBackendTest, CanCreateCublasBackend) {"
        },
        {
            "sha": "228adf65e96f9ee66dec1b182fef36209eecaa8e",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -202,15 +202,15 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonBackend::RunHloPasses(\n   priority_fusion_options.count_multiple_input_accesses = true;\n   PriorityFusion priority_fusion(\n       /*thread_pool=*/nullptr, gpu_device_info, priority_fusion_options,\n-      mlir_context_);\n+      symbolic_expr_context_);\n   TF_RETURN_IF_ERROR(priority_fusion.Run(hlo_module.get()).status());\n \n   // If the priority fusion pass above skipped some instructions, turn them\n   // into fusions.\n   FusionWrapper fusion_wrapper(gpu_device_info);\n   TF_RETURN_IF_ERROR(fusion_wrapper.Run(hlo_module.get()).status());\n \n-  NestGemmFusion nest_gemm_fusion(gpu_device_info, mlir_context_);\n+  NestGemmFusion nest_gemm_fusion(gpu_device_info, symbolic_expr_context_);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(hlo_module.get()).status());\n   return hlo_module;\n }"
        },
        {
            "sha": "e8efef8a619a83c9c79535417312f3b4666ef13a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -21,12 +21,12 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla.pb.h\"\n \n@@ -38,9 +38,9 @@ class TritonBackend : public GpuCodegenBackend {\n  public:\n   explicit TritonBackend(const DebugOptions* debug_options, Compiler* compiler,\n                          const Compiler::TargetConfig* target_config,\n-                         mlir::MLIRContext* mlir_context)\n+                         SymbolicExprContext* symbolic_expr_context)\n       : GpuCodegenBackend(\"Triton\", debug_options, compiler, target_config),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;\n@@ -58,7 +58,7 @@ class TritonBackend : public GpuCodegenBackend {\n       const Compiler::CompileOptions& options) override;\n \n   bool IsSupported(const HloInstruction& instr);\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "112f593258907b50a7783401a28710ff99a5d507",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n@@ -78,7 +79,8 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n                              ->ExecutorForDevice(0)\n                              .value()),\n         target_config_(stream_executor_),\n-        backend_(&debug_options_, &compiler_, &target_config_, &mlir_context_) {\n+        backend_(&debug_options_, &compiler_, &target_config_,\n+                 &symbolic_expr_context_) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n@@ -90,6 +92,7 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n   Compiler::TargetConfig target_config_;\n   TritonBackend backend_;\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(TritonBackendTest, GetSupportedConfigs) {"
        },
        {
            "sha": "bea41e2db7ff0da607432733e4c01eb1f95e61d0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -250,6 +250,7 @@ cc_library(\n         \"//xla/service/gpu:ir_emitter_context\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:target_util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:ir_array\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n@@ -296,8 +297,8 @@ cc_library(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )"
        },
        {
            "sha": "38f7e00b9bd45e3cde1fbff5505acb352dd34a31",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -931,7 +931,9 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n             : custom_call.raw_backend_config_string();\n     if (!backend_config_str.empty()) {\n       mlir::Attribute attr = mlir::parseAttribute(\n-          backend_config_str, ir_emitter_context.mlir_context());\n+          backend_config_str,\n+          // TODO: b/451959933 - Use reference or check pointer.\n+          ir_emitter_context.symbolic_expr_context()->GetMLIRContext());\n       auto dict = mlir::dyn_cast_or_null<mlir::DictionaryAttr>(attr);\n       if (dict == nullptr) {\n         return absl::InternalError("
        },
        {
            "sha": "638a72fca1ca470200ffaf28ec42402a7de3e908",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -31,6 +31,7 @@ cc_library(\n         \"//xla/service/gpu:gpu_fusible\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -74,6 +75,7 @@ cc_library(\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:target_util\",\n         \"//xla/service/gpu/llvm_gpu_backend:ptx_version_util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n@@ -139,6 +141,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n@@ -169,7 +172,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/codegen/emitters:computation_partitioner\",\n         \"//xla/codegen/emitters:dynamic_update_slice_kernel_emitter\",\n-        \"//xla/codegen/emitters:elemental_hlo_to_mlir\",\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/hlo/ir:hlo\",\n@@ -181,15 +183,14 @@ cc_library(\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\",\n-        \"@llvm-project//mlir:TensorDialect\",\n     ],\n )\n \n@@ -213,6 +214,7 @@ cc_library(\n         \"//xla/service/gpu:gpu_fusible\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -247,6 +249,7 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:reduction_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:launch_dim\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -337,6 +340,7 @@ cc_library(\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n@@ -378,6 +382,7 @@ cc_library(\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:inlined_vector\","
        },
        {
            "sha": "cb5d7081ead0246017d9f0db7ea61cb741d5d634",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/emitters/concatenate_kernel_emitter.h\"\n@@ -38,6 +37,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -59,13 +59,13 @@ LaunchDimensions ConcatenateFusion::launch_dimensions() const {\n }\n \n std::optional<IndexingMap> ConcatenateFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, mlir::MLIRContext* ctx) const {\n+    int64_t root_index, SymbolicExprContext* ctx) const {\n   return std::nullopt;\n }\n \n std::optional<std::vector<IndexingMap>>\n ConcatenateFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, mlir::MLIRContext* ctx) const {\n+    int64_t root_index, SymbolicExprContext* ctx) const {\n   IndexingMap map_for_largest_shape =\n       KernelEmitter::ComputeWorkItemIdToOutputIndexing(GetWorkDimensions(),\n                                                        largest_shape_, ctx);\n@@ -78,11 +78,11 @@ ConcatenateFusion::ComputeThreadIdToInputIndexing(\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>>\n ConcatenateFusion::CreateMLIRModule(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n-    const std::string& entry_function_name,\n+    SymbolicExprContext& symbolic_expr_context,\n+    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n   emitters::ConcatenateFusionKernelEmitter emitter(\n-      context, fusion, analysis_.fusion_spec(), buffer_assignment,\n+      symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,\n       BackendKind::kGpu);\n "
        },
        {
            "sha": "55be6511ed7bcc69cd0c1b679fb817b9a387e910",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -25,7 +25,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n@@ -35,6 +34,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n \n namespace xla {\n@@ -47,14 +47,14 @@ class ConcatenateFusion final : public EmitterBase {\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override;\n+      int64_t root_index, SymbolicExprContext* ctx) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override;\n+      int64_t root_index, SymbolicExprContext* ctx) const override;\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+      SymbolicExprContext& context, const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const override;\n "
        },
        {
            "sha": "712c3a13809575747917816b5a0c923c9e354edf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 18,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -106,6 +106,7 @@ limitations under the License.\n #include \"xla/service/gpu/kernel_reuse_cache.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/llvm_gpu_backend/ptx_version_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/target_util.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/status_macros.h\"\n@@ -278,7 +279,7 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n               TF_ASSIGN_OR_RETURN(\n                   auto module,\n                   CreateLLVMModule(\n-                      *ir_emitter_context.mlir_context(),\n+                      *ir_emitter_context.symbolic_expr_context(),\n                       ir_emitter_context.llvm_module()->getContext(),\n                       ir_emitter_context.gpu_device_info(), fusion, kernel_name,\n                       &ir_emitter_context.buffer_assignment()));\n@@ -325,15 +326,16 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n }\n \n absl::StatusOr<std::unique_ptr<llvm::Module>> EmitterBase::CreateLLVMModule(\n-    mlir::MLIRContext& mlir_context, llvm::LLVMContext& llvm_context,\n+    SymbolicExprContext& symbolic_expr_context, llvm::LLVMContext& llvm_context,\n     const se::DeviceDescription& device, const HloFusionInstruction& fusion,\n     const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n+  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   mlir_context.appendDialectRegistry(GetDialectRegistry());\n   mlir_context.loadAllAvailableDialects();\n-  TF_ASSIGN_OR_RETURN(\n-      auto module, CreateMLIRModule(mlir_context, fusion, entry_function_name,\n-                                    buffer_assignment));\n+  TF_ASSIGN_OR_RETURN(auto module,\n+                      CreateMLIRModule(symbolic_expr_context, fusion,\n+                                       entry_function_name, buffer_assignment));\n \n   mlir::PassManager pm(&mlir_context);\n   emitters::RegisterOptimizationPasses(pm);\n@@ -351,28 +353,30 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> EmitterBase::CreateLLVMModule(\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitterBase::CreateMLIRModule(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n-    const std::string& entry_function_name,\n+    SymbolicExprContext& symbolic_expr_context,\n+    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n-  mlir::OpBuilder builder(&context);\n+  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n+  mlir::OpBuilder builder(&mlir_context);\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(loc);\n \n   TF_ASSIGN_OR_RETURN(mlir::func::FuncOp entry_func,\n                       emitters::EmitKernelApi(\n                           *module, fusion, buffer_assignment,\n                           GetDefaultBufferAlignment(), entry_function_name));\n-  SetBackendKind(&context, entry_func, BackendKind::kGpu);\n+  SetBackendKind(&mlir_context, entry_func, BackendKind::kGpu);\n \n-  TF_RETURN_IF_ERROR(EmitMlir(module.get(), entry_func, fusion));\n+  TF_RETURN_IF_ERROR(\n+      EmitMlir(module.get(), entry_func, fusion, symbolic_expr_context));\n   return module;\n }\n \n emitters::EpilogueSpecification EmitterBase::GetEpilogueForOutputIndexing(\n     const HloFusionAnalysis& analysis,\n     const std::vector<const HloInstruction*>& heroes,\n     const std::vector<const HloInstruction*>& roots,\n-    mlir::MLIRContext* mlir_context) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n   emitters::EpilogueSpecification result;\n \n   absl::flat_hash_map<const HloInstruction*, const HloInstruction*>\n@@ -388,8 +392,8 @@ emitters::EpilogueSpecification EmitterBase::GetEpilogueForOutputIndexing(\n \n   result.root_indexing.reserve(roots.size());\n   for (auto* root : roots) {\n-    auto indexing =\n-        ComputeThreadIdToOutputIndexing(root_to_index[root], mlir_context);\n+    auto indexing = ComputeThreadIdToOutputIndexing(root_to_index[root],\n+                                                    symbolic_expr_context);\n     if (result.index_ranges.empty()) {\n       result.index_ranges.reserve(indexing->GetDimensionCount() +\n                                   indexing->GetSymbolCount());\n@@ -402,7 +406,8 @@ emitters::EpilogueSpecification EmitterBase::GetEpilogueForOutputIndexing(\n     }\n     auto* hero = root_to_hero[root];\n     auto epilogue_indexing = ComputeEpilogueInputToOutputIndexing(\n-        {*hero, &analysis.fusion()}, {*root, &analysis.fusion()}, mlir_context);\n+        {*hero, &analysis.fusion()}, {*root, &analysis.fusion()},\n+        symbolic_expr_context);\n     result.root_indexing.push_back(\n         ComposeIndexingMaps(*indexing, epilogue_indexing));\n   }\n@@ -429,12 +434,15 @@ mlir::DialectRegistry EmitterBase::GetDialectRegistry() {\n   return registry;\n }\n \n-absl::Status EmitterBase::EmitMlir(mlir::ModuleOp module, FuncOp entry_function,\n-                                   const HloFusionInstruction& fusion) const {\n+absl::Status EmitterBase::EmitMlir(\n+    mlir::ModuleOp module, FuncOp entry_function,\n+    const HloFusionInstruction& fusion,\n+    SymbolicExprContext& symbolic_expr_context) const {\n   std::vector<emitters::EpilogueSpecification> epilogues =\n-      GetEpilogues(fusion, module->getContext());\n+      GetEpilogues(fusion, &symbolic_expr_context);\n   emitters::PartitionedComputations computations(\n-      fusion.fused_instructions_computation(), module->getContext(), epilogues);\n+      fusion.fused_instructions_computation(), &symbolic_expr_context,\n+      epilogues);\n \n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              module, computations));"
        },
        {
            "sha": "9c3d2c499aeadad7da269884abe856c24fcdfc69",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.h",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,7 +30,6 @@ limitations under the License.\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n@@ -45,6 +44,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {\n@@ -59,15 +59,17 @@ class EmitterBase : public KernelFusionInterface {\n   // Visible for testing. `buffer_assignment` is optional for testing (assigns\n   // a different buffer to each tensor).\n   absl::StatusOr<std::unique_ptr<llvm::Module>> CreateLLVMModule(\n-      mlir::MLIRContext& mlir_context, llvm::LLVMContext& llvm_context,\n-      const se::DeviceDescription& device, const HloFusionInstruction& fusion,\n+      SymbolicExprContext& symbolic_expr_context,\n+      llvm::LLVMContext& llvm_context, const se::DeviceDescription& device,\n+      const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const;\n \n   // Visible for testing. `buffer_assignment` is optional for testing (assigns\n   // a different buffer to each tensor).\n   virtual absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+      SymbolicExprContext& symbolic_expr_context,\n+      const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const;\n \n@@ -79,7 +81,7 @@ class EmitterBase : public KernelFusionInterface {\n   // functions for these instructions.\n   virtual std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      mlir::MLIRContext* mlir_context) const {\n+      SymbolicExprContext* symbolic_expr_context) const {\n     return {};\n   }\n \n@@ -89,7 +91,7 @@ class EmitterBase : public KernelFusionInterface {\n       const HloFusionAnalysis& analysis,\n       const std::vector<const HloInstruction*>& heroes,\n       const std::vector<const HloInstruction*>& roots,\n-      mlir::MLIRContext* mlir_context) const;\n+      SymbolicExprContext* symbolic_expr_context) const;\n \n   virtual absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -110,7 +112,8 @@ class EmitterBase : public KernelFusionInterface {\n   // The fuson outputs may only be used with `tensor.insert` ops.a\n   absl::Status EmitMlir(mlir::ModuleOp module,\n                         mlir::func::FuncOp entry_function,\n-                        const HloFusionInstruction& fusion) const;\n+                        const HloFusionInstruction& fusion,\n+                        SymbolicExprContext& symbolic_expr_context) const;\n };\n \n // Adds passes that transform XLA_GPU and SCF loops, e.g. peel, pipeline,"
        },
        {
            "sha": "b5b42a0d29fbdb2371fdfcfee2286245bc23274a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -56,12 +57,12 @@ class DummyCopyEmitter : public EmitterBase {\n   LaunchDimensions launch_dimensions() const final { return {1, 100}; }\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t, mlir::MLIRContext*) const final {\n+      int64_t, SymbolicExprContext*) const final {\n     return std::nullopt;\n   }\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t, mlir::MLIRContext*) const final {\n+      int64_t, SymbolicExprContext*) const final {\n     return std::nullopt;\n   }\n \n@@ -86,11 +87,12 @@ class DummyCopyEmitter : public EmitterBase {\n class EmitterBaseTest : public HloHardwareIndependentTestBase {\n  protected:\n   EmitterBaseTest() {\n-    context_.appendDialectRegistry(EmitterBase::GetDialectRegistry());\n-    context_.loadAllAvailableDialects();\n+    mlir_context_.appendDialectRegistry(EmitterBase::GetDialectRegistry());\n+    mlir_context_.loadAllAvailableDialects();\n   }\n \n-  mlir::MLIRContext context_;\n+  mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   stream_executor::DeviceDescription device_info_ =\n       TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n };\n@@ -111,7 +113,7 @@ TEST_F(EmitterBaseTest, CreateMlirModule) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto mlir_module,\n       emitter.CreateMLIRModule(\n-          context_,\n+          symbolic_expr_context_,\n           *Cast<HloFusionInstruction>(\n               module->entry_computation()->root_instruction()),\n           \"fusion\",\n@@ -142,7 +144,7 @@ TEST_F(EmitterBaseTest, CreateLLVMModule) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto llvm_module,\n       emitter.CreateLLVMModule(\n-          context_, llvm_context, device_info_,\n+          symbolic_expr_context_, llvm_context, device_info_,\n           *Cast<HloFusionInstruction>(\n               module->entry_computation()->root_instruction()),\n           \"fusion\","
        },
        {
            "sha": "f9d4568488d51a5e230120fe804cd37a2de80ab1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/emitters/dynamic_update_slice_kernel_emitter.h\"\n@@ -39,6 +38,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -59,7 +59,7 @@ LaunchDimensions InPlaceDynamicUpdateSliceFusion::launch_dimensions() const {\n \n std::optional<std::vector<IndexingMap>>\n InPlaceDynamicUpdateSliceFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, mlir::MLIRContext* indexing_context) const {\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   // TODO(b/331355203): Implement thread ID -> operand indexing.\n   std::vector<IndexingMap> result(\n       analysis_.fusion_hero(root_index).GetOperands().size(),\n@@ -69,20 +69,21 @@ InPlaceDynamicUpdateSliceFusion::ComputeThreadIdToInputIndexing(\n   result[kDUSUpdateIndex] = KernelEmitter::ComputeWorkItemIdToOutputIndexing(\n       GetWorkDimensions(),\n       KernelEmitter::GetIndexingShape(analysis_.fusion_spec()),\n-      indexing_context);\n+      symbolic_expr_context);\n   return result;\n }\n \n std::vector<emitters::EpilogueSpecification>\n InPlaceDynamicUpdateSliceFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion, mlir::MLIRContext* mlir_context) const {\n+    const HloFusionInstruction& fusion,\n+    SymbolicExprContext* symbolic_expr_context) const {\n   // We don't actually support epilogues for DUS, but this is how we tell\n   // the base class that we don't want it to generate code for the DUS.\n   std::vector<emitters::EpilogueSpecification> epilogues;\n   for (const auto& [dus_op, root] :\n        llvm::zip(dus_ops_, analysis_.fusion_roots())) {\n     epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-        &dus_op.instruction(), &root.instruction(), mlir_context));\n+        &dus_op.instruction(), &root.instruction(), symbolic_expr_context));\n   }\n   return epilogues;\n }\n@@ -95,11 +96,11 @@ WorkDimensions InPlaceDynamicUpdateSliceFusion::GetWorkDimensions() const {\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>>\n InPlaceDynamicUpdateSliceFusion::CreateMLIRModule(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n-    const std::string& entry_function_name,\n+    SymbolicExprContext& symbolic_expr_context,\n+    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n   emitters::DynamicUpdateSliceKernelEmitter emitter(\n-      context, fusion, analysis_.fusion_spec(), buffer_assignment,\n+      symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,\n       BackendKind::kGpu);\n "
        },
        {
            "sha": "02e8df8d0d3b262981e997043803f19902aa7756",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.h",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n@@ -33,6 +32,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace gpu {\n@@ -56,18 +56,21 @@ class InPlaceDynamicUpdateSliceFusion : public EmitterBase {\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* indexing_context) const override {\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override {\n     // The mapping cannot be statically computed in general, since the offsets\n     // are unknown.\n     return std::nullopt;\n   }\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, mlir::MLIRContext* indexing_context) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+      SymbolicExprContext& symbolic_expr_context,\n+      const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const override;\n \n@@ -79,7 +82,7 @@ class InPlaceDynamicUpdateSliceFusion : public EmitterBase {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      mlir::MLIRContext* mlir_context) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   WorkDimensions GetWorkDimensions() const;\n "
        },
        {
            "sha": "b2ebabc0ab1fe966925f02a2a34cb8e53e8d080a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n@@ -57,24 +56,25 @@ const Shape& GetIndexShape(const Shape& shape) {\n }  // namespace\n \n std::optional<IndexingMap> LoopFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, mlir::MLIRContext* ctx) const {\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   return emitters::LoopFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n       GetWorkDimensions(),\n-      GetIndexShape(analysis_.fusion_root(root_index).shape()), ctx);\n+      GetIndexShape(analysis_.fusion_root(root_index).shape()),\n+      symbolic_expr_context);\n }\n \n std::optional<std::vector<IndexingMap>>\n-LoopFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n-                                           mlir::MLIRContext* ctx) const {\n+LoopFusion::ComputeThreadIdToInputIndexing(\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   std::optional<IndexingMap> thread_id_to_output_indexing =\n-      ComputeThreadIdToOutputIndexing(root_index, ctx);\n+      ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n   if (!thread_id_to_output_indexing.has_value()) {\n     return std::nullopt;\n   }\n   const HloInstruction* fusion_root =\n       &analysis_.fusion_root(root_index).instruction();\n-  auto output_to_input_indexing =\n-      ComputeOutputToInputIndexing(fusion_root, /*output_id=*/0, ctx);\n+  auto output_to_input_indexing = ComputeOutputToInputIndexing(\n+      fusion_root, /*output_id=*/0, symbolic_expr_context);\n   std::vector<IndexingMap> result;\n   result.reserve(fusion_root->operand_count());\n   for (int64_t operand_index = 0; operand_index < fusion_root->operand_count();\n@@ -107,11 +107,11 @@ WorkDimensions LoopFusion::GetWorkDimensions() const {\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> LoopFusion::CreateMLIRModule(\n-    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n-    const std::string& entry_function_name,\n+    SymbolicExprContext& symbolic_expr_context,\n+    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n   emitters::LoopFusionKernelEmitter emitter(\n-      context, fusion, analysis_.fusion_spec(), buffer_assignment,\n+      symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,\n       BackendKind::kGpu);\n "
        },
        {
            "sha": "3fc76b9b92c0532904d756340820e0929b0f57fd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.h",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,34 +22,37 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace gpu {\n \n // Generic loop fusion. Lowers to LLVM via MLIR.\n class LoopFusion final : public EmitterBase {\n  public:\n-  LoopFusion(const HloFusionAnalysis& analysis, mlir::MLIRContext* ctx)\n+  LoopFusion(const HloFusionAnalysis& analysis,\n+             gpu::SymbolicExprContext* symbolic_expr_context)\n       : analysis_(analysis), config_(ComputeLoopFusionConfig(analysis)) {}\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n  private:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+      SymbolicExprContext& context, const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const override;\n "
        },
        {
            "sha": "5c79f3f19fb54630a522654c667cee8b3f0cb6bb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.cc",
            "status": "modified",
            "additions": 152,
            "deletions": 110,
            "changes": 262,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -44,7 +44,6 @@ limitations under the License.\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/TypeRange.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n@@ -64,6 +63,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/reduction_utils.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/shape.h\"\n@@ -100,15 +100,17 @@ struct ReductionFusion::EmitterState {\n   EmitterState(const ReductionFusion& owner, mlir::func::FuncOp entry_function,\n                const HloFusionInstruction& fusion,\n                const PartitionedComputations& computations,\n-               const emitters::CallTargetProvider& call_target)\n+               const emitters::CallTargetProvider& call_target,\n+               SymbolicExprContext* symbolic_expr_context)\n       : owner(owner),\n         entry_function(entry_function),\n         fusion(fusion),\n         computations(computations),\n         call_target(call_target),\n         builder(entry_function.getLoc(), entry_function),\n         computation(computations.FindPartitionedComputation(\n-            fusion.fused_instructions_computation())) {\n+            fusion.fused_instructions_computation())),\n+        symbolic_expr_context(symbolic_expr_context) {\n     int output_index = 0;\n     for (const auto& [root_index, root] :\n          llvm::enumerate(owner.analysis_.fusion_roots())) {\n@@ -174,12 +176,13 @@ struct ReductionFusion::EmitterState {\n   absl::flat_hash_map<const HloInstruction*, int> fusion_result_index_starts;\n   absl::flat_hash_map<const HloInstruction*, int> root_indices;\n   SmallVector<Value> thread_and_block_ids;\n+  SymbolicExprContext* symbolic_expr_context;\n };\n \n PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n     int group_id, const HloValueMap& inits, const SmallVector<Value>& outputs) {\n   auto tile_indexing =\n-      owner.ComputeReductionInputIndexing(builder.getContext());\n+      owner.ComputeReductionInputIndexing(symbolic_expr_context);\n   tile_indexing\n       .GetMutableDimensionBound(\n           KernelFusionInterface::kIndexingMapBlockIdxDims[1])\n@@ -207,7 +210,7 @@ PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n       SmallVector<Value> reduce_args = iter_args.slice(start, arity);\n       auto indices = emitters::ApplyIndexing(\n           GetBitcastMap(owner.input_shape_, reduction->operand(0)->shape(),\n-                        nested_b.getContext()),\n+                        symbolic_expr_context),\n           map_results, {}, nested_b);\n       reduce_args.append(ProvideParameterRange(computation, reduction, 0, arity,\n                                                indices, call_target,\n@@ -232,7 +235,7 @@ PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n     for (auto* side_output : side_outputs) {\n       auto indices = emitters::ApplyIndexing(\n           GetBitcastMap(owner.input_shape_, side_output->shape(),\n-                        builder.getContext()),\n+                        symbolic_expr_context),\n           map_results, {}, builder);\n       auto* root_tuple = fusion.fused_expression_root();\n       Value value = emitters::ProvideParameter(\n@@ -268,7 +271,7 @@ SmallVector<Value> ReductionFusion::EmitterState::WriteToSharedMemory(\n     absl::Span<const HloInstruction* const> reductions,\n     const HloValueMap& values, std::optional<int> padding) {\n   SmallVector<int64_t> shape;\n-  auto map = owner.GetSharedMemoryWriteMap(builder.getContext());\n+  auto map = owner.GetSharedMemoryWriteMap(symbolic_expr_context);\n   for (auto result : map.GetAffineMap().getResults()) {\n     shape.push_back(\n         map.GetRangeEvaluator().ComputeExpressionRange(result).upper + 1);\n@@ -336,7 +339,7 @@ mlir::ValueRange ReductionFusion::EmitterState::ReduceViaSharedMemory(\n     std::optional<int> padding, int max_dist) {\n   const auto& reductions = owner.reduction_heroes_[group_id];\n   auto read_indexing =\n-      owner.GetSharedMemoryReductionReadMap(builder.getContext());\n+      owner.GetSharedMemoryReductionReadMap(symbolic_expr_context);\n   auto loop_indexing = read_indexing;\n   // All threads must participate in the shuffle, so we clear the constraints\n   // for the iteration. Otherwise, some threads might not be part of the loop,\n@@ -377,8 +380,9 @@ mlir::ValueRange ReductionFusion::EmitterState::ReduceViaSharedMemory(\n       });\n }\n \n-ReductionFusion::ReductionFusion(const HloFusionAnalysis& analysis)\n-    : analysis_(analysis) {\n+ReductionFusion::ReductionFusion(const HloFusionAnalysis& analysis,\n+                                 SymbolicExprContext* symbolic_expr_context)\n+    : analysis_(analysis), symbolic_expr_context_(symbolic_expr_context) {\n   auto* hero_reduction = analysis.FindHeroReduction();\n   CHECK_NE(hero_reduction, nullptr);\n   reduction_dimensions_ =\n@@ -418,13 +422,14 @@ ReductionFusion::ReductionFusion(const HloFusionAnalysis& analysis)\n IndexingMap ReductionFusion::GetIndexingMap(\n     llvm::ArrayRef<mlir::AffineExpr> results,\n     absl::Span<int64_t const> symbol_sizes) const {\n-  auto* ctx = results.front().getContext();\n+  auto* mlir_context = results.front().getContext();\n   auto num_groups = static_cast<int64_t>(reduction_heroes_.size());\n-  return IndexingMap{AffineMap::get(6, symbol_sizes.size(), results, ctx),\n-                     DimVarsFromGPUGrid({Product(num_threads_), 1, 1,\n-                                         Product(num_blocks_), num_groups, 1}),\n-                     RangeVarsFromTensorSizes(symbol_sizes),\n-                     /*rt_vars=*/{}};\n+  return IndexingMap{\n+      AffineMap::get(6, symbol_sizes.size(), results, mlir_context),\n+      DimVarsFromGPUGrid(\n+          {Product(num_threads_), 1, 1, Product(num_blocks_), num_groups, 1}),\n+      RangeVarsFromTensorSizes(symbol_sizes),\n+      /*rt_vars=*/{}};\n }\n \n IndexingMap ReductionFusion::GetThreadIndexingMap(\n@@ -451,20 +456,21 @@ LaunchDimensions ReductionFusion::launch_dimensions() const {\n }\n \n std::vector<emitters::EpilogueSpecification> ReductionFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n+    const HloFusionInstruction& fusion,\n+    SymbolicExprContext* symbolic_expr_context) const {\n   std::vector<emitters::EpilogueSpecification> epilogues;\n   epilogues.reserve(reduction_heroes_.size());\n   for (const auto& [heroes, roots] :\n        llvm::zip(reduction_heroes_, reduction_roots_)) {\n-    epilogues.push_back(\n-        GetEpilogueForOutputIndexing(analysis_, heroes, roots, mlir_context));\n+    epilogues.push_back(GetEpilogueForOutputIndexing(analysis_, heroes, roots,\n+                                                     symbolic_expr_context));\n   }\n   // Add empty epilogues for the side outputs. This ensures their roots don't\n   // get \"fused\" into the tuple function.\n   for (const auto& roots : side_output_roots_) {\n     for (const auto* root : roots) {\n       epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-          root, root, mlir_context));\n+          root, root, symbolic_expr_context));\n     }\n   }\n   return epilogues;\n@@ -475,7 +481,8 @@ absl::Status ReductionFusion::EmitEntryFunction(\n     const emitters::CallTargetProvider& call_targets,\n     mlir::func::FuncOp entry_function,\n     const HloFusionInstruction& fusion) const {\n-  EmitterState state{*this, entry_function, fusion, computations, call_targets};\n+  EmitterState state{*this,        entry_function, fusion,\n+                     computations, call_targets,   symbolic_expr_context_};\n   auto& b = state.builder;\n   b.setInsertionPointToStart(entry_function.addEntryBlock());\n   state.thread_and_block_ids = EmitThreadAndBlockIds(b);\n@@ -508,14 +515,14 @@ HloValueMap ReductionFusion::GetInits(int group_id, EmitterState& state) const {\n }\n \n std::optional<std::vector<IndexingMap>>\n-ReductionFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n-                                                MLIRContext* ctx) const {\n+ReductionFusion::ComputeThreadIdToInputIndexing(\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   std::vector<IndexingMap> result(hero.operand_count(),\n                                   IndexingMap::GetUndefined());\n   if (!groups_.is_reduction_root[root_index]) {\n     auto thread_id_to_output_indexing =\n-        ComputeThreadIdToOutputIndexing(root_index, ctx);\n+        ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n     if (!thread_id_to_output_indexing.has_value()) {\n       return std::nullopt;\n     }\n@@ -524,7 +531,8 @@ ReductionFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n       result[operand_index] = ComposeIndexingMaps(\n           *thread_id_to_output_indexing,\n           ComputeOutputToInputIndexing(\n-              &analysis_.fusion_root(root_index).instruction(), 0, ctx)\n+              &analysis_.fusion_root(root_index).instruction(), 0,\n+              symbolic_expr_context)\n               .indexing_maps[operand_index]\n               .begin()\n               ->map());\n@@ -535,29 +543,31 @@ ReductionFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n   // We don't have indexing for the init values.\n   for (int64_t operand_index = 0; operand_index < hero.operand_count() / 2;\n        ++operand_index) {\n-    auto projected_map = ComputeReductionInputIndexing(ctx);\n+    auto projected_map = ComputeReductionInputIndexing(symbolic_expr_context);\n     AddGroupIdConstraint(projected_map, root_index, groups_);\n     result[operand_index] =\n-        projected_map *\n-        GetBitcastMap(input_shape_, hero.operand(operand_index)->shape(), ctx);\n+        projected_map * GetBitcastMap(input_shape_,\n+                                      hero.operand(operand_index)->shape(),\n+                                      symbolic_expr_context);\n     result[operand_index].Simplify();\n   }\n   return result;\n }\n \n std::optional<IndexingMap> ReductionFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, MLIRContext* ctx) const {\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   if (!groups_.is_reduction_root[root_index]) {\n     auto map = ComposeIndexingMaps(\n-        ComputeReductionInputIndexing(ctx),\n+        ComputeReductionInputIndexing(symbolic_expr_context),\n         GetBitcastMap(input_shape_, analysis_.fusion_root(root_index).shape(),\n-                      ctx));\n+                      symbolic_expr_context));\n     AddGroupIdConstraint(map, root_index, groups_);\n     map.Simplify();\n     return map;\n   }\n \n-  auto projected_indexing = ComputeReductionOutputIndexing(ctx);\n+  auto projected_indexing =\n+      ComputeReductionOutputIndexing(symbolic_expr_context);\n   auto output_shape = reduction_dimensions_.GetOutputShape();\n   CHECK_EQ(output_shape.size(),\n            projected_indexing.GetAffineMap().getNumResults());\n@@ -570,8 +580,8 @@ std::optional<IndexingMap> ReductionFusion::ComputeThreadIdToOutputIndexing(\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   auto physical_shape =\n       ShapeUtil::DeleteDimensions(hero.dimensions(), hero.operand(0)->shape());\n-  auto map =\n-      projected_indexing * GetBitcastMap(output_shape, physical_shape, ctx);\n+  auto map = projected_indexing *\n+             GetBitcastMap(output_shape, physical_shape, symbolic_expr_context);\n   map.Simplify();\n   return map;\n }\n@@ -589,9 +599,10 @@ SmallVector<Value> ReductionFusion::EvaluateEpilogue(\n   auto values = EmitEpilogue(group_id, state.computations, state.entry_function,\n                              results, epilogue_input_indices, b);\n   int first_root_index = state.root_indices[epilogue.roots.front()];\n-  auto thread_has_output = emitters::CheckConstraints(\n-      *ComputeThreadIdToOutputIndexing(first_root_index, b.getContext()),\n-      state.thread_and_block_ids, symbol_values, b);\n+  auto thread_has_output =\n+      emitters::CheckConstraints(*ComputeThreadIdToOutputIndexing(\n+                                     first_root_index, symbolic_expr_context_),\n+                                 state.thread_and_block_ids, symbol_values, b);\n   for (auto [index, root] : llvm::enumerate(epilogue.roots)) {\n     auto output_indices =\n         emitters::ApplyIndexing(epilogue.root_indexing[index],\n@@ -605,8 +616,10 @@ SmallVector<Value> ReductionFusion::EvaluateEpilogue(\n   return outputs;\n }\n \n-ColumnReductionFusion::ColumnReductionFusion(const HloFusionAnalysis& analysis)\n-    : ReductionFusion(analysis) {\n+ColumnReductionFusion::ColumnReductionFusion(\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context)\n+    : ReductionFusion(analysis, symbolic_expr_context) {\n   CHECK(!reduction_dimensions_.is_row_reduction);\n \n   input_shape_ = {reduction_dimensions_.dimensions[0],\n@@ -634,12 +647,13 @@ ColumnReductionFusion::ColumnReductionFusion(const HloFusionAnalysis& analysis)\n }\n \n IndexingMap ColumnReductionFusion::ComputeReductionOutputIndexing(\n-    MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto thread_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(0, ctx), num_threads_);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(3, ctx), num_blocks_);\n-  auto vector_index = getAffineSymbolExpr(0, ctx);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(3, mlir_context), num_blocks_);\n+  auto vector_index = getAffineSymbolExpr(0, mlir_context);\n   SmallVector<AffineExpr, 2> results{\n       block_id[0],\n       (block_id[1] * kTileSize + thread_id[0]) * vector_size_ + vector_index};\n@@ -650,13 +664,14 @@ IndexingMap ColumnReductionFusion::ComputeReductionOutputIndexing(\n }\n \n IndexingMap ColumnReductionFusion::ComputeReductionInputIndexing(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto thread_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(0, ctx), num_threads_);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(3, ctx), num_blocks_);\n-  AffineExpr element_index = getAffineSymbolExpr(0, ctx);\n-  AffineExpr vector_index = getAffineSymbolExpr(1, ctx);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(3, mlir_context), num_blocks_);\n+  AffineExpr element_index = getAffineSymbolExpr(0, mlir_context);\n+  AffineExpr vector_index = getAffineSymbolExpr(1, mlir_context);\n \n   SmallVector<AffineExpr, 3> results{\n       block_id[0], thread_id[0] + element_index * num_threads_[1],\n@@ -670,20 +685,22 @@ IndexingMap ColumnReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap ColumnReductionFusion::GetSharedMemoryReductionReadMap(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto thread_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(0, ctx), num_threads_);\n-  auto vector_index = getAffineSymbolExpr(0, ctx);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n+  auto vector_index = getAffineSymbolExpr(0, mlir_context);\n   return GetThreadIndexingMap(\n       {thread_id[0], thread_id[1] * vector_size_ + vector_index}, {},\n       /*symbol_sizes=*/{vector_size_});\n }\n \n IndexingMap ColumnReductionFusion::GetSharedMemoryWriteMap(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto thread_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(0, ctx), num_threads_);\n-  auto vector_index = getAffineSymbolExpr(0, ctx);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n+  auto vector_index = getAffineSymbolExpr(0, mlir_context);\n   return GetThreadIndexingMap(\n       {thread_id[1], thread_id[0] * vector_size_ + vector_index}, {},\n       /*symbol_sizes=*/{vector_size_});\n@@ -699,8 +716,9 @@ llvm::SmallVector<mlir::Value> ColumnReductionFusion::EmitReduction(\n }\n \n SmallColumnReductionFusion::SmallColumnReductionFusion(\n-    const HloFusionAnalysis& analysis)\n-    : ReductionFusion(analysis) {\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context)\n+    : ReductionFusion(analysis, symbolic_expr_context) {\n   CHECK(!reduction_dimensions_.is_row_reduction);\n \n   input_shape_ = {reduction_dimensions_.dimensions[0],\n@@ -731,10 +749,11 @@ SmallColumnReductionFusion::SmallColumnReductionFusion(\n }\n \n IndexingMap SmallColumnReductionFusion::ComputeReductionOutputIndexing(\n-    MLIRContext* ctx) const {\n-  auto thread_id = getAffineDimExpr(0, ctx);\n-  auto block_id = getAffineDimExpr(3, ctx);\n-  auto vector_index = getAffineSymbolExpr(0, ctx);\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto thread_id = getAffineDimExpr(0, mlir_context);\n+  auto block_id = getAffineDimExpr(3, mlir_context);\n+  auto vector_index = getAffineSymbolExpr(0, mlir_context);\n   SmallVector<AffineExpr, 2> results{\n       block_id,\n       (thread_id + vector_index * num_threads_[0]).floorDiv(shared_rows_)};\n@@ -745,11 +764,12 @@ IndexingMap SmallColumnReductionFusion::ComputeReductionOutputIndexing(\n }\n \n IndexingMap SmallColumnReductionFusion::ComputeReductionInputIndexing(\n-    mlir::MLIRContext* ctx) const {\n-  auto thread_id = getAffineDimExpr(0, ctx);\n-  auto block_id = getAffineDimExpr(3, ctx);\n-  AffineExpr loop_index = getAffineSymbolExpr(0, ctx);\n-  AffineExpr vector_index = getAffineSymbolExpr(1, ctx);\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto thread_id = getAffineDimExpr(0, mlir_context);\n+  auto block_id = getAffineDimExpr(3, mlir_context);\n+  AffineExpr loop_index = getAffineSymbolExpr(0, mlir_context);\n+  AffineExpr vector_index = getAffineSymbolExpr(1, mlir_context);\n \n   AffineExpr linear_index = thread_id * vector_size_ + vector_index +\n                             loop_index * (vector_size_ * num_threads_[0]);\n@@ -758,7 +778,7 @@ IndexingMap SmallColumnReductionFusion::ComputeReductionInputIndexing(\n       GetBitcastMap({num_blocks_[0], input_shape_[1] * input_shape_[2]},\n                     ShapeUtil::MakeShapeWithDescendingLayout(PrimitiveType::U8,\n                                                              input_shape_),\n-                    ctx);\n+                    symbolic_expr_context);\n \n   for (auto [result, dim_size] :\n        llvm::zip(map.GetAffineMap().getResults(), input_shape_)) {\n@@ -768,18 +788,22 @@ IndexingMap SmallColumnReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap SmallColumnReductionFusion::GetSharedMemoryReductionReadMap(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto indices = DelinearizeInBoundsIndex(\n-      getAffineDimExpr(0, ctx) + getAffineSymbolExpr(0, ctx) * num_threads_[0],\n+      getAffineDimExpr(0, mlir_context) +\n+          getAffineSymbolExpr(0, mlir_context) * num_threads_[0],\n       {input_shape_[2], shared_rows_});\n   return GetThreadIndexingMap({indices[1], indices[0]}, {},\n                               /*symbol_sizes=*/{vector_size_});\n }\n \n IndexingMap SmallColumnReductionFusion::GetSharedMemoryWriteMap(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto indices = DelinearizeInBoundsIndex(\n-      getAffineDimExpr(0, ctx) * vector_size_ + getAffineSymbolExpr(0, ctx),\n+      getAffineDimExpr(0, mlir_context) * vector_size_ +\n+          getAffineSymbolExpr(0, mlir_context),\n       {shared_rows_, input_shape_[2]});\n   return GetThreadIndexingMap(indices, {},\n                               /*symbol_sizes=*/{vector_size_});\n@@ -801,8 +825,10 @@ llvm::SmallVector<mlir::Value> SmallColumnReductionFusion::EmitReduction(\n                                      shared_rows_ / 2);\n }\n \n-RowReductionFusion::RowReductionFusion(const HloFusionAnalysis& analysis)\n-    : ReductionFusion(analysis) {\n+RowReductionFusion::RowReductionFusion(\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context)\n+    : ReductionFusion(analysis, symbolic_expr_context) {\n   CHECK(reduction_dimensions_.is_row_reduction);\n   Vector3 shape = reduction_dimensions_.dimensions;\n   int64_t kMinorReducedElementsPerThread = 8;\n@@ -876,14 +902,15 @@ RowReductionFusion::RowReductionFusion(const HloFusionAnalysis& analysis)\n }\n \n IndexingMap RowReductionFusion::ComputeReductionInputIndexing(\n-    mlir::MLIRContext* ctx) const {\n-  auto thread_id =\n-      DelinearizeInBoundsIndex(mlir::getAffineDimExpr(0, ctx), num_threads_);\n-  auto block_id =\n-      DelinearizeInBoundsIndex(mlir::getAffineDimExpr(3, ctx), num_blocks_);\n-  auto major_reduced = getAffineSymbolExpr(0, ctx);\n-  auto minor_reduced = getAffineSymbolExpr(1, ctx);\n-  auto vector_index = getAffineSymbolExpr(2, ctx);\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto thread_id = DelinearizeInBoundsIndex(\n+      mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n+  auto block_id = DelinearizeInBoundsIndex(\n+      mlir::getAffineDimExpr(3, mlir_context), num_blocks_);\n+  auto major_reduced = getAffineSymbolExpr(0, mlir_context);\n+  auto minor_reduced = getAffineSymbolExpr(1, mlir_context);\n+  auto vector_index = getAffineSymbolExpr(2, mlir_context);\n \n   SmallVector<AffineExpr> indices{\n       major_reduced,\n@@ -901,11 +928,12 @@ IndexingMap RowReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap RowReductionFusion::ComputeReductionOutputIndexing(\n-    MLIRContext* ctx) const {\n-  auto thread_id =\n-      DelinearizeInBoundsIndex(mlir::getAffineDimExpr(0, ctx), num_threads_);\n-  auto block_id =\n-      DelinearizeInBoundsIndex(mlir::getAffineDimExpr(3, ctx), num_blocks_);\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto thread_id = DelinearizeInBoundsIndex(\n+      mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n+  auto block_id = DelinearizeInBoundsIndex(\n+      mlir::getAffineDimExpr(3, mlir_context), num_blocks_);\n   IndexingMap projected_index =\n       GetIndexingMap(block_id[0] * tile_sizes_per_block_[0] + thread_id[0]);\n   projected_index.AddConstraint(thread_id[1], {0, 0});\n@@ -917,18 +945,20 @@ int RowReductionFusion::GetWarpsPerRow() const {\n }\n \n IndexingMap RowReductionFusion::GetSharedMemoryReductionReadMap(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto thread_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(0, ctx), num_threads_);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto lane_id = thread_id[1] % WarpSize();\n   return GetThreadIndexingMap({thread_id[0], lane_id},\n                               {{thread_id[1], {0, GetWarpsPerRow() - 1}}});\n }\n \n IndexingMap RowReductionFusion::GetSharedMemoryWriteMap(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto thread_id =\n-      DelinearizeInBoundsIndex(getAffineDimExpr(0, ctx), num_threads_);\n+      DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   // The reduced dimension is tiled; each warp writes one element to shared\n   // memory (from lane 0).\n   auto lane_id = thread_id[1] % WarpSize();\n@@ -958,8 +988,9 @@ llvm::SmallVector<mlir::Value> RowReductionFusion::EmitReduction(\n }\n \n MultiRowReductionFusion::MultiRowReductionFusion(\n-    const HloFusionAnalysis& analysis, int vector_size)\n-    : ReductionFusion(analysis) {\n+    const HloFusionAnalysis& analysis, int vector_size,\n+    SymbolicExprContext* symbolic_expr_context)\n+    : ReductionFusion(analysis, symbolic_expr_context) {\n   CHECK(reduction_dimensions_.is_row_reduction);\n   Vector3 shape = reduction_dimensions_.dimensions;\n   input_shape_ = {shape[0], shape[1], shape[2]};\n@@ -969,7 +1000,8 @@ MultiRowReductionFusion::MultiRowReductionFusion(\n }\n \n std::unique_ptr<ReductionFusion> MultiRowReductionFusion::TryCreate(\n-    const HloFusionAnalysis& analysis) {\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto* hero_reduction = analysis.FindHeroReduction();\n   CHECK_NE(hero_reduction, nullptr);\n   auto reduction_dimensions =\n@@ -1050,7 +1082,8 @@ std::unique_ptr<ReductionFusion> MultiRowReductionFusion::TryCreate(\n \n   VLOG(3) << \"MultiRowReductionFusion::TryCreate selected vector_size = \"\n           << vector_size;\n-  return std::make_unique<MultiRowReductionFusion>(analysis, vector_size);\n+  return std::make_unique<MultiRowReductionFusion>(analysis, vector_size,\n+                                                   symbolic_expr_context);\n }\n \n absl::InlinedVector<int64_t, 4> MultiRowReductionFusion::GetNumThreads(\n@@ -1080,13 +1113,15 @@ int64_t MultiRowReductionFusion::GetNumBlocks(\n }\n \n IndexingMap MultiRowReductionFusion::ComputeReductionInputIndexing(\n-    mlir::MLIRContext* ctx) const {\n-  auto thread_id =\n-      DelinearizeInBoundsIndex(mlir::getAffineDimExpr(0, ctx), num_threads_);\n-  auto block_id = num_blocks_.front() == 1 ? mlir::getAffineConstantExpr(0, ctx)\n-                                           : mlir::getAffineDimExpr(3, ctx);\n-  auto major_reduced = getAffineSymbolExpr(0, ctx);\n-  auto vector_index = getAffineSymbolExpr(1, ctx);\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto thread_id = DelinearizeInBoundsIndex(\n+      mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n+  auto block_id = num_blocks_.front() == 1\n+                      ? mlir::getAffineConstantExpr(0, mlir_context)\n+                      : mlir::getAffineDimExpr(3, mlir_context);\n+  auto major_reduced = getAffineSymbolExpr(0, mlir_context);\n+  auto vector_index = getAffineSymbolExpr(1, mlir_context);\n \n   SmallVector<AffineExpr> indices{\n       major_reduced, block_id * num_threads_[0] + thread_id[0],\n@@ -1100,11 +1135,13 @@ IndexingMap MultiRowReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap MultiRowReductionFusion::ComputeReductionOutputIndexing(\n-    MLIRContext* ctx) const {\n-  auto thread_id =\n-      DelinearizeInBoundsIndex(mlir::getAffineDimExpr(0, ctx), num_threads_);\n-  auto block_id = num_blocks_.front() == 1 ? mlir::getAffineConstantExpr(0, ctx)\n-                                           : mlir::getAffineDimExpr(3, ctx);\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto thread_id = DelinearizeInBoundsIndex(\n+      mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n+  auto block_id = num_blocks_.front() == 1\n+                      ? mlir::getAffineConstantExpr(0, mlir_context)\n+                      : mlir::getAffineDimExpr(3, mlir_context);\n   IndexingMap projected_index =\n       GetIndexingMap(block_id * num_threads_[0] + thread_id[0]);\n   projected_index.AddConstraint(thread_id[1] % num_threads_[1], {0, 0});\n@@ -1127,24 +1164,29 @@ llvm::SmallVector<mlir::Value> MultiRowReductionFusion::EmitReduction(\n }\n \n std::unique_ptr<ReductionFusion> CreateReductionFusion(\n-    const HloFusionAnalysis& analysis) {\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto* hero_reduction = analysis.FindHeroReduction();\n   CHECK_NE(hero_reduction, nullptr);\n   ReductionDimensions reduction_dimensions =\n       GetReductionKindAndContiguousComponents(*hero_reduction);\n   if (reduction_dimensions.is_row_reduction) {\n-    auto multi_row_emitter = MultiRowReductionFusion::TryCreate(analysis);\n+    auto multi_row_emitter =\n+        MultiRowReductionFusion::TryCreate(analysis, symbolic_expr_context);\n     if (multi_row_emitter != nullptr) {\n       return multi_row_emitter;\n     }\n-    return std::make_unique<RowReductionFusion>(analysis);\n+    return std::make_unique<RowReductionFusion>(analysis,\n+                                                symbolic_expr_context);\n   }\n \n   const int64_t warp_size = analysis.device_info().threads_per_warp();\n   if (warp_size % reduction_dimensions.dimensions[kColMinorKept] == 0) {\n-    return std::make_unique<SmallColumnReductionFusion>(analysis);\n+    return std::make_unique<SmallColumnReductionFusion>(analysis,\n+                                                        symbolic_expr_context);\n   }\n-  return std::make_unique<ColumnReductionFusion>(analysis);\n+  return std::make_unique<ColumnReductionFusion>(analysis,\n+                                                 symbolic_expr_context);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "aeb238ae23193be9083bbbe4cb1672b9bc44774a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.h",
            "status": "modified",
            "additions": 44,
            "deletions": 29,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h\"\n #include \"mlir/IR/AffineExpr.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n@@ -39,6 +38,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/reduction_utils.h\"\n #include \"xla/shape.h\"\n \n@@ -53,13 +53,16 @@ using HloValueMap =\n // reductions.\n class ReductionFusion : public EmitterBase {\n  public:\n-  explicit ReductionFusion(const HloFusionAnalysis& analysis);\n+  explicit ReductionFusion(const HloFusionAnalysis& analysis,\n+                           SymbolicExprContext* symbolic_expr_context);\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   LaunchDimensions launch_dimensions() const override;\n \n@@ -80,7 +83,7 @@ class ReductionFusion : public EmitterBase {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      mlir::MLIRContext* mlir_context) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   llvm::SmallVector<mlir::Value> EvaluateEpilogue(\n       const HloValueMap& results, llvm::SmallVector<mlir::Value> outputs,\n@@ -101,22 +104,23 @@ class ReductionFusion : public EmitterBase {\n   // Returns the input indexing. The inputs are given in the projected shape\n   // (i.e., the indexing map has three results).\n   virtual IndexingMap ComputeReductionInputIndexing(\n-      mlir::MLIRContext* ctx) const = 0;\n+      SymbolicExprContext* symbolic_expr_context) const = 0;\n   // Returns the output indexing. The outputs are given in the  projected\n   // reduced shape (i.e., one or two results, depending on the reduction type).\n   virtual IndexingMap ComputeReductionOutputIndexing(\n-      mlir::MLIRContext* ctx) const = 0;\n+      SymbolicExprContext* symbolic_expr_context) const = 0;\n \n   // Returns the (thread ID, vector index) -> (shared index...) map for the\n   // shared memory reduction.\n   virtual IndexingMap GetSharedMemoryReductionReadMap(\n-      mlir::MLIRContext* ctx) const {\n+      SymbolicExprContext* symbolic_expr_context) const {\n     return IndexingMap::GetUndefined();\n   }\n \n   // Returns the (thread ID, vector index) -> (shared index...) map for the\n   // write to shared memory.\n-  virtual IndexingMap GetSharedMemoryWriteMap(mlir::MLIRContext* ctx) const {\n+  virtual IndexingMap GetSharedMemoryWriteMap(\n+      SymbolicExprContext* symbolic_expr_context) const {\n     return IndexingMap::GetUndefined();\n   }\n \n@@ -131,6 +135,7 @@ class ReductionFusion : public EmitterBase {\n   // The side output roots for each reduction group.\n   std::vector<std::vector<const HloInstruction*>> side_output_roots_;\n   const HloFusionAnalysis& analysis_;\n+  SymbolicExprContext* symbolic_expr_context_;\n \n   // The number of elements in each dimension.\n   absl::InlinedVector<int64_t, 4> input_shape_;\n@@ -149,32 +154,36 @@ class ReductionFusion : public EmitterBase {\n \n class RowReductionFusion : public ReductionFusion {\n  public:\n-  explicit RowReductionFusion(const HloFusionAnalysis& analysis);\n+  explicit RowReductionFusion(const HloFusionAnalysis& analysis,\n+                              SymbolicExprContext* symbolic_expr_context);\n \n  protected:\n   // The number of warps working on one output element.\n   int GetWarpsPerRow() const;\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n   IndexingMap GetSharedMemoryReductionReadMap(\n-      mlir::MLIRContext* ctx) const override;\n-  IndexingMap GetSharedMemoryWriteMap(mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n+  IndexingMap GetSharedMemoryWriteMap(\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   absl::InlinedVector<int64_t, 4> tile_sizes_per_block_;\n };\n \n class MultiRowReductionFusion : public ReductionFusion {\n  public:\n-  MultiRowReductionFusion(const HloFusionAnalysis& analysis, int vector_size);\n+  MultiRowReductionFusion(const HloFusionAnalysis& analysis, int vector_size,\n+                          SymbolicExprContext* symbolic_expr_context);\n \n   // Attempts to create a multi-row reduction emitter for the given analysis.\n   // Returns nullptr if the fusion is not supported.\n   static std::unique_ptr<ReductionFusion> TryCreate(\n-      const HloFusionAnalysis& analysis);\n+      const HloFusionAnalysis& analysis,\n+      SymbolicExprContext* symbolic_expr_context);\n \n  protected:\n   // Returns the number of {kept, reduced} threads for the given reduction and\n@@ -188,25 +197,27 @@ class MultiRowReductionFusion : public ReductionFusion {\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n };\n \n class ColumnReductionFusion : public ReductionFusion {\n  public:\n-  explicit ColumnReductionFusion(const HloFusionAnalysis& analysis);\n+  explicit ColumnReductionFusion(const HloFusionAnalysis& analysis,\n+                                 SymbolicExprContext* symbolic_expr_context);\n \n  protected:\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n   IndexingMap GetSharedMemoryReductionReadMap(\n-      mlir::MLIRContext* ctx) const override;\n-  IndexingMap GetSharedMemoryWriteMap(mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n+  IndexingMap GetSharedMemoryWriteMap(\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   const int64_t kTileSize = 32;\n };\n@@ -215,18 +226,21 @@ class ColumnReductionFusion : public ReductionFusion {\n // the warp size.\n class SmallColumnReductionFusion : public ReductionFusion {\n  public:\n-  explicit SmallColumnReductionFusion(const HloFusionAnalysis& analysis);\n+  explicit SmallColumnReductionFusion(\n+      const HloFusionAnalysis& analysis,\n+      SymbolicExprContext* symbolic_expr_context);\n \n  protected:\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n   IndexingMap GetSharedMemoryReductionReadMap(\n-      mlir::MLIRContext* ctx) const override;\n-  IndexingMap GetSharedMemoryWriteMap(mlir::MLIRContext* ctx) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n+  IndexingMap GetSharedMemoryWriteMap(\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   const int64_t kTileSize = 32;\n \n@@ -235,7 +249,8 @@ class SmallColumnReductionFusion : public ReductionFusion {\n };\n \n std::unique_ptr<ReductionFusion> CreateReductionFusion(\n-    const HloFusionAnalysis& analysis);\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "961ccd85204c1eb9df2bf9545631b1d883f03296",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 33,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -63,6 +63,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/scatter_simplifier.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -360,15 +361,17 @@ Value EmitterHelper::GetElement(ImplicitLocOpBuilder& b, int operand_index,\n \n ScatterFusion::ScatterFusion(const HloFusionAnalysis& analysis,\n                              const ScatterDescription& description,\n-                             int64_t vector_size)\n+                             int64_t vector_size,\n+                             SymbolicExprContext* symbolic_expr_context)\n     : analysis_(analysis),\n       description_(description),\n+      symbolic_expr_context_(symbolic_expr_context),\n       warp_size_(WarpSize(analysis_.device_info())),\n       vector_size_(vector_size) {}\n \n std::optional<std::vector<IndexingMap>>\n ScatterFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n-                                              MLIRContext* ctx) const {\n+                                              SymbolicExprContext* ctx) const {\n   CHECK(ScatterSimplifier::IsSimplifiedScatter(description_.scatter))\n       << \"Non-simplified HLO Scatter is not supported.\";\n \n@@ -393,18 +396,19 @@ ScatterFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n }\n \n std::vector<emitters::EpilogueSpecification> ScatterFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n+    const HloFusionInstruction& fusion,\n+    SymbolicExprContext* symbolic_expr_context) const {\n   // We don't actually support epilogues for scatter, but this is how we tell\n   // the base class that we don't want it to generate code for the scatter.\n   return {emitters::EpilogueSpecification::FromIdentityIndexing(\n       &analysis_.fusion_hero(0).instruction(),\n-      &analysis_.fusion_root(0).instruction(), mlir_context)};\n+      &analysis_.fusion_root(0).instruction(), symbolic_expr_context)};\n }\n \n ScatterWithDistributedUpdates::ScatterWithDistributedUpdates(\n     const HloFusionAnalysis& analysis, const ScatterDescription& description,\n-    int64_t vector_size)\n-    : ScatterFusion(analysis, description, vector_size) {\n+    int64_t vector_size, SymbolicExprContext* symbolic_expr_context)\n+    : ScatterFusion(analysis, description, vector_size, symbolic_expr_context) {\n   // We have to make sure that there is no thread that processes elements of\n   // two different update slice.\n   auto launch_dimensions = CalculateLaunchDimensions(\n@@ -417,21 +421,23 @@ ScatterWithDistributedUpdates::ScatterWithDistributedUpdates(\n }\n \n void ScatterWithDistributedUpdates::ComputeIndexing(\n-    MLIRContext* ctx, IndexingMap* updates_map,\n+    SymbolicExprContext* symbolic_expr_context, IndexingMap* updates_map,\n     IndexingMap* indices_map) const {\n   // Compute thread id mapping based on the first update operand.\n   IndexingMap scatter_update_map = GetDefaultThreadIdIndexingMap(\n-      launch_dimensions(), vector_size_, description_.update_shape, ctx);\n+      launch_dimensions(), vector_size_, description_.update_shape,\n+      symbolic_expr_context);\n \n   // For scatter indices we project indexing for scatter updates and take the\n   // first result of the affine map only, because they coincide.\n   if (indices_map) {\n     // Create a map from scatter update to scatter indices.\n     *indices_map = IndexingMap{\n-        AffineMap::get(6, 1,\n-                       {scatter_update_map.GetAffineMap().getResult(0),\n-                        getAffineSymbolExpr(0, ctx)},\n-                       ctx),\n+        AffineMap::get(\n+            6, 1,\n+            {scatter_update_map.GetAffineMap().getResult(0),\n+             getAffineSymbolExpr(0, symbolic_expr_context->GetMLIRContext())},\n+            symbolic_expr_context->GetMLIRContext()),\n         DimVarsFromGPUGrid({num_warps_ * warp_size_, 1, 1, num_blocks_, 1, 1}),\n         RangeVarsFromTensorSizes({description_.index_vector_length}),\n         /*rt_vars=*/{}};\n@@ -455,11 +461,9 @@ absl::Status ScatterFusion::EmitEntryFunction(\n   auto thread_and_block_ids = EmitThreadAndBlockIds(b);\n   Value output_tensor = entry_function.getArguments().back();\n \n-  // Compute indexing maps.\n-  MLIRContext* mlir_context = entry_function.getContext();\n   IndexingMap updates_map = IndexingMap::GetUndefined();\n   IndexingMap indices_map = IndexingMap::GetUndefined();\n-  ComputeIndexing(mlir_context, &updates_map, &indices_map);\n+  ComputeIndexing(symbolic_expr_context_, &updates_map, &indices_map);\n   updates_map.Simplify();\n \n   return EmitEntryFunctionImpl(b, helper, updates_map, indices_map,\n@@ -547,8 +551,9 @@ absl::Status ScatterWithDistributedUpdates::EmitEntryFunctionImpl(\n ScatterWithDistributedIndices::ScatterWithDistributedIndices(\n     const HloFusionAnalysis& analysis, const ScatterDescription& description,\n     int64_t vector_size, int64_t num_warps_per_slice,\n-    int64_t num_indices_per_warp, int64_t indices_vector_size)\n-    : ScatterFusion(analysis, description, vector_size),\n+    int64_t num_indices_per_warp, int64_t indices_vector_size,\n+    SymbolicExprContext* symbolic_expr_context)\n+    : ScatterFusion(analysis, description, vector_size, symbolic_expr_context),\n       num_warps_per_slice_(num_warps_per_slice),\n       num_indices_per_warp_(num_indices_per_warp),\n       indices_vector_size_(indices_vector_size) {\n@@ -558,21 +563,22 @@ ScatterWithDistributedIndices::ScatterWithDistributedIndices(\n }\n \n void ScatterWithDistributedIndices::ComputeIndexing(\n-    MLIRContext* ctx, IndexingMap* updates_map,\n+    SymbolicExprContext* symbolic_expr_context, IndexingMap* updates_map,\n     IndexingMap* indices_map) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   // Compute thread id mapping based on the first update operand.\n   auto thread_x = getAffineDimExpr(\n-      KernelFusionInterface::kIndexingMapThreadIdxDims[0], ctx);\n-  auto block_x =\n-      getAffineDimExpr(KernelFusionInterface::kIndexingMapBlockIdxDims[0], ctx);\n+      KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n+  auto block_x = getAffineDimExpr(\n+      KernelFusionInterface::kIndexingMapBlockIdxDims[0], mlir_context);\n   auto warp_id = thread_x.floorDiv(warp_size_);\n   auto slice_id =\n       (block_x * num_warps_ + warp_id).floorDiv(num_warps_per_slice_);\n   auto warp_id_in_slice =\n       (block_x * num_warps_ + warp_id) % num_warps_per_slice_;\n   auto lane_id = thread_x % warp_size_;\n-  auto index_id_loop = getAffineSymbolExpr(0, ctx);\n-  auto index_vector_id = getAffineSymbolExpr(1, ctx);\n+  auto index_id_loop = getAffineSymbolExpr(0, mlir_context);\n+  auto index_vector_id = getAffineSymbolExpr(1, mlir_context);\n \n   auto vectorized_index_id_expr = slice_id * num_indices_per_warp_ +\n                                   index_id_loop * indices_vector_size_ +\n@@ -581,9 +587,10 @@ void ScatterWithDistributedIndices::ComputeIndexing(\n   auto grid_vars =\n       DimVarsFromGPUGrid({num_warps_ * warp_size_, 1, 1, num_blocks_, 1, 1});\n   if (indices_map) {\n-    auto index_dim_loop = getAffineSymbolExpr(2, ctx);\n+    auto index_dim_loop = getAffineSymbolExpr(2, mlir_context);\n     *indices_map = IndexingMap{\n-        AffineMap::get(6, 3, {vectorized_index_id_expr, index_dim_loop}, ctx),\n+        AffineMap::get(6, 3, {vectorized_index_id_expr, index_dim_loop},\n+                       mlir_context),\n         grid_vars,\n         {IndexingMap::Variable{\n              {0, num_indices_per_warp_ / indices_vector_size_ - 1},\n@@ -600,9 +607,9 @@ void ScatterWithDistributedIndices::ComputeIndexing(\n   }\n \n   if (updates_map) {\n-    auto index_id = getAffineSymbolExpr(0, ctx);\n-    auto update_dim_loop = getAffineSymbolExpr(1, ctx);\n-    auto vector_id = getAffineSymbolExpr(2, ctx);\n+    auto index_id = getAffineSymbolExpr(0, mlir_context);\n+    auto update_dim_loop = getAffineSymbolExpr(1, mlir_context);\n+    auto vector_id = getAffineSymbolExpr(2, mlir_context);\n     auto num_elements_per_slice = Product(description_.slice_shape);\n \n     auto index_id_expr = slice_id * num_indices_per_warp_ + index_id;\n@@ -616,7 +623,7 @@ void ScatterWithDistributedIndices::ComputeIndexing(\n         DelinearizeInBoundsIndex(linear_slice_index, description_.slice_shape));\n \n     *updates_map = IndexingMap{\n-        AffineMap::get(6, 3, updates_indexing, ctx),\n+        AffineMap::get(6, 3, updates_indexing, mlir_context),\n         grid_vars,\n         {IndexingMap::Variable{{0, num_indices_per_warp_ - 1}, \"index_id_loop\"},\n          IndexingMap::Variable{\n@@ -880,7 +887,8 @@ int64_t GetNumPossibleValidIndices(absl::Span<const int64_t> slice_shape,\n }\n \n std::unique_ptr<ScatterFusion> CreateScatterFusion(\n-    const HloFusionAnalysis& analysis) {\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto description = GetScatterDescription(analysis);\n   int64_t warp_size = WarpSize(analysis.device_info());\n   int64_t num_elements_per_slice = Product(description.slice_shape);\n@@ -930,15 +938,15 @@ std::unique_ptr<ScatterFusion> CreateScatterFusion(\n     }\n     return std::make_unique<ScatterWithDistributedIndices>(\n         analysis, description, vector_size, num_warps_per_slice,\n-        num_indices_per_warp, indices_vector_size);\n+        num_indices_per_warp, indices_vector_size, symbolic_expr_context);\n   }\n   // Otherwise, we distribute the linearized updates tensor.\n   vector_size =\n       std::gcd(num_elements_per_slice,\n                ComputeLoopFusionConfig(analysis, description.update_shape)\n                    .unroll_factor);\n-  return std::make_unique<ScatterWithDistributedUpdates>(analysis, description,\n-                                                         vector_size);\n+  return std::make_unique<ScatterWithDistributedUpdates>(\n+      analysis, description, vector_size, symbolic_expr_context);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "35230182c3ac24a9c78098e7a90ec11d915372f1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.h",
            "status": "modified",
            "additions": 23,
            "deletions": 18,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -25,7 +25,6 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n@@ -35,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/util.h\"\n@@ -64,7 +64,8 @@ class ScatterFusion : public EmitterBase {\n  public:\n   explicit ScatterFusion(const HloFusionAnalysis& analysis,\n                          const ScatterDescription& description,\n-                         int64_t vector_size);\n+                         int64_t vector_size,\n+                         SymbolicExprContext* symbolic_expr_context);\n \n   absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -77,14 +78,14 @@ class ScatterFusion : public EmitterBase {\n   }\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override {\n+      int64_t root_index, SymbolicExprContext* ctx) const override {\n     // Since the access pattern to the output is not statically known, we cannot\n     // compute the output->input indexing map.\n     return std::nullopt;\n   }\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const override;\n+      int64_t root_index, SymbolicExprContext* ctx) const override;\n \n  protected:\n   virtual absl::Status EmitEntryFunctionImpl(\n@@ -93,15 +94,17 @@ class ScatterFusion : public EmitterBase {\n       mlir::ValueRange thread_and_block_ids,\n       mlir::Value output_tensor) const = 0;\n \n-  virtual void ComputeIndexing(mlir::MLIRContext* ctx, IndexingMap* updates_map,\n+  virtual void ComputeIndexing(SymbolicExprContext* ctx,\n+                               IndexingMap* updates_map,\n                                IndexingMap* indices_map) const = 0;\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      mlir::MLIRContext* mlir_context) const final;\n+      SymbolicExprContext* symbolic_expr_context) const final;\n \n   const HloFusionAnalysis& analysis_;\n   ScatterDescription description_;\n+  SymbolicExprContext* symbolic_expr_context_;\n \n   // The grid is {num_warps_ * WarpSize(), 1, 1, num_blocks_, 1, 1}.\n   int64_t warp_size_;\n@@ -119,9 +122,9 @@ class ScatterFusion : public EmitterBase {\n // index to scatter an element(s) of the update.\n class ScatterWithDistributedUpdates : public ScatterFusion {\n  public:\n-  explicit ScatterWithDistributedUpdates(const HloFusionAnalysis& analysis,\n-                                         const ScatterDescription& description,\n-                                         int64_t vector_size);\n+  explicit ScatterWithDistributedUpdates(\n+      const HloFusionAnalysis& analysis, const ScatterDescription& description,\n+      int64_t vector_size, SymbolicExprContext* symbolic_expr_context);\n \n  protected:\n   absl::Status EmitEntryFunctionImpl(mlir::ImplicitLocOpBuilder& b,\n@@ -131,7 +134,8 @@ class ScatterWithDistributedUpdates : public ScatterFusion {\n                                      mlir::ValueRange thread_and_block_ids,\n                                      mlir::Value output_tensor) const override;\n \n-  void ComputeIndexing(mlir::MLIRContext* ctx, IndexingMap* updates_map,\n+  void ComputeIndexing(SymbolicExprContext* symbolic_expr_context,\n+                       IndexingMap* updates_map,\n                        IndexingMap* indices_map) const override;\n };\n \n@@ -183,15 +187,15 @@ class ScatterWithDistributedUpdates : public ScatterFusion {\n */\n class ScatterWithDistributedIndices : public ScatterFusion {\n  public:\n-  explicit ScatterWithDistributedIndices(const HloFusionAnalysis& analysis,\n-                                         const ScatterDescription& description,\n-                                         int64_t vector_size,\n-                                         int64_t num_warps_per_slice,\n-                                         int64_t num_indices_per_warp,\n-                                         int64_t indices_vector_size);\n+  explicit ScatterWithDistributedIndices(\n+      const HloFusionAnalysis& analysis, const ScatterDescription& description,\n+      int64_t vector_size, int64_t num_warps_per_slice,\n+      int64_t num_indices_per_warp, int64_t indices_vector_size,\n+      SymbolicExprContext* symbolic_expr_context);\n \n  protected:\n-  void ComputeIndexing(mlir::MLIRContext* ctx, IndexingMap* updates_map,\n+  void ComputeIndexing(SymbolicExprContext* symbolic_expr_context,\n+                       IndexingMap* updates_map,\n                        IndexingMap* indices_map) const override;\n \n   absl::Status EmitEntryFunctionImpl(mlir::ImplicitLocOpBuilder& b,\n@@ -215,7 +219,8 @@ class ScatterWithDistributedIndices : public ScatterFusion {\n };\n \n std::unique_ptr<ScatterFusion> CreateScatterFusion(\n-    const HloFusionAnalysis& analysis);\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "b859b61f3718926c7c0b5652115c5ba306f68b11",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.cc",
            "status": "modified",
            "additions": 109,
            "deletions": 88,
            "changes": 197,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -69,6 +69,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/util.h\"\n@@ -163,21 +164,24 @@ absl::Status TransposeFusionBase::EmitEntryFunction(\n }\n \n std::vector<emitters::EpilogueSpecification> TransposeFusionBase::GetEpilogues(\n-    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n+    const HloFusionInstruction& fusion,\n+    SymbolicExprContext* symbolic_expr_context) const {\n   std::vector<emitters::EpilogueSpecification> epilogues{\n       GetEpilogueForOutputIndexing(analysis_, shmem_transposes_,\n-                                   shmem_transpose_roots_, mlir_context)};\n+                                   shmem_transpose_roots_,\n+                                   symbolic_expr_context)};\n   // Add empty epilogues for the side outputs. This ensures their roots don't\n   // get \"fused\" into the tuple function.\n   for (const auto* root : side_output_roots_) {\n     epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-        root, root, mlir_context));\n+        root, root, symbolic_expr_context));\n   }\n   return epilogues;\n }\n \n-TransposeFusion::TransposeFusion(const HloFusionAnalysis& analysis)\n-    : TransposeFusionBase(analysis),\n+TransposeFusion::TransposeFusion(const HloFusionAnalysis& analysis,\n+                                 SymbolicExprContext* symbolic_expr_context)\n+    : TransposeFusionBase(analysis, symbolic_expr_context),\n       transpose_(analysis.tiled_transpose()),\n       permutation_(transpose_.permutation),\n       input_shape_(\n@@ -259,29 +263,30 @@ TransposeFusion::TransposeFusion(const HloFusionAnalysis& analysis)\n }\n \n std::optional<IndexingMap> TransposeFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, MLIRContext* mlir_context) const {\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   if (!GetDescriptionForTiledTransposeEmitter(hero)) {\n     // The shape of non-transpose roots are bitcast compatible with the input\n     // shape of transpose heroes.\n     return GetIndexing(/*input=*/true,\n-                       analysis_.fusion_root(root_index).shape(), mlir_context);\n+                       analysis_.fusion_root(root_index).shape(),\n+                       symbolic_expr_context);\n   }\n-  return GetIndexing(/*input=*/false, hero.shape(), mlir_context);\n+  return GetIndexing(/*input=*/false, hero.shape(), symbolic_expr_context);\n }\n \n std::optional<std::vector<IndexingMap>>\n TransposeFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, MLIRContext* mlir_context) const {\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   if (GetDescriptionForTiledTransposeEmitter(hero)) {\n     return std::vector<IndexingMap>{GetIndexing(\n-        /*input=*/true, hero.operand(0)->shape(), mlir_context)};\n+        /*input=*/true, hero.operand(0)->shape(), symbolic_expr_context)};\n   }\n   std::vector<IndexingMap> result;\n   result.reserve(hero.operand_count());\n   auto thread_id_to_output_indexing =\n-      ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n+      ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n   if (!thread_id_to_output_indexing.has_value()) {\n     return std::nullopt;\n   }\n@@ -290,7 +295,8 @@ TransposeFusion::ComputeThreadIdToInputIndexing(\n     auto map = ComposeIndexingMaps(\n         *thread_id_to_output_indexing,\n         ComputeOutputToInputIndexing(\n-            &analysis_.fusion_root(root_index).instruction(), 0, mlir_context)\n+            &analysis_.fusion_root(root_index).instruction(), 0,\n+            symbolic_expr_context)\n             .indexing_maps[operand_index]\n             .begin()\n             ->map());\n@@ -305,8 +311,9 @@ LaunchDimensions TransposeFusion::launch_dimensions() const {\n }\n \n IndexingMap TransposeFusion::GetSharedMemoryIndexing(\n-    bool read, mlir::MLIRContext* ctx) const {\n-  auto thread_offsets = GetThreadOffsets(/*read=*/true, ctx);\n+    bool read, SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto thread_offsets = GetThreadOffsets(/*read=*/true, symbolic_expr_context);\n   if (!read) {\n     // Regarding shared memory indexing, the permutation we need to apply is\n     // just a swap of the two dimensions that are tiled.\n@@ -322,7 +329,7 @@ IndexingMap TransposeFusion::GetSharedMemoryIndexing(\n       kNumThreadsPerBlock;\n   dim_var_sizes[KernelFusionInterface::kIndexingMapBlockIdxDims[0]] =\n       Product(block_counts_);\n-  return {mlir::AffineMap::get(6, 2, thread_offsets, ctx),\n+  return {mlir::AffineMap::get(6, 2, thread_offsets, mlir_context),\n           DimVarsFromGPUGrid(dim_var_sizes),\n           RangeVarsFromTensorSizes({block_size_ / kNumRows, vector_size_}),\n           {}};\n@@ -334,7 +341,6 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n     const emitters::PartitionedComputation& root_computation,\n     const emitters::CallTargetProvider& call_target_provider,\n     ValueRange output_args, mlir::ValueRange thread_and_block_ids) const {\n-  MLIRContext* ctx = builder.getContext();\n   auto shmem_tensor_size = block_sizes_;\n   // Avoid bank conflicts.\n   if (MostMinorDimensionUnchanged()) {\n@@ -360,13 +366,15 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n     inits.push_back(entry_function.getArgument(num_inputs + index));\n   }\n \n-  IndexingMap write_indexing = GetSharedMemoryIndexing(/*read=*/false, ctx);\n+  IndexingMap write_indexing =\n+      GetSharedMemoryIndexing(/*read=*/false, symbolic_expr_context_);\n   auto body_builder = [&](ImplicitLocOpBuilder& nested_b,\n                           ValueRange symbol_values, ValueRange map_results,\n                           ValueRange output_tensors) -> SmallVector<Value> {\n     auto input_indices = [&](const HloInstruction* instr) {\n-      return ApplyIndexing(GetIndexing(/*input=*/true, instr->shape(), ctx),\n-                           thread_and_block_ids, symbol_values, nested_b);\n+      return ApplyIndexing(\n+          GetIndexing(/*input=*/true, instr->shape(), symbolic_expr_context_),\n+          thread_and_block_ids, symbol_values, nested_b);\n     };\n \n     SmallVector<Value> result_tensors;\n@@ -407,7 +415,8 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n   };\n \n   auto indexing = GetIndexing(\n-      /*input=*/true, shmem_transposes_.front()->operand(0)->shape(), ctx);\n+      /*input=*/true, shmem_transposes_.front()->operand(0)->shape(),\n+      symbolic_expr_context_);\n   auto written_vector = emitters::EmitXlaLoopOp(builder, thread_and_block_ids,\n                                                 inits, indexing, body_builder);\n   ValueRange written = written_vector;\n@@ -432,11 +441,10 @@ void TransposeFusion::EmitReadFromShMemMlir(\n     const HloFusionInstruction& fusion,\n     const emitters::PartitionedComputations& computations,\n     const WriteResult& written, mlir::ValueRange thread_and_block_ids) const {\n-  auto* mlir_context = builder.getContext();\n   auto output_indexing = *ComputeThreadIdToOutputIndexing(\n-      shmem_transpose_root_indices_[0], mlir_context);\n+      shmem_transpose_root_indices_[0], symbolic_expr_context_);\n   auto shmem_read_indexing =\n-      GetSharedMemoryIndexing(/*read=*/true, mlir_context);\n+      GetSharedMemoryIndexing(/*read=*/true, symbolic_expr_context_);\n   auto result_tensors = emitters::EmitXlaLoopOp(\n       builder, thread_and_block_ids, written.updated_outputs, output_indexing,\n       [&](ImplicitLocOpBuilder& nested_b, ValueRange symbol_values,\n@@ -473,11 +481,12 @@ void TransposeFusion::EmitReadFromShMemMlir(\n }\n \n llvm::SmallVector<mlir::AffineExpr, 4> TransposeFusion::GetThreadOffsets(\n-    bool read, mlir::MLIRContext* ctx) const {\n+    bool read, SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto thread = getAffineDimExpr(\n-      KernelFusionInterface::kIndexingMapThreadIdxDims[0], ctx);\n-  auto loop = getAffineSymbolExpr(0, ctx);\n-  auto vector = getAffineSymbolExpr(1, ctx);\n+      KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n+  auto loop = getAffineSymbolExpr(0, mlir_context);\n+  auto vector = getAffineSymbolExpr(1, mlir_context);\n   int loop_stride = block_size_ * kNumRows;\n   if (MostMinorDimensionUnchanged()) {\n     loop_stride *= vector_size_;\n@@ -487,15 +496,17 @@ llvm::SmallVector<mlir::AffineExpr, 4> TransposeFusion::GetThreadOffsets(\n                                   read ? block_sizes_ : output_block_sizes_);\n }\n \n-IndexingMap TransposeFusion::GetIndexing(bool input, const xla::Shape& shape,\n-                                         mlir::MLIRContext* ctx) const {\n-  auto raw_id =\n-      getAffineDimExpr(KernelFusionInterface::kIndexingMapBlockIdxDims[0], ctx);\n+IndexingMap TransposeFusion::GetIndexing(\n+    bool input, const xla::Shape& shape,\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  auto raw_id = getAffineDimExpr(\n+      KernelFusionInterface::kIndexingMapBlockIdxDims[0], mlir_context);\n   auto block_ids = DelinearizeInBoundsIndex(raw_id, block_counts_);\n   if (!input) {\n     absl::c_copy(Permute(block_ids, permutation_), block_ids.begin());\n   }\n-  auto thread_offsets = GetThreadOffsets(input, ctx);\n+  auto thread_offsets = GetThreadOffsets(input, symbolic_expr_context);\n   const auto& permuted_block_sizes = input ? block_sizes_ : output_block_sizes_;\n   llvm::SmallVector<AffineExpr, 3> offsets;\n   for (auto [block_id, block_size, thread] :\n@@ -508,7 +519,7 @@ IndexingMap TransposeFusion::GetIndexing(bool input, const xla::Shape& shape,\n   dim_var_sizes[KernelFusionInterface::kIndexingMapBlockIdxDims[0]] =\n       Product(block_counts_);\n   IndexingMap result{\n-      mlir::AffineMap::get(6, 2, offsets, ctx),\n+      mlir::AffineMap::get(6, 2, offsets, mlir_context),\n       DimVarsFromTensorSizes(dim_var_sizes),\n       RangeVarsFromTensorSizes({block_size_ / kNumRows, vector_size_}),\n       {}};\n@@ -519,8 +530,8 @@ IndexingMap TransposeFusion::GetIndexing(bool input, const xla::Shape& shape,\n                                     result.GetAffineMap().getResults())) {\n     result.AddConstraint(dim, {0, size - 1});\n   }\n-  result =\n-      ComposeIndexingMaps(result, GetBitcastMap(normalized_shape, shape, ctx));\n+  result = ComposeIndexingMaps(\n+      result, GetBitcastMap(normalized_shape, shape, symbolic_expr_context));\n   result.Simplify();\n   return result;\n }\n@@ -541,8 +552,9 @@ std::vector<int64_t> GetBlockCounts(absl::Span<const int64_t> shape,\n PackedTranspose::PackedTranspose(const HloFusionAnalysis& analysis,\n                                  const TransposeSpec& spec,\n                                  absl::Span<const int64_t> output_block_tile,\n-                                 int64_t num_warps)\n-    : TransposeFusionBase(analysis),\n+                                 int64_t num_warps,\n+                                 SymbolicExprContext* symbolic_expr_context)\n+    : TransposeFusionBase(analysis, symbolic_expr_context),\n       spec_(spec),\n       output_tile_(output_block_tile.begin(), output_block_tile.end()),\n       input_tile_(Permute(output_tile_, spec_.canonical_inv_permutation)),\n@@ -579,21 +591,21 @@ PackedTranspose::PackedTranspose(const HloFusionAnalysis& analysis,\n }\n \n std::optional<IndexingMap> PackedTranspose::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, MLIRContext* mlir_context) const {\n-  return GetOutputIndexing(mlir_context);\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+  return GetOutputIndexing(symbolic_expr_context);\n }\n \n std::optional<std::vector<IndexingMap>>\n PackedTranspose::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, MLIRContext* mlir_context) const {\n+    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   if (GetDescriptionForTiledTransposeEmitter(hero)) {\n-    return std::vector<IndexingMap>{GetInputIndexing(mlir_context)};\n+    return std::vector<IndexingMap>{GetInputIndexing(symbolic_expr_context)};\n   }\n   std::vector<IndexingMap> result;\n   result.reserve(hero.operand_count());\n   auto thread_id_to_output_indexing =\n-      ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n+      ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n   if (!thread_id_to_output_indexing.has_value()) {\n     return std::nullopt;\n   }\n@@ -602,7 +614,8 @@ PackedTranspose::ComputeThreadIdToInputIndexing(\n     auto map = ComposeIndexingMaps(\n         *thread_id_to_output_indexing,\n         ComputeOutputToInputIndexing(\n-            &analysis_.fusion_root(root_index).instruction(), 0, mlir_context)\n+            &analysis_.fusion_root(root_index).instruction(), 0,\n+            symbolic_expr_context)\n             .indexing_maps[operand_index]\n             .begin()\n             ->map());\n@@ -622,9 +635,9 @@ PackedTranspose::WriteResult PackedTranspose::EmitWriteToShMemMlir(\n     const emitters::PartitionedComputation& root_computation,\n     const emitters::CallTargetProvider& call_target_provider,\n     ValueRange output_args, mlir::ValueRange thread_and_block_ids) const {\n-  MLIRContext* ctx = builder.getContext();\n-  IndexingMap input_indexing = GetInputIndexing(ctx);\n-  IndexingMap shmem_write_indexing = GetShmemWriteIndexing(ctx);\n+  IndexingMap input_indexing = GetInputIndexing(symbolic_expr_context_);\n+  IndexingMap shmem_write_indexing =\n+      GetShmemWriteIndexing(symbolic_expr_context_);\n \n   int64_t shmem_dim = kNumShmemBanks * vector_size_;\n   SmallVector<Value> shmem_tensors;\n@@ -672,8 +685,8 @@ PackedTranspose::WriteResult PackedTranspose::EmitWriteToShMemMlir(\n     auto* root_tuple = fusion.fused_expression_root();\n     for (auto root : side_output_roots_) {\n       auto indexing = ComposeIndexingMaps(\n-          input_indexing,\n-          GetBitcastMap(spec_.input_shape(), root->shape(), ctx));\n+          input_indexing, GetBitcastMap(spec_.input_shape(), root->shape(),\n+                                        symbolic_expr_context_));\n       indexing.Simplify();\n       side_output_indices.push_back(ApplyIndexing(\n           indexing, thread_and_block_ids, symbol_values, nested_b));\n@@ -726,11 +739,10 @@ void PackedTranspose::EmitReadFromShMemMlir(\n     const HloFusionInstruction& fusion,\n     const emitters::PartitionedComputations& computations,\n     const WriteResult& written, mlir::ValueRange thread_and_block_ids) const {\n-  auto* mlir_context = builder.getContext();\n-  auto shmem_read_indexing = GetShmemReadIndexing(mlir_context);\n+  auto shmem_read_indexing = GetShmemReadIndexing(symbolic_expr_context_);\n   auto outer_loop_indexing = ConvertRangeVariablesToDimensions(\n       shmem_read_indexing, /*range_var_indices=*/{1, 2});\n-  auto output_indexing = GetOutputIndexing(mlir_context);\n+  auto output_indexing = GetOutputIndexing(symbolic_expr_context_);\n   auto output_indexing_over_vectors = ConvertRangeVariablesToDimensions(\n       output_indexing, /*range_var_indices=*/{0});\n \n@@ -805,21 +817,23 @@ void PackedTranspose::EmitReadFromShMemMlir(\n   builder.create<ReturnOp>(outer_loop_results);\n }\n \n-IndexingMap PackedTranspose::GetInputIndexing(MLIRContext* ctx) const {\n+IndexingMap PackedTranspose::GetInputIndexing(\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n-      KernelFusionInterface::kIndexingMapThreadIdxDims[0], ctx);\n-  auto block_id =\n-      getAffineDimExpr(KernelFusionInterface::kIndexingMapBlockIdxDims[0], ctx);\n+      KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n+  auto block_id = getAffineDimExpr(\n+      KernelFusionInterface::kIndexingMapBlockIdxDims[0], mlir_context);\n   auto warp_size = kNumShmemBanks;\n   auto lane_id = thread_id % warp_size;\n   auto warp_id = thread_id.floorDiv(warp_size);\n   std::vector<IndexingMap::Variable> dim_vars = DimVarsFromGPUGrid(\n       {num_warps_per_block_ * warp_size, 1, 1, Product(block_counts_), 1, 1});\n \n   // Range variables.\n-  auto loop = getAffineSymbolExpr(0, ctx);\n-  auto vector_element_id = getAffineSymbolExpr(1, ctx);\n+  auto loop = getAffineSymbolExpr(0, mlir_context);\n+  auto vector_element_id = getAffineSymbolExpr(1, mlir_context);\n   std::vector<IndexingMap::Variable> range_vars = RangeVarsFromTensorSizes(\n       {{CeilOfRatio(tile_size_t2_, num_warps_per_block_), vector_size_}});\n \n@@ -833,7 +847,7 @@ IndexingMap PackedTranspose::GetInputIndexing(MLIRContext* ctx) const {\n   auto shmem_col = lane_id * vector_size_ + vector_element_id;\n \n   // Offsets within the block.\n-  auto c0 = getAffineConstantExpr(0, ctx);\n+  auto c0 = getAffineConstantExpr(0, mlir_context);\n   int64_t canonical_rank = spec_.canonical_rank();\n   llvm::SmallVector<AffineExpr, 4> offsets_within_tile(canonical_rank, c0);\n   offsets_within_tile[spec_.dim_A_id()] = shmem_col.floorDiv(tile_size_t1_);\n@@ -852,13 +866,13 @@ IndexingMap PackedTranspose::GetInputIndexing(MLIRContext* ctx) const {\n       {shmem_row, Interval{0, populated_shmem_rows_ - 1}}};\n   IndexingMap canonical_input_indexing{\n       mlir::AffineMap::get(/*num_dims=*/6, /*num_symbols=*/2, canonical_offsets,\n-                           ctx),\n+                           mlir_context),\n       std::move(dim_vars), std::move(range_vars), /*rt_vars=*/{}, constraints};\n   canonical_input_indexing.Simplify();\n \n   // Actual indexing.\n-  auto canonical_input_shape_to_real_shape =\n-      GetBitcastMap(spec_.canonical_input_shape, spec_.input_shape(), ctx);\n+  auto canonical_input_shape_to_real_shape = GetBitcastMap(\n+      spec_.canonical_input_shape, spec_.input_shape(), symbolic_expr_context);\n   // When we compose, the constraints w.r.t. to the input dimension sizes will\n   // be added.\n   auto input_indexing = ComposeIndexingMaps(\n@@ -868,19 +882,20 @@ IndexingMap PackedTranspose::GetInputIndexing(MLIRContext* ctx) const {\n }\n \n IndexingMap PackedTranspose::GetShmemWriteIndexing(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n-      KernelFusionInterface::kIndexingMapThreadIdxDims[0], ctx);\n+      KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n   auto warp_size = kNumShmemBanks;\n   auto lane_id = thread_id % warp_size;\n   auto warp_id = thread_id.floorDiv(warp_size);\n   std::vector<IndexingMap::Variable> dim_vars = DimVarsFromGPUGrid(\n       {num_warps_per_block_ * warp_size, 1, 1, Product(block_counts_), 1, 1});\n \n   // Range variables.\n-  auto loop = getAffineSymbolExpr(0, ctx);\n-  auto vector_element_id = getAffineSymbolExpr(1, ctx);\n+  auto loop = getAffineSymbolExpr(0, mlir_context);\n+  auto vector_element_id = getAffineSymbolExpr(1, mlir_context);\n   std::vector<IndexingMap::Variable> range_vars = RangeVarsFromTensorSizes(\n       {CeilOfRatio(tile_size_t2_, num_warps_per_block_), vector_size_});\n \n@@ -893,27 +908,28 @@ IndexingMap PackedTranspose::GetShmemWriteIndexing(\n   shmem_col = Swizzle(shmem_row, shmem_col, vector_size_);\n \n   IndexingMap shmem_write_indexing_map{\n-      mlir::AffineMap::get(6, 2, {shmem_row, shmem_col}, ctx), dim_vars,\n-      range_vars, /*rt_vars=*/{}, constraints};\n+      mlir::AffineMap::get(6, 2, {shmem_row, shmem_col}, mlir_context),\n+      dim_vars, range_vars, /*rt_vars=*/{}, constraints};\n   shmem_write_indexing_map.Simplify();\n   return shmem_write_indexing_map;\n }\n \n IndexingMap PackedTranspose::GetShmemReadIndexing(\n-    mlir::MLIRContext* ctx) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n-      KernelFusionInterface::kIndexingMapThreadIdxDims[0], ctx);\n+      KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n   auto warp_size = kNumShmemBanks;\n   auto lane_id = thread_id % warp_size;\n   auto warp_id = thread_id.floorDiv(warp_size);\n   std::vector<IndexingMap::Variable> dim_vars = DimVarsFromGPUGrid(\n       {num_warps_per_block_ * warp_size, 1, 1, Product(block_counts_), 1, 1});\n \n   // Range variables.\n-  auto loop = getAffineSymbolExpr(0, ctx);\n-  auto vector_horizontal = getAffineSymbolExpr(1, ctx);\n-  auto vector_vertical = getAffineSymbolExpr(2, ctx);\n+  auto loop = getAffineSymbolExpr(0, mlir_context);\n+  auto vector_horizontal = getAffineSymbolExpr(1, mlir_context);\n+  auto vector_vertical = getAffineSymbolExpr(2, mlir_context);\n   std::vector<IndexingMap::Variable> range_vars = RangeVarsFromTensorSizes(\n       {CeilOfRatio(populated_shmem_cols_,\n                    (vector_size_ * num_warps_per_block_)),\n@@ -929,28 +945,30 @@ IndexingMap PackedTranspose::GetShmemReadIndexing(\n   shmem_col = Swizzle(shmem_row, shmem_col, vector_size_);\n \n   IndexingMap shmem_read_indexing_map{\n-      mlir::AffineMap::get(6, 3, {shmem_row, shmem_col}, ctx), dim_vars,\n-      range_vars, /*rt_vars=*/{}, constraints};\n+      mlir::AffineMap::get(6, 3, {shmem_row, shmem_col}, mlir_context),\n+      dim_vars, range_vars, /*rt_vars=*/{}, constraints};\n   shmem_read_indexing_map.Simplify();\n   return shmem_read_indexing_map;\n }\n \n-IndexingMap PackedTranspose::GetOutputIndexing(mlir::MLIRContext* ctx) const {\n+IndexingMap PackedTranspose::GetOutputIndexing(\n+    SymbolicExprContext* symbolic_expr_context) const {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n-      KernelFusionInterface::kIndexingMapThreadIdxDims[0], ctx);\n-  auto block_id =\n-      getAffineDimExpr(KernelFusionInterface::kIndexingMapBlockIdxDims[0], ctx);\n+      KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n+  auto block_id = getAffineDimExpr(\n+      KernelFusionInterface::kIndexingMapBlockIdxDims[0], mlir_context);\n   auto warp_size = kNumShmemBanks;\n   auto lane_id = thread_id % warp_size;\n   auto warp_id = thread_id.floorDiv(warp_size);\n   std::vector<IndexingMap::Variable> dim_vars = DimVarsFromGPUGrid(\n       {num_warps_per_block_ * warp_size, 1, 1, Product(block_counts_), 1, 1});\n \n   // Range variables.\n-  auto loop = getAffineSymbolExpr(0, ctx);\n-  auto vector_horizontal = getAffineSymbolExpr(1, ctx);\n-  auto vector_vertical = getAffineSymbolExpr(2, ctx);\n+  auto loop = getAffineSymbolExpr(0, mlir_context);\n+  auto vector_horizontal = getAffineSymbolExpr(1, mlir_context);\n+  auto vector_vertical = getAffineSymbolExpr(2, mlir_context);\n   std::vector<IndexingMap::Variable> range_vars = RangeVarsFromTensorSizes(\n       {CeilOfRatio(populated_shmem_cols_, vector_size_ * num_warps_per_block_),\n        vector_size_, vector_size_});\n@@ -964,7 +982,7 @@ IndexingMap PackedTranspose::GetOutputIndexing(mlir::MLIRContext* ctx) const {\n   auto shmem_row = lane_id * vector_size_ + vector_vertical;\n \n   // Offsets within the block.\n-  auto c0 = getAffineConstantExpr(0, ctx);\n+  auto c0 = getAffineConstantExpr(0, mlir_context);\n   int64_t canonical_rank = spec_.canonical_rank();\n   llvm::SmallVector<AffineExpr, 4> offsets_within_tile(canonical_rank, c0);\n   offsets_within_tile[spec_.dim_A_id()] = shmem_col.floorDiv(tile_size_t1_);\n@@ -982,13 +1000,14 @@ IndexingMap PackedTranspose::GetOutputIndexing(mlir::MLIRContext* ctx) const {\n       {shmem_col, Interval{0, populated_shmem_cols_ - 1}},\n       {shmem_row, Interval{0, populated_shmem_rows_ - 1}}};\n   IndexingMap canonical_output_indexing{\n-      mlir::AffineMap::get(6, 3, canonical_offsets, ctx), std::move(dim_vars),\n-      std::move(range_vars), /*rt_vars=*/{}, constraints};\n+      mlir::AffineMap::get(6, 3, canonical_offsets, mlir_context),\n+      std::move(dim_vars), std::move(range_vars), /*rt_vars=*/{}, constraints};\n   canonical_output_indexing.Simplify();\n \n   // Actual indexing.\n   auto canonical_output_shape_to_real_shape =\n-      GetBitcastMap(spec_.canonical_output_shape, spec_.output_shape(), ctx);\n+      GetBitcastMap(spec_.canonical_output_shape, spec_.output_shape(),\n+                    symbolic_expr_context);\n   // When we compose, the constraints w.r.t. to the output dimension sizes will\n   // be added.\n   auto output_indexing = ComposeIndexingMaps(\n@@ -998,15 +1017,17 @@ IndexingMap PackedTranspose::GetOutputIndexing(mlir::MLIRContext* ctx) const {\n }\n \n std::unique_ptr<EmitterBase> CreateTransposeFusion(\n-    const HloFusionAnalysis& analysis) {\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto spec = GetTransposeSpec(\n       Cast<HloTransposeInstruction>(analysis.tiled_transpose().instr));\n   auto packed_transpose_tile = GetPackedTransposeTileSizes(spec);\n   if (packed_transpose_tile.ok()) {\n     return std::make_unique<PackedTranspose>(\n-        analysis, spec, *packed_transpose_tile, /* num_warps= */ 4);\n+        analysis, spec, *packed_transpose_tile,\n+        /* num_warps= */ 4, symbolic_expr_context);\n   }\n-  return std::make_unique<TransposeFusion>(analysis);\n+  return std::make_unique<TransposeFusion>(analysis, symbolic_expr_context);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "72f0232f545b682351e194103269ad0e0ff6c999",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.h",
            "status": "modified",
            "additions": 28,
            "deletions": 18,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -29,7 +29,6 @@ limitations under the License.\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n@@ -41,6 +40,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -49,8 +49,9 @@ namespace gpu {\n \n class TransposeFusionBase : public EmitterBase {\n  public:\n-  explicit TransposeFusionBase(const HloFusionAnalysis& analysis)\n-      : analysis_(analysis) {}\n+  explicit TransposeFusionBase(const HloFusionAnalysis& analysis,\n+                               SymbolicExprContext* symbolic_expr_context)\n+      : analysis_(analysis), symbolic_expr_context_(symbolic_expr_context) {}\n \n  protected:\n   absl::Status EmitEntryFunction(\n@@ -61,7 +62,7 @@ class TransposeFusionBase : public EmitterBase {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      mlir::MLIRContext* mlir_context) const override;\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   struct WriteResult {\n     // All output tensors of the fusion, with side outputs written to them.\n@@ -86,6 +87,7 @@ class TransposeFusionBase : public EmitterBase {\n       mlir::ValueRange thread_and_block_ids) const = 0;\n \n   const HloFusionAnalysis& analysis_;\n+  SymbolicExprContext* symbolic_expr_context_;\n \n   // Transpose instructions that require shared memory. Note that not all\n   // transposes require shared memory, e.g. the ones with a large innermost\n@@ -115,14 +117,17 @@ class TransposeFusionBase : public EmitterBase {\n // https://goo.gl/MStRV6.\n class TransposeFusion : public TransposeFusionBase {\n  public:\n-  explicit TransposeFusion(const HloFusionAnalysis& analysis);\n+  explicit TransposeFusion(const HloFusionAnalysis& analysis,\n+                           SymbolicExprContext* symbolic_expr_context);\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n  protected:\n   WriteResult EmitWriteToShMemMlir(\n@@ -141,11 +146,12 @@ class TransposeFusion : public TransposeFusionBase {\n \n  private:\n   IndexingMap GetIndexing(bool input, const xla::Shape& shape,\n-                          mlir::MLIRContext* ctx) const;\n-  IndexingMap GetSharedMemoryIndexing(bool read, mlir::MLIRContext* ctx) const;\n+                          SymbolicExprContext* symbolic_expr_context) const;\n+  IndexingMap GetSharedMemoryIndexing(\n+      bool read, SymbolicExprContext* symbolic_expr_context) const;\n \n   llvm::SmallVector<mlir::AffineExpr, 4> GetThreadOffsets(\n-      bool read, mlir::MLIRContext* ctx) const;\n+      bool read, SymbolicExprContext* symbolic_expr_context) const;\n   bool MostMinorDimensionUnchanged() const;\n \n   TransposeDescription transpose_;\n@@ -230,15 +236,18 @@ class PackedTranspose : public TransposeFusionBase {\n   explicit PackedTranspose(const HloFusionAnalysis& analysis,\n                            const TransposeSpec& spec,\n                            absl::Span<const int64_t> output_block_tile,\n-                           int64_t num_warps);\n+                           int64_t num_warps,\n+                           SymbolicExprContext* symbolic_expr_context);\n \n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n+      int64_t root_index,\n+      SymbolicExprContext* symbolic_expr_context) const override;\n \n  protected:\n   WriteResult EmitWriteToShMemMlir(\n@@ -257,11 +266,11 @@ class PackedTranspose : public TransposeFusionBase {\n       mlir::ValueRange thread_and_block_ids) const override;\n \n  private:\n-  IndexingMap GetInputIndexing(mlir::MLIRContext* ctx) const;\n-  IndexingMap GetShmemWriteIndexing(mlir::MLIRContext* ctx) const;\n+  IndexingMap GetInputIndexing(SymbolicExprContext* ctx) const;\n+  IndexingMap GetShmemWriteIndexing(SymbolicExprContext* ctx) const;\n \n-  IndexingMap GetShmemReadIndexing(mlir::MLIRContext* ctx) const;\n-  IndexingMap GetOutputIndexing(mlir::MLIRContext* ctx) const;\n+  IndexingMap GetShmemReadIndexing(SymbolicExprContext* ctx) const;\n+  IndexingMap GetOutputIndexing(SymbolicExprContext* ctx) const;\n \n   TransposeSpec spec_;\n \n@@ -294,7 +303,8 @@ class PackedTranspose : public TransposeFusionBase {\n };\n \n std::unique_ptr<EmitterBase> CreateTransposeFusion(\n-    const HloFusionAnalysis& analysis);\n+    const HloFusionAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "73f5e3478a876b1baf72f04bfb1eeb91b2b389d9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/target_util.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n@@ -100,10 +101,11 @@ absl::Status AnnotateKernelLaunchDimensions(\n \n IndexingMap KernelFusionInterface::GetDefaultThreadIdIndexingMap(\n     const LaunchDimensions& launch_dims, int unroll_factor, const Shape& shape,\n-    mlir::MLIRContext* ctx) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   WorkDimensions work_dimensions = launch_dims.AsWorkDimensions();\n   work_dimensions.work_tile_size.dimensions.push_back(unroll_factor);\n-  return emitters::GetDefaultWorkItemIndexingMap(work_dimensions, shape, ctx);\n+  return emitters::GetDefaultWorkItemIndexingMap(work_dimensions, shape,\n+                                                 symbolic_expr_context);\n }\n \n std::string GetSanitizedUniqueName(IrEmitterContext& ir_emitter_context,"
        },
        {
            "sha": "4a5c26245d929e07d6fe0a6b3e856da23e51fd44",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,7 +30,6 @@ limitations under the License.\n #include \"llvm/IR/Function.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"mlir/IR/AffineMap.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n@@ -78,14 +77,14 @@ class KernelFusionInterface : public FusionInterface {\n   // unsupported (scatter, in-place DUS). Implementations will return nullopt.\n   // Note: Work in progress, not implemented for all emitters.\n   virtual std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, mlir::MLIRContext* ctx) const = 0;\n+      int64_t root_index, SymbolicExprContext* ctx) const = 0;\n \n   // Computes indexing maps from thread id to input elements of the root's\n   // **hero**. Note that in many cases this is not computable from the output\n   // indexing. The indexing may only be known for some operands of the hero.\n   virtual std::optional<std::vector<IndexingMap>>\n   ComputeThreadIdToInputIndexing(int64_t root_index,\n-                                 mlir::MLIRContext* ctx) const = 0;\n+                                 SymbolicExprContext* ctx) const = 0;\n \n   static constexpr std::array<int, 3> kIndexingMapThreadIdxDims = {0, 1, 2};\n   static constexpr std::array<int, 3> kIndexingMapBlockIdxDims = {3, 4, 5};\n@@ -97,7 +96,7 @@ class KernelFusionInterface : public FusionInterface {\n   // block sizes in the given launch dimensions.\n   static IndexingMap GetDefaultThreadIdIndexingMap(\n       const LaunchDimensions& launch_dims, int unroll_factor,\n-      const Shape& shape, mlir::MLIRContext* ctx);\n+      const Shape& shape, SymbolicExprContext* ctx);\n };\n \n absl::StatusOr<llvm::Function*> BuildKernelPrototype("
        },
        {
            "sha": "305bd50b42ca6e9d6eb6faf86feb9875c847780e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/copy.h\"\n #include \"xla/backends/gpu/codegen/cudnn.h\"\n #include \"xla/backends/gpu/codegen/custom.h\"\n@@ -39,6 +38,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n \n namespace xla {\n@@ -77,8 +77,9 @@ bool HloFusionInfo::CanEmitDynamicUpdateSliceInPlace() const {\n   return ret.ok() && *ret;\n }\n \n-std::unique_ptr<FusionInterface> GetFusionEmitter(const FusionInfo& fusion_info,\n-                                                  mlir::MLIRContext* ctx) {\n+std::unique_ptr<FusionInterface> GetFusionEmitter(\n+    const FusionInfo& fusion_info,\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   const auto& analysis = fusion_info.analysis();\n   const FusionBackendConfig& backend_config = analysis.fusion_backend_config();\n \n@@ -109,16 +110,16 @@ std::unique_ptr<FusionInterface> GetFusionEmitter(const FusionInfo& fusion_info,\n           fusion_info.CanEmitDynamicUpdateSliceInPlace()) {\n         return std::make_unique<InPlaceDynamicUpdateSliceFusion>(analysis);\n       }\n-      return std::make_unique<LoopFusion>(analysis, ctx);\n+      return std::make_unique<LoopFusion>(analysis, symbolic_expr_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kReduction: {\n-      return CreateReductionFusion(analysis);\n+      return CreateReductionFusion(analysis, symbolic_expr_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kScatter: {\n-      return CreateScatterFusion(analysis);\n+      return CreateScatterFusion(analysis, symbolic_expr_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kTranspose: {\n-      return CreateTransposeFusion(analysis);\n+      return CreateTransposeFusion(analysis, symbolic_expr_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kConcatenate: {\n       return std::make_unique<ConcatenateFusion>(analysis);"
        },
        {
            "sha": "422fcbf3cfea72c74fd54170e196fd44b1ba18a4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,12 +19,12 @@ limitations under the License.\n #include <optional>\n \n #include \"absl/status/statusor.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace gpu {\n@@ -95,8 +95,9 @@ class PreBufferAssignmentFusionInfo : public FusionInfo {\n };\n \n // Returns the emitter for the given fusion.\n-std::unique_ptr<FusionInterface> GetFusionEmitter(const FusionInfo& fusion_info,\n-                                                  mlir::MLIRContext* ctx);\n+std::unique_ptr<FusionInterface> GetFusionEmitter(\n+    const FusionInfo& fusion_info,\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "bbe33b134eed8feeebec0f628827e88aa1496a39",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -32,28 +32,13 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla/backends/gpu/codegen:fusions\",\n         \"//xla/backends/gpu/codegen/emitters:emitter_base\",\n-        \"//xla/backends/gpu/codegen/emitters/ir:xla_gpu\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/mlir_hlo\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/tools:hlo_module_loader\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//mlir:AffineDialect\",\n-        \"@llvm-project//mlir:ArithDialect\",\n-        \"@llvm-project//mlir:ComplexDialect\",\n-        \"@llvm-project//mlir:DLTIDialect\",\n-        \"@llvm-project//mlir:FuncDialect\",\n-        \"@llvm-project//mlir:FuncExtensions\",\n-        \"@llvm-project//mlir:GPUDialect\",\n         \"@llvm-project//mlir:IR\",\n-        \"@llvm-project//mlir:MathDialect\",\n-        \"@llvm-project//mlir:MlirOptLib\",\n-        \"@llvm-project//mlir:SCFDialect\",\n-        \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:VectorDialect\",\n     ],\n )\n@@ -70,6 +55,7 @@ xla_cc_binary(\n     deps = [\n         \":test_lib\",\n         \"//xla/codegen/tools:test_lib\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@llvm-project//llvm:Support\","
        },
        {
            "sha": "dab207a9c2316b983b0574335d03d0969c00381e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/fusion_to_mlir.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,21 +19,23 @@ limitations under the License.\n #include \"llvm/Support/raw_ostream.h\"\n #include \"xla/backends/gpu/codegen/tools/test_lib.h\"\n #include \"xla/codegen/tools/test_lib.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"tsl/platform/init_main.h\"\n #include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n \n absl::Status Run(const std::string& filename) {\n+  auto mlir_context = GetMlirContextForTest();\n+  mlir_context.loadAllAvailableDialects();\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_ASSIGN_OR_RETURN(auto module, LoadTestModule(filename));\n-  TF_ASSIGN_OR_RETURN(auto emitter_data, GetEmitter(*module));\n-\n-  auto context = GetMlirContextForTest();\n-  context.loadAllAvailableDialects();\n+  TF_ASSIGN_OR_RETURN(auto emitter_data,\n+                      GetEmitter(*module, symbolic_expr_context));\n   TF_ASSIGN_OR_RETURN(auto mlir_module,\n                       emitter_data->emitter->CreateMLIRModule(\n-                          context, *emitter_data->fusion, \"main\",\n+                          symbolic_expr_context, *emitter_data->fusion, \"main\",\n                           /*buffer_assignment=*/nullptr));\n   llvm::outs() << *mlir_module;\n   return absl::OkStatus();"
        },
        {
            "sha": "dcb42bb47df992e5e92093a90628932cb16a23d5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/gpu_test_correctness.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -107,14 +107,16 @@ std::pair<std::string, std::vector<int64_t>> ParseHeroAndIds(\n }\n \n TEST_F(CorrectnessTest, InputIndexingIsBijection) {\n-  auto context = GetMlirContextForTest();\n+  auto mlir_context = GetMlirContextForTest();\n+  auto symbolic_expr_context = GetSymbolicExprContextForTest(&mlir_context);\n   TF_ASSERT_OK_AND_ASSIGN(auto module, LoadTestModule(flags.input_file));\n-  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data, GetEmitter(*module));\n+  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data,\n+                          GetEmitter(*module, symbolic_expr_context));\n   for (const auto& [hero_name, ids] : flags.bijection_inputs) {\n     TF_ASSERT_OK_AND_ASSIGN(int64_t hero_index,\n                             GetHeroIndex(hero_name, *emitter_data->analysis));\n     auto indexing = emitter_data->emitter->ComputeThreadIdToInputIndexing(\n-        hero_index, &context);\n+        hero_index, &symbolic_expr_context);\n     ASSERT_TRUE(indexing.has_value());\n     for (int64_t id : ids) {\n       TF_ASSERT_OK(TestBijection(indexing.value()[id],\n@@ -129,14 +131,16 @@ TEST_F(CorrectnessTest, InputIndexingIsBijection) {\n }\n \n TEST_F(CorrectnessTest, OutputIndexingIsBijection) {\n-  auto context = GetMlirContextForTest();\n+  auto mlir_context = GetMlirContextForTest();\n+  auto symbolic_expr_context = GetSymbolicExprContextForTest(&mlir_context);\n   TF_ASSERT_OK_AND_ASSIGN(auto module, LoadTestModule(flags.input_file));\n-  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data, GetEmitter(*module));\n+  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data,\n+                          GetEmitter(*module, symbolic_expr_context));\n   for (const auto& hero_name : flags.bijection_outputs) {\n     TF_ASSERT_OK_AND_ASSIGN(int64_t hero_index,\n                             GetHeroIndex(hero_name, *emitter_data->analysis));\n     auto indexing = emitter_data->emitter->ComputeThreadIdToOutputIndexing(\n-        hero_index, &context);\n+        hero_index, &symbolic_expr_context);\n     ASSERT_TRUE(indexing.has_value());\n     TF_ASSERT_OK(TestBijection(\n         *indexing, GetFirstArrayShape("
        },
        {
            "sha": "283012e446098ea6324f5042eed4ccbefef31099",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/test_lib.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -26,13 +26,14 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/status_macros.h\"\n \n namespace xla {\n namespace gpu {\n \n absl::StatusOr<std::unique_ptr<EmitterData>> GetEmitter(\n-    const HloModule& module) {\n+    const HloModule& module, SymbolicExprContext& symbolic_expr_context) {\n   auto data = std::make_unique<EmitterData>();\n   data->fusion = DynCast<HloFusionInstruction>(\n       module.entry_computation()->root_instruction());\n@@ -41,8 +42,7 @@ absl::StatusOr<std::unique_ptr<EmitterData>> GetEmitter(\n   data->analysis.emplace(\n       HloFusionAnalysis::Create(*data->fusion, data->device.value()));\n   PreBufferAssignmentFusionInfo info(data->analysis.value());\n-  mlir::MLIRContext ctx = GetMlirContextForTest();\n-  auto fusion_emitter = GetFusionEmitter(info, &ctx);\n+  auto fusion_emitter = GetFusionEmitter(info, &symbolic_expr_context);\n \n   auto emitter = dynamic_cast<EmitterBase*>(fusion_emitter.get());\n   TF_RET_CHECK(emitter != nullptr) << \"Expected emitter to be an EmitterBase\";\n@@ -56,5 +56,10 @@ mlir::MLIRContext GetMlirContextForTest() {\n   return mlir::MLIRContext(EmitterBase::GetDialectRegistry());\n }\n \n+SymbolicExprContext GetSymbolicExprContextForTest(\n+    mlir::MLIRContext* mlir_context) {\n+  return SymbolicExprContext(mlir_context);\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "9505fd925d9240a81951771b929215c83af32159",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/test_lib.h",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {\n@@ -39,11 +40,16 @@ struct EmitterData {\n   std::unique_ptr<EmitterBase> emitter;\n };\n absl::StatusOr<std::unique_ptr<EmitterData>> GetEmitter(\n-    const HloModule& module);\n+    const HloModule& module, SymbolicExprContext& symbolic_expr_context);\n \n // Returns an MLIR context with all the dialects needed for testing.\n mlir::MLIRContext GetMlirContextForTest();\n \n+// Returns a symbolic expression context with all the dialects needed for\n+// testing.\n+SymbolicExprContext GetSymbolicExprContextForTest(\n+    mlir::MLIRContext* mlir_context);\n+\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "3a8cec7ae4a867f2ece8ddd131d4924c875d1541",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -84,6 +84,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_absl//absl/status\",\n@@ -212,6 +213,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n@@ -410,17 +412,16 @@ cc_library(\n     ],\n     deps = [\n         \"//xla:autotuning_proto_cc\",\n-        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:triton_fusion_analysis\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n@@ -449,6 +450,7 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:hlo_module_config\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n@@ -463,12 +465,12 @@ xla_cc_test(\n         \":fusion_emitter\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:xla_internal_test_main\",\n@@ -512,14 +514,14 @@ xla_test(\n         \"//xla:error_spec\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:pattern_matcher_gmock\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n@@ -558,7 +560,6 @@ xla_test(\n         \"//xla:error_spec\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:pattern_matcher_gmock\",\n@@ -567,6 +568,7 @@ xla_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/stream_executor:device_description\",\n@@ -612,6 +614,7 @@ xla_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/stream_executor:device_description\",\n@@ -823,13 +826,13 @@ xla_test(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/service:algorithm_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -864,7 +867,6 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/hlo/testlib:filecheck\",\n@@ -877,10 +879,11 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n-        \"//xla/tests:hlo_test_base_with_mlir_context\",\n+        \"//xla/tests:hlo_test_base_with_symbolic_expr_context\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -1001,7 +1004,7 @@ xla_cc_test(\n         \":test_utils\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n-        \"//xla/tests:hlo_test_base_with_mlir_context\",\n+        \"//xla/tests:hlo_test_base_with_symbolic_expr_context\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1094,7 +1097,6 @@ xla_test(\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:triton_fusion_analysis\","
        },
        {
            "sha": "b101b465a418b5b5441ab68768fef83d4283d9de",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -94,7 +94,7 @@ absl::StatusOr<TritonWrapperResult>\n TritonFusion::GenerateTritonKernelAndWrapper(\n     const HloFusionInstruction& fusion, absl::string_view impl_fn_name,\n     const se::DeviceDescription& device_info, llvm::Module* llvm_module,\n-    mlir::MLIRContext* mlir_context) const {\n+    SymbolicExprContext* symbolic_expr_context) const {\n   const se::GpuComputeCapability& cc = device_info.gpu_compute_capability();\n   auto backend_config =\n       fusion.backend_config<GpuBackendConfig>()->fusion_backend_config();\n@@ -113,7 +113,7 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n     TF_ASSIGN_OR_RETURN(triton_wrapper_result,\n                         TritonWrapper(impl_fn_name, &fusion, cc, device_info,\n                                       launch_config->block_level_parameters,\n-                                      llvm_module, *mlir_context));\n+                                      llvm_module, *symbolic_expr_context));\n   } else {  // Must be a MatMul\n     CHECK_EQ(fusion_kind, kTritonGemmFusionKind);\n     // TODO(bchetioui): port matmul emitter to fully use the new\n@@ -130,10 +130,10 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n       block_level_parameters.num_warps = triton_config.num_warps();\n     }\n \n-    TF_ASSIGN_OR_RETURN(\n-        triton_wrapper_result,\n-        TritonWrapper(impl_fn_name, &fusion, cc, device_info,\n-                      block_level_parameters, llvm_module, *mlir_context));\n+    TF_ASSIGN_OR_RETURN(triton_wrapper_result,\n+                        TritonWrapper(impl_fn_name, &fusion, cc, device_info,\n+                                      block_level_parameters, llvm_module,\n+                                      *symbolic_expr_context));\n   }\n \n   return triton_wrapper_result;\n@@ -164,10 +164,10 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n \n     TF_ASSIGN_OR_RETURN(\n         TritonWrapperResult triton_wrapper_result,\n-        GenerateTritonKernelAndWrapper(fusion, impl_fn_name,\n-                                       ir_emitter_context.gpu_device_info(),\n-                                       ir_emitter_context.llvm_module(),\n-                                       ir_emitter_context.mlir_context()));\n+        GenerateTritonKernelAndWrapper(\n+            fusion, impl_fn_name, ir_emitter_context.gpu_device_info(),\n+            ir_emitter_context.llvm_module(),\n+            ir_emitter_context.symbolic_expr_context()));\n \n     auto backend_config =\n         fusion.backend_config<GpuBackendConfig>()->fusion_backend_config();"
        },
        {
            "sha": "af054dd5eaeb54dc1069289316df66d3ff950ddb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/Module.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/codegen/tiling/tiled_hlo_computation.h\"\n@@ -58,7 +57,7 @@ class TritonFusion : public FusionInterface {\n   absl::StatusOr<TritonWrapperResult> GenerateTritonKernelAndWrapper(\n       const HloFusionInstruction& fusion, absl::string_view impl_fn_name,\n       const se::DeviceDescription& device_info, llvm::Module* llvm_module,\n-      mlir::MLIRContext* mlir_context) const;\n+      SymbolicExprContext* symbolic_expr_context) const;\n \n  private:\n   const HloFusionAnalysis& analysis_;"
        },
        {
            "sha": "9863d8b2b2325046779feffcfb6f2bff824d9f09",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 13,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -126,6 +126,7 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/llvm_gpu_backend/nvptx_libdevice_path.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/triton_emitter_constraints.h\"\n #include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -1726,15 +1727,16 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n                          const se::DeviceDescription& device_info,\n                          const HloFusionInstruction* fusion,\n                          mlir::FunctionOpInterface fn,\n-                         const BlockLevelParameters& block_level_parameters) {\n+                         const BlockLevelParameters& block_level_parameters,\n+                         SymbolicExprContext* symbolic_expr_context) {\n   if (VLOG_IS_ON(6)) {\n     VLOG(6) << \"Emitting Triton IR for fusion\\n\"\n             << ExtractInstructionIntoNewModule(*fusion)->ToString();\n   }\n   const HloComputation* computation = fusion->fused_instructions_computation();\n   SymbolicTileAnalysisOrError symbolic_tile_analysis_or =\n       SymbolicTileAnalysis::AnalyzeComputation(\n-          *computation, builder.getContext(),\n+          *computation, symbolic_expr_context,\n           TritonEmitterConstraints::GetBuilder(device_info));\n   if (std::holds_alternative<FusionDecision>(symbolic_tile_analysis_or)) {\n     return Internal(\n@@ -1952,11 +1954,13 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::MLIRContext& mlir_context) {\n-  TF_ASSIGN_OR_RETURN(\n-      auto triton_module,\n-      ir_emitter_triton_internal::EmitXTileModule(\n-          fn_name, fusion, device_info, block_level_parameters, mlir_context));\n+    SymbolicExprContext& symbolic_expr_context) {\n+  // TODO: b/451959933 - Use reference or check pointer.\n+  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n+  TF_ASSIGN_OR_RETURN(auto triton_module,\n+                      ir_emitter_triton_internal::EmitXTileModule(\n+                          fn_name, fusion, device_info, block_level_parameters,\n+                          symbolic_expr_context));\n \n   const HloComputation* hlo_computation =\n       fusion->fused_instructions_computation();\n@@ -2016,12 +2020,14 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& gpu_cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, mlir::MLIRContext& mlir_context) {\n+    llvm::Module* llvm_module, SymbolicExprContext& symbolic_expr_context) {\n+  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   TF_RETURN_IF_ERROR(CheckAtLeastAmpere(gpu_cc));\n \n-  TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n-                      CreateTritonModule(fn_name, fusion, device_info,\n-                                         block_level_parameters, mlir_context));\n+  TF_ASSIGN_OR_RETURN(\n+      mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n+      CreateTritonModule(fn_name, fusion, device_info, block_level_parameters,\n+                         symbolic_expr_context));\n \n   VLOG(3) << fusion->ToString(HloPrintOptions::ShortParsable());\n   VLOG(3) << fusion->fused_instructions_computation()->ToString(\n@@ -2232,7 +2238,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::MLIRContext& mlir_context) {\n+    SymbolicExprContext& symbolic_expr_context) {\n+  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   LoadMlirDialectsForTriton(mlir_context);\n   const auto debug_options = fusion->GetModule()->config().debug_options();\n \n@@ -2299,7 +2306,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n              fusion_kind == kTritonNestedGemmFusionKind ||\n              fusion_kind == kTritonScaledDotFusionKind) {\n     TF_RETURN_IF_ERROR(EmitGeneric(b, libdevice_path, device_info, fusion, fn,\n-                                   block_level_parameters));\n+                                   block_level_parameters,\n+                                   &symbolic_expr_context));\n   } else {\n     return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n   }"
        },
        {
            "sha": "d0fe7d95c665348be6bd0fe1d260fce92e4d8fdd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n@@ -76,15 +77,15 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, mlir::MLIRContext& mlir_context);\n+    llvm::Module* llvm_module, SymbolicExprContext& symbolic_expr_context);\n \n // Creates the initial Triton module for the given fusion. Visible for testing,\n // use TritonWrapper instead.\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::MLIRContext& mlir_context);\n+    SymbolicExprContext& symbolic_expr_context);\n \n // Compiles a given Triton module to LLVM IR.\n // If `emit_kernels` is false, then the function skips emitting\n@@ -152,7 +153,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::MLIRContext& mlir_context);\n+    SymbolicExprContext& symbolic_expr_context);\n \n // This function lowers the shared dialect module to Triton. It is exposed for\n // testing with the same motivation as EmitXTileModule."
        },
        {
            "sha": "68e7bac27be34efa830ebd88983e6bfb455f3b97",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -46,6 +46,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n #include \"xla/service/pattern_matcher.h\"\n@@ -127,9 +128,9 @@ class TritonTest : public GpuCodegenTest {\n   GetModuleAndNestedFusionMetadata(absl::string_view hlo_text) {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<VerifiedHloModule> module,\n                         ParseAndReturnVerifiedModule(hlo_text));\n-    TF_ASSIGN_OR_RETURN(\n-        bool fusion_was_nested,\n-        NestGemmFusion(device_desc(), &mlir_context_).Run(module.get()));\n+    TF_ASSIGN_OR_RETURN(bool fusion_was_nested,\n+                        NestGemmFusion(device_desc(), &symbolic_expr_context_)\n+                            .Run(module.get()));\n     if (!fusion_was_nested) {\n       return absl::InternalError(\"Failed to nest the GEMM fusion.\");\n     }\n@@ -152,6 +153,7 @@ class TritonTest : public GpuCodegenTest {\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n class TritonGemmTest : public TritonTest {\n@@ -513,7 +515,7 @@ ENTRY entry {\n       module1_and_metadata.computation->FusionInstruction());\n   EXPECT_THAT(TritonWrapper(\"test_fn\", fusion1, cc, device_info,\n                             module1_and_metadata.block_level_parameters,\n-                            &llvm_module, mlir_context),\n+                            &llvm_module, symbolic_expr_context_),\n               absl_testing::StatusIs(\n                   tsl::error::RESOURCE_EXHAUSTED,\n                   ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n@@ -529,7 +531,7 @@ ENTRY entry {\n       const auto result,\n       TritonWrapper(\"test_fn\", fusion2, cc, device_info,\n                     module2_and_metadata.block_level_parameters, &llvm_module,\n-                    mlir_context));\n+                    symbolic_expr_context_));\n   // Use optin shared memory which is > shared_memory_per_block.\n   EXPECT_GT(result.shmem_bytes, device_info.shared_memory_per_block());\n }\n@@ -859,7 +861,7 @@ ENTRY entry {\n       module1_and_metadata.computation->FusionInstruction());\n   EXPECT_THAT(TritonWrapper(\"test_fn\", fusion1, cc, device_info,\n                             module1_and_metadata.block_level_parameters,\n-                            &llvm_module, mlir_context),\n+                            &llvm_module, symbolic_expr_context_),\n               absl_testing::StatusIs(tsl::error::RESOURCE_EXHAUSTED,\n                                      \"Tiling complexity heuristic exceeded\"));\n \n@@ -873,7 +875,7 @@ ENTRY entry {\n \n   TF_EXPECT_OK(TritonWrapper(\"test_fn\", fusion2, cc, device_info,\n                              module2_and_metadata.block_level_parameters,\n-                             &llvm_module, mlir_context)\n+                             &llvm_module, symbolic_expr_context_)\n                    .status());\n }\n \n@@ -2022,7 +2024,7 @@ ENTRY e {\n       TritonWrapper(\"test_fn\", triton_dot_fusion, GpuComputeCapability(),\n                     dev_info,\n                     optin_shmem_module_and_metadata.block_level_parameters,\n-                    &llvm_module, mlir_context));\n+                    &llvm_module, symbolic_expr_context_));\n   // The config is chosen so that the used memory size is slightly above the\n   // 48 kB boundary of standard / opt-in shared memory so that any GPU that\n   // has the opt-in one should be able to execute the test."
        },
        {
            "sha": "3d2d400bcbc4fc33c615ab73a9710dec2a344d5a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 16,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -697,6 +698,7 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   auto backend_config_or =\n       triton_dot_fusion->backend_config<GpuBackendConfig>();\n@@ -721,12 +723,12 @@ ENTRY entry {\n   block_level_parameters.num_stages = 4;\n   block_level_parameters.num_warps = 8;\n \n-  EXPECT_THAT(\n-      TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(), dev_info,\n-                    block_level_parameters, &llvm_module, mlir_context),\n-      absl_testing::StatusIs(\n-          tsl::error::RESOURCE_EXHAUSTED,\n-          ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n+  EXPECT_THAT(TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(),\n+                            dev_info, block_level_parameters, &llvm_module,\n+                            symbolic_expr_context),\n+              absl_testing::StatusIs(\n+                  tsl::error::RESOURCE_EXHAUSTED,\n+                  ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n \n   config.set_block_m(64);\n   config.set_block_n(128);\n@@ -737,7 +739,8 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(\n       const auto result,\n       TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(), dev_info,\n-                    block_level_parameters, &llvm_module, mlir_context));\n+                    block_level_parameters, &llvm_module,\n+                    symbolic_expr_context));\n   // Use optin shared memory which is > shared_memory_per_block.\n   EXPECT_GT(result.shmem_bytes, dev_info.shared_memory_per_block());\n }\n@@ -1292,6 +1295,7 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   auto backend_config_or =\n       triton_dot_fusion->backend_config<GpuBackendConfig>();\n@@ -1315,12 +1319,12 @@ ENTRY entry {\n   block_level_parameters.num_ctas = 1;\n   block_level_parameters.num_stages = 1;\n   block_level_parameters.num_warps = 2;\n-  EXPECT_THAT(\n-      TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(), dev_info,\n-                    block_level_parameters, &llvm_module, mlir_context),\n-      absl_testing::StatusIs(\n-          tsl::error::RESOURCE_EXHAUSTED,\n-          \"Tiling complexity heuristic exceeded: 147456 > 9000\"));\n+  EXPECT_THAT(TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(),\n+                            dev_info, block_level_parameters, &llvm_module,\n+                            symbolic_expr_context),\n+              absl_testing::StatusIs(\n+                  tsl::error::RESOURCE_EXHAUSTED,\n+                  \"Tiling complexity heuristic exceeded: 147456 > 9000\"));\n \n   // Succeeds if the tiling is not too complex.\n   config.set_block_m(32);\n@@ -1330,7 +1334,7 @@ ENTRY entry {\n \n   TF_ASSERT_OK(TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(),\n                              dev_info, block_level_parameters, &llvm_module,\n-                             mlir_context)\n+                             symbolic_expr_context)\n                    .status());\n }\n \n@@ -1868,6 +1872,7 @@ ENTRY e  {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto gpu_config, triton_dot_fusion->backend_config<GpuBackendConfig>());\n@@ -1880,7 +1885,7 @@ ENTRY e  {\n \n   TF_ASSERT_OK(TritonWrapper(\"test_fn\", triton_dot_fusion, GpuComputeComp(),\n                              dev_info, block_level_parameters, &llvm_module,\n-                             mlir_context)\n+                             symbolic_expr_context)\n                    .status());\n }\n \n@@ -3028,6 +3033,7 @@ ENTRY e {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto gpu_config, triton_dot_fusion->backend_config<GpuBackendConfig>());\n@@ -3040,7 +3046,8 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(\n       const auto result,\n       TritonWrapper(\"test_fn\", triton_dot_fusion, GpuComputeComp(), dev_info,\n-                    block_level_parameters, &llvm_module, mlir_context));\n+                    block_level_parameters, &llvm_module,\n+                    symbolic_expr_context));\n   // The config is chosen so that the used memory size is slightly above the\n   // 48 kB boundary of standard / optin shared memory so that any GPU that\n   // has the optin one should be able to execute the test."
        },
        {
            "sha": "9d791048577b01d408f4e2d501600333873a09ad",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -54,6 +54,7 @@ limitations under the License.\n #include \"xla/service/algorithm_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n@@ -1363,13 +1364,14 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", triton_fusion,\n                     se::CudaComputeCapability{se::CudaComputeCapability::kVolta,\n                                               /*minor=*/0},\n                     dev_info, BlockLevelParameters(), &llvm_module,\n-                    mlir_context),\n+                    symbolic_expr_context),\n       absl_testing::StatusIs(\n           absl::StatusCode::kFailedPrecondition,\n           ::testing::HasSubstr(\"Triton support is only enabled for Ampere GPUs \"\n@@ -1421,6 +1423,7 @@ ENTRY entry_computation {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   BlockLevelParameters block_level_parameters;\n   block_level_parameters.output_tile_sizes = {{1024, 1}};\n@@ -1431,7 +1434,8 @@ ENTRY entry_computation {\n   // 1048576.\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", triton_fusion, compute_capability, dev_info,\n-                    block_level_parameters, &llvm_module, mlir_context),\n+                    block_level_parameters, &llvm_module,\n+                    symbolic_expr_context),\n       absl_testing::StatusIs(\n           absl::StatusCode::kInvalidArgument,\n           ::testing::HasSubstr(\"Tiling does not satisfy constraints.\")));\n@@ -3931,6 +3935,7 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   std::vector<std::string> paths;\n   std::string triton_passes_log;\n \n@@ -3939,7 +3944,7 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n       TestGpuDeviceInfo::AMDMI210DeviceInfo();\n   TF_ASSERT_OK(TritonWrapper(\n       \"test_fn\", triton_fusion, se::RocmComputeCapability(\"gfx942\"), dev_info,\n-      BlockLevelParameters(), &llvm_module, mlir_context));\n+      BlockLevelParameters(), &llvm_module, symbolic_expr_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));\n   EXPECT_EQ(paths.size(), 1);\n@@ -3955,7 +3960,7 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n       TestGpuDeviceInfo::AMDRX7900DeviceInfo();\n   TF_ASSERT_OK(TritonWrapper(\n       \"test_fn\", triton_fusion, se::RocmComputeCapability(\"gfx1100\"),\n-      dev_info_n, BlockLevelParameters(), &llvm_module, mlir_context));\n+      dev_info_n, BlockLevelParameters(), &llvm_module, symbolic_expr_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));\n   EXPECT_EQ(paths.size(), 1);"
        },
        {
            "sha": "c486caa65d84007bfca263bba22a67a27c818b6d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n@@ -87,12 +88,13 @@ ENTRY e {\n   auto* fusion = Cast<HloFusionInstruction>(\n       module->entry_computation()->root_instruction());\n \n-  mlir::MLIRContext context;\n+  mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto triton_module,\n       CreateTritonModule(\"triton_fn\", fusion,\n                          TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         BlockLevelParameters(), context));\n+                         BlockLevelParameters(), symbolic_expr_context));\n \n   std::string annotated_ir = DumpTritonIR(triton_module.get(), true);\n \n@@ -137,14 +139,16 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   BlockLevelParameters block_level_parameters;\n   block_level_parameters.output_tile_sizes = {{1, 1}};\n   block_level_parameters.num_warps = 0;\n \n   EXPECT_THAT(TritonWrapper(\"test_fn\", triton_fusion,\n                             se::CudaComputeCapability::Hopper(), dev_info,\n-                            block_level_parameters, &llvm_module, mlir_context),\n+                            block_level_parameters, &llvm_module,\n+                            symbolic_expr_context),\n               absl_testing::StatusIs(\n                   absl::StatusCode::kFailedPrecondition,\n                   ::testing::HasSubstr("
        },
        {
            "sha": "8aeee22e9c3bc53d85f6b6c0bc218e116ea5417f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_int4_device_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -111,7 +112,8 @@ class TritonTest : public GpuCodegenTest {\n     emitter_opts->Add(\n         DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n     absl::StatusOr<bool> nested_or =\n-        NestGemmFusion(device_desc(), &mlir_context_).Run(module.get());\n+        NestGemmFusion(device_desc(), &symbolic_expr_context_)\n+            .Run(module.get());\n     if (!nested_or.ok()) {\n       return ::testing::AssertionFailure() << nested_or.status().message();\n     }\n@@ -152,6 +154,7 @@ class TritonTest : public GpuCodegenTest {\n     return backend().default_stream_executor()->GetDeviceDescription();\n   }\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n // The following tests are for the channel and subchannel dequantization"
        },
        {
            "sha": "fe96644f6875c72d8d083230fba6da5118ce27bd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n-#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n+#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -37,7 +37,7 @@ namespace {\n // emitter becomes a reality.\n // *****************************************************************************\n \n-using XTileDialectTest = HloTestBaseWithMlirContext;\n+using XTileDialectTest = HloTestBaseWithSymbolicExprContext;\n \n TEST_F(XTileDialectTest, TestEmittingStableHloTranspose) {\n   constexpr absl::string_view kHloText = R\"("
        },
        {
            "sha": "27e3ca4cc1ce0187ead41ebba25eef5d4a6d6962",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_stub.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n@@ -49,15 +50,15 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, mlir::MLIRContext& mlir_context) {\n+    llvm::Module* llvm_module, SymbolicExprContext& symbolic_expr_context) {\n   return absl::UnimplementedError(\"not supported for this build configuration\");\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::MLIRContext& mlir_context) {\n+    SymbolicExprContext& symbolic_expr_context) {\n   return absl::UnimplementedError(\"not supported for this build configuration\");\n }\n "
        },
        {
            "sha": "fe842129d3d2dc15dc812ca039009f98910bc230",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -26,19 +26,24 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n \n namespace xla::gpu {\n namespace {\n \n TEST(TritonStub, CallStubApi) {\n-  mlir::MLIRContext context;\n+  mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n-  LoadMlirDialectsForTriton(context);\n-  EXPECT_FALSE(TritonWrapper({}, nullptr, {}, {}, {}, nullptr, context).ok());\n-  EXPECT_FALSE(CreateTritonModule({}, nullptr, {}, {}, context).ok());\n+  LoadMlirDialectsForTriton(mlir_context);\n+  EXPECT_FALSE(\n+      TritonWrapper({}, nullptr, {}, {}, {}, nullptr, symbolic_expr_context)\n+          .ok());\n+  EXPECT_FALSE(\n+      CreateTritonModule({}, nullptr, {}, {}, symbolic_expr_context).ok());\n   EXPECT_FALSE(CompileTritonToLLVM(\"\", HloModule(\"test\", HloModuleConfig()), {},\n-                                   {}, {}, nullptr, context,\n+                                   {}, {}, nullptr, mlir_context,\n                                    /*is_xla_fusion=*/true, {})\n                    .ok());\n "
        },
        {
            "sha": "54cda36fe4f2e8193ea44fcbec50d6f58b72a2a9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"tsl/platform/status_matchers.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -67,8 +68,9 @@ ENTRY entry_computation {\n   HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n \n   mlir::MLIRContext mlir_context;\n-  std::unique_ptr<FusionInterface> emitter =\n-      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis}, &mlir_context);\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n+  std::unique_ptr<FusionInterface> emitter = GetFusionEmitter(\n+      PreBufferAssignmentFusionInfo{analysis}, &symbolic_expr_context);\n   auto triton_fusion = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion, nullptr);\n   std::optional<TritonFusion::LaunchConfig> launch_config =\n@@ -105,16 +107,17 @@ ENTRY entry_computation {\n   HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n \n   mlir::MLIRContext mlir_context;\n-  std::unique_ptr<FusionInterface> emitter =\n-      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis}, &mlir_context);\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n+  std::unique_ptr<FusionInterface> emitter = GetFusionEmitter(\n+      PreBufferAssignmentFusionInfo{analysis}, &symbolic_expr_context);\n   auto triton_fusion_emitter = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion_emitter, nullptr);\n   EXPECT_EQ(triton_fusion_emitter->launch_config(), std::nullopt);\n \n   // Ensure that the emitter fails gracefully when the launch config is not set.\n   EXPECT_THAT(triton_fusion_emitter->GenerateTritonKernelAndWrapper(\n                   *::xla::Cast<HloFusionInstruction>(root), \"random_name\",\n-                  device_info, /*llvm_module=*/nullptr, &mlir_context),\n+                  device_info, /*llvm_module=*/nullptr, &symbolic_expr_context),\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n "
        },
        {
            "sha": "eeb7a0bee679d38578704250247bc12469548d8c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -129,7 +129,7 @@ ENTRY e {\n       EXPECT_THAT(\n           TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n                         dev_info, block_level_parameters, &llvm_module_,\n-                        mlir_context_),\n+                        symbolic_expr_context_),\n           absl_testing::StatusIs(\n               absl::StatusCode::kInternal,\n               ::testing::HasSubstr(\"Failed to compile Triton kernel\")));\n@@ -307,7 +307,7 @@ ENTRY e {\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n                     dev_info, block_level_parameters, &llvm_module_,\n-                    mlir_context_),\n+                    symbolic_expr_context_),\n       absl_testing::StatusIs(\n           absl::StatusCode::kInternal,\n           ::testing::HasSubstr(\"Failed to verify Triton module for fusion\")));\n@@ -351,7 +351,7 @@ ENTRY e {\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n                     dev_info, block_level_parameters, &llvm_module_,\n-                    mlir_context_),\n+                    symbolic_expr_context_),\n       absl_testing::StatusIs(absl::StatusCode::kInternal,\n                              ::testing::HasSubstr(\"num_batch_dims <= 1\")));\n }"
        },
        {
            "sha": "3efd266574816824320733c01d82988399427c02",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -270,7 +270,7 @@ class TritonSupportTest : public TritonSupportTestBase {\n     auto run_triton_codegen = [&]() {\n       return TritonWrapper(\"test_fn\", &ti.TritonFusion(), cc, dev_info,\n                            block_level_parameters, &llvm_module_,\n-                           mlir_context_);\n+                           symbolic_expr_context_);\n     };\n \n     if (IsTritonSupportedInstruction(ti.Instruction(), cc)) {"
        },
        {
            "sha": "f66c20097ab959db96c3c21ca390a2f985aaf3d5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -56,7 +56,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n+#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n@@ -119,12 +119,13 @@ absl::Status CreateTritonIrAndFileCheck(\n     absl::string_view filecheck_pattern) {\n   auto* fusion = Cast<HloFusionInstruction>(computation.FusionInstruction());\n \n-  mlir::MLIRContext context;\n+  mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_ASSIGN_OR_RETURN(\n       mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n       CreateTritonModule(\"triton_fn\", fusion,\n                          TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         block_level_parameters, context));\n+                         block_level_parameters, symbolic_expr_context));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);\n@@ -138,7 +139,7 @@ absl::Status CreateTritonIrAndFileCheck(\n \n absl::StatusOr<\n     std::pair<mlir::OwningOpRef<mlir::ModuleOp>, std::unique_ptr<HloModule>>>\n-CreateXTileIrAndFileCheck(HloTestBaseWithMlirContext* test,\n+CreateXTileIrAndFileCheck(HloTestBaseWithSymbolicExprContext* test,\n                           absl::string_view hlo_text,\n                           absl::string_view triton_fusion_name,\n                           absl::string_view filecheck_pattern) {\n@@ -161,7 +162,7 @@ CreateXTileIrAndFileCheck(HloTestBaseWithMlirContext* test,\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n-    HloTestBaseWithMlirContext* test, const HloComputation& computation,\n+    HloTestBaseWithSymbolicExprContext* test, const HloComputation& computation,\n     const BlockLevelParameters& block_level_parameters,\n     absl::string_view filecheck_pattern) {\n   auto* fusion = Cast<HloFusionInstruction>(computation.FusionInstruction());\n@@ -170,7 +171,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n       mlir::OwningOpRef<mlir::ModuleOp> xtile_dialect_module,\n       ir_emitter_triton_internal::EmitXTileModule(\n           \"xtile_dialect_fn\", fusion, TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-          block_level_parameters, *test->mlir_context()));\n+          block_level_parameters, *test->symbolic_expr_context()));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);\n@@ -183,10 +184,12 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n }\n \n absl::Status LowerXTileIrToTritonAndFileCheck(\n-    HloTestBaseWithMlirContext* test, mlir::ModuleOp xtile_dialect_module,\n-    absl::string_view filecheck_pattern, const HloFusionInstruction& fusion) {\n+    HloTestBaseWithSymbolicExprContext* test,\n+    mlir::ModuleOp xtile_dialect_module, absl::string_view filecheck_pattern,\n+    const HloFusionInstruction& fusion) {\n   TF_RETURN_IF_ERROR(ir_emitter_triton_internal::LowerXTileToTriton(\n-      xtile_dialect_module, *test->mlir_context(), fusion));\n+      xtile_dialect_module, *test->symbolic_expr_context()->GetMLIRContext(),\n+      fusion));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);"
        },
        {
            "sha": "4e9742b47b6cb1d3311eef93366d555ec9f80384",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.h",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -39,9 +39,10 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n+#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -73,7 +74,7 @@ absl::Status CreateTritonIrAndFileCheck(\n // `filecheck_pattern`.\n absl::StatusOr<\n     std::pair<mlir::OwningOpRef<mlir::ModuleOp>, std::unique_ptr<HloModule>>>\n-CreateXTileIrAndFileCheck(HloTestBaseWithMlirContext* test,\n+CreateXTileIrAndFileCheck(HloTestBaseWithSymbolicExprContext* test,\n                           absl::string_view hlo_text,\n                           absl::string_view triton_fusion_name,\n                           absl::string_view filecheck_pattern);\n@@ -82,15 +83,16 @@ CreateXTileIrAndFileCheck(HloTestBaseWithMlirContext* test,\n // This function also checks the generated shared dialect IR against the\n // `filecheck_pattern`.\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n-    HloTestBaseWithMlirContext* test, const HloComputation& computation,\n+    HloTestBaseWithSymbolicExprContext* test, const HloComputation& computation,\n     const BlockLevelParameters& block_level_parameters,\n     absl::string_view filecheck_pattern);\n \n // Lowers the given shared dialect IR to Triton IR and checks the result against\n // the `filecheck_pattern`.\n absl::Status LowerXTileIrToTritonAndFileCheck(\n-    HloTestBaseWithMlirContext* test, mlir::ModuleOp xtile_dialect_module,\n-    absl::string_view filecheck_pattern, const HloFusionInstruction& fusion);\n+    HloTestBaseWithSymbolicExprContext* test,\n+    mlir::ModuleOp xtile_dialect_module, absl::string_view filecheck_pattern,\n+    const HloFusionInstruction& fusion);\n \n absl::Status CreateTritonIrAndFileCheckForDot(\n     HloTestBase* test, absl::string_view hlo_text,\n@@ -166,6 +168,7 @@ class TritonSupportTestBase : public HloTestBase {\n   llvm::LLVMContext llvm_ctx_;\n   llvm::Module llvm_module_{\"module\", llvm_ctx_};\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   TritonGemmConfig config_{16, 32, 512, 1, 4, 8};\n };\n "
        },
        {
            "sha": "3dd20af7debfd82ed0765c43d4f031dc837db217",
            "filename": "third_party/xla/xla/codegen/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -47,6 +47,7 @@ xla_cc_test(\n         \"//xla/backends/gpu/codegen/triton:fusion_emitter\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n@@ -117,6 +118,7 @@ cc_library(\n     deps = [\n         \":kernel_source\",\n         \"//xla:util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//llvm:Support\","
        },
        {
            "sha": "885218eaa16b05ff2f9ec8856104c81b3925409f",
            "filename": "third_party/xla/xla/codegen/emitter_loc_op_builder_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"tsl/platform/status_matchers.h\"\n \n@@ -44,9 +45,10 @@ using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n \n class EmitterLocOpBuilderTest : public HloHardwareIndependentTestBase {\n  protected:\n-  void SetUp() override { gpu::LoadMlirDialectsForTriton(context_); }\n+  void SetUp() override { gpu::LoadMlirDialectsForTriton(mlir_context_); }\n \n-  mlir::MLIRContext context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+  mlir::MLIRContext mlir_context_;\n };\n \n NameLoc NameLoc(mlir::MLIRContext& context, absl::string_view name) {\n@@ -65,9 +67,9 @@ mlir::OwningOpRef<mlir::ModuleOp> MakeModuleWithOneOp(\n }\n \n TEST_F(EmitterLocOpBuilderTest, IRWithAnnotations) {\n-  auto loc = NameLoc(context_, \"IRWithAnnotations\");\n-  EmitterLocOpBuilder b(loc, &context_, /*annotate_loc=*/true);\n-  auto triton_module = MakeModuleWithOneOp(context_, b);\n+  auto loc = NameLoc(mlir_context_, \"IRWithAnnotations\");\n+  EmitterLocOpBuilder b(loc, &mlir_context_, /*annotate_loc=*/true);\n+  auto triton_module = MakeModuleWithOneOp(mlir_context_, b);\n   std::string ir = DumpTritonIR(triton_module.get(), /*dump_annotations=*/true);\n   if constexpr (EmitterLocOpBuilder::kSourceLocationSupported) {\n     EXPECT_THAT(RunFileCheck(ir, R\"(\n@@ -83,9 +85,9 @@ TEST_F(EmitterLocOpBuilderTest, IRWithAnnotations) {\n }\n \n TEST_F(EmitterLocOpBuilderTest, IRWithoutAnnotations) {\n-  auto loc = NameLoc(context_, \"IRWithoutAnnotations\");\n-  EmitterLocOpBuilder b(loc, &context_, /*annotate_loc=*/false);\n-  auto triton_module = MakeModuleWithOneOp(context_, b);\n+  auto loc = NameLoc(mlir_context_, \"IRWithoutAnnotations\");\n+  EmitterLocOpBuilder b(loc, &mlir_context_, /*annotate_loc=*/false);\n+  auto triton_module = MakeModuleWithOneOp(mlir_context_, b);\n   std::string ir =\n       DumpTritonIR(triton_module.get(), /*dump_annotations=*/false);\n   EXPECT_THAT(RunFileCheck(ir, R\"("
        },
        {
            "sha": "6adede7bed2db31738d70d91926d8ec4379cad38",
            "filename": "third_party/xla/xla/codegen/emitters/BUILD",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -24,6 +24,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -49,6 +50,7 @@ xla_cc_test(\n         \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//mlir:FuncDialect\",\n@@ -95,6 +97,7 @@ cc_library(\n         \"//xla/mlir_hlo\",\n         \"//xla/mlir_hlo:map_mhlo_to_scalar_op\",\n         \"//xla/service:algorithm_util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -138,6 +141,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/mlir_hlo\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n@@ -255,6 +259,7 @@ cc_library(\n         \"//xla/runtime:work_group\",\n         \"//xla/runtime:work_item\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -284,6 +289,7 @@ xla_cc_test(\n         \"//xla/runtime:work_group\",\n         \"//xla/runtime:work_item\",\n         \"//xla/runtime:work_tile_size\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:AffineDialect\",\n         \"@llvm-project//mlir:IR\",\n@@ -312,9 +318,9 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/runtime:work_dimensions\",\n-        \"//xla/runtime:work_group\",\n         \"//xla/runtime:work_item\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -369,6 +375,7 @@ cc_library(\n         \"//xla/runtime:work_dimensions\",\n         \"//xla/runtime:work_item\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -429,6 +436,7 @@ cc_library(\n         \"//xla/runtime:work_dimensions\",\n         \"//xla/runtime:work_item\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "c21bc59283765a3a3fb8123ddbdde587565133a7",
            "filename": "third_party/xla/xla/codegen/emitters/computation_partitioner.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 16,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -50,6 +50,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -67,7 +68,8 @@ const Shape& TupleShape(const Shape& shape, int index) {\n }\n \n std::vector<IndexingMapSet> ComputeOperandIndexingMaps(\n-    const HloInstruction* instr, mlir::MLIRContext* mlir_context) {\n+    const HloInstruction* instr,\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   std::vector<IndexingMapSet> indexing_maps_per_operand;\n   // For some ops, there is no indexing map implemented for the operands (e.g.\n   // scatter) or there are multiple results and the common iteration space is\n@@ -77,12 +79,12 @@ std::vector<IndexingMapSet> ComputeOperandIndexingMaps(\n     int64_t num_operands = instr->operand_count();\n     indexing_maps_per_operand.reserve(num_operands);\n     for (int64_t i = 0; i < num_operands; ++i) {\n-      indexing_maps_per_operand.push_back(\n-          {CreateIdentityMap(instr->operand(i)->shape(), mlir_context)});\n+      indexing_maps_per_operand.push_back({CreateIdentityMap(\n+          instr->operand(i)->shape(), symbolic_expr_context)});\n     }\n   } else {\n-    auto operands_indexing =\n-        ComputeOutputToInputIndexing(instr, /*output_id=*/0, mlir_context);\n+    auto operands_indexing = ComputeOutputToInputIndexing(\n+        instr, /*output_id=*/0, symbolic_expr_context);\n     operands_indexing.Simplify();\n     indexing_maps_per_operand.reserve(operands_indexing.indexing_maps.size());\n     for (auto& indexing_maps : operands_indexing.indexing_maps) {\n@@ -104,15 +106,15 @@ bool HasNoCompute(const HloInstruction* instr) {\n \n EpilogueSpecification EpilogueSpecification::FromIdentityIndexing(\n     const HloInstruction* hero, const HloInstruction* root,\n-    mlir::MLIRContext* mlir_context) {\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   EpilogueSpecification result;\n   if (root->shape().IsArray()) {\n     absl::c_copy(root->shape().dimensions(),\n                  std::back_inserter(result.index_ranges));\n   }\n   result.roots.push_back(root);\n   result.root_indexing.push_back(\n-      CreateIdentityMap(root->shape(), mlir_context));\n+      CreateIdentityMap(root->shape(), symbolic_expr_context));\n   result.heroes.push_back(hero);\n   return result;\n }\n@@ -203,7 +205,8 @@ struct HloSubgraphData {\n };\n \n PartitionedComputation::PartitionedComputation(\n-    const HloComputation* computation, mlir::MLIRContext* mlir_context,\n+    const HloComputation* computation,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n     std::function<bool(const HloInstruction*)> is_subgraph_root)\n     : computation_(computation) {\n   CHECK_NE(computation, nullptr);\n@@ -251,7 +254,8 @@ PartitionedComputation::PartitionedComputation(\n       instr_subgraph_data.indexings.clear();\n       num_ops_per_subgraph.push_back(1);\n     }\n-    auto operands_indexing = ComputeOperandIndexingMaps(instr, mlir_context);\n+    auto operands_indexing =\n+        ComputeOperandIndexingMaps(instr, symbolic_expr_context);\n     // Iterate over the operands and add the func_ids of the current instruction\n     // to their HloSubgraphIndexing and compute the indexing maps.\n     for (auto [operand_instr, operand_maps] :\n@@ -306,16 +310,17 @@ PartitionedComputation::PartitionedComputation(\n             root_indexing.push_back(root_indexing.front());\n           } else {\n             // Bitcast from the first root to the target shape.\n-            root_indexing.push_back(GetBitcastMap(\n-                *first_root_shape, instruction->shape(), mlir_context));\n+            root_indexing.push_back(GetBitcastMap(*first_root_shape,\n+                                                  instruction->shape(),\n+                                                  symbolic_expr_context));\n           }\n         } else {\n           first_root_shape = &instruction->shape();\n           while (first_root_shape->IsTuple()) {\n             first_root_shape = &first_root_shape->tuple_shapes()[0];\n           }\n           root_indexing.push_back(\n-              CreateIdentityMap(*first_root_shape, mlir_context));\n+              CreateIdentityMap(*first_root_shape, symbolic_expr_context));\n         }\n       }\n     }\n@@ -388,9 +393,10 @@ PartitionedComputation::Subgraph PartitionedComputation::Subgraph::ForEpilogue(\n }\n \n PartitionedComputations::PartitionedComputations(\n-    const HloComputation* fusion, mlir::MLIRContext* mlir_context,\n+    const HloComputation* fusion,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n     std::vector<EpilogueSpecification> epilogues)\n-    : fusion_(fusion) {\n+    : fusion_(fusion), symbolic_expr_context_(symbolic_expr_context) {\n   // Collect all transitively called computations (including the fusion itself).\n   absl::flat_hash_set<const HloComputation*> seen;\n   std::vector<const HloComputation*> computations;\n@@ -422,8 +428,8 @@ PartitionedComputations::PartitionedComputations(\n   partitioned_computations_.reserve(computations.size());\n   for (auto* computation : computations) {\n     computation_to_partitioning_[computation] =\n-        &partitioned_computations_.emplace_back(\n-            PartitionedComputation{computation, mlir_context, is_root});\n+        &partitioned_computations_.emplace_back(PartitionedComputation{\n+            computation, symbolic_expr_context, is_root});\n   }\n }\n "
        },
        {
            "sha": "bd208e37541e08f6246424b5296bf53546d89feb",
            "filename": "third_party/xla/xla/codegen/emitters/computation_partitioner.h",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n@@ -39,7 +40,7 @@ struct EpilogueSpecification {\n   // Creates an epilogue with output indices matching the given root's shape.\n   static EpilogueSpecification FromIdentityIndexing(\n       const HloInstruction* hero, const HloInstruction* root,\n-      mlir::MLIRContext* mlir_context);\n+      gpu::SymbolicExprContext* symbolic_expr_context);\n \n   std::vector<const HloInstruction*> heroes;\n   std::vector<const HloInstruction*> roots;\n@@ -80,10 +81,11 @@ struct EpilogueSpecification {\n // than its users.\n class PartitionedComputation {\n  public:\n-  explicit PartitionedComputation(const HloComputation* computation,\n-                                  mlir::MLIRContext* mlir_context,\n-                                  std::function<bool(const HloInstruction*)>\n-                                      is_subgraph_root = HloPredicateFalse);\n+  explicit PartitionedComputation(\n+      const HloComputation* computation,\n+      gpu::SymbolicExprContext* symbolic_expr_context,\n+      std::function<bool(const HloInstruction*)> is_subgraph_root =\n+          HloPredicateFalse);\n \n   struct Subgraph {\n     // A unique name of the subgraph. Used for function names.\n@@ -153,7 +155,8 @@ class PartitionedComputations {\n   // Partition the given fusion computation and optionally generate an epilogue\n   // for the given heroes.\n   explicit PartitionedComputations(\n-      const HloComputation* fusion, mlir::MLIRContext* mlir_context,\n+      const HloComputation* fusion,\n+      gpu::SymbolicExprContext* symbolic_expr_context,\n       std::vector<EpilogueSpecification> epilogues = {});\n \n   const PartitionedComputation& FindPartitionedComputation(\n@@ -176,6 +179,10 @@ class PartitionedComputations {\n \n   const HloComputation* fusion() const { return fusion_; }\n \n+  gpu::SymbolicExprContext* symbolic_expr_context() const {\n+    return symbolic_expr_context_;\n+  }\n+\n   // Creates a call target lookup function for use with SubgraphToMlir.\n   CallTargetProvider CreateCallTargetProvider(\n       const absl::flat_hash_map<const PartitionedComputation::Subgraph*,\n@@ -193,6 +200,7 @@ class PartitionedComputations {\n       computation_to_partitioning_;\n   const HloComputation* fusion_;\n   std::vector<PartitionedComputation::Subgraph> epilogues_;\n+  gpu::SymbolicExprContext* symbolic_expr_context_;\n };\n \n // Returns an MLIR function declaration for the given subgraph. For subgraphs of"
        },
        {
            "sha": "32e2ff97803cc8c700835fee0ae3f46649a7315d",
            "filename": "third_party/xla/xla/codegen/emitters/computation_partitioner_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace emitters {\n@@ -39,11 +40,12 @@ using ::testing::UnorderedElementsAre;\n \n class ComputationPartitionerTest : public HloHardwareIndependentTestBase {\n  protected:\n-  ComputationPartitionerTest() {\n+  ComputationPartitionerTest() : symbolic_expr_context_(&mlir_context_) {\n     mlir_context_.loadDialect<mlir::func::FuncDialect>();\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_;\n };\n \n std::string PrintAndErase(mlir::func::FuncOp func) {\n@@ -77,7 +79,7 @@ TEST_F(ComputationPartitionerTest, PartitionDiamonds) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &mlir_context_);\n+  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n \n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n       SUBGRAPH fused_computation_add3 {\n@@ -120,7 +122,7 @@ TEST_F(ComputationPartitionerTest, SimpleConcatenate) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &mlir_context_);\n+  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n \n   EXPECT_THAT(computation.subgraphs(), SizeIs(1));\n }\n@@ -141,7 +143,7 @@ TEST_F(ComputationPartitionerTest, DiamondConcatenate) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &mlir_context_);\n+  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n \n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n       SUBGRAPH fused_computation_concat {\n@@ -172,7 +174,7 @@ TEST_F(ComputationPartitionerTest, TupleRoot) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &mlir_context_);\n+  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n       SUBGRAPH fused_computation_root {\n         %p0 = f32[6]{0} parameter(0)\n@@ -215,8 +217,9 @@ TEST_F(ComputationPartitionerTest, Epilogue) {\n       /*index_ranges=*/{1, 42},\n       {CreateIdentityMap(\n           fused_computation->root_instruction()->shape().tuple_shapes(0),\n-          &mlir_context_)}};\n-  PartitionedComputations fusion(fused_computation, &mlir_context_, {epilogue});\n+          &symbolic_expr_context_)}};\n+  PartitionedComputations fusion(fused_computation, &symbolic_expr_context_,\n+                                 {epilogue});\n \n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&mlir_context_),\n                                      &mlir_context_);\n@@ -245,7 +248,7 @@ TEST_F(ComputationPartitionerTest, TransposeAsRoot) {\n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n   PartitionedComputation computation(\n-      fusion, &mlir_context_, [](const HloInstruction* instr) {\n+      fusion, &symbolic_expr_context_, [](const HloInstruction* instr) {\n         return instr->opcode() == HloOpcode::kTranspose;\n       });\n   ASSERT_THAT(computation.subgraphs(), SizeIs(2));\n@@ -267,7 +270,7 @@ TEST_F(ComputationPartitionerTest, TransposeReverse) {\n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n   PartitionedComputation computation(\n-      fusion, &mlir_context_, [](const HloInstruction* instr) {\n+      fusion, &symbolic_expr_context_, [](const HloInstruction* instr) {\n         return instr->opcode() == HloOpcode::kTranspose;\n       });\n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n@@ -296,7 +299,7 @@ TEST_F(ComputationPartitionerTest, PartiallyMergable) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &mlir_context_);\n+  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n \n   auto transpose = fusion->GetInstructionWithName(\"transpose\");\n   auto sub = fusion->GetInstructionWithName(\"sub\");\n@@ -337,7 +340,7 @@ TEST_F(ComputationPartitionerTest, SubgraphSignatures) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &mlir_context_);\n+                                &symbolic_expr_context_);\n   EXPECT_EQ(\n       PrintAndErase(\n           CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder)),\n@@ -346,7 +349,7 @@ TEST_F(ComputationPartitionerTest, SubgraphSignatures) {\n       \"index]}) -> f32\");\n \n   PartitionedComputation add(module->GetComputationWithName(\"add\"),\n-                             &mlir_context_);\n+                             &symbolic_expr_context_);\n   EXPECT_EQ(\n       PrintAndErase(CreateSubgraphMlirFunction(add.GetRootSubgraph(), builder)),\n       \"func.func private @add_add(f32, f32) -> f32\");\n@@ -376,7 +379,7 @@ TEST_F(ComputationPartitionerTest, ConcatWithTuple) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &mlir_context_);\n+                                &symbolic_expr_context_);\n   EXPECT_THAT(fusion.subgraphs(), SizeIs(2));\n   PrintAndErase(CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder));\n }\n@@ -399,7 +402,7 @@ TEST_F(ComputationPartitionerTest, DUS) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &mlir_context_);\n+                                &symbolic_expr_context_);\n   EXPECT_THAT(fusion.subgraphs(), SizeIs(1));\n   PrintAndErase(CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder));\n }\n@@ -437,7 +440,7 @@ TEST_F(ComputationPartitionerTest, ScatterFusion) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &mlir_context_);\n+                                &symbolic_expr_context_);\n   EXPECT_THAT(fusion.subgraphs(), SizeIs(1));\n   PrintAndErase(CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder));\n }\n@@ -480,7 +483,7 @@ TEST_F(ComputationPartitionerTest, PartitioningIsDeterministic) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &mlir_context_);\n+  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n   EXPECT_EQ(computation.subgraphs().size(), 1);\n }\n "
        },
        {
            "sha": "bc7fbb3f1f089d20ddc38648ef123cb2bb307f02",
            "filename": "third_party/xla/xla/codegen/emitters/concatenate_kernel_emitter.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 18,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n #include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n@@ -64,6 +63,7 @@ limitations under the License.\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/runtime/work_item.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -74,12 +74,13 @@ limitations under the License.\n namespace xla::emitters {\n \n ConcatenateFusionKernelEmitter::ConcatenateFusionKernelEmitter(\n-    mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n-    const HloFusionSpec& fusion_spec, const BufferAssignment* buffer_assignment,\n+    gpu::SymbolicExprContext& symbolic_expr_context,\n+    const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n+    const BufferAssignment* buffer_assignment,\n     KernelArguments::BufferAlignment buffer_alignment,\n     WorkDimensions work_dimensions, absl::string_view entry_function_name,\n     BackendKind backend_kind)\n-    : mlir_context_(mlir_context),\n+    : symbolic_expr_context_(symbolic_expr_context),\n       fusion_(fusion),\n       fusion_spec_(fusion_spec),\n       buffer_assignment_(buffer_assignment),\n@@ -91,7 +92,7 @@ ConcatenateFusionKernelEmitter::ConcatenateFusionKernelEmitter(\n \n absl::StatusOr<MlirKernelDefinition>\n ConcatenateFusionKernelEmitter::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(&mlir_context_);\n+  mlir::OpBuilder builder(symbolic_expr_context_.GetMLIRContext());\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion_.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(\n       loc, absl::StrCat(fusion_.name(), \"_kernel_module\"));\n@@ -103,12 +104,13 @@ ConcatenateFusionKernelEmitter::EmitKernelDefinition() {\n       mlir::func::FuncOp entry_func,\n       emitters::EmitKernelApi(*module, fusion_, buffer_assignment_,\n                               buffer_alignment_, entry_function_name_));\n-  SetBackendKind(&mlir_context_, entry_func, backend_kind_);\n+  SetBackendKind(symbolic_expr_context_.GetMLIRContext(), entry_func,\n+                 backend_kind_);\n \n   std::vector<emitters::EpilogueSpecification> epilogues =\n-      GetEpilogues(fusion_, module->getContext());\n+      GetEpilogues(fusion_, &symbolic_expr_context_);\n   emitters::PartitionedComputations computations(\n-      fusion_.fused_instructions_computation(), module->getContext(),\n+      fusion_.fused_instructions_computation(), &symbolic_expr_context_,\n       epilogues);\n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              *module, computations));\n@@ -151,12 +153,12 @@ int ConcatenateFusionKernelEmitter::GetValidUnrollFactor(\n \n IndexingMap ConcatenateFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n     const WorkDimensions& work_dimensions, const Shape& largest_shape,\n-    mlir::MLIRContext* ctx) {\n+    gpu::SymbolicExprContext* ctx) {\n   return GetDefaultWorkItemIndexingMap(work_dimensions, largest_shape, ctx);\n }\n \n IndexingMap ConcatenateFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n-    mlir::MLIRContext* ctx) const {\n+    gpu::SymbolicExprContext* ctx) const {\n   return ComputeWorkItemIdToOutputIndexing(work_dimensions_, largest_shape_,\n                                            ctx);\n }\n@@ -169,7 +171,6 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n   const auto& root_computation = computations.FindPartitionedComputation(\n       fusion.fused_instructions_computation());\n \n-  mlir::MLIRContext* context = entry_function.getContext();\n   mlir::ImplicitLocOpBuilder builder(entry_function.getLoc(), entry_function);\n   builder.setInsertionPointToStart(entry_function.addEntryBlock());\n \n@@ -185,9 +186,11 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n   llvm::SmallVector<mlir::Value> result_tensors{output_tensor_args.begin(),\n                                                 output_tensor_args.end()};\n \n-  auto work_item_id_to_input_map = ComputeWorkItemIdToOutputIndexing(context);\n+  auto work_item_id_to_input_map =\n+      ComputeWorkItemIdToOutputIndexing(&symbolic_expr_context_);\n   auto epilogue_indexing = ComputeEpilogueInputToOutputIndexing(\n-      fusion_spec_.fusion_hero(0), fusion_spec_.fusion_root(0), context);\n+      fusion_spec_.fusion_hero(0), fusion_spec_.fusion_root(0),\n+      &symbolic_expr_context_);\n \n   const auto* concat = &fusion_spec_.fusion_hero(0).instruction();\n \n@@ -208,7 +211,7 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n     for (auto [operand_index, operand] : llvm::enumerate(concat->operands())) {\n       IndexingMap input_to_output_map =\n           ComputeInputToOutputIndexing(concat, /*input_id=*/operand_index,\n-                                       context)\n+                                       &symbolic_expr_context_)\n               .indexing_maps.front()\n               .begin()\n               ->map();\n@@ -260,7 +263,8 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n       llvm::SmallVector<mlir::OpFoldResult> offsets(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(0));\n       llvm::SmallVector<mlir::OpFoldResult> sizes =\n-          mlir::getAsIndexOpFoldResult(context, output_tensor.getShape());\n+          mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+                                       output_tensor.getShape());\n       llvm::SmallVector<mlir::OpFoldResult> strides(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(1));\n       nested_b.create<mlir::tensor::ParallelInsertSliceOp>(\n@@ -270,7 +274,7 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n \n   const NumWorkItems& num_work_items = work_dimensions_.num_work_items;\n   llvm::SmallVector<mlir::OpFoldResult> upper_bounds =\n-      mlir::getAsIndexOpFoldResult(context,\n+      mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n                                    {static_cast<int64_t>(num_work_items.x),\n                                     static_cast<int64_t>(num_work_items.y),\n                                     static_cast<int64_t>(num_work_items.z)});\n@@ -285,10 +289,11 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n \n std::vector<emitters::EpilogueSpecification>\n ConcatenateFusionKernelEmitter::GetEpilogues(\n-    const HloFusionInstruction& fusion, mlir::MLIRContext* mlir_context) const {\n+    const HloFusionInstruction& fusion,\n+    gpu::SymbolicExprContext* symbolic_expr_context) const {\n   return {emitters::EpilogueSpecification::FromIdentityIndexing(\n       &fusion_spec_.fusion_hero(0).instruction(),\n-      &fusion_spec_.fusion_root(0).instruction(), mlir_context)};\n+      &fusion_spec_.fusion_root(0).instruction(), symbolic_expr_context)};\n }\n \n }  // namespace xla::emitters"
        },
        {
            "sha": "6472ff9e42c90c3213c2910c8fd764643bfe8e0e",
            "filename": "third_party/xla/xla/codegen/emitters/concatenate_kernel_emitter.h",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -37,15 +36,16 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n \n namespace xla::emitters {\n \n class ConcatenateFusionKernelEmitter final : public MlirKernelEmitter {\n  public:\n   ConcatenateFusionKernelEmitter(\n-      mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n-      const HloFusionSpec& fusion_spec,\n+      gpu::SymbolicExprContext& symbolic_expr_context,\n+      const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n       const BufferAssignment* buffer_assignment,\n       KernelArguments::BufferAlignment buffer_alignment,\n       WorkDimensions work_dimensions, absl::string_view entry_function_name,\n@@ -55,7 +55,7 @@ class ConcatenateFusionKernelEmitter final : public MlirKernelEmitter {\n \n   static IndexingMap ComputeWorkItemIdToOutputIndexing(\n       const WorkDimensions& work_dimensions, const Shape& largest_shape,\n-      mlir::MLIRContext* ctx);\n+      gpu::SymbolicExprContext* ctx);\n \n   // Get the shape used for indexing.\n   // For concatenate, this is the largest shape.\n@@ -71,7 +71,8 @@ class ConcatenateFusionKernelEmitter final : public MlirKernelEmitter {\n   std::string name() const final { return \"concatenate_fusion_kernel_emitter\"; }\n \n  private:\n-  IndexingMap ComputeWorkItemIdToOutputIndexing(mlir::MLIRContext* ctx) const;\n+  IndexingMap ComputeWorkItemIdToOutputIndexing(\n+      gpu::SymbolicExprContext* ctx) const;\n \n   absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -81,10 +82,10 @@ class ConcatenateFusionKernelEmitter final : public MlirKernelEmitter {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      mlir::MLIRContext* mlir_context) const;\n+      gpu::SymbolicExprContext* symbolic_expr_context) const;\n \n  private:\n-  mlir::MLIRContext& mlir_context_;\n+  gpu::SymbolicExprContext& symbolic_expr_context_;\n   const HloFusionInstruction& fusion_;\n   const HloFusionSpec& fusion_spec_;\n   const BufferAssignment* buffer_assignment_;"
        },
        {
            "sha": "4c53a6a0a680e9576b6ec03e604976f24d3da79f",
            "filename": "third_party/xla/xla/codegen/emitters/dynamic_update_slice_kernel_emitter.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 20,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -39,7 +39,6 @@ limitations under the License.\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n@@ -64,6 +63,7 @@ limitations under the License.\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/runtime/work_item.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -76,12 +76,13 @@ namespace xla::emitters {\n constexpr int kDUSUpdateIndex = 1;\n \n DynamicUpdateSliceKernelEmitter::DynamicUpdateSliceKernelEmitter(\n-    mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n-    const HloFusionSpec& fusion_spec, const BufferAssignment* buffer_assignment,\n+    gpu::SymbolicExprContext& symbolic_expr_context,\n+    const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n+    const BufferAssignment* buffer_assignment,\n     KernelArguments::BufferAlignment buffer_alignment,\n     WorkDimensions work_dimensions, absl::string_view entry_function_name,\n     BackendKind backend_kind)\n-    : mlir_context_(mlir_context),\n+    : symbolic_expr_context_(symbolic_expr_context),\n       fusion_(fusion),\n       fusion_spec_(fusion_spec),\n       dus_ops_(\n@@ -94,7 +95,7 @@ DynamicUpdateSliceKernelEmitter::DynamicUpdateSliceKernelEmitter(\n \n absl::StatusOr<MlirKernelDefinition>\n DynamicUpdateSliceKernelEmitter::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(&mlir_context_);\n+  mlir::OpBuilder builder(symbolic_expr_context_.GetMLIRContext());\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion_.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(\n       loc, absl::StrCat(fusion_.name(), \"_kernel_module\"));\n@@ -106,10 +107,12 @@ DynamicUpdateSliceKernelEmitter::EmitKernelDefinition() {\n       mlir::func::FuncOp entry_func,\n       emitters::EmitKernelApi(*module, fusion_, buffer_assignment_,\n                               buffer_alignment_, entry_function_name_));\n-  SetBackendKind(&mlir_context_, entry_func, backend_kind_);\n+  SetBackendKind(symbolic_expr_context_.GetMLIRContext(), entry_func,\n+                 backend_kind_);\n \n   emitters::PartitionedComputations computations(\n-      fusion_.fused_instructions_computation(), &mlir_context_, GetEpilogues());\n+      fusion_.fused_instructions_computation(), &symbolic_expr_context_,\n+      GetEpilogues());\n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              *module, computations));\n \n@@ -123,11 +126,12 @@ DynamicUpdateSliceKernelEmitter::EmitKernelDefinition() {\n }\n \n IndexingMap DynamicUpdateSliceKernelEmitter::ComputeWorkItemIdToInputIndexing(\n-    mlir::MLIRContext* ctx) const {\n+    gpu::SymbolicExprContext* symbolic_expr_context) const {\n   // It is guaranteed that all DUS ops have the same output shape at this point.\n   const auto& update_shape =\n       dus_ops_.front().GetOperand(kDUSUpdateIndex).shape();\n-  return ComputeWorkItemIdToOutputIndexing(work_dimensions_, update_shape, ctx);\n+  return ComputeWorkItemIdToOutputIndexing(work_dimensions_, update_shape,\n+                                           symbolic_expr_context);\n }\n \n Shape DynamicUpdateSliceKernelEmitter::GetIndexingShape(\n@@ -139,8 +143,9 @@ Shape DynamicUpdateSliceKernelEmitter::GetIndexingShape(\n \n IndexingMap DynamicUpdateSliceKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n     const WorkDimensions& work_dimensions, const Shape& update_shape,\n-    mlir::MLIRContext* ctx) {\n-  return GetDefaultWorkItemIndexingMap(work_dimensions, update_shape, ctx);\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n+  return GetDefaultWorkItemIndexingMap(work_dimensions, update_shape,\n+                                       symbolic_expr_context);\n }\n \n absl::StatusOr<KernelSpec> DynamicUpdateSliceKernelEmitter::GetKernelSpec()\n@@ -192,12 +197,10 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n     const emitters::CallTargetProvider& call_targets,\n     mlir::func::FuncOp entry_function,\n     const HloFusionInstruction& fusion) const {\n-  mlir::MLIRContext* context = entry_function.getContext();\n-\n   mlir::ImplicitLocOpBuilder builder(entry_function.getLoc(), entry_function);\n   builder.setInsertionPointToStart(entry_function.addEntryBlock());\n \n-  auto indexing = ComputeWorkItemIdToInputIndexing(context);\n+  auto indexing = ComputeWorkItemIdToInputIndexing(&symbolic_expr_context_);\n   indexing.Simplify();\n   indexing.RemoveUnusedSymbols();\n \n@@ -242,9 +245,10 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n           call_targets, entry_function, nested_b);\n       // Handle bitcasts under the DUS.\n       if (dus_instr->shape() != root.shape()) {\n-        update_indices = ApplyIndexing(\n-            GetBitcastMap(dus_instr->shape(), root.shape(), context),\n-            update_indices, {}, nested_b);\n+        update_indices =\n+            ApplyIndexing(GetBitcastMap(dus_instr->shape(), root.shape(),\n+                                        &symbolic_expr_context_),\n+                          update_indices, {}, nested_b);\n       }\n       results.push_back(nested_b.create<mlir::tensor::InsertOp>(\n           updated_value[0], output, update_indices));\n@@ -276,7 +280,8 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n       llvm::SmallVector<mlir::OpFoldResult> offsets(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(0));\n       llvm::SmallVector<mlir::OpFoldResult> sizes =\n-          mlir::getAsIndexOpFoldResult(context, output_tensor.getShape());\n+          mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+                                       output_tensor.getShape());\n       llvm::SmallVector<mlir::OpFoldResult> strides(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(1));\n       nested_b.create<mlir::tensor::ParallelInsertSliceOp>(\n@@ -286,7 +291,7 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n \n   const NumWorkItems& num_work_items = work_dimensions_.num_work_items;\n   llvm::SmallVector<mlir::OpFoldResult> upper_bounds =\n-      mlir::getAsIndexOpFoldResult(context,\n+      mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n                                    {static_cast<int64_t>(num_work_items.x),\n                                     static_cast<int64_t>(num_work_items.y),\n                                     static_cast<int64_t>(num_work_items.z)});\n@@ -304,7 +309,7 @@ DynamicUpdateSliceKernelEmitter::GetEpilogues() const {\n   for (const auto& [dus_op, root] :\n        llvm::zip(dus_ops_, fusion_spec_.fusion_roots())) {\n     epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-        &dus_op.instruction(), &root.instruction(), &mlir_context_));\n+        &dus_op.instruction(), &root.instruction(), &symbolic_expr_context_));\n   }\n   return epilogues;\n }"
        },
        {
            "sha": "41cfb5c503559a15821309dcf8e86c9603e4529c",
            "filename": "third_party/xla/xla/codegen/emitters/dynamic_update_slice_kernel_emitter.h",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -37,6 +36,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n \n namespace xla::emitters {\n@@ -50,8 +50,8 @@ namespace xla::emitters {\n class DynamicUpdateSliceKernelEmitter final : public MlirKernelEmitter {\n  public:\n   DynamicUpdateSliceKernelEmitter(\n-      mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n-      const HloFusionSpec& fusion_spec,\n+      gpu::SymbolicExprContext& symbolic_expr_context,\n+      const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n       const BufferAssignment* buffer_assignment,\n       KernelArguments::BufferAlignment buffer_alignment,\n       WorkDimensions work_dimensions, absl::string_view entry_function_name,\n@@ -65,14 +65,15 @@ class DynamicUpdateSliceKernelEmitter final : public MlirKernelEmitter {\n   // Get the mapping from work item id to output.\n   static IndexingMap ComputeWorkItemIdToOutputIndexing(\n       const WorkDimensions& work_dimensions, const Shape& update_shape,\n-      mlir::MLIRContext* ctx);\n+      gpu::SymbolicExprContext* ctx);\n \n   std::string name() const final {\n     return \"dynamic_update_slice_kernel_emitter\";\n   }\n \n  private:\n-  IndexingMap ComputeWorkItemIdToInputIndexing(mlir::MLIRContext* ctx) const;\n+  IndexingMap ComputeWorkItemIdToInputIndexing(\n+      gpu::SymbolicExprContext* symbolic_expr_context) const;\n   absl::StatusOr<KernelSpec> GetKernelSpec() const;\n \n   absl::Status EmitEntryFunction(\n@@ -84,7 +85,7 @@ class DynamicUpdateSliceKernelEmitter final : public MlirKernelEmitter {\n   std::vector<emitters::EpilogueSpecification> GetEpilogues() const;\n \n  private:\n-  mlir::MLIRContext& mlir_context_;\n+  gpu::SymbolicExprContext& symbolic_expr_context_;\n   const HloFusionInstruction& fusion_;\n   const HloFusionSpec& fusion_spec_;\n   std::vector<HloInstructionAdaptor> dus_ops_;"
        },
        {
            "sha": "515c075bb7804b7d2589abdddb48e109ca9089b5",
            "filename": "third_party/xla/xla/codegen/emitters/elemental_hlo_to_mlir.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 38,
            "changes": 99,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -77,6 +77,7 @@ limitations under the License.\n #include \"xla/mlir_hlo/mhlo/transforms/map_mhlo_to_scalar_op.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/algorithm_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -87,6 +88,7 @@ namespace xla {\n namespace emitters {\n namespace {\n \n+using gpu::SymbolicExprContext;\n using llvm::SmallVector;\n using llvm::SmallVectorImpl;\n using mlir::Block;\n@@ -176,10 +178,10 @@ absl::StatusOr<Value> GetSingleOperandValue(\n absl::StatusOr<SmallVector<Value, 1>> EmitReduce(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider,\n-    const CallTargetProvider& call_target_provider, ImplicitLocOpBuilder& b) {\n-  auto* mlir_context = b.getContext();\n+    const CallTargetProvider& call_target_provider, ImplicitLocOpBuilder& b,\n+    SymbolicExprContext* symbolic_expr_context) {\n   HloInstructionIndexing indexing =\n-      ComputeOutputToInputIndexing(instr, 0, mlir_context);\n+      ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context);\n   const IndexingMap& indexing_map = indexing.indexing_maps[0].begin()->map();\n \n   SmallVector<Value, 1> init_values;\n@@ -211,10 +213,10 @@ absl::StatusOr<SmallVector<Value, 1>> EmitReduce(\n absl::StatusOr<SmallVector<Value, 1>> EmitReduceWindow(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider,\n-    const CallTargetProvider& call_target_provider, ImplicitLocOpBuilder& b) {\n-  MLIRContext* mlir_context = b.getContext();\n+    const CallTargetProvider& call_target_provider, ImplicitLocOpBuilder& b,\n+    SymbolicExprContext* symbolic_expr_context) {\n   HloInstructionIndexing indexing =\n-      ComputeOutputToInputIndexing(instr, 0, mlir_context);\n+      ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context);\n   IndexingMap indexing_map = indexing.indexing_maps[0].begin()->map();\n   indexing_map.RescaleSymbols();\n \n@@ -450,10 +452,11 @@ SmallVector<SmallVector<Value, 3>, 2> GetInputIndices(\n \n absl::StatusOr<SmallVector<Value, 1>> EmitPad(\n     const HloInstruction* instr, ValueRange indices,\n-    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b) {\n+    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto result_element_type =\n       PrimitiveTypeToMlirType(instr->shape().element_type(), b);\n-  auto indexing = ComputeOutputToInputIndexing(instr, 0, b.getContext());\n+  auto indexing = ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context);\n   const IndexingMap& indexing_map = indexing.indexing_maps[0].begin()->map();\n   Value is_in_bounds = CheckConstraints(indexing_map, indices, {}, b);\n \n@@ -518,11 +521,12 @@ absl::StatusOr<Value> EmitMulAdd(Value lhs, Value rhs, Value accumulator,\n \n absl::StatusOr<SmallVector<Value, 1>> EmitDotLoop(\n     const HloInstruction* instr, ValueRange indices,\n-    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b) {\n+    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto result_element_type =\n       PrimitiveTypeToMlirType(instr->shape().element_type(), b);\n-  HloInstructionIndexing indexing =\n-      ComputeOutputToInputIndexing(instr, /*output_id=*/0, b.getContext());\n+  HloInstructionIndexing indexing = ComputeOutputToInputIndexing(\n+      instr, /*output_id=*/0, symbolic_expr_context);\n   const IndexingMap& lhs_indexing_map =\n       indexing.indexing_maps.at(0).begin()->map();\n   const IndexingMap& rhs_indexing_map =\n@@ -587,7 +591,8 @@ absl::StatusOr<SmallVector<Value, 1>> EmitDotLoop(\n \n absl::StatusOr<SmallVector<Value, 1>> EmitDot(\n     const HloInstruction* instr, ValueRange indices,\n-    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b) {\n+    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n+    SymbolicExprContext* symbolic_expr_context) {\n   VLOG(10) << \"EmitDot: \" << instr->ToString();\n \n   if (!algorithm_util::IsSupportedByElementalIrEmitter(\n@@ -600,14 +605,17 @@ absl::StatusOr<SmallVector<Value, 1>> EmitDot(\n   auto* dot = DynCast<HloDotInstruction>(instr);\n   TF_RET_CHECK(dot != nullptr);\n \n-  return EmitDotLoop(instr, indices, operand_provider, b);\n+  return EmitDotLoop(instr, indices, operand_provider, b,\n+                     symbolic_expr_context);\n }\n \n absl::StatusOr<SmallVector<Value, 1>> EmitConvolution(\n     const HloInstruction* instr, ValueRange indices,\n-    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b) {\n+    const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n+    SymbolicExprContext* symbolic_expr_context) {\n   VLOG(10) << \"EmitConvolution: \" << instr->ToString();\n-  return EmitDotLoop(instr, indices, operand_provider, b);\n+  return EmitDotLoop(instr, indices, operand_provider, b,\n+                     symbolic_expr_context);\n }\n \n absl::StatusOr<SmallVector<Value, 1>> EmitParameter(const HloInstruction* instr,\n@@ -709,7 +717,8 @@ namespace {\n \n absl::StatusOr<SmallVector<Value, 1>> EmitTuple(\n     const HloInstruction* instr, ValueRange indices,\n-    const OperandProvider& operand_provider, ImplicitLocOpBuilder& builder) {\n+    const OperandProvider& operand_provider, ImplicitLocOpBuilder& builder,\n+    SymbolicExprContext* symbolic_expr_context) {\n   const auto* first_shape = &instr->shape().tuple_shapes(0);\n   while (first_shape->IsTuple()) {\n     first_shape = &first_shape->tuple_shapes(0);\n@@ -729,7 +738,7 @@ absl::StatusOr<SmallVector<Value, 1>> EmitTuple(\n     if (i > 0 && !ShapeUtil::EqualIgnoringElementType(*first_shape,\n                                                       *operand_index_shape)) {\n       auto operand_map = GetBitcastMap(*first_shape, *operand_index_shape,\n-                                       builder.getContext());\n+                                       symbolic_expr_context);\n       operand_indices = ApplyIndexing(operand_map, indices, {}, builder);\n     } else {\n       operand_indices = indices;\n@@ -774,7 +783,8 @@ absl::StatusOr<SmallVector<Value, 1>> EmitConstant(\n \n absl::StatusOr<SmallVector<Value, 2>> GetOperands(\n     const HloInstruction* instr, ValueRange indices,\n-    const OperandProvider& operand_provider, ImplicitLocOpBuilder& builder) {\n+    const OperandProvider& operand_provider, ImplicitLocOpBuilder& builder,\n+    SymbolicExprContext* symbolic_expr_context) {\n   SmallVector<Value, 2> operands;\n   bool is_elementwise = HloInstruction::IsOpElementwise(instr->opcode()) ||\n                         instr->opcode() == HloOpcode::kMap;\n@@ -798,7 +808,7 @@ absl::StatusOr<SmallVector<Value, 2>> GetOperands(\n     }\n   } else {\n     auto input_indices = GetInputIndices(\n-        ComputeOutputToInputIndexing(instr, 0, builder.getContext()), indices,\n+        ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context), indices,\n         builder);\n     for (auto&& [operand_number, operand_indices] :\n          llvm::enumerate(input_indices)) {\n@@ -958,7 +968,7 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n     const HloInstruction* instr, mlir::func::FuncOp this_fn, ValueRange indices,\n     const OperandProvider& operand_provider,\n     const CallTargetProvider& call_target_provider,\n-    ImplicitLocOpBuilder& builder) {\n+    ImplicitLocOpBuilder& builder, SymbolicExprContext* symbolic_expr_context) {\n   CHECK(!kUnsupportedOps.contains(instr->opcode())) << instr->ToShortString();\n \n   auto element_type = instr->shape().element_type();\n@@ -971,7 +981,8 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n     case HloOpcode::kConstant:\n       return EmitConstant(instr, indices, builder);\n     case HloOpcode::kConvolution:\n-      return EmitConvolution(instr, indices, operand_provider, builder);\n+      return EmitConvolution(instr, indices, operand_provider, builder,\n+                             symbolic_expr_context);\n     case HloOpcode::kDynamicSlice:\n       return EmitDynamicSlice(instr, indices, operand_provider, builder);\n     case HloOpcode::kDynamicUpdateSlice:\n@@ -981,19 +992,23 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n     case HloOpcode::kIota:\n       return EmitIota(instr, indices, builder);\n     case HloOpcode::kPad:\n-      return EmitPad(instr, indices, operand_provider, builder);\n+      return EmitPad(instr, indices, operand_provider, builder,\n+                     symbolic_expr_context);\n     case HloOpcode::kDot:\n-      return EmitDot(instr, indices, operand_provider, builder);\n+      return EmitDot(instr, indices, operand_provider, builder,\n+                     symbolic_expr_context);\n     case HloOpcode::kParameter:\n       return EmitParameter(instr, this_fn, indices, builder);\n     case HloOpcode::kReduce:\n       return EmitReduce(instr, indices, operand_provider, call_target_provider,\n-                        builder);\n+                        builder, symbolic_expr_context);\n     case HloOpcode::kReduceWindow:\n       return EmitReduceWindow(instr, indices, operand_provider,\n-                              call_target_provider, builder);\n+                              call_target_provider, builder,\n+                              symbolic_expr_context);\n     case HloOpcode::kTuple:\n-      return EmitTuple(instr, indices, operand_provider, builder);\n+      return EmitTuple(instr, indices, operand_provider, builder,\n+                       symbolic_expr_context);\n     case HloOpcode::kGetTupleElement: {\n       // We have to generate the entire tuple, but since we don't support\n       // internal tuple operations (only root tuples), this will always be\n@@ -1015,7 +1030,8 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n   }\n \n   TF_ASSIGN_OR_RETURN(auto operands,\n-                      GetOperands(instr, indices, operand_provider, builder));\n+                      GetOperands(instr, indices, operand_provider, builder,\n+                                  symbolic_expr_context));\n \n   llvm::SmallVector<mlir::NamedAttribute> attributes;\n   switch (instr->opcode()) {\n@@ -1279,14 +1295,16 @@ class SubgraphConverter {\n                     mlir::func::FuncOp this_fn,\n                     const CallTargetProvider& call_target_provider,\n                     ValueRange parameters, ValueRange indices,\n-                    ImplicitLocOpBuilder& builder)\n+                    ImplicitLocOpBuilder& builder,\n+                    SymbolicExprContext* symbolic_expr_context)\n       : computation_(computation),\n         subgraph_(subgraph),\n         this_fn_(this_fn),\n         call_target_provider_(call_target_provider),\n         parameters_(parameters),\n         indices_(indices),\n         builder_(builder),\n+        symbolic_expr_context_(symbolic_expr_context),\n         provide_operand_fn_(\n             std::bind(std::mem_fn(&SubgraphConverter::ProvideOperand), this,\n                       std::placeholders::_1, std::placeholders::_2,\n@@ -1322,6 +1340,7 @@ class SubgraphConverter {\n   ValueRange parameters_;\n   ValueRange indices_;\n   ImplicitLocOpBuilder& builder_;\n+  SymbolicExprContext* symbolic_expr_context_;\n   absl::node_hash_map<std::pair<const HloInstruction*, std::vector<void*>>,\n                       SmallVector<Value>>\n       cached_instructions_;\n@@ -1376,9 +1395,10 @@ absl::StatusOr<SmallVector<Value>> SubgraphConverter::EmitInstruction(\n     return EmitElementwiseInstruction(instr, indices);\n   }\n \n-  TF_ASSIGN_OR_RETURN(auto entry,\n-                      HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n-                                call_target_provider_, builder_));\n+  TF_ASSIGN_OR_RETURN(\n+      auto entry,\n+      HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n+                call_target_provider_, builder_, symbolic_expr_context_));\n   CHECK(!absl::c_linear_search(entry, nullptr))\n       << \"Failed to lower \" << instr->name();\n   return CacheInstruction(instr, indices, std::move(entry));\n@@ -1413,9 +1433,10 @@ SubgraphConverter::EmitElementwiseInstruction(const HloInstruction* root,\n   }\n \n   for (auto* instr : llvm::reverse(pre_order)) {\n-    TF_ASSIGN_OR_RETURN(auto entry,\n-                        HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n-                                  call_target_provider_, builder_));\n+    TF_ASSIGN_OR_RETURN(\n+        auto entry,\n+        HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n+                  call_target_provider_, builder_, symbolic_expr_context_));\n     CacheInstruction(instr, indices, std::move(entry));\n   }\n   return cached_instructions_[{root, IndicesToPtrs(indices)}];\n@@ -1480,9 +1501,10 @@ absl::StatusOr<SmallVector<Value>> SubgraphToMlir(\n     const PartitionedComputation& computation,\n     const PartitionedComputation::Subgraph& subgraph,\n     mlir::func::FuncOp this_fn, const CallTargetProvider& call_target_provider,\n-    ValueRange parameters, ValueRange indices, ImplicitLocOpBuilder& builder) {\n+    ValueRange parameters, ValueRange indices, ImplicitLocOpBuilder& builder,\n+    SymbolicExprContext* symbolic_expr_context) {\n   return SubgraphConverter(computation, subgraph, this_fn, call_target_provider,\n-                           parameters, indices, builder)\n+                           parameters, indices, builder, symbolic_expr_context)\n       .Convert();\n }\n \n@@ -1505,7 +1527,8 @@ void GetLoopBoundsFromIndexingMap(ImplicitLocOpBuilder& b,\n absl::Status SubgraphToMlirFunction(\n     const PartitionedComputation& computation,\n     const PartitionedComputation::Subgraph& subgraph, mlir::func::FuncOp& func,\n-    const CallTargetProvider& call_target_provider) {\n+    const CallTargetProvider& call_target_provider,\n+    SymbolicExprContext* symbolic_expr_context) {\n   TF_RET_CHECK(func != nullptr);\n   ImplicitLocOpBuilder builder(func.getLoc(), func->getContext());\n   builder.setInsertionPointToStart(func.addEntryBlock());\n@@ -1518,7 +1541,7 @@ absl::Status SubgraphToMlirFunction(\n   TF_ASSIGN_OR_RETURN(\n       auto results,\n       SubgraphToMlir(computation, subgraph, func, call_target_provider,\n-                     parameters, indices, builder));\n+                     parameters, indices, builder, symbolic_expr_context));\n   CHECK_EQ(results.size(), func.getResultTypes().size());\n \n   for (auto& result : results) {"
        },
        {
            "sha": "cedf90fae47ca7c9a8858531ee1e916defda9712",
            "filename": "third_party/xla/xla/codegen/emitters/elemental_hlo_to_mlir.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -70,7 +70,8 @@ llvm::SmallVector<mlir::Value, 2> ProvideParameterRange(\n absl::Status SubgraphToMlirFunction(\n     const PartitionedComputation& computation,\n     const PartitionedComputation::Subgraph& subgraph, mlir::func::FuncOp& func,\n-    const CallTargetProvider& call_target_provider);\n+    const CallTargetProvider& call_target_provider,\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Creates an `apply_indexing` op for the given map.\n llvm::SmallVector<mlir::Value, 3> ApplyIndexing(IndexingMap map,"
        },
        {
            "sha": "f97d3bf8d15de41ee60b561a84a1a1c2be59e1b8",
            "filename": "third_party/xla/xla/codegen/emitters/elemental_hlo_to_mlir_test.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 18,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n@@ -64,12 +65,12 @@ using ::testing::HasSubstr;\n class ElementalHloToMlirTest : public HloHardwareIndependentTestBase {\n  public:\n   ElementalHloToMlirTest() {\n-    context_.loadDialect<mlir::tensor::TensorDialect, mlir::func::FuncDialect,\n-                         mlir::affine::AffineDialect, mlir::arith::ArithDialect,\n-                         mlir::math::MathDialect, mlir::scf::SCFDialect,\n-                         mlir::mhlo::MhloDialect, mlir::LLVM::LLVMDialect,\n-                         mlir::DLTIDialect, xla::XlaDialect,\n-                         xla::gpu::XlaGpuDialect>();\n+    mlir_context_.loadDialect<\n+        mlir::tensor::TensorDialect, mlir::func::FuncDialect,\n+        mlir::affine::AffineDialect, mlir::arith::ArithDialect,\n+        mlir::math::MathDialect, mlir::scf::SCFDialect, mlir::mhlo::MhloDialect,\n+        mlir::LLVM::LLVMDialect, mlir::DLTIDialect, xla::XlaDialect,\n+        xla::gpu::XlaGpuDialect>();\n   }\n \n   // Converts the root subgraph of the entry function of the given hlo module to\n@@ -82,8 +83,8 @@ class ElementalHloToMlirTest : public HloHardwareIndependentTestBase {\n                    std::optional<xla::BackendKind> xla_backend = std::nullopt) {\n     auto hlo_module = ParseAndReturnVerifiedModule(hlo).value();\n \n-    mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context_),\n-                                       &context_);\n+    mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&mlir_context_),\n+                                       &mlir_context_);\n     auto module = llvm_ir::CreateMlirModuleOp(builder.getLoc());\n     (*module)->setAttr(\n         mlir::DLTIDialect::kDataLayoutAttrName,\n@@ -95,32 +96,34 @@ class ElementalHloToMlirTest : public HloHardwareIndependentTestBase {\n     if (epilogue_spec_fn) {\n       epilogue_spec.push_back(epilogue_spec_fn(entry_computation));\n     }\n-    PartitionedComputations partitioned_computations(entry_computation,\n-                                                     &context_, epilogue_spec);\n+    PartitionedComputations partitioned_computations(\n+        entry_computation, &symbolic_expr_context_, epilogue_spec);\n     auto fns = partitioned_computations.DeclareFunctions(module.get());\n     auto entry_func = fns[&partitioned_computations\n                                .FindPartitionedComputation(entry_computation)\n                                .GetRootSubgraph()];\n     if (set_xla_entry) {\n-      entry_func->setAttr(\"xla.entry\", mlir::UnitAttr::get(&context_));\n+      entry_func->setAttr(\"xla.entry\", mlir::UnitAttr::get(&mlir_context_));\n     }\n     if (xla_backend) {\n-      SetBackendKind(&context_, entry_func, *xla_backend);\n+      SetBackendKind(&mlir_context_, entry_func, *xla_backend);\n     }\n     auto& entry_pc =\n         partitioned_computations.FindPartitionedComputation(entry_computation);\n     auto call_targets = partitioned_computations.CreateCallTargetProvider(fns);\n-    TF_RETURN_IF_ERROR(SubgraphToMlirFunction(\n-        entry_pc, entry_pc.GetRootSubgraph(), entry_func, call_targets));\n+    TF_RETURN_IF_ERROR(\n+        SubgraphToMlirFunction(entry_pc, entry_pc.GetRootSubgraph(), entry_func,\n+                               call_targets, &symbolic_expr_context_));\n \n     if (!partitioned_computations.epilogues().empty()) {\n       const auto& epilogue = partitioned_computations.epilogues().front();\n       TF_RETURN_IF_ERROR(SubgraphToMlirFunction(entry_pc, epilogue,\n-                                                fns[&epilogue], call_targets));\n+                                                fns[&epilogue], call_targets,\n+                                                &symbolic_expr_context_));\n     }\n \n     // Canonicalize and CSE for better readability of check tests.\n-    mlir::PassManager pm(&context_);\n+    mlir::PassManager pm(&mlir_context_);\n     pm.addPass(mlir::createCanonicalizerPass());\n     pm.addPass(mlir::createCSEPass());\n     TF_RET_CHECK(pm.run(module.get()).succeeded());\n@@ -135,7 +138,8 @@ class ElementalHloToMlirTest : public HloHardwareIndependentTestBase {\n     return absl::OkStatus();\n   }\n \n-  mlir::MLIRContext context_;\n+  mlir::MLIRContext mlir_context_;\n+  xla::gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(ElementalHloToMlirTest, Reduce) {\n@@ -1384,7 +1388,7 @@ class ElementalHloToMlirEpilogueTest : public ElementalHloToMlirTest {\n       epilogue.roots.push_back(entry->GetInstructionWithName(\"add\"));\n       epilogue.index_ranges = {2, 16, 17};\n       epilogue.root_indexing.push_back(\n-          IndexingMap{mlir::AffineMap::getMultiDimIdentityMap(3, &context_)\n+          IndexingMap{mlir::AffineMap::getMultiDimIdentityMap(3, &mlir_context_)\n                           .getSubMap({0, 2, 1}),\n                       DimVarsFromTensorSizes({2, 17, 17}),\n                       {},"
        },
        {
            "sha": "826250a3992cf7a58ee3dfdd4555a0b8485ef7cb",
            "filename": "third_party/xla/xla/codegen/emitters/ir/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -93,13 +93,13 @@ cc_library(\n         \":xla_ops_inc_gen\",\n         \"//xla/codegen/emitters:type_util\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:BytecodeOpInterface\",\n-        \"@llvm-project//mlir:CallOpInterfaces\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:InferTypeOpInterface\",\n@@ -120,6 +120,7 @@ xla_test(\n         \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/mlir/utils:error_util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tests:hlo_pjrt_test_base\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "d91b3ce8c61578a7aa2436ef8c6468408d36318a",
            "filename": "third_party/xla/xla/codegen/emitters/ir/xla_ops.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -57,10 +57,12 @@ limitations under the License.\n #include \"xla/codegen/emitters/ir/xla_dialect.cc.inc\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/analysis/indexing_map_serialization.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace {\n \n+using gpu::SymbolicExprContext;\n using llvm::ArrayRef;\n using mlir::AffineExpr;\n using mlir::AffineMap;\n@@ -296,7 +298,7 @@ struct IndexingMapWithAdditions {\n absl::StatusOr<IndexingMapWithAdditions> GetNewIndexingMapAfterFoldingSequence(\n     IndexingMap indexing_map,\n     SmallVector<std::pair<int, ApplyIndexingOp>, 2> apply_indexing_ops,\n-    mlir::DenseMap<Value, AffineExpr> operand_exprs, MLIRContext* ctx) {\n+    mlir::DenseMap<Value, AffineExpr> operand_exprs, SymbolicExprContext* ctx) {\n   int num_dims = indexing_map.GetDimensionCount();\n \n   SmallVector<Value> added_dim_args;\n@@ -323,8 +325,8 @@ absl::StatusOr<IndexingMapWithAdditions> GetNewIndexingMapAfterFoldingSequence(\n       auto& replacement_expr = operand_exprs[producer_operand.get()];\n       if (!replacement_expr) {\n         int dim_num = producer_operand_number;\n-        replacement_expr =\n-            getAffineDimExpr(num_dims + added_dim_args.size(), ctx);\n+        replacement_expr = getAffineDimExpr(num_dims + added_dim_args.size(),\n+                                            ctx->GetMLIRContext());\n         added_dim_args.push_back(producer_operand.get());\n         new_dim_vars.push_back(producer_map.GetDimVar(dim_num));\n       }\n@@ -447,8 +449,11 @@ struct FoldApplyIndexingSequence\n               : getAffineSymbolExpr(operand_number - num_dims, ctx);\n     }\n \n+    // TODO(b/446856303): Get SymbolicExprContext from IndexingMap.\n+    SymbolicExprContext symbolic_expr_context(ctx);\n     auto replacement = GetNewIndexingMapAfterFoldingSequence(\n-        indexing_map, apply_indexing_ops, operand_exprs, ctx);\n+        indexing_map, apply_indexing_ops, operand_exprs,\n+        &symbolic_expr_context);\n \n     if (!replacement.ok()) {\n       return rewriter.notifyMatchFailure(indexing_op,\n@@ -929,7 +934,9 @@ struct SimplifyLoopOfApplyIndexing : public mlir::OpRewritePattern<LoopOp> {\n   LogicalResult matchAndRewrite(LoopOp loop_op,\n                                 PatternRewriter& rewriter) const override {\n     auto loop_indexing_map = loop_op.getIndexingMap();\n-    MLIRContext* ctx = loop_op.getContext();\n+    MLIRContext* mlir_context = loop_op.getContext();\n+    // TODO(b/446856303): Get context from IndexingMap instead.\n+    SymbolicExprContext symbolic_expr_context(mlir_context);\n     int num_dims = loop_indexing_map.GetDimVarsCount();\n \n     SmallVector<std::pair<int, ApplyIndexingOp>, 2> apply_indexing_ops;\n@@ -957,11 +964,13 @@ struct SimplifyLoopOfApplyIndexing : public mlir::OpRewritePattern<LoopOp> {\n     mlir::DenseMap<Value, AffineExpr> operand_exprs;\n     for (auto& operand : loop_op->getOpOperands().take_front(num_dims)) {\n       int operand_number = operand.getOperandNumber();\n-      operand_exprs[operand.get()] = getAffineDimExpr(operand_number, ctx);\n+      operand_exprs[operand.get()] =\n+          getAffineDimExpr(operand_number, mlir_context);\n     }\n \n     auto replacement = GetNewIndexingMapAfterFoldingSequence(\n-        loop_indexing_map, apply_indexing_ops, operand_exprs, ctx);\n+        loop_indexing_map, apply_indexing_ops, operand_exprs,\n+        &symbolic_expr_context);\n \n     if (!replacement.ok()) {\n       return rewriter.notifyMatchFailure(loop_op,\n@@ -1104,7 +1113,8 @@ std::optional<IndexingMap> parseChainOfStringsAsIndexingMap(\n   while (parser.parseOptionalAttribute(indexing_map_attr).has_value()) {\n     indexing_map_str.append(indexing_map_attr.getValue());\n   }\n-  return ParseIndexingMap(indexing_map_str, parser.getContext());\n+  gpu::SymbolicExprContext symbolic_expr_context(parser.getContext());\n+  return ParseIndexingMap(indexing_map_str, &symbolic_expr_context);\n }\n \n void LoopOp::getCanonicalizationPatterns(mlir::RewritePatternSet& results,"
        },
        {
            "sha": "1386ae488174eb4371594e38191da170e9b44104",
            "filename": "third_party/xla/xla/codegen/emitters/ir/xla_ops_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/hlo/analysis/indexing_map_serialization.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/mlir/utils/error_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tests/hlo_pjrt_test_base.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n@@ -70,6 +71,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> ParseMlirModuleString(\n class XLAOpsTest : public HloPjRtTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n std::string VariableConstraintsToString(const IndexingMap& map) {\n@@ -127,7 +129,7 @@ TEST_F(XLAOpsTest, GetConstraintsForVariables) {\n     s0 mod 4 in [0, 1],\n     w mod 4 in [0, 2],\n   )\",\n-                               &mlir_context_);\n+                               &symbolic_expr_context_);\n   EXPECT_EQ(VariableConstraintsToString(map),\n             R\"(x: no constraints\n y: y + w in [0, 4], y mod 32 in [0, 6]\n@@ -144,7 +146,7 @@ TEST_F(XLAOpsTest, GetConstraintsForVariablesEmpty) {\n     s0 in [0, 32],\n     s1 in [0, 1024],\n   )\",\n-                               &mlir_context_);\n+                               &symbolic_expr_context_);\n   EXPECT_EQ(VariableConstraintsToString(map),\n             R\"(d0: no constraints\n d1: no constraints"
        },
        {
            "sha": "61f909cd5e3fc4200728f4906ecdfc7e1b9f05ce",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_api_builder.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 12,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/runtime/work_group.h\"\n #include \"xla/runtime/work_item.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -218,9 +219,9 @@ void SetIndexDataLayout(mlir::ModuleOp module,\n       mlir::DataLayoutSpecAttr::get(module->getContext(), {index_layout}));\n }\n \n-IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n-                                          const Shape& shape,\n-                                          mlir::MLIRContext* ctx) {\n+IndexingMap GetDefaultWorkItemIndexingMap(\n+    const WorkDimensions& work_dimensions, const Shape& shape,\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   std::vector<mlir::AffineExpr> output_dims(shape.dimensions().size());\n \n   const NumWorkItems& num_work_items = work_dimensions.num_work_items;\n@@ -235,13 +236,15 @@ IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n       num_work_items.y * num_work_groups.y,\n       num_work_items.z * num_work_groups.z};\n \n-  mlir::AffineExpr c0 = mlir::getAffineConstantExpr(0, ctx);\n+  mlir::AffineExpr c0 =\n+      mlir::getAffineConstantExpr(0, symbolic_expr_context->GetMLIRContext());\n   uint64_t stride = 1;\n   mlir::AffineExpr linear_index = c0;\n   // Reverse to get minor to major order.\n   for (auto [idx, dim] : llvm::enumerate(llvm::reverse(work_tile_dimensions))) {\n     uint64_t symbol_index = work_tile_dimensions.size() - idx;\n-    auto tile_coord = mlir::getAffineSymbolExpr(symbol_index, ctx);\n+    auto tile_coord = mlir::getAffineSymbolExpr(\n+        symbol_index, symbolic_expr_context->GetMLIRContext());\n     auto tile_component = tile_coord * stride;\n \n     linear_index = linear_index + tile_component;\n@@ -258,9 +261,12 @@ IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n   // loop emitter doesn't support. This is safe, since the latter CHECK fails\n   // if its assumptions are not fulfilled.\n   for (int i = 0; i < 3; ++i) {\n-    auto coord = mlir::getAffineDimExpr(kIndexingMapWorkItemDims[i], ctx) +\n-                 mlir::getAffineDimExpr(kIndexingMapWorkGroupDims[i], ctx) *\n-                     work_item_array[i];\n+    auto coord =\n+        mlir::getAffineDimExpr(kIndexingMapWorkItemDims[i],\n+                               symbolic_expr_context->GetMLIRContext()) +\n+        mlir::getAffineDimExpr(kIndexingMapWorkGroupDims[i],\n+                               symbolic_expr_context->GetMLIRContext()) *\n+            work_item_array[i];\n     auto linear_component = coord * stride;\n     linear_index = linear_index + linear_component;\n     stride *= total_item_array[i];\n@@ -270,7 +276,8 @@ IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n   // chunk.\n   uint64_t items_per_chunk = stride;\n \n-  mlir::AffineExpr chunk_id = mlir::getAffineSymbolExpr(0, ctx);\n+  mlir::AffineExpr chunk_id =\n+      mlir::getAffineSymbolExpr(0, symbolic_expr_context->GetMLIRContext());\n   linear_index = chunk_id * items_per_chunk + linear_index;\n \n   // See IndexUtil::LinearIndexToMultidimensionalIndex.\n@@ -296,7 +303,8 @@ IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n \n   IndexingMap indexing_map(\n       mlir::AffineMap::get(/*dimCount=*/6,\n-                           /*symbolCount=*/range_vars_size, output_dims, ctx),\n+                           /*symbolCount=*/range_vars_size, output_dims,\n+                           symbolic_expr_context->GetMLIRContext()),\n       std::move(dim_vars), std::move(range_vars), /*rt_vars=*/{});\n   indexing_map.AddConstraint(linear_index, Interval{0, num_elements - 1});\n   indexing_map.Simplify();\n@@ -363,7 +371,8 @@ absl::StatusOr<CallTargetProvider> EmitPartitionedComputations(\n     for (const auto& subgraph : comp.subgraphs()) {\n       if (subgraph_to_mlir_fn.contains(&subgraph)) {\n         TF_RETURN_IF_ERROR(SubgraphToMlirFunction(\n-            comp, subgraph, subgraph_to_mlir_fn[&subgraph], call_targets));\n+            comp, subgraph, subgraph_to_mlir_fn[&subgraph], call_targets,\n+            computations.symbolic_expr_context()));\n       }\n     }\n   }\n@@ -375,7 +384,8 @@ absl::StatusOr<CallTargetProvider> EmitPartitionedComputations(\n     }\n     TF_RETURN_IF_ERROR(SubgraphToMlirFunction(\n         computations.FindPartitionedComputation(fused_computation), epilogue,\n-        subgraph_to_mlir_fn[&epilogue], call_targets));\n+        subgraph_to_mlir_fn[&epilogue], call_targets,\n+        computations.symbolic_expr_context()));\n   }\n \n   return call_targets;"
        },
        {
            "sha": "1c7acab079dd0eaf318445537158566d0e2fd6ed",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_api_builder.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Value.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -32,6 +31,7 @@ limitations under the License.\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/runtime/work_group.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n \n namespace xla::emitters {\n@@ -52,9 +52,9 @@ void SetIndexDataLayout(mlir::ModuleOp module,\n \n // Get the default indexing map for the given work dimensions, unroll factor,\n // and output shape.\n-IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n-                                          const Shape& shape,\n-                                          mlir::MLIRContext* ctx);\n+IndexingMap GetDefaultWorkItemIndexingMap(\n+    const WorkDimensions& work_dimensions, const Shape& shape,\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Emits the work group id ops annotated with the range of each dimension.\n llvm::SmallVector<mlir::Value> EmitWorkGroupIds("
        },
        {
            "sha": "0bbacf91555c4df51b1f40e11ce4dac379d009ce",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_api_builder_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -26,24 +26,26 @@ limitations under the License.\n #include \"xla/runtime/work_group.h\"\n #include \"xla/runtime/work_item.h\"\n #include \"xla/runtime/work_tile_size.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::emitters {\n namespace {\n \n TEST(DefaultWorkItemIndexingMap, MultiDimensionTile) {\n-  mlir::MLIRContext context;\n-  context.loadDialect<mlir::affine::AffineDialect>();\n+  mlir::MLIRContext mlir_context;\n+  gpu::SymbolicExprContext symbolic_expr_context(&mlir_context);\n+  mlir_context.loadDialect<mlir::affine::AffineDialect>();\n \n   WorkDimensions work_dimensions{NumWorkClusters{}, NumWorkGroups{2},\n                                  NumWorkItems{3}, WorkTileSize{{4, 5, 6}}};\n \n   Shape shape(PrimitiveType::F32, {6, 4, 5, 6});\n   *shape.mutable_layout() = LayoutUtil::GetDefaultLayoutForShape(shape);\n \n-  IndexingMap indexing_map =\n-      GetDefaultWorkItemIndexingMap(work_dimensions, shape, &context);\n+  IndexingMap indexing_map = GetDefaultWorkItemIndexingMap(\n+      work_dimensions, shape, &symbolic_expr_context);\n \n   // The shape is the same as the number of elements work dimensions, so there\n   // are no constraints.\n@@ -57,18 +59,18 @@ TEST(DefaultWorkItemIndexingMap, MultiDimensionTile) {\n \n   mlir::AffineMap affine_map = indexing_map.GetAffineMap();\n \n-  mlir::AffineExpr work_item_sym = mlir::getAffineDimExpr(0, &context);\n-  mlir::AffineExpr work_group_sym = mlir::getAffineDimExpr(3, &context);\n+  mlir::AffineExpr work_item_sym = mlir::getAffineDimExpr(0, &mlir_context);\n+  mlir::AffineExpr work_group_sym = mlir::getAffineDimExpr(3, &mlir_context);\n \n   EXPECT_EQ(affine_map.getResult(0), 3 * work_group_sym + work_item_sym);\n \n-  mlir::AffineExpr tile_sym_x = mlir::getAffineSymbolExpr(0, &context);\n+  mlir::AffineExpr tile_sym_x = mlir::getAffineSymbolExpr(0, &mlir_context);\n   EXPECT_EQ(affine_map.getResult(1), tile_sym_x);\n \n-  mlir::AffineExpr tile_sym_y = mlir::getAffineSymbolExpr(1, &context);\n+  mlir::AffineExpr tile_sym_y = mlir::getAffineSymbolExpr(1, &mlir_context);\n   EXPECT_EQ(affine_map.getResult(2), tile_sym_y);\n \n-  mlir::AffineExpr tile_sym_z = mlir::getAffineSymbolExpr(2, &context);\n+  mlir::AffineExpr tile_sym_z = mlir::getAffineSymbolExpr(2, &mlir_context);\n   EXPECT_EQ(affine_map.getResult(3), tile_sym_z);\n }\n "
        },
        {
            "sha": "ad5af8fc53f9ea55b137abe513617ec783c195f8",
            "filename": "third_party/xla/xla/codegen/emitters/loop_kernel_emitter.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -38,7 +38,6 @@ limitations under the License.\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n #include \"mlir/IR/Location.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n@@ -60,6 +59,7 @@ limitations under the License.\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/runtime/work_item.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -70,12 +70,13 @@ limitations under the License.\n namespace xla::emitters {\n \n LoopFusionKernelEmitter::LoopFusionKernelEmitter(\n-    mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n-    const HloFusionSpec& fusion_spec, const BufferAssignment* buffer_assignment,\n+    gpu::SymbolicExprContext& symbolic_expr_context,\n+    const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n+    const BufferAssignment* buffer_assignment,\n     KernelArguments::BufferAlignment buffer_alignment,\n     WorkDimensions work_dimensions, absl::string_view entry_function_name,\n     BackendKind backend_kind)\n-    : mlir_context_(mlir_context),\n+    : symbolic_expr_context_(symbolic_expr_context),\n       fusion_(fusion),\n       fusion_spec_(fusion_spec),\n       buffer_assignment_(buffer_assignment),\n@@ -86,7 +87,7 @@ LoopFusionKernelEmitter::LoopFusionKernelEmitter(\n \n absl::StatusOr<MlirKernelDefinition>\n LoopFusionKernelEmitter::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(&mlir_context_);\n+  mlir::OpBuilder builder(symbolic_expr_context_.GetMLIRContext());\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion_.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(\n       loc, absl::StrCat(fusion_.name(), \"_kernel_module\"));\n@@ -98,11 +99,12 @@ LoopFusionKernelEmitter::EmitKernelDefinition() {\n       mlir::func::FuncOp entry_func,\n       emitters::EmitKernelApi(*module, fusion_, buffer_assignment_,\n                               buffer_alignment_, entry_function_name_));\n-  SetBackendKind(&mlir_context_, entry_func, backend_kind_);\n+  SetBackendKind(symbolic_expr_context_.GetMLIRContext(), entry_func,\n+                 backend_kind_);\n \n   // Loop emitters don't support epilogues.\n   emitters::PartitionedComputations computations(\n-      fusion_.fused_instructions_computation(), module->getContext());\n+      fusion_.fused_instructions_computation(), &symbolic_expr_context_);\n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              *module, computations));\n \n@@ -119,12 +121,12 @@ LoopFusionKernelEmitter::EmitKernelDefinition() {\n \n IndexingMap LoopFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n     const WorkDimensions& work_dimensions, const Shape& root_shape,\n-    mlir::MLIRContext* ctx) {\n+    gpu::SymbolicExprContext* ctx) {\n   return GetDefaultWorkItemIndexingMap(work_dimensions, root_shape, ctx);\n }\n \n IndexingMap LoopFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n-    mlir::MLIRContext* ctx) const {\n+    gpu::SymbolicExprContext* ctx) const {\n   return ComputeWorkItemIdToOutputIndexing(work_dimensions_,\n                                            GetIndexingShape(fusion_spec_), ctx);\n }\n@@ -147,12 +149,10 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n   mlir::ImplicitLocOpBuilder builder(entry_function.getLoc(), entry_function);\n   builder.setInsertionPointToStart(entry_function.addEntryBlock());\n \n-  mlir::MLIRContext* context = builder.getContext();\n-\n   auto workgroup_ids =\n       EmitWorkGroupIds(builder, work_dimensions_.num_work_groups);\n \n-  auto indexing = ComputeWorkItemIdToOutputIndexing(context);\n+  auto indexing = ComputeWorkItemIdToOutputIndexing(&symbolic_expr_context_);\n \n   int num_inputs = fusion.fused_instructions_computation()->num_parameters();\n   auto output_tensor_args =\n@@ -187,7 +187,8 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n     for (auto [root_shape, tensor, value] :\n          llvm::zip(result_shapes, output_tensors, result_scalars)) {\n       llvm::SmallVector<mlir::Value> output_indices = emitters::ApplyIndexing(\n-          GetBitcastMap(*result_shapes.front(), *root_shape, context),\n+          GetBitcastMap(*result_shapes.front(), *root_shape,\n+                        &symbolic_expr_context_),\n           map_results, {}, nested_b);\n       result_tensors.push_back(nested_b.create<mlir::tensor::InsertOp>(\n           value, tensor, output_indices));\n@@ -216,7 +217,8 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n       llvm::SmallVector<mlir::OpFoldResult> offsets(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(0));\n       llvm::SmallVector<mlir::OpFoldResult> sizes =\n-          mlir::getAsIndexOpFoldResult(context, output_tensor.getShape());\n+          mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+                                       output_tensor.getShape());\n       llvm::SmallVector<mlir::OpFoldResult> strides(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(1));\n       nested_b.create<mlir::tensor::ParallelInsertSliceOp>(\n@@ -226,7 +228,7 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n \n   const NumWorkItems& num_work_items = work_dimensions_.num_work_items;\n   llvm::SmallVector<mlir::OpFoldResult> upper_bounds =\n-      mlir::getAsIndexOpFoldResult(context,\n+      mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n                                    {static_cast<int64_t>(num_work_items.x),\n                                     static_cast<int64_t>(num_work_items.y),\n                                     static_cast<int64_t>(num_work_items.z)});"
        },
        {
            "sha": "05c9fb6c4ae96fad4fdce6168e0c9feafd2c6f7c",
            "filename": "third_party/xla/xla/codegen/emitters/loop_kernel_emitter.h",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/codegen/emitters/computation_partitioner.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -36,14 +35,15 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n \n namespace xla::emitters {\n \n // Generic loop fusion.\n class LoopFusionKernelEmitter final : public MlirKernelEmitter {\n  public:\n-  LoopFusionKernelEmitter(mlir::MLIRContext& mlir_context,\n+  LoopFusionKernelEmitter(gpu::SymbolicExprContext& symbolic_expr_context,\n                           const HloFusionInstruction& fusion,\n                           const HloFusionSpec& fusion_spec,\n                           const BufferAssignment* buffer_assignment,\n@@ -56,7 +56,7 @@ class LoopFusionKernelEmitter final : public MlirKernelEmitter {\n \n   static IndexingMap ComputeWorkItemIdToOutputIndexing(\n       const WorkDimensions& work_dimensions, const Shape& root_shape,\n-      mlir::MLIRContext* ctx);\n+      gpu::SymbolicExprContext* ctx);\n \n   // Get the shape that will be used for loop indexing for the given fusion\n   // specification.\n@@ -65,7 +65,8 @@ class LoopFusionKernelEmitter final : public MlirKernelEmitter {\n   std::string name() const final { return \"loop_fusion_kernel_emitter\"; }\n \n  private:\n-  IndexingMap ComputeWorkItemIdToOutputIndexing(mlir::MLIRContext* ctx) const;\n+  IndexingMap ComputeWorkItemIdToOutputIndexing(\n+      gpu::SymbolicExprContext* ctx) const;\n \n   absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -74,7 +75,7 @@ class LoopFusionKernelEmitter final : public MlirKernelEmitter {\n       const HloFusionInstruction& fusion) const;\n \n  private:\n-  mlir::MLIRContext& mlir_context_;\n+  gpu::SymbolicExprContext& symbolic_expr_context_;\n   const HloFusionInstruction& fusion_;\n   const HloFusionSpec& fusion_spec_;\n   const BufferAssignment* buffer_assignment_;"
        },
        {
            "sha": "fd96cdd9828f7014e375426c369bbbade34cc3ab",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -84,6 +84,7 @@ cc_library(\n         \"//xla/mlir_hlo\",\n         \"//xla/mlir_hlo:map_mhlo_to_scalar_op\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\","
        },
        {
            "sha": "65b240ddfbf0f28632ef09c0630436f4b8d6f8a8",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/flatten_tensors.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -51,6 +51,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/emitters/ir/xla_gpu_ops.h\"\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n #include \"xla/layout_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -61,6 +62,7 @@ namespace {\n #define GEN_PASS_DEF_FLATTENTENSORSPASS\n #include \"xla/codegen/emitters/transforms/passes.h.inc\"\n \n+using gpu::SymbolicExprContext;\n using mlir::Attribute;\n using mlir::Location;\n using mlir::LogicalResult;\n@@ -227,8 +229,10 @@ Value LinearizeIndex(Location loc, ShapedType type, ValueRange indices,\n   }\n   auto linear_shape =\n       ShapeUtil::MakeShape(U8, {ShapeUtil::ElementsIn(byte_shape)});\n+  // TODO(b/446856820): Get SymbolicExprContext from a different source..\n+  SymbolicExprContext symbolic_expr_context(rewriter.getContext());\n   auto linearized_map =\n-      GetBitcastMap(byte_shape, linear_shape, rewriter.getContext());\n+      GetBitcastMap(byte_shape, linear_shape, &symbolic_expr_context);\n   mlir::SmallVector<Value> result;\n   rewriter.createOrFold<ApplyIndexingOp>(result, loc, indices, ValueRange{},\n                                          linearized_map);"
        },
        {
            "sha": "f96a24631052f1c5ad836679c675d67417ef918c",
            "filename": "third_party/xla/xla/codegen/mlir_kernel_source.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,30 +30,35 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/Parser/Parser.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n \n absl::StatusOr<MlirKernelSource> MlirKernelSource::ParseFromString(\n-    absl::string_view ir, std::unique_ptr<mlir::MLIRContext> context) {\n+    absl::string_view ir, std::unique_ptr<mlir::MLIRContext> mlir_context) {\n+  auto symbolic_expr_context =\n+      std::make_unique<gpu::SymbolicExprContext>(mlir_context.get());\n   llvm::SourceMgr source_mgr;\n \n   std::string error_string;\n   llvm::raw_string_ostream error_stream(error_string);\n-  mlir::SourceMgrDiagnosticHandler source_mgr_handler(source_mgr, context.get(),\n-                                                      error_stream);\n+  mlir::SourceMgrDiagnosticHandler source_mgr_handler(\n+      source_mgr, mlir_context.get(), error_stream);\n \n   source_mgr.AddNewSourceBuffer(llvm::MemoryBuffer::getMemBuffer(ir),\n                                 llvm::SMLoc());\n \n   mlir::OwningOpRef<mlir::ModuleOp> mlir_module =\n-      mlir::parseSourceFile<mlir::ModuleOp>(source_mgr, context.get());\n+      mlir::parseSourceFile<mlir::ModuleOp>(source_mgr, mlir_context.get());\n \n   if (!mlir_module) {\n     return Internal(\"Failed to parse MLIR IR: %s\", error_string);\n   }\n \n-  return MlirKernelSource(std::move(context), std::move(mlir_module));\n+  return MlirKernelSource(std::move(mlir_context),\n+                          std::move(symbolic_expr_context),\n+                          std::move(mlir_module));\n }\n \n }  // namespace xla"
        },
        {
            "sha": "89b4f5694447ff8ff67a332d9bf6e77cf4cfd6b4",
            "filename": "third_party/xla/xla/codegen/mlir_kernel_source.h",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/Support/DebugStringHelper.h\"\n #include \"xla/codegen/kernel_source.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n \n@@ -40,20 +41,24 @@ namespace xla {\n class MlirKernelSource final : public KernelSource {\n  public:\n   struct Storage {\n-    std::unique_ptr<mlir::MLIRContext> context;\n+    std::unique_ptr<mlir::MLIRContext> mlir_context;\n+    std::unique_ptr<gpu::SymbolicExprContext> symbolic_expr_context;\n     mlir::OwningOpRef<mlir::ModuleOp> module;\n   };\n \n   // Construct a MLIR kernel source from a module and take ownership of its MLIR\n   // context.\n-  MlirKernelSource(std::unique_ptr<mlir::MLIRContext> context,\n-                   mlir::OwningOpRef<mlir::ModuleOp> module)\n-      : storage_{std::move(context), std::move(module)} {}\n+  MlirKernelSource(\n+      std::unique_ptr<mlir::MLIRContext> mlir_context,\n+      std::unique_ptr<gpu::SymbolicExprContext> symbolic_expr_context,\n+      mlir::OwningOpRef<mlir::ModuleOp> module)\n+      : storage_{std::move(mlir_context), std::move(symbolic_expr_context),\n+                 std::move(module)} {}\n \n   // Construct a MLIR kernel source from a module but don't take any ownership\n   // of the MLIR context.\n   explicit MlirKernelSource(mlir::OwningOpRef<mlir::ModuleOp> module)\n-      : storage_{nullptr, std::move(module)} {}\n+      : storage_{nullptr, nullptr, std::move(module)} {}\n \n   MlirKernelSource(MlirKernelSource&& other) noexcept = default;\n   MlirKernelSource& operator=(MlirKernelSource&& other) noexcept = default;\n@@ -62,6 +67,9 @@ class MlirKernelSource final : public KernelSource {\n       absl::string_view ir, std::unique_ptr<mlir::MLIRContext> context);\n \n   mlir::ModuleOp module() { return *storage_.module; }\n+  gpu::SymbolicExprContext* symbolic_expr_context() {\n+    return storage_.symbolic_expr_context.get();\n+  }\n \n   Storage ReleaseStorage() && { return std::move(storage_); }\n "
        },
        {
            "sha": "b4803d5ac5a5949c7a934f1f04018fea4312a85d",
            "filename": "third_party/xla/xla/codegen/tiling/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -118,9 +118,9 @@ cc_library(\n     srcs = [\"tiled_hlo_schedule.cc\"],\n     hdrs = [\"tiled_hlo_schedule.h\"],\n     deps = [\n-        \":tiling_specification\",\n         \"//xla:util\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -139,6 +139,7 @@ xla_cc_test(\n         \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/hlo/analysis:interval\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n@@ -282,6 +283,7 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/utils:hlo_traversal\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:statusor\",\n@@ -322,6 +324,7 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_googletest//:gtest_main\",\n@@ -352,6 +355,7 @@ cc_library(\n         \"//xla/service:name_uniquer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -392,6 +396,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:instruction_fusion\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "cfabcebb39380586cc59e400ad0110c26f6d3ece",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 44,
            "changes": 107,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -69,6 +69,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/name_uniquer.h\"\n #include \"xla/shape.h\"\n@@ -163,7 +164,8 @@ IndexingMap LinearizeTileOffsets(\n     const IndexingMap& tile_offsets_indexing,\n     absl::Span<const int64_t> num_output_tiles_per_dim,\n     absl::Span<const int64_t> major_to_minor_active_tiling_parameters,\n-    mlir::MLIRContext* mlir_context) {\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   // Gather the active output tile sizes in major-to-minor order so as to\n   // produce the right delinearized index.\n   std::vector<int64_t> active_num_output_tiles_per_dim;\n@@ -181,7 +183,7 @@ IndexingMap LinearizeTileOffsets(\n   for (auto [dim_id, tile_expr] :\n        llvm::zip(major_to_minor_active_tiling_parameters,\n                  DelinearizeIndex(active_num_output_tiles_per_dim, program_id,\n-                                  mlir_context))) {\n+                                  symbolic_expr_context))) {\n     tile_exprs[dim_id] = tile_expr;\n   }\n   std::vector<IndexingMap::Variable> dim_vars{{0, num_tiles - 1, \"pid_0\"}};\n@@ -202,9 +204,10 @@ IndexingMap LinearizeTileOffsets(\n absl::StatusOr<OutputTilingInfo> ComputeOutputTilingInfo(\n     const IndexingMap& root_indexing, absl::Span<const int64_t> tile_sizes,\n     absl::Span<const int64_t> major_to_minor_active_tiling_parameters,\n-    mlir::MLIRContext* mlir_context,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n     const std::optional<absl::Span<const Interval>>&\n         parent_output_tile_dim_bounds = std::nullopt) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   int64_t num_tiling_parameters = root_indexing.GetDimVarsCount();\n   CHECK_EQ(num_tiling_parameters, tile_sizes.size());  // Crash OK\n   CHECK_EQ(0, root_indexing.GetRangeVarsCount())\n@@ -277,7 +280,7 @@ absl::StatusOr<OutputTilingInfo> ComputeOutputTilingInfo(\n   // like grid tiling, for instance.\n   IndexingMap linear_output_tile_offset_indexing = LinearizeTileOffsets(\n       output_tile_offset_indexing, outer_loop_bounds,\n-      major_to_minor_active_tiling_parameters, mlir_context);\n+      major_to_minor_active_tiling_parameters, symbolic_expr_context);\n   return OutputTilingInfo{outer_loop_bounds,\n                           output_tile_offset_indexing,\n                           {major_to_minor_active_tiling_parameters.begin(),\n@@ -306,7 +309,8 @@ class SymbolicTiledHloFusionInstruction : public SymbolicTiledHloInstruction {\n absl::StatusOr<IndexingMap> ComputeTileOffsetIndexing(\n     const SymbolicTiledHloInstruction& tiled_hlo,\n     const IndexingMap& output_tile_offset_indexing,\n-    mlir::MLIRContext* mlir_context) {\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   VLOG(4) << \"ComputeTileOffsetIndexing, combining output \"\n           << ToString(output_tile_offset_indexing) << \" with operation \"\n           << tiled_hlo.ToString();\n@@ -467,9 +471,11 @@ FusionDecision ShouldProceedWithSymbolicTileDerivation(\n   if (hlo->opcode() == HloOpcode::kReshape ||\n       hlo->opcode() == HloOpcode::kBitcast) {\n     mlir::MLIRContext* ctx = indexing_map.GetMLIRContext();\n-\n+    // TODO(b/446856303): Get SymbolicExprContext from indexing_map.\n+    gpu::SymbolicExprContext symbolic_expr_context(ctx);\n     IndexingMap reshape_indexing_map =\n-        ComputeOutputToInputIndexing(hlo, /*output_id=*/0, ctx)\n+        ComputeOutputToInputIndexing(hlo, /*output_id=*/0,\n+                                     &symbolic_expr_context)\n             .indexing_maps[0]\n             .begin()\n             ->map();\n@@ -840,7 +846,7 @@ std::vector<int64_t> InputSpaceForParameterMapping(\n absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n     const HloInstruction* root,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    MLIRContext* ctx) {\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   std::vector<int64_t> input_space =\n       InputSpaceForParameterMapping(parameter_mapping);\n   int64_t num_output_parameters = root->shape().dimensions().size();\n@@ -856,12 +862,14 @@ absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n       for (int64_t parameter_index = num_hidden_parameters;\n            parameter_index < num_tiling_parameters; ++parameter_index) {\n         result_exprs.push_back(\n-            mlir::getAffineDimExpr(dim_offset + parameter_index, ctx));\n+            mlir::getAffineDimExpr(dim_offset + parameter_index,\n+                                   symbolic_expr_context->GetMLIRContext()));\n       }\n       CHECK_EQ(result_exprs.size(), num_output_parameters);\n \n       mlir::AffineMap affine_map = mlir::AffineMap::get(\n-          input_space.size(), /*symbolCount=*/0, result_exprs, ctx);\n+          input_space.size(), /*symbolCount=*/0, result_exprs,\n+          symbolic_expr_context->GetMLIRContext());\n \n       return IndexingMap::FromTensorSizes(affine_map, std::move(input_space),\n                                           /*symbol_upper_bounds=*/{});\n@@ -878,7 +886,7 @@ absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n /*static*/ absl::StatusOr<RootIndexing> SymbolicTileAnalysis::GetRootIndexing(\n     const HloFusionAdaptor& fusion,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    MLIRContext* ctx) {\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   auto fusion_adaptor_roots = fusion.GetRoots();\n \n   TF_ASSIGN_OR_RETURN(int64_t real_root_index,\n@@ -891,27 +899,30 @@ absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n   absl::InlinedVector<const HloInstruction*, 2> roots =\n       ToInstructions(fusion_adaptor_roots);\n \n-  TF_ASSIGN_OR_RETURN(IndexingMap indexing_map,\n-                      IndexingMapForRootInstruction(roots[real_root_index],\n-                                                    parameter_mapping, ctx));\n+  TF_ASSIGN_OR_RETURN(\n+      IndexingMap indexing_map,\n+      IndexingMapForRootInstruction(roots[real_root_index], parameter_mapping,\n+                                    symbolic_expr_context));\n \n   return RootIndexing{real_root_index, std::move(roots),\n                       /*real_root_indexing=*/std::move(indexing_map)};\n }\n \n /*static*/ SymbolicTileAnalysisOrError SymbolicTileAnalysis::AnalyzeComputation(\n-    const HloComputation& computation, MLIRContext* ctx,\n+    const HloComputation& computation,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder) {\n   auto fusion = HloFusionAdaptor::ForComputation(&computation);\n   return SymbolicTileAnalysis::AnalyzeFusion(\n-      *fusion, ctx, emitter_specific_constraints_builder);\n+      *fusion, symbolic_expr_context, emitter_specific_constraints_builder);\n }\n \n /*static*/ SymbolicTileAnalysisOrError\n SymbolicTileAnalysis::AnalyzeNestedFusion(\n     const HloFusionAdaptor& fusion_adaptor,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    MLIRContext* ctx, const IndexingMap& indexing_map,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n+    const IndexingMap& indexing_map,\n     IndexingMap::SimplifyPointDimensions simplification_mode,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     std::vector<SymbolicTiledHloInstruction*> root_runtime_variables) {\n@@ -929,9 +940,9 @@ SymbolicTileAnalysis::AnalyzeNestedFusion(\n                                     /*real_root_indexing=*/indexing_map};\n \n   return SymbolicTileAnalysis::AnalyzeFusionImpl(\n-      fusion_adaptor, parameter_mapping, ctx, nested_root_indexing,\n-      simplification_mode, emitter_specific_constraints_builder,\n-      root_runtime_variables);\n+      fusion_adaptor, parameter_mapping, symbolic_expr_context,\n+      nested_root_indexing, simplification_mode,\n+      emitter_specific_constraints_builder, root_runtime_variables);\n }\n \n namespace {\n@@ -1153,10 +1164,11 @@ ComposeIndexingResult ComposeInstructionIndexing(\n }\n \n std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n-    const HloInstruction* hlo, MLIRContext* ctx) {\n+    const HloInstruction* hlo,\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   std::vector<OperandIndexingSet> indexing_maps;\n   HloInstructionIndexing operands_indexing =\n-      ComputeOutputToInputIndexing(hlo, /*output_id=*/0, ctx);\n+      ComputeOutputToInputIndexing(hlo, /*output_id=*/0, symbolic_expr_context);\n   if (hlo->opcode() == HloOpcode::kPad) {\n     OperandIndexing pad_indexing_map =\n         *operands_indexing.indexing_maps[0].begin();\n@@ -1180,7 +1192,8 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n /*static*/ SymbolicTileAnalysisOrError SymbolicTileAnalysis::AnalyzeFusionImpl(\n     const HloFusionAdaptor& fusion,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    MLIRContext* ctx, const RootIndexing& root_indexing,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n+    const RootIndexing& root_indexing,\n     IndexingMap::SimplifyPointDimensions simplification_mode,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     std::vector<SymbolicTiledHloInstruction*> root_runtime_variables) {\n@@ -1214,8 +1227,8 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n       continue;  // Don't analyze parameter operands of nested fusions.\n     }\n \n-    auto operands_indexing =\n-        GetOperandIndexingMaps(tiled_hlo_instruction->hlo(), ctx);\n+    auto operands_indexing = GetOperandIndexingMaps(\n+        tiled_hlo_instruction->hlo(), symbolic_expr_context);\n \n     HloInstructionAdaptor instruction_adaptor(*hlo, &fusion);\n     for (auto [operand_pos, operand_and_indexing_map_set] : llvm::enumerate(\n@@ -1248,7 +1261,7 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n             operand.instruction().fused_instructions_computation());\n \n         auto analysis_or = SymbolicTileAnalysis::AnalyzeNestedFusion(\n-            *nested_fusion_adaptor, parameter_mapping, ctx,\n+            *nested_fusion_adaptor, parameter_mapping, symbolic_expr_context,\n             composed_indexing.indexing_map, simplification_mode,\n             emitter_specific_constraints_builder,\n             composed_indexing.rt_operands);\n@@ -1308,11 +1321,13 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n \n   return SymbolicTileAnalysis(std::move(tiled_hlo_instructions), root_indexing,\n                               std::move(tiling_specification),\n-                              std::move(emitter_specific_constraints), ctx);\n+                              std::move(emitter_specific_constraints),\n+                              symbolic_expr_context);\n }\n \n /*static*/ SymbolicTileAnalysisOrError SymbolicTileAnalysis::AnalyzeFusion(\n-    const HloFusionAdaptor& fusion, MLIRContext* ctx,\n+    const HloFusionAdaptor& fusion,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder) {\n   auto real_root_index_or = GetRealRootIndex(fusion.GetRoots());\n   if (!real_root_index_or.ok()) {\n@@ -1325,7 +1340,8 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n     return FusionDecision(parameter_mapping_or.status());\n   }\n \n-  auto root_indexing_or = GetRootIndexing(fusion, *parameter_mapping_or, ctx);\n+  auto root_indexing_or =\n+      GetRootIndexing(fusion, *parameter_mapping_or, symbolic_expr_context);\n   if (!root_indexing_or.ok()) {\n     return FusionDecision(root_indexing_or.status());\n   }\n@@ -1334,8 +1350,9 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n           ? IndexingMap::SimplifyPointDimensions::kReplace\n           : IndexingMap::SimplifyPointDimensions::kPreserve;\n \n-  return AnalyzeFusionImpl(fusion, std::move(*parameter_mapping_or), ctx,\n-                           std::move(*root_indexing_or), simplification_mode,\n+  return AnalyzeFusionImpl(fusion, std::move(*parameter_mapping_or),\n+                           symbolic_expr_context, std::move(*root_indexing_or),\n+                           simplification_mode,\n                            emitter_specific_constraints_builder,\n                            /*root_runtime_variables=*/{});\n }\n@@ -1399,9 +1416,9 @@ namespace {\n // the buffer sharing logic is adapted.\n // This method assumes that `output` has tile_offset_indexing computed, and\n // returns a FailedPrecondition error if not.\n-absl::StatusOr<bool> IsSafeForBufferSharing(const TiledHloInstruction& output,\n-                                            int64_t reference_num_output_tiles,\n-                                            mlir::MLIRContext* mlir_context) {\n+absl::StatusOr<bool> IsSafeForBufferSharing(\n+    const TiledHloInstruction& output, int64_t reference_num_output_tiles,\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   // For expanding reshapes, we can have the case that the number of\n   // blocks are different. This is not supported by the triton emitter.\n   llvm::SmallVector<int64_t> num_tiles_per_dim =\n@@ -1413,7 +1430,7 @@ absl::StatusOr<bool> IsSafeForBufferSharing(const TiledHloInstruction& output,\n   // iteration order on output tile sizes and a tile stride of 1, which means\n   // that we can take the identity map.\n   auto identity_indexing_map =\n-      CreateIdentityMap(output.hlo()->shape(), mlir_context);\n+      CreateIdentityMap(output.hlo()->shape(), symbolic_expr_context);\n   auto iota = llvm::seq<int64_t>(0, output.hlo()->shape().dimensions().size());\n   std::vector<int64_t> major_to_minor_active_tiling_parameters(iota.begin(),\n                                                                iota.end());\n@@ -1422,7 +1439,7 @@ absl::StatusOr<bool> IsSafeForBufferSharing(const TiledHloInstruction& output,\n       auto tiling_info,\n       ComputeOutputTilingInfo(identity_indexing_map, output.tile_sizes(),\n                               major_to_minor_active_tiling_parameters,\n-                              mlir_context));\n+                              symbolic_expr_context));\n \n   // Check whether the tile_offsets_indexing expression is the same as one\n   // computed directly for this root.\n@@ -1442,7 +1459,7 @@ absl::StatusOr<std::vector<const TiledHloInstruction*>> InitializeTiledRoots(\n     const std::vector<std::unique_ptr<TiledHloInstruction>>&\n         tiled_hlo_instructions,\n     absl::Span<const int64_t> num_output_tiles_per_dim,\n-    mlir::MLIRContext* mlir_context) {\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   // TODO(b/390559452): Investigate whether it is faster to use linear lookup.\n   absl::flat_hash_map<const HloInstruction*, int64_t> roots_to_output_index;\n   roots_to_output_index.reserve(roots.size());\n@@ -1473,7 +1490,7 @@ absl::StatusOr<std::vector<const TiledHloInstruction*>> InitializeTiledRoots(\n         bool valid, IsSafeForBufferSharing(*tiled_hlo_instr,\n                                            /*reference_num_output_tiles=*/\n                                            Product(num_output_tiles_per_dim),\n-                                           mlir_context));\n+                                           symbolic_expr_context));\n     if (!valid) {\n       continue;\n     }\n@@ -1518,7 +1535,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n     bool compute_all_tile_offset_indexing_maps,\n     const std::optional<absl::Span<const Interval>>&\n         parent_output_tile_dim_bounds,\n-    MLIRContext* context,\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n     absl::flat_hash_map<const SymbolicTiledHloInstruction*,\n                         TiledHloInstruction*>\n         symbolic_to_tiled_hlo_map) {\n@@ -1600,7 +1617,8 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n   TF_ASSIGN_OR_RETURN(\n       OutputTilingInfo output_tiling_info,\n       ComputeOutputTilingInfo(real_root_indexing, flat_tiling_parameters,\n-                              major_to_minor_active_tiling_parameters, context,\n+                              major_to_minor_active_tiling_parameters,\n+                              symbolic_expr_context,\n                               parent_output_tile_dim_bounds));\n \n   VLOG(3) << \"output_tiling_info: \" << output_tiling_info.ToString(\"; \");\n@@ -1641,7 +1659,8 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n           tile_offset_indexing,\n           ComputeTileOffsetIndexing(\n               *symbolic_tiled_hlo,\n-              output_tiling_info.linear_output_tile_offset_indexing, context));\n+              output_tiling_info.linear_output_tile_offset_indexing,\n+              symbolic_expr_context));\n       runtime_variables = MapToTiledInstructions(\n           symbolic_tiled_hlo->runtime_variables(), symbolic_to_tiled_hlo_map);\n       // Symbols here can only be runtime variables.\n@@ -1671,7 +1690,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n               symbolic_fusion_tiling->analysis_, flat_tiling_parameters,\n               major_to_minor_active_tiling_parameters,\n               compute_all_tile_offset_indexing_maps, fusion_tile_dim_bounds,\n-              context, symbolic_to_tiled_hlo_map));\n+              symbolic_expr_context, symbolic_to_tiled_hlo_map));\n \n       TF_ASSIGN_OR_RETURN(\n           tiled_instruction,\n@@ -1700,7 +1719,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n       auto tiled_roots,\n       InitializeTiledRoots(analysis.GetRoots(), tiled_hlo_instructions,\n                            output_tiling_info.num_output_tiles_per_dim,\n-                           context));\n+                           symbolic_expr_context));\n   return TiledHloComputation::FromSortedTiledHloInstructions(\n       std::move(tiled_hlo_instructions), tiled_roots,\n       output_tiling_info.num_output_tiles_per_dim);\n@@ -1731,7 +1750,7 @@ SymbolicTileAnalysis::ComputeTiledHloInstructions(\n       *this, flat_tiling_parameters,\n       /*major_to_minor_active_tiling_parameters=*/{},\n       compute_all_tile_offset_indexing_maps,\n-      /*parent_output_tile_dim_bounds=*/std::nullopt, context_,\n+      /*parent_output_tile_dim_bounds=*/std::nullopt, symbolic_expr_context_,\n       /*symbolic_to_tiled_hlo_map=*/{});\n }\n "
        },
        {
            "sha": "7e4d3338d7534ab49cca70c91ae6a9a9fcab62bf",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis.h",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/instruction_fusion.h\"\n \n namespace xla {\n@@ -124,11 +125,12 @@ class SymbolicTileAnalysis {\n   // tiles of these operands may contain expressions with symbols which would\n   // fail to be tiled.\n   static SymbolicTileAnalysisOrError AnalyzeComputation(\n-      const HloComputation& computation, mlir::MLIRContext* ctx,\n+      const HloComputation& computation,\n+      gpu::SymbolicExprContext* symbolic_expr_context,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder =\n           nullptr);\n   static SymbolicTileAnalysisOrError AnalyzeFusion(\n-      const HloFusionAdaptor& fusion, mlir::MLIRContext* ctx,\n+      const HloFusionAdaptor& fusion, gpu::SymbolicExprContext* ctx,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder =\n           nullptr);\n \n@@ -191,7 +193,9 @@ class SymbolicTileAnalysis {\n       const ::xla::Tiling& tiling) const;\n \n   // Return the underlying MLIRContext.\n-  mlir::MLIRContext* GetMLIRContext() const { return context_; };\n+  mlir::MLIRContext* GetMLIRContext() const {\n+    return symbolic_expr_context_->GetMLIRContext();\n+  };\n \n   // Returns a string representation of the analysis. Used only for error\n   // messages and debugging.\n@@ -208,24 +212,25 @@ class SymbolicTileAnalysis {\n       const RootIndexing& root_indexing,\n       TilingSpecification tiling_specification,\n       std::unique_ptr<EmitterSpecificConstraints> emitter_specific_constraints,\n-      mlir::MLIRContext* context)\n+      gpu::SymbolicExprContext* symbolic_expr_context)\n       : symbolic_tiled_hlo_instructions_(\n             std::move(symbolic_tiled_hlo_instructions)),\n         root_indexing_(std::move(root_indexing)),\n         tiling_specification_(std::move(tiling_specification)),\n         emitter_specific_constraints_(std::move(emitter_specific_constraints)),\n-        context_(context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   // Computes indexing information for the roots of the computation.\n   static absl::StatusOr<RootIndexing> GetRootIndexing(\n       const HloFusionAdaptor& fusion,\n       const TilingSpecification::ParameterMapping& parameter_mapping,\n-      mlir::MLIRContext* ctx);\n+      gpu::SymbolicExprContext* symbolic_expr_context);\n \n   static SymbolicTileAnalysisOrError AnalyzeFusionImpl(\n       const HloFusionAdaptor& fusion,\n       const TilingSpecification::ParameterMapping& parameter_mapping,\n-      mlir::MLIRContext* ctx, const RootIndexing& root_indexing,\n+      gpu::SymbolicExprContext* symbolic_expr_context,\n+      const RootIndexing& root_indexing,\n       IndexingMap::SimplifyPointDimensions simplification_mode,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n       std::vector<SymbolicTiledHloInstruction*> root_runtime_variables);\n@@ -234,7 +239,8 @@ class SymbolicTileAnalysis {\n   static SymbolicTileAnalysisOrError AnalyzeNestedFusion(\n       const HloFusionAdaptor& fusion,\n       const TilingSpecification::ParameterMapping& parameter_mapping,\n-      mlir::MLIRContext* ctx, const IndexingMap& indexing_map,\n+      gpu::SymbolicExprContext* symbolic_expr_context,\n+      const IndexingMap& indexing_map,\n       IndexingMap::SimplifyPointDimensions simplification_mode,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n       std::vector<SymbolicTiledHloInstruction*> root_runtime_variables);\n@@ -254,7 +260,7 @@ class SymbolicTileAnalysis {\n   // no builder was provided when constructing the analysis.\n   std::unique_ptr<EmitterSpecificConstraints> emitter_specific_constraints_;\n \n-  mlir::MLIRContext* context_;\n+  gpu::SymbolicExprContext* symbolic_expr_context_;\n };\n \n namespace detail {"
        },
        {
            "sha": "cdd6f8d63ccabf6a69b0331f11a6b36079ff43b0",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -50,6 +50,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -147,7 +148,7 @@ class SymbolicTileAnalysisTest : public HloHardwareIndependentTestBase {\n             *module->entry_computation()\n                  ->root_instruction()\n                  ->fused_instructions_computation(),\n-            &mlir_context_, emitter_specific_constraints_builder);\n+            &symbolic_expr_context_, emitter_specific_constraints_builder);\n \n     if (std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error)) {\n       SymbolicTileAnalysis analysis =\n@@ -188,6 +189,7 @@ class SymbolicTileAnalysisTest : public HloHardwareIndependentTestBase {\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(SymbolicTileAnalysisTest, SimpleNormalizationDiamondIsSupported) {\n@@ -357,7 +359,7 @@ ENTRY main {\n   auto fusion = HloFusionAdaptor::ForProducerConsumer(producer, consumer);\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n-      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &mlir_context_);\n+      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &symbolic_expr_context_);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n   SymbolicTileAnalysis analysis =\n       std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n@@ -424,7 +426,7 @@ ENTRY entry_computation {\n       producer, consumer, /*with_extra_outputs=*/true);\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n-      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &mlir_context_);\n+      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &symbolic_expr_context_);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n   SymbolicTileAnalysis analysis =\n       std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n@@ -1810,7 +1812,8 @@ ENTRY main {\n           *module->entry_computation()\n                ->root_instruction()\n                ->fused_instructions_computation(),\n-          &mlir_context_, /*emitter_specific_constraints_builder=*/nullptr);\n+          &symbolic_expr_context_,\n+          /*emitter_specific_constraints_builder=*/nullptr);\n \n   ASSERT_TRUE(std::holds_alternative<FusionDecision>(analysis_or_error));\n   EXPECT_THAT(std::get<FusionDecision>(analysis_or_error).Explain(),"
        },
        {
            "sha": "40073d07ba396805c7a94c41b98cb2da56b675a7",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tiled_hlo_instruction_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tiled_hlo_instruction_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tiled_hlo_instruction_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tiled_hlo_instruction_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -53,11 +54,12 @@ ENTRY main {\n )\"));\n \n   mlir::MLIRContext mlir_ctx;\n+  gpu::SymbolicExprContext symbolic_expr_context(&mlir_ctx);\n   auto fusion = module->entry_computation()->root_instruction();\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(fusion);\n \n   auto output_to_input_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_ctx);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context);\n \n   HloInstruction* subtract = fusion->fused_expression_root();\n   HloInstruction* p0 = subtract->mutable_operand(0)->mutable_operand(0);"
        },
        {
            "sha": "559e18847b1347754cd6a25b0295606c4c7a248e",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_schedule.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,13 +28,15 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n \n absl::StatusOr<IndexingMap> MajorToMinorTiledHloSchedule::Schedule(\n     const IndexingMap& tile_offsets_indexing, IterationSpace iteration_space,\n-    mlir::MLIRContext* ctx) const {\n+    gpu::SymbolicExprContext* symbolic_expr_context) const {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   if (iteration_space.size() != tile_offsets_indexing.GetDimVarsCount()) {\n     return absl::InvalidArgumentError(absl::StrFormat(\n         \"Expected iteration space to have exactly as many dimensions as there \"\n@@ -43,7 +45,7 @@ absl::StatusOr<IndexingMap> MajorToMinorTiledHloSchedule::Schedule(\n         iteration_space.size(), tile_offsets_indexing.GetDimVarsCount()));\n   }\n \n-  mlir::AffineExpr program_id = mlir::getAffineDimExpr(0, ctx);\n+  mlir::AffineExpr program_id = mlir::getAffineDimExpr(0, mlir_context);\n \n   std::vector<int64_t> iteration_space_sizes;\n   iteration_space_sizes.reserve(iteration_space.size());\n@@ -53,11 +55,11 @@ absl::StatusOr<IndexingMap> MajorToMinorTiledHloSchedule::Schedule(\n \n   std::vector<mlir::AffineExpr> tile_exprs(\n       tile_offsets_indexing.GetDimVarsCount(),\n-      mlir::getAffineConstantExpr(0, ctx));\n+      mlir::getAffineConstantExpr(0, mlir_context));\n \n-  for (auto [dim_info, tile_expr] :\n-       llvm::zip(iteration_space,\n-                 DelinearizeIndex(iteration_space_sizes, program_id, ctx))) {\n+  for (auto [dim_info, tile_expr] : llvm::zip(\n+           iteration_space, DelinearizeIndex(iteration_space_sizes, program_id,\n+                                             symbolic_expr_context))) {\n     if (dim_info.dimension_id >= tile_exprs.size()) {\n       return absl::InvalidArgumentError(absl::StrFormat(\n           \"Dimension id %d is out of bounds for tile offsets indexing map with \"\n@@ -70,7 +72,7 @@ absl::StatusOr<IndexingMap> MajorToMinorTiledHloSchedule::Schedule(\n       {0, Product(iteration_space_sizes) - 1, \"pid_0\"}};\n   IndexingMap program_id_to_output_dims{\n       mlir::AffineMap::get(\n-          /*dimCount=*/1, /*symbolCount=*/0, tile_exprs, ctx),\n+          /*dimCount=*/1, /*symbolCount=*/0, tile_exprs, mlir_context),\n       dim_vars, /*range_vars=*/{}, /*rt_vars=*/{}};\n   auto scheduled_indexing =\n       ComposeIndexingMaps(program_id_to_output_dims, tile_offsets_indexing);"
        },
        {
            "sha": "be3e1caf4695ebd20eae8aa422dd398a97b7c6c0",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_schedule.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -20,8 +20,8 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n \n@@ -68,7 +68,7 @@ class TiledHloSchedule {\n   //     themselves);\n   virtual absl::StatusOr<IndexingMap> Schedule(\n       const IndexingMap& tile_offsets_indexing, IterationSpace iteration_space,\n-      mlir::MLIRContext* ctx) const = 0;\n+      gpu::SymbolicExprContext* symbolic_expr_context) const = 0;\n };\n \n // The indexing map returned by this schedule iterates over the iteration space\n@@ -77,9 +77,9 @@ class TiledHloSchedule {\n // dimension).\n class MajorToMinorTiledHloSchedule : public TiledHloSchedule {\n  public:\n-  absl::StatusOr<IndexingMap> Schedule(const IndexingMap& tile_offsets_indexing,\n-                                       IterationSpace iteration_space,\n-                                       mlir::MLIRContext* ctx) const override;\n+  absl::StatusOr<IndexingMap> Schedule(\n+      const IndexingMap& tile_offsets_indexing, IterationSpace iteration_space,\n+      gpu::SymbolicExprContext* symbolic_expr_context) const override;\n };\n \n // TODO(b/417977182): implement the `PlanarSnakeTiledHloSchedule` schedule."
        },
        {
            "sha": "1e22bdf8029f850946c250cc225c0342e67c85e3",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_schedule_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/hlo/analysis/indexing_map_serialization.h\"\n #include \"xla/hlo/analysis/interval.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -38,7 +39,8 @@ using ::testing::HasSubstr;\n \n class TiledHloScheduleTest : public HloHardwareIndependentTestBase {\n  protected:\n-  mlir::MLIRContext ctx_;\n+  mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n using MajorToMinorTiledHloScheduleTest = TiledHloScheduleTest;\n@@ -48,7 +50,7 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   IndexingMap offsets_indexing = *ParseIndexingMap(R\"(\n       (d0, d1, d2, d3) -> (d2, d3),\n       domain: d0 in [0, 1], d1 in [0, 2], d2 in [0, 4], d3 in [0, 6])\",\n-                                                   &ctx_);\n+                                                   &symbolic_expr_context_);\n   auto bound = [&offsets_indexing](int64_t dim) {\n     return offsets_indexing.GetDimensionBound(dim).upper + 1;\n   };\n@@ -60,9 +62,9 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   };\n \n   MajorToMinorTiledHloSchedule scheduler;\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      IndexingMap scheduled_indexing,\n-      scheduler.Schedule(offsets_indexing, iteration_space, &ctx_));\n+  TF_ASSERT_OK_AND_ASSIGN(IndexingMap scheduled_indexing,\n+                          scheduler.Schedule(offsets_indexing, iteration_space,\n+                                             &symbolic_expr_context_));\n \n   // (1) the map must have a single input whose range of values is the size of\n   //     the iteration space (i.e. the product of `iteration_space`'s\n@@ -80,7 +82,7 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   EXPECT_EQ(scheduled_indexing, *ParseIndexingMap(R\"(\n     (pid_0) -> (pid_0 floordiv 42, pid_0 mod 7), domain: pid_0 in [0, 209]\n   )\",\n-                                                  &ctx_));\n+                                                  &symbolic_expr_context_));\n \n   // `pid_0 floordiv 42` has the same upper bound as `d2`.\n   EXPECT_EQ(iteration_space_size / 42, bound(2));\n@@ -90,13 +92,15 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n \n TEST_F(MajorToMinorTiledHloScheduleTest,\n        MajorToMinorTiledHloScheduleFailsForInvalidIterationSpace) {\n-  IndexingMap offsets_indexing = *ParseIndexingMap(\n-      \"(d0, d1) -> (d1), domain: d0 in [0, 1], d1 in [0, 2]\", &ctx_);\n+  IndexingMap offsets_indexing =\n+      *ParseIndexingMap(\"(d0, d1) -> (d1), domain: d0 in [0, 1], d1 in [0, 2]\",\n+                        &symbolic_expr_context_);\n   MajorToMinorTiledHloSchedule scheduler;\n \n   // The iteration space has the wrong number of dimensions.\n   EXPECT_THAT(\n-      scheduler.Schedule(offsets_indexing, /*iteration_space=*/{}, &ctx_),\n+      scheduler.Schedule(offsets_indexing, /*iteration_space=*/{},\n+                         &symbolic_expr_context_),\n       StatusIs(\n           absl::StatusCode::kInvalidArgument,\n           HasSubstr(\n@@ -106,7 +110,7 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   EXPECT_THAT(scheduler.Schedule(offsets_indexing, /*iteration_space=*/\n                                  {{/*dimension_id=*/0, /*dimension_size=*/1},\n                                   {/*dimension_id=*/2, /*dimension_size=*/0}},\n-                                 &ctx_),\n+                                 &symbolic_expr_context_),\n               StatusIs(absl::StatusCode::kInvalidArgument,\n                        HasSubstr(\"Dimension id 2 is out of bounds\")));\n }"
        },
        {
            "sha": "63ddaf46c4114735644e428352305f683474cb8a",
            "filename": "third_party/xla/xla/codegen/tiling/tiling_specification_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiling_specification_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiling_specification_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiling_specification_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -53,13 +54,15 @@ class TilingSpecificationTest : public HloHardwareIndependentTestBase {\n             *module->entry_computation()\n                  ->root_instruction()\n                  ->fused_instructions_computation(),\n-            &mlir_context_, /*emitter_specific_constraints_builder=*/nullptr);\n+            &symbolic_expr_context_,\n+            /*emitter_specific_constraints_builder=*/nullptr);\n \n     CHECK(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n     return std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(TilingSpecificationTest, TilingSpecificationDerivesOutputParameters) {"
        },
        {
            "sha": "9bdd19581ff70b34b130cfca9513016ce17ea5d5",
            "filename": "third_party/xla/xla/hlo/analysis/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -668,6 +668,7 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:gather_simplifier\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:matmul_indexing_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n@@ -713,8 +714,8 @@ xla_cc_test(\n     deps = [\n         \":indexing_analysis\",\n         \":indexing_test_utils\",\n-        \":interval\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\","
        },
        {
            "sha": "448d3e964e1265b6dea3096943f4f9fc46a3933a",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.cc",
            "status": "modified",
            "additions": 217,
            "deletions": 137,
            "changes": 354,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -54,6 +54,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/layout.h\"\n #include \"xla/permutation_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/matmul_indexing_utils.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -63,6 +64,7 @@ limitations under the License.\n namespace xla {\n namespace {\n \n+using gpu::SymbolicExprContext;\n using llvm::SmallVector;\n using mlir::AffineExpr;\n using mlir::AffineMap;\n@@ -112,8 +114,8 @@ inline bool operator!=(const HLORTVar& lhs, const HLORTVar& rhs) {\n // Note: we had a more complex logic here that handled more instruction types\n // but was removed due to previous version not updating value ranges\n // (b/419279949).\n-std::optional<AffineExpr> OptimizeRTVar(HLORTVar rt_var,\n-                                        MLIRContext* mlir_context) {\n+std::optional<AffineExpr> OptimizeRTVar(\n+    HLORTVar rt_var, SymbolicExprContext* symbolic_expr_context) {\n   if (auto constant_expr = DynCast<HloConstantInstruction>(rt_var.hlo)) {\n     if (rt_var.map.isConstant()) {\n       const auto idx = rt_var.map.getConstantResults();\n@@ -123,7 +125,8 @@ std::optional<AffineExpr> OptimizeRTVar(HLORTVar rt_var,\n         // the runtime to handle that.\n         return std::nullopt;\n       }\n-      return getAffineConstantExpr(const_value, mlir_context);\n+      return getAffineConstantExpr(const_value,\n+                                   symbolic_expr_context->GetMLIRContext());\n     }\n   }\n   if (auto iota_expr = DynCast<HloIotaInstruction>(rt_var.hlo)) {\n@@ -148,12 +151,15 @@ IndexingMap FoldRTVarsAndConstructIndexingMap(\n     AffineMap affine_map, std::vector<IndexingMap::Variable> dim_vars,\n     std::vector<HLORTVar> hlo_rt_vars) {\n   auto* ctx = affine_map.getContext();\n+  // TODO (b/446856820): Get context from SymbolicMap after refactoring.\n+  SymbolicExprContext symbolic_expr_context(ctx);\n   // Range and runtime variables share the symbol space in the affine map but\n   // currently we never have range variables here.\n   CHECK_EQ(affine_map.getNumSymbols(), hlo_rt_vars.size());\n   for (auto idx = 0; idx < affine_map.getNumSymbols(); ++idx) {\n     auto& rt_var = hlo_rt_vars[idx];\n-    std::optional<AffineExpr> result = OptimizeRTVar(rt_var, ctx);\n+    std::optional<AffineExpr> result =\n+        OptimizeRTVar(rt_var, &symbolic_expr_context);\n     if (!result) {\n       continue;\n     }\n@@ -186,8 +192,10 @@ OperandIndexing CreateOperandIndexingWithRTVars(\n }\n \n HloInstructionIndexing ComputeOutputToInputCwiseOpIndexing(\n-    const HloInstruction* instr, MLIRContext* mlir_context) {\n-  IndexingMap identity_map = CreateIdentityMap(instr->shape(), mlir_context);\n+    const HloInstruction* instr, SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+  IndexingMap identity_map =\n+      CreateIdentityMap(instr->shape(), symbolic_expr_context);\n   IndexingMap unit_map(\n       mlir::AffineMap::get(identity_map.GetAffineMap().getNumDims(),\n                            /*symbolCount=*/0, mlir_context),\n@@ -211,13 +219,16 @@ HloInstructionIndexing ComputeOutputToInputCwiseOpIndexing(\n }\n \n HloInstructionIndexing ComputeInputToOutputCwiseOpIndexing(\n-    const HloInstruction* instr, MLIRContext* mlir_context) {\n-  IndexingMap identity_map = CreateIdentityMap(instr->shape(), mlir_context);\n+    const HloInstruction* instr, SymbolicExprContext* symbolic_expr_context) {\n+  IndexingMap identity_map =\n+      CreateIdentityMap(instr->shape(), symbolic_expr_context);\n   return HloInstructionIndexing::FromIndexingMaps({identity_map});\n }\n \n HloInstructionIndexing ComputeOutputToInputBroadcastOpIndexing(\n-    const HloBroadcastInstruction* bcast, MLIRContext* mlir_context) {\n+    const HloBroadcastInstruction* bcast,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto output_dims = bcast->shape().dimensions();\n \n   std::vector<AffineExpr> exprs;\n@@ -233,7 +244,9 @@ HloInstructionIndexing ComputeOutputToInputBroadcastOpIndexing(\n }\n \n HloInstructionIndexing ComputeInputToOutputBroadcastOpIndexing(\n-    const HloBroadcastInstruction* bcast, MLIRContext* mlir_context) {\n+    const HloBroadcastInstruction* bcast,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   absl::Span<const int64_t> bcast_dims = bcast->dimensions();\n \n   const Shape& input_shape = bcast->operand(0)->shape();\n@@ -264,7 +277,9 @@ HloInstructionIndexing ComputeInputToOutputBroadcastOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputConcatenateOpIndexing(\n-    const HloConcatenateInstruction* concat, MLIRContext* mlir_context) {\n+    const HloConcatenateInstruction* concat,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   const auto& operand_0_dims = concat->operand(0)->shape().dimensions();\n \n   // Initialize affine map and domain. Only concat_dim elements of both have to\n@@ -294,7 +309,8 @@ HloInstructionIndexing ComputeOutputToInputConcatenateOpIndexing(\n \n HloInstructionIndexing ComputeInputToOutputConcatenateOpIndexing(\n     const HloConcatenateInstruction* concat, int input_id,\n-    MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   int64_t concat_dim = concat->concatenate_dimension();\n   int64_t offset = 0;\n   for (int64_t operand_id = 0; operand_id < input_id; ++operand_id) {\n@@ -316,10 +332,11 @@ HloInstructionIndexing ComputeInputToOutputConcatenateOpIndexing(\n // until the HloParameterInstruction is found.\n HloInstructionIndexing ComputeOutputToInputFusionOpIndexing(\n     const HloFusionInstruction* fusion, int output_id,\n-    MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(fusion);\n   auto grouped_indexing_maps = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[output_id], mlir_context);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[output_id],\n+      symbolic_expr_context);\n \n   // After the traversal, `grouped_indexing_maps` is keyed by\n   // HloParameterInstructions. Convert them back to the operand id and return.\n@@ -333,7 +350,9 @@ HloInstructionIndexing ComputeOutputToInputFusionOpIndexing(\n \n std::pair<IndexingMap, IndexingMap> ComputeDotOperandsIndexingImpl(\n     const Shape& lhs_shape, const Shape& rhs_shape, const Shape& output_shape,\n-    const DotDimensionNumbers& dim_numbers, MLIRContext* mlir_context) {\n+    const DotDimensionNumbers& dim_numbers,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   absl::Span<const int64_t> lhs_contracting_dims(\n       dim_numbers.lhs_contracting_dimensions());\n   absl::Span<const int64_t> rhs_contracting_dims =\n@@ -428,26 +447,27 @@ IndexingMap RescaleIndexingMap(const IndexingMap& operand_map,\n }\n \n HloInstructionIndexing ComputeOutputToInputDotOpIndexing(\n-    const HloDotInstruction* dot, MLIRContext* mlir_context) {\n+    const HloDotInstruction* dot, SymbolicExprContext* symbolic_expr_context) {\n   const Shape& lhs_shape = dot->operand(0)->shape();\n   const Shape& rhs_shape = dot->operand(1)->shape();\n \n   auto [lhs_map, rhs_map] = ComputeDotOperandsIndexingImpl(\n       lhs_shape, rhs_shape, dot->shape(), dot->dot_dimension_numbers(),\n-      mlir_context);\n+      symbolic_expr_context);\n   return HloInstructionIndexing::FromIndexingMaps({lhs_map, rhs_map});\n }\n \n HloInstructionIndexing ComputeOutputToInputScaledDotOpIndexing(\n-    const HloScaledDotInstruction* scaled_dot, MLIRContext* mlir_context) {\n+    const HloScaledDotInstruction* scaled_dot,\n+    SymbolicExprContext* symbolic_expr_context) {\n   const Shape& lhs_shape = scaled_dot->operand(0)->shape();\n   const Shape& rhs_shape = scaled_dot->operand(1)->shape();\n   const Shape& lhs_scale_shape = scaled_dot->operand(2)->shape();\n   const Shape& rhs_scale_shape = scaled_dot->operand(3)->shape();\n \n   auto [lhs_map, rhs_map] = ComputeDotOperandsIndexingImpl(\n       lhs_shape, rhs_shape, scaled_dot->shape(),\n-      scaled_dot->dot_dimension_numbers(), mlir_context);\n+      scaled_dot->dot_dimension_numbers(), symbolic_expr_context);\n \n   IndexingMap lhs_scale_map =\n       RescaleIndexingMap(lhs_map, lhs_shape, lhs_scale_shape);\n@@ -460,7 +480,8 @@ HloInstructionIndexing ComputeOutputToInputScaledDotOpIndexing(\n \n HloInstructionIndexing ComputeOutputToInputDynamicSliceOpIndexing(\n     const HloDynamicSliceInstruction* dynamic_slice,\n-    MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   const Shape& input_shape = dynamic_slice->operand(0)->shape();\n   const Shape& output_shape = dynamic_slice->shape();\n   int64_t rank = output_shape.dimensions().size();\n@@ -502,7 +523,9 @@ HloInstructionIndexing ComputeOutputToInputDynamicSliceOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputDynamicUpdateSliceOpIndexing(\n-    const HloDynamicUpdateSliceInstruction* dus, MLIRContext* mlir_context) {\n+    const HloDynamicUpdateSliceInstruction* dus,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   const Shape& update_shape = dus->update()->shape();\n   const Shape& output_shape = dus->shape();\n   int64_t rank = output_shape.dimensions().size();\n@@ -549,7 +572,9 @@ HloInstructionIndexing ComputeOutputToInputDynamicUpdateSliceOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputGatherOpIndexing(\n-    const HloGatherInstruction* gather, MLIRContext* mlir_context) {\n+    const HloGatherInstruction* gather,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   CHECK(GatherSimplifier::IsSimplifiedGather(gather))\n       << \"Non-simplified HLO Gather is not supported.\";\n   const Shape& operand_shape = gather->operand(0)->shape();\n@@ -615,7 +640,9 @@ IndexingMap ComputeOutputToInputPadOpIndexingImpl(\n     absl::Span<const int64_t> output_dims,\n     absl::Span<const int64_t> padding_low,\n     absl::Span<const int64_t> padding_high,\n-    absl::Span<const int64_t> padding_interior, MLIRContext* mlir_context) {\n+    absl::Span<const int64_t> padding_interior,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   int64_t output_rank = output_dims.size();\n \n   std::vector<AffineExpr> exprs;\n@@ -647,7 +674,8 @@ IndexingMap ComputeOutputToInputPadOpIndexingImpl(\n }\n \n HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n-    const HloPadInstruction* pad, MLIRContext* mlir_context) {\n+    const HloPadInstruction* pad, SymbolicExprContext* symbolic_expr_context) {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   const Shape& output_shape = pad->shape();\n   int64_t rank = output_shape.dimensions().size();\n   SmallVector<int64_t> padding_low, padding_high, padding_interior;\n@@ -661,7 +689,7 @@ HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n   }\n   IndexingMap input_indexing_map = ComputeOutputToInputPadOpIndexingImpl(\n       output_shape.dimensions(), padding_low, padding_high, padding_interior,\n-      mlir_context);\n+      symbolic_expr_context);\n   IndexingMap padding_value_indexing_map = IndexingMap::FromTensorSizes(\n       AffineMap::get(output_shape.dimensions().size(), /*symbolCount=*/0, {},\n                      mlir_context),\n@@ -671,7 +699,9 @@ HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputReduceOpIndexing(\n-    const HloReduceInstruction* reduce, MLIRContext* mlir_context) {\n+    const HloReduceInstruction* reduce,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   absl::flat_hash_set<int64_t> reduce_dims_ids(reduce->dimensions().begin(),\n                                                reduce->dimensions().end());\n \n@@ -716,7 +746,8 @@ HloInstructionIndexing ComputeOutputToInputReduceOpIndexing(\n \n HloInstructionIndexing ComputeInputToOutputReduceOpIndexing(\n     const HloReduceInstruction* reduce, int input_id,\n-    MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   const Shape& output_shape = GetOutputShape(reduce, 0);\n   int64_t output_rank = output_shape.dimensions().size();\n \n@@ -766,7 +797,8 @@ HloInstructionIndexing ComputeInputToOutputReduceOpIndexing(\n IndexingMap ComposeIndexingMapsForWindow(\n     absl::Span<const int64_t> input_dimensions,\n     absl::Span<const int64_t> output_dimensions, const Window& window,\n-    MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   size_t rank = input_dimensions.size();\n \n   // Compute shape of the padded input and the indexing map of pad op required\n@@ -806,7 +838,7 @@ IndexingMap ComposeIndexingMapsForWindow(\n   // Indexing map for pad op that pads the input.\n   IndexingMap padded_input_indexing = ComputeOutputToInputPadOpIndexingImpl(\n       padded_input_dimensions, padding_low, padding_high, padding_interior,\n-      mlir_context);\n+      symbolic_expr_context);\n   // Indexing map for reduce-window, that does not do any padding.\n   IndexingMap input_indexing_no_padding(\n       AffineMap::get(rank, rank, exprs, mlir_context), dim_vars, range_vars,\n@@ -825,14 +857,15 @@ IndexingMap ComposeIndexingMapsForWindow(\n // of bounds.\n HloInstructionIndexing ComputeOutputToInputReduceWindowOpIndexing(\n     const HloReduceWindowInstruction* reduce_window,\n-    MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   const Shape& input_shape = reduce_window->operand(0)->shape();\n   const Shape& output_shape = GetOutputShape(reduce_window, 0);\n \n   // Indexing map for the input value.\n   IndexingMap inputs_indexing = ComposeIndexingMapsForWindow(\n       input_shape.dimensions(), output_shape.dimensions(),\n-      reduce_window->window(), mlir_context);\n+      reduce_window->window(), symbolic_expr_context);\n \n   // Indexing map for the init value.\n   IndexingMap inits_indexing_map = IndexingMap::FromTensorSizes(\n@@ -854,7 +887,9 @@ HloInstructionIndexing ComputeOutputToInputReduceWindowOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputConvolutionOpIndexing(\n-    const HloConvolutionInstruction* convolution, MLIRContext* mlir_context) {\n+    const HloConvolutionInstruction* convolution,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   const Shape& input_shape = convolution->operand(0)->shape();\n   const Shape& kernel_shape = convolution->operand(1)->shape();\n   const Shape& output_shape = convolution->shape();\n@@ -879,9 +914,9 @@ HloInstructionIndexing ComputeOutputToInputConvolutionOpIndexing(\n   // Indexing map for the input value (spatial dimensions only).\n   // The dimension numbers in the resulting affine expressions have to be\n   // remapped to correspond to the correct output dimensions.\n-  IndexingMap input_spatial_indexing =\n-      ComposeIndexingMapsForWindow(input_spatial_sizes, output_spatial_sizes,\n-                                   convolution->window(), mlir_context);\n+  IndexingMap input_spatial_indexing = ComposeIndexingMapsForWindow(\n+      input_spatial_sizes, output_spatial_sizes, convolution->window(),\n+      symbolic_expr_context);\n   std::vector<AffineExpr> replacement_dims(spatial_rank);\n   for (int i = 0; i < spatial_rank; ++i) {\n     replacement_dims[i] =\n@@ -985,8 +1020,9 @@ std::vector<int64_t> ComputeStrides(absl::Span<const int64_t> dims) {\n \n AffineExpr LinearizeShape(absl::Span<const int64_t> dims,\n                           absl::Span<const AffineExpr> dimension_exprs,\n-                          MLIRContext* mlir_context) {\n-  AffineExpr linear_index = getAffineConstantExpr(0, mlir_context);\n+                          SymbolicExprContext* symbolic_expr_context) {\n+  AffineExpr linear_index =\n+      getAffineConstantExpr(0, symbolic_expr_context->GetMLIRContext());\n \n   auto strides = ComputeStrides(dims);\n   for (auto [stride, dimension_expr] : llvm::zip(strides, dimension_exprs)) {\n@@ -995,9 +1031,9 @@ AffineExpr LinearizeShape(absl::Span<const int64_t> dims,\n   return linear_index;\n }\n \n-std::vector<AffineExpr> DelinearizeIndex(absl::Span<const int64_t> dims,\n-                                         AffineExpr linear_index,\n-                                         MLIRContext* mlir_context) {\n+std::vector<AffineExpr> DelinearizeIndex(\n+    absl::Span<const int64_t> dims, AffineExpr linear_index,\n+    SymbolicExprContext* symbolic_expr_context) {\n   std::vector<AffineExpr> multi_index;\n   multi_index.reserve(dims.size());\n \n@@ -1029,7 +1065,8 @@ namespace {\n void ComputeMinimalReshapeIndexing(\n     absl::Span<const int64_t> input_dims, absl::Span<const int64_t> output_dims,\n     absl::Span<const AffineExpr> output_dims_exprs,\n-    std::vector<AffineExpr>* exprs, MLIRContext* mlir_context) {\n+    std::vector<AffineExpr>* exprs,\n+    SymbolicExprContext* symbolic_expr_context) {\n   // The shape does not change.\n   if (input_dims.size() == 1 && output_dims.size() == 1) {\n     absl::c_copy(output_dims_exprs, std::back_inserter(*exprs));\n@@ -1038,20 +1075,21 @@ void ComputeMinimalReshapeIndexing(\n   // Expand shape.\n   if (input_dims.size() == 1) {\n     exprs->push_back(\n-        LinearizeShape(output_dims, output_dims_exprs, mlir_context));\n+        LinearizeShape(output_dims, output_dims_exprs, symbolic_expr_context));\n     return;\n   }\n   // Collapse shape.\n   if (output_dims.size() == 1) {\n-    auto multi_index =\n-        DelinearizeIndex(input_dims, output_dims_exprs.front(), mlir_context);\n+    auto multi_index = DelinearizeIndex(input_dims, output_dims_exprs.front(),\n+                                        symbolic_expr_context);\n     absl::c_copy(multi_index, std::back_inserter(*exprs));\n     return;\n   }\n   // Generic case.\n   AffineExpr linear_index =\n-      LinearizeShape(output_dims, output_dims_exprs, mlir_context);\n-  auto multi_index = DelinearizeIndex(input_dims, linear_index, mlir_context);\n+      LinearizeShape(output_dims, output_dims_exprs, symbolic_expr_context);\n+  auto multi_index =\n+      DelinearizeIndex(input_dims, linear_index, symbolic_expr_context);\n   absl::c_copy(multi_index, std::back_inserter(*exprs));\n }\n \n@@ -1071,8 +1109,10 @@ void ComputeMinimalReshapeIndexing(\n // This is an optimization that allows us to construct simpler affine maps,\n // otherwise we would need to linearize/delinearize even some of the simpler\n // cases.\n-AffineMap ComputeReshapeIndexingMap(const Shape& input, const Shape& output,\n-                                    MLIRContext* mlir_context) {\n+AffineMap ComputeReshapeIndexingMap(\n+    const Shape& input, const Shape& output,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   absl::Span<const int64_t> input_dims = input.dimensions();\n   absl::Span<const int64_t> output_dims = output.dimensions();\n \n@@ -1116,7 +1156,8 @@ AffineMap ComputeReshapeIndexingMap(const Shape& input, const Shape& output,\n       continue;\n     }\n     ComputeMinimalReshapeIndexing(input_subshape, output_subshape,\n-                                  output_dims_exprs, &exprs, mlir_context);\n+                                  output_dims_exprs, &exprs,\n+                                  symbolic_expr_context);\n     input_num_elements = 1;\n     output_num_elements = 1;\n     input_subshape.clear();\n@@ -1128,32 +1169,36 @@ AffineMap ComputeReshapeIndexingMap(const Shape& input, const Shape& output,\n };\n \n HloInstructionIndexing ComputeOutputToInputReshapeOpIndexing(\n-    const HloReshapeInstruction* reshape, MLIRContext* mlir_context) {\n+    const HloReshapeInstruction* reshape,\n+    SymbolicExprContext* symbolic_expr_context) {\n   const auto& input = reshape->operand(0)->shape();\n   const auto& output = reshape->shape();\n \n   IndexingMap reshape_indexing_map = IndexingMap::FromTensorSizes(\n-      ComputeReshapeIndexingMap(input, output, mlir_context),\n+      ComputeReshapeIndexingMap(input, output, symbolic_expr_context),\n       output.dimensions(), {});\n   reshape_indexing_map.Simplify(\n       IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({reshape_indexing_map});\n }\n HloInstructionIndexing ComputeInputToOutputReshapeOpIndexing(\n-    const HloReshapeInstruction* reshape, MLIRContext* mlir_context) {\n+    const HloReshapeInstruction* reshape,\n+    SymbolicExprContext* symbolic_expr_context) {\n   const auto& input = reshape->operand(0)->shape();\n   const auto& output = reshape->shape();\n \n   IndexingMap reshape_indexing_map = IndexingMap::FromTensorSizes(\n-      ComputeReshapeIndexingMap(output, input, mlir_context),\n+      ComputeReshapeIndexingMap(output, input, symbolic_expr_context),\n       input.dimensions(), {});\n   reshape_indexing_map.Simplify(\n       IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({reshape_indexing_map});\n }\n \n HloInstructionIndexing ComputeReverseOpIndexing(\n-    const HloReverseInstruction* reverse, MLIRContext* mlir_context) {\n+    const HloReverseInstruction* reverse,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   absl::flat_hash_set<int64_t> reverse_dims(reverse->dimensions().begin(),\n                                             reverse->dimensions().end());\n   auto output_dims = reverse->shape().dimensions();\n@@ -1178,7 +1223,9 @@ HloInstructionIndexing ComputeReverseOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputSliceOpIndexing(\n-    const HloSliceInstruction* slice, MLIRContext* mlir_context) {\n+    const HloSliceInstruction* slice,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto output_rank = slice->shape().dimensions().size();\n \n   std::vector<AffineExpr> exprs;\n@@ -1195,7 +1242,9 @@ HloInstructionIndexing ComputeOutputToInputSliceOpIndexing(\n }\n \n HloInstructionIndexing ComputeInputToOutputSliceOpIndexing(\n-    const HloSliceInstruction* slice, MLIRContext* mlir_context) {\n+    const HloSliceInstruction* slice,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   auto output_rank = slice->shape().dimensions().size();\n \n   std::vector<AffineExpr> exprs;\n@@ -1223,25 +1272,29 @@ HloInstructionIndexing ComputeInputToOutputSliceOpIndexing(\n   return HloInstructionIndexing::FromIndexingMaps({std::move(indexing_map)});\n }\n \n-AffineMap ComputeTransposeIndexingMap(absl::Span<const int64_t> permutation,\n-                                      MLIRContext* mlir_context) {\n+AffineMap ComputeTransposeIndexingMap(\n+    absl::Span<const int64_t> permutation,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   return AffineMap::getPermutationMap(\n       std::vector<unsigned>(permutation.begin(), permutation.end()),\n       mlir_context);\n }\n \n HloInstructionIndexing ComputeOutputToInputTransposeOpIndexing(\n-    const HloTransposeInstruction* transpose, MLIRContext* mlir_context) {\n+    const HloTransposeInstruction* transpose,\n+    SymbolicExprContext* symbolic_expr_context) {\n   AffineMap inverse_permutation = ComputeTransposeIndexingMap(\n-      InversePermutation(transpose->dimensions()), mlir_context);\n+      InversePermutation(transpose->dimensions()), symbolic_expr_context);\n   return HloInstructionIndexing::FromIndexingMaps({IndexingMap::FromTensorSizes(\n       inverse_permutation, transpose->shape().dimensions(), {})});\n }\n \n HloInstructionIndexing ComputeInputToOutputTransposeOpIndexing(\n-    const HloTransposeInstruction* transpose, MLIRContext* mlir_context) {\n-  AffineMap forward_permutation =\n-      ComputeTransposeIndexingMap(transpose->dimensions(), mlir_context);\n+    const HloTransposeInstruction* transpose,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  AffineMap forward_permutation = ComputeTransposeIndexingMap(\n+      transpose->dimensions(), symbolic_expr_context);\n   return HloInstructionIndexing::FromIndexingMaps({IndexingMap::FromTensorSizes(\n       forward_permutation, transpose->operand(0)->shape().dimensions(), {})});\n }\n@@ -1250,21 +1303,21 @@ HloInstructionIndexing ComputeInputToOutputTransposeOpIndexing(\n \n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           const Shape& output_shape,\n-                          mlir::MLIRContext* mlir_context) {\n+                          SymbolicExprContext* symbolic_expr_context) {\n   return GetBitcastMap(ShapeUtil::MakeShapeWithDescendingLayout(\n                            output_shape.element_type(), input_shape),\n-                       output_shape, mlir_context);\n+                       output_shape, symbolic_expr_context);\n }\n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           absl::Span<const int64_t> output_shape,\n-                          mlir::MLIRContext* mlir_context) {\n+                          SymbolicExprContext* symbolic_expr_context) {\n   return GetBitcastMap(\n       ShapeUtil::MakeShapeWithDescendingLayout(PrimitiveType::S8, input_shape),\n       ShapeUtil::MakeShapeWithDescendingLayout(PrimitiveType::S8, output_shape),\n-      mlir_context);\n+      symbolic_expr_context);\n }\n IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n-                          MLIRContext* mlir_context) {\n+                          SymbolicExprContext* symbolic_expr_context) {\n   ShapeUtil::BitcastDecomposition decomposed_bitcast =\n       ShapeUtil::DecomposeBitcast(input_shape, output_shape);\n   if (!decomposed_bitcast.has_value()) {\n@@ -1279,25 +1332,26 @@ IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n         << \"Failed to deduce permutation for a bitcast.\";\n \n     return IndexingMap::FromTensorSizes(\n-        ComputeTransposeIndexingMap(permutation.value(), mlir_context),\n+        ComputeTransposeIndexingMap(permutation.value(), symbolic_expr_context),\n         input_shape.dimensions(), {});\n   }\n   if (std::holds_alternative<ShapeUtil::BitcastDecompositionReshape>(\n           *decomposed_bitcast)) {\n     // Note: ComputeReshapeIndexingMap assumes it's computing an output->input\n     // indexing, so input and output are reversed.\n     return IndexingMap::FromTensorSizes(\n-        ComputeReshapeIndexingMap(output_shape, input_shape, mlir_context),\n+        ComputeReshapeIndexingMap(output_shape, input_shape,\n+                                  symbolic_expr_context),\n         input_shape.dimensions(), {});\n   }\n   // `trt` stands for transpose-reshape-transpose decomposition of bitcast.\n   auto trt = std::get<ShapeUtil::BitcastDecompositionTrt>(*decomposed_bitcast);\n   auto transpose_map_1 =\n-      ComputeTransposeIndexingMap(trt.transpose1_dims, mlir_context);\n+      ComputeTransposeIndexingMap(trt.transpose1_dims, symbolic_expr_context);\n   auto reshape_map = ComputeReshapeIndexingMap(\n-      trt.reshape_shape, trt.transpose1_shape, mlir_context);\n+      trt.reshape_shape, trt.transpose1_shape, symbolic_expr_context);\n   auto transpose_map_2 =\n-      ComputeTransposeIndexingMap(trt.transpose2_dims, mlir_context);\n+      ComputeTransposeIndexingMap(trt.transpose2_dims, symbolic_expr_context);\n   auto bitcast_map =\n       transpose_map_2.compose(reshape_map).compose(transpose_map_1);\n   return IndexingMap::FromTensorSizes(bitcast_map, input_shape.dimensions(),\n@@ -1307,17 +1361,17 @@ IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n namespace {\n \n HloInstructionIndexing ComputeOutputToInputBitcastOpIndexing(\n-    const HloInstruction* bitcast, MLIRContext* mlir_context) {\n-  auto bitcast_map = GetBitcastMap(bitcast->shape(),\n-                                   bitcast->operand(0)->shape(), mlir_context);\n+    const HloInstruction* bitcast, SymbolicExprContext* symbolic_expr_context) {\n+  auto bitcast_map = GetBitcastMap(\n+      bitcast->shape(), bitcast->operand(0)->shape(), symbolic_expr_context);\n   bitcast_map.Simplify(IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({bitcast_map});\n }\n \n HloInstructionIndexing ComputeInputToOutputBitcastOpIndexing(\n-    const HloInstruction* bitcast, MLIRContext* mlir_context) {\n+    const HloInstruction* bitcast, SymbolicExprContext* symbolic_expr_context) {\n   auto bitcast_map = GetBitcastMap(bitcast->operand(0)->shape(),\n-                                   bitcast->shape(), mlir_context);\n+                                   bitcast->shape(), symbolic_expr_context);\n   bitcast_map.Simplify(IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({bitcast_map});\n }\n@@ -1334,19 +1388,21 @@ std::vector<int64_t> ToTransposeDimensions(const Layout& l) {\n }  // namespace\n \n IndexingMap CreateIdentityMap(absl::Span<const int64_t> dimensions,\n-                              mlir::MLIRContext* mlir_context) {\n+                              SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   return IndexingMap::FromTensorSizes(\n       AffineMap::getMultiDimIdentityMap(dimensions.size(), mlir_context),\n       /*dim_upper_bounds=*/dimensions, /*symbol_upper_bounds=*/{});\n }\n \n-IndexingMap CreateIdentityMap(const Shape& shape, MLIRContext* mlir_context) {\n+IndexingMap CreateIdentityMap(const Shape& shape,\n+                              SymbolicExprContext* symbolic_expr_context) {\n   if (shape.IsTuple()) {\n     // Should happen only for variadic reduce. In that case all tuple shapes are\n     // equal.\n-    return CreateIdentityMap(shape.tuple_shapes(0), mlir_context);\n+    return CreateIdentityMap(shape.tuple_shapes(0), symbolic_expr_context);\n   }\n-  return CreateIdentityMap(shape.dimensions(), mlir_context);\n+  return CreateIdentityMap(shape.dimensions(), symbolic_expr_context);\n }\n \n llvm::SmallVector<AffineExpr, 4> DelinearizeInBoundsIndex(\n@@ -1377,29 +1433,31 @@ llvm::SmallVector<AffineExpr, 4> DelinearizeInBoundsIndex(\n }\n \n IndexingMap GetIndexingMapFromPhysicalLayoutToLogical(\n-    const Shape& shape, MLIRContext* mlir_context) {\n+    const Shape& shape, SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   if (shape.dimensions().size() == 0) {\n     return IndexingMap(AffineMap::get(mlir_context),\n                        /*dimensions=*/{}, /*range vars=*/{}, /*rt_vars=*/{});\n   }\n   return IndexingMap::FromTensorSizes(\n       ComputeTransposeIndexingMap(\n           InversePermutation(ToTransposeDimensions(shape.layout())),\n-          mlir_context),\n+          symbolic_expr_context),\n       ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(shape)\n           .dimensions(),\n       {});\n }\n \n IndexingMap GetIndexingMapFromLogicalToPhysicalLayout(\n-    const Shape& shape, MLIRContext* mlir_context) {\n+    const Shape& shape, SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   if (shape.dimensions().size() == 0) {\n     return IndexingMap(AffineMap::get(mlir_context),\n                        /*dimensions=*/{}, /*range vars=*/{}, /*rt_vars=*/{});\n   }\n   return IndexingMap::FromTensorSizes(\n       ComputeTransposeIndexingMap(ToTransposeDimensions(shape.layout()),\n-                                  mlir_context),\n+                                  symbolic_expr_context),\n       shape.dimensions(), {});\n }\n \n@@ -1492,9 +1550,9 @@ GroupedByOpIndexing GroupIndexingMapsByProducers(\n \n GroupedByOpIndexing ComputeGroupedOutputToInputIndexing(\n     const HloFusionAdaptor& fusion_adaptor, HloInstructionAdaptor target_instr,\n-    MLIRContext* ctx) {\n-  OperandIndexing initial_map = OperandIndexing(\n-      CreateIdentityMap(target_instr.instruction().shape(), ctx));\n+    SymbolicExprContext* symbolic_expr_context) {\n+  OperandIndexing initial_map = OperandIndexing(CreateIdentityMap(\n+      target_instr.instruction().shape(), symbolic_expr_context));\n \n   GroupedByOpIndexing grouped_indexing_maps;\n   // If target_instr is a parameter of a fusion, then we create an identity map\n@@ -1516,8 +1574,9 @@ GroupedByOpIndexing ComputeGroupedOutputToInputIndexing(\n   // Iterator in reversed post-order (use-before-def).\n   auto it = std::find(post_order.rbegin(), post_order.rend(), target_instr);\n   for (; it != post_order.rend(); ++it) {\n-    auto producer_indexing = ComputeOutputToInputIndexing(&it->instruction(),\n-                                                          /*output_id=*/0, ctx);\n+    auto producer_indexing =\n+        ComputeOutputToInputIndexing(&it->instruction(),\n+                                     /*output_id=*/0, symbolic_expr_context);\n     auto consumer_indexing_maps =\n         grouped_indexing_maps.find(&it->instruction());\n     if (consumer_indexing_maps == grouped_indexing_maps.end()) {\n@@ -1548,7 +1607,8 @@ GroupedByOpIndexing ComputeGroupedOutputToInputIndexing(\n }\n \n namespace {\n-// Returns a linearized shape, i.e. tensor<num_elements(input) x element_type>.\n+// Returns a linearized shape, i.e. tensor<num_elements(input) x\n+// element_type>.\n Shape GetLinearizedShape(const Shape& shape) {\n   if (shape.dimensions().empty()) {\n     return shape;\n@@ -1562,19 +1622,20 @@ Shape GetLinearizedShape(const Shape& shape) {\n \n llvm::SmallVector<IndexingMap, 4> MapLogicalToLinearizedPhysicalShape(\n     absl::Span<const HloInstruction* const> operands,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   llvm::SmallVector<IndexingMap, 4> indexing_maps;\n   // For every operand compute thread ID -> physical layout of operand\n   // indexing map.\n   for (const HloInstruction* operand : operands) {\n     const Shape& operand_shape = operand->shape();\n \n     IndexingMap operand_logical_to_physical_map =\n-        GetIndexingMapFromLogicalToPhysicalLayout(operand_shape, mlir_context);\n+        GetIndexingMapFromLogicalToPhysicalLayout(operand_shape,\n+                                                  symbolic_expr_context);\n     IndexingMap operand_physical_to_linearized_shape = GetBitcastMap(\n         ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(\n             operand_shape),\n-        GetLinearizedShape(operand_shape), mlir_context);\n+        GetLinearizedShape(operand_shape), symbolic_expr_context);\n     IndexingMap operand_logical_to_linearized_physical_shape =\n         operand_logical_to_physical_map * operand_physical_to_linearized_shape;\n     operand_logical_to_linearized_physical_shape.Simplify();\n@@ -1590,7 +1651,8 @@ void GetThreadIdToInputMemoryLayoutsMaps(\n     const HloInstructionAdaptor& hero,\n     absl::Span<const HloInstruction* const> operands,\n     absl::Span<const IndexingMap> operand_logical_to_linearized_physical_maps,\n-    MLIRContext* mlir_context, GroupedByOpIndexingMap& result) {\n+    SymbolicExprContext* symbolic_expr_context,\n+    GroupedByOpIndexingMap& result) {\n   for (const auto& [hero_operand_index, hero_operand] :\n        llvm::enumerate(hero.GetOperands())) {\n     if (hero_operand.shape().dimensions().empty()) {\n@@ -1602,7 +1664,7 @@ void GetThreadIdToInputMemoryLayoutsMaps(\n     // Compute indexing from output to inputs for logical layout.\n     GroupedByOpIndexing instr_indexing_keyed_by_operands =\n         ComputeGroupedOutputToInputIndexing(fusion_adaptor, hero_operand,\n-                                            mlir_context);\n+                                            symbolic_expr_context);\n     // For every operand compute thread ID -> physical layout of operand\n     // indexing map.\n     for (auto&& [operand, operand_linearized_physical_map] :\n@@ -1665,7 +1727,9 @@ void AssignValuesToRTVars(IndexingMap* indexing_map) {\n }\n \n HloInstructionIndexing ComputeOutputToInputAllGatherOpIndexing(\n-    const HloAllGatherInstruction* instr, MLIRContext* ctx) {\n+    const HloAllGatherInstruction* instr,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  MLIRContext* ctx = symbolic_expr_context->GetMLIRContext();\n   // CHECK_EQ(instr->all_gather_dimension(), 0);\n   // if (instr->all_gather_dimension() != 0) {\n   //   return CreateUnknownIndexing(instr->operand_count());\n@@ -1703,78 +1767,89 @@ HloInstructionIndexing ComputeOutputToInputAllGatherOpIndexing(\n   return HloInstructionIndexing::FromOperandIndexing({operand_indexing});\n }\n \n-HloInstructionIndexing ComputeOutputToInputIndexing(const HloInstruction* instr,\n-                                                    int output_id,\n-                                                    MLIRContext* ctx) {\n+HloInstructionIndexing ComputeOutputToInputIndexing(\n+    const HloInstruction* instr, int output_id,\n+    SymbolicExprContext* symbolic_expr_context) {\n   if (HloInstruction::IsOpElementwise(instr->opcode()) ||\n       instr->opcode() == HloOpcode::kMap) {\n     // Note: map has a `dimensions` attribute, but it does nothing. See\n     // b/65689298.\n-    return ComputeOutputToInputCwiseOpIndexing(instr, ctx);\n+    return ComputeOutputToInputCwiseOpIndexing(instr, symbolic_expr_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {\n-    return ComputeOutputToInputBitcastOpIndexing(instr, ctx);\n+    return ComputeOutputToInputBitcastOpIndexing(instr, symbolic_expr_context);\n   }\n   // go/keep-sorted start\n   if (auto all_gather = DynCast<HloAllGatherInstruction>(instr)) {\n-    return ComputeOutputToInputAllGatherOpIndexing(all_gather, ctx);\n+    return ComputeOutputToInputAllGatherOpIndexing(all_gather,\n+                                                   symbolic_expr_context);\n   }\n   if (auto broadcast = DynCast<HloBroadcastInstruction>(instr)) {\n-    return ComputeOutputToInputBroadcastOpIndexing(broadcast, ctx);\n+    return ComputeOutputToInputBroadcastOpIndexing(broadcast,\n+                                                   symbolic_expr_context);\n   }\n   if (auto concat = DynCast<HloConcatenateInstruction>(instr)) {\n-    return ComputeOutputToInputConcatenateOpIndexing(concat, ctx);\n+    return ComputeOutputToInputConcatenateOpIndexing(concat,\n+                                                     symbolic_expr_context);\n   }\n   if (auto constant = DynCast<HloConstantInstruction>(instr)) {\n     return HloInstructionIndexing{};\n   }\n   if (auto convolution = DynCast<HloConvolutionInstruction>(instr)) {\n-    return ComputeOutputToInputConvolutionOpIndexing(convolution, ctx);\n+    return ComputeOutputToInputConvolutionOpIndexing(convolution,\n+                                                     symbolic_expr_context);\n   }\n   if (auto dot = DynCast<HloDotInstruction>(instr)) {\n-    return ComputeOutputToInputDotOpIndexing(dot, ctx);\n+    return ComputeOutputToInputDotOpIndexing(dot, symbolic_expr_context);\n   }\n   if (auto dus = DynCast<HloDynamicUpdateSliceInstruction>(instr)) {\n-    return ComputeOutputToInputDynamicUpdateSliceOpIndexing(dus, ctx);\n+    return ComputeOutputToInputDynamicUpdateSliceOpIndexing(\n+        dus, symbolic_expr_context);\n   }\n   if (auto dynamic_slice = DynCast<HloDynamicSliceInstruction>(instr)) {\n-    return ComputeOutputToInputDynamicSliceOpIndexing(dynamic_slice, ctx);\n+    return ComputeOutputToInputDynamicSliceOpIndexing(dynamic_slice,\n+                                                      symbolic_expr_context);\n   }\n   if (auto fusion = DynCast<HloFusionInstruction>(instr)) {\n-    return ComputeOutputToInputFusionOpIndexing(fusion, output_id, ctx);\n+    return ComputeOutputToInputFusionOpIndexing(fusion, output_id,\n+                                                symbolic_expr_context);\n   }\n   if (auto gather = DynCast<HloGatherInstruction>(instr)) {\n-    return ComputeOutputToInputGatherOpIndexing(gather, ctx);\n+    return ComputeOutputToInputGatherOpIndexing(gather, symbolic_expr_context);\n   }\n   if (auto iota = DynCast<HloIotaInstruction>(instr)) {\n     return HloInstructionIndexing{};\n   }\n   if (auto pad = DynCast<HloPadInstruction>(instr)) {\n-    return ComputeOutputToInputPadOpIndexing(pad, ctx);\n+    return ComputeOutputToInputPadOpIndexing(pad, symbolic_expr_context);\n   }\n   if (auto parameter = DynCast<HloParameterInstruction>(instr)) {\n     return HloInstructionIndexing{};\n   }\n   if (auto reduce = DynCast<HloReduceInstruction>(instr)) {\n-    return ComputeOutputToInputReduceOpIndexing(reduce, ctx);\n+    return ComputeOutputToInputReduceOpIndexing(reduce, symbolic_expr_context);\n   }\n   if (auto reduce_window = DynCast<HloReduceWindowInstruction>(instr)) {\n-    return ComputeOutputToInputReduceWindowOpIndexing(reduce_window, ctx);\n+    return ComputeOutputToInputReduceWindowOpIndexing(reduce_window,\n+                                                      symbolic_expr_context);\n   }\n   if (auto reshape = DynCast<HloReshapeInstruction>(instr)) {\n-    return ComputeOutputToInputReshapeOpIndexing(reshape, ctx);\n+    return ComputeOutputToInputReshapeOpIndexing(reshape,\n+                                                 symbolic_expr_context);\n   }\n   if (auto reverse = DynCast<HloReverseInstruction>(instr)) {\n-    return ComputeReverseOpIndexing(reverse, ctx);\n+    return ComputeReverseOpIndexing(reverse, symbolic_expr_context);\n   }\n   if (auto scaled_dot = DynCast<HloScaledDotInstruction>(instr)) {\n-    return ComputeOutputToInputScaledDotOpIndexing(scaled_dot, ctx);\n+    return ComputeOutputToInputScaledDotOpIndexing(scaled_dot,\n+                                                   symbolic_expr_context);\n   }\n   if (auto slice = DynCast<HloSliceInstruction>(instr)) {\n-    return ComputeOutputToInputSliceOpIndexing(slice, ctx);\n+    return ComputeOutputToInputSliceOpIndexing(slice, symbolic_expr_context);\n   }\n   if (auto transpose = DynCast<HloTransposeInstruction>(instr)) {\n-    return ComputeOutputToInputTransposeOpIndexing(transpose, ctx);\n+    return ComputeOutputToInputTransposeOpIndexing(transpose,\n+                                                   symbolic_expr_context);\n   }\n   // go/keep-sorted end\n   LOG(ERROR) << \"ComputeOutputToInputIndexing is not implemented for opcode \"\n@@ -1784,44 +1859,49 @@ HloInstructionIndexing ComputeOutputToInputIndexing(const HloInstruction* instr,\n   return CreateUnknownIndexing(instr->operand_count());\n }\n \n-HloInstructionIndexing ComputeInputToOutputIndexing(const HloInstruction* instr,\n-                                                    int input_id,\n-                                                    MLIRContext* ctx) {\n+HloInstructionIndexing ComputeInputToOutputIndexing(\n+    const HloInstruction* instr, int input_id,\n+    SymbolicExprContext* symbolic_expr_context) {\n   if (HloInstruction::IsOpElementwise(instr->opcode()) ||\n       instr->opcode() == HloOpcode::kMap) {\n     // Note: map has a `dimensions` attribute, but it does nothing. See\n     // b/65689298.\n-    return ComputeInputToOutputCwiseOpIndexing(instr, ctx);\n+    return ComputeInputToOutputCwiseOpIndexing(instr, symbolic_expr_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {\n-    return ComputeInputToOutputBitcastOpIndexing(instr, ctx);\n+    return ComputeInputToOutputBitcastOpIndexing(instr, symbolic_expr_context);\n   }\n   // go/keep-sorted start\n   if (auto broadcast = DynCast<HloBroadcastInstruction>(instr)) {\n-    return ComputeInputToOutputBroadcastOpIndexing(broadcast, ctx);\n+    return ComputeInputToOutputBroadcastOpIndexing(broadcast,\n+                                                   symbolic_expr_context);\n   }\n   if (auto concat = DynCast<HloConcatenateInstruction>(instr)) {\n-    return ComputeInputToOutputConcatenateOpIndexing(concat, input_id, ctx);\n+    return ComputeInputToOutputConcatenateOpIndexing(concat, input_id,\n+                                                     symbolic_expr_context);\n   }\n   if (auto reduce = DynCast<HloReduceInstruction>(instr)) {\n-    return ComputeInputToOutputReduceOpIndexing(reduce, input_id, ctx);\n+    return ComputeInputToOutputReduceOpIndexing(reduce, input_id,\n+                                                symbolic_expr_context);\n   }\n   if (auto reshape = DynCast<HloReshapeInstruction>(instr)) {\n-    return ComputeInputToOutputReshapeOpIndexing(reshape, ctx);\n+    return ComputeInputToOutputReshapeOpIndexing(reshape,\n+                                                 symbolic_expr_context);\n   }\n   if (auto reverse = DynCast<HloReverseInstruction>(instr)) {\n-    return ComputeReverseOpIndexing(reverse, ctx);\n+    return ComputeReverseOpIndexing(reverse, symbolic_expr_context);\n   }\n   if (auto slice = DynCast<HloSliceInstruction>(instr)) {\n-    return ComputeInputToOutputSliceOpIndexing(slice, ctx);\n+    return ComputeInputToOutputSliceOpIndexing(slice, symbolic_expr_context);\n   }\n   if (auto transpose = DynCast<HloTransposeInstruction>(instr)) {\n-    return ComputeInputToOutputTransposeOpIndexing(transpose, ctx);\n+    return ComputeInputToOutputTransposeOpIndexing(transpose,\n+                                                   symbolic_expr_context);\n   }\n   // go/keep-sorted end\n   if (instr->opcode() == HloOpcode::kTuple) {\n-    return HloInstructionIndexing::FromIndexingMaps(\n-        {CreateIdentityMap(instr->shape().tuple_shapes(input_id), ctx)});\n+    return HloInstructionIndexing::FromIndexingMaps({CreateIdentityMap(\n+        instr->shape().tuple_shapes(input_id), symbolic_expr_context)});\n   }\n   // If we cannot compute input-to-output indexing, we return std::nullopt for\n   // every op result.\n@@ -1832,17 +1912,17 @@ HloInstructionIndexing ComputeInputToOutputIndexing(const HloInstruction* instr,\n \n IndexingMap ComputeEpilogueInputToOutputIndexing(\n     HloInstructionAdaptor epilogue_parent, HloInstructionAdaptor epilogue_root,\n-    MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   std::vector<HloInstructionAdaptor> chain =\n       HloFindUseChain(epilogue_parent, epilogue_root);\n   CHECK(!chain.empty()) << \"There is no use chain from parent to root\";\n   OperandIndexing root_indexing(\n-      CreateIdentityMap(epilogue_parent.shape(), mlir_context));\n+      CreateIdentityMap(epilogue_parent.shape(), symbolic_expr_context));\n   for (int i = 1; i < chain.size(); ++i) {\n     const auto& producer = chain[i - 1].instruction();\n     const auto& user = chain[i].instruction();\n     auto user_indexing = ComputeInputToOutputIndexing(\n-        &user, user.operand_index(&producer), mlir_context);\n+        &user, user.operand_index(&producer), symbolic_expr_context);\n     root_indexing = ComposeOperandIndexing(\n         {root_indexing}, *user_indexing.indexing_maps[0].begin());\n     root_indexing.Simplify();"
        },
        {
            "sha": "4342100c0713450c48d8c698217463b76d36d3ab",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.h",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,10 +30,10 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n \n namespace xla {\n@@ -71,15 +71,15 @@ std::ostream& operator<<(std::ostream& out,\n \n // Computes indexing maps for all input operands necessary to compute an element\n // of the `output_id` instruction output.\n-HloInstructionIndexing ComputeOutputToInputIndexing(const HloInstruction* instr,\n-                                                    int output_id,\n-                                                    mlir::MLIRContext* ctx);\n+HloInstructionIndexing ComputeOutputToInputIndexing(\n+    const HloInstruction* instr, int output_id,\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Computes indexing maps for all output operands that the element of the\n // `input_id` instruction input will participate in.\n-HloInstructionIndexing ComputeInputToOutputIndexing(const HloInstruction* instr,\n-                                                    int input_id,\n-                                                    mlir::MLIRContext* ctx);\n+HloInstructionIndexing ComputeInputToOutputIndexing(\n+    const HloInstruction* instr, int input_id,\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Computes the indexing for `epilogue_parent`'s epilogue. For example, if\n // `epilogue_parent` is a transpose, computes the input to output indexing for\n@@ -107,7 +107,7 @@ HloInstructionIndexing ComputeInputToOutputIndexing(const HloInstruction* instr,\n // fusion does not make much sense, but they are created sometimes.\n IndexingMap ComputeEpilogueInputToOutputIndexing(\n     HloInstructionAdaptor epilogue_parent, HloInstructionAdaptor epilogue_root,\n-    mlir::MLIRContext* mlir_context);\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Indexing of the runtime variable of the HLO instruction.\n struct RuntimeVarIndexing {\n@@ -206,13 +206,13 @@ using GroupedByOpIndexing =\n // cluster starting with `target_instr` and going from def to use.\n GroupedByOpIndexing ComputeGroupedOutputToInputIndexing(\n     const HloFusionAdaptor& fusion_adaptor, HloInstructionAdaptor target_instr,\n-    mlir::MLIRContext* ctx);\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Returns the indexing map from logical to linearized physical shape for each\n // operand.\n llvm::SmallVector<IndexingMap, 4> MapLogicalToLinearizedPhysicalShape(\n     absl::Span<const HloInstruction* const> operands,\n-    mlir::MLIRContext* mlir_context);\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Computes the indexing map from logical to linearized physical shape for each\n // operand and adds them to `result`. `result` may be non-empty when this\n@@ -224,7 +224,8 @@ void GetThreadIdToInputMemoryLayoutsMaps(\n     const HloInstructionAdaptor& hero,\n     absl::Span<const HloInstruction* const> operands,\n     absl::Span<const IndexingMap> operand_logical_to_linearized_physical_maps,\n-    mlir::MLIRContext* mlir_context, GroupedByOpIndexingMap& result);\n+    gpu::SymbolicExprContext* symbolic_expr_context,\n+    GroupedByOpIndexingMap& result);\n \n // Replaces RTVars with the midpoints of the feasible intervals.\n void AssignValuesToRTVars(IndexingMap* indexing_map);\n@@ -237,23 +238,23 @@ GroupedByOpIndexing GroupIndexingMapsByProducers(\n // Equivalent to linearizing the input_shape index and then delinearizing it\n // to output_shape.\n IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n-                          mlir::MLIRContext* mlir_context);\n+                          gpu::SymbolicExprContext* symbolic_expr_context);\n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           const Shape& output_shape,\n-                          mlir::MLIRContext* mlir_context);\n+                          gpu::SymbolicExprContext* symbolic_expr_context);\n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           absl::Span<const int64_t> output_shape,\n-                          mlir::MLIRContext* mlir_context);\n+                          gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Creates an indexing map from the physical layout of the tensor to its logical\n // layout.\n IndexingMap GetIndexingMapFromPhysicalLayoutToLogical(\n-    const Shape& shape, mlir::MLIRContext* mlir_context);\n+    const Shape& shape, gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Creates an indexing map from the logical layout of the tensor to its physical\n // layout.\n IndexingMap GetIndexingMapFromLogicalToPhysicalLayout(\n-    const Shape& shape, mlir::MLIRContext* mlir_context);\n+    const Shape& shape, gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Returns the shape of the output of the instruction.\n const Shape& GetOutputShape(const HloInstruction* instr, int64_t output_id);\n@@ -262,18 +263,18 @@ const Shape& GetOutputShape(const HloInstruction* instr, int64_t output_id);\n mlir::AffineExpr LinearizeShape(\n     absl::Span<const int64_t> dims,\n     absl::Span<const mlir::AffineExpr> dimension_exprs,\n-    mlir::MLIRContext* mlir_context);\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Computes N-d indexing expressions given a linear index and a shape.\n-std::vector<mlir::AffineExpr> DelinearizeIndex(absl::Span<const int64_t> dims,\n-                                               mlir::AffineExpr linear_index,\n-                                               mlir::MLIRContext* mlir_context);\n+std::vector<mlir::AffineExpr> DelinearizeIndex(\n+    absl::Span<const int64_t> dims, mlir::AffineExpr linear_index,\n+    gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Creates an identity indexing map corresponding to the parameter shape.\n IndexingMap CreateIdentityMap(const Shape& shape,\n-                              mlir::MLIRContext* mlir_context);\n+                              gpu::SymbolicExprContext* symbolic_expr_context);\n IndexingMap CreateIdentityMap(absl::Span<const int64_t> dimensions,\n-                              mlir::MLIRContext* mlir_context);\n+                              gpu::SymbolicExprContext* symbolic_expr_context);\n \n llvm::SmallVector<mlir::AffineExpr, 4> DelinearizeInBoundsIndex(\n     mlir::AffineExpr linear, absl::Span<const int64_t> sizes);"
        },
        {
            "sha": "9d2e6673a53750dcb8b8eba6f2335c09a72ac49d",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -90,7 +90,7 @@ TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(transpose, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n   EXPECT_THAT(grouped_indexing,\n               UnorderedElementsAre(\n                   Pair(root, ElementsAre(MatchOperandIndexing(R\"(\n@@ -147,7 +147,7 @@ TEST_F(IndexingAnalysisTest,\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n \n   EXPECT_THAT(grouped_indexing,\n               UnorderedElementsAre(\n@@ -200,7 +200,7 @@ TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing_SingleOp) {\n   HloInstructionAdaptor parameter_adaptor =\n       fusion_adaptor->GetRoots()[0].GetOperand(0);\n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, parameter_adaptor, &mlir_context_);\n+      *fusion_adaptor, parameter_adaptor, &symbolic_expr_context_);\n   EXPECT_THAT(\n       grouped_indexing,\n       UnorderedElementsAre(Pair(parameter, ElementsAre(MatchOperandIndexing(R\"(\n@@ -241,7 +241,7 @@ TEST_F(IndexingAnalysisTest,\n   auto parameter_0 = bcast.GetOperand(0);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, bcast, &mlir_context_);\n+      *fusion_adaptor, bcast, &symbolic_expr_context_);\n   EXPECT_THAT(\n       grouped_indexing,\n       UnorderedElementsAre(\n@@ -2584,8 +2584,8 @@ TEST_F(IndexingAnalysisTest, EpilogueIndexing) {\n   HloInstructionAdaptor log(*computation->GetInstructionWithName(\"log\"),\n                             fusion.get());\n \n-  EXPECT_THAT(ToString(ComputeEpilogueInputToOutputIndexing(transpose, log,\n-                                                            &mlir_context_)),\n+  EXPECT_THAT(ToString(ComputeEpilogueInputToOutputIndexing(\n+                  transpose, log, &symbolic_expr_context_)),\n               MatchIndexingString(R\"(\n                   (d0, d1) -> (d1 * 1000 + d0),\n                   domain:\n@@ -2614,7 +2614,7 @@ TEST_F(IndexingAnalysisTest, EpilogueIndexing_NoEpilogue) {\n                                   fusion.get());\n \n   EXPECT_THAT(ToString(ComputeEpilogueInputToOutputIndexing(\n-                  transpose, transpose, &mlir_context_)),\n+                  transpose, transpose, &symbolic_expr_context_)),\n               MatchIndexingString(R\"(\n                   (d0, d1) -> (d0, d1),\n                   domain:\n@@ -2927,7 +2927,7 @@ TEST_F(IndexingAnalysisTest, AllGatherFusionWithReshape) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n \n   EXPECT_THAT(grouped_indexing[root], ElementsAre(MatchOperandIndexing(R\"(\n     (d0) -> (d0),\n@@ -2971,7 +2971,7 @@ TEST_F(IndexingAnalysisTest, ChainedAllGatherFusion) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n \n   EXPECT_THAT(grouped_indexing[parameter],\n               ElementsAre(UndefinedOperandIndexing()));\n@@ -2995,7 +2995,7 @@ TEST_F(IndexingAnalysisTest, AllGatherDotFusion_GatherNonContractingDim) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n \n   EXPECT_THAT(grouped_indexing[parameter], ElementsAre(MatchOperandIndexing(R\"(\n     (d0, d1)[s0] -> (d0 mod 64, s0),\n@@ -3030,7 +3030,7 @@ TEST_F(IndexingAnalysisTest, AllGatherDotFusion_GatherContractingDim) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n \n   EXPECT_THAT(grouped_indexing[parameter], ElementsAre(MatchOperandIndexing(R\"(\n     (d0, d1)[s0] -> (d0, s0 mod 128),"
        },
        {
            "sha": "0b3e875324baa0c8bc9a09877c1261faf5c4842d",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_serialization.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -39,13 +39,15 @@ limitations under the License.\n #include \"mlir/AsmParser/AsmParser.h\"\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/hlo/analysis/interval.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n namespace {\n \n+using gpu::SymbolicExprContext;\n using llvm::SmallVector;\n using llvm::SmallVectorImpl;\n using llvm::StringRef;\n@@ -58,7 +60,6 @@ using mlir::AffineMap;\n using mlir::AffineMapAttr;\n using mlir::AffineSymbolExpr;\n using mlir::ArrayRef;\n-using mlir::MLIRContext;\n \n enum class Delimeter { kParen, kBracket, kBrace };\n \n@@ -393,15 +394,16 @@ bool ParseAffineMapResults(Parser& parser,\n bool ParseAffineExprsWithMLIR(ArrayRef<std::string> dim_var_names,\n                               ArrayRef<std::string> symbol_var_names,\n                               ArrayRef<std::string> affine_expr_strings,\n-                              MLIRContext* context,\n+                              SymbolicExprContext* context,\n                               SmallVectorImpl<AffineExpr>& affine_exprs) {\n   std::stringstream ss;\n   ss << \"affine_map<(\" << absl::StrJoin(dim_var_names, \", \") << \") \";\n   if (!symbol_var_names.empty()) {\n     ss << '[' << absl::StrJoin(symbol_var_names, \", \") << \"] \";\n   }\n   ss << \" -> (\" << absl::StrJoin(affine_expr_strings, \", \") << \")>\";\n-  auto affine_map_attr = mlir::parseAttribute(ss.str(), context);\n+  auto affine_map_attr =\n+      mlir::parseAttribute(ss.str(), context->GetMLIRContext());\n   if (!affine_map_attr) {\n     llvm::errs() << \"Failed to parse affine map: \" << ss.str() << \"\\n\";\n     return false;\n@@ -597,8 +599,8 @@ void PrintAffineExprImpl(const AffineExpr affine_expr,\n \n }  // namespace\n \n-std::optional<IndexingMap> ParseIndexingMap(llvm::StringRef input,\n-                                            MLIRContext* context) {\n+std::optional<IndexingMap> ParseIndexingMap(\n+    llvm::StringRef input, gpu::SymbolicExprContext* symbolic_expr_context) {\n   Parser parser(input);\n \n   // Parse variable names.\n@@ -631,8 +633,8 @@ std::optional<IndexingMap> ParseIndexingMap(llvm::StringRef input,\n       llvm::errs() << \"Expected an empty indexing map\\n\";\n       return std::nullopt;\n     }\n-    return IndexingMap{AffineMap::get(context), /*dimensions=*/{},\n-                       /*range_vars=*/{}, /*rt_vars=*/{}};\n+    return IndexingMap{AffineMap::get(symbolic_expr_context->GetMLIRContext()),\n+                       /*dimensions=*/{}, /*range_vars=*/{}, /*rt_vars=*/{}};\n   }\n \n   if (!parser.ConsumeToken(Token::Kind::kComma) ||\n@@ -733,7 +735,8 @@ std::optional<IndexingMap> ParseIndexingMap(llvm::StringRef input,\n   symbol_var_names.append(rt_var_names.begin(), rt_var_names.end());\n   SmallVector<AffineExpr> affine_exprs;\n   if (!ParseAffineExprsWithMLIR(dim_var_names, symbol_var_names,\n-                                affine_expr_strs, context, affine_exprs)) {\n+                                affine_expr_strs, symbolic_expr_context,\n+                                affine_exprs)) {\n     llvm::errs() << \"Failed to parse affine expressions\\n\";\n     return std::nullopt;\n   }\n@@ -750,7 +753,8 @@ std::optional<IndexingMap> ParseIndexingMap(llvm::StringRef input,\n     constraints.push_back(std::make_pair(expr, bounds));\n   }\n   auto map = AffineMap::get(dim_vars.size(), range_vars.size() + rt_vars.size(),\n-                            affine_map_results, context);\n+                            affine_map_results,\n+                            symbolic_expr_context->GetMLIRContext());\n   return IndexingMap{map, std::move(dim_vars), std::move(range_vars),\n                      std::move(rt_vars), constraints};\n }"
        },
        {
            "sha": "88fd7920c1530df4b5e186b6602cef504911396c",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_serialization.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -25,14 +25,14 @@ limitations under the License.\n #include \"llvm/ADT/StringRef.h\"\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/AffineMap.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n \n namespace xla {\n \n // Parses the given string into an IndexingMap.\n-std::optional<IndexingMap> ParseIndexingMap(llvm::StringRef input,\n-                                            mlir::MLIRContext* context);\n+std::optional<IndexingMap> ParseIndexingMap(\n+    llvm::StringRef input, gpu::SymbolicExprContext* symbolic_expr_context);\n \n // Prints AffineExpr using the default (d0, d1, ..., s0, s1, ...) variable\n // names."
        },
        {
            "sha": "ee7eccb5e9b156ac364dfffa3ef15a432bb57157",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_serialization_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/analysis/indexing_test_utils.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"tsl/platform/test.h\"\n \n namespace xla {\n@@ -34,8 +35,10 @@ using ::testing::HasSubstr;\n class IndexingMapSerializationTest : public HloHardwareIndependentTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   void ParseAndCheck(absl::string_view indexing_map_str) {\n-    auto indexing_map = ParseIndexingMap(indexing_map_str, &mlir_context_);\n+    auto indexing_map =\n+        ParseIndexingMap(indexing_map_str, &symbolic_expr_context_);\n     ASSERT_TRUE(indexing_map.has_value());\n     EXPECT_THAT(ToString(*indexing_map), MatchIndexingString(indexing_map_str));\n   }\n@@ -49,8 +52,8 @@ TEST_F(IndexingMapSerializationTest, UndefinedMap) {\n }\n \n TEST_F(IndexingMapSerializationTest, KnownEmptyMap) {\n-  auto map =\n-      ParseIndexingMap(\"(d0) -> (), domain: d0 in [1, 0]\", &mlir_context_);\n+  auto map = ParseIndexingMap(\"(d0) -> (), domain: d0 in [1, 0]\",\n+                              &symbolic_expr_context_);\n   EXPECT_TRUE(map->IsKnownEmpty());\n   EXPECT_THAT(ToString(*map), MatchIndexingString(\"KNOWN EMPTY\"));\n }"
        },
        {
            "sha": "ab51c158806f2f39723a3544c2ddf06681f91cc8",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -52,8 +52,8 @@ class IndexingMapTest : public HloHardwareIndependentTestBase {\n   IndexingMapTest() : symbolic_expr_context_(&mlir_context_) {}\n \n   IndexingMap Parse(absl::string_view indexing_map_str) {\n-    auto indexing_map = ParseIndexingMap(\n-        indexing_map_str, symbolic_expr_context_.GetMLIRContext());\n+    auto indexing_map =\n+        ParseIndexingMap(indexing_map_str, &symbolic_expr_context_);\n     EXPECT_TRUE(indexing_map.has_value());\n     return *indexing_map;\n   }"
        },
        {
            "sha": "bdf1c0b754cd4489379237b7b5abf00f78e6e057",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_test_utils.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -39,7 +39,6 @@ limitations under the License.\n #include \"mlir/AsmParser/AsmParser.h\"\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n@@ -53,7 +52,6 @@ namespace {\n \n using ::mlir::AffineExpr;\n using ::mlir::AffineMap;\n-using ::mlir::MLIRContext;\n \n std::string FormatDimsAndSyms(absl::Span<int64_t const> dims,\n                               absl::Span<int64_t const> syms) {\n@@ -73,22 +71,20 @@ HloInstruction* IndexingTestBase::ParseAndGetRoot(\n \n HloInstructionIndexing IndexingTestBase::GetOutputToInputIndexing(\n     const HloInstruction* instr, int output_id, bool use_physical_layout) {\n-  HloInstructionIndexing indexing = ComputeOutputToInputIndexing(\n-      instr, output_id, symbolic_expr_context_.GetMLIRContext());\n+  HloInstructionIndexing indexing =\n+      ComputeOutputToInputIndexing(instr, output_id, &symbolic_expr_context_);\n \n   if (!use_physical_layout) {\n     return indexing;\n   }\n \n   IndexingMap output_permutation = GetIndexingMapFromPhysicalLayoutToLogical(\n-      GetOutputShape(instr, output_id),\n-      symbolic_expr_context_.GetMLIRContext());\n+      GetOutputShape(instr, output_id), &symbolic_expr_context_);\n \n   for (const auto& [operand_id, indexing_maps] :\n        llvm::enumerate(indexing.indexing_maps)) {\n     IndexingMap operand_permutation = GetIndexingMapFromLogicalToPhysicalLayout(\n-        instr->operand(operand_id)->shape(),\n-        symbolic_expr_context_.GetMLIRContext());\n+        instr->operand(operand_id)->shape(), &symbolic_expr_context_);\n \n     OperandIndexingSet operand_indexing_maps;\n     for (const OperandIndexing& indexing_map : indexing_maps) {\n@@ -110,24 +106,22 @@ HloInstructionIndexing IndexingTestBase::GetOutputToInputIndexing(\n \n HloInstructionIndexing IndexingTestBase::GetInputToOutputIndexing(\n     const HloInstruction* instr, int input_id, bool use_physical_layout) {\n-  HloInstructionIndexing indexing = ComputeInputToOutputIndexing(\n-      instr, input_id, symbolic_expr_context_.GetMLIRContext());\n+  HloInstructionIndexing indexing =\n+      ComputeInputToOutputIndexing(instr, input_id, &symbolic_expr_context_);\n \n   if (!use_physical_layout) {\n     return indexing;\n   }\n \n   OperandIndexing input_permutation =\n       OperandIndexing(GetIndexingMapFromPhysicalLayoutToLogical(\n-          instr->operand(input_id)->shape(),\n-          symbolic_expr_context_.GetMLIRContext()));\n+          instr->operand(input_id)->shape(), &symbolic_expr_context_));\n \n   for (const auto& [output_id, indexing_maps] :\n        llvm::enumerate(indexing.indexing_maps)) {\n     OperandIndexing operand_permutation =\n         OperandIndexing(GetIndexingMapFromLogicalToPhysicalLayout(\n-            GetOutputShape(instr, output_id),\n-            symbolic_expr_context_.GetMLIRContext()));\n+            GetOutputShape(instr, output_id), &symbolic_expr_context_));\n \n     OperandIndexingSet operand_indexing_maps;\n     for (const OperandIndexing& indexing_map : indexing_maps) {"
        },
        {
            "sha": "a7b37d2f6df21b7cef461fd33773c225aca5b26d",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -1000,6 +1000,7 @@ cc_library(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:pattern_matcher\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n@@ -2058,6 +2059,7 @@ cc_library(\n         \"//xla/codegen:mlir_kernel_source\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\","
        },
        {
            "sha": "707bcce9805bd2a5e0bffe99c1b72eff5b2b99e2",
            "filename": "third_party/xla/xla/service/cpu/parallel_fusion_emitter.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_fusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_fusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_fusion_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -38,13 +38,15 @@ limitations under the License.\n #include \"xla/codegen/mlir_kernel_source.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n \n namespace xla::cpu {\n \n struct ParallelFusionEmitter::CompilerInstance {\n-  std::unique_ptr<mlir::MLIRContext> context;\n+  std::unique_ptr<mlir::MLIRContext> mlir_context;\n+  std::unique_ptr<gpu::SymbolicExprContext> symbolic_expr_context;\n   std::unique_ptr<FusionCompiler> compiler;\n };\n \n@@ -97,12 +99,18 @@ auto ParallelFusionEmitter::FusionCompilerPool::GetInstance()\n     return CreateSharedInstance(std::move(instance));\n   }\n \n-  std::unique_ptr<mlir::MLIRContext> context = FusionCompiler::CreateContext();\n+  std::unique_ptr<mlir::MLIRContext> mlir_context =\n+      FusionCompiler::CreateContext();\n \n-  auto compiler = std::make_unique<FusionCompiler>(context.get(), options_,\n+  auto symbolic_expr_context =\n+      std::make_unique<gpu::SymbolicExprContext>(mlir_context.get());\n+\n+  auto compiler = std::make_unique<FusionCompiler>(mlir_context.get(), options_,\n                                                    GetNestedHooks());\n \n-  return CreateSharedInstance({std::move(context), std::move(compiler)});\n+  return CreateSharedInstance({std::move(mlir_context),\n+                               std::move(symbolic_expr_context),\n+                               std::move(compiler)});\n }\n \n auto ParallelFusionEmitter::FusionCompilerPool::CreateSharedInstance(\n@@ -163,9 +171,10 @@ absl::StatusOr<KernelSpec> ParallelFusionEmitter::AddFusion(\n   // returned immediately, we have to do it in the main thread. This can be\n   // fixed but will require a rework of the ThunkEmitter.\n   auto compiler_instance = fusion_compiler_pool_->GetInstance();\n-  TF_ASSIGN_OR_RETURN(MlirKernelDefinition mlir_kernel_definition,\n-                      EmitFusionKernel(*compiler_instance->context, *fusion,\n-                                       buffer_assignment_, use_unique_c_name_));\n+  TF_ASSIGN_OR_RETURN(\n+      MlirKernelDefinition mlir_kernel_definition,\n+      EmitFusionKernel(*compiler_instance->symbolic_expr_context, *fusion,\n+                       buffer_assignment_, use_unique_c_name_));\n \n   {\n     absl::MutexLock lock(kernels_mutex_);"
        },
        {
            "sha": "5fe7ffc889cd323495f63a216d245e8eec47308c",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -834,7 +834,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitFusionKernelThunk(\n   if (ir_emitter_.IsSupportedByFusionEmitter(fusion) &&\n       fusion->fused_expression_root()->opcode() == HloOpcode::kScatter) {\n     auto kernel_emitter = std::make_unique<CpuScatterFusion>(\n-        buffer_assignment_, fusion, mlir_context_.get());\n+        buffer_assignment_, fusion, &symbolic_expr_context_);\n \n     TF_ASSIGN_OR_RETURN(MlirKernelDefinition kernel_definition,\n                         kernel_emitter->EmitKernelDefinition());"
        },
        {
            "sha": "46368cf5be8ac552aac165725f567dc837d9224e",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/cpu/ir_emitter2.h\"\n #include \"xla/service/cpu/parallel_fusion_emitter.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n@@ -265,6 +266,9 @@ class ThunkEmitter {\n   std::vector<EmittedKernel> kernels_;\n \n   std::unique_ptr<mlir::MLIRContext> mlir_context_;\n+  // TODO (b/449934916): SymbolicExprContext should be moved to a more generic\n+  // (not GPU specific) place.\n+  gpu::SymbolicExprContext symbolic_expr_context_{mlir_context_.get()};\n   FusionCompiler fusion_compiler_;\n \n   absl::flat_hash_map<std::string, KernelSpec> kernel_spec_cache_;"
        },
        {
            "sha": "ea6570d005079f5285401f3c2f44e7b87a9e6cc2",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -406,6 +406,7 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:call_inliner\",\n         \"//xla/service:name_uniquer\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -619,6 +620,7 @@ cc_library(\n     srcs = [\"kernel_call.cc\"],\n     hdrs = [\"kernel_call.h\"],\n     deps = [\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n@@ -639,6 +641,7 @@ xla_cc_test(\n     srcs = [\"kernel_call_test.cc\"],\n     deps = [\n         \":kernel_call\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n@@ -1496,6 +1499,7 @@ cc_library(\n         \"//xla/service:dump\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:logical_buffer\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -1532,10 +1536,10 @@ cc_library(\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n         \"//xla/service:hlo_cost_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:fusion_block_level_rewriter\",\n         \"//xla/service/gpu/transforms:fusion_dynamic_memcpy_rewriter\",\n         \"//xla/stream_executor:device_description\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -1555,6 +1559,7 @@ cc_library(\n         \"//xla/service:hlo_verifier\",\n         \"//xla/service:layout_assignment\",\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:multi_output_fusion\",\n         \"//xla/service/gpu/transforms:priority_fusion\",\n         \"//xla/service/gpu/transforms:variadic_op_splitter\",\n@@ -1773,6 +1778,7 @@ cc_library(\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n         \"//xla/service/gpu/model:matmul_ptable_stats_collection\",\n         \"//xla/service/gpu/model:sol_gpu_cost_model_stats_collection\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:add_tracking_suffix_to_instruction_names\",\n         \"//xla/service/gpu/transforms:algebraic_simplifier\",\n         \"//xla/service/gpu/transforms:algorithm_checker\",\n@@ -2179,6 +2185,7 @@ xla_test(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:buffer_value\",\n         \"//xla/service:logical_buffer\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tests:xla_internal_test_main\",\n@@ -2435,6 +2442,7 @@ cc_library(\n         \"//xla/service/gpu/model:analytical_latency_estimator\",\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n         \"//xla/service/gpu/model:sol_latency_estimator\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:pgle_accuracy_checker\",\n         \"//xla/service/gpu/transforms:scheduling_instruction_annotator\",\n         \"//xla/service/gpu/transforms:stream_attribute_async_wrapper\",\n@@ -2452,7 +2460,6 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:protobuf\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n@@ -3196,6 +3203,7 @@ xla_cc_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:latency_hiding_scheduler\",\n         \"//xla/service:profile_guided_latency_estimator\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "70aa9f7bc64621aac5777f5fc3d5953b9dd0d773",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -324,7 +324,7 @@ absl::Status AMDGPUCompiler::AddGemmFusionAutotuningPasses(\n     se::StreamExecutor* stream_executor) {\n   pipeline->AddPass<GemmFusionAutotuner>(autotune_config, toolkit_version,\n                                          thread_pool, key_value_store,\n-                                         mlir_context());\n+                                         symbolic_expr_context());\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "b6cde9f3e4626b1159c018aa2823c772580cbd29",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -58,6 +58,7 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:stream_executor_util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:cudnn_fusion_compiler\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n@@ -69,7 +70,6 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@llvm-project//mlir:IR\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n     ],\n )\n@@ -97,6 +97,7 @@ cc_library(\n         \"//xla/service:executable\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/rocm:rocblas_plugin\",\n@@ -107,7 +108,6 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@llvm-project//mlir:IR\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n     ],\n )\n@@ -161,6 +161,7 @@ cc_library(\n         \"//xla/service/gpu/kernels:custom_kernel\",\n         \"//xla/service/gpu/kernels:custom_kernel_fusion\",\n         \"//xla/service/gpu/kernels:custom_kernel_fusion_pattern\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:custom_kernel_fusion_rewriter\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:fusion_wrapper\",\n@@ -245,6 +246,7 @@ xla_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:gemm_fusion\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n         \"//xla/stream_executor:device_description\","
        },
        {
            "sha": "ab9000911eff8e6bc02a5c1b6f8189156ea60999",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 17,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotune_results.pb.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n@@ -74,6 +73,7 @@ limitations under the License.\n #include \"xla/service/gpu/kernels/custom_kernel_fusion.h\"\n #include \"xla/service/gpu/kernels/custom_kernel_fusion_pattern.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/split_k_gemm_rewriter.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/service/gpu/transforms/custom_kernel_fusion_rewriter.h\"\n@@ -275,7 +275,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     const TritonGemmConfig& config,\n     const se::DeviceDescription& gpu_device_info,\n     const HloFusionInstruction* fusion, DebugOptions debug_opts,\n-    mlir::MLIRContext* mlir_context,\n+    SymbolicExprContext* symbolic_expr_context,\n     bool allow_filtering_kernels_spilling_registers) {\n   tsl::profiler::TraceMe traceme(\"TritonGemmAutotuneExtractor\");\n   std::unique_ptr<HloModule> new_module =\n@@ -309,7 +309,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n \n     PriorityFusion priority_fusion(\n         /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions(),\n-        mlir_context);\n+        symbolic_expr_context);\n     TF_RETURN_IF_ERROR(priority_fusion.Run(new_module.get()).status());\n \n     // If the priority fusion pass above skipped some instructions, turn them\n@@ -318,7 +318,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     TF_RETURN_IF_ERROR(fusion_wrapper.Run(new_module.get()).status());\n   }\n \n-  NestGemmFusion nest_gemm_fusion(gpu_device_info, mlir_context);\n+  NestGemmFusion nest_gemm_fusion(gpu_device_info, symbolic_expr_context);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(new_module.get()).status());\n   return new_module;\n }\n@@ -327,7 +327,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CublasGemmAutotuneExtractor(\n     const AutotuneConfig& config, const se::DeviceDescription& gpu_device_info,\n     const se::SemanticVersion& toolkit_version,\n     const HloFusionInstruction* fusion, const DebugOptions& debug_opts,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   tsl::profiler::TraceMe traceme(\"CublasGemmAutotuneExtractor\");\n   const HloComputation* fusion_computation = fusion->called_computation();\n   std::unique_ptr<HloModule> new_module =\n@@ -358,7 +358,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CublasGemmAutotuneExtractor(\n     DotAlgorithmRewriter dot_algorithm_rewriter;\n     PriorityFusion fusion_pass(\n         /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions(),\n-        mlir_context);\n+        symbolic_expr_context);\n     TF_RETURN_IF_ERROR(scaled_dot_rewriter.Run(new_module.get()).status());\n     TF_RETURN_IF_ERROR(dot_algorithm_rewriter.Run(new_module.get()).status());\n     TF_RETURN_IF_ERROR(gemm_rewriter.Run(new_module.get()).status());\n@@ -383,7 +383,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CustomFusionKernelAutotuneExtractor(\n     const GemmFusionAutotunerImpl::CustomKernelFusionConfig& cutlass_config,\n     const AutotuneConfig& config, const se::SemanticVersion& toolkit_version,\n     const HloFusionInstruction* fusion, const DebugOptions& debug_opts,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   tsl::profiler::TraceMe traceme(\"CustomFusionKernelAutotuneExtractor\");\n   const HloComputation* fusion_computation = fusion->called_computation();\n   std::unique_ptr<HloModule> new_module =\n@@ -393,7 +393,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CustomFusionKernelAutotuneExtractor(\n   CustomKernelFusionRewriter rewriter(&config.GetDeviceDescription());\n   PriorityFusion fusion_pass(\n       /*thread_pool=*/nullptr, config.GetDeviceDescription(),\n-      PriorityFusionOptions(), mlir_context);\n+      PriorityFusionOptions(), symbolic_expr_context);\n   TF_RETURN_IF_ERROR(rewriter.Run(new_module.get()).status());\n   TF_RETURN_IF_ERROR(fusion_pass.Run(new_module.get()).status());\n \n@@ -492,7 +492,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetAutotunedModule(\n     const AutotuneConfig& autotune_config,\n     const se::SemanticVersion& toolkit_version, AutotunerCompileUtil& util,\n     const AutotuneResult result, const HloFusionInstruction* fusion,\n-    int fusion_id, mlir::MLIRContext* mlir_context) {\n+    int fusion_id, SymbolicExprContext* symbolic_expr_context) {\n   TritonGemmConfig triton_gemm_config;\n   if (result.has_triton()) {\n     TF_ASSIGN_OR_RETURN(triton_gemm_config,\n@@ -513,13 +513,14 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetAutotunedModule(\n         }\n         if (result.has_triton()) {\n           return TritonGemmAutotuneExtractor(\n-              triton_gemm_config, device_desc, fusion, debug_opts, mlir_context,\n+              triton_gemm_config, device_desc, fusion, debug_opts,\n+              symbolic_expr_context,\n               /*allow_filtering_kernels_spilling_registers=*/true);\n         }\n         if (result.has_gemm()) {\n           return CublasGemmAutotuneExtractor(autotune_config, device_desc,\n                                              toolkit_version, fusion,\n-                                             debug_opts, mlir_context);\n+                                             debug_opts, symbolic_expr_context);\n         }\n         LOG(FATAL) << \"Unknown result type: \" << result.DebugString();\n       }));\n@@ -1063,7 +1064,7 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n       auto executable_or = compile_util.Compile([&](const DebugOptions& opts) {\n         return TritonGemmAutotuneExtractor(\n             std::get<TritonGemmConfig>(config), config_.GetDeviceDescription(),\n-            fusion, opts, mlir_context_,\n+            fusion, opts, symbolic_expr_context_,\n             allow_filtering_kernels_spilling_registers);\n       });\n       if (absl::c_contains(\n@@ -1103,15 +1104,15 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n       return compile_util.Compile([&](const DebugOptions& opts) {\n         return CublasGemmAutotuneExtractor(\n             config_, config_.GetDeviceDescription(), toolkit_version_, fusion,\n-            opts, mlir_context_);\n+            opts, symbolic_expr_context_);\n       });\n     }\n \n     if (std::holds_alternative<CustomKernelFusionConfig>(config)) {\n       return compile_util.Compile([&](const DebugOptions& opts) {\n         return CustomFusionKernelAutotuneExtractor(\n             std::get<CustomKernelFusionConfig>(config), config_,\n-            toolkit_version_, fusion, opts, mlir_context_);\n+            toolkit_version_, fusion, opts, symbolic_expr_context_);\n       });\n     }\n \n@@ -1422,8 +1423,9 @@ absl::Status GemmFusionAutotunerImpl::Autotune(\n     if (debug_options_.xla_gpu_dump_autotuned_gemm_fusions() ||\n         !debug_options_.xla_gpu_dump_autotune_logs_to().empty()) {\n       TF_ASSIGN_OR_RETURN(\n-          module, GetAutotunedModule(config_, toolkit_version_, compile_util,\n-                                     best, fusion, fusion_id, mlir_context_));\n+          module,\n+          GetAutotunedModule(config_, toolkit_version_, compile_util, best,\n+                             fusion, fusion_id, symbolic_expr_context_));\n     }\n \n     if (debug_options_.xla_gpu_dump_autotuned_gemm_fusions()) {\n@@ -1543,7 +1545,7 @@ absl::StatusOr<bool> GemmFusionAutotuner::Run(\n \n   const DebugOptions& debug_options = module->config().debug_options();\n   GemmFusionAutotunerImpl autotuner(config_, toolkit_version_, debug_options,\n-                                    thread_pool_, mlir_context_);\n+                                    thread_pool_, symbolic_expr_context_);\n   GemmFusionCollector fusion_collector(&autotuner);\n   TF_ASSIGN_OR_RETURN(\n       GemmFusionCollectorResult fusions,"
        },
        {
            "sha": "50a2cf4b95a170b3a676d136f37ca4939b964697",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -42,6 +41,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n@@ -79,12 +79,12 @@ class GemmFusionAutotuner : public HloModulePass {\n                                const se::SemanticVersion& toolkit_version,\n                                tsl::thread::ThreadPool* thread_pool,\n                                const MultiProcessKeyValueStore& key_value_store,\n-                               mlir::MLIRContext* mlir_context)\n+                               SymbolicExprContext* symbolic_expr_context)\n       : config_(config),\n         toolkit_version_(toolkit_version),\n         thread_pool_(thread_pool),\n         key_value_store_(key_value_store),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override { return \"gemm-fusion-autotuner\"; }\n \n@@ -98,7 +98,7 @@ class GemmFusionAutotuner : public HloModulePass {\n   se::SemanticVersion toolkit_version_;\n   tsl::thread::ThreadPool* thread_pool_;\n   MultiProcessKeyValueStore key_value_store_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n class GemmFusionAutotunerImpl {\n@@ -107,12 +107,12 @@ class GemmFusionAutotunerImpl {\n       AutotuneConfig& config,\n       const stream_executor::SemanticVersion& toolkit_version,\n       DebugOptions debug_options, tsl::thread::ThreadPool* thread_pool,\n-      mlir::MLIRContext* mlir_context)\n+      SymbolicExprContext* symbolic_expr_context)\n       : config_(std::move(config)),\n         toolkit_version_(toolkit_version),\n         debug_options_(std::move(debug_options)),\n         thread_pool_(thread_pool),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   struct CuBlasConfig {\n     bool operator<(const CuBlasConfig& other) const;\n@@ -219,7 +219,7 @@ class GemmFusionAutotunerImpl {\n   DebugOptions debug_options_;\n   tsl::thread::ThreadPool* thread_pool_;\n   std::vector<TritonGemmConfig> triton_configs_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "441d7f2137bb1a4021b256c7e98dd713820c15ab",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 25,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/transforms/gemm_fusion.h\"\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -192,7 +193,8 @@ class StatelessAutotunerTest : public HloTestBase {\n       const HloModule& module,\n       const se::GpuComputeCapability& compute_capability,\n       const se::SemanticVersion& toolkit_version,\n-      const DebugOptions& debug_options, mlir::MLIRContext* mlir_context) {\n+      const DebugOptions& debug_options,\n+      SymbolicExprContext* symbolic_expr_context) {\n     const HloFusionInstruction& fusion = *Cast<HloFusionInstruction>(\n         module.entry_computation()->root_instruction());\n     if (!isRocm()) {\n@@ -209,7 +211,8 @@ class StatelessAutotunerTest : public HloTestBase {\n     AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n         DeviceOrDevicelessConfig{test_config}, debug_options);\n     GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n-                                      debug_options, nullptr, mlir_context);\n+                                      debug_options, nullptr,\n+                                      symbolic_expr_context);\n     return autotuner.GenerateConfigs(fusion);\n   }\n \n@@ -247,7 +250,7 @@ class StatelessAutotunerTest : public HloTestBase {\n         DeviceOrDevicelessConfig{device_config}, GetDebugOptionsForTest());\n     GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n                                       GetDebugOptionsForTest(), nullptr,\n-                                      &mlir_context_);\n+                                      &symbolic_expr_context_);\n     const HloFusionInstruction& fusion = *Cast<HloFusionInstruction>(\n         module.entry_computation()->root_instruction());\n     return autotuner.GenerateConfigs(fusion);\n@@ -265,6 +268,7 @@ class StatelessAutotunerTest : public HloTestBase {\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n constexpr absl::string_view kHloDotFusionWithAlgorithm = R\"(\n@@ -383,7 +387,8 @@ class GemmFusionAutotunerTest : public StatelessAutotunerTest {\n                 DeviceConfig{backend().default_stream_executor(),\n                              backend().memory_allocator()}},\n             opts),\n-        GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n+        GetToolkitVersion(), &thread_pool, key_value_store,\n+        &symbolic_expr_context_);\n \n     RunAndFilecheckHloRewrite(\n         hlo, std::move(pipeline), expected, [](const HloModule* m) {\n@@ -417,7 +422,8 @@ absl::StatusOr<std::vector<TritonGemmConfig>>\n GetPossibleMatmulAutotuneTritonConfigs(\n     const D& dot, const se::CudaComputeCapability& compute_capability,\n     const se::SemanticVersion& toolkit_version,\n-    const DebugOptions& debug_options, mlir::MLIRContext* mlir_context) {\n+    const DebugOptions& debug_options,\n+    SymbolicExprContext* symbolic_expr_context) {\n   TF_ASSIGN_OR_RETURN(se::DeviceDescription device_description,\n                       se::DeviceDescription::FromProto(\n                           se::GpuDeviceInfoProto::default_instance()));\n@@ -434,7 +440,8 @@ GetPossibleMatmulAutotuneTritonConfigs(\n   AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n       DeviceOrDevicelessConfig{test_config}, debug_options);\n   GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n-                                    debug_options, nullptr, mlir_context);\n+                                    debug_options, nullptr,\n+                                    symbolic_expr_context);\n   return autotuner.GenerateTritonConfigs(dot);\n }\n \n@@ -458,7 +465,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &mlir_context_));\n+          &symbolic_expr_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.num_stages > 2; }));\n@@ -481,7 +488,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &mlir_context_));\n+          &symbolic_expr_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k >= 4; }));\n@@ -504,7 +511,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &mlir_context_));\n+          &symbolic_expr_context_));\n   EXPECT_FALSE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k > 1; }));\n@@ -789,7 +796,7 @@ ENTRY main {\n   MultiProcessKeyValueStore key_value_store;\n   pipeline.AddPass<GemmFusionAutotuner>(autotune_config, GetToolkitVersion(),\n                                         &thread_pool, key_value_store,\n-                                        &mlir_context_);\n+                                        &symbolic_expr_context_);\n   pipeline.AddPass<CallInliner>();\n   for (GemmRewriterOptions::DType dtype :\n        {GemmRewriterOptions::DType::kFp8Only,\n@@ -1018,7 +1025,8 @@ ENTRY e {\n           DeviceOrDevicelessConfig{DevicelessConfig{\n               backend().default_stream_executor()->GetDeviceDescription()}},\n           opts),\n-      GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n+      GetToolkitVersion(), &thread_pool, key_value_store,\n+      &symbolic_expr_context_);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(hlo));\n@@ -1067,7 +1075,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &mlir_context_));\n+          &symbolic_expr_context_));\n \n   if (GetDebugOptionsForTest().xla_gpu_autotune_level() == 0) {\n     EXPECT_EQ(configs.size(), 1);\n@@ -1134,7 +1142,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &mlir_context_));\n+          &symbolic_expr_context_));\n   EXPECT_TRUE(std::all_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k == 1; }));\n@@ -1156,7 +1164,7 @@ TEST_F(GemmFusionAutotunerTest, SplitKFLoatNormalization) {\n       DeviceOrDevicelessConfig{test_config}, GetDebugOptionsForTest());\n   GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n                                     GetDebugOptionsForTest(), nullptr,\n-                                    &mlir_context_);\n+                                    &symbolic_expr_context_);\n   TF_ASSERT_OK_AND_ASSIGN(\n       AutotunerCompileUtil compile_util,\n       AutotunerCompileUtil::Create(autotune_config.DeviceConfig(),\n@@ -1223,7 +1231,7 @@ TEST_F(GemmFusionAutotunerTest, CreatesCustomKernelFusionConfigs) {\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &mlir_context_));\n+          GetDebugOptionsForTest(), &symbolic_expr_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const GemmFusionAutotunerImpl::BackendConfig& config) {\n@@ -1266,7 +1274,7 @@ TEST_F(GemmFusionAutotunerTest, GeneratesTwoConfigsForUpcastGemmWithPrologue) {\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &mlir_context_));\n+          GetDebugOptionsForTest(), &symbolic_expr_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1312,7 +1320,7 @@ TEST_F(GemmFusionAutotunerTest, GeneratesOneConfigForUpcastGemmWithPrologue) {\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &mlir_context_));\n+          GetDebugOptionsForTest(), &symbolic_expr_context_));\n   EXPECT_EQ(\n       1, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1361,7 +1369,7 @@ TEST_F(GemmFusionAutotunerTest,\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &mlir_context_));\n+          GetDebugOptionsForTest(), &symbolic_expr_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1470,7 +1478,8 @@ class GemmFusionShardedAutotunerTest : public GemmFusionAutotunerTest {\n       MultiProcessKeyValueStore& multi_process_key_value_store) {\n     return GemmFusionAutotuner(GetAutotuneConfigForTest(), GetToolkitVersion(),\n                                /*thread_pool=*/{},\n-                               multi_process_key_value_store, &mlir_context_);\n+                               multi_process_key_value_store,\n+                               &symbolic_expr_context_);\n   }\n };\n \n@@ -1721,14 +1730,16 @@ TEST_F(GemmFusionAutotunerTest, VerifyHopperConfigsAreDifferentFromBlackwell) {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kBlackwell, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<TritonGemmConfig> hopper_configs,\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n \n   std::set<TritonGemmConfig> blackwell_configs_set(blackwell_configs.begin(),\n                                                    blackwell_configs.end());\n@@ -1762,7 +1773,8 @@ TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsAreGenerated) {\n           *Cast<HloScaledDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kBlackwell, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n   std::set<TritonGemmConfig> blackwell_configs_set(blackwell_configs.begin(),\n                                                    blackwell_configs.end());\n   EXPECT_GT(blackwell_configs_set.size(), 0);\n@@ -1836,15 +1848,17 @@ TEST_F(GemmFusionAutotunerEnableTma,\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kAmpere, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<TritonGemmConfig> hopper_configs,\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n \n   std::set<TritonGemmConfig> ampere_configs_set(ampere_configs.begin(),\n                                                 ampere_configs.end());\n@@ -1891,7 +1905,8 @@ TEST_F(GemmFusionAutotunerEnableTma,\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n \n   auto is_disallowed_tma_config = [](const TritonGemmConfig& c) {\n     return c.num_stages > 2 && c.is_tma_allowed;"
        },
        {
            "sha": "f6755f94fb442c88e136ff49e5c5505a95d9c13e",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -61,6 +61,7 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/ir_emitter_unnested.h\"\n #include \"xla/service/gpu/metrics.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/logical_buffer.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n@@ -309,10 +310,12 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(\n           << \": \" << hlo_module->GetFingerprint128();\n \n   std::unique_ptr<mlir::MLIRContext> mlir_context = CreateMlirContext();\n+  auto symbolic_expr_context =\n+      std::make_unique<SymbolicExprContext>(mlir_context.get());\n   IrEmitterContext ir_emitter_context(\n       hlo_module, results.buffer_assignment.get(),\n       results.execution_stream_assignment.get(), platform->Name(), device_desc,\n-      mlir_context.get(), results.llvm_module.get(),\n+      symbolic_expr_context.get(), results.llvm_module.get(),\n       results.llvm_module_constants.get(),\n       /*emit_kernels=*/true);\n "
        },
        {
            "sha": "0aad3b040f6e708d6db39f919c6560dcdb2b4d4f",
            "filename": "third_party/xla/xla/service/gpu/custom_kernel_emitter_cuda.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -42,7 +42,7 @@ absl::StatusOr<std::unique_ptr<Thunk>> EmitPtxCustomKernelThunk(\n \n   TF_ASSIGN_OR_RETURN(\n       KernelCall call,\n-      KernelCall::Parse(backend_config_str, context->mlir_context()));\n+      KernelCall::Parse(backend_config_str, context->symbolic_expr_context()));\n   if (call.kernel_type != KernelCall::KernelType::kPtxSource) {\n     return absl::InvalidArgumentError(\n         \"PTX custom call backend config is not a PTX source\");"
        },
        {
            "sha": "e13fbe58a16d486c5c07492c6c46f3e36284351c",
            "filename": "third_party/xla/xla/service/gpu/fusion_dispatch_pipeline.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -15,9 +15,9 @@ limitations under the License.\n \n #include \"xla/service/gpu/fusion_dispatch_pipeline.h\"\n \n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/transforms/fusion_block_level_rewriter.h\"\n #include \"xla/service/gpu/transforms/fusion_dynamic_memcpy_rewriter.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -30,11 +30,11 @@ namespace gpu {\n HloPassPipeline FusionDispatchPipeline(\n     const se::DeviceDescription& device_description,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   HloPassPipeline pipeline(\"fusion-dispatch-pipeline\");\n   pipeline.AddPass<HloDCE>();\n   pipeline.AddPass<FusionBlockLevelRewriter>(device_description, shape_size_fn,\n-                                             mlir_context);\n+                                             symbolic_expr_context);\n   pipeline.AddPass<FusionDynamicMemcpyRewriter>();\n   return pipeline;\n }"
        },
        {
            "sha": "019c8605acb8f240efe4472b125672e384befc25",
            "filename": "third_party/xla/xla/service/gpu/fusion_dispatch_pipeline.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -16,8 +16,8 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_FUSION_DISPATCH_PIPELINE_H_\n #define XLA_SERVICE_GPU_FUSION_DISPATCH_PIPELINE_H_\n \n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/xla.pb.h\"\n@@ -30,7 +30,7 @@ namespace gpu {\n HloPassPipeline FusionDispatchPipeline(\n     const se::DeviceDescription& device_description,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-    mlir::MLIRContext* mlir_context);\n+    SymbolicExprContext* symbolic_expr_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "146db8df64140baba6e7545c3449c7fd6c0aae7f",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n #include \"xla/service/cpu_gpu_shape_verifier.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/transforms/multi_output_fusion.h\"\n #include \"xla/service/gpu/transforms/priority_fusion.h\"\n@@ -44,7 +45,7 @@ HloPassPipeline FusionPipeline(\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n     tsl::thread::ThreadPool* thread_pool,\n     const se::DeviceDescription& gpu_device_info,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   HloPassFix<HloPassPipeline> fusion(\"fusion\");\n   // We try to split variadic ops with many parameters into several such ops\n   // to avoid exceeding the parameter space.\n@@ -64,7 +65,7 @@ HloPassPipeline FusionPipeline(\n       /*count_multiple_input_accesses=*/true};\n   fusion.AddPass<PriorityFusion>(thread_pool, gpu_device_info,\n                                  std::move(cost_analysis_options),\n-                                 mlir_context);\n+                                 symbolic_expr_context);\n \n   // Running CSE affects how many users an op has. This plays a role in what\n   // we detect as a tiled transpose fusion.\n@@ -73,7 +74,7 @@ HloPassPipeline FusionPipeline(\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);\n   fusion.AddPass<HloDCE>();\n   fusion.AddPass<MultiOutputFusion>(gpu_device_info, shape_size_bytes_function,\n-                                    mlir_context);\n+                                    symbolic_expr_context);\n   fusion.AddPass<HloCSE>(\n       /*is_layout_sensitive=*/true, /*ignore_control_dependencies=*/false,\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);"
        },
        {
            "sha": "a95396efba3cc5952208d7f3da1f174985300742",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -16,8 +16,8 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_FUSION_PIPELINE_H_\n #define XLA_SERVICE_GPU_FUSION_PIPELINE_H_\n \n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/xla.pb.h\"\n@@ -33,7 +33,7 @@ HloPassPipeline FusionPipeline(\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n     tsl::thread::ThreadPool* thread_pool,\n     const se::DeviceDescription& gpu_device_info,\n-    mlir::MLIRContext* mlir_context);\n+    SymbolicExprContext* symbolic_expr_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "c3fbe70bbbf5fe76068726a347a88ed3a6baa378",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 23,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -191,6 +191,7 @@ limitations under the License.\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/metrics.h\"\n #include \"xla/service/gpu/model/collective_ptable_stats_collection.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_cost_model_stats_collection.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/matmul_ptable_stats_collection.h\"\n@@ -505,7 +506,7 @@ GpuThunkAotCompilationResult::LoadExecutable(\n \n   IrEmitterContext ir_emitter_context(\n       hlo_module.get(), buffer_assignment.get(), &execution_stream_assignment,\n-      platform_name, gpu_device_info, gpu_compiler->mlir_context(),\n+      platform_name, gpu_device_info, gpu_compiler->symbolic_expr_context(),\n       llvm_module.get(),\n       /*llvm_module_constants=*/nullptr,\n       /*emit_kernels=*/false);\n@@ -1198,7 +1199,7 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n                              const Compiler::TargetConfig& gpu_target_config,\n                              tsl::thread::ThreadPool* thread_pool,\n                              HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-                             mlir::MLIRContext* mlir_context) {\n+                             SymbolicExprContext* symbolic_expr_context) {\n   const se::DeviceDescription& gpu_device_info =\n       gpu_target_config.device_description;\n \n@@ -1208,7 +1209,7 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n \n   TF_RETURN_IF_ERROR(\n       FusionPipeline(hlo_module->config().debug_options(), shape_size_fn,\n-                     thread_pool, gpu_device_info, mlir_context)\n+                     thread_pool, gpu_device_info, symbolic_expr_context)\n           .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n           .status());\n \n@@ -1267,13 +1268,13 @@ void AddCollectiveCombinerPasses(\n     const se::DeviceDescription& device_description,\n     const GpuAliasInfo* alias_info, int pointer_size,\n     const GpuCompiler::CompileOptions& options,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   const DebugOptions& opts = module.config().debug_options();\n \n   if (EnableHeuristicCollectiveCombining(module.config(), device_description,\n                                          options.slice_size)) {\n     pipeline.AddPass<CollectiveCombinerAnnotator>(\n-        device_description, alias_info, pointer_size, mlir_context);\n+        device_description, alias_info, pointer_size, symbolic_expr_context);\n   }\n \n   pipeline.AddPass<GpuAllGatherCombiner>(\n@@ -1299,11 +1300,12 @@ absl::Status RunPostFusionPasses(\n     HloModule* hlo_module, const se::DeviceDescription& device_description,\n     const GpuAliasInfo* alias_info, int pointer_size,\n     const GpuCompiler::CompileOptions& options,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   HloPassPipeline pipeline(\"post-fusion optimization\");\n   pipeline.AddPass<RenameFusions>();\n   AddCollectiveCombinerPasses(pipeline, *hlo_module, device_description,\n-                              alias_info, pointer_size, options, mlir_context);\n+                              alias_info, pointer_size, options,\n+                              symbolic_expr_context);\n \n   pipeline.AddPass<AllReduceContiguous>();\n \n@@ -1355,7 +1357,7 @@ absl::Status RunPostFusionVerificationPasses(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n     const GpuCompiler::CompileOptions& options,\n     const Compiler::TargetConfig& gpu_target_config,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   HloPassPipeline pipeline(\"post-fusion-verification-pipeline optimization\");\n \n   if (hlo_module->config()\n@@ -1365,7 +1367,7 @@ absl::Status RunPostFusionVerificationPasses(\n         GetDeviceConfig(stream_exec, options, gpu_target_config);\n     if (!device_config.IsDeviceless()) {\n       pipeline.AddPass<TritonFusionNumericsVerifier>(device_config,\n-                                                     mlir_context);\n+                                                     symbolic_expr_context);\n     }\n   }\n \n@@ -1620,12 +1622,12 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   TF_RETURN_IF_ERROR(\n       RunDynamicSliceFusionPasses(hlo_module, /*platform_id=*/PlatformId()));\n \n-  TF_RETURN_IF_ERROR(RunFusionPasses(hlo_module, gpu_target_config,\n-                                     thread_pool.get_mutable(),\n-                                     ShapeSizeBytesFunction(), &mlir_context_));\n+  TF_RETURN_IF_ERROR(\n+      RunFusionPasses(hlo_module, gpu_target_config, thread_pool.get_mutable(),\n+                      ShapeSizeBytesFunction(), &symbolic_expr_context_));\n   TF_RETURN_IF_ERROR(RunPostFusionPasses(hlo_module, device_description,\n                                          alias_info, pointer_size_, options,\n-                                         &mlir_context_));\n+                                         &symbolic_expr_context_));\n   TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(hlo_module));\n   TF_RETURN_IF_ERROR(RunPostFusionSimplificationPasses(\n       hlo_module,\n@@ -1635,8 +1637,9 @@ absl::Status GpuCompiler::OptimizeHloModule(\n           gpu_target_config.platform_name == \"ROCM\"),\n       gpu_version, gpu_target_config));\n \n-  TF_RETURN_IF_ERROR(RunPostFusionVerificationPasses(\n-      hlo_module, stream_exec, options, gpu_target_config, &mlir_context_));\n+  TF_RETURN_IF_ERROR(RunPostFusionVerificationPasses(hlo_module, stream_exec,\n+                                                     options, gpu_target_config,\n+                                                     &symbolic_expr_context_));\n \n   TF_RETURN_IF_ERROR(\n       RunCollectiveScheduleLinearizerPasses(hlo_module, stream_exec));\n@@ -1836,7 +1839,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       pipeline.AddPass<HloDCE>();\n       pipeline.AddPass<SoftmaxRewriterTriton>(\n           gpu_target_config.device_description, ShapeSizeBytesFunction(),\n-          &mlir_context_,\n+          &symbolic_expr_context_,\n           /*only_fuse_if_profitable=*/true);\n     }\n \n@@ -1915,7 +1918,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n   // Match the location of this pass in `gemm_fusion_autotuner.cc` to make sure\n   // that there is no discrepancy.\n   pipeline.AddPass<NestGemmFusion>(gpu_target_config.device_description,\n-                                   &mlir_context_);\n+                                   &symbolic_expr_context_);\n \n   // Clean up new_tuple described above.\n   pipeline.AddPass<TupleSimplifier>();\n@@ -2542,9 +2545,10 @@ GpuCompiler::CompileToBackendResult(\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n   TF_RETURN_IF_ERROR(\n       RunPreSchedulingPasses(module, gpu_device_info, alias_info.get()));\n-  TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n-                      ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n-                                        &mlir_context_, alias_info.get()));\n+  TF_ASSIGN_OR_RETURN(\n+      ScheduleMetadata schedule_metadata,\n+      ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n+                        &symbolic_expr_context_, alias_info.get()));\n   HloPassPipeline pipeline(\"scheduled-gpu-module\");\n   AddHloVerifier(&pipeline);\n   TF_RETURN_IF_ERROR(pipeline.Run(module).status());\n@@ -2876,14 +2880,14 @@ absl::Status GpuCompiler::RunPreSchedulingPasses(\n         /*count_multiple_input_accesses=*/true};\n     // Cost model analysis for compute.\n     pipeline.AddPass<GpuCostModelStatsCollection>(\n-        gpu_device_info, cost_analysis_options, &mlir_context_);\n+        gpu_device_info, cost_analysis_options, &symbolic_expr_context_);\n     // S-curve model analysis for collectives.\n     if (module->config()\n             .debug_options()\n             .xla_gpu_enable_analytical_sol_latency_estimator()) {\n       pipeline.AddPass<SolGpuCostModelStatsCollection>(\n           gpu_device_info, ShapeSizeBytesFunction(), pointer_size_,\n-          &mlir_context_);\n+          &symbolic_expr_context_);\n     }\n \n     // Perf tables model analysis for collectives.\n@@ -3030,7 +3034,7 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(\n     // This needs to run after every pass affecting fusions. The last passes\n     // that create new fusions are FusionWrapper and StreamAttributeAnnotator.\n     main_pipeline.AddPass<HloPassPipeline>(FusionDispatchPipeline(\n-        gpu_device_info, ShapeSizeBytesFunction(), mlir_context()));\n+        gpu_device_info, ShapeSizeBytesFunction(), &symbolic_expr_context_));\n   }\n \n   // Pipeline with passes which wrap a scheduled module into command buffers."
        },
        {
            "sha": "cc900a631366df3946bb77702b0d677327370b2e",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/service/gpu/compile_module_to_llvm_ir.h\"\n #include \"xla/service/gpu/executable.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -113,6 +114,9 @@ class GpuCompiler : public LLVMCompiler {\n       se::StreamExecutor* executor);\n \n   mlir::MLIRContext* mlir_context() { return &mlir_context_; }\n+  SymbolicExprContext* symbolic_expr_context() {\n+    return &symbolic_expr_context_;\n+  }\n \n   virtual std::unique_ptr<GpuAliasInfo> GetAliasInfo(\n       const se::DeviceDescription& device_description) const {\n@@ -294,6 +298,9 @@ class GpuCompiler : public LLVMCompiler {\n   // A MLIR context that can be used by pre-codegen passes. For codegen, we will\n   // need to have a context with more dialects registered.\n   mlir::MLIRContext mlir_context_;\n+  // A symbolic expression context that can be used by pre-codegen passes to\n+  // create symbolic expressions.\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "a247dd0b03ed44e049756947f0522bb71cebe178",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -121,7 +121,7 @@ class GpuCompilerTest : public HloTestBase {\n     std::unique_ptr<GpuAliasInfo> alias_info =\n         gpu_compiler->GetAliasInfo(gpu_device_info);\n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, 4, gpu_device_info,\n-                                         gpu_compiler->mlir_context(),\n+                                         gpu_compiler->symbolic_expr_context(),\n                                          alias_info.get())\n                            .status());\n     return gpu_compiler->RunPostSchedulingPipelines("
        },
        {
            "sha": "0b01cedab2a5148a0ce35dcdb0e62a6b09f5a4ba",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/service/gpu/gpu_hlo_schedule.h\"\n \n-\n #include <cstddef>\n #include <cstdint>\n #include <deque>\n@@ -37,7 +36,6 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n@@ -58,6 +56,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_latency_hiding_scheduler.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/model/analytical_latency_estimator.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/sol_latency_estimator.h\"\n #include \"xla/service/gpu/transforms/collectives/async_collective_annotator.h\"\n@@ -463,7 +462,7 @@ std::string TagWithFingerprint(HloModule* module) {\n std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     const HloModule& module, int pointer_size,\n     const se::DeviceDescription& gpu_device_info, absl::string_view fingerprint,\n-    const SchedulerConfig& config, mlir::MLIRContext* mlir_context) {\n+    const SchedulerConfig& config, SymbolicExprContext* symbolic_expr_context) {\n   const DebugOptions& options = module.config().debug_options();\n \n   auto gpu_latency_estimator =\n@@ -488,7 +487,7 @@ std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     return std::make_unique<AnalyticalLatencyEstimator>(\n         config, std::move(gpu_latency_estimator), gpu_device_info,\n         ShapeSizeBytesFunction(pointer_size), module.entry_computation(),\n-        mlir_context);\n+        symbolic_expr_context);\n   }\n \n   if (SolLatencyEstimator::IsSupportedForModule(module, gpu_device_info)) {\n@@ -511,7 +510,7 @@ std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     auto sol_latency_estimator = SolLatencyEstimator::Create(\n         config, std::move(gpu_latency_estimator), gpu_device_info,\n         ShapeSizeBytesFunction(pointer_size), module.entry_computation(),\n-        mlir_context, std::move(cost_analysis));\n+        symbolic_expr_context, std::move(cost_analysis));\n     if (sol_latency_estimator.ok()) {\n       return std::move(*sol_latency_estimator);\n     }\n@@ -565,7 +564,8 @@ LegalizeSchedulingAnnotations::Config SchedulingAnnotationsConfig() {\n absl::Status RunLatencyHidingSchedulerPasses(\n     HloModule* module, int pointer_size, absl::string_view fingerprint,\n     uint64_t memory_limit, const se::DeviceDescription& gpu_device_info,\n-    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info) {\n+    SymbolicExprContext* symbolic_expr_context,\n+    const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"RunLatencyHidingSchedulerPasses\");\n   HloPassPipeline pipeline(\"latency-hiding-scheduler\");\n   const DebugOptions& options = module->config().debug_options();\n@@ -580,7 +580,7 @@ absl::Status RunLatencyHidingSchedulerPasses(\n \n   std::unique_ptr<LatencyEstimator> estimator =\n       GetLatencyEstimator(*module, pointer_size, gpu_device_info, fingerprint,\n-                          config, mlir_context);\n+                          config, symbolic_expr_context);\n \n   if (NeedAccuracyChecker(options, *estimator)) {\n     pipeline.AddPass<PGLEAccuracyChecker>(\n@@ -723,7 +723,8 @@ absl::Status RunAsyncCollectivesConversionPasses(HloModule* module) {\n absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n     HloModule* module, int64_t pointer_size,\n     const se::DeviceDescription& gpu_device_info,\n-    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info) {\n+    SymbolicExprContext* symbolic_expr_context,\n+    const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"ScheduleGpuModule\");\n \n   // Tag the module with its 128 bit fingerprint. The fingerprint should include\n@@ -757,7 +758,7 @@ absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n   if (enable_latency_hiding_scheduler) {\n     TF_RETURN_IF_ERROR(RunLatencyHidingSchedulerPasses(\n         module, pointer_size, fingerprint, memory_limit, gpu_device_info,\n-        mlir_context, alias_info));\n+        symbolic_expr_context, alias_info));\n   }\n \n   return ScheduleMetadata{memory_limit, peak_memory_bytes};"
        },
        {
            "sha": "2371692b7f236085ca82363e3b4532cba5b01331",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -21,10 +21,10 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/gpu/alias_info.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/latency_hiding_scheduler.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"tsl/profiler/protobuf/profiled_instructions.pb.h\"\n@@ -57,7 +57,7 @@ uint64_t GetSchedulerMemoryLimit(const HloModule& module,\n absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n     HloModule* module, int64_t pointer_size,\n     const se::DeviceDescription& gpu_device_info,\n-    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info);\n+    SymbolicExprContext* symbolic_expr_context, const GpuAliasInfo* alias_info);\n \n HloInstructionSequence PostProcessSchedule(const HloInstructionSequence& input);\n "
        },
        {
            "sha": "8b71129e020b2e080f8ab8ce16d99bf17ec42254",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -81,7 +81,7 @@ class GpuHloScheduleTest : public HloTestBase {\n         gpu_compiler->GetAliasInfo(gpu_device_info);\n     int64_t pointer_size = gpu_compiler->GetPointerSize();\n     return xla::gpu::ScheduleGpuModule(module, pointer_size, gpu_device_info,\n-                                       gpu_compiler->mlir_context(),\n+                                       gpu_compiler->symbolic_expr_context(),\n                                        alias_info.get());\n   }\n "
        },
        {
            "sha": "e6f378cfe2130bb14de89dfbbf7320bb82dde7bb",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/gpu_hlo_schedule.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/latency_hiding_scheduler.h\"\n #include \"xla/service/profile_guided_latency_estimator.h\"\n@@ -78,8 +79,8 @@ class GpuLatencyHidingSchedulerBaseTest\n     options.set_xla_gpu_pgle_accuracy_checker(strictness);\n \n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, /*pointer_size=*/8,\n-                                         gpu_device_info, &mlir_context_,\n-                                         &alias_info)\n+                                         gpu_device_info,\n+                                         &symbolic_expr_context_, &alias_info)\n                            .status());\n     return module;\n   }\n@@ -99,6 +100,7 @@ class GpuLatencyHidingSchedulerBaseTest\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(GpuLatencyHidingSchedulerBaseTest,"
        },
        {
            "sha": "b16d6e3690c89fd7bac6daa26207b435fafb1578",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -1,3 +1,4 @@\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n /* Copyright 2017 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -27,7 +28,6 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Module.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Operation.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/host_execute_thunk.h\"\n@@ -69,14 +69,15 @@ class IrEmitterContext {\n                    const ExecutionStreamAssignment* execution_stream_assignment,\n                    std::string platform_name,\n                    const se::DeviceDescription& gpu_device_info,\n-                   mlir::MLIRContext* mlir_context, llvm::Module* llvm_module,\n+                   SymbolicExprContext* symbolic_expr_context,\n+                   llvm::Module* llvm_module,\n                    llvm::Module* llvm_module_constants, bool emit_kernels)\n       : hlo_module_(hlo_module),\n         buffer_assignment_(buffer_assignment),\n         execution_stream_assignment_(execution_stream_assignment),\n         platform_name_(std::move(platform_name)),\n         gpu_device_info_(gpu_device_info),\n-        mlir_context_(mlir_context),\n+        symbolic_expr_context_(symbolic_expr_context),\n         llvm_module_(llvm_module),\n         llvm_module_constants_(llvm_module_constants),\n         emit_kernels_(emit_kernels) {}\n@@ -109,7 +110,14 @@ class IrEmitterContext {\n         std::get_if<se::RocmComputeCapability>(&gpu_compute_capability());\n     return cc != nullptr ? *cc : se::RocmComputeCapability();\n   }\n-  mlir::MLIRContext* mlir_context() { return mlir_context_; }\n+\n+  // TODO: b/451959933 - Add nullability annotation to be explicit about this\n+  // pointer: go/totw/230. Alternatively, return by reference instead of pointer\n+  // (and require reference in ctor) to signal that it is always present.\n+  SymbolicExprContext* symbolic_expr_context() {\n+    return symbolic_expr_context_;\n+  }\n+\n   llvm::Module* llvm_module() { return llvm_module_; }\n   // A separate module can optionally be used to emit constants.\n   llvm::Module* llvm_module_constants() {\n@@ -160,7 +168,7 @@ class IrEmitterContext {\n   const ExecutionStreamAssignment* execution_stream_assignment_;\n   std::string platform_name_;\n   const se::DeviceDescription& gpu_device_info_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n   llvm::Module* llvm_module_;\n   llvm::Module* llvm_module_constants_;\n   NameUniquer name_uniquer_;"
        },
        {
            "sha": "776f05b5e67705a9c9d08c6a97a43cb3b0e3e407",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -1217,7 +1217,8 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n             : instr->raw_backend_config_string();\n     if (!backend_config_str.empty()) {\n       mlir::Attribute attr = mlir::parseAttribute(\n-          backend_config_str, ir_emitter_context_->mlir_context());\n+          backend_config_str,\n+          ir_emitter_context_->symbolic_expr_context()->GetMLIRContext());\n       auto dict = mlir::dyn_cast_or_null<mlir::DictionaryAttr>(attr);\n       if (dict == nullptr) {\n         return absl::InternalError(\n@@ -1449,7 +1450,8 @@ absl::Status IrEmitterUnnested::EmitTopKCustomCall(\n absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n     const HloCustomCallInstruction* instr) {\n   auto generate = [this, &instr]() -> absl::StatusOr<KernelReuseCache::Entry> {\n-    mlir::MLIRContext& mlir_context = *ir_emitter_context_->mlir_context();\n+    mlir::MLIRContext& mlir_context =\n+        *ir_emitter_context_->symbolic_expr_context()->GetMLIRContext();\n     LoadMlirDialectsForTriton(mlir_context);\n     auto call =\n         TritonCall::Parse(instr->raw_backend_config_string(), &mlir_context);\n@@ -1626,7 +1628,7 @@ absl::Status IrEmitterUnnested::EmitFusion(const HloFusionInstruction* instr) {\n           /*buffer_assignment=*/\n           &ir_emitter_context_->buffer_assignment(),\n           /*call_graph=*/*call_graph_),\n-      ir_emitter_context_->mlir_context());\n+      ir_emitter_context_->symbolic_expr_context());\n   TF_ASSIGN_OR_RETURN(auto result, emitter->Emit(*ir_emitter_context_, *instr));\n \n   const ExecutionStreamAssignment& stream_assignment ="
        },
        {
            "sha": "2c9ba3084652546101ca025089769841ae6326b7",
            "filename": "third_party/xla/xla/service/gpu/kernel_call.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -29,9 +29,9 @@ limitations under the License.\n #include \"mlir/AsmParser/AsmParser.h\"\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Parser/Parser.h\"\n #include \"mlir/Support/LLVM.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -52,10 +52,11 @@ absl::StatusOr<KernelCall::KernelType> ParseKernelType(\n   }\n }\n \n-absl::StatusOr<KernelCall> KernelCall::Parse(absl::string_view backend_config,\n-                                             mlir::MLIRContext* mlir_context) {\n-  auto attrs = mlir::cast<mlir::DictionaryAttr>(\n-      mlir::parseAttribute(backend_config, mlir_context));\n+absl::StatusOr<KernelCall> KernelCall::Parse(\n+    absl::string_view backend_config,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  auto attrs = mlir::cast<mlir::DictionaryAttr>(mlir::parseAttribute(\n+      backend_config, symbolic_expr_context->GetMLIRContext()));\n \n   // Check for required \"name\" field\n   auto name_attr = attrs.getAs<mlir::StringAttr>(\"name\");"
        },
        {
            "sha": "6f7aeb3708af8d900c01a2f8a44d6867ba83c5df",
            "filename": "third_party/xla/xla/service/gpu/kernel_call.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,7 +22,7 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n \n namespace xla::gpu {\n@@ -41,8 +41,9 @@ struct KernelCall {\n   std::vector<int32_t> output_indices;\n \n   // Parse the metadata of a __gpu$xla.gpu.ptx call.\n-  static absl::StatusOr<KernelCall> Parse(absl::string_view backend_config,\n-                                          mlir::MLIRContext* mlir_context);\n+  static absl::StatusOr<KernelCall> Parse(\n+      absl::string_view backend_config,\n+      SymbolicExprContext* symbolic_expr_context);\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "c03ae4e687bd12deaeca08c7b8c8da6bf989cf00",
            "filename": "third_party/xla/xla/service/gpu/kernel_call_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/strings/string_view.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n@@ -33,9 +34,12 @@ class KernelCallTest : public ::testing::Test {\n  protected:\n   void SetUp() override {\n     mlir_context_ = std::make_unique<mlir::MLIRContext>();\n+    symbolic_expr_context_ =\n+        std::make_unique<SymbolicExprContext>(mlir_context_.get());\n   }\n \n   std::unique_ptr<mlir::MLIRContext> mlir_context_;\n+  std::unique_ptr<SymbolicExprContext> symbolic_expr_context_;\n };\n \n TEST_F(KernelCallTest, ParseBasicConfiguration) {\n@@ -54,7 +58,7 @@ TEST_F(KernelCallTest, ParseBasicConfiguration) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, mlir_context_.get()));\n+      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"test_kernel\");\n   EXPECT_EQ(kernel_call.kernel_data,\n@@ -86,7 +90,7 @@ TEST_F(KernelCallTest, ParseWithOutputIndices) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, mlir_context_.get()));\n+      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"kernel_with_outputs\");\n   EXPECT_EQ(kernel_call.block_dim.x, 10);\n@@ -119,7 +123,7 @@ TEST_F(KernelCallTest, ParseMinimalConfiguration) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, mlir_context_.get()));\n+      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"minimal_kernel\");\n   EXPECT_EQ(kernel_call.kernel_data, \".entry minimal_kernel() { ret; }\");\n@@ -149,7 +153,7 @@ TEST_F(KernelCallTest, ParseLargeDimensions) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, mlir_context_.get()));\n+      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"large_kernel\");\n   EXPECT_EQ(kernel_call.block_dim.x, 65535);\n@@ -178,7 +182,7 @@ TEST_F(KernelCallTest, ParseEmptyOutputIndices) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, mlir_context_.get()));\n+      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"no_outputs\");\n   EXPECT_EQ(kernel_call.shared_mem, 512);\n@@ -202,7 +206,7 @@ TEST_F(KernelCallTest, ParseSingleOutputIndex) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, mlir_context_.get()));\n+      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"single_output\");\n   EXPECT_EQ(kernel_call.shared_mem, 256);\n@@ -227,7 +231,7 @@ TEST_F(KernelCallTest, ParseComplexkernel_data) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, mlir_context_.get()));\n+      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"complex_kernel\");\n   EXPECT_THAT(kernel_call.kernel_data, HasSubstr(\".version 7.5\"));"
        },
        {
            "sha": "9a9a6a9914371a4e8ce737929270d0cb08b8d6b7",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -36,6 +36,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:latency_hiding_scheduler\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/log\",\n@@ -66,6 +67,7 @@ cc_library(\n         \"//xla/service:latency_hiding_scheduler\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:flag_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:env\",\n@@ -77,7 +79,6 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/time\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -97,6 +98,7 @@ xla_cc_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:latency_hiding_scheduler\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n@@ -169,6 +171,7 @@ xla_test(\n         \"//xla/service:latency_hiding_scheduler\",\n         \"//xla/service/gpu:alias_info\",\n         \"//xla/service/gpu:gpu_compiler\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -226,11 +229,11 @@ cc_library(\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_graph_dumper\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -245,6 +248,7 @@ xla_cc_test(\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//mlir:IR\",\n@@ -319,6 +323,7 @@ cc_library(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n@@ -327,7 +332,6 @@ cc_library(\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -343,6 +347,7 @@ xla_cc_test(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n@@ -368,14 +373,14 @@ cc_library(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -397,6 +402,7 @@ xla_cc_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n@@ -533,6 +539,7 @@ cc_library(\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n@@ -569,6 +576,7 @@ xla_cc_test(\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n@@ -615,6 +623,7 @@ cc_library(\n         \"//xla/hlo/analysis:interval\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n@@ -640,8 +649,8 @@ xla_cc_test(\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:instruction_fusion\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/log\",\n@@ -668,6 +677,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service/gpu:gpu_fusible\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/types:span\",\n@@ -694,6 +704,7 @@ xla_cc_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n@@ -869,6 +880,7 @@ cc_library(\n         \"//xla/service/gpu:cublas_cudnn\",\n         \"//xla/service/gpu:gpu_hlo_schedule\",\n         \"//xla/service/gpu:gpu_latency_hiding_scheduler\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -877,7 +889,6 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -890,6 +901,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1106,6 +1118,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "020ac9a6426ce9761de5735e3543b28ddb11392b",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -20,10 +20,10 @@ limitations under the License.\n \n #include \"absl/log/log.h\"\n #include \"absl/time/time.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_collective_performance_model.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n@@ -77,10 +77,10 @@ AnalyticalLatencyEstimator::AnalyticalLatencyEstimator(\n     std::unique_ptr<LatencyEstimator> latency_estimator,\n     const se::DeviceDescription& gpu_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-    HloComputation* computation, mlir::MLIRContext* mlir_context)\n+    HloComputation* computation, SymbolicExprContext* symbolic_expr_context)\n     : config_(config),\n       gpu_info_(gpu_info),\n-      gpu_performance_model_(gpu_info, mlir_context),\n+      gpu_performance_model_(gpu_info, symbolic_expr_context),\n       latency_estimator_(std::move(latency_estimator)),\n       shape_size_function_(shape_size_function) {\n   cost_analysis_.emplace("
        },
        {
            "sha": "9e09359ad2a8e002d047a1220128bc3e71f19cea",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -40,7 +41,7 @@ class AnalyticalLatencyEstimator : public LatencyEstimator {\n       std::unique_ptr<LatencyEstimator> latency_estimator,\n       const se::DeviceDescription& gpu_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      HloComputation* computation, mlir::MLIRContext* mlir_context);\n+      HloComputation* computation, SymbolicExprContext* symbolic_expr_context);\n \n   TimeCost GetLatencyBetween(const HloGraphNode& from,\n                              const HloGraphNode& target) const override;"
        },
        {
            "sha": "88ebc1ba97762031fbe3b66a3924ddb41b830dba",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/gpu_compiler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/latency_hiding_scheduler.h\"\n@@ -171,11 +172,13 @@ ENTRY entry {\n   EXPECT_TRUE(hlo_module->has_entry_computation());\n \n   auto mlir_context = std::make_unique<mlir::MLIRContext>();\n+  auto symbolic_expr_context =\n+      std::make_unique<SymbolicExprContext>(mlir_context.get());\n   auto scheduler_config = GetDefaultSchedulerConfig();\n   auto latency_estimator = std::make_unique<AnalyticalLatencyEstimator>(\n       scheduler_config, std::make_unique<ApproximateLatencyEstimator>(),\n       dev_info, HloCostAnalysis::DefaultShapeSize,\n-      hlo_module->entry_computation(), mlir_context.get());\n+      hlo_module->entry_computation(), symbolic_expr_context.get());\n   auto alias_info = GetAliasInfo();\n   EXPECT_TRUE(RunScheduler(hlo_module.get(), scheduler_config, alias_info.get(),\n                            std::move(latency_estimator))"
        },
        {
            "sha": "ecf93a97b6a438a5d1550aef0db119731ebb9331",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"xla/layout.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -502,9 +503,9 @@ bool IsIndexingCoalesced(IndexingMap& thread_x_to_linearized_input,\n std::optional<CoalescingMap> ComputeCoalescingForAllOperands(\n     const HloFusionAnalysis& fusion_analysis,\n     absl::Span<const HloInstruction* const> operands,\n-    MLIRContext* mlir_context) {\n+    gpu::SymbolicExprContext* symbolic_expr_context) {\n   auto emitter = GetFusionEmitter(\n-      PreBufferAssignmentFusionInfo{fusion_analysis}, mlir_context);\n+      PreBufferAssignmentFusionInfo{fusion_analysis}, symbolic_expr_context);\n   const auto* fusion_interface =\n       dynamic_cast<const KernelFusionInterface*>(emitter.get());\n \n@@ -513,21 +514,21 @@ std::optional<CoalescingMap> ComputeCoalescingForAllOperands(\n   }\n   llvm::SmallVector<IndexingMap, 4>\n       operand_logical_to_linearized_physical_maps =\n-          MapLogicalToLinearizedPhysicalShape(operands, mlir_context);\n+          MapLogicalToLinearizedPhysicalShape(operands, symbolic_expr_context);\n   GroupedByOpIndexingMap thread_id_to_input_memory_layouts;\n   for (const auto& [root_index, hero] :\n        llvm::enumerate(fusion_analysis.fusion_heroes())) {\n     // Compute thread ID -> hero operand indexing maps.\n     std::optional<std::vector<IndexingMap>> hero_indexing_maps =\n         fusion_interface->ComputeThreadIdToInputIndexing(root_index,\n-                                                         mlir_context);\n+                                                         symbolic_expr_context);\n     if (!hero_indexing_maps.has_value()) {\n       return std::nullopt;\n     }\n     GetThreadIdToInputMemoryLayoutsMaps(\n         fusion_analysis.fusion(), *hero_indexing_maps,\n         fusion_analysis.fusion_hero(root_index), operands,\n-        operand_logical_to_linearized_physical_maps, mlir_context,\n+        operand_logical_to_linearized_physical_maps, symbolic_expr_context,\n         thread_id_to_input_memory_layouts);\n   }\n \n@@ -567,23 +568,23 @@ std::optional<CoalescingMap> ComputeCoalescingForAllOperands(\n CoalescingAnalysis CoalescingAnalysis::Create(\n     const HloInstruction* instr,\n     absl::Span<const HloInstruction* const> operands,\n-    const HloFusionAnalysis& fusion_analysis, MLIRContext* mlir_context,\n-    bool use_heuristic) {\n+    const HloFusionAnalysis& fusion_analysis,\n+    gpu::SymbolicExprContext* symbolic_expr_context, bool use_heuristic) {\n   return Create(/*producer=*/instr, /*consumer=*/nullptr, operands,\n-                fusion_analysis, mlir_context, use_heuristic);\n+                fusion_analysis, symbolic_expr_context, use_heuristic);\n }\n \n /*static*/\n CoalescingAnalysis CoalescingAnalysis::Create(\n     const HloInstruction* producer, const HloInstruction* consumer,\n     absl::Span<const HloInstruction* const> operands,\n-    const HloFusionAnalysis& fusion_analysis, MLIRContext* mlir_context,\n-    bool use_heuristic) {\n+    const HloFusionAnalysis& fusion_analysis,\n+    gpu::SymbolicExprContext* symbolic_expr_context, bool use_heuristic) {\n   std::optional<CoalescingMap> coalescing_per_operand;\n \n   if (!use_heuristic) {\n     coalescing_per_operand = ComputeCoalescingForAllOperands(\n-        fusion_analysis, operands, mlir_context);\n+        fusion_analysis, operands, symbolic_expr_context);\n   }\n \n   if (coalescing_per_operand.has_value()) {"
        },
        {
            "sha": "519db0300e74e629ef8888ea13b34fad7b21bce5",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.h",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -21,11 +21,11 @@ limitations under the License.\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/SmallVector.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla::gpu {\n@@ -43,14 +43,16 @@ class CoalescingAnalysis {\n       const HloInstruction* instr,\n       absl::Span<const HloInstruction* const> operands,\n       const HloFusionAnalysis& fusion_analysis,\n-      mlir::MLIRContext* mlir_context = nullptr, bool use_heuristic = true);\n+      SymbolicExprContext* symbolic_expr_context = nullptr,\n+      bool use_heuristic = true);\n \n   // Computes read coalescing for operands of fused `producer` and `consumer`.\n   static CoalescingAnalysis Create(\n       const HloInstruction* producer, const HloInstruction* consumer,\n       absl::Span<const HloInstruction* const> operands,\n       const HloFusionAnalysis& fusion_analysis,\n-      mlir::MLIRContext* mlir_context = nullptr, bool use_heuristic = true);\n+      SymbolicExprContext* symbolic_expr_context = nullptr,\n+      bool use_heuristic = true);\n \n   // Returns true if the operand is read coalesced.\n   bool IsReadCoalesced(const HloInstruction* operand) const;"
        },
        {
            "sha": "07bab772570078b253c6ca034301581cef2cc1e4",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -60,13 +61,13 @@ class CoalescingTest : public HloHardwareIndependentTestBase {\n     auto fusion_adaptor = HloFusionAdaptor::ForInstruction(root);\n     auto analysis = HloFusionAnalysis::Create(*root, device_info_);\n     auto emitter = GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis},\n-                                    &mlir_context_);\n+                                    &symbolic_expr_context_);\n     auto fusion = dynamic_cast<KernelFusionInterface*>(emitter.get());\n     EXPECT_NE(fusion, nullptr);\n \n-    CoalescingAnalysis coalescing_analysis =\n-        CoalescingAnalysis::Create(root, root->operands(), analysis,\n-                                   &mlir_context_, /*use_heuristic=*/false);\n+    CoalescingAnalysis coalescing_analysis = CoalescingAnalysis::Create(\n+        root, root->operands(), analysis, &symbolic_expr_context_,\n+        /*use_heuristic=*/false);\n \n     std::vector<bool> results;\n     for (const HloInstruction* operand : root->operands()) {\n@@ -87,6 +88,7 @@ class CoalescingTest : public HloHardwareIndependentTestBase {\n   stream_executor::DeviceDescription device_info_ =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(CoalescingTest, IdentityLayout) {\n@@ -588,7 +590,7 @@ class CoalescingForTiledHloTest : public CoalescingTest {\n \n     SymbolicTileAnalysis symbolic_tile_analysis =\n         std::get<SymbolicTileAnalysis>(SymbolicTileAnalysis::AnalyzeFusion(\n-            *fusion_adaptor, &mlir_context_));\n+            *fusion_adaptor, &symbolic_expr_context_));\n \n     TiledHloComputation tiled_hlo_computation =\n         *symbolic_tile_analysis.ComputeTiledHloInstructions(\n@@ -611,7 +613,7 @@ class CoalescingForTiledHloTest : public CoalescingTest {\n \n     SymbolicTileAnalysis symbolic_tile_analysis =\n         std::get<SymbolicTileAnalysis>(SymbolicTileAnalysis::AnalyzeFusion(\n-            *fusion_adaptor, &mlir_context_));\n+            *fusion_adaptor, &symbolic_expr_context_));\n \n     TiledHloComputation tiled_hlo_computation =\n         *symbolic_tile_analysis.ComputeTiledHloInstructions("
        },
        {
            "sha": "35f84b1b7bd943590a7b88c267f979c334d6bf8e",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -32,7 +32,8 @@ absl::StatusOr<bool> GpuCostModelStatsCollection::Run(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   // Scan all computations for fusion instructions.\n \n-  GpuPerformanceModelOwning gpu_performance_model{device_info_, mlir_context_};\n+  GpuPerformanceModelOwning gpu_performance_model{device_info_,\n+                                                  symbolic_expr_context_};\n   for (auto* computation : module->MakeComputationPostOrder()) {\n     TF_CHECK_OK(computation->Accept(&cost_analysis_));\n "
        },
        {
            "sha": "e6a80f6d03ed4f92f6601a8e18413e548553fb9a",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,10 +19,10 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -37,10 +37,10 @@ class GpuCostModelStatsCollection : public HloModulePass {\n   explicit GpuCostModelStatsCollection(\n       const se::DeviceDescription& d,\n       const GpuHloCostAnalysis::Options& cost_analysis_options,\n-      mlir::MLIRContext* mlir_context)\n+      SymbolicExprContext* symbolic_expr_context)\n       : device_info_(d),\n         cost_analysis_(cost_analysis_options, device_info_),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override {\n     return \"gpu_cost_model_stats_collection\";\n@@ -54,7 +54,7 @@ class GpuCostModelStatsCollection : public HloModulePass {\n  private:\n   se::DeviceDescription device_info_;\n   GpuHloCostAnalysis cost_analysis_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "70c0a3684af1a9dd050a3c19a4756c6e9b746e45",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -37,10 +38,11 @@ class GpuCostModelStatsCollectionTest : public HloHardwareIndependentTestBase {\n   GpuCostModelStatsCollection cost_model_stats_{\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n       GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true},\n-      &mlir_context_};\n+      &symbolic_expr_context_};\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(GpuCostModelStatsCollectionTest, FusinInEntryComputation) {"
        },
        {
            "sha": "bdafd9f22ccea0530af4cc46f9d471f8bf5ce1d3",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -310,15 +310,15 @@ GpuPerformanceModelWithIndexingAnalysis::EstimateRunTimeForFusion(\n   auto root_shape = roots.front().shape();\n \n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n+      EstimateFusionLaunchDimensions(fusion_analysis, symbolic_expr_context_);\n \n   int64_t num_blocks = launch_dimensions.num_blocks();\n \n   // Compute indexing from root to each instruction in the fusion and fusion\n   // operands. For each instruction, tells which elements of the instructions\n   // result will be used to compute one result element of the fusion.\n   auto grouped_fusion_indexing = ComputeGroupedOutputToInputIndexing(\n-      fusion_adaptor, roots[0], mlir_context_);\n+      fusion_adaptor, roots[0], symbolic_expr_context_);\n \n   int64_t flops = 0;\n   int64_t bytes_read = 0;\n@@ -571,7 +571,7 @@ GpuPerformanceModelWithIndexingAnalysis::EstimateRunTimeForTiledFusion(\n   // TODO(b/332714755): Add caching for SymbolicTileAnalysis.\n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          fusion_adaptor, mlir_context_,\n+          fusion_adaptor, symbolic_expr_context_,\n           /*emitter_specific_constraints_builder=*/nullptr);\n   if (const auto* fusion_decision =\n           std::get_if<FusionDecision>(&analysis_or_error)) {\n@@ -638,7 +638,7 @@ GpuPerformanceModelWithIndexingAnalysis::TryFindBestTilingForFusion(\n     const HloFusionAdaptor& fusion_adaptor) {\n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          fusion_adaptor, mlir_context_,\n+          fusion_adaptor, symbolic_expr_context_,\n           TritonEmitterConstraints::GetBuilder(*device_info_));\n \n   if (const auto* fusion_decision ="
        },
        {
            "sha": "bd1f9543411b80084f013c982ba19321b7db8f91",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model_base.h\"\n@@ -57,7 +58,7 @@ class GpuPerformanceModelWithIndexingAnalysis : public GpuPerformanceModelBase {\n       const se::DeviceDescription* device_info,\n       HloFusionAnalysisCache* fusion_analysis_cache,\n       HloCostAnalysis::ShapeSizeFunction shape_size,\n-      mlir::MLIRContext* mlir_context)\n+      SymbolicExprContext* symbolic_expr_context)\n       : hlo_op_profile_(&HloOpProfiles::Singleton().GetProfile(*device_info)),\n         device_info_(device_info),\n         fusion_analysis_cache_(fusion_analysis_cache),\n@@ -68,7 +69,7 @@ class GpuPerformanceModelWithIndexingAnalysis : public GpuPerformanceModelBase {\n                                         /*min_latencies_seconds=*/{},\n                                         /*count_multiple_input_accesses=*/true},\n             *device_info_),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   // Returns the launch dimensions for the given tiled HLO computation.\n   static LaunchDimensions GetLaunchDimensionsForTiledFusion(\n@@ -133,7 +134,7 @@ class GpuPerformanceModelWithIndexingAnalysis : public GpuPerformanceModelBase {\n   HloFusionAnalysisCache* fusion_analysis_cache_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_;\n   GpuHloCostAnalysis cost_analysis_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "36c34f3ebe1b540eb2242f356a74744994322d68",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model_base.h\"\n@@ -58,13 +59,14 @@ using ::tsl::testing::StatusIs;\n class GpuIndexingPerformanceModelTest : public HloHardwareIndependentTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   // The reference times in the test cases below are measured\n   // on A6000 by profiling the execution of the HLOs.\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   HloFusionAnalysisCache fusion_analysis_cache_{device_info_};\n   GpuPerformanceModelWithIndexingAnalysis indexing_cost_model_{\n       &device_info_, &fusion_analysis_cache_, HloCostAnalysis::DefaultShapeSize,\n-      &mlir_context_};\n+      &symbolic_expr_context_};\n \n   size_t WarpSize() const { return ::xla::gpu::WarpSize(device_info_); }\n };\n@@ -847,7 +849,7 @@ ENTRY main {\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          *fusion_adaptor, &mlir_context_,\n+          *fusion_adaptor, &symbolic_expr_context_,\n           /*emitter_specific_constraints_builder=*/nullptr);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n \n@@ -897,7 +899,7 @@ ENTRY main {\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          *fusion_adaptor, &mlir_context_,\n+          *fusion_adaptor, &symbolic_expr_context_,\n           /*emitter_specific_constraints_builder=*/nullptr);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n "
        },
        {
            "sha": "9906bf42d03383362919316c760088371e5b8dbd",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -33,6 +32,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/model/coalescing_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model_base.h\"\n@@ -48,11 +48,11 @@ GpuPerformanceModel::GpuPerformanceModel(\n     const se::DeviceDescription& device_info,\n     HloFusionAnalysisCache& fusion_analysis_cache,\n     GpuPerformanceModelCache& gpu_performance_model_cache,\n-    mlir::MLIRContext* mlir_context)\n+    SymbolicExprContext* symbolic_expr_context)\n     : device_info_(device_info),\n       fusion_analysis_cache_(fusion_analysis_cache),\n       gpu_performance_model_cache_(gpu_performance_model_cache),\n-      mlir_context_(mlir_context) {};\n+      symbolic_expr_context_(symbolic_expr_context) {};\n \n EstimateRunTimeData GpuPerformanceModel::EstimateRunTimeForInstructionImpl(\n     const HloInstruction* instr, const GpuHloCostAnalysis* cost_analysis) {\n@@ -63,7 +63,7 @@ EstimateRunTimeData GpuPerformanceModel::EstimateRunTimeForInstructionImpl(\n \n   const auto& fusion_analysis = fusion_analysis_cache_.Get(*instr);\n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n+      EstimateFusionLaunchDimensions(fusion_analysis, symbolic_expr_context_);\n   int64_t num_blocks = launch_dimensions.num_blocks();\n \n   absl::Duration compute_time =\n@@ -145,7 +145,7 @@ absl::Duration GpuPerformanceModel::EstimateRunTimeForFusionImpl(\n       fusion_analysis_cache_.Get(*producer, *consumer);\n \n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n+      EstimateFusionLaunchDimensions(fusion_analysis, symbolic_expr_context_);\n \n   int64_t flops = producer_runtime.flops * utilization_by_this_consumer +\n                   consumer_runtime.flops;"
        },
        {
            "sha": "706e10791edd860af8798a7c50d5ca0243390d54",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -20,8 +20,8 @@ limitations under the License.\n \n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model_base.h\"\n@@ -36,7 +36,7 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n   GpuPerformanceModel(const se::DeviceDescription& device_info,\n                       HloFusionAnalysisCache& fusion_analysis_cache,\n                       GpuPerformanceModelCache& gpu_performance_model_cache,\n-                      mlir::MLIRContext* mlir_context);\n+                      SymbolicExprContext* symbolic_expr_context);\n \n   EstimateRunTimeData EstimateRunTimeForInstruction(\n       const HloInstruction* instr, const GpuHloCostAnalysis* cost_analysis);\n@@ -78,7 +78,7 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n   // this is not possible because the cache is used directly by\n   // xla::gpu::PriorityFusionQueue\n   GpuPerformanceModelCache& gpu_performance_model_cache_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n // An owning wrapper around GpuPerformanceModel that also owns the caches.\n@@ -89,11 +89,11 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n class GpuPerformanceModelOwning {\n  public:\n   GpuPerformanceModelOwning(const se::DeviceDescription& device_info,\n-                            mlir::MLIRContext* mlir_context)\n+                            SymbolicExprContext* symbolic_expr_context)\n       : fusion_analysis_cache_(device_info),\n         gpu_performance_model_(std::make_unique<GpuPerformanceModel>(\n             device_info, fusion_analysis_cache_, gpu_performance_model_cache_,\n-            mlir_context)) {};\n+            symbolic_expr_context)) {};\n \n   GpuPerformanceModel& Get() const { return *gpu_performance_model_; }\n "
        },
        {
            "sha": "b4a748e5a319638c1703e976d0a0fa523834b0af",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/time.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion.h\"\n@@ -37,6 +36,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -143,9 +143,10 @@ void GpuPerformanceModelCache::Invalidate(const HloInstruction& instruction) {\n \n /*static*/\n LaunchDimensions GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n-    const HloFusionAnalysis& fusion_analysis, mlir::MLIRContext* ctx) {\n-  auto emitter =\n-      GetFusionEmitter(PreBufferAssignmentFusionInfo{fusion_analysis}, ctx);\n+    const HloFusionAnalysis& fusion_analysis,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  auto emitter = GetFusionEmitter(\n+      PreBufferAssignmentFusionInfo{fusion_analysis}, symbolic_expr_context);\n   if (const auto* kernel_emitter =\n           dynamic_cast<const KernelFusionInterface*>(emitter.get())) {\n     return kernel_emitter->launch_dimensions();"
        },
        {
            "sha": "83583e1bc88f46af1dcf83f0df8a140097b4a607",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -25,10 +25,10 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/time.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -155,7 +155,8 @@ class GpuPerformanceModelBase {\n   // Uses HloFusionAnalysis for computing the actual number of threads and\n   // blocks that the IR emitter will use.\n   static LaunchDimensions EstimateFusionLaunchDimensions(\n-      const HloFusionAnalysis& fusion_analysis, mlir::MLIRContext* ctx);\n+      const HloFusionAnalysis& fusion_analysis,\n+      SymbolicExprContext* symbolic_expr_context);\n \n   // Returns bytes accessed of operand output by instruction. Returns 0, if the\n   // operand is not used by the instruction."
        },
        {
            "sha": "e46de6cffe239150da3b7da136cffebc83e6f43a",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -44,6 +45,7 @@ class GpuPerformanceModelBaseTest : public HloHardwareIndependentTestBase {\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   std::unique_ptr<GpuHloCostAnalysis> analysis_;\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n \n   GpuPerformanceModelBaseTest() {\n     options_.count_multiple_input_accesses = true;\n@@ -240,8 +242,8 @@ ENTRY entry_computation {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n-                                                              &mlir_context_);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n+          fusion_analysis, &symbolic_expr_context_);\n \n   EXPECT_EQ(launch_dimensions.num_blocks(), 128);\n   EXPECT_EQ(launch_dimensions.num_threads_per_block(), 128);\n@@ -277,8 +279,8 @@ ENTRY e {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n-                                                              &mlir_context_);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n+          fusion_analysis, &symbolic_expr_context_);\n \n   EXPECT_EQ(launch_dimensions.num_blocks(), 16);\n   EXPECT_EQ(launch_dimensions.num_threads_per_block(), 64);\n@@ -307,8 +309,8 @@ ENTRY e {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n-                                                              &mlir_context_);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n+          fusion_analysis, &symbolic_expr_context_);\n \n   // CuNnnFusion doesn't implement KernelLaunchInsterface, so\n   // EstimateFusionLaunchDimensions returns a default estimate."
        },
        {
            "sha": "080a76895bfc5b69d047fca570c5ac845f03aacc",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/test_helpers.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_indexing_performance_model.h\"\n@@ -63,6 +64,7 @@ class GpuPerformanceModelTest : public HloHardwareIndependentTestBase {\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   GpuHloCostAnalysis::Options options_{.count_multiple_input_accesses = true};\n   // The reference times in the test cases below are measured\n   // on A6000 by profiling the execution of the HLOs.\n@@ -72,11 +74,11 @@ class GpuPerformanceModelTest : public HloHardwareIndependentTestBase {\n   GpuPerformanceModelCache gpu_performance_model_cache_;\n   GpuPerformanceModel gpu_performance_model_{\n       device_info_, fusion_analysis_cache_, gpu_performance_model_cache_,\n-      &mlir_context_};\n+      &symbolic_expr_context_};\n \n   GpuPerformanceModelWithIndexingAnalysis indexing_cost_model_{\n       &device_info_, &fusion_analysis_cache_, HloCostAnalysis::DefaultShapeSize,\n-      &mlir_context_};\n+      &symbolic_expr_context_};\n };\n \n TEST_F(GpuPerformanceModelTest, LargeWrite) {"
        },
        {
            "sha": "7c1757a0423b8f6d6a79bda9cda2fc0c39609831",
            "filename": "third_party/xla/xla/service/gpu/model/matmul_ptable_stats_collection.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_dot_fusion_cost_model.h\"\n #include \"xla/service/gpu/model/hlo_op_profile.pb.h\"\n #include \"xla/service/gpu/model/hlo_op_profiles.h\"\n@@ -93,8 +94,9 @@ HloDotInstruction* GetTritonGemmInstruction(const HloInstruction& dot_fusion) {\n absl::StatusOr<BlockLevelParameters> GetBlockLevelParams(\n     HloDotInstruction& dot, TritonGemmConfig& config) {\n   mlir::MLIRContext ctx;\n-  return ::xla::gpu::detail::FindBlockLevelParameters(&dot, config, &ctx,\n-                                                      se::DeviceDescription());\n+  SymbolicExprContext symbolic_expr_context(&ctx);\n+  return ::xla::gpu::detail::FindBlockLevelParameters(\n+      &dot, config, &symbolic_expr_context, se::DeviceDescription());\n }\n \n absl::Status SetReificationCost(HloInstruction& instr, absl::Duration exec_time,"
        },
        {
            "sha": "e713feb2c9757c0a9bd0e9ec3dc808e13726e241",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -104,8 +104,8 @@ absl::StatusOr<bool> SolGpuCostModelStatsCollection::Run(\n       SolLatencyEstimator::Create(\n           scheduler_config,\n           std::make_unique<GpuLatencyEstimator>(pointer_size_), device_info_,\n-          shape_size_in_bytes_fn_, module->entry_computation(), mlir_context_,\n-          std::move(cost_analysis)));\n+          shape_size_in_bytes_fn_, module->entry_computation(),\n+          symbolic_expr_context_, std::move(cost_analysis)));\n \n   for (HloComputation* comp : module->MakeComputationPostOrder()) {\n     for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {"
        },
        {
            "sha": "de512c38c9ed3f2513e2d7afa6163fdbd1cfa304",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,9 +19,9 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_verifier.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n@@ -32,11 +32,11 @@ class SolGpuCostModelStatsCollection : public HloModulePass {\n   explicit SolGpuCostModelStatsCollection(\n       const se::DeviceDescription& device_description,\n       ShapeSizeFn shape_size_in_bytes_fn, int pointer_size,\n-      mlir::MLIRContext* mlir_context)\n+      SymbolicExprContext* symbolic_expr_context)\n       : device_info_(device_description),\n         shape_size_in_bytes_fn_(shape_size_in_bytes_fn),\n         pointer_size_(pointer_size),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override {\n     return \"sol-gpu-cost-model-stats-collection\";\n@@ -52,7 +52,7 @@ class SolGpuCostModelStatsCollection : public HloModulePass {\n   se::DeviceDescription device_info_;\n   ShapeSizeFn shape_size_in_bytes_fn_;\n   int pointer_size_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "b9355f55128a8a94c92903b67e29a124ce7496f6",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n@@ -64,6 +65,7 @@ class SolGpuCostModelStatsCollectionTest\n   ShapeSizeFn shape_size_fn_;\n   int pointer_size_ = 8;\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(SolGpuCostModelStatsCollectionTest,\n@@ -87,10 +89,11 @@ TEST_F(SolGpuCostModelStatsCollectionTest,\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n \n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, SolGpuCostModelStatsCollection(\n-                                            device_info_, shape_size_fn_,\n-                                            pointer_size_, &mlir_context_)\n-                                            .Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      bool changed,\n+      SolGpuCostModelStatsCollection(device_info_, shape_size_fn_,\n+                                     pointer_size_, &symbolic_expr_context_)\n+          .Run(module.get()));\n \n   VLOG(1) << module->ToString();\n "
        },
        {
            "sha": "ea443a520d2e0b8dabcce87510cb5c1d87db79aa",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/time/time.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -37,6 +36,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/flag_utils.h\"\n #include \"xla/service/gpu/model/collective_interpolator.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n #include \"xla/service/gpu/model/gpu_performance_model_base.h\"\n@@ -106,7 +106,8 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n     int num_participating_hosts, int num_communicators,\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     const SolGPUCostModel::Config& sol_flags,\n-    const GpuHloCostAnalysis& analysis, mlir::MLIRContext* mlir_context) {\n+    const GpuHloCostAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context) {\n   SolGPUCostModel sol_model(sol_flags);\n   const int64_t msg_size = analysis.BytesTransferred(instr);\n \n@@ -115,7 +116,7 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n   absl::Duration result = absl::Seconds(1.0f * analysis.bytes_accessed(instr) /\n                                         gpu_device_info.memory_bandwidth());\n   GpuPerformanceModelOwning gpu_performance_model{gpu_device_info,\n-                                                  mlir_context};\n+                                                  symbolic_expr_context};\n   switch (instr.opcode()) {\n     case HloOpcode::kAllGather:\n     case HloOpcode::kAllGatherStart: {\n@@ -195,7 +196,7 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n     const SolGPUCostModel::Config& sol_flags,\n     const GpuHloCostAnalysis& analysis,\n     const CollectiveInterpolator* collective_interpolator,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   TF_RETURN_IF_ERROR(communication_type.status());\n \n   GPUCommunicationType comm = *communication_type;\n@@ -207,13 +208,13 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second / sol_flags.gpus_per_node,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n-          gpu_device_info, sol_flags, analysis, mlir_context);\n+          gpu_device_info, sol_flags, analysis, symbolic_expr_context);\n     }\n     case GPUCommunicationType::NON_RAIL_ALIGNED: {\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n-          gpu_device_info, sol_flags, analysis, mlir_context);\n+          gpu_device_info, sol_flags, analysis, symbolic_expr_context);\n     }\n     case GPUCommunicationType::SINGLE_HOST: {\n       if (collective_interpolator == nullptr) {\n@@ -265,7 +266,8 @@ absl::StatusOr<std::unique_ptr<MatmulInterpolator>> CreateMatmulInterpolator(\n SolLatencyEstimator::ComputeCollectiveTime(\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-    const SolGPUCostModel::Config& sol_flags, mlir::MLIRContext* mlir_context,\n+    const SolGPUCostModel::Config& sol_flags,\n+    SymbolicExprContext* symbolic_expr_context,\n     const CollectiveInterpolator* collective_interpolator) {\n   GpuHloCostAnalysis analysis(\n       GpuHloCostAnalysis::Options{shape_size_fn,\n@@ -281,16 +283,17 @@ SolLatencyEstimator::ComputeCollectiveTime(\n   }\n \n   return SolLatencyEstimator::ComputeCollectiveTime(\n-      instr, gpu_device_info, shape_size_fn, sol_flags, analysis, mlir_context,\n-      collective_interpolator);\n+      instr, gpu_device_info, shape_size_fn, sol_flags, analysis,\n+      symbolic_expr_context, collective_interpolator);\n }\n \n /*static*/ absl::StatusOr<absl::Duration>\n SolLatencyEstimator::ComputeCollectiveTime(\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n     const SolGPUCostModel::Config& sol_flags,\n-    const GpuHloCostAnalysis& analysis, mlir::MLIRContext* mlir_context,\n+    const GpuHloCostAnalysis& analysis,\n+    SymbolicExprContext* symbolic_expr_context,\n     const CollectiveInterpolator* collective_interpolator) {\n   if (HloDataflowAnalysis::IsAsynchronousOperationDone(instr.opcode())) {\n     VLOG(8) << \"Returning 0 cost for async done op \" << instr.name();\n@@ -314,7 +317,7 @@ SolLatencyEstimator::ComputeCollectiveTime(\n       absl::Duration result,\n       DispatchEstimation(communication_type, *collective_instr, gpu_device_info,\n                          sol_flags, analysis, collective_interpolator,\n-                         mlir_context));\n+                         symbolic_expr_context));\n   return result;\n }\n \n@@ -324,7 +327,8 @@ SolLatencyEstimator::Create(\n     std::unique_ptr<LatencyEstimator> latency_estimator,\n     const se::DeviceDescription& gpu_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-    const HloComputation* computation, mlir::MLIRContext* mlir_context,\n+    const HloComputation* computation,\n+    SymbolicExprContext* symbolic_expr_context,\n     std::unique_ptr<GpuHloCostAnalysis> cost_analysis) {\n   if (cost_analysis == nullptr) {\n     cost_analysis =\n@@ -349,7 +353,7 @@ SolLatencyEstimator::Create(\n   return std::unique_ptr<SolLatencyEstimator>(new SolLatencyEstimator(\n       config, std::move(latency_estimator), gpu_info, std::move(cost_analysis),\n       shape_size_function, sol_config, std::move(collective_interpolator),\n-      std::move(matmul_interpolator), mlir_context));\n+      std::move(matmul_interpolator), symbolic_expr_context));\n }\n \n /*static*/ bool SolLatencyEstimator::IsSupportedForModule(\n@@ -395,7 +399,7 @@ LatencyEstimator::TimeCost SolLatencyEstimator::GetLatencyBetween(\n \n   absl::StatusOr<absl::Duration> coll_time = ComputeCollectiveTime(\n       from.GetInstr(), gpu_info_, shape_size_function_, sol_flags_,\n-      *cost_analysis_, mlir_context_, collective_interpolator_.get());\n+      *cost_analysis_, symbolic_expr_context_, collective_interpolator_.get());\n   if (!coll_time.ok()) {\n     VLOG(1) << \"Failed to compute collective time: \" << coll_time.status()\n             << \" for \" << from.GetInstr().name();\n@@ -460,17 +464,17 @@ SolLatencyEstimator::SolLatencyEstimator(\n     const SolGPUCostModel::Config sol_flags,\n     std::unique_ptr<CollectiveInterpolator> collective_interpolator,\n     std::unique_ptr<MatmulInterpolator> matmul_interpolator,\n-    mlir::MLIRContext* mlir_context)\n+    SymbolicExprContext* symbolic_expr_context)\n     : config_(config),\n       gpu_info_(gpu_info),\n-      gpu_performance_model_(gpu_info, mlir_context),\n+      gpu_performance_model_(gpu_info, symbolic_expr_context),\n       cost_analysis_(std::move(cost_analysis)),\n       latency_estimator_(std::move(latency_estimator)),\n       shape_size_function_(shape_size_function),\n       sol_flags_(sol_flags),\n       collective_interpolator_(std::move(collective_interpolator)),\n       matmul_interpolator_(std::move(matmul_interpolator)),\n-      mlir_context_(mlir_context) {}\n+      symbolic_expr_context_(symbolic_expr_context) {}\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "7f728eb3223683e6198d540275773c6b9ade8643",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.h",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -21,10 +21,10 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/time/time.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/model/collective_interpolator.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n #include \"xla/service/gpu/model/matmul_interpolator.h\"\n@@ -66,7 +66,8 @@ class SolLatencyEstimator : public LatencyEstimator {\n   static absl::StatusOr<absl::Duration> ComputeCollectiveTime(\n       const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-      const SolGPUCostModel::Config& sol_flags, mlir::MLIRContext* mlir_context,\n+      const SolGPUCostModel::Config& sol_flags,\n+      SymbolicExprContext* symbolic_expr_context,\n       const CollectiveInterpolator* collective_interpolator = nullptr);\n \n   // Computes the time it takes to execute the given collective instruction.\n@@ -78,7 +79,8 @@ class SolLatencyEstimator : public LatencyEstimator {\n       const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n       const SolGPUCostModel::Config& sol_flags,\n-      const GpuHloCostAnalysis& cost_analysis, mlir::MLIRContext* mlir_context,\n+      const GpuHloCostAnalysis& cost_analysis,\n+      SymbolicExprContext* symbolic_expr_context,\n       const CollectiveInterpolator* collective_interpolator = nullptr);\n \n   // Factory method to create a `SolLatencyEstimator`.\n@@ -87,7 +89,8 @@ class SolLatencyEstimator : public LatencyEstimator {\n       std::unique_ptr<LatencyEstimator> latency_estimator,\n       const se::DeviceDescription& gpu_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      const HloComputation* computation, mlir::MLIRContext* mlir_context,\n+      const HloComputation* computation,\n+      SymbolicExprContext* symbolic_expr_context,\n       std::unique_ptr<GpuHloCostAnalysis> cost_analysis = nullptr);\n \n   // Returns true if the module is supported by the SoL latency estimator.\n@@ -109,7 +112,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n       SolGPUCostModel::Config sol_flags,\n       std::unique_ptr<CollectiveInterpolator> collective_interpolator,\n       std::unique_ptr<MatmulInterpolator> matmul_interpolator,\n-      mlir::MLIRContext* mlir_context);\n+      SymbolicExprContext* symbolic_expr_context);\n \n   const SchedulerConfig config_;\n   const se::DeviceDescription& gpu_info_;\n@@ -120,7 +123,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n   const SolGPUCostModel::Config sol_flags_;\n   const std::unique_ptr<const CollectiveInterpolator> collective_interpolator_;\n   const std::unique_ptr<const MatmulInterpolator> matmul_interpolator_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "3cff750694af1557440e2f3de29c11fe47112ef7",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/literal_util.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/collective_interpolator.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/sol_gpu_cost_model.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -96,16 +97,17 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n   absl::StatusOr<absl::Duration> ComputeCollectiveTime(\n       const HloInstruction& instr) {\n     return SolLatencyEstimator::ComputeCollectiveTime(\n-        instr, gpu_device_info_, shape_size_fn_, sol_flags_, &mlir_context_,\n-        collective_interpolator_.get());\n+        instr, gpu_device_info_, shape_size_fn_, sol_flags_,\n+        &symbolic_expr_context_, collective_interpolator_.get());\n   }\n \n   absl::Duration ComputeNodeCost(const HloInstruction& instr,\n                                  const HloComputation* computation) {\n     std::unique_ptr<SolLatencyEstimator> estimator =\n-        *SolLatencyEstimator::Create(\n-            scheduler_config_, std::make_unique<DummyLatencyEstimator>(),\n-            gpu_device_info_, shape_size_fn_, computation, &mlir_context_);\n+        *SolLatencyEstimator::Create(scheduler_config_,\n+                                     std::make_unique<DummyLatencyEstimator>(),\n+                                     gpu_device_info_, shape_size_fn_,\n+                                     computation, &symbolic_expr_context_);\n     LatencyEstimator::TimeCost cost_val = estimator->NodeCost(&instr);\n     return absl::Microseconds(static_cast<int64_t>(cost_val));\n   }\n@@ -114,9 +116,10 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n                                    const HloGraphNode& target,\n                                    const HloComputation* computation) {\n     std::unique_ptr<SolLatencyEstimator> estimator =\n-        *SolLatencyEstimator::Create(\n-            scheduler_config_, std::make_unique<DummyLatencyEstimator>(),\n-            gpu_device_info_, shape_size_fn_, computation, &mlir_context_);\n+        *SolLatencyEstimator::Create(scheduler_config_,\n+                                     std::make_unique<DummyLatencyEstimator>(),\n+                                     gpu_device_info_, shape_size_fn_,\n+                                     computation, &symbolic_expr_context_);\n     LatencyEstimator::TimeCost cost_val =\n         estimator->GetLatencyBetween(from, target);\n     return absl::Microseconds(static_cast<int64_t>(cost_val));\n@@ -128,6 +131,7 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n   SchedulerConfig scheduler_config_;\n   std::unique_ptr<CollectiveInterpolator> collective_interpolator_;\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_P(SolLatencyEstimatorTest, TestLatencyEstimation) {"
        },
        {
            "sha": "4ed8705e16777a02d5cbb494e007e241c7faa313",
            "filename": "third_party/xla/xla/service/gpu/model/triton_emitter_constraints.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/util.h\"\n@@ -118,8 +119,12 @@ TritonEmitterConstraints::DeriveCustomConstraints(\n         hlo->opcode() == HloOpcode::kBitcast) {\n       MLIRContext* ctx = instruction->symbolic_tile().size_map().getContext();\n \n+      // TODO(b/446856820): Remove this once we use SymbolicMap here and\n+      // therefore we can get the context directly.\n+      SymbolicExprContext symbolic_expr_context{ctx};\n       IndexingMap reshape_indexing_map =\n-          ComputeOutputToInputIndexing(hlo, /*output_id=*/0, ctx)\n+          ComputeOutputToInputIndexing(hlo, /*output_id=*/0,\n+                                       &symbolic_expr_context)\n               .indexing_maps[0]\n               .begin()\n               ->map();"
        },
        {
            "sha": "0258a77783328eb2bc1a28ee30ca1c2c07c91e62",
            "filename": "third_party/xla/xla/service/gpu/model/triton_emitter_constraints_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -58,7 +59,7 @@ class TritonEmitterConstraintsTest : public HloHardwareIndependentTestBase {\n             *module->entry_computation()\n                  ->root_instruction()\n                  ->fused_instructions_computation(),\n-            &mlir_context_, constraints_builder);\n+            &symbolic_expr_context_, constraints_builder);\n \n     if (std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error)) {\n       return std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n@@ -69,6 +70,7 @@ class TritonEmitterConstraintsTest : public HloHardwareIndependentTestBase {\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   se::DeviceDescription device_description_ =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n };"
        },
        {
            "sha": "55506d71d9f1c0109c73815280881d7ec27f513a",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -390,7 +390,7 @@ absl::Status NVPTXCompiler::AddGemmFusionAutotuningPasses(\n     se::StreamExecutor* stream_executor) {\n   pipeline->AddPass<GemmFusionAutotuner>(autotune_config, toolkit_version,\n                                          thread_pool, key_value_store,\n-                                         mlir_context());\n+                                         symbolic_expr_context());\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "40a97153031c4b2defa579e82e2f46f4bf89d7ab",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/gpu_hlo_schedule.h\"\n #include \"xla/service/gpu/gpu_latency_hiding_scheduler.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/logical_buffer.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n@@ -75,7 +76,8 @@ class NVPTXCompilerTest : public HloTestBase {\n     std::unique_ptr<GpuAliasInfo> alias_info =\n         compiler.GetAliasInfo(gpu_device_info);\n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, pointer_size, gpu_device_info,\n-                                         &mlir_context_, alias_info.get())\n+                                         &symbolic_expr_context_,\n+                                         alias_info.get())\n                            .status());\n \n     auto buffer_size_bytes_function =\n@@ -92,6 +94,7 @@ class NVPTXCompilerTest : public HloTestBase {\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n class NVPTXCompilerTestTriton : public NVPTXCompilerTest {"
        },
        {
            "sha": "59fc4da3cbc504f34264210907256060994dd08f",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -206,6 +206,7 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu/model:fusion_analysis_cache\",\n         \"//xla/service/gpu/model:gpu_indexing_performance_model\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -216,7 +217,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -234,6 +234,7 @@ xla_cc_test(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1925,6 +1926,7 @@ cc_library(\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n         \"//xla/service/gpu/model:gpu_performance_model\",\n         \"//xla/service/gpu/model:gpu_performance_model_base\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:status\",\n@@ -1936,7 +1938,6 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -1958,6 +1959,7 @@ xla_cc_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:gpu_fusible\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:IR\",\n@@ -1989,6 +1991,7 @@ cc_library(\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/model:triton_emitter_constraints\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tools:hlo_extractor\",\n@@ -2005,7 +2008,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2025,6 +2027,7 @@ xla_cc_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/lib/core:status_test_util\",\n@@ -2068,6 +2071,7 @@ cc_library(\n         \"//xla/service/gpu/model:gpu_performance_model\",\n         \"//xla/service/gpu/model:gpu_performance_model_base\",\n         \"//xla/service/gpu/model:triton_emitter_constraints\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n@@ -2086,7 +2090,6 @@ cc_library(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2107,6 +2110,7 @@ xla_cc_test(\n         \"//xla/service/gpu:gpu_fusible\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:statusor\",\n@@ -2556,7 +2560,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n@@ -2573,6 +2576,7 @@ cc_library(\n         \"//xla/service/gpu/model:gpu_performance_model\",\n         \"//xla/service/gpu/model:gpu_performance_model_base\",\n         \"//xla/service/gpu/model:triton_emitter_constraints\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tsl/platform:errors\",\n@@ -2605,6 +2609,7 @@ xla_cc_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:errors\",\n@@ -2986,6 +2991,7 @@ cc_library(\n         \"//xla/service/gpu/autotuning:autotuner_compile_util\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n         \"//xla/service/gpu/autotuning:redzone_buffers\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tools:hlo_decomposer_lib\",\n@@ -2999,7 +3005,6 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -3023,6 +3028,7 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu/autotuning:autotuner_compile_util\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/tests:hlo_pjrt_test_base\",\n         \"//xla/tsl/lib/core:status_test_util\","
        },
        {
            "sha": "97bba64a5f1248a864ca57388456636e2b3f9c75",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -307,6 +307,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service/gpu:alias_info\",\n         \"//xla/service/gpu:gpu_hlo_schedule\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -316,7 +317,6 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -329,6 +329,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:alias_info\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "c957ac78a64b137b6e7f5447cf374f3f4c84992c",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,14 +27,14 @@ limitations under the License.\n #include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/gpu_hlo_schedule.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/gpu/transforms/collectives/convert_async_collectives_to_sync.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -97,14 +97,16 @@ struct Metadata {\n //   synchronous post scheduling.\n absl::StatusOr<Metadata> GetSchedulingMetadata(\n     const HloModule& module, int64_t pointer_size,\n-    const se::DeviceDescription& device_info, mlir::MLIRContext* mlir_context,\n+    const se::DeviceDescription& device_info,\n+    SymbolicExprContext* symbolic_expr_context,\n     const GpuAliasInfo* alias_info) {\n   std::unique_ptr<HloModule> cloned_module = module.Clone();\n   AnnotateCollectives(cloned_module.get());\n   TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(cloned_module.get()));\n-  TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n-                      ScheduleGpuModule(cloned_module.get(), pointer_size,\n-                                        device_info, mlir_context, alias_info));\n+  TF_ASSIGN_OR_RETURN(\n+      ScheduleMetadata schedule_metadata,\n+      ScheduleGpuModule(cloned_module.get(), pointer_size, device_info,\n+                        symbolic_expr_context, alias_info));\n   TF_RETURN_IF_ERROR(AnnotateSyncCollectives(cloned_module.get()));\n   return Metadata{schedule_metadata.peak_memory_usage,\n                   SyncCollectiveIds(*cloned_module)};\n@@ -127,8 +129,8 @@ absl::StatusOr<bool> CollectiveCombinerAnnotator::Run(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   TF_ASSIGN_OR_RETURN(\n       Metadata metadata,\n-      GetSchedulingMetadata(*module, pointer_size_, device_info_, mlir_context_,\n-                            alias_info_));\n+      GetSchedulingMetadata(*module, pointer_size_, device_info_,\n+                            symbolic_expr_context_, alias_info_));\n   int64_t combiner_threshold =\n       MaxAvailableMemory(*module, device_info_) - metadata.peak_memory_bytes;\n   if (combiner_threshold <= 0) {"
        },
        {
            "sha": "135eee6c8c2bdc80f6224d629e0c409004867582",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -23,11 +23,11 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/gpu/alias_info.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla::gpu {\n@@ -39,11 +39,11 @@ class CollectiveCombinerAnnotator : public HloModulePass {\n   CollectiveCombinerAnnotator(se::DeviceDescription device_info,\n                               const GpuAliasInfo* alias_info,\n                               int64_t pointer_size,\n-                              mlir::MLIRContext* mlir_context)\n+                              SymbolicExprContext* symbolic_expr_context)\n       : device_info_(std::move(device_info)),\n         alias_info_(alias_info),\n         pointer_size_(pointer_size),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::StatusOr<bool> Run(\n       HloModule* module,\n@@ -57,7 +57,7 @@ class CollectiveCombinerAnnotator : public HloModulePass {\n   const se::DeviceDescription device_info_;\n   const GpuAliasInfo* alias_info_;\n   const int64_t pointer_size_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n // Returns true if `instr` is a combinable sync collective. False otherwise."
        },
        {
            "sha": "0305462952cd1dc69ffb4ccac7ab5bdf41aa19a8",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/alias_info.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n@@ -48,10 +49,11 @@ class CollectiveCombinerAnnotatorTest : public HloHardwareIndependentTestBase {\n     GpuAliasInfo alias_info(device_info);\n     return RunHloPass(\n         CollectiveCombinerAnnotator(std::move(device_info), &alias_info,\n-                                    pointer_size, &mlir_context_),\n+                                    pointer_size, &symbolic_expr_context_),\n         module);\n   }\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(CollectiveCombinerAnnotatorTest, SynchronousCollectivesNoOverlap) {"
        },
        {
            "sha": "3b7f9c229997e546dc0bb2dbc9a97932d42964d1",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/Support/MathExtras.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -40,6 +39,7 @@ limitations under the License.\n #include \"xla/layout_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_indexing_performance_model.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -56,7 +56,6 @@ namespace gpu {\n \n namespace {\n \n-using ::mlir::MLIRContext;\n namespace m = ::xla::match;\n \n // Pattern-matches slow loop fusions that can likely be handled better by\n@@ -164,7 +163,8 @@ absl::StatusOr<bool> ShouldTryRewriteFusion(\n absl::StatusOr<bool> ProcessFusionInstruction(\n     HloFusionInstruction* fusion_instruction,\n     const se::DeviceDescription& device_info,\n-    HloCostAnalysis::ShapeSizeFunction shape_size, MLIRContext* ctx) {\n+    HloCostAnalysis::ShapeSizeFunction shape_size,\n+    SymbolicExprContext* symbolic_expr_context) {\n   TF_ASSIGN_OR_RETURN(bool should_try_rewrite,\n                       ShouldTryRewriteFusion(fusion_instruction, device_info));\n   if (!should_try_rewrite) {\n@@ -195,7 +195,7 @@ absl::StatusOr<bool> ProcessFusionInstruction(\n \n   HloFusionAnalysisCache fusion_analysis_cache(device_info);\n   GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n-      &device_info, &fusion_analysis_cache, shape_size, ctx);\n+      &device_info, &fusion_analysis_cache, shape_size, symbolic_expr_context);\n \n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(\n       Cast<HloFusionInstruction>(fusion_instruction));\n@@ -252,9 +252,9 @@ absl::StatusOr<bool> FusionBlockLevelRewriter::Run(\n     }\n     HloFusionInstruction* fusion_instruction =\n         ::xla::Cast<HloFusionInstruction>(computation->FusionInstruction());\n-    TF_ASSIGN_OR_RETURN(\n-        bool changed, ProcessFusionInstruction(fusion_instruction, device_info_,\n-                                               shape_size_, mlir_context_));\n+    TF_ASSIGN_OR_RETURN(bool changed, ProcessFusionInstruction(\n+                                          fusion_instruction, device_info_,\n+                                          shape_size_, symbolic_expr_context_));\n \n     has_changed |= changed;\n   }"
        },
        {
            "sha": "5397f8e0ffc3763947bb16d1d4996e686ef3a0fa",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,9 +19,9 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n@@ -33,10 +33,10 @@ class FusionBlockLevelRewriter : public HloModulePass {\n   explicit FusionBlockLevelRewriter(\n       const se::DeviceDescription& device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size,\n-      mlir::MLIRContext* mlir_context)\n+      SymbolicExprContext* symbolic_expr_context)\n       : device_info_(device_info),\n         shape_size_(shape_size),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override {\n     return \"fusion-block-level-rewriter\";\n@@ -50,7 +50,7 @@ class FusionBlockLevelRewriter : public HloModulePass {\n  private:\n   const se::DeviceDescription& device_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "7686b7170cd340e6b6969fc8789df66559ac84fb",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -34,6 +34,7 @@ License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -69,6 +70,7 @@ class FusionBlockLevelRewriterTest : public HloHardwareIndependentTestBase {\n     return debug_options;\n   }\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(FusionBlockLevelRewriterTest,\n@@ -89,7 +91,7 @@ ENTRY entry {\n                           ParseAndReturnVerifiedModule(hlo_text));\n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &mlir_context_)\n+                               &symbolic_expr_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(false));\n }\n@@ -111,7 +113,7 @@ ENTRY entry {\n \n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &mlir_context_)\n+                               &symbolic_expr_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(true));\n   const HloInstruction* root = module->entry_computation()->root_instruction();\n@@ -135,14 +137,14 @@ ENTRY entry {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(hlo_text));\n-  mlir::MLIRContext ctx;\n \n   ASSERT_FALSE(std::holds_alternative<SymbolicTileAnalysis>(\n       SymbolicTileAnalysis::AnalyzeComputation(\n-          *module->GetComputationWithName(\"fusion_computation\"), &ctx)));\n+          *module->GetComputationWithName(\"fusion_computation\"),\n+          &symbolic_expr_context_)));\n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &mlir_context_)\n+                               &symbolic_expr_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(false));\n }\n@@ -167,7 +169,7 @@ ENTRY entry {\n       device_info_.gpu_compute_capability()));\n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &mlir_context_)\n+                               &symbolic_expr_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(false));\n }\n@@ -203,7 +205,7 @@ ENTRY entry  {\n   se::DeviceDescription device_info{TestGpuDeviceInfo::RTXA6000DeviceInfo(\n       se::CudaComputeCapability::Ampere())};\n   FusionBlockLevelRewriter rewriter(\n-      device_info, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n+      device_info, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_);\n   EXPECT_THAT(rewriter.Run(module.get()), absl_testing::IsOkAndHolds(true));\n   const HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_EQ(root->opcode(), HloOpcode::kFusion);"
        },
        {
            "sha": "283d3c9f31ed772a9b45e43dbd4b7936e8316a69",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -431,7 +431,8 @@ absl::StatusOr<bool> MultiOutputFusion::DoMultiOutputFusion() {\n       computation_->MakeInstructionPostOrder();\n \n   FusionInfoCache fusion_info_cache(device_info_);\n-  GpuPerformanceModelOwning gpu_performance_model(device_info_, mlir_context_);\n+  GpuPerformanceModelOwning gpu_performance_model(device_info_,\n+                                                  symbolic_expr_context_);\n   // Traverse the HLO in uses-before-defs order.\n   for (auto it = defs_before_uses.rbegin(); it != defs_before_uses.rend();\n        ++it) {"
        },
        {
            "sha": "88d5d4c2295769cb623de84f9c9f3e27e83f50d6",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -21,13 +21,13 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/hlo_dfs_reachability.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -98,10 +98,10 @@ class MultiOutputFusion : public HloModulePass {\n   explicit MultiOutputFusion(\n       const se::DeviceDescription& device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      mlir::MLIRContext* mlir_context)\n+      SymbolicExprContext* symbolic_expr_context)\n       : device_info_(device_info),\n         shape_size_function_(shape_size_function),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override { return \"multi_output_fusion\"; }\n \n@@ -130,7 +130,7 @@ class MultiOutputFusion : public HloModulePass {\n \n   se::DeviceDescription device_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_function_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "8d2db3da7cd3f4b805bc96bbc485384c0dd4a44a",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/shape.h\"\n@@ -47,19 +48,22 @@ namespace m = ::xla::match;\n class MultiOutputFusionTest : public HloHardwareIndependentTestBase {\n  public:\n   MultiOutputFusion mof_{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n+                         HloCostAnalysis::DefaultShapeSize,\n+                         &symbolic_expr_context_};\n \n   void CheckMultiOutputFusion(absl::string_view hlo,\n                               std::optional<absl::string_view> expected) {\n     RunAndFilecheckHloRewrite(\n         hlo,\n         MultiOutputFusion{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                          HloCostAnalysis::DefaultShapeSize, &mlir_context_},\n+                          HloCostAnalysis::DefaultShapeSize,\n+                          &symbolic_expr_context_},\n         expected);\n   }\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n const char kModulePrefix[] = R\"("
        },
        {
            "sha": "e50f57623f2e3984cb170a535662403d84a6b5d9",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -40,7 +40,6 @@ limitations under the License.\n #include \"llvm/ADT/SetVector.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/iterator_range.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/codegen/tiling/symbolic_tile.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n@@ -59,6 +58,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/triton_emitter_constraints.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/matmul_indexing_utils.h\"\n@@ -266,7 +266,7 @@ absl::Status FuseAndAnnotateConcatOperands(HloComputation* computation) {\n // Transforms a fusion into an equivalent nested fusion if it has a single dot.\n // Returns ok if the transformation was successful.\n absl::Status MakeNestedFusionFromGemmFusion(\n-    HloFusionInstruction* fusion, HloInstruction* dot, mlir::MLIRContext* ctx,\n+    HloFusionInstruction* fusion, HloInstruction* dot, SymbolicExprContext* ctx,\n     const se::DeviceDescription& device_description) {\n   TF_RETURN_IF_ERROR(IsDot(*dot));\n   const bool is_scaled_dot = dot->opcode() == HloOpcode::kScaledDot;\n@@ -1113,9 +1113,9 @@ bool IsFeatureEnabled(const HloModule* module,\n class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n  public:\n   explicit NestGemmFusionVisitor(\n-      mlir::MLIRContext* ctx, CallGraph* call_graph,\n+      SymbolicExprContext* symbolic_expr_context, CallGraph* call_graph,\n       const se::DeviceDescription& device_description)\n-      : ctx_(ctx),\n+      : symbolic_expr_context_(symbolic_expr_context),\n         call_graph_(call_graph),\n         device_description_(device_description) {}\n \n@@ -1243,8 +1243,8 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n \n     TF_RETURN_IF_ERROR(\n         TryHoistBitcastsInComputationToCallers(instr, call_graph));\n-    TF_RETURN_IF_ERROR(MakeNestedFusionFromGemmFusion(fusion, instr, ctx_,\n-                                                      device_description_));\n+    TF_RETURN_IF_ERROR(MakeNestedFusionFromGemmFusion(\n+        fusion, instr, symbolic_expr_context_, device_description_));\n \n     MarkAsChanged();\n     bool scaled_dot_enabled =\n@@ -1323,7 +1323,7 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n   }\n \n  private:\n-  mlir::MLIRContext* ctx_;\n+  SymbolicExprContext* symbolic_expr_context_;\n   CallGraph* call_graph_;\n   const se::DeviceDescription& device_description_;\n };\n@@ -1337,7 +1337,7 @@ absl::StatusOr<bool> NestGemmFusion::RunOnModule(\n   auto call_graph = CallGraph::Build(module, execution_threads);\n   for (HloComputation* computation :\n        module->MakeNonfusionComputations(execution_threads)) {\n-    NestGemmFusionVisitor visitor(mlir_context_, call_graph.get(),\n+    NestGemmFusionVisitor visitor(symbolic_expr_context_, call_graph.get(),\n                                   device_description_);\n     TF_RETURN_IF_ERROR(computation->Accept(&visitor));\n     changed |= visitor.changed();\n@@ -1367,8 +1367,8 @@ absl::StatusOr<bool> NestGemmFusion::Run(\n namespace detail {\n \n absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n-    HloInstruction* dot, const TritonGemmConfig& config, mlir::MLIRContext* ctx,\n-    const se::DeviceDescription& device_description) {\n+    HloInstruction* dot, const TritonGemmConfig& config,\n+    SymbolicExprContext* ctx, const se::DeviceDescription& device_description) {\n   TF_RETURN_IF_ERROR(IsDot(*dot));\n   HloComputation* computation = dot->parent();\n   VLOG(3) << \"FindOutputTileSizesForEpilogue of computation: \""
        },
        {
            "sha": "505eebb2f89c063e85673611bb13c0e3635dbd36",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.h",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -19,12 +19,12 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla::gpu {\n@@ -47,8 +47,9 @@ namespace xla::gpu {\n class NestGemmFusion : public HloModulePass {\n  public:\n   explicit NestGemmFusion(const se::DeviceDescription& device_description,\n-                          mlir::MLIRContext* mlir_context)\n-      : device_description_(device_description), mlir_context_(mlir_context) {}\n+                          SymbolicExprContext* symbolic_expr_context)\n+      : device_description_(device_description),\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override { return \"nest_gemm_fusion\"; }\n \n@@ -59,7 +60,7 @@ class NestGemmFusion : public HloModulePass {\n \n  private:\n   const se::DeviceDescription device_description_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n   absl::StatusOr<bool> RunOnModule(\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads);\n@@ -77,7 +78,8 @@ namespace detail {\n // function can be removed once `GpuDotFusionCostModel::EstimateRunTimeForDotOp`\n // is implemented.\n absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n-    HloInstruction* dot, const TritonGemmConfig& config, mlir::MLIRContext* ctx,\n+    HloInstruction* dot, const TritonGemmConfig& config,\n+    SymbolicExprContext* symbolic_expr_context,\n     const se::DeviceDescription& device_description);\n \n }  // namespace detail"
        },
        {
            "sha": "4e08e135987a8c8f44d00bcf5a016e24c5a6d19e",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 96,
            "deletions": 94,
            "changes": 190,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -76,6 +77,7 @@ class NestGemmFusionTest : public HloHardwareIndependentTestBase {\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(\n           se::CudaComputeCapability::Ampere())};\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n \n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions options =\n@@ -115,9 +117,9 @@ ENTRY entry {\n })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n-  ASSERT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -174,9 +176,9 @@ ENTRY e {\n                          \"num_ctas\":1}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   HloComputation* fusion_computation = module->entry_computation()\n                                            ->root_instruction()\n@@ -238,8 +240,8 @@ ENTRY e {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n   TF_ASSERT_OK_AND_ASSIGN(\n-      bool updated,\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()));\n+      bool updated, NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                        .Run(module.get()));\n   EXPECT_TRUE(updated);\n   HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_EQ(root->opcode(), HloOpcode::kTuple);\n@@ -292,9 +294,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  ASSERT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -339,9 +341,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  ASSERT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -386,9 +388,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -421,9 +423,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -455,9 +457,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK: f16[3,11]{1,0} convert(\n@@ -501,9 +503,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -537,9 +539,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n   CHECK: ENTRY\n@@ -581,9 +583,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -620,9 +622,9 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -656,9 +658,9 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -691,9 +693,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -723,9 +725,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -769,7 +771,7 @@ ENTRY e {\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n   // We can nest the fusion including the broadcast.\n-  EXPECT_TRUE(NestGemmFusion(device_description_, &mlir_context_)\n+  EXPECT_TRUE(NestGemmFusion(device_description_, &symbolic_expr_context_)\n                   .Run(module.get())\n                   .ok());\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n@@ -811,7 +813,7 @@ ENTRY e {\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n   // We can nest the fusion including the broadcast.\n-  EXPECT_TRUE(NestGemmFusion(device_description_, &mlir_context_)\n+  EXPECT_TRUE(NestGemmFusion(device_description_, &symbolic_expr_context_)\n                   .Run(module.get())\n                   .ok());\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n@@ -854,9 +856,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -894,9 +896,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            R\"(\n@@ -945,9 +947,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -996,9 +998,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1032,9 +1034,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1069,9 +1071,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1106,9 +1108,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -1145,9 +1147,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   // Checks that transpose is on rank 3 tensor from hoisting bitcast1, not rank\n   // 4 tensor from hoisting bitcast0 first and then failing to hoist bitcast1.\n@@ -1184,9 +1186,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1220,9 +1222,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1257,9 +1259,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -1292,9 +1294,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1329,9 +1331,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1370,9 +1372,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast\n@@ -1421,9 +1423,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast\n@@ -1469,9 +1471,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(\n-      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+                  .Run(module.get()),\n+              absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast"
        },
        {
            "sha": "dead7f43ede47f9b99303a6172420e6708b095ed",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -40,7 +40,6 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/analysis/hlo_dfs_reachability.h\"\n@@ -58,6 +57,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_indexing_performance_model.h\"\n@@ -150,7 +150,7 @@ class PriorityFusionQueue {\n                       const se::DeviceDescription* device_info,\n                       FusionProcessDumpProto* fusion_process_dump,\n                       tsl::thread::ThreadPool* thread_pool,\n-                      mlir::MLIRContext* mlir_context,\n+                      SymbolicExprContext* symbolic_expr_context,\n                       HloFusionAnalysisCache& fusion_analysis_cache,\n                       FusionDeduplicationCache& fusion_deduplication_cache,\n                       bool triton_heroless_fusion_enabled)\n@@ -159,12 +159,13 @@ class PriorityFusionQueue {\n         cost_analysis_(cost_analysis_options, *device_info),\n         gpu_indexing_performance_model_(device_info, &fusion_analysis_cache,\n                                         cost_analysis_options.shape_size,\n-                                        mlir_context),\n+                                        symbolic_expr_context),\n         fusion_process_dump_(fusion_process_dump),\n         thread_pool_(thread_pool),\n         fusion_analysis_cache_(fusion_analysis_cache),\n         gpu_performance_model_(*device_info, fusion_analysis_cache,\n-                               gpu_performance_model_cache_, mlir_context),\n+                               gpu_performance_model_cache_,\n+                               symbolic_expr_context),\n         fusion_deduplication_cache_(fusion_deduplication_cache),\n         fusion_info_cache_(*device_info_),\n         reachability_(HloDfsReachability::Build(computation)),\n@@ -1160,7 +1161,7 @@ absl::StatusOr<bool> PriorityFusion::Run(\n \n     auto fusion_queue = std::make_unique<PriorityFusionQueue>(\n         computation, cost_analysis_options_, &device_info_,\n-        fusion_process_dump_.get(), thread_pool_, mlir_context_,\n+        fusion_process_dump_.get(), thread_pool_, symbolic_expr_context_,\n         fusion_analysis_cache_, fusion_deduplication_cache,\n         triton_heroless_fusion_enabled);\n "
        },
        {
            "sha": "240ee909a052be085e169f663a5b54c10c8221ef",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,12 +22,12 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/gpu/fusion_process_dump.pb.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -44,12 +44,12 @@ class PriorityFusion : public HloModulePass {\n   PriorityFusion(tsl::thread::ThreadPool* thread_pool,\n                  const se::DeviceDescription& device,\n                  GpuHloCostAnalysis::Options cost_analysis_options,\n-                 mlir::MLIRContext* mlir_context)\n+                 SymbolicExprContext* symbolic_expr_context)\n       : thread_pool_(thread_pool),\n         device_info_(device),\n         cost_analysis_options_(std::move(cost_analysis_options)),\n         fusion_analysis_cache_(device_info_),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override { return \"priority-fusion\"; }\n \n@@ -86,7 +86,7 @@ class PriorityFusion : public HloModulePass {\n \n   HloFusionAnalysisCache fusion_analysis_cache_;\n \n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "4ef54ed4b830c2ff6c3d660f50d76b983498ef01",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/pattern_matcher.h\"\n@@ -75,10 +76,11 @@ class PriorityFusionTest : public HloHardwareIndependentTestBase {\n \n   se::DeviceDescription device_info_ = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   PriorityFusion priority_fusion_{\n       /*thread_pool=*/nullptr, device_info_,\n       GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true},\n-      &mlir_context_};\n+      &symbolic_expr_context_};\n };\n \n TEST_F(PriorityFusionTest, FuseWithSharedArgument) {\n@@ -1375,7 +1377,7 @@ TEST_F(PriorityFusionWithTritonEnabledTest,\n   GpuHloCostAnalysis::Options options;\n   options.count_multiple_input_accesses = true;\n   PriorityFusion priority_fusion_with_thread_pool{\n-      /*thread_pool=*/&pool, device_info_, options, &mlir_context_};\n+      /*thread_pool=*/&pool, device_info_, options, &symbolic_expr_context_};\n   EXPECT_THAT(priority_fusion_with_thread_pool.Run(module.get()),\n               absl_testing::IsOkAndHolds(true));\n   HloInstruction* root = module->entry_computation()->root_instruction();"
        },
        {
            "sha": "cbd01f0a91d167b34caed16f87f8eef7b198bd6a",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 19,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -46,6 +46,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/fusion_pipeline.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_indexing_performance_model.h\"\n@@ -266,7 +267,7 @@ absl::StatusOr<HloFusionInstruction*> MakeFusionForDiamond(\n absl::Status RunFusionPipeline(\n     HloModule* module, const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   HloPassPipeline reduction_pipeline(\"reduction_pipeline\");\n   // Passes that run after SoftmaxRewriterTriton and before PriorityFusion and\n   // transform reductions.\n@@ -279,7 +280,8 @@ absl::Status RunFusionPipeline(\n   TF_RETURN_IF_ERROR(reduction_pipeline.Run(module).status());\n \n   return FusionPipeline(module->config().debug_options(), shape_size,\n-                        /*thread_pool=*/nullptr, device_info, mlir_context)\n+                        /*thread_pool=*/nullptr, device_info,\n+                        symbolic_expr_context)\n       .Run(module)\n       .status();\n }\n@@ -298,14 +300,14 @@ EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n     const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   auto new_module = ExtractComputationIntoNewModule(\n       *fusion->fused_instructions_computation());\n \n   // After this call, the `new_module` will have instruction fused without\n   // SoftmaxRewriterTriton.\n   TF_RETURN_IF_ERROR(RunFusionPipeline(new_module.get(), device_info,\n-                                       shape_size, mlir_context));\n+                                       shape_size, symbolic_expr_context));\n \n   VLOG(3) << \"priority fusion module: \" << new_module->ToString();\n \n@@ -320,7 +322,8 @@ EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n \n   absl::Duration total_run_time = absl::ZeroDuration();\n \n-  GpuPerformanceModelOwning gpu_performance_model(device_info, mlir_context);\n+  GpuPerformanceModelOwning gpu_performance_model(device_info,\n+                                                  symbolic_expr_context);\n   for (const HloInstruction* instr : entry_computation->instructions()) {\n     total_run_time += gpu_performance_model.Get()\n                           .EstimateRunTimeForInstruction(instr, &cost_analysis)\n@@ -345,7 +348,8 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    mlir::MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n+    SymbolicExprContext* symbolic_expr_context,\n+    bool use_cost_model_to_evaluate_fusions) {\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(normalization_fusion);\n \n   TF_ASSIGN_OR_RETURN(\n@@ -362,10 +366,10 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n       std::get<TiledRunTimeData>(std::move(tiled_runtime_data_or));\n \n   if (use_cost_model_to_evaluate_fusions) {\n-    TF_ASSIGN_OR_RETURN(\n-        absl::Duration run_time_without_softmax_rewriter,\n-        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n-            normalization_fusion, device_info, shape_size, mlir_context));\n+    TF_ASSIGN_OR_RETURN(absl::Duration run_time_without_softmax_rewriter,\n+                        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n+                            normalization_fusion, device_info, shape_size,\n+                            symbolic_expr_context));\n \n     VLOG(2) << \"run time estimate if normalization diamond fused together: \"\n             << tiled_runtime_data.runtime_data.exec_time;\n@@ -397,18 +401,19 @@ absl::StatusOr<bool> MaybeFuseDiamondImpl(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    mlir::MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n+    SymbolicExprContext* symbolic_expr_context,\n+    bool use_cost_model_to_evaluate_fusions) {\n   TF_ASSIGN_OR_RETURN(HloFusionInstruction * normalization_fusion,\n                       MakeFusionForDiamond(diamond));\n   HloInstruction* root = diamond.root;\n \n   VLOG(2) << \"MaybeFuseDiamondImpl: \" << normalization_fusion->ToString();\n \n-  TF_ASSIGN_OR_RETURN(\n-      FusionDecision fusion_decision,\n-      DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n-          normalization_fusion, indexing_performance_model, device_info,\n-          shape_size, mlir_context, use_cost_model_to_evaluate_fusions));\n+  TF_ASSIGN_OR_RETURN(FusionDecision fusion_decision,\n+                      DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n+                          normalization_fusion, indexing_performance_model,\n+                          device_info, shape_size, symbolic_expr_context,\n+                          use_cost_model_to_evaluate_fusions));\n \n   if (!fusion_decision.CanFuse()) {\n     VLOG(2) << \"Not fusing: \" << fusion_decision.Explain();\n@@ -437,9 +442,10 @@ absl::StatusOr<bool> CanSymbolicTileAnalysisTileDiamond(\n   TF_ASSIGN_OR_RETURN(HloFusionInstruction * normalization_fusion,\n                       MakeFusionForDiamond(diamond));\n   mlir::MLIRContext context;\n+  SymbolicExprContext symbolic_expr_context(&context);\n   SymbolicTileAnalysisOrError symbolic_tile_analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeComputation(\n-          *normalization_fusion->called_computation(), &context,\n+          *normalization_fusion->called_computation(), &symbolic_expr_context,\n           TritonEmitterConstraints::GetBuilder(device_info));\n \n   bool can_tile = std::holds_alternative<SymbolicTileAnalysis>(\n@@ -633,10 +639,11 @@ absl::StatusOr<bool> SoftmaxRewriterTriton::MaybeFuseNormalizationDiamond(\n     const DiamondDescriptor& diamond) {\n   HloFusionAnalysisCache fusion_analysis_cache(device_info_);\n   GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n-      &device_info_, &fusion_analysis_cache, shape_size_, mlir_context_);\n+      &device_info_, &fusion_analysis_cache, shape_size_,\n+      symbolic_expr_context_);\n \n   return MaybeFuseDiamondImpl(diamond, indexing_performance_model, device_info_,\n-                              shape_size_, mlir_context_,\n+                              shape_size_, symbolic_expr_context_,\n                               use_cost_model_to_evaluate_fusions_);\n }\n "
        },
        {
            "sha": "5f740e79f9f153206b3c22075bf88aca37542168",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -22,10 +22,10 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -51,12 +51,12 @@ class SoftmaxRewriterTriton : public HloModulePass {\n  public:\n   explicit SoftmaxRewriterTriton(const se::DeviceDescription& device_info,\n                                  HloCostAnalysis::ShapeSizeFunction shape_size,\n-                                 mlir::MLIRContext* mlir_context,\n+                                 SymbolicExprContext* symbolic_expr_context,\n                                  bool only_fuse_if_profitable = false)\n       : device_info_(device_info),\n         shape_size_(shape_size),\n         use_cost_model_to_evaluate_fusions_(only_fuse_if_profitable),\n-        mlir_context_(mlir_context) {}\n+        symbolic_expr_context_(symbolic_expr_context) {}\n \n   absl::string_view name() const override { return \"triton-softmax-rewriter\"; }\n \n@@ -106,7 +106,7 @@ class SoftmaxRewriterTriton : public HloModulePass {\n   const se::DeviceDescription& device_info_;\n   const HloCostAnalysis::ShapeSizeFunction shape_size_;\n   bool use_cost_model_to_evaluate_fusions_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "8a08d754e817418eee201b3b5a6da18c9ce42c9f",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/pattern_matcher.h\"\n@@ -63,8 +64,9 @@ class SoftmaxRewriterTritonTest\n  protected:\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   SoftmaxRewriterTriton fusion_rewriter_{\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_};\n };\n \n TEST_F(SoftmaxRewriterTritonTest, CanFuseSingleNormalizationF32) {\n@@ -564,7 +566,7 @@ ENTRY main {\n       SoftmaxRewriterTriton(\n           TestGpuDeviceInfo::RTXA6000DeviceInfo(\n               se::CudaComputeCapability{se::CudaComputeCapability::kVolta, 0}),\n-          HloCostAnalysis::DefaultShapeSize, &mlir_context_)\n+          HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_)\n           .Run(module.get()),\n       absl_testing::StatusIs(\n           tsl::error::FAILED_PRECONDITION,\n@@ -593,7 +595,7 @@ ENTRY main {\n \n   EXPECT_TRUE(SoftmaxRewriterTriton(TestGpuDeviceInfo::AMDMI210DeviceInfo(),\n                                     HloCostAnalysis::DefaultShapeSize,\n-                                    &mlir_context_)\n+                                    &symbolic_expr_context_)\n                   .Run(module.get())\n                   .ok());\n }\n@@ -679,7 +681,7 @@ ENTRY main {\n )\";\n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n   SoftmaxRewriterTriton fusion_rewriter(\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_);\n   EXPECT_FALSE(fusion_rewriter_.Run(module.get()).value());\n }\n \n@@ -746,7 +748,6 @@ ENTRY main {\n           m::Fusion(m::Parameter()).WithPredicate(HasBlockLevelFusionConfig)));\n }\n \n-\n TEST_F(SoftmaxRewriterTritonTest,\n        CanFuseBinaryElementwiseOperationWhereOneOperandIsASharedSplatProducer) {\n   const std::string hlo_string = R\"(\n@@ -827,7 +828,7 @@ ENTRY main {\n \n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n   SoftmaxRewriterTriton softmax_rewriter_triton(\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_);\n   int unmatched = 0, matched = 0;\n   for (HloInstruction* instruction :\n        module->entry_computation()->MakeInstructionPostOrder()) {\n@@ -1082,7 +1083,8 @@ ENTRY main {\n     // Verify that SoftmaxRewriterTriton without Cost Model will fuse the\n     // normalization diamond.\n     SoftmaxRewriterTriton fusion_rewriter_without_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n+        device_info_, HloCostAnalysis::DefaultShapeSize,\n+        &symbolic_expr_context_,\n         /*only_fuse_if_profitable=*/false};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n@@ -1097,7 +1099,8 @@ ENTRY main {\n     // SoftmaxRewriterTriton with Cost Model will discard the normalization\n     // diamond, because row size is too large.\n     SoftmaxRewriterTriton fusion_rewriter_with_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n+        device_info_, HloCostAnalysis::DefaultShapeSize,\n+        &symbolic_expr_context_,\n         /*only_fuse_if_profitable=*/true};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();"
        },
        {
            "sha": "935c391cf4e73bcc863ae878f9e13f2eb076c469",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -25,7 +25,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -44,6 +43,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n #include \"xla/service/gpu/transforms/fusion_wrapper.h\"\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n@@ -126,7 +126,7 @@ absl::Status InlineModuleFusions(HloModule* hlo_module) {\n absl::StatusOr<std::unique_ptr<HloModule>> NewHloModuleFromFusionComputation(\n     const HloFusionInstruction& fusion, const DebugOptions& debug_opts,\n     const se::DeviceDescription& gpu_device_info,\n-    mlir::MLIRContext* mlir_context) {\n+    SymbolicExprContext* symbolic_expr_context) {\n   std::unique_ptr<HloModule> new_module =\n       ExtractComputationIntoNewModule(*fusion.fused_instructions_computation());\n   new_module->mutable_config().set_debug_options(debug_opts);\n@@ -150,7 +150,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> NewHloModuleFromFusionComputation(\n           .status());\n   PriorityFusion fusion_pass(\n       /*thread_pool=*/nullptr, gpu_device_info, HloCostAnalysis::Options{},\n-      mlir_context);\n+      symbolic_expr_context);\n   TF_RETURN_IF_ERROR(fusion_pass.Run(new_module.get()).status());\n \n   // If the priority fusion pass above skipped some instructions, turn them\n@@ -176,13 +176,13 @@ namespace triton_fusion_numerics_pass_internal {\n absl::StatusOr<ScopedShapedBuffer> CompileAndRunFusion(\n     AutotunerCompileUtil& util, const HloFusionInstruction& fusion,\n     const DeviceOrDevicelessConfig& config, const DebugOptions& debug_opts,\n-    bool disable_triton, mlir::MLIRContext* mlir_context) {\n+    bool disable_triton, SymbolicExprContext* symbolic_expr_context) {\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<Executable> executable,\n       util.Compile([&](const DebugOptions& opts) {\n         return disable_triton ? NewHloModuleFromFusionComputation(\n                                     fusion, opts, config.GetDeviceDescription(),\n-                                    mlir_context)\n+                                    symbolic_expr_context)\n                               : NewHloModuleWithTritonFromFusion(fusion, opts);\n       }));\n   if (executable == nullptr) {\n@@ -254,15 +254,15 @@ absl::Status VerifyTritonFusion(AutotunerCompileUtil& util,\n                                 const HloFusionInstruction& fusion,\n                                 const DeviceOrDevicelessConfig& config,\n                                 const DebugOptions& debug_opts,\n-                                mlir::MLIRContext* mlir_context) {\n+                                SymbolicExprContext* symbolic_expr_context) {\n   TF_ASSIGN_OR_RETURN(auto triton_result,\n                       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n                           util, fusion, config, debug_opts,\n-                          /*disable_triton=*/false, mlir_context));\n+                          /*disable_triton=*/false, symbolic_expr_context));\n   TF_ASSIGN_OR_RETURN(auto emitters_result,\n                       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n                           util, fusion, config, debug_opts,\n-                          /*disable_triton=*/true, mlir_context));\n+                          /*disable_triton=*/true, symbolic_expr_context));\n \n   TF_ASSIGN_OR_RETURN(auto stream, config.GetStream());\n   auto status = triton_fusion_numerics_pass_internal::CompareBuffers(\n@@ -325,7 +325,7 @@ absl::StatusOr<bool> TritonFusionNumericsVerifier::Run(\n           return it->second;\n         }\n         auto result = VerifyTritonFusion(compile_util, fusion, config_,\n-                                         debug_options, mlir_context_);\n+                                         debug_options, symbolic_expr_context_);\n         fusion_result_cache_[key] = result;\n         return result;\n       }));"
        },
        {
            "sha": "363e7d8b1d8ab8584f8c61f4e8680e451d947018",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -24,12 +24,12 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/gpu/autotuning/autotuner_compile_util.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -43,8 +43,8 @@ namespace xla::gpu {\n class TritonFusionNumericsVerifier : public HloModulePass {\n  public:\n   TritonFusionNumericsVerifier(const DeviceOrDevicelessConfig& config,\n-                               mlir::MLIRContext* mlir_context)\n-      : config_(config), mlir_context_(mlir_context) {}\n+                               SymbolicExprContext* symbolic_expr_context)\n+      : config_(config), symbolic_expr_context_(symbolic_expr_context) {}\n \n   static absl::string_view Name() { return \"triton-numerics-verifier\"; }\n   absl::string_view name() const override { return Name(); }\n@@ -60,7 +60,7 @@ class TritonFusionNumericsVerifier : public HloModulePass {\n \n  private:\n   DeviceOrDevicelessConfig config_;\n-  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext* symbolic_expr_context_;\n \n   // In some models there are many identical fusions. These are cached to avoid\n   // expensive recomputations.\n@@ -73,7 +73,7 @@ namespace triton_fusion_numerics_pass_internal {\n absl::StatusOr<ScopedShapedBuffer> CompileAndRunFusion(\n     AutotunerCompileUtil& util, const HloFusionInstruction& fusion,\n     const DeviceOrDevicelessConfig& config, const DebugOptions& debug_opts,\n-    bool disable_triton, mlir::MLIRContext* mlir_context);\n+    bool disable_triton, SymbolicExprContext* symbolic_expr_context);\n absl::Status CompareBuffers(const ScopedShapedBuffer& current,\n                             const ScopedShapedBuffer& expected,\n                             const Shape& shape, const DebugOptions& debug_opts,"
        },
        {
            "sha": "0f31231ba8e0668ea2edc4682328a3311904ebfe",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/autotuning/autotuner_compile_util.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/tests/hlo_pjrt_test_base.h\"\n@@ -89,6 +90,7 @@ class TritonFusionNumericsVerifierTest\n   }\n \n   mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n constexpr absl::string_view kSoftmaxHlo = R\"(\n@@ -134,7 +136,7 @@ TEST_P(TritonFusionNumericsVerifierTest, VerifyExactSoftmaxFusionNumerics) {\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &mlir_context_);\n+                                               &symbolic_expr_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -189,7 +191,7 @@ ENTRY entry {\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &mlir_context_);\n+                                               &symbolic_expr_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -220,7 +222,7 @@ ENTRY main{\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &mlir_context_);\n+                                               &symbolic_expr_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -310,7 +312,7 @@ ENTRY main (p0: bf16[128,512], p1: bf16[256,512], p2: bf16[512,512]) -> bf16[384\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &mlir_context_);\n+                                               &symbolic_expr_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -342,12 +344,12 @@ TEST_F(TritonFusionNumericsVerifierTest, CheckMismatch) {\n \n   auto f64_result = triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n       compile_util, *fusion_f64, autotune_config, debug_options,\n-      /*disable_triton=*/false, &mlir_context_);\n+      /*disable_triton=*/false, &symbolic_expr_context_);\n   TF_EXPECT_OK(f64_result);\n \n   auto f32_result = triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n       compile_util, *fusion_f32, autotune_config, debug_options,\n-      /*disable_triton=*/false, &mlir_context_);\n+      /*disable_triton=*/false, &symbolic_expr_context_);\n   TF_EXPECT_OK(f32_result);\n \n   auto stream = autotune_config.GetStream();\n@@ -397,7 +399,7 @@ ENTRY main {\n                        \"\");\n \n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &mlir_context_);\n+                                               &symbolic_expr_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n   auto fusion = TritonFusion(*module);\n   EXPECT_NE(fusion, nullptr);\n@@ -408,7 +410,7 @@ ENTRY main {\n   auto compilation_result =\n       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n           compile_util, *fusion, autotune_config, GetDebugOptionsForTest(),\n-          /*disable_triton=*/false, &mlir_context_);\n+          /*disable_triton=*/false, &symbolic_expr_context_);\n \n   // Verify that the compilation with default flags fails. The compilation\n   // fails, because the kernel will spill registers, but the error is\n@@ -467,7 +469,7 @@ ENTRY main {\n \n   std::unique_ptr<HloModule> module = Module(hlo_text, \"\");\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &mlir_context_);\n+                                               &symbolic_expr_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n   EXPECT_EQ(verifier.CacheHitsForTestingOnly(), 1);\n }\n@@ -522,7 +524,7 @@ ENTRY main {\n   auto module = Module(hlo_text, \"\");\n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &mlir_context_);\n+                                               &symbolic_expr_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n "
        },
        {
            "sha": "f7c4b6eb782524b8f598b56e5bb4d284ed7cdbce",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -416,10 +416,10 @@ cc_library(\n     srcs = [\"codegen_test_base.cc\"],\n     hdrs = [\"codegen_test_base.h\"],\n     deps = [\n+        \":hlo_test_base_with_symbolic_expr_context\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n-        \"//xla/tests:hlo_test_base_with_mlir_context\",\n     ],\n )\n \n@@ -1208,8 +1208,8 @@ xla_test(\n         \"test_migrated_to_hlo_runner_pjrt\",\n     ],\n     deps = [\n+        \":hlo_pjrt_test_base\",\n         \"//xla/service:platform_util\",\n-        \"//xla/tests:hlo_pjrt_test_base\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/status\",\n@@ -3800,11 +3800,12 @@ cc_library(\n )\n \n cc_library(\n-    name = \"hlo_test_base_with_mlir_context\",\n+    name = \"hlo_test_base_with_symbolic_expr_context\",\n     testonly = True,\n-    hdrs = [\"hlo_test_base_with_mlir_context.h\"],\n+    hdrs = [\"hlo_test_base_with_symbolic_expr_context.h\"],\n     deps = [\n-        \"//xla/tests:hlo_test_base\",\n+        \":hlo_test_base\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"@llvm-project//mlir:IR\",\n     ],\n )"
        },
        {
            "sha": "18b7ae07c7ee5b8e9cf6525d4938ca38809b4992",
            "filename": "third_party/xla/xla/tests/codegen_test_base.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftests%2Fcodegen_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftests%2Fcodegen_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcodegen_test_base.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -21,12 +21,12 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n-#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n+#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n \n namespace xla {\n \n // Provides access to both the JIT and the AOT compiler for testing.\n-class CodegenTestBase : public HloTestBaseWithMlirContext {\n+class CodegenTestBase : public HloTestBaseWithSymbolicExprContext {\n  protected:\n   // Compiles hlo_module with the JIT compiler.\n   absl::StatusOr<std::unique_ptr<Executable>> CompileToExecutable("
        },
        {
            "sha": "6116a8b147e8a9553234f1905856f5e29a6e3f68",
            "filename": "third_party/xla/xla/tests/hlo_test_base_with_symbolic_expr_context.h",
            "status": "renamed",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_test_base_with_symbolic_expr_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_test_base_with_symbolic_expr_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_test_base_with_symbolic_expr_context.h?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -13,22 +13,26 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#ifndef XLA_TESTS_HLO_TEST_BASE_WITH_MLIR_CONTEXT_H_\n-#define XLA_TESTS_HLO_TEST_BASE_WITH_MLIR_CONTEXT_H_\n+#ifndef XLA_TESTS_HLO_TEST_BASE_WITH_SYMBOLIC_EXPR_CONTEXT_H_\n+#define XLA_TESTS_HLO_TEST_BASE_WITH_SYMBOLIC_EXPR_CONTEXT_H_\n \n #include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tests/hlo_test_base.h\"\n \n namespace xla {\n \n-class HloTestBaseWithMlirContext : public HloTestBase {\n+class HloTestBaseWithSymbolicExprContext : public HloTestBase {\n  public:\n-  mlir::MLIRContext* mlir_context() { return &mlir_context_; }\n+  gpu::SymbolicExprContext* symbolic_expr_context() {\n+    return &symbolic_expr_context_;\n+  }\n \n  private:\n   mlir::MLIRContext mlir_context_;\n+  gpu::SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n }  // namespace xla\n \n-#endif  // XLA_TESTS_HLO_TEST_BASE_WITH_MLIR_CONTEXT_H_\n+#endif  // XLA_TESTS_HLO_TEST_BASE_WITH_SYMBOLIC_EXPR_CONTEXT_H_",
            "previous_filename": "third_party/xla/xla/tests/hlo_test_base_with_mlir_context.h"
        },
        {
            "sha": "1eadf77e54253ab3181000bce3ac1e335af7dc17",
            "filename": "third_party/xla/xla/tools/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2FBUILD?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -1142,6 +1142,7 @@ xla_cc_binary(\n         \":hlo_module_loader\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util:command_line_flags\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "988bca17aed3f4f194ab02238cf0c74aa4944f1d",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_opt.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -213,11 +213,12 @@ class GpuOptProvider : public CompiledOptProvider {\n     std::unique_ptr<gpu::GpuAliasInfo> alias_info =\n         gpu_compiler->GetAliasInfo(device_description);\n     if (!optimized_module->has_schedule()) {\n-      TF_ASSIGN_OR_RETURN(gpu::ScheduleMetadata schedule_metadata,\n-                          gpu::ScheduleGpuModule(\n-                              optimized_module, gpu_compiler->GetPointerSize(),\n-                              device_description, gpu_compiler->mlir_context(),\n-                              alias_info.get()));\n+      TF_ASSIGN_OR_RETURN(\n+          gpu::ScheduleMetadata schedule_metadata,\n+          gpu::ScheduleGpuModule(\n+              optimized_module, gpu_compiler->GetPointerSize(),\n+              device_description, gpu_compiler->symbolic_expr_context(),\n+              alias_info.get()));\n       TF_RETURN_IF_ERROR(gpu_compiler->RunPostSchedulingPipelines(\n           optimized_module, schedule_metadata.scheduler_mem_limit,\n           device_description, alias_info.get()));"
        },
        {
            "sha": "6ce09783af834858ba43d8a4d5d0e214d3b717f0",
            "filename": "third_party/xla/xla/tools/print_indexing.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftools%2Fprint_indexing.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f23728131ea8fe7e602f9007b8cba0eaced22a2e/third_party%2Fxla%2Fxla%2Ftools%2Fprint_indexing.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fprint_indexing.cc?ref=f23728131ea8fe7e602f9007b8cba0eaced22a2e",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/tools/hlo_module_loader.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/command_line_flags.h\"\n@@ -50,7 +51,8 @@ absl::Status Run(const std::string& filename, int operand_id, int output_id) {\n   if (print_all) {\n     get_operand_id = 0;\n   }\n-  mlir::MLIRContext ctx;\n+  mlir::MLIRContext mlir_context;\n+  gpu::SymbolicExprContext symbolic_expr_context(&mlir_context);\n   VLOG(1) << \"module:\\n\" << module->ToString() << std::endl;\n   LOG(INFO) << \"root instruction is: \" << root->ToString() << std::endl;\n   VLOG(1) << \"root is tuple: \" << root->shape().IsTuple();\n@@ -74,7 +76,7 @@ absl::Status Run(const std::string& filename, int operand_id, int output_id) {\n \n   for (int out_id : output_ids) {\n     HloInstructionIndexing indexing =\n-        ComputeOutputToInputIndexing(root, out_id, &ctx);\n+        ComputeOutputToInputIndexing(root, out_id, &symbolic_expr_context);\n     LOG(INFO) << absl::StrFormat(\"output id %d has %d indexing maps\", out_id,\n                                  indexing.indexing_maps.size());\n     if (indexing.indexing_maps.empty()) {"
        }
    ],
    "stats": {
        "total": 3933,
        "additions": 2306,
        "deletions": 1627
    }
}