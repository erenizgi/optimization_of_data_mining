{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Remove RaggedAllToAll from command buffer conversion pass.\n\nRaggedAllToAll is not yet supported by command buffers. Added a test to verify that RaggedAllToAll executes correctly when command buffers are enabled for collectives.\n\nPiperOrigin-RevId: 845197490",
    "sha": "b47342de0a5911fee447c7cbadd6420e3072fc6d",
    "files": [
        {
            "sha": "e25b81c61bfbfe9f0d8348e9be79e49736a7fe83",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b47342de0a5911fee447c7cbadd6420e3072fc6d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b47342de0a5911fee447c7cbadd6420e3072fc6d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc?ref=b47342de0a5911fee447c7cbadd6420e3072fc6d",
            "patch": "@@ -144,7 +144,6 @@ std::optional<DebugOptions::CommandBufferCmdType> GetCommandBufferCmdType(\n     case Thunk::kAllToAllStart:\n     case Thunk::kCollectiveBroadcastStart:\n     case Thunk::kCollectivePermuteStart:\n-    case Thunk::kRaggedAllToAllStart:\n     case Thunk::kRecv:\n     case Thunk::kSend:\n       return DebugOptions::COLLECTIVES;"
        },
        {
            "sha": "d229e0b0fa47454d4bb4f580973b92d17c943e10",
            "filename": "third_party/xla/xla/tests/ragged_all_to_all_e2e_test.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b47342de0a5911fee447c7cbadd6420e3072fc6d/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b47342de0a5911fee447c7cbadd6420e3072fc6d/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc?ref=b47342de0a5911fee447c7cbadd6420e3072fc6d",
            "patch": "@@ -361,6 +361,54 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs) {\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n }\n \n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_CommandBuffer) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[4] parameter(0)\n+    output = f32[4] parameter(1)\n+    input_offsets = s32[2] parameter(2)\n+    send_sizes = s32[2] parameter(3)\n+    output_offsets = s32[2] parameter(4)\n+    recv_sizes = s32[2] parameter(5)\n+    ROOT ra2a = f32[4] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n+\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                        kModuleReplicatedStr, kNumReplicas));\n+\n+  // Verify correctness of ragged-all-to-all when command buffers for\n+  // collectives are enabled.\n+  // As of Dec 2025, ragged-all-to-all command is not implemented, so this test\n+  // verifies that we don't try to accidentally create a command buffer and\n+  // crash.\n+  DebugOptions& debug_options =\n+      module->mutable_config().mutable_debug_options();\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n+  debug_options.set_xla_gpu_graph_min_graph_size(1);\n+\n+  ASSERT_OK(CreateRandomTestData(module.get(),\n+                                 /*input_sizes=*/{/*replica_0=*/{1, 1},\n+                                                  /*replica_1=*/{3, 1}}));\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecutionResult execution_result,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n+\n+  const std::vector<Literal>& results = execution_result.results;\n+\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_S4) {\n   absl::string_view kModuleReplicatedStr = R\"(\n   HloModule module, num_partitions=1"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 48,
        "deletions": 1
    }
}