{
    "author": "junwhanahn",
    "message": "Allow setting sub-allocator visitors from `xla::GpuAllocatorConfig`\n\nPiperOrigin-RevId: 835360145",
    "sha": "05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
    "files": [
        {
            "sha": "54358393bc1e704f8889bf0be23b446d0651a3c2",
            "filename": "third_party/xla/xla/pjrt/gpu/gpu_helpers.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fgpu_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fgpu_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fgpu_helpers.cc?ref=05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
            "patch": "@@ -90,7 +90,10 @@ void EnablePeerAccess(absl::Span<se::StreamExecutor* const> executors) {\n // Builds a BFCAllocator for all local GPUs.\n absl::StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(\n     se::StreamExecutor* executor, double memory_fraction, bool preallocate,\n-    std::optional<int64_t> gpu_system_memory_size) {\n+    std::optional<int64_t> gpu_system_memory_size,\n+    const std::vector<tsl::SubAllocator::Visitor>& sub_allocator_alloc_visitors,\n+    const std::vector<tsl::SubAllocator::Visitor>&\n+        sub_allocator_free_visitors) {\n   bool enable_unified_memory;\n   absl::Status status = tsl::ReadBoolFromEnvVar(\"TF_FORCE_UNIFIED_MEMORY\",\n                                                 false, &enable_unified_memory);\n@@ -108,10 +111,12 @@ absl::StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(\n         executor->CreateMemoryAllocator(stream_executor::MemoryType::kUnified));\n     sub_allocator = std::make_unique<se::StreamExecutorAllocator>(\n         std::move(unified_memory_allocator),\n-        stream_executor::MemoryType::kUnified, device_ordinal);\n+        stream_executor::MemoryType::kUnified, device_ordinal,\n+        sub_allocator_alloc_visitors, sub_allocator_free_visitors);\n   } else {\n     sub_allocator = std::make_unique<se::DeviceMemAllocator>(\n-        executor, tsl::PlatformDeviceId(device_ordinal));\n+        executor, tsl::PlatformDeviceId(device_ordinal),\n+        sub_allocator_alloc_visitors, sub_allocator_free_visitors);\n   }\n \n   int64_t free_memory;"
        },
        {
            "sha": "dc00d4f41c7b06ff7361df852a75c3b52ae57786",
            "filename": "third_party/xla/xla/pjrt/gpu/gpu_helpers.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fgpu_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fgpu_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fgpu_helpers.h?ref=05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
            "patch": "@@ -16,18 +16,21 @@ limitations under the License.\n #ifndef XLA_PJRT_GPU_GPU_HELPERS_H_\n #define XLA_PJRT_GPU_GPU_HELPERS_H_\n \n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <optional>\n #include <set>\n #include <string>\n+#include <vector>\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/client/local_client.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_allocator_config.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/framework/allocator.h\"\n #include \"xla/tsl/framework/bfc_allocator.h\"\n #include \"xla/types.h\"\n \n@@ -47,7 +50,9 @@ absl::StatusOr<std::unique_ptr<tsl::BFCAllocator>> GetGpuHostAllocator(\n // Builds a BFCAllocator for all local GPUs.\n absl::StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(\n     se::StreamExecutor* executor, double memory_fraction, bool preallocate,\n-    std::optional<int64_t> gpu_system_memory_size);\n+    std::optional<int64_t> gpu_system_memory_size,\n+    const std::vector<tsl::SubAllocator::Visitor>& sub_allocator_alloc_visitors,\n+    const std::vector<tsl::SubAllocator::Visitor>& sub_allocator_free_visitors);\n \n // Builds a BFCAllocator for all local GPUs that uses collective memory.\n absl::StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateCollectiveBFCAllocator("
        },
        {
            "sha": "8a58ee76b3417b1a7f52af22c050fd3879d24ea1",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
            "patch": "@@ -1042,7 +1042,9 @@ GetStreamExecutorGpuDeviceAllocator(\n             CreateBFCAllocator(ordinal_and_device.second->executor(),\n                                allocator_config.memory_fraction,\n                                allocator_config.preallocate,\n-                               allocator_config.gpu_system_memory_size));\n+                               allocator_config.gpu_system_memory_size,\n+                               allocator_config.sub_allocator_alloc_visitors,\n+                               allocator_config.sub_allocator_free_visitors));\n         allocators.emplace_back(\n             std::move(bfc_allocator),\n             ordinal_and_device.second->compute_stream(),"
        },
        {
            "sha": "c6a4836f4a11799ed2251805aee21ad24f8fed22",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc?ref=05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
            "patch": "@@ -640,7 +640,9 @@ absl::StatusOr<std::unique_ptr<tsl::Allocator>> CreateAllocatorForDevice(\n       LOG_FIRST_N(INFO, 1) << \"Using BFC allocator.\";\n       return CreateBFCAllocator(executor, allocator_config.memory_fraction,\n                                 allocator_config.preallocate,\n-                                allocator_config.gpu_system_memory_size);\n+                                allocator_config.gpu_system_memory_size,\n+                                allocator_config.sub_allocator_alloc_visitors,\n+                                allocator_config.sub_allocator_free_visitors);\n     case GpuAllocatorConfig::Kind::kPlatform:\n       LOG(FATAL) << \"Platform allocator should be handled before calling this \"\n                     \"function.\";"
        },
        {
            "sha": "c7e1c6daa747d678f9aa2596bdff1f7ea13439ed",
            "filename": "third_party/xla/xla/pjrt/plugin/xla_gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2FBUILD?ref=05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
            "patch": "@@ -39,6 +39,7 @@ cc_library(\n     srcs = [],\n     hdrs = [\"xla_gpu_allocator_config.h\"],\n     deps = [\n+        \"//xla/tsl/framework:allocator\",\n     ],\n )\n "
        },
        {
            "sha": "fe75d777ece0ab817c7cdeb275fa3ed93f8b17fa",
            "filename": "third_party/xla/xla/pjrt/plugin/xla_gpu/xla_gpu_allocator_config.h",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2Fxla_gpu_allocator_config.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2Fxla_gpu_allocator_config.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2Fxla_gpu_allocator_config.h?ref=05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
            "patch": "@@ -19,6 +19,9 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <optional>\n+#include <vector>\n+\n+#include \"xla/tsl/framework/allocator.h\"\n \n namespace xla {\n \n@@ -57,6 +60,11 @@ struct GpuAllocatorConfig {\n   // should be set to a multiple of 512MB to avoid wasting memory due to\n   // granularity requirements.\n   size_t collective_memory_size = 0;\n+\n+  // Callbacks that get called when the underlying suballocator allocates or\n+  // deallocates memory. See `SubAllocator::Visitor` for more details.\n+  std::vector<tsl::SubAllocator::Visitor> sub_allocator_alloc_visitors;\n+  std::vector<tsl::SubAllocator::Visitor> sub_allocator_free_visitors;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "2f15ba3f2be9d448b4b8360f95c78d1d282bdbb9",
            "filename": "third_party/xla/xla/stream_executor/integrations/device_mem_allocator.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fdevice_mem_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05cd7f2f9522a939f26bdbb6f874b4cf80f258d2/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fdevice_mem_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fdevice_mem_allocator.h?ref=05cd7f2f9522a939f26bdbb6f874b4cf80f258d2",
            "patch": "@@ -33,8 +33,9 @@ class DeviceMemAllocator : public tsl::SubAllocator {\n   // Note: stream_exec cannot be null.\n   DeviceMemAllocator(StreamExecutor* stream_exec,\n                      tsl::PlatformDeviceId device_id,\n-                     const std::vector<Visitor>& alloc_visitors = {})\n-      : SubAllocator(alloc_visitors, {}),\n+                     const std::vector<Visitor>& alloc_visitors = {},\n+                     const std::vector<Visitor>& free_visitors = {})\n+      : SubAllocator(alloc_visitors, free_visitors),\n         stream_exec_(stream_exec),\n         device_id_(device_id) {\n     CHECK(stream_exec_ != nullptr);"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 32,
        "deletions": 8
    }
}