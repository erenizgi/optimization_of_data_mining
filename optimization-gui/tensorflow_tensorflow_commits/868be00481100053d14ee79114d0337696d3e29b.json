{
    "author": "sergachev",
    "message": "PR #32430: [GPU] Use intrinsics to accelerate f4e2m1fn conversions on Blackwell.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32430\n\nðŸ“ Summary of Changes\nUse intrinsics to accelerate f4e2m1fn conversions on Blackwell.\n\nðŸŽ¯ Justification\nAccelerates type conversions.\n\nðŸš€ Kind of Contribution\nPerformance Improvement.\n\nðŸ“Š Benchmark (for Performance Improvements)\nPublic HLOs in `compiler/xla/tools/benchmarks/hlo/` don't use f4e2m1fn.\nOn a trivial microbenchmark (bf16->f4e2m1fn conversion of a large buffer) leads to ~2x speedup.\n\nðŸ§ª Unit Tests: Yes.\n\nðŸ§ª Execution Tests: Yes.\nCopybara import of the project:\n\n--\nd40e86ccb4568f6b515543e9c3bfe2425e48d964 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Use intrinsics to accelerate f4e2m1fn conversions on Blackwell.\n\n--\na0e9e4ddfc2ddee6e57cffffca4c5278279e1c76 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review comment.\n\n--\nc8bb0a81efda7b7ffb41542d4a8b112796eb218a by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review comment.\n\nMerging this change closes #32430\n\nPiperOrigin-RevId: 817101962",
    "sha": "868be00481100053d14ee79114d0337696d3e29b",
    "files": [
        {
            "sha": "c0cd17b3037bd0c4f5c7a09a62c376d7b21ced27",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=868be00481100053d14ee79114d0337696d3e29b",
            "patch": "@@ -502,13 +502,8 @@ void AddLoweringPasses(mlir::OpPassManager& pm,\n     se::SemanticVersion ptx_version =\n         nvptx::DetermineHighestSupportedPtxVersionFromCudaVersion(\n             device.runtime_version());\n-\n-    // FP8 conversion intrinsics are available on sm89 since ptx 8.1\n-    // Older ptx versions only support FP8 conversion for sm90\n-    if ((ptx_version >= se::SemanticVersion(8, 1, 0) && cc->IsAtLeast(8, 9)) ||\n-        (ptx_version >= se::SemanticVersion(7, 8, 0) && cc->IsAtLeast(9, 0))) {\n-      pm.addPass(CreateConvertFloatNvidiaPass());\n-    }\n+    pm.addPass(CreateConvertFloatNvidiaPass(\n+        cc->major, cc->minor, ptx_version.major(), ptx_version.minor()));\n   } else if (auto* cc = std::get_if<se::RocmComputeCapability>(\n                  &device.gpu_compute_capability())) {\n     if (cc->has_fp8_support()) {"
        },
        {
            "sha": "27ebfcf03e6e9301c9960f84890cd0e882c32990",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/tests/convert/f4e2m1fn_intrinsics.hlo",
            "status": "added",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftests%2Fconvert%2Ff4e2m1fn_intrinsics.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftests%2Fconvert%2Ff4e2m1fn_intrinsics.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftests%2Fconvert%2Ff4e2m1fn_intrinsics.hlo?ref=868be00481100053d14ee79114d0337696d3e29b",
            "patch": "@@ -0,0 +1,11 @@\n+// RUN: fusion_to_mlir %s | emitters_opt -xla-gpu-convert-float-nvidia='compute_capability_major=10 compute_capability_minor=0 ptx_version_major=8 ptx_version_minor=6' | FileCheck %s\n+// RUN: gpu_test_correctness %s\n+\n+f {\n+  a = f32[15,20] parameter(0)\n+  b = f4e2m1fn[15,20] convert(a)\n+  c = f32[15,20] convert(b)\n+}\n+\n+// CHECK: llvm.nvvm.ff.to.e2m1x2.rn\n+// CHECK: llvm.nvvm.e2m1x2.to.f16x2.rn"
        },
        {
            "sha": "974875bdac571b131b934b9142198a522e052b4f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/convert_float_nvidia.cc",
            "status": "modified",
            "additions": 101,
            "deletions": 43,
            "changes": 144,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_nvidia.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_nvidia.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_nvidia.cc?ref=868be00481100053d14ee79114d0337696d3e29b",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"xla/backends/gpu/codegen/emitters/transforms/passes.h\"\n \n-\n namespace xla {\n namespace gpu {\n \n@@ -53,57 +52,85 @@ int GetExponentBias(mlir::FloatType ty) {\n   return 1 - llvm::APFloat::semanticsMinExponent(ty.getFloatSemantics());\n }\n \n+Value ConvertToF32(Value v, mlir::ImplicitLocOpBuilder& b) {\n+  mlir::FloatType f32_ty = b.getF32Type();\n+  if (v.getType() == f32_ty) {\n+    return v;\n+  }\n+  if (v.getType().getIntOrFloatBitWidth() < f32_ty.getWidth()) {\n+    return b.create<ma::ExtFOp>(f32_ty, v);\n+  }\n+  return b.create<ma::TruncFOp>(f32_ty, v);\n+}\n+\n struct RewriteTruncFPattern : public mlir::OpRewritePattern<ma::TruncFOp> {\n-  using OpRewritePattern::OpRewritePattern;\n+  RewriteTruncFPattern(mlir::MLIRContext* context, bool enable_f8,\n+                       bool enable_f4)\n+      : OpRewritePattern(context),\n+        enable_f8_(enable_f8),\n+        enable_f4_(enable_f4) {}\n \n   mlir::LogicalResult matchAndRewrite(\n       ma::TruncFOp op, mlir::PatternRewriter& rewriter) const override {\n     using FloatValue = mlir::TypedValue<mlir::FloatType>;\n     auto src = mlir::cast<FloatValue>(op.getOperand());\n     auto dst_ty = mlir::cast<mlir::FloatType>(op.getType());\n-    if (!llvm::isa<mlir::Float8E4M3FNType>(dst_ty) &&\n-        !llvm::isa<mlir::Float8E5M2Type>(dst_ty)) {\n-      return rewriter.notifyMatchFailure(op, \"unsupported float conversion\");\n+\n+    const bool is_f8 =\n+        llvm::isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>(dst_ty);\n+    const bool is_f4 = llvm::isa<mlir::Float4E2M1FNType>(dst_ty);\n+\n+    if ((is_f8 && enable_f8_) || (is_f4 && enable_f4_)) {\n+      mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n+      rewriter.replaceOp(op, EmitTruncFIntrinsic(src, dst_ty, b));\n+      return mlir::success();\n     }\n \n-    mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n-    rewriter.replaceOp(op, EmitTruncToF8Intrinsic(src, dst_ty, b));\n-    return mlir::success();\n+    return rewriter.notifyMatchFailure(op, \"unsupported float conversion\");\n   }\n \n-  Value EmitTruncToF8Intrinsic(Value value, mlir::FloatType to_ty,\n-                               mlir::ImplicitLocOpBuilder& b) const {\n-    assert((llvm::isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>(to_ty)));\n+ private:\n+  bool enable_f8_;\n+  bool enable_f4_;\n+\n+  Value EmitTruncFIntrinsic(Value value, mlir::FloatType to_ty,\n+                            mlir::ImplicitLocOpBuilder& b) const {\n+    assert((llvm::isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type,\n+                      mlir::Float4E2M1FNType>(to_ty)));\n \n     ml::CallIntrinsicOp cvtOp;\n-    if (value.getType() == b.getF16Type()) {\n+    if (llvm::isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>(to_ty) &&\n+        value.getType() == b.getF16Type()) {\n       // Fast path for truncating F16 type.\n       Value vec =\n           b.create<ml::UndefOp>(mlir::VectorType::get(2, value.getType()));\n       vec = b.create<ml::InsertElementOp>(vec, value,\n                                           b.create<ma::ConstantIntOp>(0, 8));\n-      auto cvtIntr = llvm::isa<mlir::Float8E4M3FNType>(to_ty)\n-                         ? \"llvm.nvvm.f16x2.to.e4m3x2.rn\"\n-                         : \"llvm.nvvm.f16x2.to.e5m2x2.rn\";\n+      const std::string cvtIntr = llvm::isa<mlir::Float8E4M3FNType>(to_ty)\n+                                      ? \"llvm.nvvm.f16x2.to.e4m3x2.rn\"\n+                                      : \"llvm.nvvm.f16x2.to.e5m2x2.rn\";\n       cvtOp = b.create<ml::CallIntrinsicOp>(b.getIntegerType(16),\n                                             b.getStringAttr(cvtIntr),\n                                             mlir::ValueRange{vec});\n     } else {\n       // Other FP types get converted to F32 first.\n-      mlir::FloatType f32_ty = b.getF32Type();\n-      if (value.getType().getIntOrFloatBitWidth() < f32_ty.getWidth()) {\n-        value = b.create<ma::ExtFOp>(f32_ty, value);\n-      } else if (value.getType() != f32_ty) {\n-        value = b.create<ma::TruncFOp>(f32_ty, value);\n-      }\n-      auto cvtIntr = llvm::isa<mlir::Float8E4M3FNType>(to_ty)\n-                         ? \"llvm.nvvm.ff.to.e4m3x2.rn\"\n-                         : \"llvm.nvvm.ff.to.e5m2x2.rn\";\n+      value = ConvertToF32(value, b);\n+      const std::string cvtIntr = llvm::isa<mlir::Float4E2M1FNType>(to_ty)\n+                                      ? \"llvm.nvvm.ff.to.e2m1x2.rn.satfinite\"\n+                                  : llvm::isa<mlir::Float8E4M3FNType>(to_ty)\n+                                      ? \"llvm.nvvm.ff.to.e4m3x2.rn\"\n+                                      : \"llvm.nvvm.ff.to.e5m2x2.rn\";\n       cvtOp = b.create<ml::CallIntrinsicOp>(b.getIntegerType(16),\n                                             b.getStringAttr(cvtIntr),\n                                             mlir::ValueRange{value, value});\n     }\n-    Value res = b.create<ml::TruncOp>(b.getIntegerType(8), cvtOp.getResults());\n+\n+    Value res = b.create<ml::TruncOp>(\n+        b.getIntegerType(to_ty.getIntOrFloatBitWidth()), cvtOp.getResults());\n+\n+    if (llvm::isa<mlir::Float4E2M1FNType>(to_ty)) {\n+      return b.create<ma::BitcastOp>(to_ty, res);\n+    }\n \n     // Downcasting to float8 saturates the value (uses \"satfinite\" modifier).\n     // Handle infinity separately to mitigate the issue.\n@@ -183,37 +210,53 @@ struct RewriteTruncFPattern : public mlir::OpRewritePattern<ma::TruncFOp> {\n };\n \n struct RewriteExtFPattern : public mlir::OpRewritePattern<ma::ExtFOp> {\n-  using OpRewritePattern::OpRewritePattern;\n+  RewriteExtFPattern(mlir::MLIRContext* context, bool enable_f8, bool enable_f4)\n+      : OpRewritePattern(context),\n+        enable_f8_(enable_f8),\n+        enable_f4_(enable_f4) {}\n \n   mlir::LogicalResult matchAndRewrite(\n       ma::ExtFOp op, mlir::PatternRewriter& rewriter) const override {\n     using FloatValue = mlir::TypedValue<mlir::FloatType>;\n     auto src = mlir::cast<FloatValue>(op.getOperand());\n     auto dst_ty = mlir::cast<mlir::FloatType>(op.getType());\n-    if (!llvm::isa<mlir::Float8E4M3FNType>(src.getType()) &&\n-        !llvm::isa<mlir::Float8E5M2Type>(src.getType())) {\n-      return rewriter.notifyMatchFailure(op, \"unsupported float conversion\");\n+\n+    const bool is_f8 =\n+        llvm::isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>(src.getType());\n+    const bool is_f4 = llvm::isa<mlir::Float4E2M1FNType>(src.getType());\n+\n+    if ((is_f8 && enable_f8_) || (is_f4 && enable_f4_)) {\n+      mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n+      rewriter.replaceOp(op, EmitExtFIntrinsic(src, dst_ty, b));\n+      return mlir::success();\n     }\n \n-    mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n-    rewriter.replaceOp(op, EmitExtFromF8Intrinsic(src, dst_ty, b));\n-    return mlir::success();\n+    return rewriter.notifyMatchFailure(op, \"unsupported float conversion\");\n   }\n \n-  Value EmitExtFromF8Intrinsic(Value value, mlir::FloatType to_ty,\n-                               mlir::ImplicitLocOpBuilder& b) const {\n-    assert((llvm::isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>(\n-        value.getType())));\n+ private:\n+  bool enable_f8_;\n+  bool enable_f4_;\n+\n+  Value EmitExtFIntrinsic(Value value, mlir::FloatType to_ty,\n+                          mlir::ImplicitLocOpBuilder& b) const {\n+    assert((llvm::isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type,\n+                      mlir::Float4E2M1FNType>(value.getType())));\n \n     // Extend the smaller type to the FP16 type using the intrinsic, and then\n     // to the destination type. In the case of BF16 go through the intermediate\n     // FP32 type (as there's no F2F op for f16->bf16).\n+    const std::string cvtIntr =\n+        llvm::isa<mlir::Float4E2M1FNType>(value.getType())\n+            ? \"llvm.nvvm.e2m1x2.to.f16x2.rn\"\n+        : llvm::isa<mlir::Float8E4M3FNType>(value.getType())\n+            ? \"llvm.nvvm.e4m3x2.to.f16x2.rn\"\n+            : \"llvm.nvvm.e5m2x2.to.f16x2.rn\";\n     Value input = b.create<ml::ZExtOp>(\n         b.getIntegerType(16),\n-        b.create<ma::BitcastOp>(b.getIntegerType(8), value));\n-    auto cvtIntr = llvm::isa<mlir::Float8E4M3FNType>(value.getType())\n-                       ? \"llvm.nvvm.e4m3x2.to.f16x2.rn\"\n-                       : \"llvm.nvvm.e5m2x2.to.f16x2.rn\";\n+        b.create<ma::BitcastOp>(\n+            b.getIntegerType(value.getType().getIntOrFloatBitWidth()), value));\n+\n     mlir::FloatType f16_ty = b.getF16Type();\n     auto cvtOp = b.create<ml::CallIntrinsicOp>(mlir::VectorType::get(2, f16_ty),\n                                                b.getStringAttr(cvtIntr),\n@@ -238,8 +281,16 @@ class ConvertFloatNvidiaPass\n   using ConvertFloatNvidiaPassBase::ConvertFloatNvidiaPassBase;\n \n   void runOnOperation() override {\n+    const int cc_version =\n+        compute_capability_major_ * 10 + compute_capability_minor_;\n+    const int ptx_version = ptx_version_major_ * 10 + ptx_version_minor_;\n+    const bool enable_f8 = (ptx_version >= 78 && cc_version >= 90) ||\n+                           (ptx_version >= 81 && cc_version >= 89);\n+    const bool enable_f4 = (ptx_version >= 86 && cc_version >= 100);\n+\n     mlir::RewritePatternSet patterns(&getContext());\n-    patterns.add<RewriteTruncFPattern, RewriteExtFPattern>(&getContext());\n+    patterns.add<RewriteTruncFPattern>(&getContext(), enable_f8, enable_f4);\n+    patterns.add<RewriteExtFPattern>(&getContext(), enable_f8, enable_f4);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       signalPassFailure();\n@@ -249,8 +300,15 @@ class ConvertFloatNvidiaPass\n \n }  // namespace\n \n-std::unique_ptr<mlir::Pass> CreateConvertFloatNvidiaPass() {\n-  return std::make_unique<ConvertFloatNvidiaPass>();\n+std::unique_ptr<mlir::Pass> CreateConvertFloatNvidiaPass(\n+    int compute_capability_major, int compute_capability_minor,\n+    int ptx_version_major, int ptx_version_minor) {\n+  ConvertFloatNvidiaPassOptions options;\n+  options.compute_capability_major_ = compute_capability_major;\n+  options.compute_capability_minor_ = compute_capability_minor;\n+  options.ptx_version_major_ = ptx_version_major;\n+  options.ptx_version_minor_ = ptx_version_minor;\n+  return std::make_unique<ConvertFloatNvidiaPass>(options);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "5e3b87105c049c428d0ff5385831c1ddf3b4c318",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/passes.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.h?ref=868be00481100053d14ee79114d0337696d3e29b",
            "patch": "@@ -33,7 +33,9 @@ namespace gpu {\n #define GEN_PASS_DECL\n #include \"xla/backends/gpu/codegen/emitters/transforms/passes.h.inc\"\n \n-std::unique_ptr<mlir::Pass> CreateConvertFloatNvidiaPass();\n+std::unique_ptr<mlir::Pass> CreateConvertFloatNvidiaPass(\n+    int compute_capability_major = 0, int compute_capability_minor = 0,\n+    int ptx_version_major = 0, int ptx_version_minor = 0);\n std::unique_ptr<mlir::Pass> CreateConvertFloatAMDPass(\n     const std::string& gpu_device_info = \"\");\n std::unique_ptr<mlir::Pass> CreateConvertFloatAMDPass("
        },
        {
            "sha": "f12f8bc3c081c32c833f21e114ff053e5ec1f924",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/passes.td",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.td?ref=868be00481100053d14ee79114d0337696d3e29b",
            "patch": "@@ -43,6 +43,16 @@ def ConvertFloatNvidiaPass : Pass<\"xla-gpu-convert-float-nvidia\", \"mlir::ModuleO\n     \"mlir::LLVM::LLVMDialect\",\n     \"mlir::arith::ArithDialect\",\n   ];\n+  let options = [\n+    Option<\"compute_capability_major_\", \"compute_capability_major\", \"int\", /*default=*/\"0\",\n+           \"CUDA compute capability major version.\">,\n+    Option<\"compute_capability_minor_\", \"compute_capability_minor\", \"int\", /*default=*/\"0\",\n+           \"CUDA compute capability minor version.\">,\n+    Option<\"ptx_version_major_\", \"ptx_version_major\", \"int\", /*default=*/\"0\",\n+           \"PTX version major.\">,\n+    Option<\"ptx_version_minor_\", \"ptx_version_minor\", \"int\", /*default=*/\"0\",\n+           \"PTX version minor.\">\n+  ];\n   let constructor = \"CreateConvertFloatNvidiaPass()\";\n }\n "
        },
        {
            "sha": "df2248ec5b0f07a9742372a2f6094b3cc46bb5f9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/tests/convert_float_nvidia.mlir",
            "status": "modified",
            "additions": 48,
            "deletions": 1,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fconvert_float_nvidia.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/868be00481100053d14ee79114d0337696d3e29b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fconvert_float_nvidia.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fconvert_float_nvidia.mlir?ref=868be00481100053d14ee79114d0337696d3e29b",
            "patch": "@@ -1,4 +1,4 @@\n-// RUN: emitters_opt %s -split-input-file -xla-gpu-convert-float-nvidia -canonicalize | FileCheck %s\n+// RUN: emitters_opt %s -split-input-file -xla-gpu-convert-float-nvidia='compute_capability_major=10 compute_capability_minor=0 ptx_version_major=8 ptx_version_minor=6' -canonicalize | FileCheck %s\n \n module {\n   func.func @intr_f16_to_f8(%arg0: f16) -> (f8E4M3FN, f8E5M2) {\n@@ -152,3 +152,50 @@ module {\n // CHECK: %[[UPPER:.*]] = arith.cmpi ule, %[[VAL]], %c2139095040_i32\n // CHECK: %[[ISINF:.*]] = arith.andi %[[LOWER]], %[[UPPER]]\n // CHECK: arith.select %[[ISINF]], {{.*}}, %[[RES]]\n+\n+\n+// -----\n+\n+module {\n+  func.func @intr_f16_to_f4(%arg0: f16) -> f4E2M1FN {\n+    %a = arith.truncf %arg0 : f16 to f4E2M1FN\n+    return %a : f4E2M1FN\n+  }\n+}\n+\n+// CHECK-LABEL: @intr_f16_to_f4\n+// CHECK: arith.extf %{{.+}} : f16 to f32\n+// CHECK: llvm.call_intrinsic \"llvm.nvvm.ff.to.e2m1x2.rn.satfinite\"\n+// CHECK: llvm.trunc %{{.+}} : i16 to i4\n+// CHECK: arith.bitcast %{{.+}} : i4 to f4E2M1FN\n+\n+// -----\n+\n+module {\n+  func.func @intr_f4_to_f32(%arg0: f4E2M1FN) -> f32 {\n+    %a = arith.extf %arg0 : f4E2M1FN to f32\n+    return %a : f32\n+  }\n+}\n+\n+// CHECK-LABEL: @intr_f4_to_f32\n+// CHECK: arith.bitcast %{{.+}} : f4E2M1FN to i4\n+// CHECK: llvm.zext %{{.+}} : i4 to i16\n+// CHECK: llvm.call_intrinsic \"llvm.nvvm.e2m1x2.to.f16x2.rn\"\n+// CHECK: llvm.extractelement\n+// CHECK: arith.extf %{{.+}} : f16 to f32\n+\n+// -----\n+\n+// RUN: emitters_opt %s -split-input-file -xla-gpu-convert-float-nvidia='compute_capability_major=9 compute_capability_minor=0 ptx_version_major=8 ptx_version_minor=6' -canonicalize | FileCheck %s --check-prefix=CHECK-NO-F4\n+\n+module {\n+  func.func @no_intr_f32_to_f4(%arg0: f32) -> f4E2M1FN {\n+    %a = arith.truncf %arg0 : f32 to f4E2M1FN\n+    return %a : f4E2M1FN\n+  }\n+}\n+\n+// CHECK-NO-F4-LABEL: @no_intr_f32_to_f4\n+// CHECK-NO-F4-NOT: llvm.nvvm.ff.to.e2m1x2.rn\n+// CHECK-NO-F4: arith.truncf %{{.+}} : f32 to f4E2M1FN"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 175,
        "deletions": 52
    }
}