{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Introduce `CollectiveOpsE2ETestBase` for common functionality.\n\nWe need a proper base class for common functionality. `CollectiveOpsTestE2E` is not a good base class, because it also holds a lot of fp8-specific helpers that are only used in a few tests.\n\nPiperOrigin-RevId: 826017075",
    "sha": "fd71e8be059052e95c1c0c1d9d225d77366fc5dc",
    "files": [
        {
            "sha": "c2aba137cc57eb5312484c78c12b87753abc983a",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 58,
            "changes": 123,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fd71e8be059052e95c1c0c1d9d225d77366fc5dc/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fd71e8be059052e95c1c0c1d9d225d77366fc5dc/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=fd71e8be059052e95c1c0c1d9d225d77366fc5dc",
            "patch": "@@ -112,9 +112,16 @@ Type CheckStatus(absl::StatusOr<Type> result) {\n   return *result;\n }\n \n-class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n+bool IsAsync(const HloInstruction* inst) {\n+  return !inst->backend_config<gpu::GpuBackendConfig>()\n+              .value()\n+              .collective_backend_config()\n+              .is_sync();\n+}\n+\n+class CollectiveOpsE2ETestBase : public HloHardwareIndependentTestBase {\n  public:\n-  CollectiveOpsTestE2E() {\n+  CollectiveOpsE2ETestBase() {\n     se::Platform* platform = CheckStatus(PlatformUtil::GetPlatform(\"GPU\"));\n     se::Platform* reference_platform =\n         CheckStatus(PlatformUtil::GetPlatform(\"GPU\"));\n@@ -144,54 +151,6 @@ class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n                                                  std::move(allocators)));\n     reference_hlo_runner_ = std::make_unique<HloRunner>(\n         reference_platform, /*intra_op_parallelism_threads=*/0);\n-\n-    replacements_[kF8E4M3DatatypePlaceholder] =\n-        Capability().IsCuda() ? \"f8e4m3fn\" : \"f8e4m3fnuz\";\n-    replacements_[kF8E5M2DatatypePlaceholder] =\n-        Capability().IsCuda() ? \"f8e5m2\" : \"f8e5m2fnuz\";\n-  }\n-\n-  const se::GpuComputeCapability& Capability() {\n-    return hlo_runner_->backend()\n-        .default_stream_executor()\n-        ->GetDeviceDescription()\n-        .gpu_compute_capability();\n-  }\n-\n-  bool HasFp8Support() {\n-    if (Capability().IsCuda()) {\n-      return Capability().cuda_compute_capability()->IsAtLeast(8, 9);\n-    }\n-    return Capability().rocm_compute_capability()->has_fp8_support() &&\n-           GetDebugOptionsForTest().xla_gpu_enable_cublaslt();\n-  }\n-\n-  void CollectiveOpsVerifyF8Matmul(absl::string_view hlo_text,\n-                                   const DebugOptions& options) {\n-    if (!HasFp8Support()) {\n-      return;\n-    }\n-    const int64_t kNumReplicas = 1;\n-    const int64_t kNumPartitions = 4;\n-\n-    HloModuleConfig config =\n-        GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-    config.set_debug_options(options);\n-    config.set_num_partitions(kNumPartitions);\n-    TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                            ParseAndReturnVerifiedModule(hlo_text, config));\n-\n-    TF_ASSERT_OK_AND_ASSIGN(auto executable, hlo_runner_->CreateExecutable(\n-                                                 std::move(module),\n-                                                 /*run_hlo_passes=*/true));\n-    TF_ASSERT_OK_AND_ASSIGN(\n-        const HloModule* const hlo_module,\n-        hlo_runner_->HloModuleFromWrapped(executable.get()));\n-    std::vector<HloInstruction*> gemm_ops =\n-        FindInstructions(hlo_module, HloOpcode::kCustomCall);\n-    for (HloInstruction* gemm_op : gemm_ops) {\n-      EXPECT_EQ(gemm_op->custom_call_target(), \"__cublas$lt$matmul$f8\");\n-    }\n   }\n \n   // TODO(b/449655621) Use absl::AnyInvocable instead of std::function.\n@@ -263,17 +222,65 @@ class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n         num_replicas, /*run_hlo_passes=*/false, &device_assignment);\n   }\n \n-  bool IsAsync(const HloInstruction* inst) {\n-    return !inst->backend_config<gpu::GpuBackendConfig>()\n-                .value()\n-                .collective_backend_config()\n-                .is_sync();\n+ protected:\n+  std::unique_ptr<HloRunner> hlo_runner_;\n+  std::unique_ptr<HloRunner> reference_hlo_runner_;\n+};\n+\n+class CollectiveOpsTestE2E : public CollectiveOpsE2ETestBase {\n+ public:\n+  CollectiveOpsTestE2E() {\n+    replacements_[kF8E4M3DatatypePlaceholder] =\n+        Capability().IsCuda() ? \"f8e4m3fn\" : \"f8e4m3fnuz\";\n+    replacements_[kF8E5M2DatatypePlaceholder] =\n+        Capability().IsCuda() ? \"f8e5m2\" : \"f8e5m2fnuz\";\n+  }\n+\n+  const se::GpuComputeCapability& Capability() {\n+    return hlo_runner_->backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability();\n+  }\n+\n+  bool HasFp8Support() {\n+    if (Capability().IsCuda()) {\n+      return Capability().cuda_compute_capability()->IsAtLeast(8, 9);\n+    }\n+    return Capability().rocm_compute_capability()->has_fp8_support() &&\n+           GetDebugOptionsForTest().xla_gpu_enable_cublaslt();\n+  }\n+\n+  void CollectiveOpsVerifyF8Matmul(absl::string_view hlo_text,\n+                                   const DebugOptions& options) {\n+    if (!HasFp8Support()) {\n+      return;\n+    }\n+    const int64_t kNumReplicas = 1;\n+    const int64_t kNumPartitions = 4;\n+\n+    HloModuleConfig config =\n+        GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n+    config.set_debug_options(options);\n+    config.set_num_partitions(kNumPartitions);\n+    TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                            ParseAndReturnVerifiedModule(hlo_text, config));\n+\n+    TF_ASSERT_OK_AND_ASSIGN(auto executable, hlo_runner_->CreateExecutable(\n+                                                 std::move(module),\n+                                                 /*run_hlo_passes=*/true));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        const HloModule* const hlo_module,\n+        hlo_runner_->HloModuleFromWrapped(executable.get()));\n+    std::vector<HloInstruction*> gemm_ops =\n+        FindInstructions(hlo_module, HloOpcode::kCustomCall);\n+    for (HloInstruction* gemm_op : gemm_ops) {\n+      EXPECT_EQ(gemm_op->custom_call_target(), \"__cublas$lt$matmul$f8\");\n+    }\n   }\n \n  protected:\n   absl::flat_hash_map<absl::string_view, absl::string_view> replacements_;\n-  std::unique_ptr<HloRunner> hlo_runner_;\n-  std::unique_ptr<HloRunner> reference_hlo_runner_;\n \n  private:\n   static constexpr const char* kF8E4M3DatatypePlaceholder{\"<<F8E4M3>>\"};\n@@ -287,7 +294,7 @@ class CollectiveOpsTestE2E : public HloHardwareIndependentTestBase {\n // E2E test for collectives with flags set. Has constructor arguments specifying\n // whether to enable/disable async collectives, and to set the memcpy_local_p2p\n // flag. Subclasses pass in constructor arguments based on GetParam().\n-class CollectiveOpsWithFlagsBase : public CollectiveOpsTestE2E {\n+class CollectiveOpsWithFlagsBase : public CollectiveOpsE2ETestBase {\n  public:\n   CollectiveOpsWithFlagsBase(bool enable_async, bool enable_p2p_memcpy)\n       : enable_async_(enable_async), enable_p2p_memcpy_(enable_p2p_memcpy) {"
        }
    ],
    "stats": {
        "total": 123,
        "additions": 65,
        "deletions": 58
    }
}