{
    "author": "bhatuzdaname",
    "message": "Support for running Transformer Engine HLO benchmarks.\n\nThis change introduces a new Python test (`python_hlo_runner_test.py`) that uses the `py_hlo_multihost_runner` bindings to execute HLO files containing custom calls from Transformer Engine JAX. It registers the necessary custom call targets before running the HLO.\n\nPiperOrigin-RevId: 828159624",
    "sha": "1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
    "files": [
        {
            "sha": "7472bed6cfe77e31b36486fa889a21f0b276da13",
            "filename": "third_party/xla/third_party/nvshmem/nvshmem.BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Fnvshmem%2Fnvshmem.BUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Fnvshmem%2Fnvshmem.BUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fnvshmem%2Fnvshmem.BUILD?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -1,5 +1,6 @@\n # NVSHMEM\n \n+load(\"@bazel_skylib//rules:copy_file.bzl\", \"copy_file\")\n load(\"@bazel_skylib//rules:expand_template.bzl\", \"expand_template\")\n load(\"@bazel_skylib//rules:write_file.bzl\", \"write_file\")\n \n@@ -67,12 +68,19 @@ expand_template(\n     template = \"src/include/non_abi/nvshmem_version.h.in\",\n )\n \n+copy_file(\n+    name = \"nvshmem_transfer_device_cuh\",\n+    src = \"src/include/non_abi/device/pt-to-pt/transfer_device.cuh.in\",\n+    out = \"src/include/non_abi/device/pt-to-pt/transfer_device.cuh\",\n+)\n+\n cc_library(\n     name = \"nvshmem_lib\",\n     hdrs = glob([\n         \"src/include/**\",\n     ]) + [\n         \":nvshmem_build_options_h\",\n+        \":nvshmem_transfer_device_cuh\",\n         \":nvshmem_version_h\",\n     ],\n     include_prefix = \"third_party/nvshmem\","
        },
        {
            "sha": "fe1959f93a952c7bb502494f70017b00ba525d77",
            "filename": "third_party/xla/third_party/transformer_engine/BUILD",
            "status": "added",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2FBUILD?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -0,0 +1,3 @@\n+# copybara:uncomment package(default_applicable_licenses = [\"//third_party/tensorflow:license\"])\n+\n+exports_files([\"codegen.py\"])"
        },
        {
            "sha": "a0daa028cb4b70801cdbd5af4a1baf4314ea455c",
            "filename": "third_party/xla/third_party/transformer_engine/codegen.py",
            "status": "added",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Fcodegen.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Fcodegen.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Fcodegen.py?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -0,0 +1,41 @@\n+# Copyright 2025 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Codegen script for Transformer Engine.\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+_TEMPLATE_FILE = flags.DEFINE_string(\n+    'template_file', None, 'Path to the template file.', required=True\n+)\n+_DATA_FILE = flags.DEFINE_string(\n+    'data_file', None, 'Path to the data file.', required=True\n+)\n+_STRING_NAME = flags.DEFINE_string(\n+    'string_name', None, 'String name to use in the template.', required=True\n+)\n+\n+\n+def main(_):\n+  with open(_TEMPLATE_FILE.value, 'rt') as f, open(_DATA_FILE.value, 'rt') as g:\n+    template = f.read()\n+    data = g.read()\n+  template = template.replace('@STRING_NAME@', _STRING_NAME.value)\n+  template = template.replace('@STRING@', data)\n+  print(template)\n+\n+\n+if __name__ == '__main__':\n+  app.run(main)"
        },
        {
            "sha": "5d9abf6740134629a1c41cf7ba78d189da36c143",
            "filename": "third_party/xla/third_party/transformer_engine/transformer_engine.BUILD",
            "status": "added",
            "additions": 249,
            "deletions": 0,
            "changes": 249,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Ftransformer_engine.BUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Ftransformer_engine.BUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Ftransformer_engine.BUILD?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -0,0 +1,249 @@\n+load(\"@bazel_skylib//rules:expand_template.bzl\", \"expand_template\")\n+load(\"@local_config_cuda//cuda:build_defs.bzl\", \"cuda_library\")\n+load(\"@local_xla//third_party/py/rules_pywrap:pywrap.impl.bzl\", \"python_extension\", \"pywrap_library\")\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"@rules_python//python:py_binary.bzl\", \"py_binary\")\n+\n+package(\n+    default_visibility = [\"//visibility:public\"],\n+    features = [\n+        \"-header_modules\",\n+        \"-use_header_modules\",\n+    ],\n+    licenses = [\"notice\"],\n+)\n+\n+py_binary(\n+    name = \"codegen\",\n+    srcs = [\"@local_xla//third_party/transformer_engine:codegen.py\"],\n+    deps = [\n+        \"@absl_py//absl:app\",\n+        \"@absl_py//absl/flags\",\n+    ],\n+)\n+\n+genrule(\n+    name = \"make_string_code_utils_cuh\",\n+    srcs = [\n+        \"transformer_engine/common/util/string_header.h.in\",\n+        \"transformer_engine/common/utils.cuh\",\n+    ],\n+    outs = [\"string_headers/string_code_utils_cuh.h\"],\n+    cmd = \"$(location :codegen) --template_file=$(location transformer_engine/common/util/string_header.h.in) --data_file=$(location transformer_engine/common/utils.cuh) --string_name=string_code_utils_cuh > $@\",\n+    tools = [\":codegen\"],\n+)\n+\n+genrule(\n+    name = \"make_string_code_util_math_h\",\n+    srcs = [\n+        \"transformer_engine/common/util/string_header.h.in\",\n+        \"transformer_engine/common/util/math.h\",\n+    ],\n+    outs = [\"string_headers/string_code_util_math_h.h\"],\n+    cmd = \"$(location :codegen) --template_file=$(location transformer_engine/common/util/string_header.h.in) --data_file=$(location transformer_engine/common/util/math.h) --string_name=string_code_util_math_h > $@\",\n+    tools = [\":codegen\"],\n+)\n+\n+genrule(\n+    name = \"make_string_code_transpose_rtc_transpose_cu\",\n+    srcs = [\n+        \"transformer_engine/common/util/string_header.h.in\",\n+        \"transformer_engine/common/transpose/rtc/transpose.cu\",\n+    ],\n+    outs = [\"string_headers/string_code_transpose_rtc_transpose_cu.h\"],\n+    cmd = \"$(location :codegen) --template_file=$(location transformer_engine/common/util/string_header.h.in) --data_file=$(location transformer_engine/common/transpose/rtc/transpose.cu) --string_name=string_code_transpose_rtc_transpose_cu > $@\",\n+    tools = [\":codegen\"],\n+)\n+\n+genrule(\n+    name = \"make_string_code_transpose_rtc_cast_transpose_cu\",\n+    srcs = [\n+        \"transformer_engine/common/util/string_header.h.in\",\n+        \"transformer_engine/common/transpose/rtc/cast_transpose.cu\",\n+    ],\n+    outs = [\"string_headers/string_code_transpose_rtc_cast_transpose_cu.h\"],\n+    cmd = \"$(location :codegen) --template_file=$(location transformer_engine/common/util/string_header.h.in) --data_file=$(location transformer_engine/common/transpose/rtc/cast_transpose.cu) --string_name=string_code_transpose_rtc_cast_transpose_cu > $@\",\n+    tools = [\":codegen\"],\n+)\n+\n+genrule(\n+    name = \"make_string_code_transpose_rtc_cast_transpose_fusion_cu\",\n+    srcs = [\n+        \"transformer_engine/common/util/string_header.h.in\",\n+        \"transformer_engine/common/transpose/rtc/cast_transpose_fusion.cu\",\n+    ],\n+    outs = [\"string_headers/string_code_transpose_rtc_cast_transpose_fusion_cu.h\"],\n+    cmd = \"$(location :codegen) --template_file=$(location transformer_engine/common/util/string_header.h.in) --data_file=$(location transformer_engine/common/transpose/rtc/cast_transpose_fusion.cu) --string_name=string_code_transpose_rtc_cast_transpose_fusion_cu > $@\",\n+    tools = [\":codegen\"],\n+)\n+\n+cc_library(\n+    name = \"string_headers\",\n+    hdrs = [\n+        \"string_headers/string_code_transpose_rtc_cast_transpose_cu.h\",\n+        \"string_headers/string_code_transpose_rtc_cast_transpose_fusion_cu.h\",\n+        \"string_headers/string_code_transpose_rtc_transpose_cu.h\",\n+        \"string_headers/string_code_util_math_h.h\",\n+        \"string_headers/string_code_utils_cuh.h\",\n+    ],\n+    includes = [\"string_headers\"],\n+)\n+\n+UNSUPPORTED_ARCHITECTURES_FLAGS = [\n+    \"--no-cuda-gpu-arch=sm_50\",\n+    \"--no-cuda-gpu-arch=sm_60\",\n+]\n+\n+cuda_library(\n+    name = \"nvshmem_api\",\n+    srcs = [\n+        \"transformer_engine/common/nvshmem_api/nvshmem_waitkernel.cu\",\n+        \"transformer_engine/common/util/logging.h\",\n+        \"transformer_engine/common/util/string.h\",\n+    ],\n+    hdrs = [\"transformer_engine/common/nvshmem_api/nvshmem_waitkernel.h\"],\n+    copts = [\n+        \"-fexceptions\",\n+    ] + UNSUPPORTED_ARCHITECTURES_FLAGS,\n+    includes = [\n+        \"transformer_engine\",\n+        \"transformer_engine/common\",\n+        \"transformer_engine/common/include\",\n+        \"transformer_engine/common/include/transformer_engine\",\n+    ],\n+    local_defines = [\"NVSHMEM_ENABLE_ALL_DEVICE_INLINING\"],\n+    deps = [\n+        \"@local_config_cuda//cuda:cublas\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_config_cuda//cuda:cudnn_header\",\n+        \"@local_config_cuda//cuda:nvrtc_headers\",\n+        \"@nvshmem//:nvshmem_lib\",\n+    ],\n+)\n+\n+cuda_library(\n+    name = \"common_lib\",\n+    srcs = glob(\n+        [\n+            \"transformer_engine/common/**/*.cc\",\n+            \"transformer_engine/common/**/*.cpp\",\n+            \"transformer_engine/common/**/*.cu\",\n+        ],\n+        exclude = [\n+            \"transformer_engine/common/permutation/permutation.cu\",\n+            \"transformer_engine/common/transpose/rtc/transpose.cu\",\n+            \"transformer_engine/common/transpose/rtc/cast_transpose.cu\",\n+            \"transformer_engine/common/transpose/rtc/cast_transpose_fusion.cu\",\n+            \"transformer_engine/common/nvshmem_api/nvshmem_waitkernel.cu\",\n+        ],\n+    ),\n+    hdrs = glob([\n+        \"transformer_engine/common/**/*.cuh\",\n+        \"transformer_engine/common/**/*.h\",\n+    ]),\n+    copts = [\n+        \"-fexceptions\",\n+        \"-Wno-logical-op-parentheses\",\n+        \"-Wno-missing-braces\",\n+        \"-Wno-pass-failed\",\n+        \"-Wno-reorder-ctor\",\n+        \"-Wno-unused-variable\",\n+        \"-Wno-switch\",\n+        \"-Wno-exceptions\",\n+        \"-Wno-assume\",\n+        \"-Wno-self-assign\",\n+        \"-Wno-sometimes-uninitialized\",\n+    ] + UNSUPPORTED_ARCHITECTURES_FLAGS,\n+    data = [\"@local_config_cuda//cuda:cuda_headers\"],\n+    includes = [\n+        \"transformer_engine\",\n+        \"transformer_engine/common\",\n+        \"transformer_engine/common/include\",\n+        \"transformer_engine/common/include/transformer_engine\",\n+        \"transformer_engine/common/layer_norm\",\n+    ],\n+    deps = [\n+        \":nvshmem_api\",\n+        \":string_headers\",\n+        \"@com_google_absl//absl/log\",\n+        \"@cuda_nccl//:nccl\",\n+        \"@cudnn_frontend_archive//:cudnn_frontend\",\n+        \"@cutlass_archive//:cutlass\",\n+        \"@local_config_cuda//cuda\",\n+        \"@local_config_cuda//cuda:cublas\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_config_cuda//cuda:cudart\",\n+        \"@local_config_cuda//cuda:cudnn\",\n+        \"@local_config_cuda//cuda:cufft\",\n+        \"@local_config_cuda//cuda:cusparse\",\n+        \"@local_config_cuda//cuda:nvrtc_headers\",\n+        \"@local_tsl//tsl/platform:cuda_root_path\",\n+        \"@local_xla//xla/ffi/api:ffi\",\n+        \"@pybind11\",\n+    ],\n+)\n+\n+cuda_library(\n+    name = \"transformer_engine_jax_utils\",\n+    srcs = [\"transformer_engine/jax/csrc/extensions/utils.cpp\"],\n+    hdrs = [\"transformer_engine/jax/csrc/extensions/utils.h\"],\n+    copts = [\n+        \"-fexceptions\",\n+    ] + UNSUPPORTED_ARCHITECTURES_FLAGS,\n+    includes = [\n+        \"transformer_engine\",\n+    ],\n+    deps = [\n+        \":common_lib\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@pybind11\",\n+    ],\n+)\n+\n+python_extension(\n+    name = \"transformer_engine_jax_extension\",\n+    srcs = glob(\n+        [\n+            \"transformer_engine/jax/csrc/*.cpp\",\n+            \"transformer_engine/jax/csrc/*.h\",\n+            \"transformer_engine/jax/csrc/*/*.h\",\n+            \"transformer_engine/jax/csrc/*/*.cpp\",\n+        ],\n+        exclude = [\n+            \"transformer_engine/jax/csrc/extensions/utils.h\",\n+            \"transformer_engine/jax/csrc/extensions/utils.cpp\",\n+        ],\n+    ),\n+    copts = [\n+        \"-fexceptions\",\n+        \"-Wno-unused-variable\",\n+        \"-Wno-c++11-narrowing\",\n+    ],\n+    includes = [\n+        \"transformer_engine\",\n+        \"transformer_engine/jax/csrc\",\n+        \"transformer_engine/jax/csrc/extensions\",\n+    ],\n+    visibility = [\n+        \"//visibility:public\",\n+    ],\n+    deps = [\n+        \":common_lib\",\n+        \":transformer_engine_jax_utils\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_config_cuda//cuda:cudnn\",\n+        \"@local_xla//xla/ffi/api:c_api\",\n+        \"@local_xla//xla/ffi/api:ffi\",\n+        \"@pybind11\",\n+    ],\n+)\n+\n+pywrap_library(\n+    name = \"transformer_engine_jax\",\n+    visibility = [\n+        \"//visibility:public\",\n+    ],\n+    deps = [\n+        \":transformer_engine_jax_extension\",\n+    ],\n+)"
        },
        {
            "sha": "4f23d58df7cf7effda09475b5453439db7e8714c",
            "filename": "third_party/xla/third_party/transformer_engine/transformer_engine.patch",
            "status": "added",
            "additions": 158,
            "deletions": 0,
            "changes": 158,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Ftransformer_engine.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Ftransformer_engine.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Ftransformer_engine.patch?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -0,0 +1,158 @@\n+diff --git a/transformer_engine/common/common.cu b/transformer_engine/common/common.cu\n+index 192c915..e94a92b 100644\n+--- a/transformer_engine/common/common.cu\n++++ b/transformer_engine/common/common.cu\n+@@ -6,7 +6,6 @@\n+\n+ #include <transformer_engine/transformer_engine.h>\n+\n+-#include <bit>\n+\n+ #include \"./common.h\"\n+ #include \"./utils.cuh\"\n+diff --git a/transformer_engine/common/cudnn_utils.h b/transformer_engine/common/cudnn_utils.h\n+index 0016ad7..985a9ba 100644\n+--- a/transformer_engine/common/cudnn_utils.h\n++++ b/transformer_engine/common/cudnn_utils.h\n+@@ -8,8 +8,8 @@\n+ #define TRANSFORMER_ENGINE_CUDNN_UTILS_H_\n+\n+ #include <cudnn.h>\n+-#include <cudnn_frontend.h>\n+-#include <cudnn_frontend_utils.h>\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend.h\"\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend_utils.h\"\n+ #include <cudnn_graph.h>\n+\n+ #include \"transformer_engine/transformer_engine.h\"\n+diff --git a/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu b/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu\n+index 0932b2c..7fd2a94 100644\n+--- a/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu\n++++ b/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu\n+@@ -6,8 +6,8 @@\n+\n+ #include <cuda_bf16.h>\n+ #include <cuda_fp16.h>\n+-#include <cudnn_frontend.h>\n+-#include <cudnn_frontend_utils.h>\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend.h\"\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend_utils.h\"\n+\n+ #include <map>\n+ #include <vector>\n+diff --git a/transformer_engine/common/fused_attn/fused_attn_f16_max512_seqlen.cu b/transformer_engine/common/fused_attn/fused_attn_f16_max512_seqlen.cu\n+index 89528fa..9f53123 100644\n+--- a/transformer_engine/common/fused_attn/fused_attn_f16_max512_seqlen.cu\n++++ b/transformer_engine/common/fused_attn/fused_attn_f16_max512_seqlen.cu\n+@@ -6,7 +6,7 @@\n+\n+ #include <cuda_bf16.h>\n+ #include <cuda_fp16.h>\n+-#include <cudnn_frontend.h>\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend.h\"\n+\n+ #include <map>\n+ #include <vector>\n+diff --git a/transformer_engine/common/fused_attn/utils.h b/transformer_engine/common/fused_attn/utils.h\n+index 678b636..58bcec4 100644\n+--- a/transformer_engine/common/fused_attn/utils.h\n++++ b/transformer_engine/common/fused_attn/utils.h\n+@@ -8,8 +8,8 @@\n+ #define TRANSFORMER_ENGINE_FUSED_ATTN_UTILS_H_\n+\n+ #include <cudnn.h>\n+-#include <cudnn_frontend.h>\n+-#include <cudnn_frontend_utils.h>\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend.h\"\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend_utils.h\"\n+\n+ #include <cstdint>\n+ #include <mutex>\n+diff --git a/transformer_engine/common/normalization/common.h b/transformer_engine/common/normalization/common.h\n+index 0ec1604..aef319d 100644\n+--- a/transformer_engine/common/normalization/common.h\n++++ b/transformer_engine/common/normalization/common.h\n+@@ -8,8 +8,8 @@\n+ #define TRANSFORMER_ENGINE_COMMON_NORM_COMMON_H_\n+\n+ #include <cudnn.h>\n+-#include <cudnn_frontend.h>\n+-#include <cudnn_frontend_utils.h>\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend.h\"\n++#include \"third_party/cudnn_frontend/include/cudnn_frontend_utils.h\"\n+ #include <transformer_engine/normalization.h>\n+ #include <transformer_engine/transformer_engine.h>\n+\n+diff --git a/transformer_engine/common/util/cuda_runtime.cpp b/transformer_engine/common/util/cuda_runtime.cpp\n+index ac1bb1b..febb9a1 100644\n+--- a/transformer_engine/common/util/cuda_runtime.cpp\n++++ b/transformer_engine/common/util/cuda_runtime.cpp\n+@@ -13,18 +13,12 @@\n+ #include \"../util/cuda_driver.h\"\n+ #include \"../util/system.h\"\n+ #include \"common/util/cuda_runtime.h\"\n++#include \"tsl/platform/cuda_root_path.h\"\n+\n+ namespace transformer_engine {\n+\n+ namespace cuda {\n+\n+-namespace {\n+-\n+-// String with build-time CUDA include path\n+-#include \"string_path_cuda_include.h\"\n+-\n+-}  // namespace\n+-\n+ int num_devices() {\n+   auto query_num_devices = []() -> int {\n+     int count;\n+@@ -146,8 +140,10 @@ const std::string &include_directory(bool required) {\n+     std::vector<std::pair<std::string, Path>> search_paths = {{\"NVTE_CUDA_INCLUDE_DIR\", \"\"},\n+                                                               {\"CUDA_HOME\", \"\"},\n+                                                               {\"CUDA_DIR\", \"\"},\n+-                                                              {\"\", string_path_cuda_include},\n+                                                               {\"\", \"/usr/local/cuda\"}};\n++    for (auto &candidate : tsl::CandidateCudaRoots()) {\n++      search_paths.push_back({\"\", candidate});\n++    }\n+     for (auto &[env, p] : search_paths) {\n+       if (p.empty()) {\n+         p = getenv<Path>(env.c_str());\n+diff --git a/transformer_engine/jax/csrc/extensions/attention.cpp b/transformer_engine/jax/csrc/extensions/attention.cpp\n+index 40089dc..cf256e3 100644\n+--- a/transformer_engine/jax/csrc/extensions/attention.cpp\n++++ b/transformer_engine/jax/csrc/extensions/attention.cpp\n+@@ -5,6 +5,7 @@\n+  ************************************************************************/\n+\n+ #include \"../extensions.h\"\n++#include <cudnn_graph.h>\n+ #include \"transformer_engine/fused_attn.h\"\n+ #include \"transformer_engine/transformer_engine.h\"\n+\n+diff --git a/transformer_engine/jax/csrc/extensions/ffi.h b/transformer_engine/jax/csrc/extensions/ffi.h\n+index 852a67c..92df9de 100644\n+--- a/transformer_engine/jax/csrc/extensions/ffi.h\n++++ b/transformer_engine/jax/csrc/extensions/ffi.h\n+@@ -5,7 +5,7 @@\n+  ************************************************************************/\n+\n+ #include <transformer_engine/transformer_engine.h>\n+-#include <xla/ffi/api/ffi.h>\n++#include \"xla/ffi/api/ffi.h\"\n+\n+ #include <numeric>\n+\n+diff --git a/transformer_engine/jax/csrc/extensions/utils.cpp b/transformer_engine/jax/csrc/extensions/utils.cpp\n+index 3ba0737..10b72c8 100644\n+--- a/transformer_engine/jax/csrc/extensions/utils.cpp\n++++ b/transformer_engine/jax/csrc/extensions/utils.cpp\n+@@ -6,6 +6,7 @@\n+ #include \"utils.h\"\n+\n+ #include <cuda_runtime_api.h>\n++#include <cudnn_graph.h>\n+\n+ #include <cassert>\n+"
        },
        {
            "sha": "e12528aa70fd3dea7993d3eb20e3fd1a11976599",
            "filename": "third_party/xla/third_party/transformer_engine/workspace.bzl",
            "status": "added",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftransformer_engine%2Fworkspace.bzl?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -0,0 +1,13 @@\n+\"\"\"Loads the TransformerEngine library.\"\"\"\n+\n+load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n+\n+def repo():\n+    tf_http_archive(\n+        name = \"transformer_engine\",\n+        strip_prefix = \"TransformerEngine-2.5\",\n+        sha256 = \"ee52ee9e43e44edc8598bc3d111eedc2445c9ebfe78a1fcab6f5c4c887020b72\",\n+        urls = tf_mirror_urls(\"https://github.com/NVIDIA/TransformerEngine/archive/refs/tags/v2.5.tar.gz\"),\n+        build_file = \"//third_party/transformer_engine:transformer_engine.BUILD\",\n+        patch_file = [\"//third_party/transformer_engine:transformer_engine.patch\"],\n+    )"
        },
        {
            "sha": "5a4ff6e3fd69e97e36d21bc8f321e183175cdb0b",
            "filename": "third_party/xla/workspace2.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fworkspace2.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fworkspace2.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fworkspace2.bzl?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -50,6 +50,7 @@ load(\"//third_party/spdlog:workspace.bzl\", spdlog = \"repo\")\n load(\"//third_party/stablehlo:workspace.bzl\", stablehlo = \"repo\")\n load(\"//third_party/tensorrt:tensorrt_configure.bzl\", \"tensorrt_configure\")\n load(\"//third_party/tensorrt:workspace.bzl\", tensorrt = \"repo\")\n+load(\"//third_party/transformer_engine:workspace.bzl\", transformer_engine = \"repo\")\n load(\"//third_party/triton:workspace.bzl\", triton = \"repo\")\n load(\"//third_party/uv:workspace.bzl\", uv = \"repo\")\n load(\"//third_party/xnnpack:workspace.bzl\", xnnpack = \"repo\")\n@@ -99,6 +100,7 @@ def _initialize_third_party():\n     spdlog()\n     stablehlo()\n     tensorrt()\n+    transformer_engine()\n     triton()\n     uv()\n     xnnpack()"
        },
        {
            "sha": "652e8944c6793c289d1213c8d3cb28596c4a1d55",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/BUILD",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -1,7 +1,7 @@\n load(\"@bazel_skylib//rules:build_test.bzl\", \"build_test\")\n load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm\")\n-load(\"//xla:xla.default.bzl\", \"xla_cc_binary\")\n+load(\"//xla:xla.default.bzl\", \"xla_cc_binary\", \"xla_py_strict_test\")\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_cuda_or_rocm\", \"if_google\")\n load(\"//xla/tsl:tsl.default.bzl\", \"tsl_pybind_extension\")\n@@ -339,3 +339,26 @@ tsl_pybind_extension(\n         \"//xla/stream_executor:rocm_platform\",\n     ]),\n )\n+\n+xla_py_strict_test(\n+    name = \"python_hlo_runner_test\",\n+    srcs = [\"python_hlo_runner_test.py\"],\n+    data = [\n+        \":hlo_file\",\n+    ],\n+    # Transformer engine dlopens several cuda libraries and so requires them as data dependencies.\n+    need_cuda_libs = True,\n+    tags = [\n+        \"gpu\",\n+        # Transformer engine takes a long time to compile. Disabling it for CI tests.\n+        \"no_oss\",\n+        \"requires-gpu-sm90-only\",\n+    ],\n+    deps = [\n+        \":py_hlo_multihost_runner\",\n+        \"@absl_py//absl/testing:absltest\",\n+        \"@transformer_engine//:transformer_engine_jax\",\n+    ] + if_cuda([\n+        \"//xla/stream_executor:cuda_platform\",\n+    ]),\n+)"
        },
        {
            "sha": "d95bf065ebe051e293b59b447591fbfdaba3e2f7",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/data/transformer_engine_softmax.hlo",
            "status": "added",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fdata%2Ftransformer_engine_softmax.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fdata%2Ftransformer_engine_softmax.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fdata%2Ftransformer_engine_softmax.hlo?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -0,0 +1,7 @@\n+HloModule f\n+\n+ENTRY f {\n+  // Softmax custom call requires rank 4 input.\n+  %arg1 = bf16[4,1,16,32] parameter(0)\n+  ROOT %custom-call = bf16[4,1,16,32] custom-call(bf16[4,1,16,32] %arg1), custom_call_target=\"te_scaled_softmax_forward_ffi\", api_version=API_VERSION_TYPED_FFI, backend_config={scale_factor = 0.200000e+00 : f64}\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "d2d022ad7c22ef97933ecc1626594165baf020d5",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/python_hlo_runner.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner.cc?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -337,7 +337,9 @@ absl::Status RegisterCustomTypeId(absl::string_view type_name,\n }\n \n NB_MODULE(py_hlo_multihost_runner, m) {\n-  InitializeAbslLogging();\n+#ifndef PLATFORM_GOOGLE\n+  xla::InitializeAbslLogging();\n+#endif  // PLATFORM_GOOGLE\n \n   m.def(\"RunHloFiles\", ThrowIfErrorWrapper(RunHloFiles));\n   m.def("
        },
        {
            "sha": "91fcc20f443d955838dada6c3a7a45fbd277ccf8",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/python_hlo_runner_test.py",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner_test.py?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -0,0 +1,67 @@\n+# Copyright 2025 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for running HLO files.\"\"\"\n+\n+import os\n+import pathlib\n+\n+from absl.testing import absltest\n+from transformer_engine import transformer_engine_jax\n+\n+from xla.tools.multihost_hlo_runner import py_hlo_multihost_runner\n+\n+\n+def _register_transformer_engine_custom_calls():\n+  for name, value in transformer_engine_jax.registrations().items():\n+    py_hlo_multihost_runner.register_custom_call_target(\n+        name, value, platform=\"CUDA\", api_version=1\n+    )\n+\n+\n+def _get_test_hlo_path(file_name: str) -> str:\n+  \"\"\"Returns the path to a HLO file in the data directory.\"\"\"\n+  test_srcdir = pathlib.Path(os.environ[\"TEST_SRCDIR\"])\n+  test_workspace = os.environ[\"TEST_WORKSPACE\"]\n+  test_binary = os.environ[\"TEST_BINARY\"]\n+  return os.path.join(\n+      os.path.dirname(test_srcdir / test_workspace / test_binary),\n+      \"data\",\n+      file_name,\n+  )\n+\n+\n+class RunTEHloTest(absltest.TestCase):\n+  \"\"\"Tests for running custom calls from Transformer Engine.\"\"\"\n+\n+  def setUp(self):\n+    super().setUp()\n+    _register_transformer_engine_custom_calls()\n+    self.config = py_hlo_multihost_runner.PyHloRunnerConfig()\n+    self.config.input_format = py_hlo_multihost_runner.InputFormat.Text\n+    self.config.hlo_argument_mode = (\n+        py_hlo_multihost_runner.ModuleArgumentMode.Uninitialized\n+    )\n+\n+  def test_run_custom_call_hlo(self):\n+    hlo_file = _get_test_hlo_path(\"transformer_engine_softmax.hlo\")\n+    py_hlo_multihost_runner.RunHloFiles([hlo_file], self.config)\n+\n+\n+def main():\n+  absltest.main()\n+\n+\n+if __name__ == \"__main__\":\n+  main()"
        },
        {
            "sha": "8ac4bc05f2869f368c8a23aa9a9c01d2fca38054",
            "filename": "third_party/xla/xla/tsl/platform/default/cuda_root_path.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fcuda_root_path.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fcuda_root_path.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fcuda_root_path.cc?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -43,7 +43,8 @@ std::vector<std::string> CandidateCudaRoots() {\n   auto roots = std::vector<std::string>{};\n   std::string runfiles_suffix = \"runfiles\";\n   std::vector<std::string> cuda_dir_names = {\"cuda_nvcc\", \"cuda_nvdisasm\",\n-                                             \"nvidia_nvshmem\", \"cuda_nvvm\"};\n+                                             \"nvidia_nvshmem\", \"cuda_nvvm\",\n+                                             \"cuda_cudart\"};\n \n   // The CUDA candidate root for c++ targets.\n   std::string executable_path = tsl::Env::Default()->GetExecutablePath();"
        },
        {
            "sha": "8d829c93c2eabbe535018cee194a5ebb14c43ef0",
            "filename": "third_party/xla/xla/xla.default.bzl",
            "status": "modified",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Fxla.default.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1db3767eb95f78d9219c1cad94a2c91caae7d9/third_party%2Fxla%2Fxla%2Fxla.default.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.default.bzl?ref=1f1db3767eb95f78d9219c1cad94a2c91caae7d9",
            "patch": "@@ -1,11 +1,15 @@\n \"\"\"Wrapper around proto libraries used inside the XLA codebase.\"\"\"\n \n load(\"@bazel_skylib//:bzl_library.bzl\", \"bzl_library\")\n+load(\"@bazel_skylib//lib:dicts.bzl\", \"dicts\")\n+load(\"@bazel_skylib//lib:paths.bzl\", \"paths\")\n load(\n     \"@local_config_rocm//rocm:build_defs.bzl\",\n     \"if_rocm_is_configured\",\n )\n load(\"@rules_cc//cc:cc_binary.bzl\", \"cc_binary\")\n+load(\"@rules_cc//cc/common:cc_info.bzl\", \"CcInfo\")\n+load(\"//xla:py_strict.bzl\", \"py_strict_test\")\n load(\n     \"//xla/tsl:package_groups.bzl\",\n     \"DEFAULT_LOAD_VISIBILITY\",\n@@ -20,6 +24,7 @@ load(\n     \"tf_exec_properties\",\n )\n load(\"//xla/tsl/platform/default:build_config.bzl\", \"strict_cc_test\")\n+load(\"//xla/tsl/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\")\n \n visibility(DEFAULT_LOAD_VISIBILITY + LEGACY_XLA_USERS)\n \n@@ -111,3 +116,82 @@ def xla_bzl_library(name = \"xla_bzl_library\"):\n             \"@bazel_skylib//:bzl_library\",\n         ],\n     )\n+\n+def _symlink_dynamic_libs_rule_impl(ctx):\n+    runfiles = ctx.runfiles()\n+    runfiles_symlinks = {}\n+    for dep in ctx.attr.deps:\n+        linker_inputs = dep[CcInfo].linking_context.linker_inputs.to_list()\n+        for linker_input in linker_inputs:\n+            if len(linker_input.libraries) == 0:\n+                continue\n+            lib = linker_input.libraries[0].dynamic_library\n+            if not lib:\n+                continue\n+            lib_path = paths.join(ctx.attr.lib_dir, lib.basename)\n+            runfiles_symlinks[lib_path] = lib\n+    return [\n+        DefaultInfo(runfiles = ctx.runfiles(\n+            symlinks = runfiles_symlinks,\n+        ).merge(runfiles)),\n+    ]\n+\n+_symlink_dynamic_libs_rule = rule(\n+    implementation = _symlink_dynamic_libs_rule_impl,\n+    attrs = {\n+        \"deps\": attr.label_list(allow_empty = True),\n+        \"lib_dir\": attr.string(mandatory = True),\n+    },\n+    doc = \"Symlinks all dynamic libraries for `deps` into a single `lib_dir` directory.\",\n+)\n+\n+def xla_py_strict_test(name, deps = None, data = None, env = None, need_cuda_libs = False, **kwargs):\n+    \"\"\"A wrapper around py_strict_test that adds XLA-specific dependencies.\n+\n+    Args:\n+      name: The name of the test.\n+      deps: The dependencies of the test.\n+      data: The data dependencies of the test.\n+      env: The environment variables to set for the test.\n+      need_cuda_libs: Whether to add CUDA libraries as data dependencies.\n+      **kwargs: Other arguments to pass to the test.\n+    \"\"\"\n+    deps = deps or []\n+    data = data or []\n+    env = env or {}\n+\n+    if need_cuda_libs:\n+        library_target = \"_{}_libs\".format(name)\n+        lib_dir = paths.join(\n+            native.package_name(),\n+            library_target,\n+        )\n+\n+        # If the python tests needs to have CUDA libraries as data dependencies, we symlink\n+        # them into a directory inside the runfiles directory that the test can access and add\n+        # that directory to the LD_LIBRARY_PATH and CUDA_HOME environment variables.\n+        _symlink_dynamic_libs_rule(\n+            name = library_target,\n+            lib_dir = lib_dir,\n+            deps = if_cuda_is_configured(\n+                [\n+                    \"//xla/stream_executor/cuda:all_runtime\",\n+                ],\n+            ),\n+            testonly = True,\n+            visibility = [\"//visibility:private\"],\n+        )\n+\n+        data = data + [library_target]\n+        env = dicts.add(env, {\n+            \"CUDA_HOME\": lib_dir,\n+            \"LD_LIBRARY_PATH\": lib_dir,\n+        })\n+\n+    py_strict_test(\n+        name = name,\n+        deps = deps + xla_py_test_deps(),\n+        data = data,\n+        env = env,\n+        **kwargs\n+    )"
        }
    ],
    "stats": {
        "total": 664,
        "additions": 661,
        "deletions": 3
    }
}