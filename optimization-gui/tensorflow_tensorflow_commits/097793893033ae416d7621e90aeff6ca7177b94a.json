{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Move CollectiveOpsTestE2E base into a separate build target.\n\nPiperOrigin-RevId: 836245730",
    "sha": "097793893033ae416d7621e90aeff6ca7177b94a",
    "files": [
        {
            "sha": "3202154e4547d40e77d11bf595b12a23b89bdeea",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=097793893033ae416d7621e90aeff6ca7177b94a",
            "patch": "@@ -2833,6 +2833,46 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"collective_ops_e2e_test_base\",\n+    testonly = True,\n+    srcs = [\"collective_ops_e2e_test_base.cc\"],\n+    hdrs = [\"collective_ops_e2e_test_base.h\"],\n+    deps = [\n+        \"//xla:array\",\n+        \"//xla:literal\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:backend\",\n+        \"//xla/service:computation_placer_hdr\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/service:hlo_runner\",\n+        \"//xla/service:hlo_runner_interface\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:gpu_memory_space_assignment\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/integrations:device_mem_allocator\",\n+        \"//xla/stream_executor/integrations:stream_executor_allocator\",\n+        \"//xla/stream_executor/integrations:tf_allocator_adapter\",\n+        \"//xla/tsl/framework:allocator\",\n+        \"//xla/tsl/framework:bfc_allocator\",\n+        \"//xla/tsl/framework:device_id_impl\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n xla_test(\n     name = \"collective_ops_e2e_test\",\n     srcs = [\"collective_ops_e2e_test.cc\"],\n@@ -2846,6 +2886,7 @@ xla_test(\n         \"gpu\",\n     ],\n     deps = [\n+        \":collective_ops_e2e_test_base\",\n         \":literal_test_util\",\n         \":test_utils\",\n         \":xla_internal_test_main\",\n@@ -2856,6 +2897,7 @@ xla_test(\n         \"//xla:types\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:pattern_matcher_gmock\",\n         \"//xla/hlo/testlib:verified_hlo_module\","
        },
        {
            "sha": "f35c69db0e8cf2fdae44df13245388d393d4962a",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 226,
            "changes": 229,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=097793893033ae416d7621e90aeff6ca7177b94a",
            "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #include <algorithm>\n #include <array>\n #include <cmath>\n-#include <cstddef>\n #include <cstdint>\n #include <functional>\n #include <memory>\n@@ -42,11 +41,13 @@ limitations under the License.\n #include \"xla/array.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n@@ -56,24 +57,15 @@ limitations under the License.\n #include \"xla/service/backend.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/gpu_memory_space_assignment.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_runner.h\"\n #include \"xla/service/hlo_runner_interface.h\"\n #include \"xla/service/pattern_matcher.h\"\n-#include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/integrations/device_mem_allocator.h\"\n-#include \"xla/stream_executor/integrations/stream_executor_allocator.h\"\n-#include \"xla/stream_executor/integrations/tf_allocator_adapter.h\"\n-#include \"xla/stream_executor/platform.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tests/collective_ops_e2e_test_base.h\"\n #include \"xla/tests/literal_test_util.h\"\n #include \"xla/tests/test_utils.h\"\n-#include \"xla/tsl/framework/allocator.h\"\n-#include \"xla/tsl/framework/bfc_allocator.h\"\n-#include \"xla/tsl/framework/device_id.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -89,171 +81,13 @@ namespace op = ::xla::testing::opcode_matchers;\n namespace m = ::xla::match;\n using ::testing::NotNull;\n \n-// Makes a DeviceAssignment device#i to replica_id #i.\n-DeviceAssignment MakeDeviceAssn(int64_t num_replicas) {\n-  DeviceAssignment assn(/*replica_count=*/num_replicas,\n-                        /*computation_count=*/1);\n-  for (int64_t i = 0; i < num_replicas; ++i) {\n-    assn(i, 0) = i;\n-  }\n-  return assn;\n-}\n-\n-std::unique_ptr<tsl::BFCAllocator> CreateAllocator(se::StreamExecutor* executor,\n-                                                   int64_t device_ordinal,\n-                                                   bool is_collective,\n-                                                   size_t memory_size) {\n-  std::string name_suffix = is_collective ? \"_collectives_bfc\" : \"_bfc\";\n-  tsl::BFCAllocator::Options opts;\n-  opts.allow_growth = false;\n-  std::unique_ptr<tsl::SubAllocator> device_mem_allocator;\n-  if (is_collective) {\n-    device_mem_allocator = std::make_unique<se::StreamExecutorAllocator>(\n-        executor->CreateMemoryAllocator(se::MemoryType::kCollective).value(),\n-        /*memory_type=*/stream_executor::MemoryType::kCollective,\n-        device_ordinal);\n-  } else {\n-    device_mem_allocator = std::make_unique<se::DeviceMemAllocator>(\n-        executor, tsl::PlatformDeviceId(device_ordinal));\n-  }\n-  return std::make_unique<tsl::BFCAllocator>(\n-      std::move(device_mem_allocator), memory_size,\n-      absl::StrCat(\"GPU_\", device_ordinal, name_suffix), opts);\n-}\n-\n-template <typename Type>\n-Type CheckStatus(absl::StatusOr<Type> result) {\n-  CHECK_OK(result);\n-  return *result;\n-}\n-\n bool IsAsync(const HloInstruction* inst) {\n   return !inst->backend_config<gpu::GpuBackendConfig>()\n               .value()\n               .collective_backend_config()\n               .is_sync();\n }\n \n-class CollectiveOpsE2ETestBase : public HloHardwareIndependentTestBase {\n- public:\n-  CollectiveOpsE2ETestBase() {\n-    se::Platform* platform = CheckStatus(PlatformUtil::GetPlatform(\"GPU\"));\n-    se::Platform* reference_platform =\n-        CheckStatus(PlatformUtil::GetPlatform(\"GPU\"));\n-\n-    std::vector<se::MultiDeviceAdapter::AllocatorInfo> allocators;\n-    constexpr int64_t kGB = 1024LL * 1024LL * 1024LL;\n-    size_t common_buffers_size = 8 * kGB;   // 8GB\n-    size_t collectives_buffers_size = kGB;  // 1GB\n-    for (int64_t i = 0; i < platform->VisibleDeviceCount(); ++i) {\n-      se::StreamExecutor* executor =\n-          CheckStatus(platform->ExecutorForDevice(i));\n-      // Common memory allocator for device i.\n-      allocators.emplace_back(\n-          CreateAllocator(executor, i, /*is_collective=*/false,\n-                          common_buffers_size),\n-          nullptr, 0, i, platform);\n-\n-      // Collectives and symmetric memory allocator for device i.\n-      allocators.emplace_back(\n-          CreateAllocator(executor, i, /*is_collective=*/true,\n-                          collectives_buffers_size),\n-          nullptr, (int)gpu::MemorySpaceColor::kCollective, i, platform);\n-    }\n-\n-    hlo_runner_ = std::make_unique<HloRunner>(\n-        platform, /*intra_op_parallelism_threads=*/0,\n-        std::make_unique<se::MultiDeviceAdapter>(platform,\n-                                                 std::move(allocators)));\n-    reference_hlo_runner_ = std::make_unique<HloRunner>(\n-        reference_platform, /*intra_op_parallelism_threads=*/0);\n-  }\n-\n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n-      absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n-      absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n-      const int64_t num_replicas, const bool run_hlo_passes,\n-      DeviceAssignment* const device_assignment) {\n-    // TODO(b/441865120): Use designated initializers this once XLA moves to\n-    // C++20.\n-    HloRunnerInterface::ReplicatedExecuteOptions options;\n-    options.num_replicas = num_replicas;\n-    options.run_hlo_passes = run_hlo_passes;\n-    options.use_threads = true;\n-\n-    return hlo_runner_->ExecuteReplicated(\n-        std::move(executable_provider), std::move(argument_count_provider),\n-        std::move(argument_provider), std::move(options), device_assignment);\n-  }\n-\n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      std::unique_ptr<HloModule> module,\n-      const absl::Span<const Literal* const> arguments,\n-      const int64_t num_replicas, DeviceAssignment* const device_assignment,\n-      const bool run_hlo_passes, const bool use_threads) {\n-    // TODO(b/441865120): Use designated initializers this once XLA moves to\n-    // C++20.\n-    HloRunnerInterface::ReplicatedExecuteOptions options;\n-    options.num_replicas = num_replicas;\n-    options.arguments = {arguments.begin(), arguments.end()};\n-    options.run_hlo_passes = run_hlo_passes;\n-    options.use_threads = use_threads;\n-\n-    return hlo_runner_->ExecuteReplicated(std::move(module), std::move(options),\n-                                          device_assignment);\n-  }\n-\n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      std::unique_ptr<HloModule> module,\n-      const std::vector<std::vector<Literal*>> arguments,\n-      DeviceAssignment* const device_assignment, const int64_t num_replicas,\n-      const bool run_hlo_passes) {\n-    CHECK(num_replicas > 0 && \"expect at least one replica\");\n-    CHECK(num_replicas == arguments.size() &&\n-          \"expect arguments for each replica\");\n-    int64_t argument_count = arguments.front().size();\n-    TF_ASSIGN_OR_RETURN(\n-        const std::unique_ptr<OpaqueExecutable> executable,\n-        hlo_runner_->CreateExecutable(std::move(module), run_hlo_passes));\n-    return ExecuteReplicated(\n-        /*executable_provider=*/[&](int64_t) { return executable.get(); },\n-        /*argument_count_provider=*/[&](int64_t) { return argument_count; },\n-        /*argument_provider=*/\n-        [&](int64_t replica_idx, int64_t argument_idx) -> const Literal* {\n-          return arguments[replica_idx][argument_idx];\n-        },\n-        num_replicas, /*run_hlo_passes=*/run_hlo_passes,\n-        /*device_assignment=*/device_assignment);\n-  }\n-\n-  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n-      OpaqueExecutable* executable, int64_t num_replicas) {\n-    DeviceAssignment device_assignment = MakeDeviceAssn(num_replicas);\n-    return ExecuteReplicated(\n-        /*executable_provider*/ [&](int64_t) { return executable; },\n-        /*argument_count_provider*/ [](int64_t) { return 0; },\n-        /*argument_provider*/ [](int64_t, int64_t) { return nullptr; },\n-        num_replicas, /*run_hlo_passes=*/false, &device_assignment);\n-  }\n-\n-  const se::GpuComputeCapability& Capability() {\n-    return hlo_runner_->backend()\n-        .default_stream_executor()\n-        ->GetDeviceDescription()\n-        .gpu_compute_capability();\n-  }\n-\n-  bool IsHopperAndHigher() {\n-    return Capability().IsCuda() &&\n-           Capability().cuda_compute_capability()->IsAtLeastHopper();\n-  }\n-\n- protected:\n-  std::unique_ptr<HloRunner> hlo_runner_;\n-  std::unique_ptr<HloRunner> reference_hlo_runner_;\n-};\n-\n class CollectiveOpsTestE2E : public CollectiveOpsE2ETestBase {\n  public:\n   CollectiveOpsTestE2E() {\n@@ -307,63 +141,6 @@ class CollectiveOpsTestE2E : public CollectiveOpsE2ETestBase {\n   static constexpr const char* kF8E5M2DatatypePlaceholder{\"<<F8E5M2>>\"};\n };\n \n-// E2E tests for collective ops. These will generally verify some HLO transform\n-// for collectives (for example, sync -> async conversion) and correct\n-// execution of the transformed HLO.\n-\n-// E2E test for collectives with flags set. Has constructor arguments specifying\n-// whether to enable/disable async collectives, and to set the memcpy_local_p2p\n-// flag. Subclasses pass in constructor arguments based on GetParam().\n-class CollectiveOpsWithFlagsBase : public CollectiveOpsE2ETestBase {\n- public:\n-  CollectiveOpsWithFlagsBase(bool enable_async, bool enable_p2p_memcpy)\n-      : enable_async_(enable_async), enable_p2p_memcpy_(enable_p2p_memcpy) {\n-    VLOG(1) << \"Running with \" << num_devices_ << \" devices\";\n-    num_devices_ = hlo_runner_->backend().device_count();\n-  }\n-\n- protected:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options =\n-        HloHardwareIndependentTestBase::GetDebugOptionsForTest();\n-\n-    // Disable autotuning which is unnecessary.\n-    debug_options.set_xla_gpu_autotune_level(0);\n-\n-    // Enable or disable all async collectives based on test parameter.\n-    if (!enable_async_) {\n-      for (auto option :\n-           {DebugOptions::NOOP, DebugOptions::ALLREDUCE,\n-            DebugOptions::ALLGATHER, DebugOptions::REDUCESCATTER,\n-            DebugOptions::COLLECTIVEBROADCAST, DebugOptions::ALLTOALL,\n-            DebugOptions::COLLECTIVEPERMUTE, DebugOptions::RAGGEDALLTOALL}) {\n-        debug_options.add_xla_gpu_disable_async_collectives(option);\n-      }\n-    }\n-    debug_options.add_xla_disable_hlo_passes(\n-        \"gpu-convert-async-collectives-to-sync\");\n-    if (enable_p2p_memcpy_) {\n-      debug_options.set_xla_gpu_use_memcpy_local_p2p(true);\n-    }\n-    return debug_options;\n-  }\n-\n-  absl::StatusOr<std::unique_ptr<OpaqueExecutable>> CreateExecutable(\n-      absl::string_view hlo_string, int64_t num_replicas) {\n-    HloModuleConfig config =\n-        GetModuleConfigForTest(/*replica_count=*/num_replicas);\n-\n-    TF_ASSIGN_OR_RETURN(auto module,\n-                        ParseAndReturnVerifiedModule(hlo_string, config));\n-    return hlo_runner_->CreateExecutable(std::move(module),\n-                                         /*run_hlo_passes=*/true);\n-  }\n-\n-  const bool enable_async_;\n-  const bool enable_p2p_memcpy_;\n-  int64_t num_devices_;\n-};\n-\n class AsyncCollectiveOps : public CollectiveOpsWithFlagsBase,\n                            public ::testing::WithParamInterface<bool> {\n  public:"
        },
        {
            "sha": "e7afe98e12e6503cfb92bac7c41170fa779272d8",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.cc",
            "status": "added",
            "additions": 228,
            "deletions": 0,
            "changes": 228,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc?ref=097793893033ae416d7621e90aeff6ca7177b94a",
            "patch": "@@ -0,0 +1,228 @@\n+/* Copyright 2023 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/tests/collective_ops_e2e_test_base.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/gpu_memory_space_assignment.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/hlo_runner.h\"\n+#include \"xla/service/hlo_runner_interface.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/integrations/device_mem_allocator.h\"\n+#include \"xla/stream_executor/integrations/stream_executor_allocator.h\"\n+#include \"xla/stream_executor/integrations/tf_allocator_adapter.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/framework/allocator.h\"\n+#include \"xla/tsl/framework/bfc_allocator.h\"\n+#include \"xla/tsl/framework/device_id.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace {\n+\n+std::unique_ptr<tsl::BFCAllocator> CreateAllocator(se::StreamExecutor* executor,\n+                                                   int64_t device_ordinal,\n+                                                   bool is_collective,\n+                                                   size_t memory_size) {\n+  std::string name_suffix = is_collective ? \"_collectives_bfc\" : \"_bfc\";\n+  tsl::BFCAllocator::Options opts;\n+  opts.allow_growth = false;\n+  std::unique_ptr<tsl::SubAllocator> device_mem_allocator;\n+  if (is_collective) {\n+    device_mem_allocator = std::make_unique<se::StreamExecutorAllocator>(\n+        executor->CreateMemoryAllocator(se::MemoryType::kCollective).value(),\n+        /*memory_type=*/stream_executor::MemoryType::kCollective,\n+        device_ordinal);\n+  } else {\n+    device_mem_allocator = std::make_unique<se::DeviceMemAllocator>(\n+        executor, tsl::PlatformDeviceId(device_ordinal));\n+  }\n+  return std::make_unique<tsl::BFCAllocator>(\n+      std::move(device_mem_allocator), memory_size,\n+      absl::StrCat(\"GPU_\", device_ordinal, name_suffix), opts);\n+}\n+\n+template <typename Type>\n+Type CheckStatus(absl::StatusOr<Type> result) {\n+  CHECK_OK(result);\n+  return *result;\n+}\n+\n+}  // namespace\n+\n+CollectiveOpsE2ETestBase::CollectiveOpsE2ETestBase() {\n+  se::Platform* platform = CheckStatus(PlatformUtil::GetPlatform(\"GPU\"));\n+  se::Platform* reference_platform =\n+      CheckStatus(PlatformUtil::GetPlatform(\"GPU\"));\n+\n+  std::vector<se::MultiDeviceAdapter::AllocatorInfo> allocators;\n+  constexpr int64_t kGB = 1024LL * 1024LL * 1024LL;\n+  size_t common_buffers_size = 8 * kGB;   // 8GB\n+  size_t collectives_buffers_size = kGB;  // 1GB\n+  for (int64_t i = 0; i < platform->VisibleDeviceCount(); ++i) {\n+    se::StreamExecutor* executor = CheckStatus(platform->ExecutorForDevice(i));\n+    // Common memory allocator for device i.\n+    allocators.emplace_back(\n+        CreateAllocator(executor, i, /*is_collective=*/false,\n+                        common_buffers_size),\n+        nullptr, 0, i, platform);\n+\n+    // Collectives and symmetric memory allocator for device i.\n+    allocators.emplace_back(CreateAllocator(executor, i, /*is_collective=*/true,\n+                                            collectives_buffers_size),\n+                            nullptr, (int)gpu::MemorySpaceColor::kCollective, i,\n+                            platform);\n+  }\n+\n+  hlo_runner_ =\n+      std::make_unique<HloRunner>(platform, /*intra_op_parallelism_threads=*/0,\n+                                  std::make_unique<se::MultiDeviceAdapter>(\n+                                      platform, std::move(allocators)));\n+  reference_hlo_runner_ = std::make_unique<HloRunner>(\n+      reference_platform, /*intra_op_parallelism_threads=*/0);\n+}\n+\n+absl::StatusOr<std::vector<Literal>>\n+CollectiveOpsE2ETestBase::ExecuteReplicated(\n+    absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n+    absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n+    absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n+    const int64_t num_replicas, const bool run_hlo_passes,\n+    DeviceAssignment* const device_assignment) {\n+  // TODO(b/441865120): Use designated initializers this once XLA moves to\n+  // C++20.\n+  HloRunnerInterface::ReplicatedExecuteOptions options;\n+  options.num_replicas = num_replicas;\n+  options.run_hlo_passes = run_hlo_passes;\n+  options.use_threads = true;\n+\n+  return hlo_runner_->ExecuteReplicated(\n+      std::move(executable_provider), std::move(argument_count_provider),\n+      std::move(argument_provider), std::move(options), device_assignment);\n+}\n+\n+absl::StatusOr<std::vector<Literal>>\n+CollectiveOpsE2ETestBase::ExecuteReplicated(\n+    std::unique_ptr<HloModule> module,\n+    const absl::Span<const Literal* const> arguments,\n+    const int64_t num_replicas, DeviceAssignment* const device_assignment,\n+    const bool run_hlo_passes, const bool use_threads) {\n+  // TODO(b/441865120): Use designated initializers this once XLA moves to\n+  // C++20.\n+  HloRunnerInterface::ReplicatedExecuteOptions options;\n+  options.num_replicas = num_replicas;\n+  options.arguments = {arguments.begin(), arguments.end()};\n+  options.run_hlo_passes = run_hlo_passes;\n+  options.use_threads = use_threads;\n+\n+  return hlo_runner_->ExecuteReplicated(std::move(module), std::move(options),\n+                                        device_assignment);\n+}\n+\n+absl::StatusOr<std::vector<Literal>>\n+CollectiveOpsE2ETestBase::ExecuteReplicated(\n+    std::unique_ptr<HloModule> module,\n+    const std::vector<std::vector<Literal*>> arguments,\n+    DeviceAssignment* const device_assignment, const int64_t num_replicas,\n+    const bool run_hlo_passes) {\n+  CHECK(num_replicas > 0 && \"expect at least one replica\");\n+  CHECK(num_replicas == arguments.size() &&\n+        \"expect arguments for each replica\");\n+  int64_t argument_count = arguments.front().size();\n+  TF_ASSIGN_OR_RETURN(\n+      const std::unique_ptr<OpaqueExecutable> executable,\n+      hlo_runner_->CreateExecutable(std::move(module), run_hlo_passes));\n+  return ExecuteReplicated(\n+      /*executable_provider=*/[&](int64_t) { return executable.get(); },\n+      /*argument_count_provider=*/[&](int64_t) { return argument_count; },\n+      /*argument_provider=*/\n+      [&](int64_t replica_idx, int64_t argument_idx) -> const Literal* {\n+        return arguments[replica_idx][argument_idx];\n+      },\n+      num_replicas, /*run_hlo_passes=*/run_hlo_passes,\n+      /*device_assignment=*/device_assignment);\n+}\n+\n+absl::StatusOr<std::vector<Literal>>\n+CollectiveOpsE2ETestBase::ExecuteReplicated(OpaqueExecutable* executable,\n+                                            int64_t num_replicas) {\n+  DeviceAssignment device_assignment = MakeDeviceAssn(num_replicas);\n+  return ExecuteReplicated(\n+      /*executable_provider*/ [&](int64_t) { return executable; },\n+      /*argument_count_provider*/ [](int64_t) { return 0; },\n+      /*argument_provider*/ [](int64_t, int64_t) { return nullptr; },\n+      num_replicas, /*run_hlo_passes=*/false, &device_assignment);\n+}\n+\n+DebugOptions CollectiveOpsWithFlagsBase::GetDebugOptionsForTest() const {\n+  DebugOptions debug_options =\n+      HloHardwareIndependentTestBase::GetDebugOptionsForTest();\n+\n+  // Disable autotuning which is unnecessary.\n+  debug_options.set_xla_gpu_autotune_level(0);\n+\n+  // Enable or disable all async collectives based on test parameter.\n+  if (!enable_async_) {\n+    for (auto option :\n+         {DebugOptions::NOOP, DebugOptions::ALLREDUCE, DebugOptions::ALLGATHER,\n+          DebugOptions::REDUCESCATTER, DebugOptions::COLLECTIVEBROADCAST,\n+          DebugOptions::ALLTOALL, DebugOptions::COLLECTIVEPERMUTE,\n+          DebugOptions::RAGGEDALLTOALL}) {\n+      debug_options.add_xla_gpu_disable_async_collectives(option);\n+    }\n+  }\n+  debug_options.add_xla_disable_hlo_passes(\n+      \"gpu-convert-async-collectives-to-sync\");\n+  if (enable_p2p_memcpy_) {\n+    debug_options.set_xla_gpu_use_memcpy_local_p2p(true);\n+  }\n+  return debug_options;\n+}\n+\n+absl::StatusOr<std::unique_ptr<OpaqueExecutable>>\n+CollectiveOpsWithFlagsBase::CreateExecutable(absl::string_view hlo_string,\n+                                             int64_t num_replicas) {\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/num_replicas);\n+\n+  TF_ASSIGN_OR_RETURN(auto module,\n+                      ParseAndReturnVerifiedModule(hlo_string, config));\n+  return hlo_runner_->CreateExecutable(std::move(module),\n+                                       /*run_hlo_passes=*/true);\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "1ea506bd5c4a7431ecd30d27343b269dfef6a7d2",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.h",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/097793893033ae416d7621e90aeff6ca7177b94a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h?ref=097793893033ae416d7621e90aeff6ca7177b94a",
            "patch": "@@ -0,0 +1,127 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_TESTS_COLLECTIVE_OPS_E2E_TEST_BASE_H_\n+#define XLA_TESTS_COLLECTIVE_OPS_E2E_TEST_BASE_H_\n+\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/array.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/service/backend.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/hlo_runner.h\"\n+#include \"xla/service/hlo_runner_interface.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+\n+class CollectiveOpsE2ETestBase : public HloHardwareIndependentTestBase {\n+ public:\n+  CollectiveOpsE2ETestBase();\n+\n+  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n+      absl::AnyInvocable<OpaqueExecutable*(int64_t)> executable_provider,\n+      absl::AnyInvocable<int64_t(int64_t)> argument_count_provider,\n+      absl::AnyInvocable<const Literal*(int64_t, int64_t)> argument_provider,\n+      int64_t num_replicas, bool run_hlo_passes,\n+      DeviceAssignment* device_assignment);\n+\n+  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n+      std::unique_ptr<HloModule> module,\n+      absl::Span<const Literal* const> arguments, int64_t num_replicas,\n+      DeviceAssignment* vice_assignment, bool run_hlo_passes, bool use_threads);\n+\n+  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n+      std::unique_ptr<HloModule> module,\n+      std::vector<std::vector<Literal*>> arguments,\n+      DeviceAssignment* device_assignment, int64_t num_replicas,\n+      bool run_hlo_passes);\n+\n+  absl::StatusOr<std::vector<Literal>> ExecuteReplicated(\n+      OpaqueExecutable* executable, int64_t num_replicas);\n+\n+  const se::GpuComputeCapability& Capability() {\n+    return hlo_runner_->backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability();\n+  }\n+\n+  bool IsHopperAndHigher() {\n+    return Capability().IsCuda() &&\n+           Capability().cuda_compute_capability()->IsAtLeastHopper();\n+  }\n+\n+  // Makes a DeviceAssignment device#i to replica_id #i.\n+  DeviceAssignment MakeDeviceAssn(int64_t num_replicas) {\n+    DeviceAssignment assn(/*replica_count=*/num_replicas,\n+                          /*computation_count=*/1);\n+    for (int64_t i = 0; i < num_replicas; ++i) {\n+      assn(i, 0) = i;\n+    }\n+    return assn;\n+  }\n+\n+ protected:\n+  std::unique_ptr<HloRunner> hlo_runner_;\n+  std::unique_ptr<HloRunner> reference_hlo_runner_;\n+};\n+\n+// E2E tests for collective ops. These will generally verify some HLO transform\n+// for collectives (for example, sync -> async conversion) and correct\n+// execution of the transformed HLO.\n+\n+// E2E test for collectives with flags set. Has constructor arguments specifying\n+// whether to enable/disable async collectives, and to set the memcpy_local_p2p\n+// flag. Subclasses pass in constructor arguments based on GetParam().\n+class CollectiveOpsWithFlagsBase : public CollectiveOpsE2ETestBase {\n+ public:\n+  CollectiveOpsWithFlagsBase(bool enable_async, bool enable_p2p_memcpy)\n+      : enable_async_(enable_async), enable_p2p_memcpy_(enable_p2p_memcpy) {\n+    VLOG(1) << \"Running with \" << num_devices_ << \" devices\";\n+    num_devices_ = hlo_runner_->backend().device_count();\n+  }\n+\n+ protected:\n+  DebugOptions GetDebugOptionsForTest() const override;\n+\n+  absl::StatusOr<std::unique_ptr<OpaqueExecutable>> CreateExecutable(\n+      absl::string_view hlo_string, int64_t num_replicas);\n+\n+  const bool enable_async_;\n+  const bool enable_p2p_memcpy_;\n+  int64_t num_devices_;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_TESTS_COLLECTIVE_OPS_E2E_TEST_BASE_H_"
        }
    ],
    "stats": {
        "total": 626,
        "additions": 400,
        "deletions": 226
    }
}