{
    "author": "tensorflower-gardener",
    "message": "[XLA:Original Value] Refactor: Move original value handling for multi-output fusion to HloInstruction methods.\n\nThis change moves the logic for updating `OriginalValue` during multi-output fusion from `multi_output_fusion.cc` into the `HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation` and `HloFusionInstruction::MergeFusionInstructionIntoMultiOutput` methods in `hlo_instructions.cc`. This ensures that original value tracking is handled directly within the instruction fusion logic, rather than being a separate step in the `MultiOutputFusion` pass.\n\nThis helps resolve other fusion related issues. For example in InstructionFusion pass. A test is added to reflect this.\n\nPiperOrigin-RevId: 816374306",
    "sha": "beab9d0058c60759b0f9592b4f98f3540269dbbf",
    "files": [
        {
            "sha": "084f7962a3c207cb4f6a4846bc4e862e90fe0382",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "status": "modified",
            "additions": 73,
            "deletions": 2,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc?ref=beab9d0058c60759b0f9592b4f98f3540269dbbf",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/hlo/ir/hlo_instructions.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <deque>\n #include <functional>\n@@ -46,6 +45,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_original_value.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/ir/hlo_sharding_metadata.h\"\n@@ -67,7 +67,6 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/window_util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/protobuf.h\"\n \n namespace xla {\n namespace {\n@@ -1961,6 +1960,68 @@ HloInstruction* HloCallableInstruction::AppendInstructionIntoCalledComputation(\n                                                         add_output);\n }\n \n+namespace {\n+\n+// Returns the original value that can be used for multi output fusion. The\n+// placeholder is just to provide a matching tuple tree of empty original arrays\n+// for the fusion logic in `SetOriginalValueOnFusedInstruction` to correctly\n+// populate the fused original value.\n+std::shared_ptr<OriginalValue> GetOriginalValueOrPlaceholderForFusion(\n+    HloInstruction* inst) {\n+  if (inst->original_value()) {\n+    if (!inst->original_value()->IsCompatibleWith(inst->shape())) {\n+      LOG(ERROR) << \"Instruction '\" << inst->name()\n+                 << \"' has original value incompatible with its \"\n+                    \"shape.\\nOriginal value: \"\n+                 << inst->original_value()->ToString()\n+                 << \"\\nShape: \" << inst->shape().ToString();\n+      // Return nullptr to bail out of original value tracking.\n+      return nullptr;\n+    }\n+    return inst->original_value();\n+  }\n+  return std::make_shared<OriginalValue>(inst->shape());\n+}\n+\n+// Sets the original value on the final (aka remaining) instruction after\n+// fusion. This function assumes the final instruction to have a fused\n+// shape. This fused shape should be a tuple containing elements from\n+// `first_fused_ov` and `second_fused_ov`.\n+void SetOriginalValueOnFusedInstruction(\n+    HloInstruction* final_instr, std::shared_ptr<OriginalValue> first_fused_ov,\n+    std::shared_ptr<OriginalValue> second_fused_ov) {\n+  if (!first_fused_ov || !second_fused_ov) {\n+    return;\n+  }\n+  if (first_fused_ov->is_synthetic_call() ||\n+      second_fused_ov->is_synthetic_call() ||\n+      (first_fused_ov->IsEmpty() && second_fused_ov->IsEmpty())) {\n+    // Synthetic calls are generated by optimization passes and usually they\n+    // should be inlined immediately. If somehow this multi output pass needs to\n+    // fuse synthetic calls, we just ignore the original value because it's not\n+    // clear how to fuse them.\n+    final_instr->set_original_value(nullptr);\n+    return;\n+  }\n+\n+  std::vector<std::optional<OriginalArray>> new_leaves;\n+  for (const auto& [index, value] : first_fused_ov->original_arrays()) {\n+    new_leaves.push_back(value);\n+  }\n+  for (const auto& [index, value] : second_fused_ov->original_arrays()) {\n+    new_leaves.push_back(value);\n+  }\n+\n+  auto new_ov = std::make_shared<OriginalValue>(final_instr->shape());\n+  int64_t leaf_index = 0;\n+  for (auto& [index, value] : new_ov->mutable_original_arrays()) {\n+    CHECK_LT(leaf_index, new_leaves.size());\n+    value = new_leaves[leaf_index++];\n+  }\n+  final_instr->set_original_value(new_ov);\n+}\n+}  // namespace\n+\n HloInstruction*\n HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     HloInstruction* instruction_to_append, bool add_output) {\n@@ -2090,6 +2151,9 @@ HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     }\n     // If this is already a multioutput instruction, expand the root tuple\n     // by 1.\n+    auto second_fused_ov =\n+        GetOriginalValueOrPlaceholderForFusion(instruction_to_append);\n+    auto first_fused_ov = GetOriginalValueOrPlaceholderForFusion(this);\n     HloInstruction::InstructionVector tuple_elements;\n     bool newly_created_tuple_instr = false;\n     if (root->opcode() == HloOpcode::kTuple) {\n@@ -2114,6 +2178,7 @@ HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     called_computation()->set_root_instruction(new_root,\n                                                /*accept_different_shape=*/true);\n     *mutable_shape() = new_root->shape();\n+    SetOriginalValueOnFusedInstruction(this, first_fused_ov, second_fused_ov);\n     // The instruction might have an existing sharding, which will no longer\n     // be valid after we change the shape. So clear the sharding.\n     clear_sharding();\n@@ -2381,6 +2446,9 @@ void HloFusionInstruction::MergeFusionInstruction(\n \n void HloFusionInstruction::MergeFusionInstructionIntoMultiOutput(\n     HloFusionInstruction* instruction_to_merge) {\n+  auto first_fused_ov = GetOriginalValueOrPlaceholderForFusion(this);\n+  auto second_fused_ov =\n+      GetOriginalValueOrPlaceholderForFusion(instruction_to_merge);\n   // Add all non-parameter fused instructions to 'unfused_instructions' to be\n   // merged into 'this'. `old_to_new' maps the instructions in the fused node\n   // to the disassembled fusion instructions.\n@@ -2482,6 +2550,9 @@ void HloFusionInstruction::MergeFusionInstructionIntoMultiOutput(\n     }\n     TF_CHECK_OK(instruction->parent()->RemoveInstruction(instruction));\n   }\n+  if (this->IsMultiOutputFusion()) {\n+    SetOriginalValueOnFusedInstruction(this, first_fused_ov, second_fused_ov);\n+  }\n }\n \n HloComputation* HloFusionInstruction::fused_instructions_computation() const {"
        },
        {
            "sha": "7ea8914afa343fc2cd2f2f921b12a900a002f023",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=beab9d0058c60759b0f9592b4f98f3540269dbbf",
            "patch": "@@ -1990,17 +1990,12 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/functional:function_ref\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/types:span\",\n     ],\n )\n "
        },
        {
            "sha": "18a65ac494e42c4d50c524aab87837d306eb7bcc",
            "filename": "third_party/xla/xla/service/instruction_fusion_test.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Finstruction_fusion_test.cc?ref=beab9d0058c60759b0f9592b4f98f3540269dbbf",
            "patch": "@@ -119,6 +119,52 @@ TEST_F(InstructionFusionTest, FuseInstructionsIntoMultiOutput) {\n       << module->ToString();\n }\n \n+TEST_F(InstructionFusionTest, FuseInstructionsWithOriginalValue) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+  ENTRY entry_computation {\n+    p0 = f32[4,3]{1,0} parameter(0), origin={{\"p0\"}}\n+    add = f32[4,3]{1,0} add(p0, p0), origin={{\"add\"}}\n+    ROOT sub = f32[4,3]{1,0} subtract(add, p0), origin={{\"sub\"}}\n+  })\")\n+                    .value();\n+  HloInstruction* sub = module->entry_computation()->root_instruction();\n+  HloInstruction* add = sub->mutable_operand(0);\n+  HloInstruction* fusion =\n+      InstructionFusionForTesting().Fuse(add, sub, module->entry_computation());\n+\n+  ASSERT_THAT(fusion, op::Fusion()) << module->ToString();\n+  EXPECT_THAT(fusion->fused_expression_root(),\n+              op::Subtract(op::Add(), op::Parameter()))\n+      << module->ToString();\n+  ASSERT_NE(fusion->original_value(), nullptr);\n+  EXPECT_EQ(fusion->original_value()->ToString(), \"{\\\"sub\\\"}\");\n+}\n+\n+TEST_F(InstructionFusionTest,\n+       FuseInstructionsIntoMultiOutputWithOriginalValue) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+  ENTRY entry_computation {\n+    p0 = f32[4,3]{1,0} parameter(0), origin={{\"p0\"}}\n+    abs = f32[4,3]{1,0} abs(p0), origin={{\"abs\"}}\n+    tanh = f32[4,3]{1,0} tanh(abs), origin={{\"tanh\"}}\n+    ROOT add = f32[4,3]{1,0} add(abs, tanh), origin={{\"add\"}}\n+  })\")\n+                    .value();\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  HloInstruction* abs = root->mutable_operand(0);\n+  HloInstruction* tanh = root->mutable_operand(1);\n+  HloInstruction* fusion = InstructionFusionForTesting().FuseIntoMultiOutput(\n+      abs, tanh, module->entry_computation());\n+\n+  ASSERT_THAT(fusion, op::Fusion()) << module->ToString();\n+  EXPECT_THAT(fusion->fused_expression_root(), op::Tuple(op::Tanh(), op::Abs()))\n+      << module->ToString();\n+  ASSERT_NE(fusion->original_value(), nullptr);\n+  EXPECT_EQ(fusion->original_value()->ToString(), \"({\\\"tanh\\\"}, {\\\"abs\\\"})\");\n+}\n+\n TEST_F(InstructionFusionTest, AvoidDuplicationIfNotAllFusible) {\n   HloComputation::Builder builder(TestName());\n   auto shape = ShapeUtil::MakeShape(F32, {16, 16});"
        },
        {
            "sha": "1803e586ce6abe40475beded24c8dbe22b6b85bc",
            "filename": "third_party/xla/xla/service/multi_output_fusion.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 87,
            "changes": 87,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fservice%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/beab9d0058c60759b0f9592b4f98f3540269dbbf/third_party%2Fxla%2Fxla%2Fservice%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmulti_output_fusion.cc?ref=beab9d0058c60759b0f9592b4f98f3540269dbbf",
            "patch": "@@ -17,31 +17,21 @@ limitations under the License.\n \n #include <algorithm>\n #include <cstdint>\n-#include <memory>\n #include <optional>\n-#include <utility>\n #include <vector>\n \n #include \"absl/container/flat_hash_set.h\"\n-#include \"absl/functional/function_ref.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n-#include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/analysis/hlo_reachability.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/ir/hlo_original_value.h\"\n-#include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n #include \"xla/map_util.h\"\n-#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n@@ -86,89 +76,15 @@ absl::StatusOr<bool> MultiOutputFusion::Run(\n   return changed;\n }\n \n-namespace {\n-\n-// Returns the original value that can be used for multi output fusion. The\n-// placeholder is just to provide a matching tuple tree of empty original arrays\n-// for the fusion logic in `SetOriginalValueOnFusedInstruction` to correctly\n-// populate the fused original value.\n-std::shared_ptr<OriginalValue> GetOriginalValueOrPlaceholder(\n-    HloInstruction* inst) {\n-  if (inst->original_value()) {\n-    if (!inst->original_value()->IsCompatibleWith(inst->shape())) {\n-      LOG(ERROR) << \"Instruction '\" << inst->name()\n-                 << \"' has original value incompatible with its \"\n-                    \"shape.\\nOriginal value: \"\n-                 << inst->original_value()->ToString()\n-                 << \"\\nShape: \" << inst->shape().ToString();\n-      // Return nullptr to bail out of original value tracking.\n-      return nullptr;\n-    }\n-    return inst->original_value();\n-  }\n-  return std::make_shared<OriginalValue>(inst->shape());\n-}\n-\n-// Sets the original value on the final (aka remaining) instruction after\n-// fusion. This function assumes the final instruction to have a fused\n-// shape. This fused shape should be a tuple containing elements from\n-// `first_fused_ov` and `second_fused_ov`.\n-void SetOriginalValueOnFusedInstruction(\n-    HloInstruction* final_instr, std::shared_ptr<OriginalValue> first_fused_ov,\n-    std::shared_ptr<OriginalValue> second_fused_ov) {\n-  if (!first_fused_ov || !second_fused_ov) {\n-    return;\n-  }\n-  if (first_fused_ov->is_synthetic_call() ||\n-      second_fused_ov->is_synthetic_call() ||\n-      (first_fused_ov->IsEmpty() && second_fused_ov->IsEmpty())) {\n-    // Synthetic calls are generated by optimization passes and usually they\n-    // should be inlined immediately. If somehow this multi output pass needs to\n-    // fuse synthetic calls, we just ignore the original value because it's not\n-    // clear how to fuse them.\n-    final_instr->set_original_value(nullptr);\n-    return;\n-  }\n-\n-  std::vector<std::optional<OriginalArray>> new_leaves;\n-  for (const auto& [index, value] : first_fused_ov->original_arrays()) {\n-    new_leaves.push_back(value);\n-  }\n-  for (const auto& [index, value] : second_fused_ov->original_arrays()) {\n-    new_leaves.push_back(value);\n-  }\n-\n-  auto new_ov = std::make_shared<OriginalValue>(final_instr->shape());\n-  int64_t leaf_index = 0;\n-  for (auto& [index, value] : new_ov->mutable_original_arrays()) {\n-    CHECK_LT(leaf_index, new_leaves.size());\n-    value = new_leaves[leaf_index++];\n-  }\n-  final_instr->set_original_value(new_ov);\n-}\n-}  // namespace\n-\n HloInstruction* MultiOutputFusion::Fuse(HloInstruction* instr1,\n                                         HloInstruction* instr2) {\n   HloInstruction* remaining = instr1;\n   HloInstruction* fused = instr2;\n-\n   // Make sure that if only one of the instructions is a fusion, or if only one\n   // of the instructions is a multi-output fusion, it's what will be fused into.\n   if (!remaining->IsMultiOutputFusion() && fused->IsMultiOutputFusion()) {\n     std::swap(remaining, fused);\n   }\n-\n-  std::shared_ptr<OriginalValue> remaining_ov;\n-  std::shared_ptr<OriginalValue> fused_ov;\n-  if (remaining->original_value() || fused->original_value()) {\n-    // Only set these for tracking original value if original value is at least\n-    // set for one of the instructions. Otherwise, just bail out of any original\n-    // value logic below.\n-    remaining_ov = GetOriginalValueOrPlaceholder(remaining);\n-    fused_ov = GetOriginalValueOrPlaceholder(fused);\n-  }\n-\n   if (remaining->opcode() != HloOpcode::kFusion) {\n     remaining = CreateFusion(remaining, fused);\n   }\n@@ -177,8 +93,6 @@ HloInstruction* MultiOutputFusion::Fuse(HloInstruction* instr1,\n   } else {\n     remaining->FuseInstructionIntoMultiOutput(fused);\n   }\n-\n-  SetOriginalValueOnFusedInstruction(remaining, remaining_ov, fused_ov);\n   return remaining;\n }\n \n@@ -187,7 +101,6 @@ HloInstruction* MultiOutputFusion::CreateFusion(HloInstruction* base,\n   HloInstruction* input_fusion =\n       computation()->AddInstruction(HloInstruction::CreateFusion(\n           base->shape(), HloInstruction::FusionKind::kLoop, base));\n-  input_fusion->set_original_value(base->original_value());\n \n   // Update candidate_ and all_fusion_candidates_.\n   int64_t index = candidates_.size();"
        }
    ],
    "stats": {
        "total": 213,
        "additions": 119,
        "deletions": 94
    }
}