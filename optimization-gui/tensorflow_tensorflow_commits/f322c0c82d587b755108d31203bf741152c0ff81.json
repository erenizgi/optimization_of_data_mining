{
    "author": "pschuh",
    "message": "Refactor the event loop + socket integration so that it is separately testable.\n\nThis gives us the two HalfClose events + HandleEvent() and SendRawFrame() as\nthe API from the socket integration and subclasses can handle these\naccordingly. This also moves the responsibility to destroy in the handler logic\nwith the contract that the event is removed from the loop on the second HalfClose event.\n\nPiperOrigin-RevId: 821445213",
    "sha": "f322c0c82d587b755108d31203bf741152c0ff81",
    "files": [
        {
            "sha": "82de018f2b4eccaf95e707cbb3c181cfd89446e8",
            "filename": "third_party/xla/xla/python/transfer/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2FBUILD?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -74,12 +74,15 @@ cc_library(\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n         \"@local_tsl//tsl/platform:env\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n     ],\n )\n \n@@ -219,6 +222,7 @@ cc_library(\n         \":streaming\",\n         \":transfer_socket_proto_cc\",\n         \"//xla/tsl/concurrency:ref_count\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "9f1a8426301cfb5aef42ee9694053ec650a0278d",
            "filename": "third_party/xla/xla/python/transfer/event_loop.cc",
            "status": "modified",
            "additions": 204,
            "deletions": 0,
            "changes": 204,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.cc?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -34,14 +34,17 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/any_invocable.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/clock.h\"\n #include \"absl/time/time.h\"\n #include \"tsl/platform/env.h\"\n+#include \"tsl/profiler/lib/traceme.h\"\n \n namespace aux {\n \n@@ -415,4 +418,205 @@ absl::StatusOr<SocketAddress> SocketAddress::Parse(const std::string& addr) {\n   return out;\n }\n \n+SocketFdPacketState::~SocketFdPacketState() { CHECK_EQ(fd_, -1); }\n+\n+void SocketFdPacketState::PopulatePollInfo(pollfd& events) {\n+  events.fd = fd_;\n+  events.events = 0;\n+  if (!read_closed_) {\n+    events.events |= POLLIN;\n+  }\n+  if (!can_send_ && !write_closed_) {\n+    events.events |= POLLOUT;\n+  }\n+}\n+\n+bool SocketFdPacketState::HandleEvents(const pollfd& events) {\n+  tsl::profiler::TraceMe __trace(\"SocketServer::HandleEvents\");\n+  if (!is_connected_) {\n+    // If HUP with an error happens, then schedule a reconnect.\n+    if ((events.revents & POLLHUP) && (events.revents & POLLERR)) {\n+      mu_.lock();\n+      read_closed_ = true;\n+      write_closed_ = true;\n+      bool result = CloseIfNeeded();\n+      mu_.unlock();\n+      ConnectFailed();\n+      return result;\n+    }\n+    if (!(events.revents & POLLOUT)) {\n+      return true;\n+    }\n+    absl::MutexLock l(mu_);\n+    is_connected_ = true;\n+  }\n+  if ((events.revents & POLLIN) && !read_closed_) {\n+    ssize_t recv_size =\n+        recv(fd_, network_buffer_.get() + recv_count_, 4096 - recv_count_, 0);\n+    if (recv_size > 0) {\n+      recv_count_ += recv_size;\n+      while (recv_count_ >= sizeof(uint32_t)) {\n+        uint32_t frame_size;\n+        memcpy(&frame_size, network_buffer_.get(), sizeof(uint32_t));\n+        if (frame_size > 4096 - sizeof(uint32_t)) {\n+          mu_.lock();\n+          shutdown(fd_, SHUT_RD);\n+          read_closed_ = true;\n+          bool result = CloseIfNeeded();\n+          mu_.unlock();\n+          RecvClosed(absl::InternalError(\n+              absl::StrFormat(\"frame_size is too large: %lu\", frame_size)));\n+          return result;\n+        }\n+        size_t total_frame_size =\n+            static_cast<size_t>(frame_size) + sizeof(uint32_t);\n+        // Needs more input.\n+        if (total_frame_size > recv_count_) {\n+          break;\n+        }\n+        absl::string_view buffer(network_buffer_.get() + sizeof(uint32_t),\n+                                 frame_size);\n+        HandlePacket(buffer);\n+        if (total_frame_size < recv_count_) {\n+          memmove(network_buffer_.get(),\n+                  network_buffer_.get() + total_frame_size,\n+                  recv_count_ - total_frame_size);\n+        }\n+        recv_count_ -= total_frame_size;\n+      }\n+    } else if (recv_size == -1 && errno == EAGAIN) {\n+    } else {\n+      mu_.lock();\n+      shutdown(fd_, SHUT_RD);\n+      read_closed_ = true;\n+      bool result = CloseIfNeeded();\n+      mu_.unlock();\n+      if (recv_size == 0) {\n+        RecvClosed(absl::OkStatus());\n+      } else {\n+        RecvClosed(absl::InternalError(\n+            absl::StrFormat(\"%ld = recv() failed errno: %d err: %s\", recv_size,\n+                            errno, strerror(errno))));\n+      }\n+      return result;\n+    }\n+  }\n+  if (events.revents & POLLOUT) {\n+    can_send_ = true;\n+  }\n+  if (can_send_ && !write_closed_) {\n+    mu_.lock();\n+    while (!frames_.empty() && can_send_) {\n+      auto& packet_to_send = frames_.front();\n+      if (packet_to_send.empty()) {\n+        shutdown(fd_, SHUT_WR);\n+        write_closed_ = true;\n+        bool result = CloseIfNeeded();\n+        mu_.unlock();\n+        SendClosed(absl::OkStatus());\n+        return result;\n+      }\n+      const void* base = packet_to_send.data() + write_offset_;\n+      size_t size = packet_to_send.size() - write_offset_;\n+      ssize_t send_size = send(fd_, base, size, 0);\n+      if (send_size > 0) {\n+        write_offset_ += send_size;\n+        if (send_size == size) {\n+          write_offset_ = 0;\n+          frames_.pop_front();\n+        } else {\n+          can_send_ = false;\n+        }\n+      } else if (send_size < 0 && errno == EAGAIN) {\n+        can_send_ = false;\n+      } else {\n+        shutdown(fd_, SHUT_WR);\n+        write_closed_ = true;\n+        bool result = CloseIfNeeded();\n+        mu_.unlock();\n+        if (send_size == 0) {\n+          SendClosed(absl::OkStatus());\n+        } else {\n+          SendClosed(absl::InternalError(\n+              absl::StrFormat(\"%ld = send() failed errno: %d err: %s\",\n+                              send_size, errno, strerror(errno))));\n+        }\n+        return result;\n+      }\n+    }\n+    mu_.unlock();\n+  }\n+  if ((events.revents & POLLHUP) && !write_closed_) {\n+    int error = 0;\n+    socklen_t len = sizeof(error);\n+    if (getsockopt(fd_, SOL_SOCKET, SO_ERROR, &error, &len) != 0) {\n+      error = errno;\n+    }\n+\n+    mu_.lock();\n+    write_closed_ = true;\n+    bool result = CloseIfNeeded();\n+    mu_.unlock();\n+    if (error == 0) {\n+      SendClosed(absl::OkStatus());\n+    } else {\n+      SendClosed(absl::InternalError(absl::StrFormat(\n+          \"fd failed with hup: errno: %d err: %s\", error, strerror(error))));\n+    }\n+    return result;\n+  }\n+  return true;\n+}\n+\n+bool SocketFdPacketState::SendRawFrame(std::string opacket) {\n+  bool should_send_wake = false;\n+  {\n+    absl::MutexLock l(mu_);\n+    // Allow buffering only before connect.\n+    if (is_connected_ && write_closed_) {\n+      return false;\n+    }\n+    should_send_wake = frames_.empty() && fd_ != -1;\n+    frames_.push_back(std::move(opacket));\n+  }\n+  if (should_send_wake) {\n+    loop()->SendWake(this);\n+  }\n+  return true;\n+}\n+\n+void SocketFdPacketState::RegisterFd(int fd, bool start_connected) {\n+  {\n+    absl::MutexLock l(mu_);\n+    fd_ = fd;\n+    is_connected_ = start_connected;\n+    read_closed_ = false;\n+    write_closed_ = false;\n+  }\n+  Register();\n+}\n+\n+void SocketFdPacketState::Shutdown(int how) {\n+  absl::MutexLock l(mu_);\n+  shutdown(fd_, how);\n+}\n+\n+bool SocketFdPacketState::CloseIfNeeded() {\n+  if (is_connected_ && write_closed_) {\n+    frames_.clear();\n+  }\n+  if (write_closed_) {\n+    shutdown(fd_, SHUT_WR);\n+  }\n+  if (read_closed_) {\n+    shutdown(fd_, SHUT_RD);\n+  }\n+  bool result = !read_closed_ || !write_closed_;\n+  if (!result) {\n+    close(fd_);\n+    fd_ = -1;\n+  }\n+  return result;\n+}\n+\n }  // namespace aux"
        },
        {
            "sha": "d120e70652e513bad33bd0eb1938d850cc4177a5",
            "filename": "third_party/xla/xla/python/transfer/event_loop.h",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.h?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -18,11 +18,13 @@ limitations under the License.\n #include <netinet/in.h>\n #include <poll.h>\n \n+#include <deque>\n #include <memory>\n \n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/time/time.h\"\n \n // socket.h in conda sysroot include directory does not define\n@@ -131,6 +133,52 @@ class SocketListener {\n   Handler* handler_ = nullptr;\n };\n \n+class SocketFdPacketState : public PollEventLoop::Handler {\n+ public:\n+  // Must be closed and cleared properly before destruction.\n+  ~SocketFdPacketState() override;\n+\n+  // Subclasses must handle the incoming packet entirely during this call\n+  // (or else copy).\n+  virtual void HandlePacket(absl::string_view packet) = 0;\n+\n+  // All of these may destroy the handler if both directions are closed.\n+  virtual void ConnectFailed() = 0;\n+  // Clean half close.\n+  virtual void RecvClosed(absl::Status error) = 0;\n+  // Clean half close\n+  virtual void SendClosed(absl::Status error) = 0;\n+\n+  // Schedules the frame (returns false if send is closed).\n+  bool SendRawFrame(std::string opacket);\n+\n+  // Starts listening for fd.\n+  // ConnectFailed() only called if start_connected=false.\n+  void RegisterFd(int fd, bool start_connected);\n+\n+  // Calls shutdown on the fd.\n+  void Shutdown(int how);\n+\n+ private:\n+  void PopulatePollInfo(pollfd& events) override;\n+\n+  bool HandleEvents(const pollfd& events) override;\n+\n+  bool CloseIfNeeded() ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n+\n+  absl::Mutex mu_;\n+  int fd_ = -1;\n+  bool can_send_ = false;\n+  bool is_connected_ = false;\n+  bool read_closed_ = false;\n+  bool write_closed_ = false;\n+  size_t write_offset_ = 0;\n+  size_t recv_count_ = 0;\n+  std::unique_ptr<char[]> network_buffer_ =\n+      std::unique_ptr<char[]>(new char[4096]);\n+  std::deque<std::string> frames_ ABSL_GUARDED_BY(mu_);\n+};\n+\n }  // namespace aux\n \n #endif  // XLA_PYTHON_TRANSFER_EVENT_LOOP_H_"
        },
        {
            "sha": "22f4f36d5a330c654fc3d357bdc0f0d22a7254a0",
            "filename": "third_party/xla/xla/python/transfer/event_loop_test.cc",
            "status": "modified",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop_test.cc?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -19,13 +19,17 @@ limitations under the License.\n #include <sys/socket.h>\n \n #include <atomic>\n+#include <cstdint>\n+#include <deque>\n+#include <optional>\n #include <string>\n \n #include <gtest/gtest.h>\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"absl/synchronization/notification.h\"\n #include \"absl/time/clock.h\"\n #include \"absl/time/time.h\"\n@@ -166,5 +170,88 @@ TEST(EventLoopTest, TestScheduleAt) {\n   ASSERT_GE(absl::Now(), wake_time);\n }\n \n+class TestSocketFdPacketState : public SocketFdPacketState {\n+ public:\n+  void HandlePacket(absl::string_view packet) override {\n+    absl::MutexLock l(mu_);\n+    messages_.push_back(std::string(packet));\n+  }\n+\n+  void ConnectFailed() override { LOG(FATAL) << \"Not tested.\"; }\n+\n+  void RecvClosed(absl::Status error) override {\n+    absl::MutexLock l(mu_);\n+    recv_closed_status_ = error;\n+  }\n+\n+  void SendClosed(absl::Status error) override {\n+    absl::MutexLock l(mu_);\n+    send_closed_status_ = error;\n+  }\n+\n+  std::string ReadMessage() {\n+    absl::MutexLock l(mu_);\n+    auto cond = [this]() -> bool { return !messages_.empty(); };\n+    mu_.Await(absl::Condition(&cond));\n+    auto out = messages_.front();\n+    messages_.pop_front();\n+    return out;\n+  }\n+\n+  absl::Status WaitForRecvClosed() {\n+    absl::MutexLock l(mu_);\n+    auto cond = [this]() -> bool { return recv_closed_status_.has_value(); };\n+    mu_.Await(absl::Condition(&cond));\n+    return *recv_closed_status_;\n+  }\n+\n+  absl::Status WaitForSendClosed() {\n+    absl::MutexLock l(mu_);\n+    auto cond = [this]() -> bool { return send_closed_status_.has_value(); };\n+    mu_.Await(absl::Condition(&cond));\n+    return *send_closed_status_;\n+  }\n+\n+ private:\n+  absl::Mutex mu_;\n+  std::deque<std::string> messages_;\n+  std::optional<absl::Status> recv_closed_status_;\n+  std::optional<absl::Status> send_closed_status_;\n+};\n+\n+std::string MakeTestMessage(std::string msg) {\n+  uint32_t len = static_cast<uint32_t>(msg.size());\n+  return std::string(reinterpret_cast<const char*>(&len), sizeof(len)) + msg;\n+}\n+\n+TEST(EventLoopTest, SocketFdPacketState) {\n+  int fd[2];\n+  ASSERT_NE(-1, socketpair(PF_LOCAL, SOCK_STREAM | SOCK_NONBLOCK, 0, fd))\n+      << strerror(errno) << \" \" << errno;\n+  auto* conn1 = new TestSocketFdPacketState();\n+  auto* conn2 = new TestSocketFdPacketState();\n+  conn2->RegisterFd(fd[1], /*start_connected=*/true);\n+\n+  conn1->SendRawFrame(MakeTestMessage(\"secret\"));\n+  conn1->RegisterFd(fd[0], /*start_connected=*/true);\n+\n+  EXPECT_EQ(conn2->ReadMessage(), \"secret\");\n+  conn2->Shutdown(SHUT_RDWR);\n+\n+  EXPECT_TRUE(conn1->WaitForRecvClosed().ok());\n+  EXPECT_TRUE(conn1->WaitForSendClosed().ok());\n+  EXPECT_TRUE(conn2->WaitForRecvClosed().ok());\n+  EXPECT_TRUE(conn2->WaitForSendClosed().ok());\n+  PollEventLoop::GetDefault()->SendWake(conn1);\n+  PollEventLoop::GetDefault()->SendWake(conn2);\n+  delete conn1;\n+  delete conn2;\n+  // Wakes are safe.\n+  absl::Notification done_notify;\n+  PollEventLoop::GetDefault()->Schedule(\n+      [&done_notify]() { done_notify.Notify(); });\n+  done_notify.WaitForNotification();\n+}\n+\n }  // namespace\n }  // namespace aux"
        },
        {
            "sha": "2a082f0f5a7488c299b822250bfb196e458e0272",
            "filename": "third_party/xla/xla/python/transfer/socket-server.cc",
            "status": "modified",
            "additions": 101,
            "deletions": 167,
            "changes": 268,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket-server.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket-server.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket-server.cc?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/log/check.h\"\n@@ -46,24 +47,23 @@ limitations under the License.\n \n namespace aux {\n \n-class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n+class SocketServer::SocketNetworkState : public SocketFdPacketState {\n  public:\n   explicit SocketNetworkState(std::shared_ptr<PullTable> table,\n                               std::shared_ptr<BulkTransportFactory> factory,\n                               int fd)\n-      : table_(std::move(table)), factory_(std::move(factory)), fd_(fd) {\n-    is_connected_ = true;\n+      : table_(std::move(table)), factory_(std::move(factory)) {\n+    RegisterFd(fd, /*start_connected=*/true);\n   }\n   explicit SocketNetworkState(std::shared_ptr<PullTable> table,\n                               std::shared_ptr<BulkTransportFactory> factory,\n                               const SocketAddress& addr)\n       : table_(std::move(table)),\n         factory_(std::move(factory)),\n-        fd_(-1),\n         remote_addr_(addr) {\n-    StartConnect();\n   }\n-  ~SocketNetworkState() override { close(fd_); }\n+\n+  ~SocketNetworkState() override = default;\n \n   void StartConnect() {\n     int send_fd = socket(remote_addr_.address().sa_family,\n@@ -77,144 +77,41 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n     CHECK_GE(\n         setsockopt(send_fd, IPPROTO_TCP, TCP_NODELAY, &value, sizeof(value)), 0)\n         << strerror(errno) << \" \" << errno;\n-    fd_ = send_fd;\n+    RegisterFd(send_fd, /*start_connected=*/false);\n   }\n \n-  void PopulatePollInfo(pollfd& events) override {\n-    events.fd = fd_;\n-    events.events = POLLIN;\n-    if (!can_send_) {\n-      events.events = POLLOUT;\n-    }\n+  void ConnectFailed() override {\n+    loop()->ScheduleAt(absl::Now() + absl::Seconds(2),\n+                       [this]() { StartConnect(); });\n   }\n \n-  bool HandleEvents(const pollfd& events) override {\n-    tsl::profiler::TraceMe __trace(\"SocketServer::HandleEvents\");\n-    if (!is_connected_) {\n-      // poll() may remind us that fd_ is invalid while waiting to reconnect.\n-      if (fd_ == -1) {\n-        return true;\n-      }\n-      // If HUP with an error happens, then schedule a reconnect.\n-      if ((events.revents & POLLHUP) && (events.revents & POLLERR)) {\n-        fd_ = -1;\n-        loop()->ScheduleAt(absl::Now() + absl::Seconds(2),\n-                           [this]() { StartConnect(); });\n-        return true;\n-      }\n-      if (!(events.revents & POLLOUT)) {\n-        return true;\n-      }\n-      is_connected_ = true;\n-    }\n-    if (events.revents & POLLIN) {\n-      ssize_t recv_size =\n-          recv(fd_, network_buffer_.get() + recv_count_, 4096 - recv_count_, 0);\n-      if (recv_size == 0) {\n-        {\n-          absl::MutexLock l(mu_);\n-          is_poisoned_ = true;\n-          peer_is_closed_ = true;\n-          poison_status_ = absl::InternalError(\n-              \"SocketServer: Connection closed recv() == 0.\");\n-        }\n-        ClearDestTable();\n-      } else if (recv_size == -1 && errno == EAGAIN) {\n-      } else {\n-        if (recv_size < 0) {\n-          Poison(absl::InternalError(\n-              absl::StrFormat(\"%ld = recv() failed errno: %d err: %s\",\n-                              recv_size, errno, strerror(errno))));\n-          return true;\n-        }\n-        recv_count_ += recv_size;\n-        while (recv_count_ >= sizeof(uint32_t)) {\n-          uint32_t frame_size;\n-          memcpy(&frame_size, network_buffer_.get(), sizeof(uint32_t));\n-          if (frame_size < 0 || frame_size > 4096 - sizeof(uint32_t)) {\n-            Poison(absl::InternalError(\n-                absl::StrFormat(\"frame_size is too large: %lu\", frame_size)));\n-            return true;\n-          }\n-          size_t total_frame_size =\n-              static_cast<size_t>(frame_size) + sizeof(uint32_t);\n-          // Needs more input.\n-          if (total_frame_size > recv_count_) {\n-            break;\n-          }\n-          absl::string_view buffer(network_buffer_.get() + sizeof(uint32_t),\n-                                   frame_size);\n-          SocketTransferRequest req;\n-          if (!req.ParseFromArray(buffer.data(), buffer.size())) {\n-            Poison(\n-                absl::InternalError(\"Could not parse SocketTransferRequest.\"));\n-            return true;\n-          }\n-          HandlePacket(req);\n-          if (total_frame_size < recv_count_) {\n-            memmove(network_buffer_.get(),\n-                    network_buffer_.get() + total_frame_size,\n-                    recv_count_ - total_frame_size);\n-          }\n-          recv_count_ -= total_frame_size;\n-        }\n-      }\n+  void RecvClosed(absl::Status error) override {\n+    Shutdown(SHUT_RDWR);\n+    if (error.ok()) {\n+      error =\n+          absl::InternalError(\"SocketServer: Connection closed recv() == 0.\");\n     }\n-    if (events.revents & POLLOUT) {\n-      can_send_ = true;\n-    }\n-    mu_.lock();\n-    while (!frames_.empty() && can_send_) {\n-      auto& packet_to_send = frames_.front();\n-      if (packet_to_send.empty()) {\n-        shutdown(fd_, SHUT_WR);\n-        break;\n-      }\n-      const void* base = packet_to_send.data() + write_offset_;\n-      size_t size = packet_to_send.size() - write_offset_;\n-      ssize_t send_size = send(fd_, base, size, 0);\n-      if (send_size > 0) {\n-        write_offset_ += send_size;\n-        if (send_size == size) {\n-          write_offset_ = 0;\n-          frames_.pop_front();\n-        } else {\n-          can_send_ = false;\n-        }\n-      } else if (send_size < 0 && errno == EAGAIN) {\n-        can_send_ = false;\n-      } else {\n-        mu_.unlock();\n-        Poison(absl::InternalError(\n-            absl::StrFormat(\"%ld = send() failed errno: %d err: %s\", send_size,\n-                            errno, strerror(errno))));\n-        return true;\n-      }\n-    }\n-    if (peer_is_closed_ && num_refs_ == 0) {\n-      mu_.unlock();\n-      delete this;\n-      return false;\n-    }\n-    mu_.unlock();\n-    return true;\n+    Poison(error);\n+    DropSysRef();\n   }\n \n-  bool can_send_ = false;\n-  bool is_connected_ = false;\n-  size_t write_offset_ = 0;\n-  std::deque<std::string> frames_;\n+  void SendClosed(absl::Status error) override {\n+    Shutdown(SHUT_RDWR);\n+    {\n+      absl::MutexLock l(mu_);\n+      is_poisoned_ = true;\n+      poison_status_ =\n+          absl::InternalError(\"SocketServer: Connection closed recv() == 0.\");\n+    }\n+    DropSysRef();\n+  }\n \n-  void SendFrame(const SocketTransferRequest& req) {\n+  bool SendFrame(const SocketTransferRequest& req) {\n     uint32_t header = req.ByteSizeLong();\n     std::string opacket = std::string(absl::string_view(\n         reinterpret_cast<const char*>(&header), sizeof(header)));\n     req.AppendToString(&opacket);\n-    {\n-      absl::MutexLock l(mu_);\n-      frames_.push_back(std::move(opacket));\n-    }\n-    loop()->SendWake(this);\n+    return SendRawFrame(std::move(opacket));\n   }\n \n   tsl::RCReference<ChunkDestination> GetNextDest(size_t req_id, size_t offset,\n@@ -232,6 +129,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n       if (it->second.transferred_size == 0) {\n         dest = std::move(it->second.dest);\n         dests_.erase(it);\n+        CheckSendNoMorePulls();\n       } else {\n         dest = it->second.dest;\n       }\n@@ -285,6 +183,15 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n \n   BulkTransportInterface* bulk_transport() { return bulk_transport_.get(); }\n \n+  void HandlePacket(absl::string_view buffer) override {\n+    SocketTransferRequest req;\n+    if (!req.ParseFromArray(buffer.data(), buffer.size())) {\n+      Poison(absl::InternalError(\"Could not parse SocketTransferRequest.\"));\n+      return;\n+    }\n+    HandlePacket(req);\n+  }\n+\n   void HandlePacket(const SocketTransferPullRequest& req) {\n     class SocketConnectionState : public ConnectionState {\n      public:\n@@ -390,41 +297,74 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n         packet.size(), packet.bulk_transport_id(),\n         [offset = packet.offset(), dest = std::move(dest)](\n             absl::StatusOr<BulkTransportInterface::Message> msgor) {\n-          auto msg = std::move(msgor).value();\n-          CHECK_OK(\n-              dest->Put(msg.data, offset, msg.size, std::move(msg.on_done)));\n+          if (!msgor.ok()) {\n+            dest->Poison(msgor.status());\n+          } else {\n+            auto msg = std::move(msgor).value();\n+            CHECK_OK(\n+                dest->Put(msg.data, offset, msg.size, std::move(msg.on_done)));\n+          }\n         });\n   }\n \n-  void DropRef() {\n-    {\n-      absl::MutexLock l(mu_);\n-      CHECK_NE(num_refs_, 0);\n-      --num_refs_;\n-      ShutdownIfNeeded();\n+  std::unique_ptr<SocketNetworkState> DropRef() {\n+    absl::MutexLock l(mu_);\n+    CHECK_NE(num_refs_, 0);\n+    --num_refs_;\n+    ShutdownIfNeeded();\n+    return ReturnCheckIfRefsAreZero();\n+  }\n+\n+  std::unique_ptr<SocketNetworkState> DropSysRef() {\n+    absl::MutexLock l(mu_);\n+    CHECK_NE(num_sys_refs_, 0);\n+    --num_sys_refs_;\n+    ShutdownIfNeeded();\n+    return ReturnCheckIfRefsAreZero();\n+  }\n+\n+  std::unique_ptr<SocketNetworkState> ReturnCheckIfRefsAreZero()\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n+    if (num_refs_ == 0 && num_sys_refs_ == 0) {\n+      // destroy outside of mutex scope.\n+      return std::unique_ptr<SocketNetworkState>(this);\n     }\n+    return {};\n   }\n \n-  void NoMorePulls() {\n-    SocketTransferRequest msg;\n-    msg.mutable_half_close();\n-    SendFrame(msg);\n-    DropRef();\n+  void CheckSendNoMorePulls() ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n+    if (dests_.empty() && no_more_pulls_) {\n+      SocketTransferRequest msg;\n+      msg.mutable_half_close();\n+      SendFrame(msg);\n+    }\n+  }\n+\n+  void IncRef() {\n+    absl::MutexLock l(mu_);\n+    ++num_refs_;\n+  }\n+\n+  std::unique_ptr<SocketNetworkState> NoMorePulls() {\n+    absl::MutexLock l(mu_);\n+    no_more_pulls_ = true;\n+    CheckSendNoMorePulls();\n+    CHECK_NE(num_refs_, 0);\n+    --num_refs_;\n+    return ReturnCheckIfRefsAreZero();\n   }\n \n   void HandlePacket(const SocketTransferHalfClose& half_close) {\n     mu_.lock();\n-    CHECK(!peer_is_closed_);\n-    peer_is_closed_ = true;\n+    peer_half_closed_ = true;\n     ShutdownIfNeeded();\n     mu_.unlock();\n   }\n \n-  void ShutdownIfNeeded() {\n-    if (!peer_is_closed_ || num_refs_ != 0) {\n-      return;\n+  void ShutdownIfNeeded() ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n+    if (num_refs_ == 0 && peer_half_closed_) {\n+      Shutdown(SHUT_RDWR);\n     }\n-    loop()->SendWake(this);\n   }\n \n   void Pull(uint64_t uuid, int buf_id,\n@@ -470,18 +410,13 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n     std::string opacket = std::string(absl::string_view(\n         reinterpret_cast<const char*>(&header), sizeof(header)));\n     opacket += \"Injected Failure.\";\n-    {\n-      absl::MutexLock l(mu_);\n-      frames_.push_back(std::move(opacket));\n-    }\n-    loop()->SendWake(this);\n+    SendRawFrame(std::move(opacket));\n   }\n \n   static void Accept(std::shared_ptr<PullTable> table,\n                      std::shared_ptr<BulkTransportFactory> factory,\n                      int sockfd) {\n-    auto* remote = new SocketNetworkState(table, factory, sockfd);\n-    remote->Register();\n+    new SocketNetworkState(table, factory, sockfd);\n   }\n \n   void ClearDestTable() {\n@@ -501,7 +436,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n     {\n       absl::MutexLock l(mu_);\n       is_poisoned_ = true;\n-      shutdown(fd_, SHUT_RDWR);\n+      Shutdown(SHUT_RDWR);\n       poison_status_ = s;\n     }\n     ClearDestTable();\n@@ -511,15 +446,13 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n   std::shared_ptr<PullTable> table_;\n   std::shared_ptr<BulkTransportFactory> factory_;\n   absl::Mutex mu_;\n-  size_t num_refs_ = 1;\n-  bool peer_is_closed_ = false;\n+  size_t num_refs_ ABSL_GUARDED_BY(mu_) = 0;\n+  size_t num_sys_refs_ ABSL_GUARDED_BY(mu_) = 2;\n+  bool no_more_pulls_ = false;\n+  bool peer_half_closed_ = false;\n   bool is_poisoned_ = false;\n   absl::Status poison_status_;\n-  int fd_ = -1;\n   SocketAddress remote_addr_;\n-  size_t recv_count_ = 0;\n-  std::unique_ptr<char[]> network_buffer_ =\n-      std::unique_ptr<char[]>(new char[4096]);\n \n   uint64_t next_req_id_ = 0;\n   struct DestState {\n@@ -577,8 +510,9 @@ tsl::RCReference<SocketServer::Connection> SocketServer::Connect(\n     const SocketAddress& other_addr) {\n   auto* local_ =\n       new SocketNetworkState(pull_table_, bulk_transport_factory_, other_addr);\n-  local_->Register();\n   local_->StartBulkTransporting();\n+  local_->IncRef();\n+  local_->StartConnect();\n   return tsl::MakeRef<Connection>(local_);\n }\n "
        },
        {
            "sha": "49b6776278e3ed8fd404571acf84d10e90a0d10c",
            "filename": "third_party/xla/xla/python/transfer/socket_bulk_transport.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 17,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.cc?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -169,7 +169,7 @@ void ZeroCopySendAckTable::ClearAll() {\n \n class SendConnectionHandler : public PollEventLoop::Handler {\n  public:\n-  SendConnectionHandler(int fd, int bond_id,\n+  SendConnectionHandler(std::shared_ptr<int> fd, int bond_id,\n                         std::shared_ptr<SharedSendMsgQueue> msg_queue,\n                         std::shared_ptr<SharedSendWorkQueue> work_queue,\n                         size_t artificial_send_limit)\n@@ -184,7 +184,6 @@ class SendConnectionHandler : public PollEventLoop::Handler {\n #ifdef MSG_ZEROCOPY\n     table_.ClearAll();\n #endif\n-    close(fd_);\n   }\n \n   void ScheduleSendWork(aux::BulkTransportInterface::SendMessage msg) {\n@@ -198,7 +197,7 @@ class SendConnectionHandler : public PollEventLoop::Handler {\n     size_t offset = 0;\n     while (offset < msg.size) {\n       ssize_t send_count =\n-          send(fd_, reinterpret_cast<char*>(msg.data) + offset,\n+          send(*fd_, reinterpret_cast<char*>(msg.data) + offset,\n                std::min(msg.size - offset, artificial_send_limit_), 0);\n       if (send_count <= 0) {\n         break;\n@@ -215,7 +214,7 @@ class SendConnectionHandler : public PollEventLoop::Handler {\n     size_t offset = 0;\n     while (offset < msg.size) {\n       ssize_t send_count = send(\n-          fd_, reinterpret_cast<char*>(msg.data) + offset,\n+          *fd_, reinterpret_cast<char*>(msg.data) + offset,\n           std::min(msg.size - offset, artificial_send_limit_), MSG_ZEROCOPY);\n       if (send_count <= 0) {\n         break;\n@@ -254,7 +253,7 @@ class SendConnectionHandler : public PollEventLoop::Handler {\n   }\n \n   void PopulatePollInfo(pollfd& events) override {\n-    events.fd = fd_;\n+    events.fd = *fd_;\n     events.events = POLLERR | POLLRDHUP;\n     if (state_.load() == SocketState::kNotReady) {\n       events.events |= POLLOUT;\n@@ -267,7 +266,7 @@ class SendConnectionHandler : public PollEventLoop::Handler {\n       return false;\n     } else if (events.revents & POLLERR) {\n #ifdef MSG_ZEROCOPY\n-      CHECK_OK(table_.HandleSocketErrors(fd_));\n+      CHECK_OK(table_.HandleSocketErrors(*fd_));\n #endif\n     } else if (events.revents & POLLOUT) {\n       if (no_more_messages_.load() == true) {\n@@ -313,7 +312,7 @@ class SendConnectionHandler : public PollEventLoop::Handler {\n #ifdef MSG_ZEROCOPY\n   ZeroCopySendAckTable table_;\n #endif\n-  int fd_;\n+  std::shared_ptr<int> fd_;\n   int bond_id_;\n   std::shared_ptr<SharedSendMsgQueue> msg_queue_;\n   std::shared_ptr<SharedSendWorkQueue> work_queue_;\n@@ -423,7 +422,8 @@ void SharedSendMsgQueue::NoMoreMessages() {\n }\n \n void SharedSendMsgQueue::StartSubConnectionSender(\n-    int fd, int bond_id, std::shared_ptr<SharedSendMsgQueue> msg_queue,\n+    std::shared_ptr<int> fd, int bond_id,\n+    std::shared_ptr<SharedSendMsgQueue> msg_queue,\n     std::shared_ptr<SharedSendWorkQueue> work_queue,\n     size_t artificial_send_limit) {\n   auto* handler =\n@@ -464,7 +464,7 @@ void RecvThreadState::DoRecvWork() {\n }\n \n void RecvThreadState::ScheduleRecvWork(\n-    size_t recv_size, int fd,\n+    size_t recv_size, std::shared_ptr<int> fd,\n     absl::AnyInvocable<\n         void(absl::StatusOr<aux::BulkTransportInterface::Message> msg) &&>\n         on_recv) {\n@@ -517,11 +517,11 @@ absl::Status RecvThreadState::HandleRecvItem(recv_work_item& work,\n       zc.length = work.recv_size - offset;\n \n       struct pollfd fds[1];\n-      fds[0] = {.fd = work.fd, .events = POLLIN, .revents = 0};\n+      fds[0] = {.fd = *work.fd, .events = POLLIN, .revents = 0};\n       poll(&fds[0], 1, -1);\n \n       res =\n-          getsockopt(work.fd, IPPROTO_TCP, TCP_ZEROCOPY_RECEIVE, &zc, &zc_len);\n+          getsockopt(*work.fd, IPPROTO_TCP, TCP_ZEROCOPY_RECEIVE, &zc, &zc_len);\n       if (res == -1) {\n         std::move(alloc.on_done)();\n         return absl::ErrnoToStatus(errno, \"zero-copy-recv\");\n@@ -549,7 +549,7 @@ absl::Status RecvThreadState::HandleRecvItem(recv_work_item& work,\n     }\n     while (offset != work.recv_size) {\n       ssize_t recv_count =\n-          recv(work.fd, reinterpret_cast<char*>(alloc.data) + offset,\n+          recv(*work.fd, reinterpret_cast<char*>(alloc.data) + offset,\n                work.recv_size - offset, 0);\n       if (recv_count == 0) {\n         std::move(alloc.on_done)();\n@@ -599,7 +599,7 @@ class SocketBulkTransport : public BulkTransportInterface {\n       override {\n     auto& conn = connections_[bond_id];\n     absl::MutexLock l(conn->mu);\n-    if (conn->fd == -1) {\n+    if (!conn->fd) {\n       conn->pending_recvs.push_back({size, std::move(on_recv)});\n     } else {\n       conn->thread_state->ScheduleRecvWork(size, conn->fd, std::move(on_recv));\n@@ -617,16 +617,21 @@ class SocketBulkTransport : public BulkTransportInterface {\n     std::shared_ptr<SharedSendMsgQueue> send_msg_queue;\n     std::shared_ptr<SharedSendWorkQueue> send_work_queue;\n     std::vector<PendingRecv> pending_recvs;\n-    int fd = -1;\n+    std::shared_ptr<int> fd;\n \n     void AcceptSock(int accept_fd) {\n+      auto shared_fd =\n+          std::shared_ptr<int>(new int{accept_fd}, [](int* fd_ptr) {\n+            close(*fd_ptr);\n+            delete fd_ptr;\n+          });\n       SharedSendMsgQueue::StartSubConnectionSender(\n-          accept_fd, connection_id, send_msg_queue, send_work_queue);\n+          shared_fd, connection_id, send_msg_queue, send_work_queue);\n       {\n         absl::MutexLock l(mu);\n-        fd = accept_fd;\n+        fd = shared_fd;\n         for (auto& pending_recv : pending_recvs) {\n-          thread_state->ScheduleRecvWork(pending_recv.size, accept_fd,\n+          thread_state->ScheduleRecvWork(pending_recv.size, fd,\n                                          std::move(pending_recv.on_recv));\n         }\n       }"
        },
        {
            "sha": "de3c4d73a2742d0516a7807f397652483b6d699e",
            "filename": "third_party/xla/xla/python/transfer/socket_bulk_transport.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.h?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -125,7 +125,8 @@ class SharedSendMsgQueue {\n \n   // Starts a sender for 1 part of the thread.\n   static void StartSubConnectionSender(\n-      int fd, int bond_id, std::shared_ptr<SharedSendMsgQueue> msg_queue,\n+      std::shared_ptr<int> fd, int bond_id,\n+      std::shared_ptr<SharedSendMsgQueue> msg_queue,\n       std::shared_ptr<SharedSendWorkQueue> work_queue,\n       size_t artificial_send_limiti = std::numeric_limits<size_t>::max());\n \n@@ -148,7 +149,7 @@ class RecvThreadState {\n  public:\n   // Schedules recv() syscall on a particular fd.\n   void ScheduleRecvWork(\n-      size_t recv_size, int fd,\n+      size_t recv_size, std::shared_ptr<int> fd,\n       absl::AnyInvocable<\n           void(absl::StatusOr<aux::BulkTransportInterface::Message> msg) &&>\n           on_recv);\n@@ -166,7 +167,7 @@ class RecvThreadState {\n \n   struct recv_work_item {\n     size_t recv_size;\n-    int fd;\n+    std::shared_ptr<int> fd;\n     absl::AnyInvocable<\n         void(absl::StatusOr<aux::BulkTransportInterface::Message> msg) &&>\n         on_recv;"
        },
        {
            "sha": "9bffcd72b8506f053b673dd73c358d3c418216cf",
            "filename": "third_party/xla/xla/python/transfer/socket_bulk_transport_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 8,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f322c0c82d587b755108d31203bf741152c0ff81/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport_test.cc?ref=f322c0c82d587b755108d31203bf741152c0ff81",
            "patch": "@@ -69,6 +69,13 @@ absl::Status SetupSocketPairUsingEventLoop(int& send_fd, int& recv_fd) {\n   return absl::OkStatus();\n }\n \n+std::shared_ptr<int> WrapSocket(int fd) {\n+  return std::shared_ptr<int>(new int{fd}, [](int* fd_ptr) {\n+    close(*fd_ptr);\n+    delete fd_ptr;\n+  });\n+};\n+\n TEST(SendQueue, TestZeroCopyQueueCleanRemoteShutdown) {\n   int send_fd, recv_fd;\n   auto status = SetupSocketPairUsingEventLoop(send_fd, recv_fd);\n@@ -77,8 +84,8 @@ TEST(SendQueue, TestZeroCopyQueueCleanRemoteShutdown) {\n   auto work_queue = SharedSendWorkQueue::Start();\n   auto msg_queue = std::make_shared<SharedSendMsgQueue>();\n \n-  SharedSendMsgQueue::StartSubConnectionSender(send_fd, 0, msg_queue,\n-                                               work_queue);\n+  SharedSendMsgQueue::StartSubConnectionSender(WrapSocket(send_fd), 0,\n+                                               msg_queue, work_queue);\n \n   std::string txt_msg(\"hello world\");\n   absl::Notification notify;\n@@ -101,15 +108,16 @@ TEST(SendQueue, SendAndRecvQueuesArtificialLimit) {\n                            packet_size);\n   auto recv_thread = RecvThreadState::Create(allocator, uallocator);\n \n-  int send_fd, recv_fd;\n-  auto status = SetupSocketPairUsingEventLoop(send_fd, recv_fd);\n+  int send_fd, raw_recv_fd;\n+  auto status = SetupSocketPairUsingEventLoop(send_fd, raw_recv_fd);\n   ASSERT_TRUE(status.ok()) << status;\n+  auto recv_fd = WrapSocket(raw_recv_fd);\n \n   auto work_queue = SharedSendWorkQueue::Start();\n   auto msg_queue = std::make_shared<SharedSendMsgQueue>();\n \n-  SharedSendMsgQueue::StartSubConnectionSender(send_fd, 0, msg_queue,\n-                                               work_queue, 64);\n+  SharedSendMsgQueue::StartSubConnectionSender(WrapSocket(send_fd), 0,\n+                                               msg_queue, work_queue, 64);\n \n   std::string txt_msg;\n \n@@ -162,9 +170,10 @@ TEST(SendQueue, SendAndRecvQueuesEarlyClose) {\n                            packet_size);\n   auto recv_thread = RecvThreadState::Create(std::nullopt, uallocator);\n \n-  int send_fd, recv_fd;\n-  auto status = SetupSocketPairUsingEventLoop(send_fd, recv_fd);\n+  int send_fd, raw_recv_fd;\n+  auto status = SetupSocketPairUsingEventLoop(send_fd, raw_recv_fd);\n   ASSERT_TRUE(status.ok()) << status;\n+  auto recv_fd = WrapSocket(raw_recv_fd);\n \n   close(send_fd);\n "
        }
    ],
    "stats": {
        "total": 682,
        "additions": 487,
        "deletions": 195
    }
}