{
    "author": "mkuperst",
    "message": "[XLA] Remove module groups from more APIs.\n\nThis changes Compiler::Compile and Service::BuildExecutables() to take a single HLO module rather than a module group.\n\nThis isn't a complete cleanup in the sense that while the methods now *take* a single module, they still *return* vectors of results even though they shouldn't, but I'd rather defer that to a separate CL.\n\nPiperOrigin-RevId: 816668279",
    "sha": "31f5a90587336ead1ac1d3b49b69eebaa967001c",
    "files": [
        {
            "sha": "5aebab0ce49c9e0b8f3ce93761cfbe8e095a5553",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n@@ -198,8 +198,8 @@ class MockCompiler : public Compiler {\n                const CompileOptions& options),\n               (override));\n   MOCK_METHOD(absl::StatusOr<std::vector<std::unique_ptr<Executable>>>, Compile,\n-              (std::unique_ptr<HloModuleGroup> module_group,\n-               std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n+              (std::unique_ptr<HloModule> hlo_module,\n+               std::vector<se::StreamExecutor*> stream_execs,\n                const CompileOptions& options),\n               (override));\n   MOCK_METHOD("
        },
        {
            "sha": "ab5008af59d4c3280d7c208079777d814bb20c67",
            "filename": "third_party/xla/xla/backends/interpreter/compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 20,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fcompiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fcompiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fcompiler.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -29,7 +29,6 @@ limitations under the License.\n #include \"xla/hlo/evaluator/hlo_evaluator.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/hlo/transforms/expanders/cholesky_expander.h\"\n #include \"xla/hlo/transforms/expanders/dynamic_index_splitter.h\"\n@@ -147,25 +146,13 @@ absl::StatusOr<std::unique_ptr<Executable>> InterpreterCompiler::RunBackend(\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<Executable>>>\n-InterpreterCompiler::Compile(\n-    std::unique_ptr<HloModuleGroup> module_group,\n-    std::vector<std::vector<se::StreamExecutor*>> stream_exec,\n-    const CompileOptions& options) {\n-  if (module_group->empty()) {\n-    return std::vector<std::unique_ptr<Executable>>();\n-  }\n-  if (module_group->size() > 1) {\n-    return tsl::errors::Unimplemented(\n-        \"Compilation of multiple HLO modules is not supported on Interpreter.\");\n-  }\n-  if (stream_exec.size() != 1 || stream_exec[0].size() != 1) {\n-    return tsl::errors::Unimplemented(\"Unexpected number of StreamExecutor's.\");\n-  }\n-  auto hlo_modules = module_group->ConsumeModules();\n-  TF_ASSIGN_OR_RETURN(auto module, RunHloPasses(std::move(hlo_modules[0]),\n-                                                stream_exec[0][0], options));\n-  TF_ASSIGN_OR_RETURN(auto executable, RunBackend(std::move(module),\n-                                                  stream_exec[0][0], options));\n+InterpreterCompiler::Compile(std::unique_ptr<HloModule> hlo_module,\n+                             std::vector<se::StreamExecutor*> stream_exec,\n+                             const CompileOptions& options) {\n+  TF_ASSIGN_OR_RETURN(\n+      hlo_module, RunHloPasses(std::move(hlo_module), stream_exec[0], options));\n+  TF_ASSIGN_OR_RETURN(auto executable, RunBackend(std::move(hlo_module),\n+                                                  stream_exec[0], options));\n   std::vector<std::unique_ptr<Executable>> ret;\n   ret.push_back(std::move(executable));\n   return std::move(ret);"
        },
        {
            "sha": "b8ab717247a2c1d4d2b35665ad65f5cebd01420e",
            "filename": "third_party/xla/xla/backends/interpreter/compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Finterpreter%2Fcompiler.h?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/interpreter/platform_id.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -51,8 +50,8 @@ class InterpreterCompiler : public Compiler {\n       std::unique_ptr<HloModule> hlo_module, se::StreamExecutor* stream_exec,\n       const CompileOptions& options) override;\n   absl::StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(\n-      std::unique_ptr<HloModuleGroup> module_group,\n-      std::vector<std::vector<se::StreamExecutor*>> stream_exec,\n+      std::unique_ptr<HloModule> hlo_module,\n+      std::vector<se::StreamExecutor*> stream_exec,\n       const CompileOptions& options) override;\n \n   absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>"
        },
        {
            "sha": "351a900ec7f2cb16b7b5e140572a7f8b1ec29ecb",
            "filename": "third_party/xla/xla/client/client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fclient%2Fclient.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fclient%2Fclient.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fclient%2Fclient.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -139,18 +139,16 @@ absl::StatusOr<std::unique_ptr<GlobalData>> Client::Execute(\n     *options_storage->add_device_handles() = std::move(device_handles[0]);\n   }\n \n-  std::vector<XlaComputationInstance> computation_instances = {\n-      XlaComputationInstance{\n-          computation,\n-          std::vector<GlobalData*>(arguments.begin(), arguments.end()),\n-          *execution_options, execution_profile}};\n+  XlaComputationInstance computation_instance{\n+      computation, std::vector<GlobalData*>(arguments.begin(), arguments.end()),\n+      *execution_options, execution_profile};\n \n   // Instead of invoking Compile() and Execute(), invoke\n   // Service::ExecuteParallel() to execute our one computation.  Compile()\n   // caches the executable forever, which isn't what we want.\n   VLOG(1) << \"Making ExecuteParallel request: \"\n           << execution_options->DebugString();\n-  TF_ASSIGN_OR_RETURN(auto results, ExecuteParallel(computation_instances));\n+  TF_ASSIGN_OR_RETURN(auto results, stub_->ExecuteGraph(computation_instance));\n   VLOG(1) << \"ExecuteParallel request done.\";\n \n   // The result selection is a bit hacky, but better than assuming it is\n@@ -170,11 +168,6 @@ absl::StatusOr<std::unique_ptr<GlobalData>> Client::Execute(\n   return std::move(results[0]);\n }\n \n-absl::StatusOr<std::vector<std::unique_ptr<GlobalData>>>\n-Client::ExecuteParallel(absl::Span<const XlaComputationInstance> computations) {\n-  return stub_->ExecuteGraphParallel(computations);\n-}\n-\n absl::StatusOr<std::vector<DeviceHandle>> Client::GetDeviceHandles(\n     int64_t device_count) {\n   if (device_count < 1) {"
        },
        {
            "sha": "b02411638164462fef7add3684707cdc1105d146",
            "filename": "third_party/xla/xla/service/compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/debug_options_flags.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/service/metrics_hook_interface.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/dnn.h\""
        },
        {
            "sha": "8e6b6a2440fb1b01cde7008f24a3c5e3911b1ea0",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -37,7 +37,6 @@ limitations under the License.\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_value.h\"\n@@ -259,14 +258,14 @@ class Compiler {\n   // TODO(b/68666782): Remove this method after adding support for multiple\n   // modules to RunHloPasses and RunBackends.\n   virtual absl::StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(\n-      std::unique_ptr<HloModuleGroup> module_group,\n-      std::vector<std::vector<se::StreamExecutor*>> stream_exec,\n+      std::unique_ptr<HloModule> hlo_module,\n+      std::vector<se::StreamExecutor*> stream_exec,\n       const CompileOptions& options) = 0;\n   absl::StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(\n-      std::unique_ptr<HloModuleGroup> module_group,\n-      std::vector<std::vector<se::StreamExecutor*>> stream_exec,\n+      std::unique_ptr<HloModule> hlo_module,\n+      std::vector<se::StreamExecutor*> stream_exec,\n       se::DeviceMemoryAllocator* device_allocator) {\n-    return Compile(std::move(module_group), stream_exec,\n+    return Compile(std::move(hlo_module), stream_exec,\n                    CompileOptions{device_allocator});\n   }\n "
        },
        {
            "sha": "ae6f2b55973411f84122649e4a9d81c251663777",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -113,7 +113,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/hlo/pass/hlo_pass_fix.h\"\n@@ -354,16 +353,14 @@ CpuCompiler::CpuCompiler() {\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<Executable>>> CpuCompiler::Compile(\n-    std::unique_ptr<HloModuleGroup> module_group,\n-    std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n+    std::unique_ptr<HloModule> hlo_module,\n+    std::vector<se::StreamExecutor*> stream_execs,\n     const CompileOptions& options) {\n-  for (const std::vector<se::StreamExecutor*>& se_vector : stream_execs) {\n-    if (se_vector.size() != 1) {\n-      return Unimplemented(\n-          \"Model partitioning not implemented for the CPU compiler\");\n-    }\n+  if (stream_execs.size() != 1) {\n+    return Unimplemented(\n+        \"Model partitioning not implemented for the CPU compiler\");\n   }\n-  return LLVMCompiler::Compile(std::move(module_group), stream_execs, options);\n+  return LLVMCompiler::Compile(std::move(hlo_module), stream_execs, options);\n }\n \n /* static */ void CpuCompiler::InitializeLLVMTarget() {"
        },
        {
            "sha": "0816a51979f7d2cef9bd137c65ae0f973f3210be",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.h?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"xla/backends/cpu/codegen/ir_compiler.h\"\n #include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/compiler.h\"\n@@ -62,8 +61,8 @@ class CpuCompiler : public LLVMCompiler {\n   ~CpuCompiler() override = default;\n \n   absl::StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(\n-      std::unique_ptr<HloModuleGroup> module_group,\n-      std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n+      std::unique_ptr<HloModule> hlo_module,\n+      std::vector<se::StreamExecutor*> stream_execs,\n       const CompileOptions& options) override;\n \n   absl::StatusOr<std::unique_ptr<HloModule>> RunHloPasses("
        },
        {
            "sha": "ba580c9997131df983583b396081f3d5711fe217",
            "filename": "third_party/xla/xla/service/cpu/tests/cpu_aot_export_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fcpu_aot_export_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fcpu_aot_export_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fcpu_aot_export_test.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -49,10 +49,9 @@ class CpuAotCompilationTest : public HloTestBase {\n                             platform->ExecutorForDevice(0));\n \n     // JIT compile executable\n-    auto module_group = std::make_unique<HloModuleGroup>(std::move(module));\n     TF_ASSERT_OK_AND_ASSIGN(\n         std::vector<std::unique_ptr<Executable>> executables,\n-        compiler->Compile(std::move(module_group), {{stream_exec}}, nullptr));\n+        compiler->Compile(std::move(module), {stream_exec}, nullptr));\n \n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AotCompilationResult> exported_aot_result,"
        },
        {
            "sha": "7338fcd3650d2f41abed0f3df022985ffa40f66b",
            "filename": "third_party/xla/xla/service/hlo_runner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -759,11 +759,10 @@ HloRunner::CreateExecutableWithBufferAssignment(\n       module->mutable_config().set_intra_op_parallelism_threads(\n           backend().eigen_intra_op_thread_pool()->NumThreads());\n     }\n-    auto module_group = std::make_unique<HloModuleGroup>(std::move(module));\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<Executable>> executables,\n-        backend().compiler()->Compile(std::move(module_group),\n-                                      {{backend().default_stream_executor()}},\n+        backend().compiler()->Compile(std::move(module),\n+                                      {backend().default_stream_executor()},\n                                       backend().memory_allocator()));\n     return std::make_unique<HloRunnerExecutable>(this,\n                                                  std::move(executables[0]));"
        },
        {
            "sha": "8f92011d2bc84840fe08d4684701d0e7597e9e03",
            "filename": "third_party/xla/xla/service/llvm_compiler.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/stream_pool.h\"\n #include \"tsl/platform/denormal.h\"\n@@ -35,8 +34,8 @@ limitations under the License.\n \n namespace xla {\n absl::StatusOr<std::vector<std::unique_ptr<Executable>>> LLVMCompiler::Compile(\n-    std::unique_ptr<HloModuleGroup> module_group,\n-    std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n+    std::unique_ptr<HloModule> hlo_module,\n+    std::vector<se::StreamExecutor*> stream_execs,\n     const CompileOptions& options) {\n   // Tensorflow tries to enable the following behaviors in all its threads:\n   //\n@@ -51,20 +50,16 @@ absl::StatusOr<std::vector<std::unique_ptr<Executable>>> LLVMCompiler::Compile(\n   tsl::port::ScopedDontFlushDenormal dont_flush_denormals;\n \n   std::vector<std::unique_ptr<Executable>> result;\n-  std::vector<std::unique_ptr<HloModule>> modules =\n-      module_group->ConsumeModules();\n-  for (size_t i = 0; i < modules.size(); i++) {\n-    tsl::profiler::ScopedAnnotation annotation{[&] {\n-      return absl::StrFormat(\"XlaCompile:#module=%s,program_id=%d#\",\n-                             modules[i]->name(), modules[i]->unique_id());\n-    }};\n-    TF_ASSIGN_OR_RETURN(modules[i], RunHloPasses(std::move(modules[i]),\n-                                                 stream_execs[i][0], options));\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<Executable> executable,\n-        RunBackend(std::move(modules[i]), stream_execs[i][0], options));\n-    result.push_back(std::move(executable));\n-  }\n+  tsl::profiler::ScopedAnnotation annotation{[&] {\n+    return absl::StrFormat(\"XlaCompile:#module=%s,program_id=%d#\",\n+                           hlo_module->name(), hlo_module->unique_id());\n+  }};\n+  TF_ASSIGN_OR_RETURN(hlo_module, RunHloPasses(std::move(hlo_module),\n+                                               stream_execs[0], options));\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<Executable> executable,\n+      RunBackend(std::move(hlo_module), stream_execs[0], options));\n+  result.push_back(std::move(executable));\n \n   return std::move(result);\n }"
        },
        {
            "sha": "c37065b6d752a4eb62460c01b543b6e31c5f6f30",
            "filename": "third_party/xla/xla/service/llvm_compiler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fllvm_compiler.h?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"llvm/IR/Module.h\"\n #include \"xla/executable_run_options.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n \n@@ -80,8 +80,8 @@ class LLVMCompiler : public Compiler {\n   using Compiler::RunHloPasses;\n \n   absl::StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(\n-      std::unique_ptr<HloModuleGroup> module_group,\n-      std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n+      std::unique_ptr<HloModule> hlo_module,\n+      std::vector<se::StreamExecutor*> stream_execs,\n       const CompileOptions& options) override;\n \n  protected:"
        },
        {
            "sha": "d35117e0248b0883f0d675e656d3344201d818ba",
            "filename": "third_party/xla/xla/service/local_service.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flocal_service.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -110,15 +110,12 @@ LocalService::CompileExecutables(\n     executables.push_back(std::move(executable));\n     return executables;\n   } else {\n-    std::vector<std::unique_ptr<HloModuleConfig>> module_configs;\n-    module_configs.push_back(std::move(module_config));\n     // BuildExecutables uses the executors length to determine the number of\n     // cores per module, but otherwise only uses the first executor.\n     std::vector<se::StreamExecutor*> executors(build_options.num_partitions(),\n                                                executor);\n-\n     return BuildExecutables(\n-        /*module_protos=*/{&computation.proto()}, std::move(module_configs),\n+        /*module_proto=*/&computation.proto(), std::move(module_config),\n         execute_backend_.get(), {executors}, compile_options,\n         build_options.run_backend_only());\n   }"
        },
        {
            "sha": "a5845f6c2bb62c699e16d0e9539b61233dbf1edc",
            "filename": "third_party/xla/xla/service/service.cc",
            "status": "modified",
            "additions": 107,
            "deletions": 266,
            "changes": 373,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fservice.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -257,47 +257,35 @@ absl::StatusOr<std::unique_ptr<HloModuleConfig>> Service::CreateModuleConfig(\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<Executable>>>\n-Service::BuildExecutables(\n-    const std::vector<const HloModuleProto*>& module_protos,\n-    std::vector<std::unique_ptr<HloModuleConfig>> module_configs,\n-    Backend* backend, std::vector<std::vector<se::StreamExecutor*>> executors,\n-    const Compiler::CompileOptions& options, bool run_backend_only) {\n+Service::BuildExecutables(const HloModuleProto* module_proto,\n+                          std::unique_ptr<HloModuleConfig> module_config,\n+                          Backend* backend,\n+                          std::vector<se::StreamExecutor*> executors,\n+                          const Compiler::CompileOptions& options,\n+                          bool run_backend_only) {\n   VLOG(1) << StrFormat(\"BuildExecutable on service %p\", this);\n+  VLOG(1) << \"Computation :\" << module_proto->name();\n \n-  VLOG(1) << \"Computations:\";\n-  for (const HloModuleProto* proto : module_protos) {\n-    VLOG(1) << proto->name();\n-  }\n-\n-  CHECK_EQ(module_protos.size(), module_configs.size());\n-  if (module_protos.size() != 1) {\n-    return InvalidArgument(\"BuildExecutables only supports a single module.\");\n-  }\n-  const HloModuleProto* proto = module_protos[0];\n-  const HloModuleConfig& config = *module_configs[0];\n-  TF_ASSIGN_OR_RETURN(auto module,\n-                      CreateModuleFromProto(*proto, config, run_backend_only));\n+  TF_ASSIGN_OR_RETURN(\n+      auto module,\n+      CreateModuleFromProto(*module_proto, *module_config, run_backend_only));\n   module->set_layout_canonicalization_callback(\n       options.layout_canonicalization_callback);\n   UpdateEntryComputationLayout(\n       module.get(), std::bind(&Compiler::DefaultDeviceShapeRepresentation,\n                               backend->compiler(), std::placeholders::_1));\n   DumpHloModuleIfEnabled(*module, kBeforeOptimizationsDumpName);\n-  auto module_group = std::make_unique<HloModuleGroup>(std::move(module));\n \n   std::vector<std::unique_ptr<Executable>> executables;\n   if (!run_backend_only) {\n-    TF_ASSIGN_OR_RETURN(executables, backend->compiler()->Compile(\n-                                         std::move(module_group),\n-                                         std::move(executors), options));\n+    TF_ASSIGN_OR_RETURN(executables,\n+                        backend->compiler()->Compile(\n+                            std::move(module), std::move(executors), options));\n   } else {\n-    auto modules = module_group->ConsumeModules();\n-    for (std::unique_ptr<HloModule>& module : modules) {\n-      TF_ASSIGN_OR_RETURN(std::unique_ptr<Executable> executable,\n-                          backend->compiler()->RunBackend(\n-                              std::move(module), executors[0][0], options));\n-      executables.push_back(std::move(executable));\n-    }\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<Executable> executable,\n+                        backend->compiler()->RunBackend(std::move(module),\n+                                                        executors[0], options));\n+    executables.push_back(std::move(executable));\n   }\n \n   return std::move(executables);\n@@ -329,97 +317,6 @@ Service::BuildAotResults(\n   return std::move(aot_results);\n }\n \n-absl::StatusOr<std::vector<GlobalDataHandle>>\n-Service::ExecuteParallelAndRegisterResult(\n-    absl::Span<Executable* const> executables,\n-    absl::Span<const std::vector<std::vector<const ShapedBuffer*>>> arguments,\n-    Backend* backend, absl::Span<const DeviceHandle> device_handles,\n-    absl::Span<const std::string> result_tags, ExecutionProfile* profile) {\n-  // Streams where the computation are launched, so we can wait on the streams\n-  // to complete.\n-  std::vector<StreamPool::Ptr> streams;\n-\n-  // Global data handles for the computation results, one for each computation.\n-  std::vector<GlobalDataHandle> result_handles;\n-\n-  // Device ID to stream executor, populated only with devices that are being\n-  // profiled.\n-  std::map<int64_t, se::Stream*> index_to_profiled_streams;\n-\n-  // Build DeviceAssignment for all cores based on the provided device handles.\n-  DeviceAssignment device_assignment(options_.number_of_replicas(),\n-                                     executables.size());\n-  for (int64_t i = 0; i < executables.size(); i++) {\n-    TF_ASSIGN_OR_RETURN(auto replicas, Replicas(*backend, device_handles[i]));\n-    CHECK_EQ(replicas.size(), arguments[i].size());\n-    for (int64_t replica = 0, end = replicas.size(); replica < end; ++replica) {\n-      device_assignment(replica, i) = replicas[replica]->device_ordinal();\n-    }\n-  }\n-\n-  for (int64_t i = 0, end = executables.size(); i < end; i++) {\n-    // Stream executors for the replicas of the current computation.\n-    TF_ASSIGN_OR_RETURN(auto replicas, Replicas(*backend, device_handles[i]));\n-    CHECK_EQ(replicas.size(), arguments[i].size());\n-    std::vector<ScopedShapedBuffer> result_buffers;\n-    const int64_t n = replicas.size();\n-    result_buffers.reserve(n);\n-    for (int64_t replica = 0; replica < n; ++replica) {\n-      TF_ASSIGN_OR_RETURN(StreamPool::Ptr stream,\n-                          backend->BorrowStream(replicas[replica]));\n-      streams.push_back(std::move(stream));\n-\n-      if (replica == 0 &&\n-          executables[i]->module_config().debug_options().xla_hlo_profile() &&\n-          executables[i]->hlo_profiling_enabled()) {\n-        index_to_profiled_streams[i] = streams.back().get();\n-      }\n-\n-      // Set up run options.\n-      ExecutableRunOptions options;\n-      options.set_stream(streams.back().get());\n-      options.set_allocator(backend->memory_allocator());\n-      options.set_intra_op_thread_pool(\n-          backend->eigen_intra_op_thread_pool_device());\n-      const DeviceAssignment* device_assignment_ptr = &device_assignment;\n-      if (executables[i]->module_config().has_static_device_assignment()) {\n-        device_assignment_ptr =\n-            &executables[i]->module_config().static_device_assignment();\n-      }\n-      options.set_device_assignment(device_assignment_ptr);\n-      // Use run-time profile information from execution_profile on the 0th\n-      // device.\n-      if (i == 0) {\n-        options.set_execution_profile(profile);\n-      }\n-      ServiceExecutableRunOptions run_options(\n-          options, backend->StreamBorrowerWithPriority());\n-\n-      // Asynchronously launch the computation.\n-      TF_ASSIGN_OR_RETURN(ScopedShapedBuffer result,\n-                          executables[i]->ExecuteAsyncOnStream(\n-                              &run_options, arguments[i][replica]));\n-\n-      result_buffers.push_back(std::move(result));\n-    }\n-    TF_ASSIGN_OR_RETURN(GlobalDataHandle handle,\n-                        allocation_tracker_.RegisterReplicatedBuffers(\n-                            std::move(result_buffers), result_tags[i]));\n-    result_handles.push_back(handle);\n-  }\n-\n-  // Wait for all executions to complete.\n-  for (int64_t i = 0, end = streams.size(); i < end; ++i) {\n-    absl::Status block_status = streams[i]->BlockHostUntilDone();\n-    if (!block_status.ok()) {\n-      return Internal(\"failed to complete execution for stream %d: %s\", i,\n-                      block_status.message());\n-    }\n-  }\n-\n-  return result_handles;\n-}\n-\n absl::StatusOr<GlobalDataHandle> Service::ExecuteAndRegisterResult(\n     Executable* executable,\n     absl::Span<const std::vector<const ShapedBuffer*>> arguments,\n@@ -523,23 +420,12 @@ Service::GetArguments(const ExecutionOptions& execution_options,\n   return replicated_arguments;\n }\n \n-absl::StatusOr<std::vector<std::unique_ptr<GlobalData>>>\n-Service::ExecuteGraphParallel(\n-    absl::Span<const XlaComputationInstance> computations) {\n-  VLOG(1) << \"running execute-graph-parallel request\";\n-\n-  std::vector<std::vector<std::vector<const ShapedBuffer*>>> all_arguments;\n-  std::vector<std::vector<se::StreamExecutor*>> all_executors;\n-  std::vector<const HloModuleProto*> module_protos;\n-  std::vector<std::unique_ptr<HloModuleConfig>> module_configs;\n-  std::vector<std::string> computation_names;\n-  std::vector<DeviceHandle> device_handles;\n+absl::StatusOr<std::vector<std::unique_ptr<GlobalData>>> Service::ExecuteGraph(\n+    const XlaComputationInstance& computation) {\n+  VLOG(1) << \"running execute-graph request\";\n \n   int num_requested_devices =\n-      std::accumulate(computations.begin(), computations.end(), 0,\n-                      [](int a, const XlaComputationInstance& r) -> int {\n-                        return a + r.execution_options.device_handles_size();\n-                      });\n+      computation.execution_options.device_handles_size();\n \n   if (num_requested_devices * options_.number_of_replicas() >\n       execute_backend_->device_count()) {\n@@ -548,171 +434,126 @@ Service::ExecuteGraphParallel(\n         num_requested_devices);\n   }\n \n-  for (int64_t i = 0; i < computations.size(); ++i) {\n-    const XlaComputationInstance& computation = computations[i];\n+  // Get the stream executor for the i'th computation. This stream executor\n+  // is one of the executors to run the replicated computation.\n+  const ExecutionOptions& execution_options = computation.execution_options;\n+  TF_RET_CHECK(computation.computation.proto().has_host_program_shape())\n+      << \"program shape may not be empty\";\n \n-    // Get the stream executor for the i'th computation. This stream executor\n-    // is one of the executors to run the replicated computation.\n-    const ExecutionOptions& execution_options = computation.execution_options;\n-    TF_RET_CHECK(computation.computation.proto().has_host_program_shape())\n-        << \"program shape may not be empty\";\n-\n-    // Get the executors.\n-    TF_ASSIGN_OR_RETURN(\n-        std::vector<se::StreamExecutor*> executors,\n-        GetExecutors(execution_options, computations.size(), i));\n+  // Get the executors.\n+  TF_ASSIGN_OR_RETURN(std::vector<se::StreamExecutor*> executors,\n+                      GetExecutors(execution_options, /*requests_size=*/1,\n+                                   /*request_index=*/0));\n \n-    // Get the replicated arguments.\n-    TF_ASSIGN_OR_RETURN(\n-        std::vector<std::vector<const ShapedBuffer*>> replicated_arguments,\n-        GetArguments(execution_options, computation.arguments));\n-\n-    for (auto& args : replicated_arguments) {\n-      for (auto& arg : args) {\n-        auto update_shape_with_empty_tiles = [this](\n-                                                 Shape* subshape,\n-                                                 const xla::ShapeIndex& index) {\n-          if (subshape->IsArray() && subshape->layout().tiles().empty()) {\n-            *subshape =\n-                execute_backend_->transfer_manager()->HostShapeToDeviceShape(\n-                    *subshape);\n-          }\n-        };\n-        ShapeUtil::ForEachMutableSubshape(\n-            const_cast<Shape*>(&arg->on_device_shape()),\n-            update_shape_with_empty_tiles);\n-      }\n+  // Get the replicated arguments.\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<std::vector<const ShapedBuffer*>> replicated_arguments,\n+      GetArguments(execution_options, computation.arguments));\n+\n+  for (auto& args : replicated_arguments) {\n+    for (auto& arg : args) {\n+      auto update_shape_with_empty_tiles =\n+          [this](Shape* subshape, const xla::ShapeIndex& index) {\n+            if (subshape->IsArray() && subshape->layout().tiles().empty()) {\n+              *subshape =\n+                  execute_backend_->transfer_manager()->HostShapeToDeviceShape(\n+                      *subshape);\n+            }\n+          };\n+      ShapeUtil::ForEachMutableSubshape(\n+          const_cast<Shape*>(&arg->on_device_shape()),\n+          update_shape_with_empty_tiles);\n     }\n+  }\n \n-    // Create an HloModuleConfig object for the computation, given the shape of\n-    // the program and the argument allocations. Here, we care only about the\n-    // shapes of the arguments, so, it is sufficient to use the arguments of\n-    // replica 0.\n-    TF_ASSIGN_OR_RETURN(\n-        ProgramShape program_shape,\n-        ProgramShape::FromProto(\n-            computation.computation.proto().host_program_shape()));\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<HloModuleConfig> module_config,\n-        CreateModuleConfig(program_shape, replicated_arguments.front(),\n-                           computation.execution_options));\n-    VLOG(3)\n-        << \"ExecuteGraphParallel created HloModuleConfig computation layout: \"\n-        << module_config->entry_computation_layout().ToString();\n-\n-    // Adds to the vectors to build and execute the computations after the loop.\n-    all_arguments.push_back(replicated_arguments);\n-    all_arguments.insert(all_arguments.end(), executors.size() - 1, {{}});\n-    module_protos.push_back(&computation.computation.proto());\n-    module_configs.push_back(std::move(module_config));\n-    computation_names.insert(computation_names.end(), executors.size(),\n-                             computation.computation.name());\n-    all_executors.push_back(executors);\n-    device_handles.insert(device_handles.end(),\n-                          execution_options.device_handles().begin(),\n-                          execution_options.device_handles().end());\n+  // Create an HloModuleConfig object for the computation, given the shape of\n+  // the program and the argument allocations. Here, we care only about the\n+  // shapes of the arguments, so, it is sufficient to use the arguments of\n+  // replica 0.\n+  TF_ASSIGN_OR_RETURN(\n+      ProgramShape program_shape,\n+      ProgramShape::FromProto(\n+          computation.computation.proto().host_program_shape()));\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<HloModuleConfig> module_config,\n+      CreateModuleConfig(program_shape, replicated_arguments.front(),\n+                         computation.execution_options));\n+  VLOG(3) << \"ExecuteGraph created HloModuleConfig computation layout: \"\n+          << module_config->entry_computation_layout().ToString();\n+\n+  if (execution_options.device_handles_size() > 1) {\n+    return InvalidArgument(\n+        \"The compile request does not support multiple device handles.\");\n   }\n \n   // Build the HloModules and compile to generate the executables.\n   //\n   // TODO(jlebar): There's currently no way to pass a device allocator to\n-  // ExecuteGraphParallel, so we have to pass a null device_allocator below.\n-  TF_ASSIGN_OR_RETURN(std::vector<std::unique_ptr<Executable>> executables,\n-                      BuildExecutables(module_protos, std::move(module_configs),\n-                                       execute_backend_.get(), all_executors,\n-                                       {/*device_allocator=*/nullptr}));\n-  std::vector<Executable*> executable_ptrs;\n-  executable_ptrs.reserve(executables.size());\n-  for (const auto& executable : executables) {\n-    executable_ptrs.push_back(executable.get());\n+  // ExecuteGraph, so we have to pass a null device_allocator below.\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<std::unique_ptr<Executable>> executables,\n+      BuildExecutables(&computation.computation.proto(),\n+                       std::move(module_config), execute_backend_.get(),\n+                       executors, {/*device_allocator=*/nullptr}));\n+\n+  if (executables.size() > 1) {\n+    return absl::InternalError(\"Got more than one executable\");\n   }\n \n-  std::vector<HloSnapshot> snapshots;\n-  snapshots.resize(executable_ptrs.size());\n-  for (int i = 0, end = executable_ptrs.size(); i < end; i++) {\n-    if (executable_ptrs[i]->dumping_snapshot()) {\n-      *snapshots[i].mutable_hlo() = *executable_ptrs[i]->hlo_proto();\n-      TF_ASSIGN_OR_RETURN(auto stream,\n-                          execute_backend_->BorrowStream(\n-                              all_executors[i][0]->device_ordinal()));\n-      TF_RETURN_IF_ERROR(RecordArguments(all_arguments[i].front(), stream.get(),\n-                                         execute_backend_->transfer_manager(),\n-                                         &snapshots[i]));\n-    }\n+  Executable* executable_ptr = executables[0].get();\n+\n+  HloSnapshot snapshot;\n+\n+  if (executable_ptr->dumping_snapshot()) {\n+    *snapshot.mutable_hlo() = *executable_ptr->hlo_proto();\n+    TF_ASSIGN_OR_RETURN(auto stream, execute_backend_->BorrowStream(\n+                                         executors[0]->device_ordinal()));\n+    TF_RETURN_IF_ERROR(\n+        RecordArguments(replicated_arguments.front(), stream.get(),\n+                        execute_backend_->transfer_manager(), &snapshot));\n   }\n \n-  // If we have multiple executables to run, execute them all in parallel.  But\n-  // if we only have one executable, execute it using the vanilla, non-parallel\n-  // call.\n-  //\n-  // We do this because the Client API uses ExecuteGraphParallel when it wants\n-  // to compile and run one computation without caching the executable, but not\n-  // all backends support the async StreamExecutor API required by\n-  // ExecuteParallelAndRegisterResult.\n-  //\n-  // TODO(b/122731460): Consolidate Execute{,Parallel}AndRegisterResult; they do\n-  // basically the same thing.\n   ExecutionProfile profile;\n+  // TODO: Doesn't need to be a vector.\n   std::vector<GlobalDataHandle> outputs;\n   absl::Status execution_status = absl::OkStatus();\n \n-  if (executable_ptrs.size() == 1) {\n-    absl::StatusOr<GlobalDataHandle> output_or_status =\n-        ExecuteAndRegisterResult(executable_ptrs[0], all_arguments[0],\n-                                 execute_backend_.get(), device_handles[0],\n-                                 computation_names[0], &profile);\n-    if (output_or_status.ok()) {\n-      outputs.push_back(std::move(output_or_status).value());\n-    } else {\n-      execution_status = output_or_status.status();\n-    }\n+  absl::StatusOr<GlobalDataHandle> output_or_status = ExecuteAndRegisterResult(\n+      executable_ptr, replicated_arguments, execute_backend_.get(),\n+      execution_options.device_handles(0), computation.computation.name(),\n+      &profile);\n+  if (output_or_status.ok()) {\n+    outputs.push_back(std::move(output_or_status).value());\n   } else {\n-    absl::StatusOr<std::vector<GlobalDataHandle>> outputs_or_status =\n-        ExecuteParallelAndRegisterResult(executable_ptrs, all_arguments,\n-                                         execute_backend_.get(), device_handles,\n-                                         computation_names, &profile);\n-    if (outputs_or_status.ok()) {\n-      outputs = std::move(outputs_or_status).value();\n-    } else {\n-      execution_status = outputs_or_status.status();\n-    }\n+    execution_status = output_or_status.status();\n   }\n \n-  for (int64_t i = 0; i < computations.size(); ++i) {\n-    if (computations[i].execution_profile != nullptr) {\n-      *computations[i].execution_profile = profile;\n-    }\n+  if (computation.execution_profile != nullptr) {\n+    *computation.execution_profile = profile;\n   }\n \n   if (!execution_status.ok()) {\n     // Execution failed so we don't have the results.  Dump the HLO snapshot\n     // with just the program arguments.\n-    for (int i = 0, end = executable_ptrs.size(); i < end; i++) {\n-      DumpHloSnapshotIfEnabled(executable_ptrs[i]->module(), snapshots[i]);\n-    }\n+    DumpHloSnapshotIfEnabled(executable_ptr->module(), snapshot);\n   }\n \n   TF_RETURN_IF_ERROR(execution_status);\n \n   std::vector<std::unique_ptr<GlobalData>> out;\n-\n-  out.reserve(out.size());\n   for (GlobalDataHandle& output : outputs) {\n     out.push_back(std::make_unique<GlobalData>(this, output));\n   }\n \n-  for (int i = 0, end = executable_ptrs.size(); i < end; i++) {\n-    Executable* executable = executable_ptrs[i];\n-    if (executable->dumping_snapshot()) {\n-      TF_ASSIGN_OR_RETURN(const ShapedBuffer* result_buffer,\n-                          allocation_tracker_.ResolveForReplica(outputs[i], 0));\n-      TF_ASSIGN_OR_RETURN(auto stream,\n-                          execute_backend_->BorrowStream(all_executors[i][0]));\n-      TF_RETURN_IF_ERROR(RecordResult(*result_buffer, stream.get(),\n-                                      execute_backend_->transfer_manager(),\n-                                      &snapshots[i]));\n-      DumpHloSnapshotIfEnabled(executable->module(), snapshots[i]);\n-    }\n+  if (executable_ptr->dumping_snapshot()) {\n+    TF_ASSIGN_OR_RETURN(const ShapedBuffer* result_buffer,\n+                        allocation_tracker_.ResolveForReplica(outputs[0], 0));\n+    TF_ASSIGN_OR_RETURN(auto stream,\n+                        execute_backend_->BorrowStream(executors[0]));\n+    TF_RETURN_IF_ERROR(RecordResult(*result_buffer, stream.get(),\n+                                    execute_backend_->transfer_manager(),\n+                                    &snapshot));\n+    DumpHloSnapshotIfEnabled(executable_ptr->module(), snapshot);\n   }\n \n   VLOG(1) << \"successfully completed 'execute-graph-parallel' request\";"
        },
        {
            "sha": "a9f6dde62f2c8f53f20827f96898267c3581f412",
            "filename": "third_party/xla/xla/service/service.h",
            "status": "modified",
            "additions": 6,
            "deletions": 17,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fservice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fservice%2Fservice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fservice.h?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -165,11 +165,10 @@ class Service {\n       const ExecutionHandle& handle, absl::Span<GlobalData* const> arguments,\n       ExecutionProfile* execution_profile);\n \n-  // Executes one or more computations in parallel with the provided global data\n-  // passed as immutable arguments. Returns global data output for each\n+  // Executes a computation instance. Returns global data output for the\n   // computation.\n-  absl::StatusOr<std::vector<std::unique_ptr<GlobalData>>> ExecuteGraphParallel(\n-      absl::Span<const XlaComputationInstance> computations);\n+  absl::StatusOr<std::vector<std::unique_ptr<GlobalData>>> ExecuteGraph(\n+      const XlaComputationInstance& computation);\n \n   // Requests one or more device handles from the target.\n   //\n@@ -296,9 +295,9 @@ class Service {\n   // Same as BuildExecutable() above, but builds a list of Executables for the\n   // given computations that may interact with each other.\n   absl::StatusOr<std::vector<std::unique_ptr<Executable>>> BuildExecutables(\n-      const std::vector<const HloModuleProto*>& module_protos,\n-      std::vector<std::unique_ptr<HloModuleConfig>> module_configs,\n-      Backend* backend, std::vector<std::vector<se::StreamExecutor*>> executors,\n+      const HloModuleProto* module_proto,\n+      std::unique_ptr<HloModuleConfig> module_config, Backend* backend,\n+      std::vector<se::StreamExecutor*> executors,\n       const Compiler::CompileOptions& options, bool run_backend_only = false);\n \n  protected:\n@@ -323,16 +322,6 @@ class Service {\n       Backend* backend, const DeviceHandle& device_handle,\n       const std::string& result_tag, ExecutionProfile* profile);\n \n-  // Runs the given executables with the given arguments and register the result\n-  // from each executable in the allocation tracker. The handles of the result\n-  // from the tracker are returned.\n-  absl::StatusOr<std::vector<GlobalDataHandle>>\n-  ExecuteParallelAndRegisterResult(\n-      absl::Span<Executable* const> executables,\n-      absl::Span<const std::vector<std::vector<const ShapedBuffer*>>> arguments,\n-      Backend* backend, absl::Span<const DeviceHandle> device_handles,\n-      absl::Span<const std::string> result_tags, ExecutionProfile* profile);\n-\n   // Returns the stream executors assigned to the replicas represented by the\n   // given device handle. Each device_handle is a virtual replicated device that\n   // represents a set of physical devices for the replicas."
        },
        {
            "sha": "16f9c32c89f040b96836b31c557975dbff2cdaf4",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_on_demand_compiler.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 47,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_on_demand_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_on_demand_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_on_demand_compiler.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -113,71 +113,63 @@ class TpuCompiler : public Compiler {\n   }\n \n   absl::StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(\n-      std::unique_ptr<HloModuleGroup> module_group,\n-      std::vector<std::vector<stream_executor::StreamExecutor*>> stream_exec,\n+      std::unique_ptr<HloModule> hlo_module,\n+      std::vector<stream_executor::StreamExecutor*> stream_exec,\n       const CompileOptions& options) override {\n+    HloModuleGroup module_group(std::move(hlo_module));\n     XLA_HloModuleGroup se_module_group;\n     se_module_group.proto =\n-        stream_executor::tpu::SerializeProto(module_group->ToProto());\n-    se_module_group.module_config =\n-        new XLA_HloModuleConfig[module_group->size()];\n-    int module_group_size = module_group->size();\n-    auto cleanup_config =\n-        absl::MakeCleanup([&se_module_group, module_group_size]() {\n-          for (auto i = 0; i < module_group_size; ++i) {\n-            ApiConverter::Destroy(&se_module_group.module_config[i]);\n-          }\n-          delete[] se_module_group.module_config;\n-        });\n-    for (int i = 0; i < module_group->size(); ++i) {\n-      const auto& config = module_group->module(i).config();\n-      se_module_group.module_config[i] = ApiConverter::ToC(config);\n-    }\n-    std::vector<SE_StreamExecutorList> se_lists(stream_exec.size());\n-    std::vector<std::vector<SE_StreamExecutor*>> se_lists_storage;\n-    for (int i = 0; i < stream_exec.size(); ++i) {\n-      se_lists[i].count = stream_exec[i].size();\n-      se_lists_storage.emplace_back(stream_exec[i].size());\n-      se_lists[i].exec = se_lists_storage.back().data();\n-      for (int j = 0; j < stream_exec[i].size(); ++j) {\n-        se_lists[i].exec[j] =\n-            static_cast<stream_executor::tpu::TpuExecutor*>(stream_exec[i][j])\n-                ->se_executor();\n-      }\n+        stream_executor::tpu::SerializeProto(module_group.ToProto());\n+    se_module_group.module_config = new XLA_HloModuleConfig[1];\n+    se_module_group.module_config[0] =\n+        ApiConverter::ToC(module_group.module(0).config());\n+\n+    auto cleanup_config = absl::MakeCleanup([&se_module_group]() {\n+      ApiConverter::Destroy(&se_module_group.module_config[0]);\n+      delete[] se_module_group.module_config;\n+    });\n+\n+    SE_StreamExecutorList se_list;\n+\n+    std::vector<SE_StreamExecutor*> se_lists_storage(stream_exec.size());\n+    se_list.count = stream_exec.size();\n+    se_list.exec = se_lists_storage.data();\n+    for (int j = 0; j < stream_exec.size(); ++j) {\n+      se_list.exec[j] =\n+          static_cast<stream_executor::tpu::TpuExecutor*>(stream_exec[j])\n+              ->se_executor();\n     }\n \n     SE_DeviceMemoryAllocator allocator =\n         ApiConverter::ToC(options.device_allocator);\n \n-    SE_Executable** se_executables = new SE_Executable*[module_group->size()];\n+    SE_Executable** se_executables = new SE_Executable*[1];\n \n     StatusHelper status;\n \n     ExecutorApiFn()->TpuCompiler_CompileFn(\n-        compiler_, &se_module_group, se_lists.data(), stream_exec.size(),\n-        &allocator, se_executables, status.c_status);\n+        compiler_, &se_module_group, &se_list, /*num_lists=*/1, &allocator,\n+        se_executables, status.c_status);\n \n     if (!status.ok()) {\n       return status.status();\n     }\n \n     std::vector<std::unique_ptr<Executable>> executables;\n-    for (int i = 0; i < module_group->size(); ++i) {\n-      // We get the HloModule from the compiled executable, rather than reusing\n-      // the input module from 'module_group', in case the module changed in\n-      // some way. For example, if the computation is automatically partitioned\n-      // via XLA, the executable's module may have different input/output shapes\n-      // than the input module.\n-      XLA_HloModule c_module =\n-          ExecutorApiFn()->TpuExecutable_HloModuleFn(se_executables[i]);\n-      auto cleanup_c_module = absl::MakeCleanup(\n-          [&c_module]() { ApiConverter::Destroy(&c_module); });\n-      TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n-                          ApiConverter::FromC(c_module));\n-      std::shared_ptr<HloModule> module_shared(module.release());\n-      executables.emplace_back(std::make_unique<legacy::TpuExecutable>(\n-          se_executables[i], std::move(module_shared)));\n-    }\n+    // We get the HloModule from the compiled executable, rather than reusing\n+    // the input module in case the module changed in some way. For example,\n+    // if the computation is automatically partitioned via XLA, the\n+    // executable's module may have different input/output shapes than the\n+    // input module.\n+    XLA_HloModule c_module =\n+        ExecutorApiFn()->TpuExecutable_HloModuleFn(se_executables[0]);\n+    auto cleanup_c_module =\n+        absl::MakeCleanup([&c_module]() { ApiConverter::Destroy(&c_module); });\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n+                        ApiConverter::FromC(c_module));\n+    std::shared_ptr<HloModule> module_shared(module.release());\n+    executables.emplace_back(std::make_unique<legacy::TpuExecutable>(\n+        se_executables[0], std::move(module_shared)));\n \n     stream_executor::tpu::SerializedProto_Free(se_module_group.proto);\n     delete[] se_executables;"
        },
        {
            "sha": "64c40ca3fe57a9120c22d2af26b97c953676fc1f",
            "filename": "third_party/xla/xla/tests/client_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Ftests%2Fclient_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Ftests%2Fclient_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fclient_test.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -114,48 +114,5 @@ TEST_F(ClientTest, ExecuteWithTupleLayout) {\n                                           /*minor_to_major=*/{1, 0})));\n }\n \n-// Disabled for interpreter since ExecuteAsyncOnStream is not implemented on\n-// interpreter backend.\n-TEST_F(ClientTest, ExecuteParallel) {\n-  if (test::DeviceTypeIsOneOf({test::kCpu, test::kGpu})) {\n-    GTEST_SKIP();\n-  }\n-  XlaComputation add_with_one_arg, mul_with_two_args, dot_with_one_arg;\n-  Shape shape = ShapeUtil::MakeShape(S32, {2, 2});\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<GlobalData> const_arg,\n-      client_->TransferToServer(\n-          LiteralUtil::CreateR2<int32_t>({{5, 6}, {7, 8}})));\n-\n-  XlaBuilder b(TestName() + \".add\");\n-  Add(Parameter(&b, 0, shape, \"param_0\"),\n-      ConstantR2<int32_t>(&b, {{1, 2}, {3, 4}}));\n-  TF_ASSERT_OK_AND_ASSIGN(add_with_one_arg, b.Build());\n-\n-  // We can't really test parallel execution on CPU since all of the cores in a\n-  // CPU are presented as a single device.  So for now we test \"parallel\"\n-  // execution on a single device.\n-  std::vector<XlaComputationInstance> computation_instances;\n-  TF_ASSERT_OK_AND_ASSIGN(std::vector<xla::DeviceHandle> devices,\n-                          client_->GetDeviceHandles(1));\n-  ASSERT_EQ(devices.size(), 1);\n-\n-  ExecutionOptions options = execution_options_;\n-  *options.add_device_handles() = devices[0];\n-  computation_instances.push_back(XlaComputationInstance(\n-      add_with_one_arg, {const_arg.get()}, options, nullptr));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto results,\n-                          client_->ExecuteParallel(computation_instances));\n-  auto expected_result = LiteralUtil::CreateR2<int32_t>({{6, 8}, {10, 12}});\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto result_literal,\n-      client_->Transfer(*results[0], &expected_result.shape()));\n-\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_result, result_literal));\n-}\n-\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "43da876f00d54a83ffe70ef334fb40f66ace76f7",
            "filename": "third_party/xla/xla/tools/xla_compile_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31f5a90587336ead1ac1d3b49b69eebaa967001c/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fxla_compile_lib.cc?ref=31f5a90587336ead1ac1d3b49b69eebaa967001c",
            "patch": "@@ -77,10 +77,9 @@ namespace xla {\n static absl::StatusOr<std::string> AotCompileCpuExecutable(\n     std::unique_ptr<HloModule> hlo_module) {\n   cpu::CpuCompiler cpu_compiler;\n-  auto module_group = std::make_unique<HloModuleGroup>(std::move(hlo_module));\n   TF_ASSIGN_OR_RETURN(\n       std::vector<std::unique_ptr<Executable>> executables,\n-      cpu_compiler.Compile(std::move(module_group), {{nullptr}}, {nullptr}));\n+      cpu_compiler.Compile(std::move(hlo_module), {nullptr}, {nullptr}));\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<AotCompilationResult> aot_result,\n                       cpu_compiler.Export(executables[0].get()));\n   return aot_result->SerializeAsString();\n@@ -116,7 +115,6 @@ static absl::StatusOr<std::string> CompileGpuExecutable(\n     return compile_result;\n   }\n \n-  auto module_group = std::make_unique<HloModuleGroup>(std::move(hlo_module));\n   Compiler::CompileOptions compile_options;\n   TF_ASSIGN_OR_RETURN(stream_executor::StreamExecutor * stream_executor,\n                       platform->ExecutorForDevice(0));\n@@ -127,7 +125,7 @@ static absl::StatusOr<std::string> CompileGpuExecutable(\n \n   TF_ASSIGN_OR_RETURN(\n       std::vector<std::unique_ptr<Executable>> executables,\n-      gpu_compiler->Compile(std::move(module_group), {{stream_executor}},\n+      gpu_compiler->Compile(std::move(hlo_module), {stream_executor},\n                             compile_options));\n   *result.mutable_hlo_module() = executables[0]->module().ToProto();\n   return executables[0]->module().ToString();"
        }
    ],
    "stats": {
        "total": 664,
        "additions": 202,
        "deletions": 462
    }
}