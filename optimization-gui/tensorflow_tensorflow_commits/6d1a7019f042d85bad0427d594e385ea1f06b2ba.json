{
    "author": "mrguenther",
    "message": "Fix issues in optimization patterns for `broadcast_in_dim` and `pad` ops.\n\n- Prioritize replacing `broadcast_in_dim` with `reshape` over merging nested `broadcast_in_dim` ops. The new behavior matches the relevant MHLO optimization behavior, which proved to be preferable.\n- Fix an issue where `pad` ops that didn't change the dimensions would be removed even if they shifted elements around within the tensor (e.g. padding by -1 on one side and +1 on the opposite side).\n\nPiperOrigin-RevId: 822701252",
    "sha": "6d1a7019f042d85bad0427d594e385ea1f06b2ba",
    "files": [
        {
            "sha": "f2a8787d298682bbfab9f63841dfa53862e7d337",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 202,
            "deletions": 11,
            "changes": 213,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d1a7019f042d85bad0427d594e385ea1f06b2ba/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d1a7019f042d85bad0427d594e385ea1f06b2ba/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=6d1a7019f042d85bad0427d594e385ea1f06b2ba",
            "patch": "@@ -812,7 +812,39 @@ diff --ruN a/stablehlo/stablehlo/reference/InterpreterOps.cpp b/stablehlo/stable\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -529,28 +529,15 @@\n+@@ -22,21 +22,24 @@\n+ // CHECK-LABEL: func.func @broadcast_in_dim_fold_splat\n+ // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x3xi32>)\n+ func.func @broadcast_in_dim_fold_splat(%arg0: tensor<3x3xi32>)\n+-  -> (tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>) {\n++  -> (tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>) {\n+   %c0 = stablehlo.constant dense<5> : tensor<i32>\n+   %c1 = stablehlo.constant dense<3.0> : tensor<f32>\n+-  %c2 = stablehlo.constant dense<1> : tensor<1x3xi32>\n++  %c2 = stablehlo.constant dense<(1.0,2.0)> : tensor<complex<f32>>\n++  %c3 = stablehlo.constant dense<1> : tensor<1x3xi32>\n+ \n+   %0 = stablehlo.broadcast_in_dim %c0, dims = [] : (tensor<i32>) -> tensor<6xi32>\n+   %1 = stablehlo.broadcast_in_dim %c1, dims = [] : (tensor<f32>) -> tensor<3xf32>\n+-  %2 = stablehlo.broadcast_in_dim %c2, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>\n++  %2 = stablehlo.broadcast_in_dim %c2, dims = [] : (tensor<complex<f32>>) -> tensor<5xcomplex<f32>>\n++  %3 = stablehlo.broadcast_in_dim %c3, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>\n+ \n+   // CHECK-DAG:  [[R0:%.+]] = stablehlo.constant dense<5> : tensor<6xi32>\n+   // CHECK-DAG:  [[R1:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<3xf32>\n+-  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>\n+-\n+-  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]]\n+-  return %0, %1, %2 : tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>\n++  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<(1.0{{.*}},2.0{{.*}})> : tensor<5xcomplex<f32>>\n++  // CHECK-DAG:  [[R3:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>\n++\n++  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]], [[R3]]\n++  return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>\n+ }\n+ \n+ // -----\n+@@ -529,28 +532,15 @@\n  // IotaOp\n  \n  // CHECK-LABEL: func @eval_iota\n@@ -847,7 +879,7 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n  }\n  \n  // -----\n-@@ -596,6 +583,37 @@\n+@@ -596,6 +586,37 @@\n    // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\\[\\[1, 2\\], \\[3, 4\\]\\]}}> : tensor<2x2xi32>\n    // CHECK-NEXT: return [[CST1]], [[CST2]]\n    return %0, %1 : tensor<1xi32>, tensor<2x2xi32>\n@@ -888,7 +920,62 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n-@@ -1810,6 +1810,15 @@\n+@@ -132,6 +132,35 @@\n+ \n+   // CHECK-NEXT: return [[R0]], [[R5]]\n+   return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>\n++}\n++\n++// CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape\n++// CHECK-SAME:   ([[ARG0:%[^ ]+]]: tensor<3x4xi32>)\n++func.func @broadcast_in_dim_prefer_nested_reshape(%arg0: tensor<3x4xi32>) -> (tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>) {\n++  // When `broadcast_in_dim(broadcast_in_dim(x))` could be optimized into either\n++  // `broadcast_in_dim(reshape(x))` or `broadcast_in_dim(x)`, we want to select\n++  // the former pattern.\n++  //\n++  // (We accomplish this by blocking the merge-composition pattern if the inner\n++  // op can be replaced with a `reshape`. Simply adding benefit to the\n++  // replace-with-reshape pattern isn't sufficient here because the outermost\n++  // op, which only matches the merge-composition pattern, is traversed first.)\n++\n++  // CHECK-DAG: [[INNER_RESHAPE:%[^ ]+]] = stablehlo.reshape [[ARG0]] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>\n++  // CHECK-DAG: [[BROADCAST_OF_RESHAPE:%[^ ]+]] = stablehlo.broadcast_in_dim [[INNER_RESHAPE]], dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>\n++  %0 = stablehlo.broadcast_in_dim %arg0, dims = [0, 2] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>\n++  %1 = stablehlo.broadcast_in_dim %0, dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>\n++\n++  // When the inner op doesn't qualify for replacement with a `reshape` op,\n++  // however (particularly when it meets some conditions but not others), ensure\n++  // that we allow the merge-composition pattern to match.\n++\n++  // CHECK-DAG: [[MERGED_BROADCAST:%[^ ]+]] = stablehlo.broadcast_in_dim [[ARG0]], dims = [3, 2] : (tensor<3x4xi32>) -> tensor<2x3x4x3xi32>\n++  %2 = stablehlo.broadcast_in_dim %arg0, dims = [2, 1] : (tensor<3x4xi32>) -> tensor<1x4x3xi32>\n++  %3 = stablehlo.broadcast_in_dim %2, dims = [0, 2, 3] : (tensor<1x4x3xi32>) -> tensor<2x3x4x3xi32>\n++\n++  // CHECK-DAG: return [[BROADCAST_OF_RESHAPE]], [[MERGED_BROADCAST]]\n++  return %1, %3 : tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>\n+ }\n+ \n+ // CHECK-LABEL: func.func @broadcast_in_dim_not_identity_broadcasts\n+@@ -1021,6 +1050,18 @@\n+   // CHECK-NOT: stablehlo.pad\n+   %1 = stablehlo.pad %arg0, %0, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<256x1024xbf16>, tensor<bf16>) -> tensor<256x1024xbf16>\n+   return %1 : tensor<256x1024xbf16>\n++}\n++\n++// We don't want to delete `pad` ops that move a tensor's values around without\n++// affecting its dimensions.\n++//\n++// CHECK-LABEL: @pad_rotate_tensor_no_dim_change\n++func.func @pad_rotate_tensor_no_dim_change(%arg0: tensor<50x50xf32>) -> tensor<50x50xf32> {\n++  // CHECK: %[[RES:.+]] = stablehlo.pad\n++  // CHECK: return %[[RES]]\n++  %cst = stablehlo.constant dense<0.0> : tensor<f32>\n++  %0 = stablehlo.pad %arg0, %cst, low = [0, -1], high = [0, 1], interior = [0, 0] : (tensor<50x50xf32>, tensor<f32>) -> tensor<50x50xf32>\n++  return %0 : tensor<50x50xf32>\n+ }\n+ \n+ // -----\n+@@ -1810,6 +1851,15 @@\n    return %0 : tensor<2x4x1x5xf32>\n  }\n  \n@@ -1537,7 +1624,44 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n  }\n  \n  LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,\n-@@ -1256,21 +1283,48 @@\n+@@ -530,10 +557,15 @@\n+   using FoldOpRewritePattern<OpType>::matchAndRewrite;\n+   using FoldOpRewritePattern<OpType>::options;\n+ \n++  // TODO: Generalize all relevant folder patterns to support complex data\n++  // types, then hard-code `allowComplex` to `true`.\n+   LogicalResult validateShapeFoldDtype(PatternRewriter& rewriter, OpType op,\n+-                                       ShapedType resultType) const {\n++                                       ShapedType resultType,\n++                                       bool allowComplex = false) const {\n+     if (resultType.getElementType().isInteger()) return success();\n+-    if (options.optimizeFloat && isa<FloatType>(resultType.getElementType()))\n++    if (options.optimizeFloat &&\n++        (allowComplex ? isa<FloatType, ComplexType>(resultType.getElementType())\n++                      : isa<FloatType>(resultType.getElementType())))\n+       return success();\n+     return rewriter.notifyMatchFailure(op, \"skipping fold of shape op dtype\");\n+   }\n+@@ -605,7 +637,8 @@\n+                                 PatternRewriter& rewriter) const override {\n+     auto resultType = op.getType();\n+     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||\n+-        failed(validateShapeFoldDtype(rewriter, op, resultType)))\n++        failed(validateShapeFoldDtype(rewriter, op, resultType,\n++                                      /*allowComplex=*/true)))\n+       return failure();\n+ \n+     SplatElementsAttr cstAttr;\n+@@ -1104,7 +1137,7 @@\n+         failed(validateShapeFoldDtype(rewriter, op, resultType)))\n+       return failure();\n+ \n+-    DenseIntOrFPElementsAttr attr;\n++    DenseElementsAttr attr;\n+     if (!matchPattern(op.getOperand(), m_Constant(&attr)))\n+       return rewriter.notifyMatchFailure(op, \"expected constant operand\");\n+     rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(resultType));\n+@@ -1256,21 +1289,48 @@\n        return rewriter.notifyMatchFailure(\n            op, \"expected operand with static ranked tensor type\");\n  \n@@ -1589,7 +1713,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n      return success();\n    }\n  };\n-@@ -1482,6 +1536,14 @@\n+@@ -1482,6 +1542,14 @@\n        rewriter.replaceOpWithNewOp<ConstantOp>(\n            op, DenseIntElementsAttr::get(resultType, values));\n        return success();\n@@ -1604,7 +1728,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n      }\n  \n      int64_t sequences = 1;\n-@@ -1881,6 +1943,7 @@\n+@@ -1881,6 +1949,7 @@\n    patterns->add<FoldConcatenateOpPattern>(context, options, benefit);\n    patterns->add<FoldConvertOpPattern>(context, options, benefit);\n    patterns->add<FoldDivOpPattern>(context, options, benefit);\n@@ -1645,16 +1769,83 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n-@@ -119,6 +119,8 @@\n- def InvertBroadcastDims : NativeCodeCall<\"getInvertedBroadcastDimensions($_builder, $0)\">;\n+@@ -43,6 +43,14 @@\n+     CPred<\"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\">,\n+     \"same number of elements\">;\n+ \n++def BroadcastNotReducibleToReshape : Constraint<\n++    CPred<\"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && \"\n++          \"!(\"\n++            \"llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && \"\n++            \"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\"\n++          \")\">,\n++    \"is a broadcast_in_dim op that cannot be simplified to a reshape op\">;\n++\n+ def OperandsEqual : Constraint<CPred<\"$0 == $1\">, \"operands are equal\">;\n  \n- def MergeBroadcastDims : NativeCodeCall<\"getMergedBroadcastDimensions($_builder, $0, $1)\">;\n+ def RankEqual : Constraint<\n+@@ -61,6 +69,10 @@\n+ def AnyZero : AttrConstraint<\n+     CPred<\"::mlir::matchPattern($_self, m_AnyAttrOf(m_Zero(), m_AnyZeroFloat()))\">,\n+     \"is int or float zero\">;\n +\n-+def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n++def ZeroArrayI64 : AttrConstraint<\n++    CPred<\"::llvm::all_of(::llvm::cast<DenseI64ArrayAttr>($_self).asArrayRef(), [](int64_t val) { return val == 0; })\">,\n++    \"is an array of zeros\">;\n+ \n+ def DenseIntElementsAttr : AttrConstraint<\n+     CPred<\"llvm::isa<DenseIntElementsAttr>($_self)\">,\n+@@ -120,6 +132,8 @@\n+ \n+ def MergeBroadcastDims : NativeCodeCall<\"getMergedBroadcastDimensions($_builder, $0, $1)\">;\n  \n++def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n++\n  def StableHLO_ConvertOpWithShape : NativeCodeCall<\n      \"$_builder.create<stablehlo::ConvertOp>($_loc, $0.getType(), $1)\">;\n-@@ -539,6 +541,12 @@\n+ \n+@@ -178,18 +192,23 @@\n+ \n+ // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])\n+ //       -> broadcast_in_dim(X, merge(dimsA, dimsB))\n++//          [if the nested broadcast can't be simplified to a reshape]\n+ def BroadcastInDimOp_MergeComposition\n+-  : Pat<(StableHLO_BroadcastInDimOp\n+-            (StableHLO_BroadcastInDimOp $operand, $dims_parent), $dims),\n++  : Pat<(StableHLO_BroadcastInDimOp:$outer_op\n++            (StableHLO_BroadcastInDimOp:$inner_op $operand, $inner_dims),\n++            $outer_dims),\n+         (StableHLO_BroadcastInDimOp\n+-            $operand, (MergeBroadcastDims $dims, $dims_parent))>;\n++            $operand, (MergeBroadcastDims $outer_dims, $inner_dims)),\n++        [(BroadcastNotReducibleToReshape $inner_op, $operand)]>;\n+ \n+ // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])\n+ //          [if same numel]\n+ def BroadcastInDimOp_ReplaceWithReshape\n+   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, SortedDims:$dims),\n+         (StableHLO_ReshapeOpWithShape $op, $operand),\n+-        [(NumberOfElementsEqual $op, $operand)]>;\n++        [(NumberOfElementsEqual $op, $operand)],\n++        [],\n++        (addBenefit 1)>;\n+ \n+ // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])\n+ //          [if same numel & rank]\n+@@ -424,9 +443,9 @@\n+   : Pat<(StableHLO_PadOp:$pad\n+             $operand,\n+             $padding_value,\n+-            $edge_padding_low,\n+-            $edge_padding_high,\n+-            $interior_padding),\n++            ZeroArrayI64:$edge_padding_low,\n++            ZeroArrayI64:$edge_padding_high,\n++            ZeroArrayI64:$interior_padding),\n+         (replaceWithValue $operand),\n+         [(TypesEqual $pad, $operand)]>;\n+ \n+@@ -539,6 +558,12 @@\n    : Pat<(StableHLO_TransposeOp $lhs, IotaDims:$dims),\n          (replaceWithValue $lhs)>;\n  "
        }
    ],
    "stats": {
        "total": 213,
        "additions": 202,
        "deletions": 11
    }
}