{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Add lowering for broadcast.\n\nPiperOrigin-RevId: 825578568",
    "sha": "d717d76122f62e912d1133096afc9d1163001ef8",
    "files": [
        {
            "sha": "62ad878ffcafb015f7f930c8b1ec7ec1aec56b29",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/tiled_kernel_test.py",
            "status": "modified",
            "additions": 61,
            "deletions": 2,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py?ref=d717d76122f62e912d1133096afc9d1163001ef8",
            "patch": "@@ -40,6 +40,7 @@ def compare_kernel(\n     dtype,\n     expected_output: Callable[[np.ndarray, ...], np.ndarray],\n     maxulp: Optional[int] = None,\n+    random_inputs: bool = False,\n ) -> None:\n   mlir_emitter = cpu_testlib.MlirTestKernelEmitter(\n       ir, kernel_name, (num_workgroups, 1, 1)\n@@ -51,8 +52,14 @@ def compare_kernel(\n       cpu_testlib.JitCompiler(base_testlib.HloModuleConfig()),\n   )\n \n-  # Simply use a all-ones arrays as inputs to make it easy to debug the kernel.\n-  inputs = [np.ones(shape=shape, dtype=dtype) for shape in input_shapes]\n+  # Simply use a all-ones arrays as inputs to make it easy to debug the kernel\n+  # unless random inputs are requested.\n+  def get_input(shape):\n+    if random_inputs:\n+      return get_random_array(shape, dtype)\n+    return np.ones(shape=shape, dtype=dtype)\n+\n+  inputs = [get_input(shape) for shape in input_shapes]\n \n   input_tensors = [create_literal(input) for input in inputs]\n   # Use a random array as the output to ensure all values are written to.\n@@ -407,6 +414,58 @@ def test_reduction_outer_inner(self):\n         lambda input, init: np.sum(input, axis=(0, 2)) + init,\n     )\n \n+  def test_broadcast_in_dim_inner(self):\n+    ir = \"\"\"\n+      module @broadcast_in_dim_inner {\n+        xtile.entry_func @broadcast_in_dim_inner(\n+            %input: memref<4xf32>,\n+            %output: memref<32x4xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:1, tiles_per_workgroup:1>} {\n+          %input_tile = xtile.extract %input[%tile_id][4][1] : memref<4xf32> -> tensor<4xf32>\n+          %result = stablehlo.broadcast_in_dim %input_tile, dims = [1] : (tensor<4xf32>) -> tensor<32x4xf32>\n+          xtile.insert %result into %output[%tile_id, %tile_id][32,4][1,1] : tensor<32x4xf32> -> memref<32x4xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"broadcast_in_dim_inner\",\n+        1,\n+        [(4,)],\n+        (32, 4),\n+        np.float32,\n+        lambda input: np.broadcast_to(input, (32, 4)),\n+        random_inputs=True,\n+    )\n+\n+  def test_broadcast_in_dim_outer(self):\n+    ir = \"\"\"\n+      module @broadcast_in_dim_outer {\n+        xtile.entry_func @broadcast_in_dim_outer(\n+            %input: memref<4xf32>,\n+            %output: memref<4x32xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:1, tiles_per_workgroup:1>} {\n+          %input_tile = xtile.extract %input[%tile_id][4][1] : memref<4xf32> -> tensor<4xf32>\n+          %result = stablehlo.broadcast_in_dim %input_tile, dims = [0] : (tensor<4xf32>) -> tensor<4x32xf32>\n+          xtile.insert %result into %output[%tile_id, %tile_id][4,32][1,1] : tensor<4x32xf32> -> memref<4x32xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"broadcast_in_dim_outer\",\n+        1,\n+        [(4,)],\n+        (4, 32),\n+        np.float32,\n+        lambda input: np.transpose(np.broadcast_to(input, (32, 4))),\n+        random_inputs=True,\n+    )\n+\n \n if __name__ == \"__main__\":\n   absltest.main()"
        },
        {
            "sha": "e5f63171452c7de6a8099526bf898d0b4b77e2de",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=d717d76122f62e912d1133096afc9d1163001ef8",
            "patch": "@@ -64,9 +64,6 @@ cc_library(\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//llvm:Support\",\n@@ -78,7 +75,6 @@ cc_library(\n         \"@llvm-project//mlir:FuncTransforms\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:LLVMDialect\",\n-        \"@llvm-project//mlir:LinalgTransforms\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MathOpsIncGen\",\n         \"@llvm-project//mlir:MemRefDialect\","
        },
        {
            "sha": "55656e14ece002b294ce46f3503cdf2c6870ebc1",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/shlo_to_vector.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 4,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc?ref=d717d76122f62e912d1133096afc9d1163001ef8",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -224,9 +225,6 @@ struct LowerTranspose : mlir::OpRewritePattern<mlir::stablehlo::TransposeOp> {\n   }\n };\n \n-// Lower stablehlo.reduce to vector operations.\n-//\n-\n struct LowerReduce : mlir::OpRewritePattern<mlir::stablehlo::ReduceOp> {\n   using OpRewritePattern::OpRewritePattern;\n \n@@ -266,14 +264,52 @@ struct LowerReduce : mlir::OpRewritePattern<mlir::stablehlo::ReduceOp> {\n   }\n };\n \n+struct LowerBroadcastInDim\n+    : mlir::OpRewritePattern<mlir::stablehlo::BroadcastInDimOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::stablehlo::BroadcastInDimOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    auto source_vector = CastToVector(rewriter, op.getOperand());\n+    auto result_vector_type = GetVectorType(op.getType());\n+\n+    llvm::ArrayRef<int64_t> source_shape = source_vector.getType().getShape();\n+    llvm::ArrayRef<int64_t> broadcast_dims = op.getBroadcastDimensions();\n+\n+    // First create an intermediate vector with the rank of the result vector\n+    // but with the broadcasted dimensions set to the source shape with all\n+    // additional dimensions set to 1.\n+    llvm::SmallVector<int64_t> intermediate_shape(result_vector_type.getRank(),\n+                                                  1);\n+    for (auto [input_dim, result_dim] : llvm::enumerate(broadcast_dims)) {\n+      intermediate_shape[result_dim] = source_shape[input_dim];\n+    }\n+    mlir::Value intermediate_vector = mlir::vector::ShapeCastOp::create(\n+        rewriter, op->getLoc(),\n+        mlir::VectorType::get(intermediate_shape,\n+                              result_vector_type.getElementType()),\n+        source_vector);\n+    // Now that all the inserted dimensions are size 1 we can legally call\n+    // broadcast even if they are not the most major dimensions.\n+    mlir::Value broadcast_op = mlir::vector::BroadcastOp::create(\n+        rewriter, op->getLoc(), result_vector_type, intermediate_vector);\n+\n+    rewriter.replaceOp(op, CastToTensor(rewriter, broadcast_op));\n+    return mlir::success();\n+  }\n+};\n+\n class ShloToVectorPass : public impl::ShloToVectorPassBase<ShloToVectorPass> {\n  public:\n   using ShloToVectorPassBase::ShloToVectorPassBase;\n \n   void runOnOperation() override {\n     mlir::MLIRContext* context = &getContext();\n     mlir::RewritePatternSet patterns(context);\n-    patterns.add<LowerTranspose, LowerDotGeneral, LowerReduce>(context);\n+    patterns\n+        .add<LowerTranspose, LowerDotGeneral, LowerReduce, LowerBroadcastInDim>(\n+            context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       signalPassFailure();"
        },
        {
            "sha": "5978a8c4297860dbf950bea675d38469173c2652",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/shlo_to_vector.mlir",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d717d76122f62e912d1133096afc9d1163001ef8/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir?ref=d717d76122f62e912d1133096afc9d1163001ef8",
            "patch": "@@ -141,3 +141,36 @@ func.func @reduce_outer_and_inner(%input : tensor<1024x32x8xf32>, %init : tensor\n // CHECK:   }\n // CHECK:   vector.transfer_read %[[BUFFER1]]{{.*}} : memref<32xf32>, vector<32xf32>\n // CHECK: }\n+\n+// -----\n+\n+func.func @broadcast_0D_tensor(%input : tensor<f32>) -> tensor<32xf32> {\n+  %result = stablehlo.broadcast_in_dim %input, dims = [] : (tensor<f32>) -> tensor<32xf32>\n+  return %result : tensor<32xf32>\n+}\n+\n+// CHECK-LABEL: @broadcast_0D_tensor\n+// CHECK-NOT: vector.shape_cast\n+// CHECK: vector.broadcast {{.*}} : vector<f32> to vector<32xf32>\n+\n+// -----\n+\n+func.func @broadcast_2D_tensor_inner(%input : tensor<4xf32>) -> tensor<32x4xf32> {\n+  %result = stablehlo.broadcast_in_dim %input, dims = [1] : (tensor<4xf32>) -> tensor<32x4xf32>\n+  return %result : tensor<32x4xf32>\n+}\n+\n+// CHECK-LABEL: @broadcast_2D_tensor_inner\n+// CHECK-NOT: vector.shape_cast\n+// CHECK: vector.broadcast {{.*}} : vector<4xf32> to vector<32x4xf32>\n+\n+// -----\n+\n+func.func @broadcast_2D_tensor_outer(%input : tensor<4xf32>) -> tensor<4x32xf32> {\n+  %result = stablehlo.broadcast_in_dim %input, dims = [0] : (tensor<4xf32>) -> tensor<4x32xf32>\n+  return %result : tensor<4x32xf32>\n+}\n+\n+// CHECK-LABEL: @broadcast_2D_tensor_outer\n+// CHECK: vector.shape_cast {{.*}} : vector<4xf32> to vector<4x1xf32>\n+// CHECK: vector.broadcast {{.*}} : vector<4x1xf32> to vector<4x32xf32>"
        }
    ],
    "stats": {
        "total": 144,
        "additions": 134,
        "deletions": 10
    }
}