{
    "author": "chsigg",
    "message": "[xla:gpu] Fix clangtidy warnings (except `misc-include-cleaner`).\n\nPiperOrigin-RevId: 837800257",
    "sha": "dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
    "files": [
        {
            "sha": "4a36a827ba4767b34ce44d15cf6899085a623382",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -67,7 +67,7 @@ class BlockLevelEmitterBackend : public GpuCodegenBackend {\n                            const BackendConfig& config) override;\n \n   // Determines whether the given HLO instruction is supported by this backend.\n-  bool IsSupported(const HloInstruction& instr);\n+  bool IsSupported(const HloInstruction& instr) override;\n \n   // We don't want to use the Triton emitter as a reference because it can\n   // produce wrong results."
        },
        {
            "sha": "19936a63675ce818cc31e284bc5b811db2596c11",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -587,7 +587,9 @@ SmallVector<Value> ReductionFusion::EvaluateEpilogue(\n     EmitterState& state, int group_id, ValueRange symbol_values) const {\n   ImplicitLocOpBuilder& b = state.builder;\n   const auto& epilogue = state.computations.epilogues()[group_id];\n-  if (epilogue.roots.empty()) return outputs;\n+  if (epilogue.roots.empty()) {\n+    return outputs;\n+  }\n \n   auto epilogue_input_indices = state.thread_and_block_ids;\n   epilogue_input_indices.append(symbol_values.begin(), symbol_values.end());"
        },
        {
            "sha": "7a2ba02dfda73435e36adcf0d2bd93fe55c3cfe3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -208,7 +208,9 @@ SmallVector<ValueRange> Unpack(ValueRange range, ArrayRef<int64_t> sizes) {\n SmallVector<Value, 4> PadWithZeros(ValueRange values, int64_t size,\n                                    ImplicitLocOpBuilder& b) {\n   SmallVector<Value, 4> padded_values(values.begin(), values.end());\n-  if (values.size() >= size) return padded_values;\n+  if (values.size() >= size) {\n+    return padded_values;\n+  }\n   auto zero = arith::ConstantIndexOp::create(b, 0);\n   for (int i = values.size(); i < size; ++i) {\n     padded_values.push_back(zero);"
        },
        {
            "sha": "8dead6470b4763201b2ec225d54687e646303b85",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/convert_float_amd.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_amd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_amd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_amd.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -221,9 +221,8 @@ struct RewriteFp8TruncFPattern : public Fp8OpRewritePattern<arith::TruncFOp> {\n       }\n       if (v.getType() != f32_ty) {\n         return arith::TruncFOp::create(b, f32_ty, v);\n-      } else {\n-        return v;\n       }\n+      return v;\n     });\n \n     mlir::StringAttr cvtIntr = b.getStringAttr("
        },
        {
            "sha": "27081f974b6d222abe992773f2a222d47432bdc3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -61,7 +61,7 @@ class FusionInterface {\n // Interface for fusions that are implemented using cuda kernels.\n class KernelFusionInterface : public FusionInterface {\n  public:\n-  virtual ~KernelFusionInterface() = default;\n+  ~KernelFusionInterface() override = default;\n \n   // Returns the fusion's launch dimensions.\n   virtual LaunchDimensions launch_dimensions() const = 0;"
        },
        {
            "sha": "edc204a7ef8833cd87ad4a1957e4523b381655c1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/gpu_test_correctness.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -72,7 +72,9 @@ absl::Status TestBijection(const IndexingMap& map,\n     intervals.push_back({0, size - 1});\n   }\n   auto status = VerifyBijection(map, intervals);\n-  if (status.ok()) return status;\n+  if (status.ok()) {\n+    return status;\n+  }\n   return absl::FailedPreconditionError(\n       absl::StrCat(status.message(), \" in map \", ToString(map)));\n }\n@@ -162,7 +164,9 @@ int main(int argc, char* argv[]) {\n       tsl::Flag(\n           \"bijection_inputs\",\n           [](std::string name_and_ids) {\n-            if (name_and_ids.empty()) return false;\n+            if (name_and_ids.empty()) {\n+              return false;\n+            }\n             flags.bijection_inputs.push_back(\n                 xla::gpu::ParseHeroAndIds(name_and_ids));\n             return true;\n@@ -174,7 +178,9 @@ int main(int argc, char* argv[]) {\n       tsl::Flag(\n           \"bijection_outputs\",\n           [](std::string name) {\n-            if (name.empty()) return false;\n+            if (name.empty()) {\n+              return false;\n+            }\n             flags.bijection_outputs.push_back(name);\n             return true;\n           },"
        },
        {
            "sha": "0a343917ff0cba35e1fa96be28fe2df88d209af2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -234,6 +234,7 @@ xla_cc_test(\n     deps = [\n         \":compilation_pipeline\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "69d5d43dcb20e61137a12157c147c1df6cf61633",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/strings/str_join.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Pass/PassManager.h\"\n@@ -48,12 +49,11 @@ TEST(CompilationPipelineTest, UnswitchLoopsAfterLICM) {\n   }\n   ASSERT_THAT(pass_names, Contains(\"LoopInvariantCodeMotion\"));\n   ASSERT_THAT(pass_names, Contains(\"TritonXLAUnswitchLoopsPass\"));\n-  int licm_index = std::distance(pass_names.begin(),\n-                                 std::find(pass_names.begin(), pass_names.end(),\n-                                           \"LoopInvariantCodeMotion\"));\n-  int unswitch_index = std::distance(\n-      pass_names.begin(), std::find(pass_names.begin(), pass_names.end(),\n-                                    \"TritonXLAUnswitchLoopsPass\"));\n+  int licm_index = std::distance(\n+      pass_names.begin(), absl::c_find(pass_names, \"LoopInvariantCodeMotion\"));\n+  int unswitch_index =\n+      std::distance(pass_names.begin(),\n+                    absl::c_find(pass_names, \"TritonXLAUnswitchLoopsPass\"));\n   // There is no hard requirement to run LICM **immediately** before the loop\n   // unswitcher but you should consider if the newly added pass might interact\n   // with the loop unswitcher."
        },
        {
            "sha": "a545d8979b9b5a7862cc945a8c41a7d1eb9cecc1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 17,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -229,23 +229,20 @@ absl::StatusOr<std::optional<Type>> GetForceOperandsType(\n     // If there is a single allowed operand type, we force the operands to use\n     // this type.\n     return allowed_operands_types.front();\n-\n-  } else {\n-    // If there are several allowed operand types, we just check that the\n-    // operands have the same type, and that this type is one of the allowed\n-    // ones. Raise an error otherwise.\n-    if (lhs_type != rhs_type ||\n-        !absl::c_linear_search(allowed_operands_types, lhs_type)) {\n-      std::string allowed_operands_types_str = absl::StrJoin(\n-          allowed_operands_types, \", \", [&](std::string* out, Type type) {\n-            absl::StrAppend(out, MlirToString(type));\n-          });\n-      return absl::FailedPreconditionError(absl::StrCat(\n-          \"Expected dot operands to both have the same type, and for this type \"\n-          \"to be one of the following types: \",\n-          allowed_operands_types_str, \" but got \", MlirToString(lhs_type),\n-          \" and \", MlirToString(rhs_type)));\n-    }\n+  }  // If there are several allowed operand types, we just check that the\n+  // operands have the same type, and that this type is one of the allowed\n+  // ones. Raise an error otherwise.\n+  if (lhs_type != rhs_type ||\n+      !absl::c_linear_search(allowed_operands_types, lhs_type)) {\n+    std::string allowed_operands_types_str = absl::StrJoin(\n+        allowed_operands_types, \", \", [&](std::string* out, Type type) {\n+          absl::StrAppend(out, MlirToString(type));\n+        });\n+    return absl::FailedPreconditionError(absl::StrCat(\n+        \"Expected dot operands to both have the same type, and for this type \"\n+        \"to be one of the following types: \",\n+        allowed_operands_types_str, \" but got \", MlirToString(lhs_type),\n+        \" and \", MlirToString(rhs_type)));\n   }\n \n   return std::nullopt;"
        },
        {
            "sha": "0335f7de89549ce9600871fdc9371c43231f632e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -281,9 +281,8 @@ Value Cast(mlir::ImplicitLocOpBuilder& b, Value value, Type dst_element_ty) {\n     if (src_fp_element_ty.getFPMantissaWidth() >\n         dst_fp_element_ty.getFPMantissaWidth()) {\n       return ma::TruncFOp::create(b, dst_ty, value);\n-    } else {\n-      return ma::ExtFOp::create(b, dst_ty, value);\n     }\n+    return ma::ExtFOp::create(b, dst_ty, value);\n   }\n   // int => int\n   if (mlir::isa<mlir::IntegerType>(src_element_ty) &&\n@@ -321,16 +320,14 @@ Value Cast(mlir::ImplicitLocOpBuilder& b, Value value, Type dst_element_ty) {\n     auto cst_int = [&](int64_t x) -> Value {\n       if (auto src_shaped_ty = mlir::dyn_cast<ShapedType>(src_ty)) {\n         return CreateConst(b, dst_element_ty, x, src_shaped_ty.getShape());\n-      } else {\n-        return CreateConst(b, dst_element_ty, x);\n       }\n+      return CreateConst(b, dst_element_ty, x);\n     };\n     auto cst_float = [&](int64_t x) -> Value {\n       if (auto src_shaped_ty = mlir::dyn_cast<ShapedType>(src_ty)) {\n         return CreateConst(b, src_fp_element_ty, x, src_shaped_ty.getShape());\n-      } else {\n-        return CreateConst(b, src_fp_element_ty, x);\n       }\n+      return CreateConst(b, src_fp_element_ty, x);\n     };\n     auto fptosi = ma::FPToSIOp::create(b, dst_ty, value);\n     int64_t min = llvm::minIntN(dst_element_ty.getIntOrFloatBitWidth());\n@@ -358,9 +355,8 @@ Value Cast(mlir::ImplicitLocOpBuilder& b, Value value, Type dst_element_ty) {\n Value Subtract(mlir::ImplicitLocOpBuilder& b, ValueRange values) {\n   if (mlir::isa<mlir::IntegerType>(mlir::getElementTypeOrSelf(values[0]))) {\n     return ma::SubIOp::create(b, values[0], values[1]);\n-  } else {\n-    return ma::SubFOp::create(b, values[0], values[1]);\n   }\n+  return ma::SubFOp::create(b, values[0], values[1]);\n }\n \n Value Compare(mlir::ImplicitLocOpBuilder& b, ValueRange values,\n@@ -560,10 +556,9 @@ absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n     if (constant.shape().element_type() == U64) {\n       return CreateConst(b, ty, ScalarConstantValue<uint64_t>(constant, U64),\n                          shape);\n-    } else {\n-      return CreateConst(b, ty, ScalarConstantValue<int64_t>(constant, S64),\n-                         shape);\n     }\n+    return CreateConst(b, ty, ScalarConstantValue<int64_t>(constant, S64),\n+                       shape);\n   }\n   return CreateConst(b, ty, ScalarConstantValue<double>(constant, F64), shape);\n }"
        },
        {
            "sha": "22ac2726d560e7d8a481b067f14f00c956bb3bdb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_attrs.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -31,7 +31,9 @@ static mlir::ParseResult parseI64ArrayAttr(mlir::AsmParser& parser,\n                                            mlir::DenseI64ArrayAttr& array) {\n   array = mlir::dyn_cast_or_null<mlir::DenseI64ArrayAttr>(\n       mlir::DenseI64ArrayAttr::parse(parser, mlir::Type{}));\n-  if (!array) return mlir::failure();\n+  if (!array) {\n+    return mlir::failure();\n+  }\n   return mlir::success();\n }\n "
        },
        {
            "sha": "8f40ccb743c02f6b9b4025df71c51852fc6f0711",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -119,7 +119,9 @@ CodegenDecision IsInstructionSupportsDataTypes(\n     const auto operand_type = operand->shape().element_type();\n     switch (instr.opcode()) {\n       case HloOpcode::kConvert:\n-        if (operand_type == S4) continue;\n+        if (operand_type == S4) {\n+          continue;\n+        }\n         [[fallthrough]];\n       default:\n         if (!IsTritonSupportedDataType(operand_type, gpu_version)) {\n@@ -206,8 +208,9 @@ CodegenDecision CanTritonHandleElementwise(\n   }\n   if (instr.opcode() == HloOpcode::kConstant) {\n     return CodegenDecision::Allow();\n-  } else if (!IsTritonSupportedElementwiseUpToFloatNormalization(\n-                 instr.opcode(), instr.operand(0)->shape().element_type())) {\n+  }\n+  if (!IsTritonSupportedElementwiseUpToFloatNormalization(\n+          instr.opcode(), instr.operand(0)->shape().element_type())) {\n     return CodegenDecision::Forbid(\"Unsupported elementwise operation.\");\n   }\n   return CodegenDecision::Allow();\n@@ -360,7 +363,8 @@ CodegenDecision IsTritonSupportedDynamicSlice(\n   for (int i = 0; i < input->shape().dimensions().size(); ++i) {\n     if (i == majormost_dim_id) {\n       continue;\n-    } else if (input->shape().dimensions(i) != instr.slice_sizes(i)) {\n+    }\n+    if (input->shape().dimensions(i) != instr.slice_sizes(i)) {\n       return CodegenDecision::Forbid(\n           \"Unsupported dynamic slice on non-major-most dimension.\");\n     }"
        },
        {
            "sha": "379cb0b660f712e1d7e4ca4d4799f783b8913d09",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -289,10 +289,9 @@ ENTRY e {\n         ApplyFloatNormalization(dot.Module().get(), GetComputeCapability()));\n     EXPECT_TRUE(RunAndCompareNoHloPasses(\n         std::move(dot.Module()), ErrorSpec{/*aabs=*/2e-4, /*arel=*/2e-4}));\n-  } else {\n-    EXPECT_THAT(TritonFusionAnalysis::Execute(dot.TritonComputation()),\n-                absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n   }\n+  EXPECT_THAT(TritonFusionAnalysis::Execute(dot.TritonComputation()),\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n }\n \n INSTANTIATE_TEST_SUITE_P("
        },
        {
            "sha": "c0d18094dd6b70131ac82efdd48c1b5a04834db1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -89,7 +89,8 @@ bool SupportsBF16(const stream_executor::GpuComputeCapability& cc) {\n   if (cc.IsCuda()) {\n     return cc.cuda_compute_capability()->IsAtLeast(\n         se::CudaComputeCapability::kAmpere);\n-  } else if (cc.IsRocm()) {\n+  }\n+  if (cc.IsRocm()) {\n     return cc.rocm_compute_capability()->has_bf16_dtype_support();\n   }\n   CHECK(false);\n@@ -248,10 +249,9 @@ std::string ComputeCapabilityToString(\n     const stream_executor::GpuComputeCapability& cc) {\n   if (auto* cuda_cc = cc.cuda_compute_capability()) {\n     return absl::StrReplaceAll(cuda_cc->ToString(), {{\".\", \"\"}});\n-  } else {\n-    CHECK(cc.IsRocm());\n-    return \"rocm\";\n   }\n+  CHECK(cc.IsRocm());\n+  return \"rocm\";\n }\n \n std::string TritonSupportTestTypeAndDeviceToString(\n@@ -329,10 +329,9 @@ absl::Status ConvertEntryToTritonFusion(HloModule* module,\n   gpu::GpuBackendConfig gpu_config;\n   if (use_nested_gemm_fusions) {\n     gpu_config.mutable_fusion_backend_config()->set_kind(\n-        std::string(kTritonNestedGemmFusionKind));\n+        kTritonNestedGemmFusionKind);\n   } else {\n-    gpu_config.mutable_fusion_backend_config()->set_kind(\n-        std::string(kTritonFusionKind));\n+    gpu_config.mutable_fusion_backend_config()->set_kind(kTritonFusionKind);\n   }\n   TF_RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n "
        },
        {
            "sha": "0a19aa0f294fe85dc31ac4c72ae2dae75493fcc9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/generalize_kernel_signature.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fgeneralize_kernel_signature.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fgeneralize_kernel_signature.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fgeneralize_kernel_signature.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -60,7 +60,9 @@ void StripParameterAddressSpaces(RewriterBase& rewriter,\n   SmallVector<Type> generic_func_params(\n       llvm::map_range(func_ty.getParams(), [](Type type) -> Type {\n         auto ptr_ty = dyn_cast<LLVM::LLVMPointerType>(type);\n-        if (!ptr_ty) return type;\n+        if (!ptr_ty) {\n+          return type;\n+        }\n         if (ptr_ty.getAddressSpace() != NVVM::NVVMMemorySpace::Global) {\n           return type;\n         }"
        },
        {
            "sha": "c47e19b6286b97ab29ca3adc807b127c16aa6c09",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/round_f32_to_tf32_for_tf32_dot_pass.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fround_f32_to_tf32_for_tf32_dot_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fround_f32_to_tf32_for_tf32_dot_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fround_f32_to_tf32_for_tf32_dot_pass.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -50,10 +50,18 @@ class Tf32DotPattern : public OpRewritePattern<mt::DotOp> {\n   mlir::LogicalResult matchAndRewrite(\n       mt::DotOp op, PatternRewriter &rewriter) const override {\n     constexpr auto tf32_args_rounded = \"tf32_arguments_rounded\";\n-    if (op.getInputPrecision() != mt::InputPrecision::TF32) return failure();\n-    if (!op.getA().getType().getElementType().isF32()) return failure();\n-    if (!op.getB().getType().getElementType().isF32()) return failure();\n-    if (op->hasAttr(tf32_args_rounded)) return failure();\n+    if (op.getInputPrecision() != mt::InputPrecision::TF32) {\n+      return failure();\n+    }\n+    if (!op.getA().getType().getElementType().isF32()) {\n+      return failure();\n+    }\n+    if (!op.getB().getType().getElementType().isF32()) {\n+      return failure();\n+    }\n+    if (op->hasAttr(tf32_args_rounded)) {\n+      return failure();\n+    }\n \n     auto f32ToTF32 = [&](Value value) -> Value {\n       return ElementwiseInlineAsmOp::create("
        },
        {
            "sha": "c095f12a8eb2af5d44f1fda49d34e0b24f141e8c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_block_barrier_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -136,7 +136,7 @@ LogicalResult LowerBlockBarrierOp(BlockBarrierOp block_barrier,\n         // Signal all ranks on the same block id.\n         mlir::triton::xla::AtomicWriteOp::create(\n             builder,\n-            /*result_types=*/mlir::TypeRange{},\n+            /*resultTypes=*/mlir::TypeRange{},\n             /*ptr=*/signal_addresses,\n             /*signal_value=*/signal_value,\n             /*mask=*/mlir::Value{},\n@@ -173,7 +173,7 @@ LogicalResult LowerBlockBarrierOp(BlockBarrierOp block_barrier,\n         // Wait for all ranks on the same block id to signal.\n         mlir::triton::xla::AtomicSpinWaitOp::create(\n             builder,\n-            /*result_types=*/mlir::TypeRange{},\n+            /*resultTypes=*/mlir::TypeRange{},\n             /*ptr=*/wait_addresses,\n             /*expected=*/signal_value,\n             /*mask=*/mlir::Value{},"
        },
        {
            "sha": "896ef40f61d0f670277f45f88eabc90c363cdb9b",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -52,7 +52,9 @@ std::string GpuClique::DebugString() const {\n       num_communicators());\n   int32_t cnt = 0;\n   ForEachComm([&](RankId rank, Communicator* comm) {\n-    if (cnt++) absl::StrAppend(&out, \", \");\n+    if (cnt++) {\n+      absl::StrAppend(&out, \", \");\n+    }\n     absl::StrAppendFormat(&out, \"[rank=%d, comm=%p]\", rank.value(), comm);\n   });\n   return out;\n@@ -63,7 +65,9 @@ absl::Status GpuClique::HealthCheck() const {\n   ForEachComm([&health_check](RankId rank, Communicator* comm) {\n     if (auto s = comm->HealthCheck(); !s.ok()) {\n       LOG(ERROR) << \"GPU communicator error (rank \" << rank << \"): \" << s;\n-      if (health_check.ok()) health_check = std::move(s);  // return first error\n+      if (health_check.ok()) {\n+        health_check = std::move(s);  // return first error\n+      }\n     }\n   });\n   return health_check;"
        },
        {
            "sha": "cd9fdb0f4f225a44c8944a4a6b3837c0aead53ae",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_key.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -43,10 +43,13 @@ bool IsP2PStreamKind(AsyncStreamKind stream_kind) {\n CollectiveStreamId GetCollectiveStreamId(bool is_async,\n                                          CollectiveStreamId stream_id,\n                                          AsyncStreamKind stream_kind) {\n-  if (!is_async) return CollectiveStreamId(0);\n+  if (!is_async) {\n+    return CollectiveStreamId(0);\n+  }\n   // TODO: Remove this fallback once AsyncStreamId is used everywhere.\n-  if (stream_id.value() == 0)\n+  if (stream_id.value() == 0) {\n     return CollectiveStreamId(static_cast<int64_t>(stream_kind) + 1);\n+  }\n   return stream_id;\n }\n "
        },
        {
            "sha": "c6a48b0ab219acf7fa4704399857f1670787b5f1",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_cliques.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -156,7 +156,9 @@ static void CheckClique(const GpuCliqueKey& clique_key,\n             << \" for async errors; num_communicators=\"\n             << clique->num_communicators();\n     clique->ForEachComm([](RankId rank, Communicator* comm) {\n-      if (auto status = CheckComm(comm); !status.ok()) LOG(ERROR) << status;\n+      if (auto status = CheckComm(comm); !status.ok()) {\n+        LOG(ERROR) << status;\n+      }\n     });\n   } else {\n     VLOG(5) << \"Skip checking in-use GPU clique \" << clique_key.ToString();\n@@ -225,7 +227,9 @@ static absl::StatusOr<bool> EnablePeerAccess(\n   for (int64_t i = 0; i < devices.size(); ++i) {\n     for (int64_t j = 0; j < devices.size(); ++j) {\n       // An attempt to enable peer access to itself will fail.\n-      if (i == j) continue;\n+      if (i == j) {\n+        continue;\n+      }\n \n       // To check if peer access is possible, we need to enable it and check\n       // the result. OkStatus means that peer access is possible.\n@@ -692,7 +696,9 @@ absl::StatusOr<std::shared_ptr<LockableGpuClique::Lock>> AcquireGpuClique(\n           WarnStuckTimeout(), TerminateTimeout()));\n \n   // If lock is not null return it to the caller.\n-  if (*clique) return clique;\n+  if (*clique) {\n+    return clique;\n+  }\n \n   // Maybe find if we acquired a clique with communicators that we can split.\n   static const int64_t enable_nccl_comm_splitting ="
        },
        {
            "sha": "c7d584957ed79155a66f7067d515866ee405095a",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_collectives.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -95,7 +95,9 @@ bool NcclCollectives::IsGlobalConfig() const {\n absl::StatusOr<const NcclCollectives::CliqueIdCallback*>\n NcclCollectives::GetCliqueIdCallback(const CliqueIdCallback* clique_id_callback,\n                                      bool is_local) {\n-  if (clique_id_callback != nullptr) return clique_id_callback;\n+  if (clique_id_callback != nullptr) {\n+    return clique_id_callback;\n+  }\n \n   TF_RET_CHECK(is_local || IsGlobalConfig())\n       << \"If non-local devices are taking part of a collective API on \""
        },
        {
            "sha": "215f42b89fab0ed8560066bbb2e8eb464e41dd11",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_collectives.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_collectives.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -42,7 +42,9 @@ limitations under the License.\n namespace xla::gpu {\n \n NvshmemCollectives::~NvshmemCollectives() {\n-  if (initialized_) Finalize();\n+  if (initialized_) {\n+    Finalize();\n+  }\n }\n \n NvshmemCollectives* NvshmemCollectives::Default() {"
        },
        {
            "sha": "9a61904ebf00b5e1e9f3c56cc53e6d7899823564",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -147,7 +147,9 @@ static absl::StatusOr<bool> HostCompare(const ComparisonParams& params) {\n                         std::abs(expected_value_canonical)) +\n                1) <\n           params.relative_tol)) {\n-      if (!params.verbose) return false;  // Return immediately if not verbose.\n+      if (!params.verbose) {\n+        return false;  // Return immediately if not verbose.\n+      }\n       ++differences_seen;\n       LOG(ERROR) << \"Difference at \" << i << \": \" << current_value\n                  << \", expected \" << expected_value;"
        },
        {
            "sha": "314b469cd70d99dcc23b205061dcff1b964a533b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -127,7 +127,7 @@ absl::Status CollectiveMetadataThunk::ConstructCollectiveMetadata(\n                         clique_key.ToString()));\n   }\n   metadata.multicast_buffer_ptr = multimem_address_space;\n-  TF_RET_CHECK(rendezvous_values->size() > 0)\n+  TF_RET_CHECK(!rendezvous_values->empty())\n       << \"Not enough devices in the clique.\";\n   const size_t num_parameters = (*rendezvous_values)[0].parameters.size();\n   for (const auto& value : *rendezvous_values) {"
        },
        {
            "sha": "a7ad06547e7a890d502a5d5093983a5ba9f1eb4f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 16,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -72,8 +72,12 @@ bool IsLocalPeerTransfer(const P2PConfig::SourceTargetMapEntry& source_target,\n   // We determine if it's a local peer if the source/target id is within a node\n   // if they are present.\n   int64_t host_id = (current_id / device_count);\n-  if (source_id && host_id != *source_id / device_count) return false;\n-  if (target_id && host_id != *target_id / device_count) return false;\n+  if (source_id && host_id != *source_id / device_count) {\n+    return false;\n+  }\n+  if (target_id && host_id != *target_id / device_count) {\n+    return false;\n+  }\n   return true;\n }\n \n@@ -197,15 +201,13 @@ absl::Status CollectivePermuteStartThunk::Initialize(\n \n     if (source_id) {\n       std::vector<se::DeviceMemoryBase> dest_addrs;\n-      std::transform(device_buffers.begin(), device_buffers.end(),\n-                     std::back_inserter(dest_addrs),\n-                     [](const DeviceBufferPair& buffer) {\n-                       return buffer.destination_buffer;\n-                     });\n+      absl::c_transform(device_buffers, std::back_inserter(dest_addrs),\n+                        [](const DeviceBufferPair& buffer) {\n+                          return buffer.destination_buffer;\n+                        });\n       std::vector<void*> dest_opaques;\n-      std::transform(\n-          dest_addrs.begin(), dest_addrs.end(),\n-          std::back_inserter(dest_opaques),\n+      absl::c_transform(\n+          dest_addrs, std::back_inserter(dest_opaques),\n           [](se::DeviceMemoryBase dest_addr) { return dest_addr.opaque(); });\n       TF_RETURN_IF_ERROR(recv_ptr_map_.PutRecvPtr(current_id, dest_opaques));\n     }\n@@ -368,11 +370,11 @@ absl::Status RunCollectivePermute(\n   std::optional<int64_t> target_id = source_target.target;\n \n   std::vector<se::DeviceMemoryBase> src_addrs, dest_addrs;\n-  std::transform(\n-      buffers.begin(), buffers.end(), std::back_inserter(src_addrs),\n+  absl::c_transform(\n+      buffers, std::back_inserter(src_addrs),\n       [](const DeviceBufferPair& buffer) { return buffer.source_buffer; });\n-  std::transform(\n-      buffers.begin(), buffers.end(), std::back_inserter(dest_addrs),\n+  absl::c_transform(\n+      buffers, std::back_inserter(dest_addrs),\n       [](const DeviceBufferPair& buffer) { return buffer.destination_buffer; });\n \n   VLOG(3) << absl::StreamFormat(\"%s : id = %d, source_id = %d, target_id = %d\",\n@@ -386,8 +388,12 @@ absl::Status RunCollectivePermute(\n \n     std::optional<RankId> source_rank;\n     std::vector<RankId> target_ranks;\n-    if (source_id) source_rank = RankId(*source_id);\n-    if (target_id) target_ranks.push_back(RankId(*target_id));\n+    if (source_id) {\n+      source_rank = RankId(*source_id);\n+    }\n+    if (target_id) {\n+      target_ranks.push_back(RankId(*target_id));\n+    }\n \n     if (!is_nccl_group_needed) {\n       for (uint64_t idx = 0; idx < buffers.size(); ++idx) {"
        },
        {
            "sha": "07b60b5bacfeb6a29bb2f469acb172f3ebaf0fcf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 23,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -518,12 +518,24 @@ absl::Status CommandBufferCmdExecutor::Record(\n           }\n         }\n \n-        if (has_input && !has_output && !has_temp) input_count++;\n-        if (!has_input && has_output && !has_temp) output_count++;\n-        if (has_input && !has_output && has_temp) input_temp_count++;\n-        if (!has_input && has_output && has_temp) output_temp_count++;\n-        if (has_input && has_output && !has_temp) input_output_count++;\n-        if (has_input && has_output && has_temp) input_temp_output_count++;\n+        if (has_input && !has_output && !has_temp) {\n+          input_count++;\n+        }\n+        if (!has_input && has_output && !has_temp) {\n+          output_count++;\n+        }\n+        if (has_input && !has_output && has_temp) {\n+          input_temp_count++;\n+        }\n+        if (!has_input && has_output && has_temp) {\n+          output_temp_count++;\n+        }\n+        if (has_input && has_output && !has_temp) {\n+          input_output_count++;\n+        }\n+        if (has_input && has_output && has_temp) {\n+          input_temp_output_count++;\n+        }\n       }\n \n       VLOG(5) << \"CommandBufferCmdExecutor allocation summary:\\n\"\n@@ -1620,23 +1632,22 @@ absl::StatusOr<const se::CommandBuffer::Command*> WhileCmd::Record(\n           return command_buffer->UpdateChildCommand(\n               se::CommandBuffer::ChildCommandType::kMoved, command, record_fn);\n         });\n-  } else {\n-    return Handle(\n-        std::move(record_action),\n-        [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n-          return command_buffer->CreateWhile(\n-              se::DeviceMemory<bool>(pred),\n-              CreateCommands(&cond_commands_, &execute_params, &record_params),\n-              CreateCommands(&body_commands_, &execute_params, &record_params),\n-              dependencies);\n-        },\n-        [&](const se::CommandBuffer::Command* command) {\n-          return command_buffer->UpdateWhile(\n-              command, se::DeviceMemory<bool>(pred),\n-              UpdateCommands(&cond_commands_, &execute_params, &record_params),\n-              UpdateCommands(&body_commands_, &execute_params, &record_params));\n-        });\n   }\n+  return Handle(\n+      std::move(record_action),\n+      [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n+        return command_buffer->CreateWhile(\n+            se::DeviceMemory<bool>(pred),\n+            CreateCommands(&cond_commands_, &execute_params, &record_params),\n+            CreateCommands(&body_commands_, &execute_params, &record_params),\n+            dependencies);\n+      },\n+      [&](const se::CommandBuffer::Command* command) {\n+        return command_buffer->UpdateWhile(\n+            command, se::DeviceMemory<bool>(pred),\n+            UpdateCommands(&cond_commands_, &execute_params, &record_params),\n+            UpdateCommands(&body_commands_, &execute_params, &record_params));\n+      });\n }\n \n bool WhileCmd::requires_initialization() {\n@@ -2816,7 +2827,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> DynamicSliceFusionCmd::Record(\n CommandBufferCmd::BufferUseVector DynamicSliceFusionCmd::buffers() const {\n   CommandBufferCmd::BufferUseVector buffers;\n   auto embed_buffers = embedded_commands_.buffers();\n-  for (auto buffer_usage : embed_buffers) {\n+  for (const auto& buffer_usage : embed_buffers) {\n     buffers.emplace_back(\n         *embeded_to_origin_slice_map_.at(buffer_usage.slice().index()),\n         buffer_usage.access());"
        },
        {
            "sha": "19f4087fec88bec8450a462be536ad22b413e033",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -89,7 +89,7 @@ static constexpr auto serialize =\n // buffer cmd commands. We never execute this command, we need it only to pass\n // buffer usage vector to the command buffer cmd commands.\n struct TestOnlyCommandBufferCmd : public CommandBufferCmd {\n-  TestOnlyCommandBufferCmd(BufferUseVector buffer_usage)\n+  explicit TestOnlyCommandBufferCmd(BufferUseVector buffer_usage)\n       : CommandBufferCmd(CommandBufferCmdType::kUnknownCmd, {}),\n         buffer_usage(buffer_usage) {}\n "
        },
        {
            "sha": "4fd5ac8eca55c4c3072d7d3061cf1e386722606a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -258,7 +258,9 @@ absl::Status DynamicSliceThunk::Initialize(const InitializeParams& params) {\n   TF_RETURN_IF_ERROR(embedded_thunk_->Initialize(params));\n \n   absl::MutexLock lock(mutex_);\n-  if (offsets_allocs_.contains(params.executor)) return absl::OkStatus();\n+  if (offsets_allocs_.contains(params.executor)) {\n+    return absl::OkStatus();\n+  }\n \n   VLOG(2) << \"Allocate \" << offsets_allocs_size_\n           << \" bytes for transferring offsets on executor: \" << params.executor;"
        },
        {
            "sha": "0724d3a10b7accd7dfedbd7312774f7fef273e0f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -191,9 +191,8 @@ class DynamicSliceThunk : public Thunk {\n   get_offset_function() const {\n     if (offset_as_function_of_indvar_metadata_.has_value()) {\n       return &offset_as_function_of_indvar_metadata_.value();\n-    } else {\n-      return std::nullopt;\n     }\n+    return std::nullopt;\n   }\n \n  private:"
        },
        {
            "sha": "8ff8be373520fb386ac8ac21ef6258f51b8b6262",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_execute_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -383,9 +383,9 @@ HostExecuteStartThunk::Create(\n     const HostOffloadingExecutableProto& host_offloading_executable_proto,\n     absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4> args,\n     absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4> results) {\n-  auto thunk = absl::WrapUnique(new HostExecuteStartThunk(\n+  auto thunk = std::make_unique<HostExecuteStartThunk>(\n       std::move(thunk_info), host_offloading_executable_proto, std::move(args),\n-      std::move(results)));\n+      std::move(results));\n   if (host_offloading_executable_proto.has_aot_compilation_result()) {\n     TF_RETURN_IF_ERROR(thunk->LoadExecutable());\n   }"
        },
        {
            "sha": "8ad8e3093b823b8d851b87b66bead7d24a8184d6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_send_recv_thunk.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 9,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -55,7 +55,9 @@ using tsl::profiler::TraceMeEncode;\n static absl::StatusOr<bool> ShouldSkip(\n     absl::string_view operation, const Thunk::ExecuteParams& params,\n     const std::optional<GlobalDeviceId>& device_constraint) {\n-  if (!device_constraint.has_value()) return false;\n+  if (!device_constraint.has_value()) {\n+    return false;\n+  }\n \n   GlobalDeviceId global_device_id = params.collective_params->global_device_id;\n   bool skip = global_device_id != *device_constraint;\n@@ -77,8 +79,9 @@ absl::Status HostSendRecvAsyncEvents::Emplace(\n   Key key = {executor, channel_id};\n \n   absl::MutexLock lock(mutex_);\n-  if (auto it = events_.try_emplace(key, std::move(event)); it.second)\n+  if (auto it = events_.try_emplace(key, std::move(event)); it.second) {\n     return absl::OkStatus();\n+  }\n \n   return absl::InternalError(absl::StrFormat(\n       \"Async send/recv event already exists (channel_id=%d)\", channel_id));\n@@ -90,7 +93,9 @@ HostSendRecvAsyncEvents::Extract(se::StreamExecutor* executor,\n   Key key = {executor, channel_id};\n \n   absl::MutexLock lock(mutex_);\n-  if (auto event = events_.extract(key)) return std::move(event.mapped());\n+  if (auto event = events_.extract(key)) {\n+    return std::move(event.mapped());\n+  }\n \n   return absl::InternalError(absl::StrFormat(\n       \"Async send/recv event was not found (channel_id==%d)\", channel_id));\n@@ -166,7 +171,9 @@ absl::Status HostSendThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   TF_ASSIGN_OR_RETURN(bool skip,\n                       ShouldSkip(\"sending buffer\", params, device_constraint_));\n-  if (skip) return absl::OkStatus();\n+  if (skip) {\n+    return absl::OkStatus();\n+  }\n \n   TraceMe trace(\n       [&] { return TraceMeEncode(\"Send\", {{\"channel_id\", channel_id_}}); });\n@@ -259,7 +266,9 @@ absl::Status HostSendDoneThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   TF_ASSIGN_OR_RETURN(bool skip, ShouldSkip(\"waiting for send completion\",\n                                             params, device_constraint_));\n-  if (skip) return absl::OkStatus();\n+  if (skip) {\n+    return absl::OkStatus();\n+  }\n \n   TraceMe trace(\n       [&] { return TraceMeEncode(\"SendDone\", {{\"channel_id\", channel_id_}}); });\n@@ -269,7 +278,9 @@ absl::Status HostSendDoneThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   // Wait until send handler will record an event on the stream.\n   BlockUntilReady(done_event.GetAsyncValue());\n-  if (done_event.IsError()) return done_event.GetError();\n+  if (done_event.IsError()) {\n+    return done_event.GetError();\n+  }\n \n   VLOG(5) << \"Completed Send operation: channel_id=\" << channel_id_;\n \n@@ -356,7 +367,9 @@ absl::Status HostRecvThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   TF_ASSIGN_OR_RETURN(\n       bool skip, ShouldSkip(\"receiving buffer\", params, device_constraint_));\n-  if (skip) return absl::OkStatus();\n+  if (skip) {\n+    return absl::OkStatus();\n+  }\n \n   TraceMe trace(\n       [&] { return TraceMeEncode(\"Recv\", {{\"channel_id\", channel_id_}}); });\n@@ -449,7 +462,9 @@ absl::Status HostRecvDoneThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   TF_ASSIGN_OR_RETURN(bool skip, ShouldSkip(\"waiting for recv completion\",\n                                             params, device_constraint_));\n-  if (skip) return absl::OkStatus();\n+  if (skip) {\n+    return absl::OkStatus();\n+  }\n \n   TraceMe trace(\n       [&] { return TraceMeEncode(\"RecvDone\", {{\"channel_id\", channel_id_}}); });\n@@ -459,7 +474,9 @@ absl::Status HostRecvDoneThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   // Wait until send handler will record an event on the stream.\n   BlockUntilReady(done_event.GetAsyncValue());\n-  if (done_event.IsError()) return done_event.GetError();\n+  if (done_event.IsError()) {\n+    return done_event.GetError();\n+  }\n \n   VLOG(5) << \"Completed Recv operation: channel=\" << channel_id_;\n "
        },
        {
            "sha": "477cea2b60828a1c832394e38a9c55338067b327",
            "filename": "third_party/xla/xla/backends/gpu/runtime/make_batch_pointers.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fmake_batch_pointers.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -39,9 +39,8 @@ absl::Status MakeBatchPointers(se::Stream* stream,\n     if (executor->GetPlatform()->id() ==\n         stream_executor::rocm::kROCmPlatformId) {\n       return 256;\n-    } else {\n-      return 128;\n     }\n+    return 128;\n   }();\n \n   TF_ASSIGN_OR_RETURN("
        },
        {
            "sha": "21f250136ca36f9b32b89df862cd2e18b9f7b73f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -49,7 +49,9 @@ absl::Status ExecutionCounters::Initialize(se::StreamExecutor* executor,\n                                            RunId run_id) {\n   absl::MutexLock lock(mu_);\n   CounterKey key = {executor, run_id};\n-  if (counters_.contains(key)) return absl::OkStatus();\n+  if (counters_.contains(key)) {\n+    return absl::OkStatus();\n+  }\n   counters_.emplace(key, 0);\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "a3aadbacfc7bd012d63a09952f59e109b4b35091",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -77,7 +77,9 @@ struct P2PConfig {\n   static SourceTargetMapEntry GetSourceTarget(\n       const IdToSourceTargetMap& id_to_source_target, int64_t id) {\n     auto it = id_to_source_target.find(id);\n-    if (it != id_to_source_target.end()) return it->second;\n+    if (it != id_to_source_target.end()) {\n+      return it->second;\n+    }\n     return SourceTargetMapEntry{};\n   }\n "
        },
        {
            "sha": "341cf4c5caabec6c0621660f6dfb9f7345d5c6c6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/while_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdb3eb591d6297ef5da269886d3c8e266e1fd03/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc?ref=dbdb3eb591d6297ef5da269886d3c8e266e1fd03",
            "patch": "@@ -61,7 +61,7 @@ static std::list<RunningLoop>& RunningLoops() {\n   return loops;\n }\n \n-bool WhileThunk::RunningWhileThunkLoop() { return RunningLoops().size() > 0; }\n+bool WhileThunk::RunningWhileThunkLoop() { return !RunningLoops().empty(); }\n \n absl::StatusOr<int64_t> WhileThunk::CurrentLoopIteration(int64_t depth) {\n   if (depth >= RunningLoops().size()) {"
        }
    ],
    "stats": {
        "total": 343,
        "additions": 208,
        "deletions": 135
    }
}