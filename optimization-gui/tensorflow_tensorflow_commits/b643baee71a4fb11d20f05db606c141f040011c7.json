{
    "author": "beckerhe",
    "message": "Reverts 1b78db93e6ec47c3db2fd436de1bb0bcf8f8c27c\n\nPiperOrigin-RevId: 798115320",
    "sha": "b643baee71a4fb11d20f05db606c141f040011c7",
    "files": [
        {
            "sha": "2318c2ecf2ae266f6ceef47f4ff1e68bbda6948c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -723,23 +723,20 @@ cc_library(\n     hdrs = [\"gpublas_lt_matmul_thunk.h\"],\n     deps = [\n         \":thunk\",\n-        \"//xla:status_macros\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu:stream_executor_util\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n-        \"@com_google_absl//absl/container:node_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/synchronization\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "66769f65b557461abd8746a95bfa11547b36ce3c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk.cc?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -23,17 +23,16 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/service/gpu/stream_executor_util.h\"\n-#include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n@@ -89,7 +88,7 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(\n   // pointer to the actual instruction, in this case Matmul plans are not\n   // cached.\n   if (instr != nullptr) {\n-    canonical_hlo_ = xla::gpu::AutotuneCacheKey(\"unused\", *instr).GetHlo();\n+    canonical_hlo_ = AutotuneCacheKey::HloInstructionToCanonicalString(*instr);\n   }\n }\n "
        },
        {
            "sha": "52a163cf85abd7c5b7aebc8297794ac03a595c39",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -413,6 +413,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n@@ -746,6 +747,7 @@ xla_cc_test(\n         \"//xla:autotuning_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:dump\",\n         \"//xla/stream_executor:device_description\",\n@@ -767,6 +769,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/hash:hash_testing\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_googletest//:gtest\","
        },
        {
            "sha": "dcb5124ba616e0f3e0b4795011f4499b3037bdce",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -43,9 +43,11 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/gpu/autotuning/autotuner_status_key.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -235,9 +237,8 @@ absl::StatusOr<std::string> AutotuneResultsToString(\n     std::string textproto;\n     if (tsl::protobuf::TextFormat::PrintToString(results, &textproto)) {\n       return textproto;\n-    } else {\n-      return Internal(\"Failed to serialize autotune results.\");\n     }\n+    return Internal(\"Failed to serialize autotune results.\");\n   }\n   return results.SerializeAsString();\n }\n@@ -247,8 +248,8 @@ namespace {\n void SerializeAutotuneEntry(AutotuneResults* results, const AutotuneCacheKey& k,\n                             const AutotuneResult* res) {\n   auto& entry = *results->add_results();\n-  entry.set_device(std::string(k.GetModelStr()));\n-  entry.set_hlo(std::string(k.GetHlo()));\n+  entry.set_device(k.GetModelStr());\n+  entry.set_hlo(k.GetHlo());\n   entry.set_version(k.GetVersion());\n   *entry.mutable_result() = *res;\n }\n@@ -297,12 +298,13 @@ void SerializeAutotuneEntry(AutotuneResults* results, const AutotuneCacheKey& k,\n   return autotune_cache.empty();\n }\n \n-std::string ToCanonicalString(const HloInstruction* instr) {\n+std::string AutotuneCacheKey::HloInstructionToCanonicalString(\n+    const HloInstruction& instr) {\n   auto options = HloPrintOptions::Canonical();\n-  if (instr->opcode() != HloOpcode::kFusion) {\n+  if (instr.opcode() != HloOpcode::kFusion) {\n     options.set_print_backend_config(true);\n     options.set_sort_backend_config(true);\n-    return instr->ToString(options);\n+    return instr.ToString(options);\n   }\n   options.set_print_subcomputation_mode(\n       HloPrintOptions::PrintSubcomputationMode::kOff);\n@@ -314,13 +316,9 @@ std::string ToCanonicalString(const HloInstruction* instr) {\n \n   // TODO(b/266210099): This is unsound. We should probably do the fingerprint\n   // of the HLO computation proto instead.\n-  return instr->called_computations()[0]->ToString(options);\n+  return instr.called_computations()[0]->ToString(options);\n }\n \n-AutotuneCacheKey::AutotuneCacheKey(absl::string_view model_str,\n-                                   const HloInstruction& instr)\n-    : AutotuneCacheKey(model_str, ToCanonicalString(&instr)) {}\n-\n /*static*/ std::string AutotuneCacheKey::DeviceDescriptionToCacheKey(\n     const se::DeviceDescription& device_description) {\n   std::string compute_capability;\n@@ -442,11 +440,6 @@ AutotuneConfig AutotuneConfig::FromDebugOptions(\n                         autotune_cache_dir, autotune_cache_mode);\n }\n \n-/*static*/ AutotuneCacheKey AutotunerUtil::GetKey(\n-    const HloInstruction* instr, const AutotuneConfig& config) {\n-  return AutotuneCacheKey(config.GetModelStr(), *instr);\n-}\n-\n /*static*/ absl::StatusOr<bool> AutotunerUtil::IsInCache(\n     const AutotuneCacheKey& key, const AutotuneConfig& config) {\n   TF_ASSIGN_OR_RETURN(std::optional<AutotuneResult> opt_res,\n@@ -467,7 +460,7 @@ AutotuneConfig AutotuneConfig::FromDebugOptions(\n /*static*/ absl::StatusOr<AutotuneResult> AutotunerUtil::Autotune(\n     const HloInstruction* instr, const AutotuneConfig& config,\n     const AutotuneNoCacheFn& autotune_fn) {\n-  const AutotuneCacheKey key = GetKey(instr, config);\n+  const AutotuneCacheKey key(config.GetDeviceDescription(), *instr);\n   TF_ASSIGN_OR_RETURN(std::optional<AutotuneResult> opt_res,\n                       TryFindInCache(key, config.autotune_cache_dir()));\n   if (opt_res.has_value()) {\n@@ -608,5 +601,11 @@ void AddVersionToAutotuneResults(AutotuneResults& results) {\n   }\n }\n \n+AutotuneCacheKey::AutotuneCacheKey(\n+    const se::DeviceDescription& device_description,\n+    const HloInstruction& instruction, int version)\n+    : AutotuneCacheKey(DeviceDescriptionToCacheKey(device_description),\n+                       HloInstructionToCanonicalString(instruction), version) {}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "e71b7223b850b9b0b6a0c6a6c9d33f76dc4d99e0",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.h",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -115,22 +115,8 @@ class AutotuneCacheKey {\n   static constexpr int kCurrentVersion = 10;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n-                   const HloInstruction& instruction)\n-      : AutotuneCacheKey(DeviceDescriptionToCacheKey(device_description),\n-                         instruction.ToString()) {}\n-\n-  AutotuneCacheKey(absl::string_view model_str,\n-                   const HloInstruction& instruction);\n-\n-  explicit AutotuneCacheKey(absl::string_view model_str,\n-                            absl::string_view hlo_canonical)\n-      : model_str_(model_str), hlo_canonical_(hlo_canonical) {}\n-\n-  explicit AutotuneCacheKey(absl::string_view model_str,\n-                            absl::string_view hlo_canonical, int version)\n-      : model_str_(model_str),\n-        hlo_canonical_(hlo_canonical),\n-        version_(version) {}\n+                   const HloInstruction& instruction,\n+                   int version = kCurrentVersion);\n \n   absl::string_view GetModelStr() const { return model_str_; }\n \n@@ -156,7 +142,22 @@ class AutotuneCacheKey {\n   static std::string DeviceDescriptionToCacheKey(\n       const se::DeviceDescription& device_description);\n \n+  static std::string HloInstructionToCanonicalString(\n+      const HloInstruction& instr);\n+\n  private:\n+  friend class AutotunerUtil;\n+\n+  explicit AutotuneCacheKey(absl::string_view model_str,\n+                            absl::string_view hlo_canonical)\n+      : model_str_(model_str), hlo_canonical_(hlo_canonical) {}\n+\n+  explicit AutotuneCacheKey(absl::string_view model_str,\n+                            absl::string_view hlo_canonical, int version)\n+      : model_str_(model_str),\n+        hlo_canonical_(hlo_canonical),\n+        version_(version) {}\n+\n   std::string model_str_;\n   std::string hlo_canonical_;\n   int version_ = kCurrentVersion;\n@@ -208,11 +209,6 @@ class AutotuneConfig {\n   static AutotuneConfig FromDebugOptions(const DeviceOrDevicelessConfig& config,\n                                          const DebugOptions& opts);\n \n-  std::string GetModelStr() const {\n-    return AutotuneCacheKey::DeviceDescriptionToCacheKey(\n-        GetDeviceDescription());\n-  }\n-\n   se::StreamExecutor* GetExecutor() const { return config_.GetExecutor(); }\n \n   se::DeviceMemoryAllocator* GetAllocator() const {\n@@ -250,17 +246,12 @@ class AutotuneConfig {\n \n using AutotuneNoCacheFn = std::function<absl::StatusOr<AutotuneResult>()>;\n \n-struct AutotunerUtil {\n+class AutotunerUtil {\n+ public:\n   static absl::StatusOr<AutotuneResult> Autotune(\n       const HloInstruction* instr, const AutotuneConfig& config,\n       const AutotuneNoCacheFn& autotune_fn);\n \n-  // Returns the same cache key that would be used inside Autotune().\n-  //\n-  // Normally, we don't have to use this low level method.\n-  static AutotuneCacheKey GetKey(const HloInstruction* instr,\n-                                 const AutotuneConfig& config);\n-\n   // Checks if the key is in the autotune cache.\n   //\n   // Normally, we don't have to use this low level method."
        },
        {
            "sha": "f6e7ea3e9cf8d2662903a7ba0213a6b178d1dc86",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util_test.cc",
            "status": "modified",
            "additions": 81,
            "deletions": 33,
            "changes": 114,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/hash/hash_testing.h\"\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n@@ -35,6 +36,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/gpu/autotuning/autotuner_status_key.h\"\n@@ -48,7 +50,6 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"  // IWYU pragma: keep\n #include \"xla/tsl/platform/status.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/xla.pb.h\"\n@@ -59,15 +60,39 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-using ::testing::ElementsAre;\n using ::testing::HasSubstr;\n using ::testing::IsEmpty;\n using ::testing::Ne;\n using ::testing::Not;\n using ::testing::TempDir;\n using ::testing::UnorderedElementsAre;\n-using ::tsl::testing::IsOkAndHolds;\n-using ::tsl::testing::StatusIs;\n+\n+static constexpr absl::string_view kDeviceDescriptionTextProto = R\"pb(\n+  core_count: 108\n+  clock_rate_ghz: 1.41\n+  memory_bandwidth: 1555000000000\n+  l2_cache_size: 41943040\n+  cuda_compute_capability { major: 8 }\n+)pb\";\n+\n+static constexpr absl::string_view kDotFusionHloText = R\"hlo(\n+    HloModule module\n+    fused_computation {\n+          tmp_0 = f16[1,16,17,3]{3,2,1,0} parameter(0) \n+          tmp_1 = f16[16,51]{1,0} bitcast(f16[1,16,17,3]{3,2,1,0} tmp_0)\n+          tmp_2 = s8[16,17,3]{2,1,0} parameter(1)\n+          tmp_3 = s8[51,16]{0,1} bitcast(s8[16,17,3]{2,1,0} tmp_2)\n+          tmp_4 = f16[51,16]{0,1} convert(s8[51,16]{0,1} tmp_3)\n+          tmp_5 = f16[16,16]{1,0} dot(f16[16,51]{1,0} tmp_1, f16[51,16]{0,1} tmp_4), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+          ROOT tmp_6 = f16[1,16,16]{2,1,0} bitcast(f16[16,16]{1,0} tmp_5)\n+    }\n+    \n+    ENTRY main {\n+          p0 = f16[1,16,17,3]{3,2,1,0} parameter(0) \n+          p1 = s8[16,17,3]{2,1,0} parameter(1)\n+          ROOT fusion = f16[1,16,16]{2,1,0} fusion(p0, p1), kind=kCustom, calls=fused_computation\n+    }\n+  )hlo\";\n \n class AutotunerUtilTest : public HloTestBase {\n  protected:\n@@ -82,26 +107,24 @@ ENTRY e {\n     lhs_contracting_dims={2,3}, rhs_contracting_dims={1,2}\n })\";\n \n-  static constexpr absl::string_view kResultText = R\"(\n-version: 3\n-results {\n-  device: \"CUDA: 8.0, Cores: 108, GPU clock: 1.41 GHz, Memory bandwidth: 1555 GB/s, L2 cache: 40 MB\"\n-  hlo: \"{\\n  tmp_0 = f16[1,16,17,3]{3,2,1,0} parameter(0)\\n  tmp_1 = f16[16,51]{1,0} bitcast(f16[1,16,17,3]{3,2,1,0} tmp_0)\\n  tmp_2 = s8[16,17,3]{2,1,0} parameter(1)\\n  tmp_3 = s8[51,16]{0,1} bitcast(s8[16,17,3]{2,1,0} tmp_2)\\n  tmp_4 = f16[51,16]{0,1} convert(s8[51,16]{0,1} tmp_3)\\n  tmp_5 = f16[16,16]{1,0} dot(f16[16,51]{1,0} tmp_1, f16[51,16]{0,1} tmp_4), lhs_contracting_dims={1}, rhs_contracting_dims={0}\\n  ROOT tmp_6 = f16[1,16,16]{2,1,0} bitcast(f16[16,16]{1,0} tmp_5)\\n}\"\n-  result {\n-    run_time {\n-      nanos: 31744\n-    }\n-    triton {\n-      block_m: 32\n-      block_n: 32\n-      block_k: 32\n-      split_k: 1\n-      num_stages: 1\n-      num_warps: 4\n-      num_ctas: 1\n-    }\n-  }\n-})\";\n+  static constexpr absl::string_view kResultText = R\"pb(\n+    version: 3\n+    results {\n+      device: \"CUDA: 8.0, Cores: 108, GPU clock: 1.41 GHz, Memory bandwidth: 1555 GB/s, L2 cache: 40 MB\"\n+      hlo: \"{\\n  tmp_0 = f16[1,16,17,3]{3,2,1,0} parameter(0)\\n  tmp_1 = f16[16,51]{1,0} bitcast(f16[1,16,17,3]{3,2,1,0} tmp_0)\\n  tmp_2 = s8[16,17,3]{2,1,0} parameter(1)\\n  tmp_3 = s8[51,16]{0,1} bitcast(s8[16,17,3]{2,1,0} tmp_2)\\n  tmp_4 = f16[51,16]{0,1} convert(s8[51,16]{0,1} tmp_3)\\n  tmp_5 = f16[16,16]{1,0} dot(f16[16,51]{1,0} tmp_1, f16[51,16]{0,1} tmp_4), lhs_contracting_dims={1}, rhs_contracting_dims={0}\\n  ROOT tmp_6 = f16[1,16,16]{2,1,0} bitcast(f16[16,16]{1,0} tmp_5)\\n}\"\n+      result {\n+        run_time { nanos: 31744 }\n+        triton {\n+          block_m: 32\n+          block_n: 32\n+          block_k: 32\n+          split_k: 1\n+          num_stages: 1\n+          num_warps: 4\n+          num_ctas: 1\n+        }\n+      }\n+    })pb\";\n \n   void SetUp() override {\n     AutotunerUtil::ClearAutotuneResults();\n@@ -180,21 +203,33 @@ TEST_F(AutotunerUtilTest, LoadAutotuneResultsFromFile_TextProto1) {\n   TF_EXPECT_OK(AutotunerUtil::LoadAutotuneResultsFromFile(kFilePath));\n   EXPECT_FALSE(AutotunerUtil::ResultCacheIsEmpty());\n \n+  stream_executor::GpuDeviceInfoProto device_description_proto;\n+  ASSERT_TRUE(tsl::protobuf::TextFormat::ParseFromString(\n+      kDeviceDescriptionTextProto, &device_description_proto));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnUnverifiedModule(kDotFusionHloText));\n+\n   AutotuneResults results;\n-  EXPECT_TRUE(tsl::protobuf::TextFormat::ParseFromString(\n-      std::string(kResultText), &results));\n+  EXPECT_TRUE(\n+      tsl::protobuf::TextFormat::ParseFromString(kResultText, &results));\n   ASSERT_GT(results.results().size(), 0);\n   AddVersionToAutotuneResults(results);\n-  AutotuneCacheKey key(results.results(0).device(), results.results(0).hlo(),\n-                       results.results(0).version());\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceDescription device_description,\n+      stream_executor::DeviceDescription::FromProto(device_description_proto));\n+  AutotuneCacheKey key(device_description,\n+                       *module->entry_computation()->root_instruction());\n+\n   auto options = DebugOptions();\n   options.set_xla_gpu_require_complete_aot_autotune_results(true);\n   stream_executor::StreamExecutor* executor = NewStreamExecutor();\n   AutotuneConfig config = AutotuneConfig::FromDebugOptions(\n       DeviceOrDevicelessConfig{DeviceConfig{executor}}, options);\n \n   EXPECT_THAT(AutotunerUtil::IsInCache(key, config),\n-              absl_testing::IsOkAndHolds(true));\n+              absl_testing::IsOkAndHolds(true))\n+      << \"Cache key: \" << key.ToString();\n }\n \n TEST_F(AutotunerUtilTest, LoadAutotuneResultsFromFile_TextProto2) {\n@@ -363,7 +398,7 @@ class FileBasedCacheTest : public AutotunerUtilTest {\n   }\n \n   AutotuneCacheKey GetCacheKey() const {\n-    return AutotunerUtil::GetKey(dot_, GetConfig());\n+    return AutotuneCacheKey(GetConfig().GetDeviceDescription(), *dot_);\n   }\n \n   std::string GetCacheFilename() const {\n@@ -547,14 +582,27 @@ TEST(AutotuneCacheKeyTest, DeviceDescriptionToCacheKey) {\n }\n \n TEST(AutotuneCacheKeyTest, VersionIsIncludedInCacheKey) {\n-  AutotuneCacheKey key = AutotuneCacheKey(\"model\", \"hlo\");\n+  stream_executor::DeviceDescription empty_device_description;\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnUnverifiedModule(kDotFusionHloText));\n+  AutotuneCacheKey key =\n+      AutotuneCacheKey(empty_device_description,\n+                       *module->entry_computation()->root_instruction());\n   EXPECT_THAT(key.ToString(),\n               HasSubstr(absl::StrFormat(\"version=%d\", key.GetVersion())));\n }\n \n TEST(AutotuneCacheKeyTest, VersionChangeInvalidateCacheKey) {\n-  AutotuneCacheKey key0 = AutotuneCacheKey(\"model\", \"hlo\", /*version=*/0);\n-  AutotuneCacheKey key1 = AutotuneCacheKey(\"model\", \"hlo\", /*version=*/1);\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnUnverifiedModule(kDotFusionHloText));\n+  stream_executor::DeviceDescription empty_device_description;\n+\n+  AutotuneCacheKey key0 = AutotuneCacheKey(\n+      empty_device_description,\n+      *module->entry_computation()->root_instruction(), /*version=*/0);\n+  AutotuneCacheKey key1 = AutotuneCacheKey(\n+      empty_device_description,\n+      *module->entry_computation()->root_instruction(), /*version=*/1);\n   EXPECT_FALSE(key0 == key1);\n   EXPECT_NE(key0.ToString(), key1.ToString());\n   EXPECT_TRUE(absl::VerifyTypeImplementsAbslHashCorrectly({"
        },
        {
            "sha": "b642886f8be9e1f9ebf206b1eb0b688f0e972fa5",
            "filename": "third_party/xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker.cc?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -787,7 +787,7 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n absl::StatusOr<AutotuneResult>\n GpuConvAlgorithmPicker::PickBestAlgorithmNoCacheCuda(\n     const HloCustomCallInstruction* instr) {\n-  AutotuneCacheKey instruction_info{config_.GetModelStr(), *instr};\n+  AutotuneCacheKey instruction_info{config_.GetDeviceDescription(), *instr};\n   std::string instr_str(instruction_info.GetHlo());\n   XLA_SCOPED_LOGGING_TIMER(absl::StrCat(\n       \"GpuConvAlgorithmPicker::PickBestAlgorithmImpl for \", instr_str));"
        },
        {
            "sha": "664123e3cc3df52c33c4c7512c9ffc66484e6ef5",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.cc?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -441,7 +441,7 @@ absl::StatusOr<bool> RunOnInstruction(HloInstruction* gemm,\n   }\n \n   const AutotuneConfig& config = autotuner.config();\n-  AutotuneCacheKey key(config.GetModelStr(), *gemm);\n+  AutotuneCacheKey key(config.GetDeviceDescription(), *gemm);\n   TF_ASSIGN_OR_RETURN(AutotuneResult algorithm,\n                       AutotunerUtil::Autotune(\n                           gemm, config, [&] { return autotuner(gemm, key); }));"
        },
        {
            "sha": "42dd46bd255747654186faf46b209a6462969dbb",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -179,7 +179,8 @@ class GemmFusionCollector : public ConstDfsHloVisitorWithDefault {\n       return absl::OkStatus();\n     }\n \n-    AutotuneCacheKey key = AutotunerUtil::GetKey(hlo, impl_->GetConfig());\n+    AutotuneCacheKey key =\n+        AutotuneCacheKey(impl_->GetConfig().GetDeviceDescription(), *hlo);\n     auto [iterator, inserted] = result_.fusion_count_map.insert({key, 1});\n     if (inserted) {\n       result_.fingerprint += key.GetHlo();\n@@ -1281,7 +1282,8 @@ absl::Status GemmFusionAutotunerImpl::Autotune(\n           /*contents=*/module->ToString());\n     }\n \n-    const AutotuneCacheKey key = AutotunerUtil::GetKey(fusion, config_);\n+    const AutotuneCacheKey key =\n+        AutotuneCacheKey(config_.GetDeviceDescription(), *fusion);\n     TF_ASSIGN_OR_RETURN(\n         bool added, AutotunerUtil::AddResult(key, std::move(best), config_));\n     if (!added) {\n@@ -1415,7 +1417,8 @@ absl::StatusOr<bool> GemmFusionAutotuner::Run(\n   if (!autotuner.IsAutotuningEnabled()) {\n     // Pick the first option for each gemm instead of autotuning.\n     for (const auto& [fusion, tilings] : config_sets) {\n-      const AutotuneCacheKey key = AutotunerUtil::GetKey(fusion, config_);\n+      const AutotuneCacheKey key =\n+          AutotuneCacheKey(config_.GetDeviceDescription(), *fusion);\n       AutotuneResult res = FromConfig(tilings[0]);\n       *res.mutable_run_time() =\n           tsl::proto_utils::ToDurationProto(absl::ZeroDuration());\n@@ -1429,7 +1432,8 @@ absl::StatusOr<bool> GemmFusionAutotuner::Run(\n     VLOG(1) << \"Overriding GEMM autotuner with the following config: \"\n             << gemm_key.DebugString();\n     for (const auto& [fusion, unused] : config_sets) {\n-      const AutotuneCacheKey key = AutotunerUtil::GetKey(fusion, config_);\n+      const AutotuneCacheKey key =\n+          AutotuneCacheKey(config_.GetDeviceDescription(), *fusion);\n       AutotuneResult res;\n       *res.mutable_triton() = gemm_key;\n       *res.mutable_run_time() ="
        },
        {
            "sha": "f80faae73b09faf0c4ecdc0e8fbadf381ea7b235",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b643baee71a4fb11d20f05db606c141f040011c7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=b643baee71a4fb11d20f05db606c141f040011c7",
            "patch": "@@ -734,7 +734,7 @@ ENTRY main {\n       DeviceOrDevicelessConfig{DeviceConfig{backend().default_stream_executor(),\n                                             backend().memory_allocator()}},\n       opts);\n-  AutotuneCacheKey cache_key(autotune_config.GetModelStr(),\n+  AutotuneCacheKey cache_key(autotune_config.GetDeviceDescription(),\n                              *module->entry_computation()->root_instruction());\n \n   TF_ASSERT_OK_AND_ASSIGN(AutotuneResults autotune_results_override,\n@@ -1470,7 +1470,7 @@ TEST_F(\n \n   const int kProcessCount = 2;\n   AutotuneConfig autotune_config = GetAutotuneConfigForTest();\n-  AutotuneCacheKey cache_key(autotune_config.GetModelStr(),\n+  AutotuneCacheKey cache_key(autotune_config.GetDeviceDescription(),\n                              *module->entry_computation()->root_instruction());\n   TF_ASSERT_OK_AND_ASSIGN(AutotuneResults autotune_results_override,\n                           GetDummyAutotuneResultsForCacheKey(cache_key));\n@@ -1554,7 +1554,7 @@ TEST_F(\n \n   const int kProcessCount = 2;\n   AutotuneConfig autotune_config = GetAutotuneConfigForTest();\n-  AutotuneCacheKey cache_key(autotune_config.GetModelStr(),\n+  AutotuneCacheKey cache_key(autotune_config.GetDeviceDescription(),\n                              *module1->entry_computation()->root_instruction());\n   TF_ASSERT_OK_AND_ASSIGN(AutotuneResults autotune_results_override,\n                           GetDummyAutotuneResultsForCacheKey(cache_key));\n@@ -1611,7 +1611,7 @@ TEST_F(GemmFusionAutotunerTest, RewritesGemmFusionToCustomKernelFusion) {\n       DeviceOrDevicelessConfig{DeviceConfig{backend().default_stream_executor(),\n                                             backend().memory_allocator()}},\n       opts);\n-  AutotuneCacheKey cache_key(autotune_config.GetModelStr(),\n+  AutotuneCacheKey cache_key(autotune_config.GetDeviceDescription(),\n                              *module->entry_computation()->root_instruction());\n   TF_ASSERT_OK_AND_ASSIGN(AutotuneResults autotune_results_override,\n                           GetDummyAutotuneResultsForCacheKey(cache_key));"
        }
    ],
    "stats": {
        "total": 241,
        "additions": 141,
        "deletions": 100
    }
}