{
    "author": "serach24",
    "message": "PR #34870: [XLA:GPU] Enable deterministic scatter for batched operations\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34870\n\nüìù Summary of Changes\nFix ScatterDeterminismExpander to correctly handle scatter operations that have been normalized from batched form by BatchedGatherScatterNormalizer.\nThe key fix is in FlattenIndices: when computing scalar indices for sorting, we now use scatter_dims_to_operand_dims to select the correct stride for each index column, rather than assuming index columns map to operand dimensions in order.\nüéØ Justification\nPreviously, ScatterDeterminismExpander would produce incorrect results for batched scatter operations because:\nBatchedGatherScatterNormalizer runs first and transforms batched scatter into a normalized form where batch indices are concatenated into the index tensor\nAfter normalization, scatter_dims_to_operand_dims could be {0, 2} (not {0, 1, 2})\nFlattenIndices assumed direct column‚Üídimension mapping, causing indices from different batches to collide and mix updates incorrectly\nThis enables deterministic scatter for models using batched scatter operations (e.g., batched attention, batched embedding lookups).\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüß™ Unit Tests\nExisting test ScatterTest.Scatter_Add_F32 in xla/tests/scatter_test.cc now passes - it exercises batched scatter with input_batching_dims={0} and validates correctness against the interpreter reference.\nüß™ Execution Tests\nScatterTest.Scatter_Add_F32 runs end-to-end on GPU, triggering the BatchedGatherScatterNormalizer ‚Üí ScatterDeterminismExpander pipeline and asserting correct output.\nCopybara import of the project:\n\n--\nb110942e013047c90b19a49bef0aa487061753f6 by Chenhao Jiang <chenhaoj@nvidia.com>:\n\n[XLA:GPU] Fix the issue of scatter determinism expander for scatter op with batch dims\n\nMerging this change closes #34870\n\nPiperOrigin-RevId: 840630461",
    "sha": "1476f6113d686a2e2d98ff643e0bcdf04bcacec9",
    "files": [
        {
            "sha": "f1a2267520a569ca40d8e0026b8a7b360eb7827f",
            "filename": "third_party/xla/xla/service/scatter_determinism_expander.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 10,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1476f6113d686a2e2d98ff643e0bcdf04bcacec9/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1476f6113d686a2e2d98ff643e0bcdf04bcacec9/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc?ref=1476f6113d686a2e2d98ff643e0bcdf04bcacec9",
            "patch": "@@ -131,26 +131,39 @@ absl::StatusOr<HloInstruction*> CreateBoundTensor(\n \n // indices shape: (num_indices, num_dims)\n // updates shape: (num_indices,)\n-HloInstruction* FlattenIndices(HloComputation* parent, HloInstruction* indices,\n-                               absl::Span<const int64_t> operand_dims) {\n-  if (indices->shape().dimensions(1) == 1) {\n+// scatter_dims_to_operand_dims: maps each index column to its operand dimension\n+HloInstruction* FlattenIndices(\n+    HloComputation* parent, HloInstruction* indices,\n+    absl::Span<const int64_t> operand_dims,\n+    absl::Span<const int64_t> scatter_dims_to_operand_dims) {\n+  int64_t num_index_cols = indices->shape().dimensions(1);\n+  if (num_index_cols == 1) {\n     // Originally scalar indices\n     return parent->AddInstruction(HloInstruction::CreateReshape(\n         ShapeUtil::MakeShape(indices->shape().element_type(),\n                              {indices->shape().dimensions(0)}),\n         indices));\n   }\n-  // Step 1: based on the operand_dims, calculate the strides\n-  Array2D<int64_t> strides(operand_dims.size(), 1);\n+\n+  // Calculate strides for each operand dimension\n+  std::vector<int64_t> operand_strides(operand_dims.size());\n   int64_t stride = 1;\n   for (int i = operand_dims.size() - 1; i >= 0; --i) {\n-    strides(i, 0) = stride;\n+    operand_strides[i] = stride;\n     stride *= operand_dims[i];\n   }\n+\n+  // Create strides for index columns using scatter_dims_to_operand_dims\n+  // Each index column maps to an operand dimension, use that dimension's stride\n+  Array2D<int64_t> strides(num_index_cols, 1);\n+  for (int i = 0; i < num_index_cols; ++i) {\n+    int64_t operand_dim = scatter_dims_to_operand_dims[i];\n+    strides(i, 0) = operand_strides[operand_dim];\n+  }\n   auto strides_tensor = parent->AddInstruction(HloInstruction::CreateConstant(\n       LiteralUtil::CreateR2FromArray2D<int64_t>(strides)));\n \n-  // Step 2: calculate the flattened indices\n+  // Calculate the flattened indices via dot product\n   auto dot_shape = ShapeUtil::MakeShape(indices->shape().element_type(),\n                                         {indices->shape().dimensions(0), 1});\n   DotDimensionNumbers dim_numbers;\n@@ -202,14 +215,17 @@ static std::vector<HloInstruction*> SortIndicesAndUpdates(\n     HloInstruction* scatter_indices,\n     const std::vector<HloInstruction*>& scatter_updates, int64_t num_indices,\n     HloScatterInstruction* scatter, HloComputation* parent,\n-    absl::Span<const int64_t> operand_dims, bool has_scalar_indices) {\n+    absl::Span<const int64_t> operand_dims,\n+    absl::Span<const int64_t> scatter_dims_to_operand_dims,\n+    bool has_scalar_indices) {\n   const Shape& indices_shape = scatter_indices->shape();\n   const Shape& updates_shape = scatter_updates[0]->shape();\n   auto updates_dims = updates_shape.dimensions();\n   // Since we canonicalized the scatter updates, the first dim will always be\n   // the number of updates and the rest will be the shape of each update\n   HloInstruction* scalar_indices =\n-      FlattenIndices(scatter->parent(), scatter_indices, operand_dims);\n+      FlattenIndices(scatter->parent(), scatter_indices, operand_dims,\n+                     scatter_dims_to_operand_dims);\n \n   // Create the shape for a single index tuple\n   // Create [0...num_indices] tensor for permutation in sorting\n@@ -825,7 +841,8 @@ absl::StatusOr<HloInstruction*> ScatterDeterminismExpander::ExpandInstruction(\n   int64_t num_indices = ShapeUtil::ElementsIn(scatter_updates[0]->shape());\n   std::vector<HloInstruction*> sorted_tensors = SortIndicesAndUpdates(\n       scatter_indices, scatter_updates, num_indices, scatter, parent,\n-      scatter_operands[0]->shape().dimensions(), has_scalar_indices);\n+      scatter_operands[0]->shape().dimensions(),\n+      new_dim_numbers.scatter_dims_to_operand_dims(), has_scalar_indices);\n   HloInstruction* sorted_scalar_indices = sorted_tensors[0];\n   std::vector<HloInstruction*> sorted_updates(\n       sorted_tensors.begin() + 1,"
        },
        {
            "sha": "b58fd9865da4789f563800d2c9d053e783450422",
            "filename": "third_party/xla/xla/tests/scatter_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1476f6113d686a2e2d98ff643e0bcdf04bcacec9/third_party%2Fxla%2Fxla%2Ftests%2Fscatter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1476f6113d686a2e2d98ff643e0bcdf04bcacec9/third_party%2Fxla%2Fxla%2Ftests%2Fscatter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fscatter_test.cc?ref=1476f6113d686a2e2d98ff643e0bcdf04bcacec9",
            "patch": "@@ -1050,11 +1050,6 @@ ENTRY main {\n }\n \n TEST_F(ScatterTest, Scatter_Add_F32) {\n-  if (GetDebugOptionsForTest().xla_gpu_enable_scatter_determinism_expander() &&\n-      GetDebugOptionsForTest().xla_gpu_deterministic_ops()) {\n-    // TODO(b/443204632): Re-enable this test.\n-    GTEST_SKIP() << \"Currently fails\";\n-  }\n   const std::string hlo_text = R\"(\n HloModule scatter_add\n "
        }
    ],
    "stats": {
        "total": 42,
        "additions": 27,
        "deletions": 15
    }
}