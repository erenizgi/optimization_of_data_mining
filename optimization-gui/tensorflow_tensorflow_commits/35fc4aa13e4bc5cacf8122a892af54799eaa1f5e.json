{
    "author": "BruceXinXin",
    "message": "PR #29189: Moved InstructionVerifier class declaration from implementation file to header file\n\nImported from GitHub PR https://github.com/openxla/xla/pull/29189\n\nSame as https://github.com/openxla/xla/pull/28685, moved to a new PR because of the commit email conflict issue.\n\nThis refactoring separates the class declaration from its implementation, improving code organization and potentially enabling reuse across multiple compilation units.\n\nAlso, it enables users (like our team) to use InstructionVerifier class externally.\nCopybara import of the project:\n\n--\nd211f2b57e84de74d4f26e91e461a141a0deb20e by bruce xin <1592500029xpx@gmail.com>:\n\nMove InstructionVerifier declaration to header\n\nMerging this change closes #29189\n\nPiperOrigin-RevId: 802030982",
    "sha": "35fc4aa13e4bc5cacf8122a892af54799eaa1f5e",
    "files": [
        {
            "sha": "e57d5c078d10a6ca2bc23ca5d07693215d7f7f9f",
            "filename": "third_party/xla/xla/service/hlo_verifier.cc",
            "status": "modified",
            "additions": 328,
            "deletions": 353,
            "changes": 681,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/35fc4aa13e4bc5cacf8122a892af54799eaa1f5e/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/35fc4aa13e4bc5cacf8122a892af54799eaa1f5e/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc?ref=35fc4aa13e4bc5cacf8122a892af54799eaa1f5e",
            "patch": "@@ -3141,359 +3141,6 @@ absl::Status CheckElementwiseInstruction(HloInstruction* instruction) {\n   return absl::OkStatus();\n }\n \n-// Visitor which verifies various fields on the HLO instruction. This class does\n-// not check result shape as that is checked in the ShapeVerifier.\n-class InstructionVerifier : public DfsHloVisitorWithDefault {\n- public:\n-  InstructionVerifier(const HloModule* module, const HloVerifierOpts& opts)\n-      : opts_(opts) {\n-    // TODO(b/258285553): Eliminate this check when all paths that enable SPMD\n-    // partitioning also set the num_partitions correctly.\n-    const int64_t num_partitions = module->config().num_partitions();\n-    if (module->config().use_spmd_partitioning() &&\n-        opts.verify_sharding_device_numbers && num_partitions > 1) {\n-      num_devices_ = module->config().num_partitions();\n-    }\n-  }\n-\n-  absl::Status DefaultAction(HloInstruction*) override {\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleFusion(HloInstruction* fusion) override {\n-    TF_RETURN_IF_ERROR(CheckCallableInstructionThreadName(fusion));\n-    return CheckFusionInstruction(fusion);\n-  }\n-\n-  absl::Status HandleBroadcast(HloInstruction* broadcast) override {\n-    // If you see this failure then someone has confused the difference\n-    // between the HLO broadcast op, and the UserComputation broadcast\n-    // op. See https://groups.google.com/forum/#!topic/xla-dev/9LqijHmTt_I\n-    // or ComputationLowerer::Visit()\n-    TF_RET_CHECK(broadcast->dimensions().size() ==\n-                 broadcast->operand(0)->shape().dimensions().size())\n-        << \"Broadcast HLO (\" << broadcast->ToShortString()\n-        << \") has invalid number of dimensions: \"\n-        << broadcast->dimensions().size()\n-        << \" != \" << broadcast->operand(0)->shape().dimensions().size();\n-    if (opts_.verify_broadcast_dimensions_order) {\n-      TF_RET_CHECK(absl::c_is_sorted(broadcast->dimensions()))\n-          << \"Broadcast dimensions should be ordered, got: \"\n-          << broadcast->ToString();\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleBitcastConvert(HloInstruction* c) override {\n-    // Shape verifier will check all we need.\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleWhile(HloInstruction* xla_while) override {\n-    auto* while_cond = xla_while->while_condition();\n-    auto* while_body = xla_while->while_body();\n-    if (while_cond->num_parameters() != 1) {\n-      return FailedPrecondition(\n-          \"While condition must have exactly 1 parameter; had %d : %s\",\n-          while_cond->num_parameters(), while_cond->ToString());\n-    }\n-    if (while_body->num_parameters() != 1) {\n-      return FailedPrecondition(\n-          \"While body must have exactly 1 parameter; had %d : %s\",\n-          while_body->num_parameters(), while_body->ToString());\n-    }\n-    if (xla_while->operand_count() != 1) {\n-      return FailedPrecondition(\n-          \"While loop must have exactly one operand; had %d : %s\",\n-          xla_while->operand_count(), xla_while->ToString());\n-    }\n-    // Allow kWhile to contain computations on separate thread.\n-    TF_RETURN_IF_ERROR(CheckCallableInstructionThreadName(xla_while));\n-\n-    // Verify consistency of sharding of while instructions and related\n-    // instructions (parameters, root) in its called computations.\n-    TF_RETURN_IF_ERROR(VerifyConsistentSharding(\n-        xla_while, {xla_while, xla_while->while_body()->root_instruction(),\n-                    xla_while->while_body()->parameter_instruction(0),\n-                    xla_while->while_condition()->parameter_instruction(0)}));\n-\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleCall(HloInstruction* call) override {\n-    if (opts_.verify_call_nested_computation_thread_name) {\n-      return CheckCallableInstructionThreadName(call);\n-    }\n-\n-    // As opposed to other callable instructions, nothing respects input/output\n-    // aliasing for call instructions, so make sure it's not set.\n-    const HloCallableInstruction* callable =\n-        DynCast<const HloCallableInstruction>(call);\n-    TF_RET_CHECK(callable != nullptr);\n-    TF_RET_CHECK(callable->output_to_operand_aliasing().empty())\n-        << \"Call instruction \" << call->ToString()\n-        << \" may not have an output-to-operand aliasing set.\";\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleConditional(HloInstruction* conditional) override {\n-    const std::vector<HloComputation*> branch_computations =\n-        conditional->branch_computations();\n-    std::vector<const HloInstruction*> sharding_check_instructions;\n-    sharding_check_instructions.reserve(branch_computations.size() + 1);\n-    sharding_check_instructions.push_back(conditional);\n-\n-    for (const HloComputation* branch_computation : branch_computations) {\n-      if (branch_computation->num_parameters() != 1) {\n-        return FailedPrecondition(\n-            \"Branch computation %s of %s must have 1 parameter instead of %d\",\n-            branch_computation->name(), conditional->ToString(),\n-            branch_computation->num_parameters());\n-      }\n-      sharding_check_instructions.push_back(\n-          branch_computation->root_instruction());\n-    }\n-    // Allow kConditional to contain computations on separate thread.\n-    TF_RETURN_IF_ERROR(CheckCallableInstructionThreadName(conditional));\n-\n-    // Verify consistency of sharding of conditional instructions and roots of\n-    // its branches.\n-    TF_RETURN_IF_ERROR(\n-        VerifyConsistentSharding(conditional, sharding_check_instructions));\n-\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleElementwiseUnary(HloInstruction* instruction) override {\n-    TF_RETURN_IF_ERROR(CheckUnaryOpWithResultAccuracy(instruction));\n-    return CheckElementwiseInstruction(instruction);\n-  }\n-\n-  absl::Status HandleElementwiseBinary(HloInstruction* instruction) override {\n-    return CheckElementwiseInstruction(instruction);\n-  }\n-\n-  absl::Status HandleGetTupleElement(HloInstruction* gte) override {\n-    TF_RET_CHECK(gte->operand(0)->shape().IsTuple());\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleTranspose(HloInstruction* transpose) override {\n-    const Shape& shape = transpose->shape();\n-    const HloInstruction* operand = transpose->operand(0);\n-    TF_RET_CHECK(shape.dimensions().size() == transpose->dimensions().size());\n-    TF_RET_CHECK(shape.dimensions().size() ==\n-                 transpose->operand(0)->shape().dimensions().size());\n-    TF_RET_CHECK(std::equal(\n-        shape.dimensions().begin(), shape.dimensions().end(),\n-        Permute(operand->shape().dimensions(), transpose->dimensions())\n-            .begin()))\n-        << \"shape: \" << shape << \", operand->shape(): \" << shape\n-        << \", dimensions: {\" << absl::StrJoin(transpose->dimensions(), \", \")\n-        << \"}\";\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleAllReduce(HloInstruction* crs) override {\n-    if (crs->channel_id().has_value()) {\n-      TF_RET_CHECK(crs->channel_id().value() > 0)\n-          << \"All reduce channel id must be greater than 0 for \"\n-          << crs->ToShortString();\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleReshape(HloInstruction* hlo) override {\n-    if (opts_.verify_reshape_is_bitcast && !hlo->IsFused()) {\n-      TF_RET_CHECK(\n-          ShapeUtil::ReshapeIsBitcast(hlo->operand(0)->shape(), hlo->shape()))\n-          << \"Reshape should be a physical bitcast, got: \" << hlo->ToString();\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleCustomCall(HloInstruction* hlo) override {\n-    if (opts_.verify_call_nested_computation_thread_name) {\n-      // Allow kCustomCall to contain computations on separate thread.\n-      return CheckCallableInstructionThreadName(hlo);\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status HandleScatter(HloInstruction* scatter) override {\n-    int64_t rank = scatter->operand(0)->shape().dimensions().size();\n-    for (int64_t operand_dim :\n-         scatter->scatter_dimension_numbers().scatter_dims_to_operand_dims()) {\n-      if (operand_dim > rank) {\n-        return absl::OutOfRangeError(absl::StrCat(\n-            \"The provided scatter_dims_to_operand_dim was out of range.\",\n-            \" (operand_dim: \", operand_dim, \", rank: \", rank, \")\"));\n-      }\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status Preprocess(HloInstruction* instruction) override {\n-    auto [it, inserted] =\n-        instructions_by_name_.emplace(instruction->name(), instruction);\n-    TF_RET_CHECK(inserted) << \"HLO has name that is not unique within module:\\n\"\n-                           << instruction->ToString() << \" in computation: \"\n-                           << instruction->parent()->name()\n-                           << \"\\nPrevious HLO with same name:\\n\"\n-                           << it->second->ToString() << \" in computation: \"\n-                           << it->second->parent()->name();\n-\n-    if (instruction->has_sharding()) {\n-      absl::Status status =\n-          instruction->sharding().Validate(instruction->shape(), num_devices_);\n-      if (!status.ok()) {\n-        return absl::Status(\n-            status.code(),\n-            absl::StrCat(\"Invalid sharding for instruction: \",\n-                         instruction->ToString(), \": \", status.message()));\n-      }\n-    }\n-\n-    if (opts_.verify_call_nested_computation_thread_name &&\n-        instruction->has_to_apply() &&\n-        xla::GetInstructionCallContext(instruction->opcode()) !=\n-            xla::CallContext::kEmbedded &&\n-        instruction->to_apply()->execution_thread() !=\n-            instruction->parent()->execution_thread()) {\n-      return Internal(\n-          \"Non-Embedded context callable instruction %s to_apply computation \"\n-          \"execution thread does not match (%s vs %s)\",\n-          instruction->name(), instruction->to_apply()->execution_thread(),\n-          instruction->parent()->execution_thread());\n-    }\n-\n-    return absl::OkStatus();\n-  }\n-\n-  absl::Status Postprocess(HloInstruction* instruction) override {\n-    if (opts_.verify_no_host_memory_space) {\n-      TF_RETURN_IF_ERROR(VerifyNoHostMemorySpace(instruction));\n-    }\n-    if (!opts_.InstructionCanChangeLayout(instruction) &&\n-        instruction->shape().IsArray() && instruction->shape().has_layout()) {\n-      const Shape& result_shape = instruction->shape();\n-      const Layout& result_layout = result_shape.layout();\n-      for (HloInstruction* operand : instruction->operands()) {\n-        const Shape& operand_shape = operand->shape();\n-        if (operand_shape.IsArray() &&\n-            operand_shape.dimensions().size() ==\n-                result_shape.dimensions().size() &&\n-            operand_shape.has_layout()) {\n-          const Layout& operand_layout = operand_shape.layout();\n-          Layout::Equal equal_predicate =\n-              Layout::Equal().IgnoreTiles().IgnoreMemorySpace();\n-          if (instruction->opcode() == HloOpcode::kConvert ||\n-              instruction->opcode() == HloOpcode::kCompare ||\n-              instruction->opcode() == HloOpcode::kIsFinite ||\n-              (instruction->opcode() == HloOpcode::kSelect &&\n-               operand_shape.element_type() == PRED) ||\n-              instruction->opcode() == HloOpcode::kScatter) {\n-            // Some instructions can change element_size_in_bits\n-            // Select instructions ignore element_size_in_bits for predicate\n-            equal_predicate.IgnoreElementSize();\n-          } else if (instruction->opcode() == HloOpcode::kDynamicSlice ||\n-                     instruction->opcode() == HloOpcode::kDynamicUpdateSlice ||\n-                     instruction->opcode() == HloOpcode::kCopy) {\n-            TF_RETURN_IF_ERROR(HostOffloadInstructionCanChangeMemorySpace(\n-                instruction, operand_layout.memory_space(),\n-                result_layout.memory_space()));\n-            equal_predicate.IgnoreMemorySpace();\n-          }\n-          TF_RET_CHECK(equal_predicate(result_layout, operand_layout))\n-              << \"Instruction shouldn't change layouts \"\n-              << instruction->ToString() << \" From \" << result_shape << \" To \"\n-              << operand_shape;\n-        }\n-      }\n-    }\n-    return absl::OkStatus();\n-  }\n-\n- private:\n-  static absl::Status VerifyConsistentSharding(\n-      const HloInstruction* parent,\n-      absl::Span<const HloInstruction* const> instructions) {\n-    const HloInstruction* common_sharding_inst = nullptr;\n-    for (const HloInstruction* check_inst : instructions) {\n-      if (!check_inst->has_sharding()) {\n-        continue;\n-      }\n-      if (!common_sharding_inst) {\n-        common_sharding_inst = check_inst;\n-        continue;\n-      }\n-      TF_RET_CHECK(check_inst->sharding() == common_sharding_inst->sharding())\n-          << \"Inconsistent \" << parent->opcode()\n-          << \" sharding among instructions: \\n\"\n-          << common_sharding_inst->ToString() << \"\\n\"\n-          << check_inst->ToString();\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  // Verifies whether a given `instruction` is permitted to change the layout\n-  // memory space from `operand_memory_space` to `result_memory_space`.\n-  // Returns absl::OkStatus() if the instruction's layout changes are valid;\n-  // otherwise, returns an appropriate error status.\n-  static absl::Status HostOffloadInstructionCanChangeMemorySpace(\n-      const HloInstruction* instruction, const int64_t operand_memory_space,\n-      const int64_t result_memory_space) {\n-    TF_RET_CHECK(!(operand_memory_space == Layout::kGenericFastMemorySpace &&\n-                   result_memory_space != Layout::kGenericFastMemorySpace) ||\n-                 (operand_memory_space != Layout::kGenericFastMemorySpace &&\n-                  result_memory_space == Layout::kGenericFastMemorySpace))\n-        << \"Instruction shouldn't change layout memory space between generic \"\n-           \"fast memory space and others for instruction: \"\n-        << instruction->ToString();\n-\n-    if (instruction->opcode() == HloOpcode::kDynamicSlice) {\n-      TF_RET_CHECK(!(operand_memory_space == Layout::kDefaultMemorySpace &&\n-                     result_memory_space == Layout::kHostMemorySpace))\n-          << \"DynamicSlice instruction shouldn't change layout memory \"\n-          << \"space from device to host: \" << instruction->ToString();\n-    } else if (instruction->opcode() == HloOpcode::kDynamicUpdateSlice) {\n-      TF_RET_CHECK(!(operand_memory_space == Layout::kHostMemorySpace &&\n-                     result_memory_space == Layout::kDefaultMemorySpace))\n-          << \"DynamicUpdateSlice instruction shouldn't change layout \"\n-          << \"memory space from host to device: \" << instruction->ToString();\n-    } else if (instruction->opcode() != HloOpcode::kCopy) {\n-      return absl::InvalidArgumentError(\n-          absl::StrCat(\"Instruction shouldn't change layout memory space: \",\n-                       instruction->ToString()));\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  // Returns an error status if an instruction or any operand contains host\n-  // memory space.\n-  static absl::Status VerifyNoHostMemorySpace(\n-      const HloInstruction* instruction) {\n-    return ShapeUtil::ForEachSubshapeWithStatus(\n-        instruction->shape(),\n-        [&](const Shape& subshape, const ShapeIndex& index) -> absl::Status {\n-          if (subshape.has_layout()) {\n-            const Layout& result_layout = subshape.layout();\n-            if (result_layout.memory_space() == Layout::kHostMemorySpace) {\n-              return absl::InternalError(absl::StrCat(\n-                  \"Instruction shouldn't have the layout of host memory \"\n-                  \"space: \",\n-                  instruction->ToString()));\n-            }\n-          }\n-          return absl::OkStatus();\n-        });\n-  }\n-\n-  absl::flat_hash_map<std::string, const HloInstruction*> instructions_by_name_;\n-  const HloVerifierOpts& opts_;\n-  std::optional<int64_t> num_devices_;\n-};\n-\n bool IsCollectivesGroupComputation(HloComputation* computation) {\n   auto maybe_caller = computation->GetUniqueCaller(HloOpcode::kAsyncStart);\n   if (!maybe_caller.has_value()) {\n@@ -3829,6 +3476,334 @@ absl::Status VerifyBuffers(const HloModule& module, bool layout_sentitive) {\n \n }  // namespace\n \n+absl::Status InstructionVerifier::DefaultAction(HloInstruction*) {\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleFusion(HloInstruction* fusion) {\n+  TF_RETURN_IF_ERROR(CheckCallableInstructionThreadName(fusion));\n+  return CheckFusionInstruction(fusion);\n+}\n+\n+absl::Status InstructionVerifier::HandleBroadcast(HloInstruction* broadcast) {\n+  // If you see this failure then someone has confused the difference\n+  // between the HLO broadcast op, and the UserComputation broadcast\n+  // op. See https://groups.google.com/forum/#!topic/xla-dev/9LqijHmTt_I\n+  // or ComputationLowerer::Visit()\n+  TF_RET_CHECK(broadcast->dimensions().size() ==\n+               broadcast->operand(0)->shape().dimensions().size())\n+      << \"Broadcast HLO (\" << broadcast->ToShortString()\n+      << \") has invalid number of dimensions: \"\n+      << broadcast->dimensions().size()\n+      << \" != \" << broadcast->operand(0)->shape().dimensions().size();\n+  if (opts_.verify_broadcast_dimensions_order) {\n+    TF_RET_CHECK(absl::c_is_sorted(broadcast->dimensions()))\n+        << \"Broadcast dimensions should be ordered, got: \"\n+        << broadcast->ToString();\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleBitcastConvert(HloInstruction* c) {\n+  // Shape verifier will check all we need.\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleWhile(HloInstruction* xla_while) {\n+  auto* while_cond = xla_while->while_condition();\n+  auto* while_body = xla_while->while_body();\n+  if (while_cond->num_parameters() != 1) {\n+    return FailedPrecondition(\n+        \"While condition must have exactly 1 parameter; had %d : %s\",\n+        while_cond->num_parameters(), while_cond->ToString());\n+  }\n+  if (while_body->num_parameters() != 1) {\n+    return FailedPrecondition(\n+        \"While body must have exactly 1 parameter; had %d : %s\",\n+        while_body->num_parameters(), while_body->ToString());\n+  }\n+  if (xla_while->operand_count() != 1) {\n+    return FailedPrecondition(\n+        \"While loop must have exactly one operand; had %d : %s\",\n+        xla_while->operand_count(), xla_while->ToString());\n+  }\n+  // Allow kWhile to contain computations on separate thread.\n+  TF_RETURN_IF_ERROR(CheckCallableInstructionThreadName(xla_while));\n+\n+  // Verify consistency of sharding of while instructions and related\n+  // instructions (parameters, root) in its called computations.\n+  TF_RETURN_IF_ERROR(VerifyConsistentSharding(\n+      xla_while, {xla_while, xla_while->while_body()->root_instruction(),\n+                  xla_while->while_body()->parameter_instruction(0),\n+                  xla_while->while_condition()->parameter_instruction(0)}));\n+\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleCall(HloInstruction* call) {\n+  if (opts_.verify_call_nested_computation_thread_name) {\n+    return CheckCallableInstructionThreadName(call);\n+  }\n+\n+  // As opposed to other callable instructions, nothing respects input/output\n+  // aliasing for call instructions, so make sure it's not set.\n+  const HloCallableInstruction* callable =\n+      DynCast<const HloCallableInstruction>(call);\n+  TF_RET_CHECK(callable != nullptr);\n+  TF_RET_CHECK(callable->output_to_operand_aliasing().empty())\n+      << \"Call instruction \" << call->ToString()\n+      << \" may not have an output-to-operand aliasing set.\";\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleConditional(\n+    HloInstruction* conditional) {\n+  const std::vector<HloComputation*> branch_computations =\n+      conditional->branch_computations();\n+  std::vector<const HloInstruction*> sharding_check_instructions;\n+  sharding_check_instructions.reserve(branch_computations.size() + 1);\n+  sharding_check_instructions.push_back(conditional);\n+\n+  for (const HloComputation* branch_computation : branch_computations) {\n+    if (branch_computation->num_parameters() != 1) {\n+      return FailedPrecondition(\n+          \"Branch computation %s of %s must have 1 parameter instead of %d\",\n+          branch_computation->name(), conditional->ToString(),\n+          branch_computation->num_parameters());\n+    }\n+    sharding_check_instructions.push_back(\n+        branch_computation->root_instruction());\n+  }\n+  // Allow kConditional to contain computations on separate thread.\n+  TF_RETURN_IF_ERROR(CheckCallableInstructionThreadName(conditional));\n+\n+  // Verify consistency of sharding of conditional instructions and roots of\n+  // its branches.\n+  TF_RETURN_IF_ERROR(\n+      VerifyConsistentSharding(conditional, sharding_check_instructions));\n+\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleElementwiseUnary(\n+    HloInstruction* instruction) {\n+  TF_RETURN_IF_ERROR(CheckUnaryOpWithResultAccuracy(instruction));\n+  return CheckElementwiseInstruction(instruction);\n+}\n+\n+absl::Status InstructionVerifier::HandleElementwiseBinary(\n+    HloInstruction* instruction) {\n+  return CheckElementwiseInstruction(instruction);\n+}\n+\n+absl::Status InstructionVerifier::HandleGetTupleElement(HloInstruction* gte) {\n+  TF_RET_CHECK(gte->operand(0)->shape().IsTuple());\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleTranspose(HloInstruction* transpose) {\n+  const Shape& shape = transpose->shape();\n+  const HloInstruction* operand = transpose->operand(0);\n+  TF_RET_CHECK(shape.dimensions().size() == transpose->dimensions().size());\n+  TF_RET_CHECK(shape.dimensions().size() ==\n+               transpose->operand(0)->shape().dimensions().size());\n+  TF_RET_CHECK(std::equal(\n+      shape.dimensions().begin(), shape.dimensions().end(),\n+      Permute(operand->shape().dimensions(), transpose->dimensions()).begin()))\n+      << \"shape: \" << shape << \", operand->shape(): \" << shape\n+      << \", dimensions: {\" << absl::StrJoin(transpose->dimensions(), \", \")\n+      << \"}\";\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleAllReduce(HloInstruction* crs) {\n+  if (crs->channel_id().has_value()) {\n+    TF_RET_CHECK(crs->channel_id().value() > 0)\n+        << \"All reduce channel id must be greater than 0 for \"\n+        << crs->ToShortString();\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleReshape(HloInstruction* hlo) {\n+  if (opts_.verify_reshape_is_bitcast && !hlo->IsFused()) {\n+    TF_RET_CHECK(\n+        ShapeUtil::ReshapeIsBitcast(hlo->operand(0)->shape(), hlo->shape()))\n+        << \"Reshape should be a physical bitcast, got: \" << hlo->ToString();\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleCustomCall(HloInstruction* hlo) {\n+  if (opts_.verify_call_nested_computation_thread_name) {\n+    // Allow kCustomCall to contain computations on separate thread.\n+    return CheckCallableInstructionThreadName(hlo);\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HandleScatter(HloInstruction* scatter) {\n+  int64_t rank = scatter->operand(0)->shape().dimensions().size();\n+  for (int64_t operand_dim :\n+       scatter->scatter_dimension_numbers().scatter_dims_to_operand_dims()) {\n+    if (operand_dim > rank) {\n+      return absl::OutOfRangeError(absl::StrCat(\n+          \"The provided scatter_dims_to_operand_dim was out of range.\",\n+          \" (operand_dim: \", operand_dim, \", rank: \", rank, \")\"));\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::Preprocess(HloInstruction* instruction) {\n+  auto [it, inserted] =\n+      instructions_by_name_.emplace(instruction->name(), instruction);\n+  TF_RET_CHECK(inserted) << \"HLO has name that is not unique within module:\\n\"\n+                         << instruction->ToString()\n+                         << \" in computation: \" << instruction->parent()->name()\n+                         << \"\\nPrevious HLO with same name:\\n\"\n+                         << it->second->ToString()\n+                         << \" in computation: \" << it->second->parent()->name();\n+\n+  if (instruction->has_sharding()) {\n+    absl::Status status =\n+        instruction->sharding().Validate(instruction->shape(), num_devices_);\n+    if (!status.ok()) {\n+      return absl::Status(\n+          status.code(),\n+          absl::StrCat(\"Invalid sharding for instruction: \",\n+                       instruction->ToString(), \": \", status.message()));\n+    }\n+  }\n+\n+  if (opts_.verify_call_nested_computation_thread_name &&\n+      instruction->has_to_apply() &&\n+      xla::GetInstructionCallContext(instruction->opcode()) !=\n+          xla::CallContext::kEmbedded &&\n+      instruction->to_apply()->execution_thread() !=\n+          instruction->parent()->execution_thread()) {\n+    return Internal(\n+        \"Non-Embedded context callable instruction %s to_apply computation \"\n+        \"execution thread does not match (%s vs %s)\",\n+        instruction->name(), instruction->to_apply()->execution_thread(),\n+        instruction->parent()->execution_thread());\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::Postprocess(HloInstruction* instruction) {\n+  if (opts_.verify_no_host_memory_space) {\n+    TF_RETURN_IF_ERROR(VerifyNoHostMemorySpace(instruction));\n+  }\n+  if (!opts_.InstructionCanChangeLayout(instruction) &&\n+      instruction->shape().IsArray() && instruction->shape().has_layout()) {\n+    const Shape& result_shape = instruction->shape();\n+    const Layout& result_layout = result_shape.layout();\n+    for (HloInstruction* operand : instruction->operands()) {\n+      const Shape& operand_shape = operand->shape();\n+      if (operand_shape.IsArray() &&\n+          operand_shape.dimensions().size() ==\n+              result_shape.dimensions().size() &&\n+          operand_shape.has_layout()) {\n+        const Layout& operand_layout = operand_shape.layout();\n+        Layout::Equal equal_predicate =\n+            Layout::Equal().IgnoreTiles().IgnoreMemorySpace();\n+        if (instruction->opcode() == HloOpcode::kConvert ||\n+            instruction->opcode() == HloOpcode::kCompare ||\n+            instruction->opcode() == HloOpcode::kIsFinite ||\n+            (instruction->opcode() == HloOpcode::kSelect &&\n+             operand_shape.element_type() == PRED) ||\n+            instruction->opcode() == HloOpcode::kScatter) {\n+          // Some instructions can change element_size_in_bits\n+          // Select instructions ignore element_size_in_bits for predicate\n+          equal_predicate.IgnoreElementSize();\n+        } else if (instruction->opcode() == HloOpcode::kDynamicSlice ||\n+                   instruction->opcode() == HloOpcode::kDynamicUpdateSlice ||\n+                   instruction->opcode() == HloOpcode::kCopy) {\n+          TF_RETURN_IF_ERROR(HostOffloadInstructionCanChangeMemorySpace(\n+              instruction, operand_layout.memory_space(),\n+              result_layout.memory_space()));\n+          equal_predicate.IgnoreMemorySpace();\n+        }\n+        TF_RET_CHECK(equal_predicate(result_layout, operand_layout))\n+            << \"Instruction shouldn't change layouts \"\n+            << instruction->ToString() << \" From \" << result_shape << \" To \"\n+            << operand_shape;\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::VerifyConsistentSharding(\n+    const HloInstruction* parent,\n+    absl::Span<const HloInstruction* const> instructions) {\n+  const HloInstruction* common_sharding_inst = nullptr;\n+  for (const HloInstruction* check_inst : instructions) {\n+    if (!check_inst->has_sharding()) {\n+      continue;\n+    }\n+    if (!common_sharding_inst) {\n+      common_sharding_inst = check_inst;\n+      continue;\n+    }\n+    TF_RET_CHECK(check_inst->sharding() == common_sharding_inst->sharding())\n+        << \"Inconsistent \" << parent->opcode()\n+        << \" sharding among instructions: \\n\"\n+        << common_sharding_inst->ToString() << \"\\n\"\n+        << check_inst->ToString();\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::HostOffloadInstructionCanChangeMemorySpace(\n+    const HloInstruction* instruction, const int64_t operand_memory_space,\n+    const int64_t result_memory_space) {\n+  TF_RET_CHECK(!(operand_memory_space == Layout::kGenericFastMemorySpace &&\n+                 result_memory_space != Layout::kGenericFastMemorySpace) ||\n+               (operand_memory_space != Layout::kGenericFastMemorySpace &&\n+                result_memory_space == Layout::kGenericFastMemorySpace))\n+      << \"Instruction shouldn't change layout memory space between generic \"\n+         \"fast memory space and others for instruction: \"\n+      << instruction->ToString();\n+\n+  if (instruction->opcode() == HloOpcode::kDynamicSlice) {\n+    TF_RET_CHECK(!(operand_memory_space == Layout::kDefaultMemorySpace &&\n+                   result_memory_space == Layout::kHostMemorySpace))\n+        << \"DynamicSlice instruction shouldn't change layout memory \"\n+        << \"space from device to host: \" << instruction->ToString();\n+  } else if (instruction->opcode() == HloOpcode::kDynamicUpdateSlice) {\n+    TF_RET_CHECK(!(operand_memory_space == Layout::kHostMemorySpace &&\n+                   result_memory_space == Layout::kDefaultMemorySpace))\n+        << \"DynamicUpdateSlice instruction shouldn't change layout \"\n+        << \"memory space from host to device: \" << instruction->ToString();\n+  } else if (instruction->opcode() != HloOpcode::kCopy) {\n+    return absl::InvalidArgumentError(\n+        absl::StrCat(\"Instruction shouldn't change layout memory space: \",\n+                     instruction->ToString()));\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status InstructionVerifier::VerifyNoHostMemorySpace(\n+    const HloInstruction* instruction) {\n+  return ShapeUtil::ForEachSubshapeWithStatus(\n+      instruction->shape(),\n+      [&](const Shape& subshape, const ShapeIndex& index) -> absl::Status {\n+        if (subshape.has_layout()) {\n+          const Layout& result_layout = subshape.layout();\n+          if (result_layout.memory_space() == Layout::kHostMemorySpace) {\n+            return absl::InternalError(absl::StrCat(\n+                \"Instruction shouldn't have the layout of host memory \"\n+                \"space: \",\n+                instruction->ToString()));\n+          }\n+        }\n+        return absl::OkStatus();\n+      });\n+}\n+\n absl::StatusOr<bool> HloVerifier::Run(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {"
        },
        {
            "sha": "4e58a97ff7bfccb8d18bed8d6223f763cb79b7a7",
            "filename": "third_party/xla/xla/service/hlo_verifier.h",
            "status": "modified",
            "additions": 57,
            "deletions": 0,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/35fc4aa13e4bc5cacf8122a892af54799eaa1f5e/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/35fc4aa13e4bc5cacf8122a892af54799eaa1f5e/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.h?ref=35fc4aa13e4bc5cacf8122a892af54799eaa1f5e",
            "patch": "@@ -359,6 +359,63 @@ class ShapeVerifier : public DfsHloVisitor {\n   const HloVerifierOpts& opts_;\n };\n \n+// Visitor which verifies various fields on the HLO instruction. This class does\n+// not check result shape as that is checked in the ShapeVerifier.\n+class InstructionVerifier : public DfsHloVisitorWithDefault {\n+ public:\n+  explicit InstructionVerifier(const HloModule* module,\n+                               const HloVerifierOpts& opts)\n+      : opts_(opts) {\n+    // TODO(b/258285553): Eliminate this check when all paths that enable SPMD\n+    // partitioning also set the num_partitions correctly.\n+    const int64_t num_partitions = module->config().num_partitions();\n+    if (module->config().use_spmd_partitioning() &&\n+        opts.verify_sharding_device_numbers && num_partitions > 1) {\n+      num_devices_ = module->config().num_partitions();\n+    }\n+  }\n+\n+  absl::Status DefaultAction(HloInstruction*) override;\n+  absl::Status HandleFusion(HloInstruction* fusion) override;\n+  absl::Status HandleBroadcast(HloInstruction* broadcast) override;\n+  absl::Status HandleBitcastConvert(HloInstruction* c) override;\n+  absl::Status HandleWhile(HloInstruction* xla_while) override;\n+  absl::Status HandleCall(HloInstruction* call) override;\n+  absl::Status HandleConditional(HloInstruction* conditional) override;\n+  absl::Status HandleElementwiseUnary(HloInstruction* instruction) override;\n+  absl::Status HandleElementwiseBinary(HloInstruction* instruction) override;\n+  absl::Status HandleGetTupleElement(HloInstruction* gte) override;\n+  absl::Status HandleTranspose(HloInstruction* transpose) override;\n+  absl::Status HandleAllReduce(HloInstruction* crs) override;\n+  absl::Status HandleReshape(HloInstruction* hlo) override;\n+  absl::Status HandleCustomCall(HloInstruction* hlo) override;\n+  absl::Status HandleScatter(HloInstruction* scatter) override;\n+  absl::Status Preprocess(HloInstruction* instruction) override;\n+  absl::Status Postprocess(HloInstruction* instruction) override;\n+\n+ private:\n+  static absl::Status VerifyConsistentSharding(\n+      const HloInstruction* parent,\n+      absl::Span<const HloInstruction* const> instructions);\n+\n+  // Verifies whether a given `instruction` is permitted to change the layout\n+  // memory space from `operand_memory_space` to `result_memory_space`.\n+  // Returns absl::OkStatus() if the instruction's layout changes are valid;\n+  // otherwise, returns an appropriate error status.\n+  static absl::Status HostOffloadInstructionCanChangeMemorySpace(\n+      const HloInstruction* instruction, int64_t operand_memory_space,\n+      int64_t result_memory_space);\n+\n+  // Returns an error status if an instruction or any operand contains host\n+  // memory space.\n+  static absl::Status VerifyNoHostMemorySpace(\n+      const HloInstruction* instruction);\n+\n+  absl::flat_hash_map<std::string, const HloInstruction*> instructions_by_name_;\n+  const HloVerifierOpts& opts_;\n+  std::optional<int64_t> num_devices_;\n+};\n+\n // An interface used to encapsulate target-specific verification quirks.\n class TargetVerifierMetadata {\n  public:"
        }
    ],
    "stats": {
        "total": 738,
        "additions": 385,
        "deletions": 353
    }
}