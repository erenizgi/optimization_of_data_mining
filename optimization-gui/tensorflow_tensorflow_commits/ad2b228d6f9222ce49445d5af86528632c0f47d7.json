{
    "author": "vksnk",
    "message": "[XLA:CPU] Add initial support of grouped convolutions with YNNPACK enabled.\n\nThere are currently a few limitations:\n* Only supports F32xF32->F32 and BF16xBF16->F32. S8xS8->S32 should work too, but there seem to be some rewrite in the middle which changes the op from supported convolution to unsupported -- will track it down separetely.\n\n* Only stride=1 cases are supported until the limitation of ynn_split_dim is fixed.\n\nPiperOrigin-RevId: 845977975",
    "sha": "ad2b228d6f9222ce49445d5af86528632c0f47d7",
    "files": [
        {
            "sha": "fb5f1abbb33e3b8ce4e041c49ac9e012070de2cf",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.cc",
            "status": "modified",
            "additions": 107,
            "deletions": 14,
            "changes": 121,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ad2b228d6f9222ce49445d5af86528632c0f47d7/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ad2b228d6f9222ce49445d5af86528632c0f47d7/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc?ref=ad2b228d6f9222ce49445d5af86528632c0f47d7",
            "patch": "@@ -432,19 +432,87 @@ static ynn_status DefineBatchMatrixMultiply(ynn_subgraph_t subgraph,\n }\n \n static ynn_status DefineConvolution(\n-    ynn_subgraph_t subgraph, ynn_type input1_id_type, uint32_t input1_id,\n-    uint32_t input2_id, uint32_t output_id,\n-    const std::vector<int32_t>& stencil_axes,\n-    const std::vector<int32_t> new_axes,\n+    ynn_subgraph_t subgraph, ynn_type input1_id_type, ynn_type output_id_type,\n+    uint32_t input1_id, uint32_t input2_id, uint32_t output_id,\n+    const std::vector<size_t>& filter_dims, const std::vector<size_t>& out_dims,\n+    size_t feature_group_count, size_t input_channels,\n+    size_t kernel_output_channels, const std::vector<int32_t>& stencil_axes,\n+    const std::vector<int32_t>& new_axes,\n     const std::vector<size_t>& stencil_dims,\n     const std::vector<size_t>& stencil_strides,\n     const std::vector<size_t>& stencil_dilations,\n     const std::vector<int64_t>& padding_lows,\n     const std::vector<int64_t>& padding_highs) {\n-  uint32_t stencil_id = YNN_INVALID_VALUE_ID;\n-\n   ynn_status status;\n \n+  // Make a copy in case we need to shift these for grouped convolution.\n+  std::vector<int32_t> new_axes_shifted = new_axes;\n+\n+  // We will need to create an intermediate buffer for the output if it's\n+  // grouped convolution.\n+  uint32_t output_unfused_id =\n+      feature_group_count != 1 ? YNN_INVALID_VALUE_ID : output_id;\n+\n+  if (feature_group_count != 1) {\n+    uint32_t split_id = YNN_INVALID_VALUE_ID;\n+\n+    // [n, h, w, ci] -> [n, h, w, g, 1, ci/g].\n+    size_t input_split[] = {feature_group_count, 1,\n+                            input_channels / feature_group_count};\n+    status =\n+        ynn_define_split_dim(subgraph, /*axis=*/-1, /*num_splits=*/3,\n+                             input_split, input1_id, &split_id, /*flags=*/0);\n+    if (status != ynn_status_success) {\n+      return status;\n+    }\n+    input1_id = split_id;\n+    split_id = YNN_INVALID_VALUE_ID;\n+    CHECK_EQ(filter_dims.size(), 4);\n+    // [kh, kw, ci/g, co] -> [kh, kw, ci/g, g, co/g].\n+    size_t filter_split[] = {feature_group_count,\n+                             kernel_output_channels / feature_group_count};\n+    status =\n+        ynn_define_split_dim(subgraph, /*axis=*/-1, /*num_splits=*/2,\n+                             filter_split, input2_id, &split_id, /*flags=*/0);\n+    if (status != ynn_status_success) {\n+      return status;\n+    }\n+    input2_id = split_id;\n+\n+    uint32_t transposed_filter_id = YNN_INVALID_VALUE_ID;\n+    // [kh, kw, ci/g, g, co/g] -> [g, kh, kw, ci/g, co/g]\n+    int32_t swap_co_ci[5] = {3, 0, 1, 2, 4};\n+    status =\n+        ynn_define_static_transpose(subgraph, /*rank=*/5, swap_co_ci, input2_id,\n+                                    &transposed_filter_id, /*flags=*/0);\n+\n+    if (status != ynn_status_success) {\n+      return status;\n+    }\n+    input2_id = transposed_filter_id;\n+\n+    // Create intermediate output buffer.\n+    std::vector<size_t> unfused_dims(out_dims.begin(), out_dims.end() - 1);\n+    unfused_dims.push_back(feature_group_count);\n+    unfused_dims.push_back(1);\n+    unfused_dims.push_back(kernel_output_channels / feature_group_count);\n+    status = ynn_define_tensor_value(subgraph, output_id_type,\n+                                     /*rank=*/out_dims.size() + 2,\n+                                     /*dims=*/unfused_dims.data(),\n+                                     /*data=*/nullptr,\n+                                     /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+                                     /*scale_id=*/YNN_INVALID_VALUE_ID,\n+                                     /*flags=*/0, &output_unfused_id);\n+    if (status != ynn_status_success) {\n+      return status;\n+    }\n+\n+    // Shift new stencil axes by two.\n+    for (int i = 0; i < new_axes_shifted.size(); ++i) {\n+      new_axes_shifted[i] += 2;\n+    }\n+  }\n+\n   // If any of paddings is not zero, define a padding value and pad the input.\n   if (absl::c_any_of(padding_lows, [](int32_t i) { return i != 0; }) ||\n       absl::c_any_of(padding_highs, [](int32_t i) { return i != 0; })) {\n@@ -475,18 +543,38 @@ static ynn_status DefineConvolution(\n     padding_id = YNN_INVALID_VALUE_ID;\n   }\n \n+  uint32_t stencil_id = YNN_INVALID_VALUE_ID;\n   // Make a stenciled view of the input [n, h, w, ci] -> [n, h, w, kh, kw, ci].\n   status = ynn_define_stencil_copy(\n       subgraph, /*num_stencils=*/stencil_dims.size(), stencil_axes.data(),\n-      new_axes.data(), stencil_dims.data(), stencil_strides.data(),\n+      new_axes_shifted.data(), stencil_dims.data(), stencil_strides.data(),\n       stencil_dilations.data(), input1_id, YNN_INVALID_VALUE_ID, &stencil_id,\n       /*flags=*/0);\n   if (status != ynn_status_success) {\n     return status;\n   }\n-  return ynn_define_dot(subgraph, /*num_k_dims=*/stencil_dims.size() + 1,\n-                        stencil_id, input2_id, YNN_INVALID_VALUE_ID, &output_id,\n-                        /*flags=*/0);\n+\n+  status = ynn_define_dot(subgraph, /*num_k_dims=*/stencil_dims.size() + 1,\n+                          stencil_id, input2_id, YNN_INVALID_VALUE_ID,\n+                          &output_unfused_id,\n+                          /*flags=*/0);\n+\n+  if (status != ynn_status_success) {\n+    return status;\n+  }\n+\n+  if (feature_group_count > 1) {\n+    // The output of the grouped convolution is [n, h, w, g, 1, co/g], so we\n+    // need to fuse three of the innermost dimensions.\n+    status = ynn_define_fuse_dim(subgraph, /*axis=*/-3, /*axes_count=*/3,\n+                                 output_unfused_id, &output_id,\n+                                 /*flags=*/0);\n+    if (status != ynn_status_success) {\n+      return status;\n+    }\n+  }\n+\n+  return status;\n }\n \n static absl::StatusOr<YnnSubgraph> EmitYnnDotSubgraph(\n@@ -644,10 +732,15 @@ static absl::StatusOr<YnnSubgraph> EmitYnnConvolutionSubgraph(\n \n   std::iota(new_axes.begin(), new_axes.end(), lhs_dims.size() - 1);\n \n-  YNN_RETURN_IF_ERROR(\n-      DefineConvolution(subgraph.get(), ynn_lhs_type, lhs_id, rhs_id, out_id,\n-                        stencil_axes, new_axes, stencil_dims, stencil_strides,\n-                        stencil_dilations, padding_lows, padding_highs));\n+  YNN_RETURN_IF_ERROR(DefineConvolution(\n+      subgraph.get(), ynn_lhs_type, ynn_out_type, lhs_id, rhs_id, out_id,\n+      rhs_dims, out_dims, conv->feature_group_count(),\n+      conv->operand(0)->shape().dimensions(\n+          conv_dimensions.input_feature_dimension()),\n+      conv->operand(1)->shape().dimensions(\n+          conv_dimensions.kernel_output_feature_dimension()),\n+      stencil_axes, new_axes, stencil_dims, stencil_strides, stencil_dilations,\n+      padding_lows, padding_highs));\n \n   ynn_status status = ynn_optimize_subgraph(\n       subgraph.get(), /*threadpool=*/nullptr, /*flags=*/0);"
        },
        {
            "sha": "1c7f427934a6222bd59b0b705dcdcff3119fa406",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 11,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ad2b228d6f9222ce49445d5af86528632c0f47d7/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ad2b228d6f9222ce49445d5af86528632c0f47d7/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc?ref=ad2b228d6f9222ce49445d5af86528632c0f47d7",
            "patch": "@@ -296,20 +296,42 @@ bool IsConvolutionOpSupportedByYnn(const HloInstruction* instr) {\n   CHECK_EQ(instr->opcode(), HloOpcode::kConvolution);\n   const HloConvolutionInstruction* conv =\n       Cast<HloConvolutionInstruction>(instr);\n+\n+  ConvolutionDimensionNumbers conv_dimensions =\n+      conv->convolution_dimension_numbers();\n+  Window window = conv->window();\n+\n+  if (conv->batch_group_count() != 1) {\n+    return false;\n+  }\n+\n+  // Only support 2D convolution.\n+  if (window.dimensions_size() != 2) {\n+    return false;\n+  }\n+\n   // Stores tuple of allowed (input, output) dtypes.\n   static const absl::NoDestructor<absl::flat_hash_set<\n       std::tuple<PrimitiveType, PrimitiveType, PrimitiveType>>>\n-      kAllowedTypes({{F32, F32, F32}, {BF16, BF16, F32}, {S8, S8, S32}});\n+      kAllowedTypesNonGrouped(\n+          {{F32, F32, F32}, {BF16, BF16, F32}, {S8, S8, S32}});\n+\n+  static const absl::NoDestructor<absl::flat_hash_set<\n+      std::tuple<PrimitiveType, PrimitiveType, PrimitiveType>>>\n+      kAllowedTypesGrouped({{F32, F32, F32}, {BF16, BF16, F32}});\n \n   PrimitiveType lhs_dtype = conv->operand(0)->shape().element_type();\n   PrimitiveType rhs_dtype = conv->operand(1)->shape().element_type();\n   PrimitiveType out_dtype = conv->shape().element_type();\n-  if (!kAllowedTypes->contains({lhs_dtype, rhs_dtype, out_dtype})) {\n+  if (conv->feature_group_count() == 1 &&\n+      !kAllowedTypesNonGrouped->contains({lhs_dtype, rhs_dtype, out_dtype})) {\n     return false;\n   }\n \n-  ConvolutionDimensionNumbers conv_dimensions =\n-      conv->convolution_dimension_numbers();\n+  if (conv->feature_group_count() > 1 &&\n+      !kAllowedTypesGrouped->contains({lhs_dtype, rhs_dtype, out_dtype})) {\n+    return false;\n+  }\n \n   // Make sure that this layout is supported.\n   if (conv_dimensions.input_feature_dimension() != 3 ||\n@@ -337,13 +359,6 @@ bool IsConvolutionOpSupportedByYnn(const HloInstruction* instr) {\n     return false;\n   }\n \n-  Window window = conv->window();\n-\n-  // Only support 2D convolution.\n-  if (window.dimensions_size() != 2) {\n-    return false;\n-  }\n-\n   // No dilation for now.\n   if ((window.dimensions(0).window_dilation() != 1) ||\n       (window.dimensions(1).window_dilation() != 1) ||"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 133,
        "deletions": 25
    }
}