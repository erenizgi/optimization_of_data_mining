{
    "author": "tensorflower-gardener",
    "message": "Add `xla_cpu_enable_platform_dependent_math` flag\n\nThis new flag is default true, which matches existing behavior. It enables us to add new behaviors if this is changed to false, where we avoid using platform dependent optimizations and logic, in an attempt to produce consistent results across CPUs.\n\nThis mirrors the backend flag `disable_platform_dependent_math`. This flag is currently always set, and I don't think we should change that in this change, as it would be a breaking change for clients that haven't had a chance to set this new front end flag yet.\n\nThis is currently hooked up to YNNPACK in this change, however, it is not currently tested, because:\n- YNNPACK fp32 and bf16 dots are currently disabled (in part due to lacking this flag).\n- XLA dots currently are already not deterministic (unless using backend specific environment variables).\n- Therefore, this flag has no functional changes that can be observed with any available options.\n\nI think that we should add the flag, allow clients to set the flag, then we can enable YNNPACK, and then we can add some test coverage of this behavior. We could add some more flags to control whether fp32 and bf16 is enabled in YNNPACK, but I don't think we should do that.\n\nFor now, I've manually tested this with fp32 and bf16 support in YNNPACK hacked on, and verified that this flag does cause YNNPACK to avoid using `tile_k > 1` kernels (the only behavior currently affected by this flag).\n\nPiperOrigin-RevId: 828186512",
    "sha": "539a42d194fa52628c12c71722b9759e8672b06c",
    "files": [
        {
            "sha": "774b9f35d6d57a64fab944318f3f9c25fcb44218",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=539a42d194fa52628c12c71722b9759e8672b06c",
            "patch": "@@ -232,6 +232,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n         \"//xla/backends/cpu/runtime:dot_lib\",\n         \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n         \"//xla/hlo/ir:hlo\","
        },
        {
            "sha": "357d139b8ec038ce7582291f77a0d54049ba11c9",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc?ref=539a42d194fa52628c12c71722b9759e8672b06c",
            "patch": "@@ -259,7 +259,8 @@ static absl::StatusOr<YnnSubgraph> EmitYnnSubgraph(\n       YnnSubgraph subgraph, CreateYnnSubgraph([&](ynn_subgraph_t* subgraph) {\n         return ynn_create_subgraph(\n             /*external_value_ids=*/computation->num_parameters() + 1,\n-            /*flags=*/0, subgraph);\n+            YnnFlags(computation->parent()->config().debug_options()),\n+            subgraph);\n       }));\n \n   // Traverse fused computation in post-order and define YNNPACK operations\n@@ -373,12 +374,12 @@ static absl::StatusOr<YnnSubgraph> EmitYnnDotSubgraph(\n     std::vector<std::unique_ptr<Literal>>& literals,\n     absl::Span<const se::DeviceMemoryBase> arguments_buffers,\n     bool capture_rhs) {\n-  TF_ASSIGN_OR_RETURN(YnnSubgraph subgraph,\n-                      CreateYnnSubgraph([&](ynn_subgraph_t* subgraph) {\n-                        return ynn_create_subgraph(\n-                            /*external_value_ids=*/3,\n-                            /*flags=*/0, subgraph);\n-                      }));\n+  TF_ASSIGN_OR_RETURN(\n+      YnnSubgraph subgraph, CreateYnnSubgraph([&](ynn_subgraph_t* subgraph) {\n+        return ynn_create_subgraph(\n+            /*external_value_ids=*/3,\n+            YnnFlags(dot->GetModule()->config().debug_options()), subgraph);\n+      }));\n \n   uint32_t lhs_id = 0;\n   uint32_t rhs_id = 1;"
        },
        {
            "sha": "1ab28deff62fc07613ea86b71ff0bd7578a29ba7",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc?ref=539a42d194fa52628c12c71722b9759e8672b06c",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/backends/cpu/ynn_support.h\"\n \n #include <algorithm>\n+#include <cstdint>\n #include <tuple>\n \n #include \"ynnpack/include/ynnpack.h\"\n@@ -242,4 +243,12 @@ bool IsReduceOpSupportedByYnn(const HloInstruction* hlo) {\n                                                match::Parameter(1)));\n }\n \n+uint32_t YnnFlags(const DebugOptions& debug_options) {\n+  uint32_t flags = 0;\n+  if (!debug_options.xla_cpu_enable_platform_dependent_math()) {\n+    flags |= YNN_FLAG_CONSISTENT_ARITHMETIC;\n+  }\n+  return flags;\n+}\n+\n }  // namespace xla::cpu"
        },
        {
            "sha": "c30ec35e6fd9c59ec064f0297a4c7f6030a21029",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h?ref=539a42d194fa52628c12c71722b9759e8672b06c",
            "patch": "@@ -16,13 +16,16 @@ limitations under the License.\n #ifndef XLA_BACKENDS_CPU_YNN_SUPPORT_H_\n #define XLA_BACKENDS_CPU_YNN_SUPPORT_H_\n \n+#include <cstdint>\n+\n #include \"ynnpack/include/ynnpack.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/shape.h\"\n+#include \"xla/xla.pb.h\"\n \n namespace xla::cpu {\n \n@@ -65,6 +68,9 @@ absl::StatusOr<bool> IsDotSupportedByYnn(\n // Returns true if the reduce op is supported by YNNPACK.\n bool IsReduceOpSupportedByYnn(const HloInstruction* hlo);\n \n+// Convert XLA options to YNNPACK flags.\n+uint32_t YnnFlags(const DebugOptions& debug_options);\n+\n }  // namespace xla::cpu\n \n #endif  // XLA_BACKENDS_CPU_YNN_SUPPORT_H_"
        },
        {
            "sha": "592bc70d8b16b8c70b23eb966697792d40dcccab",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=539a42d194fa52628c12c71722b9759e8672b06c",
            "patch": "@@ -224,6 +224,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_cpu_emitter_verification_level(0);\n \n   opts.set_xla_cpu_enable_fast_math(false);\n+  opts.set_xla_cpu_enable_platform_dependent_math(true);\n   // Disable forms of fast math that have caused users problems in the past.\n   opts.set_xla_cpu_fast_math_honor_nans(true);\n   opts.set_xla_cpu_fast_math_honor_infs(true);\n@@ -929,6 +930,13 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       debug_options->xla_cpu_enable_fast_math(),\n       \"Enable unsafe fast-math optimizations in the CPU compiler; this may \"\n       \"produce faster code at the expense of some accuracy.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_cpu_enable_platform_dependent_math\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_cpu_enable_platform_dependent_math),\n+      debug_options->xla_cpu_enable_platform_dependent_math(),\n+      \"Enable platform dependent math in the CPU compiler; this may \"\n+      \"produce faster code at the expense of consistent results across CPUs.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_cpu_fast_math_honor_nans\",\n       bool_setter_for(&DebugOptions::set_xla_cpu_fast_math_honor_nans),"
        },
        {
            "sha": "5efb6a3dbc9ccdcbcf5bd69c1af69179eb841cd9",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/539a42d194fa52628c12c71722b9759e8672b06c/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=539a42d194fa52628c12c71722b9759e8672b06c",
            "patch": "@@ -221,6 +221,11 @@ message DebugOptions {\n   // below!\n   optional bool xla_cpu_enable_fast_min_max = 140;\n \n+  // When xla_enable_platform_dependent_math is true, we allow operations to use\n+  // calculations that produce different results depending on the current\n+  // machine.\n+  optional bool xla_cpu_enable_platform_dependent_math = 425;\n+\n   // Call oneDNN custom call thunks in the CPU backend\n   optional bool xla_cpu_experimental_onednn_custom_call = 412;\n \n@@ -1393,7 +1398,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 425\n+  // Next id: 426\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 46,
        "additions": 38,
        "deletions": 8
    }
}