{
    "author": "akuegel",
    "message": "[XLA:GPU] Consider multi-output fusions supported by Triton codegen.\n\nCurrently we would fail when trying to check whether the element type of the\nroot tuple is supported. We should not even access the element type on a tuple\nshape. Therefore we skip the root tuple.\n\nPiperOrigin-RevId: 820096050",
    "sha": "83f3904c5f029deaaed00c8a73803e9b0ebc1c43",
    "files": [
        {
            "sha": "c95d522e82fa93620b35456b86b40079c030e8ce",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83f3904c5f029deaaed00c8a73803e9b0ebc1c43/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83f3904c5f029deaaed00c8a73803e9b0ebc1c43/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=83f3904c5f029deaaed00c8a73803e9b0ebc1c43",
            "patch": "@@ -757,6 +757,14 @@ CodegenDecision IsTritonSupportedComputation(\n     const se::GpuComputeCapability& gpu_compute_capability) {\n   VLOG(3) << \"IsTritonSupportedComputation: \" << computation.ToString();\n   for (const auto* instruction : computation.instructions()) {\n+    // TODO(b/452478982): This check can be removed if we support Tuple ops\n+    // generally.\n+    if (instruction == computation.root_instruction() &&\n+        instruction->opcode() == HloOpcode::kTuple) {\n+      // While Tuple is not generally supported by Triton codegen, it is\n+      // supported for fusion roots.\n+      continue;\n+    }\n     if (CodegenDecision can_codegen =\n             IsTritonSupportedInstruction(*instruction, gpu_compute_capability);\n         !can_codegen) {"
        },
        {
            "sha": "23621636c13da3a4d693c6a66d092f6d05caa146",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/83f3904c5f029deaaed00c8a73803e9b0ebc1c43/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/83f3904c5f029deaaed00c8a73803e9b0ebc1c43/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=83f3904c5f029deaaed00c8a73803e9b0ebc1c43",
            "patch": "@@ -300,6 +300,20 @@ class TritonSupportTest : public TritonSupportTestBase {\n   }\n };\n \n+TEST_F(TritonSupportTest, IsTritonSupportedComputationSkipsRootTuple) {\n+  const std::string kHlo = R\"(\n+  HloModule m\n+  ENTRY main {\n+    parameter_0 = f32[10] parameter(0)\n+    abs = f32[10] abs(parameter_0)\n+    negate = f32[10] negate(abs)\n+    ROOT res = (f32[10], f32[10]) tuple(abs, negate)\n+  })\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHlo));\n+  EXPECT_TRUE(IsTritonSupportedComputation(\n+      *module->entry_computation(), se::CudaComputeCapability::Hopper()));\n+}\n+\n class TritonSupportTestWithTypeAndOpcodeAndDeviceParam\n     : public TritonSupportTest,\n       public ::testing::WithParamInterface<"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 22,
        "deletions": 0
    }
}