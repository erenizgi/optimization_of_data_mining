{
    "author": "junwhanahn",
    "message": "Add more IFRT array tests\n\nPiperOrigin-RevId: 800227849",
    "sha": "31e7c04d87e3c6ae94616799d4d73e4af4e0c462",
    "files": [
        {
            "sha": "31e0a8c616f32a5ae605d964e7a62ccbc9b4c699",
            "filename": "third_party/xla/xla/backends/cpu/nanort/ifrt_client.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc?ref=31e7c04d87e3c6ae94616799d4d73e4af4e0c462",
            "patch": "@@ -51,6 +51,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/layout.h\"\n+#include \"xla/layout_util.h\"\n #include \"xla/pjrt/mlir_to_hlo.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n@@ -412,7 +413,8 @@ class NanoArray final : public NanoValue<NanoArray, ifrt::Array> {\n   absl::StatusOr<std::shared_ptr<const PjRtLayout>> pjrt_layout()\n       const override {\n     TF_RETURN_IF_ERROR(ValidateNotDeleted());\n-    return std::make_shared<PjRtLayout>(Layout(shape().dims()));\n+    return std::make_shared<PjRtLayout>(\n+        LayoutUtil::MakeDescendingLayout(shape().dims().size()));\n   }\n \n   absl::StatusOr<std::vector<ifrt::ArrayRef>> DisassembleIntoSingleDeviceArrays(\n@@ -1489,7 +1491,8 @@ NanoIfrtClient::GetDefaultPjRtLayout(ifrt::DType dtype,\n                                      absl::Span<const int64_t> dims,\n                                      ifrt::Device* device,\n                                      ifrt::MemoryKind memory_kind) const {\n-  return std::make_shared<PjRtLayout>(Layout(dims));\n+  return std::make_shared<PjRtLayout>(\n+      LayoutUtil::MakeDescendingLayout(dims.size()));\n }\n \n NanoIfrtClient::NanoIfrtClient(int32_t num_devices,"
        },
        {
            "sha": "92fc31db0b1bc63bfe757664524d9c83a1a2440e",
            "filename": "third_party/xla/xla/backends/cpu/nanort/ifrt_client_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client_test.cc?ref=31e7c04d87e3c6ae94616799d4d73e4af4e0c462",
            "patch": "@@ -321,6 +321,10 @@ int main(int argc, char** argv) {\n       \"MakeArraysFromHostBufferShardsAndCopyToHostBufferWithString:\"\n       // `MakeErrorArrays` is not supported in NanoIfrtClient.\n       \"ArrayImplTest.MakeErrorArrays:\"\n+      // Sub-byte types are not supported in NanoIfrtClient.\n+      \"ArrayImplTest.HostBufferInt4:\"\n+      // NanoRT does not handle zero-sized buffers correctly.\n+      \"ArrayImplTest.MakeAndCopyZeroSizedBuffers:\"\n       // Executable returns a wrong number of devices.\n       \"LoadedExecutableImplTest.Properties:\"\n       // Incorrect deleted state of donated inputs."
        },
        {
            "sha": "828ba1aea80fbc19e0077eea44bd4a471aff6fb6",
            "filename": "third_party/xla/xla/python/ifrt/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD?ref=31e7c04d87e3c6ae94616799d4d73e4af4e0c462",
            "patch": "@@ -475,10 +475,11 @@ cc_library(\n         \":ifrt\",\n         \":test_util\",\n         \":user_context\",\n+        \"//xla:shape_util\",\n+        \"//xla/pjrt:pjrt_layout\",\n         \"//xla/python/ifrt/ir:sharding_param\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/algorithm:container\","
        },
        {
            "sha": "5a178b0ee5286dc54b151f3df770cf59aefccbba",
            "filename": "third_party/xla/xla/python/ifrt/array_impl_test_lib.cc",
            "status": "modified",
            "additions": 288,
            "deletions": 25,
            "changes": 313,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc?ref=31e7c04d87e3c6ae94616799d4d73e4af4e0c462",
            "patch": "@@ -30,12 +30,15 @@ limitations under the License.\n #include \"absl/time/clock.h\"\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/array_spec.h\"\n #include \"xla/python/ifrt/client.h\"\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/ir/sharding_param.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/shape.h\"\n@@ -45,20 +48,19 @@ limitations under the License.\n #include \"xla/python/ifrt/value.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n \n namespace xla {\n namespace ifrt {\n namespace {\n \n+using ::absl_testing::StatusIs;\n using ::testing::_;\n using ::testing::ElementsAre;\n using ::testing::ElementsAreArray;\n using ::testing::HasSubstr;\n using ::testing::SizeIs;\n-using ::tsl::testing::StatusIs;\n \n // Returns a list of non-addressable devices in the client.\n std::vector<Device*> GetNonAddressableDevices(Client* client) {\n@@ -127,10 +129,10 @@ TEST(ArrayImplTest,\n   devices.push_back(client->addressable_devices().at(0));\n   TF_ASSERT_OK_AND_ASSIGN(DeviceListRef device_list,\n                           client->MakeDeviceList(devices));\n-  ShardingRef sharding = xla::ifrt::ConcreteEvenSharding::Create(\n-      std::move(device_list), xla::ifrt::MemoryKind(), shape,\n-      /*shard_shape=*/shape,\n-      /*is_fully_replicated=*/true);\n+  ShardingRef sharding =\n+      ConcreteEvenSharding::Create(std::move(device_list), MemoryKind(), shape,\n+                                   /*shard_shape=*/shape,\n+                                   /*is_fully_replicated=*/true);\n   UserContextScope user_context_scope(test_util::MakeUserContext(100));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -287,6 +289,36 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferZeroCopy) {\n   // There should be no use-after-free.\n }\n \n+TEST(ArrayImplTest, MakeArrayFromHostBufferDefaultLayout) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  DType dtype(DType::kF32);\n+  Shape shape({2, 3});\n+  std::vector<float> data(6);\n+  std::iota(data.begin(), data.end(), 0);\n+  Device* device = client->addressable_devices()[0];\n+\n+  for (Memory* const memory : device->Memories()) {\n+    SCOPED_TRACE(absl::StrCat(memory->Kind()));\n+\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        auto default_layout,\n+        client->GetDefaultLayout(dtype, shape.dims(), device, memory->Kind()));\n+\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        auto array,\n+        client->MakeArrayFromHostBuffer(\n+            data.data(), dtype, shape, /*byte_strides=*/std::nullopt,\n+            SingleDeviceSharding::Create(device, memory->Kind()),\n+            Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+            /*on_done_with_host_buffer=*/nullptr));\n+    TF_ASSERT_OK(array->GetReadyFuture().Await());\n+\n+    TF_ASSERT_OK_AND_ASSIGN(auto layout, array->layout());\n+    EXPECT_EQ(*layout, *default_layout);\n+  }\n+}\n+\n TEST(ArrayImplTest, MakeArrayFromHostBufferAndCopyToHostBuffer) {\n   TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n \n@@ -339,6 +371,37 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferWithByteStridesAndCopyToHostBuffer) {\n   EXPECT_THAT(out_data, ElementsAreArray(expected_out_data));\n }\n \n+TEST(ArrayImplTest, MakeArrayFromHostBufferWithNonCompactByteStrides) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DeviceListRef device_list,\n+      client->MakeDeviceList(client->addressable_devices()));\n+  ASSERT_GT(device_list->size(), 1);\n+\n+  DType dtype(DType::kS8);\n+  Shape shape({2, 2});\n+  std::vector<int8_t> data = {0, -1, 1, -1, 2, -1, 3, -1};\n+  std::vector<int64_t> byte_strides = {2, 4};\n+  ShardingRef sharding = ConcreteEvenSharding::Create(\n+      device_list, MemoryKind(), shape, /*shard_shape=*/shape,\n+      /*is_fully_replicated=*/true);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ArrayRef array, client->MakeArrayFromHostBuffer(\n+                          data.data(), dtype, shape, byte_strides, sharding,\n+                          Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+                          /*on_done_with_host_buffer=*/nullptr));\n+  TF_ASSERT_OK(array->GetReadyFuture().Await());\n+\n+  std::vector<int8_t> out_data(4);\n+  Future<> future =\n+      array->CopyToHostBuffer(out_data.data(), /*byte_strides=*/std::nullopt,\n+                              ArrayCopySemantics::kAlwaysCopy);\n+  TF_ASSERT_OK(future.Await());\n+  EXPECT_THAT(out_data, ElementsAre(0, 2, 1, 3));\n+}\n+\n TEST(ArrayImplTest, MakeArrayFromHostBufferAndCopyToHostBufferWithByteStrides) {\n   TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n \n@@ -548,8 +611,7 @@ TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithDifferentDevices) {\n   } else {\n     status = result.status();\n   }\n-  EXPECT_THAT(status,\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+  EXPECT_THAT(status, StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithDifferentMemoryKinds) {\n@@ -605,8 +667,50 @@ TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithDifferentMemoryKinds) {\n   } else {\n     status = result.status();\n   }\n-  EXPECT_THAT(status,\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+  EXPECT_THAT(status, StatusIs(absl::StatusCode::kInvalidArgument));\n+}\n+\n+TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithLayout) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  DType dtype(DType::kF32);\n+  Shape shape({2, 3});\n+  std::vector<float> data(6);\n+  std::iota(data.begin(), data.end(), 0);\n+  Device* device = client->addressable_devices()[0];\n+\n+  auto layout = std::make_shared<xla::PjRtLayout>(\n+      xla::LayoutUtil::MakeDescendingLayout(shape.dims().size()));\n+\n+  ArrayRef array;\n+  {\n+    Client::HostBuffer host_buffer = {\n+        /*data=*/data.data(),\n+        /*dtype=*/dtype,\n+        /*shape=*/shape,\n+    };\n+    Client::MakeArraysFromHostBufferShardsSpec spec = {\n+        /*buffers=*/{{{0}, host_buffer}},\n+        /*array_spec=*/\n+        {\n+            /*dtype=*/dtype,\n+            /*shape=*/shape,\n+            /*sharding=*/\n+            SingleDeviceSharding::Create(device, MemoryKind()),\n+            /*layout=*/layout,\n+        },\n+    };\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::vector<ArrayRef> arrays,\n+        client->MakeArraysFromHostBufferShards(\n+            absl::MakeSpan(&spec, 1),\n+            Client::HostBufferSemantics::kImmutableOnlyDuringCall));\n+    array = std::move(arrays.front());\n+  }\n+\n+  TF_ASSERT_OK(array->GetReadyFuture().Await());\n+  TF_ASSERT_OK_AND_ASSIGN(auto result_layout, array->layout());\n+  EXPECT_EQ(*result_layout, *layout);\n }\n \n TEST(ArrayImplTest, MakeArrayFromHostBufferAndCopyToHostBufferWithString) {\n@@ -767,33 +871,103 @@ TEST(ArrayImplTest,\n   }\n }\n \n+TEST(ArrayImplTest, HostBufferRoundTripAllMemoryKinds) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  DType dtype(DType::kF32);\n+  Shape shape({2, 3});\n+  std::vector<float> data(6);\n+  std::iota(data.begin(), data.end(), 0);\n+  Device* device = client->addressable_devices()[0];\n+\n+  for (Memory* const memory : device->Memories()) {\n+    SCOPED_TRACE(absl::StrCat(memory->Kind()));\n+\n+    ShardingRef sharding = SingleDeviceSharding::Create(device, memory->Kind());\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        auto array,\n+        client->MakeArrayFromHostBuffer(\n+            data.data(), dtype, shape, /*byte_strides=*/std::nullopt, sharding,\n+            Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+            /*on_done_with_host_buffer=*/nullptr));\n+\n+    EXPECT_EQ(array->dtype(), dtype);\n+    EXPECT_EQ(array->shape(), shape);\n+    EXPECT_EQ(array->sharding(), *sharding);\n+    TF_ASSERT_OK(array->GetReadyFuture().Await());\n+\n+    std::vector<float> new_data(6);\n+    Future<> future = array->CopyToHostBuffer(\n+        static_cast<void*>(new_data.data()), /*byte_strides=*/std::nullopt,\n+        ArrayCopySemantics::kReuseInput);\n+    TF_ASSERT_OK(future.Await());\n+    EXPECT_THAT(new_data, ElementsAreArray(data));\n+  }\n+}\n+\n+TEST(ArrayImplTest, HostBufferInt4) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DeviceListRef device_list,\n+      client->MakeDeviceList(client->addressable_devices()));\n+  ASSERT_GT(device_list->size(), 1);\n+\n+  DType dtype(DType::kS4);\n+  Shape shape({2, 2});\n+  std::vector<int8_t> data = {1, 2, 3, 4};\n+\n+  for (Memory* const memory : device_list->devices().front()->Memories()) {\n+    SCOPED_TRACE(absl::StrCat(memory->Kind()));\n+\n+    ShardingRef sharding = ConcreteEvenSharding::Create(\n+        device_list, memory->Kind(), shape,\n+        /*shard_shape=*/shape, /*is_fully_replicated=*/true);\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        ArrayRef array,\n+        client->MakeArrayFromHostBuffer(\n+            data.data(), dtype, shape,\n+            /*byte_strides=*/std::nullopt, sharding,\n+            Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+            /*on_done_with_host_buffer=*/nullptr));\n+    TF_ASSERT_OK(array->GetReadyFuture().Await());\n+\n+    std::vector<int8_t> out_data(4);\n+    Future<> future =\n+        array->CopyToHostBuffer(out_data.data(), /*byte_strides=*/std::nullopt,\n+                                ArrayCopySemantics::kAlwaysCopy);\n+    TF_ASSERT_OK(future.Await());\n+    EXPECT_THAT(out_data, ElementsAreArray(data));\n+  }\n+}\n+\n TEST(ArrayImplTest, MakeErrorArrays) {\n   TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n   TF_ASSERT_OK_AND_ASSIGN(\n-      xla::ifrt::DeviceListRef device_list,\n+      DeviceListRef device_list,\n       client->MakeDeviceList(client->addressable_devices()));\n \n   Shape shape({2, 2});\n   ArraySpec array_spec = {\n-      /*dtype=*/xla::ifrt::DType(xla::ifrt::DType::kS8),\n+      /*dtype=*/DType(DType::kS8),\n       /*shape=*/shape,\n       /*sharding=*/\n-      xla::ifrt::ConcreteEvenSharding::Create(\n-          device_list, xla::ifrt::MemoryKind(), shape, /*shard_shape=*/shape,\n-          /*is_fully_replicated=*/true),\n+      ConcreteEvenSharding::Create(device_list, MemoryKind(), shape,\n+                                   /*shard_shape=*/shape,\n+                                   /*is_fully_replicated=*/true),\n   };\n \n   const absl::Status error = absl::InternalError(\"injected error\");\n   UserContextScope user_context_scope(test_util::MakeUserContext(100));\n   TF_ASSERT_OK_AND_ASSIGN(\n-      const std::vector<xla::ifrt::ArrayRef> arrays,\n+      const std::vector<ArrayRef> arrays,\n       client->MakeErrorArrays(error, {array_spec, array_spec}));\n   ASSERT_EQ(arrays.size(), 2);\n \n   EXPECT_THAT(arrays[0]->GetReadyFuture().Await(),\n-              absl_testing::StatusIs(_, HasSubstr(\"injected error\")));\n+              StatusIs(_, HasSubstr(\"injected error\")));\n   EXPECT_THAT(arrays[1]->GetReadyFuture().Await(),\n-              absl_testing::StatusIs(_, HasSubstr(\"injected error\")));\n+              StatusIs(_, HasSubstr(\"injected error\")));\n   EXPECT_EQ(arrays[0]->user_context()->Fingerprint(), 100);\n   EXPECT_EQ(arrays[1]->user_context()->Fingerprint(), 100);\n }\n@@ -819,21 +993,21 @@ TEST(ArrayImplTest, MakeErrorArraysWithAddressableAndNonAddressableDevice) {\n       std::move(device_list), MemoryKind(), shape, /*shard_shape=*/shape,\n       /*is_fully_replicated=*/true);\n \n-  ArraySpec array_spec = {/*dtype=*/xla::ifrt::DType(xla::ifrt::DType::kS8),\n+  ArraySpec array_spec = {/*dtype=*/DType(DType::kS8),\n                           /*shape=*/shape,\n                           /*sharding=*/sharding};\n \n   const absl::Status error = absl::InternalError(\"injected error\");\n   UserContextScope user_context_scope(test_util::MakeUserContext(100));\n   TF_ASSERT_OK_AND_ASSIGN(\n-      const std::vector<xla::ifrt::ArrayRef> arrays,\n+      const std::vector<ArrayRef> arrays,\n       client->MakeErrorArrays(error, {array_spec, array_spec}));\n   ASSERT_EQ(arrays.size(), 2);\n \n   EXPECT_THAT(arrays[0]->GetReadyFuture().Await(),\n-              absl_testing::StatusIs(_, HasSubstr(\"injected error\")));\n+              StatusIs(_, HasSubstr(\"injected error\")));\n   EXPECT_THAT(arrays[1]->GetReadyFuture().Await(),\n-              absl_testing::StatusIs(_, HasSubstr(\"injected error\")));\n+              StatusIs(_, HasSubstr(\"injected error\")));\n   EXPECT_EQ(arrays[0]->user_context()->Fingerprint(), 100);\n   EXPECT_EQ(arrays[1]->user_context()->Fingerprint(), 100);\n }\n@@ -1148,7 +1322,7 @@ TEST(ArrayImplTest, CopyToDifferentDevice) {\n             SingleDeviceShardSemantics::kAddressableShards));\n   }\n \n-  absl::InlinedVector<xla::ifrt::Device*, 1> new_devices;\n+  absl::InlinedVector<Device*, 1> new_devices;\n   for (auto it = devices->devices().rbegin(); it != devices->devices().rend();\n        ++it) {\n     new_devices.push_back(*it);\n@@ -1210,7 +1384,7 @@ TEST(ArrayImplTest, CopyMixedSourceDevices) {\n                   ->CopyArrays(absl::MakeSpan(arrays), std::move(device_list),\n                                MemoryKind(), ArrayCopySemantics::kAlwaysCopy)\n                   .status(),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n TEST(ArrayImplTest, CopyMixedSourceMemoryKind) {\n@@ -1244,7 +1418,96 @@ TEST(ArrayImplTest, CopyMixedSourceMemoryKind) {\n                   ->CopyArrays(absl::MakeSpan(arrays), std::move(device_list),\n                                MemoryKind(), ArrayCopySemantics::kAlwaysCopy)\n                   .status(),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+}\n+\n+TEST(ArrayImplTest, CopyPreservesDefaultLayouts) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  DType dtype(DType::kF32);\n+  Shape shape({2, 3});\n+  std::vector<float> data(6);\n+  std::iota(data.begin(), data.end(), 0);\n+  Device* device = client->addressable_devices()[0];\n+\n+  for (Memory* const src_memory : device->Memories()) {\n+    for (Memory* const dst_memory : device->Memories()) {\n+      SCOPED_TRACE(\n+          absl::StrCat(src_memory->Kind(), \" -> \", dst_memory->Kind()));\n+\n+      ShardingRef sharding =\n+          SingleDeviceSharding::Create(device, src_memory->Kind());\n+      TF_ASSERT_OK_AND_ASSIGN(\n+          auto array,\n+          client->MakeArrayFromHostBuffer(\n+              data.data(), dtype, shape, /*byte_strides=*/std::nullopt,\n+              sharding, Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+              /*on_done_with_host_buffer=*/nullptr));\n+      TF_ASSERT_OK(array->GetReadyFuture().Await());\n+\n+      TF_ASSERT_OK_AND_ASSIGN(auto src_layout, array->layout());\n+      TF_ASSERT_OK_AND_ASSIGN(\n+          auto src_default_layout,\n+          client->GetDefaultLayout(dtype, shape.dims(), device,\n+                                   src_memory->Kind()));\n+      EXPECT_EQ(*src_layout, *src_default_layout);\n+\n+      TF_ASSERT_OK_AND_ASSIGN(\n+          auto new_arrays, client->CopyArrays(absl::MakeSpan(&array, 1),\n+                                              std::nullopt, dst_memory->Kind(),\n+                                              ArrayCopySemantics::kAlwaysCopy));\n+      ASSERT_THAT(new_arrays, SizeIs(1));\n+      TF_ASSERT_OK_AND_ASSIGN(auto dst_layout, new_arrays[0]->layout());\n+      TF_ASSERT_OK_AND_ASSIGN(\n+          auto dst_default_layout,\n+          client->GetDefaultLayout(dtype, shape.dims(), device,\n+                                   dst_memory->Kind()));\n+      EXPECT_EQ(*dst_layout, *dst_default_layout);\n+    }\n+  }\n+}\n+\n+TEST(ArrayImplTest, MakeAndCopyZeroSizedBuffers) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  Device* const device = client->addressable_devices().front();\n+  TF_ASSERT_OK_AND_ASSIGN(DeviceListRef device_list,\n+                          client->MakeDeviceList({device}));\n+\n+  DType dtype(DType::kF32);\n+  Shape shape({0, 1});\n+\n+  for (Memory* const memory : device->Memories()) {\n+    SCOPED_TRACE(absl::StrCat(memory->Kind()));\n+\n+    ShardingRef sharding = ConcreteEvenSharding::Create(\n+        device_list, memory->Kind(), shape,\n+        /*shard_shape=*/shape, /*is_fully_replicated=*/true);\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        ArrayRef array,\n+        client->MakeArrayFromHostBuffer(\n+            nullptr, dtype, shape,\n+            /*byte_strides=*/std::nullopt, sharding,\n+            Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+            /*on_done_with_host_buffer=*/nullptr));\n+    TF_ASSERT_OK(array->GetReadyFuture().Await());\n+\n+    for (Device* const device : client->addressable_devices()) {\n+      TF_ASSERT_OK_AND_ASSIGN(DeviceListRef single_device_list,\n+                              client->MakeDeviceList({device}));\n+      TF_ASSERT_OK_AND_ASSIGN(\n+          auto copied,\n+          client->CopyArrays(absl::MakeSpan(&array, 1),\n+                             std::move(single_device_list), std::nullopt,\n+                             ArrayCopySemantics::kReuseInput));\n+      TF_ASSERT_OK(copied[0]->GetReadyFuture().Await());\n+\n+      Future<> future =\n+          copied[0]->CopyToHostBuffer(nullptr, /*byte_strides=*/std::nullopt,\n+                                      ArrayCopySemantics::kAlwaysCopy);\n+      TF_ASSERT_OK(future.Await());\n+    }\n+  }\n }\n \n TEST(ArrayImplTest, GetReadyFuture) {"
        },
        {
            "sha": "978c516ad9902f1482741893d4d6b6ac16decdfe",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/31e7c04d87e3c6ae94616799d4d73e4af4e0c462/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc?ref=31e7c04d87e3c6ae94616799d4d73e4af4e0c462",
            "patch": "@@ -345,6 +345,13 @@ Future<> PjRtArray::CopyToHostBuffer(\n     ArrayCopySemantics semantics) {\n   DCHECK(this);\n   if (sharding_->devices()->size() != 1) {\n+    if (sharding_->IsFullyReplicated()) {\n+      absl::StatusOr<ArrayRef> replicated = FullyReplicatedShard(semantics);\n+      if (!replicated.ok()) {\n+        return Future<>(std::move(replicated).status());\n+      }\n+      return (*replicated)->CopyToHostBuffer(data, byte_strides, semantics);\n+    }\n     return Future<>(\n         InvalidArgument(\"Only single-shard is implemented, but got %d\",\n                         sharding_->devices()->size()));"
        }
    ],
    "stats": {
        "total": 334,
        "additions": 306,
        "deletions": 28
    }
}