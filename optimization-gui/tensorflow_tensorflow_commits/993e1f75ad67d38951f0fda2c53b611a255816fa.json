{
    "author": "beckerhe",
    "message": "Remove MLIR-related unused code from CollectiveThunk\n\nThe removed data fields are not used and always initialized from a nullptr.\nThe removed functions are not called from anywhere. So let's remove all of that.\n\nPiperOrigin-RevId: 818504444",
    "sha": "993e1f75ad67d38951f0fda2c53b611a255816fa",
    "files": [
        {
            "sha": "555a420a9a60f3bc8a03183a82e2cb6a610f209c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/993e1f75ad67d38951f0fda2c53b611a255816fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/993e1f75ad67d38951f0fda2c53b611a255816fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=993e1f75ad67d38951f0fda2c53b611a255816fa",
            "patch": "@@ -1397,8 +1397,8 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/translate/mhlo_to_hlo:attribute_exporter\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:computation_placer\",\n@@ -1425,7 +1425,6 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n "
        },
        {
            "sha": "9ab9a5a24da128ffbe7022bc480d9d120cb5f50d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 40,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/993e1f75ad67d38951f0fda2c53b611a255816fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/993e1f75ad67d38951f0fda2c53b611a255816fa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=993e1f75ad67d38951f0fda2c53b611a255816fa",
            "patch": "@@ -29,16 +29,13 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/Value.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/translate/mhlo_to_hlo/attribute_exporter.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n@@ -61,48 +58,14 @@ struct CollectiveConfig {\n   CollectiveOpGroupMode group_mode;\n   bool use_symmetric_buffer;\n \n-  template <typename OpT>\n-  void SetCollectiveOpKindAndID(OpT op);\n   void SetCollectiveOpKindAndID(const HloCollectivePermuteInstruction* instr);\n   void SetCollectiveOpKindAndID(const HloSendRecvInstruction* instr);\n   bool IsDegenerate(int64_t replica_count, int64_t partition_count) const;\n };\n \n-template <typename OpT>\n-void CollectiveConfig::SetCollectiveOpKindAndID(OpT op) {\n-  if (op.getChannelId()) {\n-    collective_op_kind = RendezvousKey::kCrossModule;\n-    op_id = static_cast<int64_t>(op.getChannelId()->getHandle());\n-  } else {\n-    collective_op_kind = RendezvousKey::kCrossReplica;\n-    mlir::ModuleOp parent = op->template getParentOfType<mlir::ModuleOp>();\n-    mlir::IntegerAttr unique_id =\n-        parent->getAttrOfType<mlir::IntegerAttr>(\"hlo.unique_id\");\n-    op_id = static_cast<int64_t>(unique_id.getInt());\n-  }\n-}\n-\n CollectiveConfig GetCollectiveConfig(const HloInstruction* hlo,\n                                      std::optional<bool> use_global_device_ids);\n \n-template <typename OpT>\n-CollectiveConfig GetCollectiveConfigForMlir(\n-    OpT op, std::optional<bool> use_global_device_ids) {\n-  CollectiveConfig config;\n-  config.operand_count = op.getInputs().size();\n-  config.operand_element_type.reserve(config.operand_count);\n-  for (int i = 0; i < config.operand_count; i++) {\n-    const Shape shape = GetShape(op.getInputs()[i]);\n-    config.operand_element_type.push_back(shape.element_type());\n-  }\n-  config.replica_groups = ConvertReplicaGroups(op.getReplicaGroups()).value();\n-  config.SetCollectiveOpKindAndID(op);\n-  config.group_mode = GetCollectiveOpGroupMode(op.getChannelId().has_value(),\n-                                               use_global_device_ids)\n-                          .value();\n-  return config;\n-}\n-\n // Handle to a communicator object with corresponding clique key.\n struct CommunicatorHandle {\n   CommunicatorHandle(Communicator* comm, GpuCliqueKey clique_key)\n@@ -277,8 +240,6 @@ class CollectiveDoneThunk : public Thunk {\n \n //===----------------------------------------------------------------------===//\n \n-absl::Status IsValidOperand(mlir::Value operand, Thunk::Kind reduction_op);\n-\n absl::Status IsValidOperand(Shape shape, Thunk::Kind reduction_op);\n \n template <typename CollectiveThunkType, typename OpT>"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 2,
        "deletions": 42
    }
}