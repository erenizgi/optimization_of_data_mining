{
    "author": "tensorflower-gardener",
    "message": "Add Cudnn backend for gemm_fusion_autotuner for cuda implementation.\n\nPiperOrigin-RevId: 840203279",
    "sha": "8124613335044c3d98598f2ed322ffb3815f3b77",
    "files": [
        {
            "sha": "6e3cf6a677de902b1b3a7cf00c23b29812bc1ccb",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=8124613335044c3d98598f2ed322ffb3815f3b77",
            "patch": "@@ -48,12 +48,15 @@ cc_library(\n         \":triton_configs\",\n         \"//xla:autotuning_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/backends/gpu/autotuner:cudnn\",\n         \"//xla/backends/gpu/codegen/triton:tma_utils\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:algorithm_util\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n@@ -65,6 +68,7 @@ cc_library(\n         \"//xla/service/gpu/transforms:cudnn_fusion_compiler\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:env\",\n@@ -95,15 +99,18 @@ cc_library(\n         \":triton_configs\",\n         \"//xla:autotuning_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/rocm:rocblas_plugin\",\n         \"//xla/tsl/platform:env\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -188,6 +195,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/cuda:ptx_compiler_helpers\",\n         \"//xla/stream_executor/gpu:redzone_allocator\","
        },
        {
            "sha": "ead6e46402b60537bad388d957ea65633f0db205",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=8124613335044c3d98598f2ed322ffb3815f3b77",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <array>\n #include <atomic>\n #include <cstdint>\n+#include <iterator>\n #include <memory>\n #include <optional>\n #include <string>\n@@ -1703,21 +1704,28 @@ absl::StatusOr<bool> GemmFusionAutotuner::RunViaNewInfra(\n   se::DeviceMemoryAllocator* device_allocator = config_.GetAllocator();\n   std::unique_ptr<Compiler::GpuTargetConfig> target_config;\n   target_config = std::make_unique<Compiler::GpuTargetConfig>(stream_exec);\n-  backends.push_back(std::make_unique<TritonBackend>(\n-      &debug_options, compiler.get(), target_config.get(), mlir_context_));\n   backends.push_back(std::make_unique<FissionBackend>(\n       &debug_options, compiler.get(), target_config.get(),\n       std::make_unique<CublasBackend>(stream_exec, &debug_options,\n                                       compiler.get(), target_config.get(),\n                                       /*fp8_lt_fallback=*/true),\n       GetCublasRewriterPipeline(&target_config->device_description),\n       mlir_context_));\n+  backends.push_back(std::make_unique<TritonBackend>(\n+      &debug_options, compiler.get(), target_config.get(), mlir_context_));\n   backends.push_back(std::make_unique<FissionBackend>(\n       &debug_options, compiler.get(), target_config.get(),\n       std::make_unique<CustomKernelBackend>(\n           stream_exec, &debug_options, compiler.get(), target_config.get()),\n       GetCustomKernelRewriterPipeline(&target_config->device_description),\n       mlir_context_));\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<std::unique_ptr<CodegenBackend>> platform_backends,\n+      GetPlatformCodegenBackends(stream_exec, compiler.get(),\n+                                 target_config.get(), &debug_options));\n+  backends.insert(backends.end(),\n+                  std::make_move_iterator(platform_backends.begin()),\n+                  std::make_move_iterator(platform_backends.end()));\n   auto should_autotune = [](const HloInstruction& instruction) -> bool {\n     if (instruction.opcode() != HloOpcode::kFusion) {\n       return false;\n@@ -1728,9 +1736,11 @@ absl::StatusOr<bool> GemmFusionAutotuner::RunViaNewInfra(\n     bool is_unassigned_triton =\n         backend_config.kind() == kTritonGemmFusionKind &&\n         !backend_config.has_triton_gemm_config();\n+    bool is_unassigned_cudnn = backend_config.kind() == kCuDnnFusionKind &&\n+                               !backend_config.has_cudnn_fusion_config();\n     bool is_unassigned_custom = backend_config.kind() == kCustomFusionKind &&\n                                 !backend_config.has_custom_fusion_config();\n-    if (is_unassigned_triton || is_unassigned_custom) {\n+    if (is_unassigned_triton || is_unassigned_cudnn || is_unassigned_custom) {\n       return true;\n     }\n     return false;"
        },
        {
            "sha": "1ca4059f91d77e901e1b727629529e25038273e7",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=8124613335044c3d98598f2ed322ffb3815f3b77",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -37,6 +38,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/autotuning/autotuner_compile_util.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n@@ -45,6 +47,7 @@ limitations under the License.\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/xla.pb.h\"\n \n@@ -97,6 +100,12 @@ class GemmFusionAutotuner : public HloModulePass {\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads);\n \n+  absl::StatusOr<std::vector<std::unique_ptr<CodegenBackend>>>\n+  GetPlatformCodegenBackends(se::StreamExecutor* stream_exec,\n+                             Compiler* compiler,\n+                             const Compiler::GpuTargetConfig* target_config,\n+                             const DebugOptions* debug_options);\n+\n  private:\n   AutotuneConfig config_;\n   se::SemanticVersion toolkit_version_;"
        },
        {
            "sha": "7dc86e8a9c2fdeee432045aa3e1a3a2de8ac9c7c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=8124613335044c3d98598f2ed322ffb3815f3b77",
            "patch": "@@ -14,15 +14,20 @@ limitations under the License.\n ==============================================================================*/\n \n #include <cstdint>\n+#include <memory>\n #include <vector>\n \n+#include \"absl/status/statusor.h\"\n #include \"third_party/gpus/cuda/include/cublas_v2.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/cudnn.h\"\n #include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/algorithm_util.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/autotuning/gemm_fusion_autotuner.h\"\n #include \"xla/service/gpu/autotuning/triton_configs.h\"\n@@ -35,6 +40,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/cudnn_fusion_compiler.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla {\n namespace gpu {\n@@ -94,6 +100,17 @@ bool GemmFusionAutotunerImpl::AddLibConfigs(\n   return false;\n }\n \n+absl::StatusOr<std::vector<std::unique_ptr<CodegenBackend>>>\n+GemmFusionAutotuner::GetPlatformCodegenBackends(\n+    se::StreamExecutor* stream_exec, Compiler* compiler,\n+    const Compiler::GpuTargetConfig* target_config,\n+    const DebugOptions* debug_options) {\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::make_unique<CudnnBackend>(stream_exec, debug_options,\n+                                                    compiler, target_config));\n+  return backends;\n+}\n+\n std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     const {\n   stream_executor::CudaComputeCapability compute_capability ="
        },
        {
            "sha": "83232e68d4e12635fc6aaec5ce324488beea5c6a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_rocm.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8124613335044c3d98598f2ed322ffb3815f3b77/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_rocm.cc?ref=8124613335044c3d98598f2ed322ffb3815f3b77",
            "patch": "@@ -14,13 +14,19 @@ limitations under the License.\n ==============================================================================*/\n \n #include <cstdint>\n+#include <memory>\n #include <vector>\n \n+#include \"absl/status/statusor.h\"\n #include \"rocm/include/hipblas/hipblas.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/autotuning/gemm_fusion_autotuner.h\"\n #include \"xla/service/gpu/autotuning/triton_configs.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla {\n namespace gpu {\n@@ -33,6 +39,14 @@ bool GemmFusionAutotunerImpl::AddLibConfigs(\n   return false;\n }\n \n+absl::StatusOr<std::vector<std::unique_ptr<CodegenBackend>>>\n+GemmFusionAutotuner::GetPlatformCodegenBackends(\n+    se::StreamExecutor* stream_exec, Compiler* compiler,\n+    const Compiler::GpuTargetConfig* target_config,\n+    const DebugOptions* debug_options) {\n+  return std::vector<std::unique_ptr<CodegenBackend>>();\n+}\n+\n std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     const {\n   return *kDefaultRocmConfigs;"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 61,
        "deletions": 3
    }
}