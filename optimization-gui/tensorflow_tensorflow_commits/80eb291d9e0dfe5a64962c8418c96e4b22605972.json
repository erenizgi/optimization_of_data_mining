{
    "author": "sergachev",
    "message": "PR #33464: Add a new version of ShapeUtil::ByteStrides() and fix its use in the GPU compiler.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33464\n\nüìù Summary of Changes\nDisable incorrect GPU copy fusions on packed sub-byte types using incorrect stride calculations.\nMultiple uses in PJRT rely on the existing behavior of ByteStrides() which ignores element_size_in_bits - this is now called UnpackedByteStrides().\n\nüéØ Justification\nFixes GPU DynamicMemcpyFusion which relies on ByteStrides().\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüß™ Unit Tests:\nYes.\n\nüß™ Execution Tests:\nNo.\nCopybara import of the project:\n\n--\ne54830e71c9dbf88b0f4613323ec964ec16bbbd9 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAdd a new version of ShapeUtil::ByteStrides() and fix its use in the GPU compiler.\n\nDisable incorrect GPU copy fusions on packed sub-byte types using\nincorrect stride calculations.\n\n--\n7947ec3fbb9aec276d22da664110d88c95e87f91 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review comment.\n\nMerging this change closes #33464\n\nPiperOrigin-RevId: 827984848",
    "sha": "80eb291d9e0dfe5a64962c8418c96e4b22605972",
    "files": [
        {
            "sha": "6a529e528f0a27e2f2544617047920229ecbef67",
            "filename": "third_party/xla/xla/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2FBUILD?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -557,6 +557,7 @@ xla_cc_test(\n         \"//xla/tsl/platform:test_benchmark\",\n         \"//xla/tsl/platform:test_main\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\","
        },
        {
            "sha": "41626debc7fc550f581d17daf2294c5c791177b6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/copy_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -314,6 +314,24 @@ TEST_F(CopyFusionTest, BuildUpdateSliceDescriptor) {\n   EXPECT_EQ(offset.byte_stride, 8 * 8 * sizeof(float));\n }\n \n+TEST_F(CopyFusionTest, PackedSubByteTypesAreNotSupported) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+    dynamic_slice {\n+      a = s4[20]{0:E(4)} parameter(0)\n+      c = s32[] constant(10)\n+      s = s4[10] dynamic-slice(a, c), dynamic_slice_sizes={10}\n+    }\n+\n+    entry {\n+      a = s4[20]{0:E(4)} parameter(0)\n+      f = s4[10] fusion(a), kind=kLoop, calls=dynamic_slice\n+    }\n+  )\"));\n+  EXPECT_FALSE(\n+      DynamicMemcpyFusion::GetMemcpyDescriptorForFusion(GetFusion(module.get()))\n+          .has_value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "2d4ae8a99d5fb890556ca27be1cafb4ef5cc5563",
            "filename": "third_party/xla/xla/pjrt/c_api_client/pjrt_c_api_client.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fc_api_client%2Fpjrt_c_api_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fc_api_client%2Fpjrt_c_api_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc_api_client%2Fpjrt_c_api_client.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -2866,8 +2866,8 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> PjRtCApiBuffer::CopyToMemorySpace(\n     TF_ASSIGN_OR_RETURN(std::shared_ptr<Literal> literal, ToLiteralSync());\n     absl::InlinedVector<int64_t, 4> byte_strides(\n         literal->shape().dimensions().size());\n-    TF_RETURN_IF_ERROR(\n-        ShapeUtil::ByteStrides(literal->shape(), absl::MakeSpan(byte_strides)));\n+    TF_RETURN_IF_ERROR(ShapeUtil::UnpackedByteStrides(\n+        literal->shape(), absl::MakeSpan(byte_strides)));\n     // Avoid use-after-free on `literal` due to unsequenced move and use.\n     Literal* literal_pointer = literal.get();\n     return dst_memory->client()->BufferFromHostBuffer("
        },
        {
            "sha": "cf68f679dbbdd2a787301a20cc32ae3e26f82296",
            "filename": "third_party/xla/xla/pjrt/common_pjrt_client.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -751,8 +751,8 @@ CommonPjRtBufferImpl::CopyToMemorySpaceSyncThroughLiteral(\n   TF_ASSIGN_OR_RETURN(std::shared_ptr<Literal> literal, ToLiteralSync());\n   absl::InlinedVector<int64_t, 4> byte_strides(\n       literal->shape().dimensions().size());\n-  TF_RETURN_IF_ERROR(\n-      ShapeUtil::ByteStrides(literal->shape(), absl::MakeSpan(byte_strides)));\n+  TF_RETURN_IF_ERROR(ShapeUtil::UnpackedByteStrides(\n+      literal->shape(), absl::MakeSpan(byte_strides)));\n   // Avoid use-after-free on `literal` due to unsequenced move and use.\n   Literal* literal_pointer = literal.get();\n   return dst_memory_space->client()->BufferFromHostBuffer("
        },
        {
            "sha": "636f98652aa069035b524bd7c9617191826cb15b",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -369,7 +369,7 @@ Future<> TfrtGpuBuffer::ToLiteralHelper(Future<MutableLiteralBase*> literal) {\n           if (on_device_shape.layout() != literal_layout) {\n             absl::InlinedVector<int64_t, 4> byte_strides(\n                 on_device_shape.dimensions().size());\n-            absl::Status s = ShapeUtil::ByteStrides(\n+            absl::Status s = ShapeUtil::UnpackedByteStrides(\n                 on_device_shape, absl::MakeSpan(byte_strides));\n             if (!s.ok()) {\n               promise.Set(s);\n@@ -745,8 +745,8 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> TfrtGpuBuffer::CopyToMemorySpace(\n     Literal* literal_pointer = literal.get();\n     absl::InlinedVector<int64_t, 4> byte_strides(\n         literal->shape().dimensions().size());\n-    TF_RETURN_IF_ERROR(\n-        ShapeUtil::ByteStrides(literal->shape(), absl::MakeSpan(byte_strides)));\n+    TF_RETURN_IF_ERROR(ShapeUtil::UnpackedByteStrides(\n+        literal->shape(), absl::MakeSpan(byte_strides)));\n     return dst_device->client()->BufferFromHostBuffer(\n         literal_pointer->untyped_data(),\n         literal_pointer->shape().element_type(),"
        },
        {
            "sha": "ffa93ef8e3f58248f8e9d61f8749662c60f57302",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -850,8 +850,8 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> TfrtGpuClient::BufferFromHostBuffer(\n   absl::InlinedVector<int64_t, 4> tmp_strides;\n   if (!byte_strides) {\n     tmp_strides.resize(dims.size());\n-    TF_RETURN_IF_ERROR(\n-        ShapeUtil::ByteStrides(device_shape, absl::MakeSpan(tmp_strides)));\n+    TF_RETURN_IF_ERROR(ShapeUtil::UnpackedByteStrides(\n+        device_shape, absl::MakeSpan(tmp_strides)));\n     byte_strides = tmp_strides;\n   }\n \n@@ -868,8 +868,8 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> TfrtGpuClient::BufferFromHostBuffer(\n \n   absl::InlinedVector<int64_t, 4> shape_strides(\n       device_shape.dimensions().size());\n-  TF_RETURN_IF_ERROR(\n-      ShapeUtil::ByteStrides(device_shape, absl::MakeSpan(shape_strides)));\n+  TF_RETURN_IF_ERROR(ShapeUtil::UnpackedByteStrides(\n+      device_shape, absl::MakeSpan(shape_strides)));\n   bool host_and_device_strides_equal =\n       (byte_size == 0 || *byte_strides == shape_strides);\n "
        },
        {
            "sha": "9db2f93f0e681a98e03e6458ce03f5b3ab6b3041",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 36,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -611,8 +611,8 @@ PjRtStreamExecutorClient::LinearizeHostBufferInto(\n   absl::InlinedVector<int64_t, 4> tmp_strides;\n   if (!byte_strides) {\n     tmp_strides.resize(dims.size());\n-    TF_RETURN_IF_ERROR(\n-        ShapeUtil::ByteStrides(on_host_shape, absl::MakeSpan(tmp_strides)));\n+    TF_RETURN_IF_ERROR(ShapeUtil::UnpackedByteStrides(\n+        on_host_shape, absl::MakeSpan(tmp_strides)));\n     byte_strides = tmp_strides;\n   }\n   int64_t size = ShapeUtil::ByteSizeOf(on_host_shape);\n@@ -621,8 +621,8 @@ PjRtStreamExecutorClient::LinearizeHostBufferInto(\n \n   absl::InlinedVector<int64_t, 4> shape_strides(\n       device_shape.dimensions().size());\n-  TF_RETURN_IF_ERROR(\n-      ShapeUtil::ByteStrides(device_shape, absl::MakeSpan(shape_strides)));\n+  TF_RETURN_IF_ERROR(ShapeUtil::UnpackedByteStrides(\n+      device_shape, absl::MakeSpan(shape_strides)));\n   bool host_and_device_strides_equal =\n       (size == 0 || *byte_strides == shape_strides);\n \n@@ -860,38 +860,36 @@ PjRtStreamExecutorClient::LinearizeInto(\n   // it includes linearization that may be slow.\n   // TODO(misard) assess if it would be preferable to introduce a heuristic to\n   // put the transfer into the calling thread for small literals.\n-  auto transfer_h2d =\n-      [this, local_client = client(), transfer_manager, local_device,\n-       raw_buffer, device, event, literal,\n-       on_device_shape = std::move(on_device_shape)]() mutable {\n-        // This function uses TF_CHECK_OK and value() since we have no way\n-        // to report failures from a callback. However, the operations here are\n-        // unlikely to fail and not recoverable even if we were to fail: DMAs to\n-        // memory that has already been allocated, and a possible Event\n-        // allocation.\n-        auto device_memory =\n-            tensorflow::down_cast<PjRtStreamExecutorRawBuffer*>(\n-                raw_buffer.get())\n-                ->device_buffer();\n-\n-        se::Stream* h2d_stream = local_device->host_to_device_stream();\n-\n-        ShapedBuffer buffer =\n-            device_memory->AsShapedBuffer(device, on_device_shape);\n-        TF_CHECK_OK(transfer_manager->TransferLiteralToDeviceAsync(\n-            h2d_stream, literal, buffer));\n-\n-        TF_CHECK_OK(AddDestinationBufferSynchronization(this, local_device,\n-                                                        event, h2d_stream));\n-\n-        local_device->ThenRelease(h2d_stream, device_memory).IgnoreError();\n-\n-        // This can sometimes catch the case where the literal memory has been\n-        // freed before the H2D transfer was issued.\n-        h2d_stream->RefreshStatus()\n-            .IgnoreError();  // Can return error::Unimplemented\n-        QCHECK(h2d_stream->ok());\n-      };\n+  auto transfer_h2d = [this, local_client = client(), transfer_manager,\n+                       local_device, raw_buffer, device, event, literal,\n+                       on_device_shape = std::move(on_device_shape)]() mutable {\n+    // This function uses TF_CHECK_OK and value() since we have no way\n+    // to report failures from a callback. However, the operations here are\n+    // unlikely to fail and not recoverable even if we were to fail: DMAs to\n+    // memory that has already been allocated, and a possible Event\n+    // allocation.\n+    auto device_memory =\n+        tensorflow::down_cast<PjRtStreamExecutorRawBuffer*>(raw_buffer.get())\n+            ->device_buffer();\n+\n+    se::Stream* h2d_stream = local_device->host_to_device_stream();\n+\n+    ShapedBuffer buffer =\n+        device_memory->AsShapedBuffer(device, on_device_shape);\n+    TF_CHECK_OK(transfer_manager->TransferLiteralToDeviceAsync(\n+        h2d_stream, literal, buffer));\n+\n+    TF_CHECK_OK(AddDestinationBufferSynchronization(this, local_device, event,\n+                                                    h2d_stream));\n+\n+    local_device->ThenRelease(h2d_stream, device_memory).IgnoreError();\n+\n+    // This can sometimes catch the case where the literal memory has been\n+    // freed before the H2D transfer was issued.\n+    h2d_stream->RefreshStatus()\n+        .IgnoreError();  // Can return error::Unimplemented\n+    QCHECK(h2d_stream->ok());\n+  };\n   thread_pool()->Schedule(WrapClosureAsCopyable(std::move(transfer_h2d)));\n   return tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(event);\n }"
        },
        {
            "sha": "01dad6f9d0c7a82afbf167a34a35ae141827556c",
            "filename": "third_party/xla/xla/pjrt/se_raw_buffer.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -299,7 +299,7 @@ void PjRtStreamExecutorRawBuffer::CopyToLiteralAsync(\n           if (on_device_shape.layout() != literal_layout) {\n             absl::InlinedVector<int64_t, 4> byte_strides(\n                 on_device_shape.dimensions().size());\n-            absl::Status s = ShapeUtil::ByteStrides(\n+            absl::Status s = ShapeUtil::UnpackedByteStrides(\n                 on_device_shape, absl::MakeSpan(byte_strides));\n             if (!s.ok()) {\n               promise.Set(s);"
        },
        {
            "sha": "321f9dfffab4cd141dc7007c2d1e3b00c6522c9c",
            "filename": "third_party/xla/xla/shape_util.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fshape_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fshape_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fshape_util.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -2201,8 +2201,8 @@ Shape ShapeUtil::DeviceShapeToHostShape(Shape s) {\n }\n \n /*static*/\n-absl::Status ShapeUtil::ByteStrides(const Shape& shape,\n-                                    absl::Span<int64_t> strides) {\n+absl::Status ShapeUtil::UnpackedByteStrides(const Shape& shape,\n+                                            absl::Span<int64_t> strides) {\n   TF_RET_CHECK(shape.IsArray());\n   TF_RET_CHECK(shape.has_layout());\n   TF_RET_CHECK(shape.dimensions().size() == strides.size());\n@@ -2216,15 +2216,29 @@ absl::Status ShapeUtil::ByteStrides(const Shape& shape,\n }\n \n /*static*/\n-std::optional<absl::InlinedVector<int64_t, 4>> ShapeUtil::ByteStrides(\n+absl::Status ShapeUtil::ByteStrides(const Shape& shape,\n+                                    absl::Span<int64_t> strides) {\n+  return UnpackedByteStrides(shape, strides);\n+}\n+\n+/*static*/\n+std::optional<absl::InlinedVector<int64_t, 4>> ShapeUtil::UnpackedByteStrides(\n     const Shape& shape) {\n   absl::InlinedVector<int64_t, 4> strides(shape.dimensions().size());\n-  if (!ByteStrides(shape, absl::MakeSpan(strides)).ok()) {\n+  if (!UnpackedByteStrides(shape, absl::MakeSpan(strides)).ok()) {\n     return std::nullopt;\n   }\n   return strides;\n }\n \n+/*static*/ std::optional<absl::InlinedVector<int64_t, 4>>\n+ShapeUtil::ByteStrides(const Shape& shape) {\n+  if (shape.layout().element_size_in_bits() % CHAR_BIT != 0) {\n+    return std::nullopt;\n+  }\n+  return UnpackedByteStrides(shape);\n+}\n+\n /*static*/ int64_t ShapeUtil::ElementSizeInBits(const Shape& shape) {\n   if (shape.has_layout() && shape.layout().element_size_in_bits() != 0) {\n     return shape.layout().element_size_in_bits();"
        },
        {
            "sha": "fde70d0dd22ef56e3508007517bc8174acbb9259",
            "filename": "third_party/xla/xla/shape_util.h",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fshape_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fshape_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fshape_util.h?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -1136,10 +1136,17 @@ class ShapeUtil {\n \n   // Computes byte strides of an array shape `shape`. `shape` must have a\n   // layout. Ignores tiling. `strides` must have size equal to the number of\n-  // dimensions of `shape`.\n+  // dimensions of `shape`. Ignores element_size_in_bits - uses its default\n+  // value, ByteSizeOfPrimitiveType - therefore `unpacked`.\n+  static absl::Status UnpackedByteStrides(const Shape& shape,\n+                                          absl::Span<int64_t> strides);\n+  ABSL_DEPRECATE_AND_INLINE()\n   static absl::Status ByteStrides(const Shape& shape,\n                                   absl::Span<int64_t> strides);\n   // Same as above but returns the stride array, or std::nullopt if error.\n+  static std::optional<absl::InlinedVector<int64_t, 4>> UnpackedByteStrides(\n+      const Shape& shape);\n+  // Same as above but returns std::nullopt if elements are not byte-aligned.\n   static std::optional<absl::InlinedVector<int64_t, 4>> ByteStrides(\n       const Shape& shape);\n "
        },
        {
            "sha": "ebfefa77e568d2374b739f72558572760b60be70",
            "filename": "third_party/xla/xla/shape_util_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fshape_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80eb291d9e0dfe5a64962c8418c96e4b22605972/third_party%2Fxla%2Fxla%2Fshape_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fshape_util_test.cc?ref=80eb291d9e0dfe5a64962c8418c96e4b22605972",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n \n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n@@ -45,6 +46,7 @@ limitations under the License.\n namespace xla {\n namespace {\n \n+using ::absl_testing::StatusIs;\n using ::testing::ElementsAre;\n \n TEST(ShapeUtilTest, GetDimensionHelperCanNegativeIndex) {\n@@ -342,12 +344,21 @@ TEST(ShapeUtilTest, ByteSizeOfWithoutPadding) {\n   EXPECT_EQ(1600, ShapeUtil::ByteSizeOf(ShapeUtil::MakeShape(C64, {10, 20})));\n }\n \n-TEST(ShapeUtilTest, ByteStrides) {\n+TEST(ShapeUtilTest, UnpackedByteStrides) {\n   Shape shape1 = ShapeUtil::MakeShape(F32, {3, 5, 7});\n   Shape shape2 = ShapeUtil::MakeShape(F16, {5, 7, 9});\n+  Shape shape3 = ShapeUtil::MakeShape(S4, {2, 4});\n+  shape3.mutable_layout()->set_element_size_in_bits(4);\n \n-  EXPECT_THAT(*ShapeUtil::ByteStrides(shape1), ElementsAre(140, 28, 4));\n-  EXPECT_THAT(*ShapeUtil::ByteStrides(shape2), ElementsAre(126, 18, 2));\n+  EXPECT_THAT(*ShapeUtil::UnpackedByteStrides(shape1), ElementsAre(140, 28, 4));\n+  EXPECT_THAT(*ShapeUtil::UnpackedByteStrides(shape2), ElementsAre(126, 18, 2));\n+  EXPECT_THAT(*ShapeUtil::UnpackedByteStrides(shape3), ElementsAre(4, 1));\n+}\n+\n+TEST(ShapeUtilTest, ByteStridesFailsOnFractionalElementSize) {\n+  Shape shape = ShapeUtil::MakeShape(S4, {10, 20});\n+  shape.mutable_layout()->set_element_size_in_bits(4);\n+  EXPECT_THAT(ShapeUtil::ByteStrides(shape), std::nullopt);\n }\n \n TEST(ShapeUtilTest, NilShape) {"
        }
    ],
    "stats": {
        "total": 161,
        "additions": 105,
        "deletions": 56
    }
}