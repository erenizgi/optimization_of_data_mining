{
    "author": "tensorflower-gardener",
    "message": "Use `YnnThreadpool`\n\nThis change moves `YnnThreadpool` to the runtime/ynnpack/ subfolder, and changes the runtime to use our custom YnnThreadpool, instead of using a thread pool created by `ynn_create_threadpool`.\n\nPiperOrigin-RevId: 822883993",
    "sha": "72bc89a5c661dc5213fed964a45e5cb27b394f4e",
    "files": [
        {
            "sha": "f9ae04240a10b56a4190b5371ec2dc10c373b5f0",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=72bc89a5c661dc5213fed964a45e5cb27b394f4e",
            "patch": "@@ -226,32 +226,6 @@ xla_cc_test(\n     ],\n )\n \n-xla_cc_test(\n-    name = \"ynn_threadpool_test\",\n-    srcs = [\"ynn_threadpool_test.cc\"],\n-    deps = [\n-        \":ynn_threadpool\",\n-        \"//xla/tsl/platform:env\",\n-        \"@com_google_googletest//:gtest_main\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"ynn_threadpool\",\n-    srcs = [\"ynn_threadpool.cc\"],\n-    hdrs = [\"ynn_threadpool.h\"],\n-    deps = [\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/base:core_headers\",\n-        \"@com_google_absl//absl/container:fixed_array\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/synchronization\",\n-        \"@eigen_archive//:eigen3\",\n-        \"@slinky//slinky/base:thread_pool\",\n-    ],\n-)\n-\n cc_library(\n     name = \"ynn_support\",\n     srcs = [\"ynn_support.cc\"],"
        },
        {
            "sha": "f8c1a24bf05d67f39cd0d013b4db8a3dd542e1e3",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/BUILD",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD?ref=72bc89a5c661dc5213fed964a45e5cb27b394f4e",
            "patch": "@@ -32,14 +32,31 @@ cc_library(\n \n cc_library(\n     name = \"ynn_threadpool\",\n-    srcs = [\"ynn_threadpool.cc\"],\n+    srcs = [\"ynn_threadpool_impl.cc\"],\n     hdrs = [\"ynn_threadpool.h\"],\n     deps = [\n         \":ynn_interop\",\n         \"@XNNPACK//ynnpack:ynnpack_h\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:fixed_array\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/synchronization\",\n         \"@eigen_archive//:eigen3\",\n+        \"@slinky//slinky/base:thread_pool\",\n+    ],\n+)\n+\n+ynn_cc_test(\n+    name = \"ynn_threadpool_test\",\n+    srcs = [\"ynn_threadpool_test.cc\"],\n+    deps = [\n+        \":ynn_threadpool\",\n+        \"//xla/tsl/platform:env\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@slinky//slinky/base:thread_pool\",\n     ],\n )\n "
        },
        {
            "sha": "1736bbbde2c3165e9e6ee68e6148905d8580f82f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_threadpool.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 62,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f4d0271cb90cb723c513e073913c666c9e7b6993/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f4d0271cb90cb723c513e073913c666c9e7b6993/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool.cc?ref=f4d0271cb90cb723c513e073913c666c9e7b6993",
            "patch": "@@ -1,62 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n-\n-#include <cstdint>\n-\n-#include \"ynnpack/include/ynnpack.h\"\n-#include \"absl/base/optimization.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n-\n-#define EIGEN_USE_THREADS\n-#include \"Eigen/ThreadPool\"\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-\n-namespace xla::cpu {\n-\n-static int32_t NumThreads(void* pool) {\n-  if (ABSL_PREDICT_FALSE(pool == nullptr)) {\n-    return 0;\n-  }\n-  return reinterpret_cast<Eigen::ThreadPoolInterface*>(pool)->NumThreads();\n-}\n-\n-static void Schedule(void* pool, void* context, void (*task)(void* context)) {\n-  if (ABSL_PREDICT_FALSE(pool == nullptr)) {\n-    (*task)(context);\n-  }\n-  reinterpret_cast<Eigen::ThreadPoolInterface*>(pool)->Schedule(\n-      [task, context]() { (*task)(context); });\n-}\n-\n-// An adaptor from Eigen::ThreadPoolInterface to xnn_threadpool_t.\n-static constexpr ynn_scheduler kYnnScheduler = {&NumThreads, &Schedule};\n-\n-absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n-    Eigen::ThreadPoolInterface* threadpool) {\n-  return CreateYnnThreadpool([&](ynn_threadpool_t* ynn_threadpool) {\n-    return ynn_create_threadpool(&kYnnScheduler, threadpool, /*flags=*/1,\n-                                 ynn_threadpool);\n-  });\n-}\n-\n-absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n-    const Eigen::ThreadPoolDevice* device) {\n-  return CreateYnnThreadpool(device->getPool());\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "6a604c3cb4372a4c7d705ecc772f023f0f78c5fc",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_threadpool_impl.cc",
            "status": "renamed",
            "additions": 79,
            "deletions": 38,
            "changes": 117,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool_impl.cc?ref=72bc89a5c661dc5213fed964a45e5cb27b394f4e",
            "patch": "@@ -13,8 +13,6 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/ynn_threadpool.h\"\n-\n #include <algorithm>\n #include <atomic>\n #include <cassert>\n@@ -25,6 +23,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"ynnpack/include/ynnpack.h\"\n #include \"absl/algorithm/container.h\"\n #include \"absl/base/optimization.h\"\n #include \"absl/base/thread_annotations.h\"\n@@ -35,19 +34,47 @@ limitations under the License.\n #include \"slinky/base/function_ref.h\"\n #include \"slinky/base/ref_count.h\"\n #include \"slinky/base/thread_pool.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n \n #define EIGEN_USE_THREADS\n #include \"Eigen/ThreadPool\"\n #include \"unsupported/Eigen/CXX11/Tensor\"\n \n namespace xla::cpu {\n \n+namespace {\n+\n+// This is an implementation of slinky::thread_pool, using absl::Mutex for\n+// synchronization, and dispatches work to Eigen::ThreadPoolInterface.\n+class YnnThreadpoolImpl final : public slinky::thread_pool {\n+ public:\n+  explicit YnnThreadpoolImpl(Eigen::ThreadPoolDevice* device);\n+  explicit YnnThreadpoolImpl(Eigen::ThreadPoolInterface* threadpool);\n+  ~YnnThreadpoolImpl() final;\n+\n+  YnnThreadpoolImpl(YnnThreadpoolImpl&&) = delete;\n+  YnnThreadpoolImpl& operator=(YnnThreadpoolImpl&&) = delete;\n+\n+  slinky::ref_count<task> enqueue(size_t n, task_body t,\n+                                  int32_t max_workers) final;\n+\n+  void wait_for(task* t) final;\n+  void wait_for(predicate_ref condition) final;\n+\n+  void atomic_call(slinky::function_ref<void()> t) final;\n+\n+  int thread_count() const final;\n+\n+ private:\n+  class impl;\n+  slinky::ref_count<impl> impl_;\n+};\n+\n //===----------------------------------------------------------------------===//\n // work_queue\n //===----------------------------------------------------------------------===//\n \n-namespace {\n-\n // Forward declare.\n class worker;\n \n@@ -199,9 +226,9 @@ namespace {\n // completed the task.\n enum class task_state { kPending, kComplete, kDone };\n \n-class task_impl final : public YnnThreadpool::task {\n+class task_impl final : public YnnThreadpoolImpl::task {\n  public:\n-  task_impl(YnnThreadpool::task_body body, size_t num_work_items,\n+  task_impl(YnnThreadpoolImpl::task_body body, size_t num_work_items,\n             size_t num_partitions);\n \n   // Runs this task by process work items in the current thread.\n@@ -212,16 +239,14 @@ class task_impl final : public YnnThreadpool::task {\n   bool done() const final;\n \n  private:\n-  YnnThreadpool::task_body body_;\n+  YnnThreadpoolImpl::task_body body_;\n   work_queue work_queue_;\n \n   ABSL_CACHELINE_ALIGNED std::atomic<size_t> worker_index_;\n   ABSL_CACHELINE_ALIGNED std::atomic<size_t> pending_work_items_;\n };\n \n-}  // namespace\n-\n-task_impl::task_impl(YnnThreadpool::task_body body, size_t num_work_items,\n+task_impl::task_impl(YnnThreadpoolImpl::task_body body, size_t num_work_items,\n                      size_t num_partitions)\n     : body_(std::move(body)),\n       work_queue_(num_work_items, num_partitions),\n@@ -241,7 +266,7 @@ task_state task_impl::run() {\n   size_t num_processed_work_items = 0;\n \n   if (std::optional<size_t> item = w.pop_work_item()) {\n-    YnnThreadpool::task_body body = body_;\n+    YnnThreadpoolImpl::task_body body = body_;\n \n     do {\n       body(*item);\n@@ -275,14 +300,14 @@ bool task_impl::done() const {\n }\n \n //===----------------------------------------------------------------------===//\n-// YnnThreadpool::impl\n+// YnnThreadpoolImpl::impl\n //===----------------------------------------------------------------------===//\n \n // We keep a stack of tasks that are currently being processed by current\n // thread, to avoid recursive calls.\n static thread_local std::vector<const task_impl*> task_stack;  // NOLINT\n \n-class YnnThreadpool::impl : public slinky::ref_counted<impl> {\n+class YnnThreadpoolImpl::impl : public slinky::ref_counted<impl> {\n  public:\n   explicit impl(Eigen::ThreadPoolInterface* threadpool);\n \n@@ -293,7 +318,7 @@ class YnnThreadpool::impl : public slinky::ref_counted<impl> {\n   void work_on_tasks(const absl::Condition& condition);\n \n   // Enqueues a new task into the queue and returns a reference to it.\n-  slinky::ref_count<task_impl> enqueue(YnnThreadpool::task_body body,\n+  slinky::ref_count<task_impl> enqueue(YnnThreadpoolImpl::task_body body,\n                                        size_t num_work_items,\n                                        size_t num_partitions);\n \n@@ -352,12 +377,12 @@ class YnnThreadpool::impl : public slinky::ref_counted<impl> {\n   ABSL_CACHELINE_ALIGNED absl::Mutex waiter_mutex_;\n };\n \n-YnnThreadpool::impl::impl(Eigen::ThreadPoolInterface* threadpool)\n+YnnThreadpoolImpl::impl::impl(Eigen::ThreadPoolInterface* threadpool)\n     : threadpool_(threadpool),\n       thread_count_(threadpool_ ? threadpool_->NumThreads() : 0) {}\n \n-slinky::ref_count<task_impl> YnnThreadpool::impl::enqueue(\n-    YnnThreadpool::task_body body, size_t num_work_items,\n+slinky::ref_count<task_impl> YnnThreadpoolImpl::impl::enqueue(\n+    YnnThreadpoolImpl::task_body body, size_t num_work_items,\n     size_t num_partitions) {\n   slinky::ref_count<task_impl> task(\n       new task_impl(std::move(body), num_work_items, num_partitions));\n@@ -366,7 +391,7 @@ slinky::ref_count<task_impl> YnnThreadpool::impl::enqueue(\n   return tasks_.emplace_back(std::move(task));\n }\n \n-slinky::ref_count<task_impl> YnnThreadpool::impl::dequeue() {\n+slinky::ref_count<task_impl> YnnThreadpoolImpl::impl::dequeue() {\n   absl::MutexLock lock(tasks_mutex_);\n \n   for (auto i = tasks_.begin(); i != tasks_.end();) {\n@@ -390,7 +415,7 @@ slinky::ref_count<task_impl> YnnThreadpool::impl::dequeue() {\n   return nullptr;\n }\n \n-task_state YnnThreadpool::impl::work_on_task(task_impl* task) {\n+task_state YnnThreadpoolImpl::impl::work_on_task(task_impl* task) {\n   DCHECK(absl::c_find(task_stack, task) == task_stack.end());\n \n   task_stack.push_back(task);\n@@ -408,7 +433,7 @@ task_state YnnThreadpool::impl::work_on_task(task_impl* task) {\n   return state;\n }\n \n-void YnnThreadpool::impl::work_on_tasks(const absl::Condition& condition) {\n+void YnnThreadpoolImpl::impl::work_on_tasks(const absl::Condition& condition) {\n   while (slinky::ref_count<task_impl> task = dequeue()) {\n     work_on_task(&*task);\n \n@@ -418,30 +443,30 @@ void YnnThreadpool::impl::work_on_tasks(const absl::Condition& condition) {\n   }\n }\n \n-void YnnThreadpool::impl::await(const absl::Condition& condition) {\n+void YnnThreadpoolImpl::impl::await(const absl::Condition& condition) {\n   if (ABSL_PREDICT_FALSE(!condition.Eval())) {\n     absl::MutexLock lock(waiter_mutex_);\n     waiter_mutex_.Await(condition);\n   }\n }\n \n-void YnnThreadpool::impl::signal_waiters() {\n+void YnnThreadpoolImpl::impl::signal_waiters() {\n   absl::MutexLock lock(waiter_mutex_);\n }\n \n-void YnnThreadpool::impl::atomic_call(slinky::function_ref<void()> t) {\n+void YnnThreadpoolImpl::impl::atomic_call(slinky::function_ref<void()> t) {\n   absl::MutexLock lock(waiter_mutex_);\n   t();\n }\n \n-bool YnnThreadpool::impl::can_schedule_workers() const {\n-  // One reference is owned by the parent YnnThreadpool, every other\n+bool YnnThreadpoolImpl::impl::can_schedule_workers() const {\n+  // One reference is owned by the parent YnnThreadpoolImpl, every other\n   // reference is owned by a worker scheduled into the underlying scheduler.\n   return ref_count() < 1 + thread_count();\n }\n \n-void YnnThreadpool::impl::schedule_workers(int64_t num_workers,\n-                                           slinky::ref_count<task_impl> task) {\n+void YnnThreadpoolImpl::impl::schedule_workers(\n+    int64_t num_workers, slinky::ref_count<task_impl> task) {\n   if (ABSL_PREDICT_TRUE(num_workers > 0 && can_schedule_workers())) {\n     slinky::ref_count<schedule_state> state(\n         new schedule_state(num_workers - 1, std::move(task), {this}));\n@@ -452,7 +477,7 @@ void YnnThreadpool::impl::schedule_workers(int64_t num_workers,\n }\n \n template <bool release_impl_ref>\n-void YnnThreadpool::impl::schedule_workers(schedule_state* context) {\n+void YnnThreadpoolImpl::impl::schedule_workers(schedule_state* context) {\n   auto state = slinky::ref_count<schedule_state>::assume(context);\n \n   // We recursively keep scheduling workers into the underlying scheduler.\n@@ -476,7 +501,7 @@ void YnnThreadpool::impl::schedule_workers(schedule_state* context) {\n     state->impl->add_ref();\n     state->impl->threadpool_->Schedule(\n         [state = slinky::ref_count<schedule_state>(state).take()]() {\n-          YnnThreadpool::impl::schedule_workers</*release_impl_ref=*/true>(\n+          YnnThreadpoolImpl::impl::schedule_workers</*release_impl_ref=*/true>(\n               state);\n         });\n   }\n@@ -493,18 +518,18 @@ void YnnThreadpool::impl::schedule_workers(schedule_state* context) {\n }\n \n //===----------------------------------------------------------------------===//\n-// YnnThreadpool\n+// YnnThreadpoolImpl\n //===----------------------------------------------------------------------===//\n \n-YnnThreadpool::YnnThreadpool(Eigen::ThreadPoolDevice* device)\n+YnnThreadpoolImpl::YnnThreadpoolImpl(Eigen::ThreadPoolDevice* device)\n     : impl_(new impl(device ? device->getPool() : nullptr)) {}\n \n-YnnThreadpool::YnnThreadpool(Eigen::ThreadPoolInterface* threadpool)\n+YnnThreadpoolImpl::YnnThreadpoolImpl(Eigen::ThreadPoolInterface* threadpool)\n     : impl_(new impl(threadpool)) {}\n \n-YnnThreadpool::~YnnThreadpool() = default;\n+YnnThreadpoolImpl::~YnnThreadpoolImpl() = default;\n \n-slinky::ref_count<YnnThreadpool::task> YnnThreadpool::enqueue(\n+slinky::ref_count<YnnThreadpoolImpl::task> YnnThreadpoolImpl::enqueue(\n     size_t n, task_body t, int32_t max_workers) {\n   CHECK_GE(max_workers, n);\n \n@@ -528,7 +553,7 @@ slinky::ref_count<YnnThreadpool::task> YnnThreadpool::enqueue(\n   return task;\n }\n \n-void YnnThreadpool::wait_for(task* t) {\n+void YnnThreadpoolImpl::wait_for(task* t) {\n   task_impl* task = static_cast<task_impl*>(t);\n   task_state state = impl_->work_on_task(task);\n \n@@ -544,15 +569,31 @@ void YnnThreadpool::wait_for(task* t) {\n   impl_->await(absl::Condition(task, &task_impl::done));\n }\n \n-void YnnThreadpool::wait_for(predicate_ref condition) {\n+void YnnThreadpoolImpl::wait_for(predicate_ref condition) {\n   impl_->work_on_tasks(absl::Condition(&condition));\n   impl_->await(absl::Condition(&condition));\n }\n \n-void YnnThreadpool::atomic_call(slinky::function_ref<void()> t) {\n+void YnnThreadpoolImpl::atomic_call(slinky::function_ref<void()> t) {\n   impl_->atomic_call(t);\n }\n \n-int YnnThreadpool::thread_count() const { return impl_->thread_count(); }\n+int YnnThreadpoolImpl::thread_count() const { return impl_->thread_count(); }\n+\n+}  // namespace\n+\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    Eigen::ThreadPoolInterface* threadpool) {\n+  return CreateYnnThreadpool([&](ynn_threadpool_t* ynn_threadpool) {\n+    *ynn_threadpool =\n+        reinterpret_cast<ynn_threadpool_t>(new YnnThreadpoolImpl(threadpool));\n+    return ynn_status_success;\n+  });\n+}\n+\n+absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n+    const Eigen::ThreadPoolDevice* device) {\n+  return CreateYnnThreadpool(device->getPool());\n+}\n \n }  // namespace xla::cpu",
            "previous_filename": "third_party/xla/xla/backends/cpu/ynn_threadpool.cc"
        },
        {
            "sha": "8d2da82d7255009c51f3e735c9e5f4be3be04ef8",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_threadpool_test.cc",
            "status": "renamed",
            "additions": 31,
            "deletions": 19,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/72bc89a5c661dc5213fed964a45e5cb27b394f4e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_threadpool_test.cc?ref=72bc89a5c661dc5213fed964a45e5cb27b394f4e",
            "patch": "@@ -13,15 +13,15 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/ynn_threadpool.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_threadpool.h\"\n \n #include <array>\n #include <atomic>\n #include <cstddef>\n #include <cstdint>\n-#include <vector>\n \n #include <gtest/gtest.h>\n+#include \"slinky/base/thread_pool.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n \n@@ -31,65 +31,77 @@ class ThreadPoolInterface;\n \n namespace xla::cpu {\n \n-TEST(YnnThreadpool, inline_scheduling) {\n-  YnnThreadpool thread_pool(static_cast<Eigen::ThreadPoolInterface*>(nullptr));\n+TEST(YnnThreadpoolImpl, inline_scheduling) {\n+  auto ynn_threadpool =\n+      CreateYnnThreadpool(static_cast<Eigen::ThreadPoolInterface*>(nullptr));\n+  auto thread_pool =\n+      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n \n   static constexpr size_t size = 10000;\n \n   std::vector<int32_t> data(size, 0);\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool.parallel_for(size, inc);\n+  thread_pool->parallel_for(size, inc);\n \n   std::vector<int32_t> expected(size, 1);\n   EXPECT_EQ(data, expected);\n }\n \n-TEST(YnnThreadpool, single_loop) {\n+TEST(YnnThreadpoolImpl, single_loop) {\n   tsl::thread::ThreadPool test_thread_pool(tsl::Env::Default(), \"test\", 4);\n-  YnnThreadpool thread_pool(test_thread_pool.AsEigenThreadPool());\n+  auto ynn_threadpool =\n+      CreateYnnThreadpool(test_thread_pool.AsEigenThreadPool());\n+  auto thread_pool =\n+      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n \n   static constexpr size_t size = 10000;\n \n   std::vector<int32_t> data(size, 0);\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool.parallel_for(size, inc);\n+  thread_pool->parallel_for(size, inc);\n \n   std::vector<int32_t> expected(size, 1);\n   EXPECT_EQ(data, expected);\n }\n \n-TEST(YnnThreadpool, loop_chain) {\n+TEST(YnnThreadpoolImpl, loop_chain) {\n   tsl::thread::ThreadPool test_thread_pool(tsl::Env::Default(), \"test\", 4);\n-  YnnThreadpool thread_pool(test_thread_pool.AsEigenThreadPool());\n+  auto ynn_threadpool =\n+      CreateYnnThreadpool(test_thread_pool.AsEigenThreadPool());\n+  auto thread_pool =\n+      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n \n   static constexpr size_t size = 10000;\n \n   std::vector<int32_t> data(size, 0);\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool.parallel_for(size, inc);\n-  thread_pool.parallel_for(size, inc);\n-  thread_pool.parallel_for(size, inc);\n-  thread_pool.parallel_for(size, inc);\n-  thread_pool.parallel_for(size, inc);\n+  thread_pool->parallel_for(size, inc);\n+  thread_pool->parallel_for(size, inc);\n+  thread_pool->parallel_for(size, inc);\n+  thread_pool->parallel_for(size, inc);\n+  thread_pool->parallel_for(size, inc);\n \n   std::vector<int32_t> expected(size, 5);\n   EXPECT_EQ(data, expected);\n }\n \n-TEST(YnnThreadpool, nested_loops) {\n+TEST(YnnThreadpoolImpl, nested_loops) {\n   tsl::thread::ThreadPool test_thread_pool(tsl::Env::Default(), \"test\", 4);\n-  YnnThreadpool thread_pool(test_thread_pool.AsEigenThreadPool());\n+  auto ynn_threadpool =\n+      CreateYnnThreadpool(test_thread_pool.AsEigenThreadPool());\n+  auto thread_pool =\n+      reinterpret_cast<slinky::thread_pool*>(ynn_threadpool->get());\n \n   static constexpr size_t size = 100;\n \n   std::array<std::atomic<int32_t>, size> data = {{0}};\n   auto inc = [&](size_t i) { data[i]++; };\n \n-  thread_pool.parallel_for(\n-      size, [&](size_t i) { thread_pool.parallel_for(size, inc); });\n+  thread_pool->parallel_for(\n+      size, [&](size_t i) { thread_pool->parallel_for(size, inc); });\n \n   for (size_t i = 0; i < size; ++i) {\n     EXPECT_EQ(data[i], size);",
            "previous_filename": "third_party/xla/xla/backends/cpu/ynn_threadpool_test.cc"
        },
        {
            "sha": "1d440d83f51c4a9def5ed53e1f4131347cff0cd8",
            "filename": "third_party/xla/xla/backends/cpu/ynn_threadpool.h",
            "status": "removed",
            "additions": 0,
            "deletions": 61,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f4d0271cb90cb723c513e073913c666c9e7b6993/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_threadpool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f4d0271cb90cb723c513e073913c666c9e7b6993/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_threadpool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_threadpool.h?ref=f4d0271cb90cb723c513e073913c666c9e7b6993",
            "patch": "@@ -1,61 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_YNN_THREADPOOL_H_\n-#define XLA_BACKENDS_CPU_YNN_THREADPOOL_H_\n-\n-#include <cstddef>\n-#include <cstdint>\n-\n-#include \"slinky/base/function_ref.h\"\n-#include \"slinky/base/ref_count.h\"\n-#include \"slinky/base/thread_pool.h\"\n-\n-namespace Eigen {\n-struct ThreadPoolDevice;\n-class ThreadPoolInterface;\n-}  // namespace Eigen\n-\n-namespace xla::cpu {\n-\n-// This is an implementation of slinky::thread_pool, using absl::Mutex for\n-// synchronization, and dispatches work to Eigen::ThreadPoolInterface.\n-class YnnThreadpool final : public slinky::thread_pool {\n- public:\n-  explicit YnnThreadpool(Eigen::ThreadPoolDevice* device);\n-  explicit YnnThreadpool(Eigen::ThreadPoolInterface* threadpool);\n-  ~YnnThreadpool() final;\n-\n-  YnnThreadpool(YnnThreadpool&&) = delete;\n-  YnnThreadpool& operator=(YnnThreadpool&&) = delete;\n-\n-  slinky::ref_count<task> enqueue(size_t n, task_body t,\n-                                  int32_t max_workers) final;\n-\n-  void wait_for(task* t) final;\n-  void wait_for(predicate_ref condition) final;\n-\n-  void atomic_call(slinky::function_ref<void()> t) final;\n-\n-  int thread_count() const final;\n-\n- private:\n-  class impl;\n-  slinky::ref_count<impl> impl_;\n-};\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_YNN_THREADPOOL_H_"
        }
    ],
    "stats": {
        "total": 335,
        "additions": 128,
        "deletions": 207
    }
}