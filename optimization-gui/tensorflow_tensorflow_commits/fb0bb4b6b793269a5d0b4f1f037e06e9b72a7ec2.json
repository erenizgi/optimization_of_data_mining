{
    "author": "GleasonK",
    "message": "[stablehlo optim] handle bounded dynamic values in optimization pass\n\nPiperOrigin-RevId: 832335884",
    "sha": "fb0bb4b6b793269a5d0b4f1f037e06e9b72a7ec2",
    "files": [
        {
            "sha": "326267d628fab67ed0fd987d4dbabc7bd7983d8e",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 74,
            "deletions": 10,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fb0bb4b6b793269a5d0b4f1f037e06e9b72a7ec2/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fb0bb4b6b793269a5d0b4f1f037e06e9b72a7ec2/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=fb0bb4b6b793269a5d0b4f1f037e06e9b72a7ec2",
            "patch": "@@ -621,7 +621,40 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplific\n  }\n  \n  // -----\n-@@ -976,6 +984,26 @@\n+@@ -120,6 +128,16 @@\n+   return %7 : tensor<3x2x3x3xi32>\n+ }\n+ \n++// CHECK-LABEL: func.func @broadcast_in_dim_nested_bounded\n++func.func @broadcast_in_dim_nested_bounded(%arg0: tensor<3x3xi32>, %arg1: tensor<i32>) -> tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>> {\n++  // CHECK: [[SDS:%.+]] = stablehlo.set_dimension_size\n++  // CHECK-NEXT: stablehlo.broadcast_in_dim [[SDS]], dims = [2, 0] : (tensor<?x3xi32, #stablehlo.bounds<3, ?>>) -> tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>>\n++  %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<3x3xi32>, tensor<i32>) -> tensor<?x3xi32, #stablehlo.bounds<3, ?>>\n++  %1 = stablehlo.broadcast_in_dim %0, dims = [1, 0] : (tensor<?x3xi32, #stablehlo.bounds<3, ?>>) -> tensor<3x?x2xi32, #stablehlo.bounds<?, 3, ?>>\n++  %2 = stablehlo.broadcast_in_dim %1, dims = [0, 2, 1] : (tensor<3x?x2xi32, #stablehlo.bounds<?, 3, ?>>) -> tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>>\n++  return %2 : tensor<3x2x?x3xi32, #stablehlo.bounds<?, ?, 3, ?>>\n++}\n++\n+ // CHECK-LABEL: func.func @broadcast_in_dim_reshape\n+ // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x6xi32>)\n+ func.func @broadcast_in_dim_reshape(%arg0: tensor<3x6xi32>)\n+@@ -132,6 +150,15 @@\n+ \n+   // CHECK-NEXT: return [[R0]], [[R5]]\n+   return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>\n++}\n++\n++// CHECK-LABEL: func.func @broadcast_in_dim_bounded_no_reshape\n++func.func @broadcast_in_dim_bounded_no_reshape(%arg0: tensor<20xf32>, %arg1: tensor<i32>) -> tensor<1x?xf32, #stablehlo.bounds<?, 20>> {\n++  %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<20xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<20>>\n++  // CHECK: stablehlo.set_dimension_size\n++  // CHECK-NEXT: stablehlo.broadcast_in_dim\n++  %1 = stablehlo.broadcast_in_dim %0, dims = [1] : (tensor<?xf32, #stablehlo.bounds<20>>) -> tensor<1x?xf32, #stablehlo.bounds<?, 20>>\n++  return %1 : tensor<1x?xf32, #stablehlo.bounds<?, 20>>\n+ }\n+ \n+ // CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape\n+@@ -976,6 +1003,26 @@\n    // CHECK-NOT: stablehlo.constant\n    // CHECK: return %arg0 : tensor<f32>\n    return %0 : tensor<f32>\n@@ -1198,7 +1231,17 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n-@@ -134,6 +134,8 @@\n+@@ -44,7 +44,8 @@\n+     \"same number of elements\">;\n+ \n+ def BroadcastNotReducibleToReshape : Constraint<\n+-    CPred<\"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && \"\n++    CPred<\"!llvm::cast<ShapedType>($0.getType()).hasStaticShape() || \"\n++          \"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && \"\n+           \"!(\"\n+             \"llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && \"\n+             \"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\"\n+@@ -134,6 +135,8 @@\n  \n  def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n  \n@@ -1207,7 +1250,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  def StableHLO_ConvertOpWithShape : NativeCodeCall<\n      \"stablehlo::ConvertOp::create($_builder, $_loc, $0.getType(), $1)\">;\n  \n-@@ -149,8 +151,9 @@\n+@@ -149,8 +152,9 @@\n  // op(cst, X) -> op(X, cst)\n  class CanonicalizeConstantToRhs<Op StableHLO_OpType>\n    : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n@@ -1219,7 +1262,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  \n  ////////\n  // AddOp\n-@@ -161,8 +164,9 @@\n+@@ -161,8 +165,9 @@\n  \n  // Pattern: add(X, 0) -> X\n  def AddOp_RemoveNoop\n@@ -1231,7 +1274,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  \n  ////////\n  // AndOp\n-@@ -173,13 +177,15 @@\n+@@ -173,13 +178,15 @@\n  \n  // Pattern: and(X, 0) -> 0\n  def AndOp_FoldToZero\n@@ -1251,7 +1294,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  \n  ////////\n  // BroadcastInDimOp\n-@@ -188,7 +194,8 @@\n+@@ -188,7 +195,8 @@\n  def BroadcastInDimOp_RemoveNoop\n    : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n          (replaceWithValue $operand),\n@@ -1261,7 +1304,28 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  \n  // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])\n  //       -> broadcast_in_dim(X, merge(dimsA, dimsB))\n-@@ -254,7 +261,8 @@\n+@@ -203,8 +211,10 @@\n+ \n+ // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])\n+ //          [if same numel]\n++// TODO: Figure out if static extents matching is valid (i.e. <=10 -> 1x[<=10])\n++// for bounded dynamism, same for BroadcastInDimOp_ReplaceWithReshape\n+ def BroadcastInDimOp_ReplaceWithReshape\n+-  : Pat<(StableHLO_BroadcastInDimOp:$op $operand, SortedDims:$dims),\n++  : Pat<(StableHLO_BroadcastInDimOp:$op AnyStaticShapeTensor:$operand, SortedDims:$dims),\n+         (StableHLO_ReshapeOpWithShape $op, $operand),\n+         [(NumberOfElementsEqual $op, $operand)],\n+         [],\n+@@ -213,7 +223,7 @@\n+ // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])\n+ //          [if same numel & rank]\n+ def BroadcastInDimOp_ReplaceWithTranspose\n+-  : Pat<(StableHLO_BroadcastInDimOp:$op $operand, $dims),\n++  : Pat<(StableHLO_BroadcastInDimOp:$op AnyStaticShapeTensor:$operand, $dims),\n+         (StableHLO_TransposeOp $operand, (InvertBroadcastDims $dims)),\n+         [(NumberOfElementsEqual $op, $operand), (RankEqual $op, $operand)]>;\n+ \n+@@ -254,7 +264,8 @@\n  def ConvertOp_RemoveNoop\n    : Pat<(StableHLO_ConvertOp:$convert $operand),\n          (replaceWithValue $operand),\n@@ -1271,7 +1335,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  \n  ////////\n  // DynamicBroadcastInDimOp\n-@@ -441,13 +449,15 @@\n+@@ -441,13 +452,15 @@\n  // Multiplication by 0. This fold is not trivial for floats in presence of NaNs,\n  // so we currently only enable it for ints.\n  def MulOp_FoldToZero\n@@ -1291,7 +1355,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  \n  ////////\n  // OrOp\n-@@ -457,13 +467,15 @@\n+@@ -457,13 +470,15 @@\n  \n  // Pattern: or(X, 1) -> 1\n  def OrOp_FoldToOne\n@@ -1311,7 +1375,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp\n  \n  ////////\n  // PadOp\n-@@ -564,8 +576,9 @@\n+@@ -564,8 +579,9 @@\n  \n  // Pattern: subtract(X, 0) -> X\n  def SubtractOp_RemoveNoop"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 74,
        "deletions": 10
    }
}