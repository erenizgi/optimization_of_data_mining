{
    "author": "arfaian",
    "message": "Add fp16 data type to TFLite for use within the runtime.\n\nThis implementation is preferred over Eigen::half which is used in some places\nbut will slowly be replaced over time.\n\nPiperOrigin-RevId: 837249919",
    "sha": "304986569a459c9f8ab3b9d922249e796553e5ea",
    "files": [
        {
            "sha": "3c6900be369ebacd55e18a4de7320a92e1db8d35",
            "filename": "tensorflow/lite/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2FBUILD?ref=304986569a459c9f8ab3b9d922249e796553e5ea",
            "patch": "@@ -1043,8 +1043,8 @@ cc_test(\n         \"//tensorflow/lite/kernels:kernel_util\",\n         \"//tensorflow/lite/kernels/internal:compatibility\",\n         \"//tensorflow/lite/testing:util\",\n+        \"//tensorflow/lite/types:half\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@eigen_archive//:eigen3\",\n     ],\n )\n "
        },
        {
            "sha": "e8074f01801996371b5991b50ed5f1b68a46e1a5",
            "filename": "tensorflow/lite/interpreter_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Finterpreter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Finterpreter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Finterpreter_test.cc?ref=304986569a459c9f8ab3b9d922249e796553e5ea",
            "patch": "@@ -29,7 +29,6 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n-#include \"Eigen/Core\"  // from @eigen_archive\n #include \"tensorflow/lite/core/c/builtin_op_data.h\"\n #include \"tensorflow/lite/core/c/c_api_types.h\"\n #include \"tensorflow/lite/core/c/common.h\"\n@@ -42,6 +41,7 @@ limitations under the License.\n #include \"tensorflow/lite/kernels/kernel_util.h\"\n #include \"tensorflow/lite/string_util.h\"\n #include \"tensorflow/lite/testing/util.h\"\n+#include \"tensorflow/lite/types/half.h\"\n #include \"tensorflow/lite/util.h\"\n \n #ifdef __APPLE__\n@@ -272,7 +272,7 @@ TEST(BasicInterpreter, CheckResize) {\n   const uint8_t uint8s[] = {3, 4};\n   const int64_t int64s[] = {6, -7};\n   const int16_t int16s[] = {8, -9};\n-  const Eigen::half float16s[] = {Eigen::half(-3.f), Eigen::half(-4.f)};\n+  const half float16s[] = {half(-3.f), half(-4.f)};\n \n   struct {\n     TfLiteType type;"
        },
        {
            "sha": "c00aadb6ae46e9c67017bd3bebfe68f23f4844e8",
            "filename": "tensorflow/lite/types/BUILD",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftypes%2FBUILD?ref=304986569a459c9f8ab3b9d922249e796553e5ea",
            "patch": "@@ -0,0 +1,31 @@\n+# Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = [\"//visibility:public\"],\n+    licenses = [\"notice\"],\n+)\n+\n+cc_library(\n+    name = \"half\",\n+    hdrs = [\n+        \"bit_cast.h\",\n+        \"fp16.h\",\n+        \"half.h\",\n+    ],\n+)"
        },
        {
            "sha": "77d9772653d10dd311d256a8e78f04ab478d111f",
            "filename": "tensorflow/lite/types/bit_cast.h",
            "status": "added",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2Fbit_cast.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2Fbit_cast.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftypes%2Fbit_cast.h?ref=304986569a459c9f8ab3b9d922249e796553e5ea",
            "patch": "@@ -0,0 +1,36 @@\n+/* Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_LITE_TYPES_BIT_CAST_H_\n+#define TENSORFLOW_LITE_TYPES_BIT_CAST_H_\n+\n+#include <cstring>\n+\n+namespace tflite {\n+\n+// Unfortunately, std::bit_cast is C++20, which we can't use. More unfortunately\n+// it seems impossible to hack together a constexpr bit_cast without compiler\n+// support.\n+template <typename To, typename From>\n+To bit_cast(From x) {\n+  static_assert(sizeof(To) == sizeof(From), \"\");\n+  To result;\n+  memcpy(&result, &x, sizeof(result));\n+  return result;\n+}\n+\n+}  // namespace tflite\n+\n+#endif  // TENSORFLOW_LITE_TYPES_BIT_CAST_H_"
        },
        {
            "sha": "cc63fe7d21fbd847c76f681aa24a9ced14967c41",
            "filename": "tensorflow/lite/types/fp16.h",
            "status": "added",
            "additions": 219,
            "deletions": 0,
            "changes": 219,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2Ffp16.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2Ffp16.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftypes%2Ffp16.h?ref=304986569a459c9f8ab3b9d922249e796553e5ea",
            "patch": "@@ -0,0 +1,219 @@\n+/* Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_LITE_TYPES_FP16_H_\n+#define TENSORFLOW_LITE_TYPES_FP16_H_\n+\n+#include <stdint.h>\n+\n+// This file is an excerpt from\n+// https://github.com/Maratyszcza/FP16/blob/master/include/fp16/fp16.h,\n+// including only the minimal functionality we need in XNNPACK. This works\n+// around some issues that we haven't been able to fix upstream\n+// (https://github.com/Maratyszcza/FP16/pull/32). See also:\n+// - https://github.com/microsoft/onnxruntime/pull/22294/files\n+// - https://github.com/google/XNNPACK/issues/6989\n+// We also don't need a lot of the functionality in the upstream library.\n+\n+static inline float fp32_from_bits(uint32_t w) {\n+  union {\n+    uint32_t as_bits;\n+    float as_value;\n+  } fp32 = {w};\n+  return fp32.as_value;\n+}\n+\n+static inline uint32_t fp32_to_bits(float f) {\n+  union {\n+    float as_value;\n+    uint32_t as_bits;\n+  } fp32 = {f};\n+  return fp32.as_bits;\n+}\n+\n+/*\n+ * Convert a 16-bit floating-point number in IEEE half-precision format, in bit\n+ * representation, to a 32-bit floating-point number in IEEE single-precision\n+ * format.\n+ *\n+ * @note The implementation relies on IEEE-like (no assumption about rounding\n+ * mode and no operations on denormals) floating-point operations and bitcasts\n+ * between integer and floating-point variables.\n+ */\n+static inline float fp16_ieee_to_fp32_value(uint16_t h) {\n+  /*\n+   * Extend the half-precision floating-point number to 32 bits and shift to the\n+   * upper part of the 32-bit word:\n+   *      +---+-----+------------+-------------------+\n+   *      | S |EEEEE|MM MMMM MMMM|0000 0000 0000 0000|\n+   *      +---+-----+------------+-------------------+\n+   * Bits  31  26-30    16-25            0-15\n+   *\n+   * S - sign bit, E - bits of the biased exponent, M - bits of the mantissa, 0\n+   * - zero bits.\n+   */\n+  const uint32_t w = (uint32_t)h << 16;\n+  /*\n+   * Extract the sign of the input number into the high bit of the 32-bit word:\n+   *\n+   *      +---+----------------------------------+\n+   *      | S |0000000 00000000 00000000 00000000|\n+   *      +---+----------------------------------+\n+   * Bits  31                 0-31\n+   */\n+  const uint32_t sign = w & UINT32_C(0x80000000);\n+  /*\n+   * Extract mantissa and biased exponent of the input number into the high bits\n+   * of the 32-bit word:\n+   *\n+   *      +-----+------------+---------------------+\n+   *      |EEEEE|MM MMMM MMMM|0 0000 0000 0000 0000|\n+   *      +-----+------------+---------------------+\n+   * Bits  27-31    17-26            0-16\n+   */\n+  const uint32_t two_w = w + w;\n+\n+  /*\n+   * Shift mantissa and exponent into bits 23-28 and bits 13-22 so they become\n+   * mantissa and exponent of a single-precision floating-point number:\n+   *\n+   *       S|Exponent |          Mantissa\n+   *      +-+---+-----+------------+----------------+\n+   *      |0|000|EEEEE|MM MMMM MMMM|0 0000 0000 0000|\n+   *      +-+---+-----+------------+----------------+\n+   * Bits   | 23-31   |           0-22\n+   *\n+   * Next, there are some adjustments to the exponent:\n+   * - The exponent needs to be corrected by the difference in exponent bias\n+   * between single-precision and half-precision formats (0x7F - 0xF = 0x70)\n+   * - Inf and NaN values in the inputs should become Inf and NaN values after\n+   * conversion to the single-precision number. Therefore, if the biased\n+   * exponent of the half-precision input was 0x1F (max possible value), the\n+   * biased exponent of the single-precision output must be 0xFF (max possible\n+   * value). We do this correction in two steps:\n+   *   - First, we adjust the exponent by (0xFF - 0x1F) = 0xE0 (see exp_offset\n+   * below) rather than by 0x70 suggested by the difference in the exponent bias\n+   * (see above).\n+   *   - Then we multiply the single-precision result of exponent adjustment by\n+   * 2**(-112) to reverse the effect of exponent adjustment by 0xE0 less the\n+   * necessary exponent adjustment by 0x70 due to difference in exponent bias.\n+   *     The floating-point multiplication hardware would ensure than Inf and\n+   * NaN would retain their value on at least partially IEEE754-compliant\n+   * implementations.\n+   *\n+   * Note that the above operations do not handle denormal inputs (where biased\n+   * exponent == 0). However, they also do not operate on denormal inputs, and\n+   * do not produce denormal results.\n+   */\n+  const uint32_t exp_offset = UINT32_C(0xE0) << 23;\n+#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) || \\\n+    defined(__GNUC__) && !defined(__STRICT_ANSI__)\n+  const float exp_scale = 0x1.0p-112f;\n+#else\n+  const float exp_scale = fp32_from_bits(UINT32_C(0x7800000));\n+#endif\n+  const float normalized_value =\n+      fp32_from_bits((two_w >> 4) + exp_offset) * exp_scale;\n+\n+  /*\n+   * Convert denormalized half-precision inputs into single-precision results\n+   * (always normalized). Zero inputs are also handled here.\n+   *\n+   * In a denormalized number the biased exponent is zero, and mantissa has\n+   * on-zero bits. First, we shift mantissa into bits 0-9 of the 32-bit word.\n+   *\n+   *                  zeros           |  mantissa\n+   *      +---------------------------+------------+\n+   *      |0000 0000 0000 0000 0000 00|MM MMMM MMMM|\n+   *      +---------------------------+------------+\n+   * Bits             10-31                0-9\n+   *\n+   * Now, remember that denormalized half-precision numbers are represented as:\n+   *    FP16 = mantissa * 2**(-24).\n+   * The trick is to construct a normalized single-precision number with the\n+   * same mantissa and thehalf-precision input and with an exponent which would\n+   * scale the corresponding mantissa bits to 2**(-24). A normalized\n+   * single-precision floating-point number is represented as: FP32 = (1 +\n+   * mantissa * 2**(-23)) * 2**(exponent - 127) Therefore, when the biased\n+   * exponent is 126, a unit change in the mantissa of the input denormalized\n+   * half-precision number causes a change of the constructud single-precision\n+   * number by 2**(-24), i.e. the same ammount.\n+   *\n+   * The last step is to adjust the bias of the constructed single-precision\n+   * number. When the input half-precision number is zero, the constructed\n+   * single-precision number has the value of FP32 = 1 * 2**(126 - 127) =\n+   * 2**(-1) = 0.5 Therefore, we need to subtract 0.5 from the constructed\n+   * single-precision number to get the numerical equivalent of the input\n+   * half-precision number.\n+   */\n+  const uint32_t magic_mask = UINT32_C(126) << 23;\n+  const float magic_bias = 0.5f;\n+  const float denormalized_value =\n+      fp32_from_bits((two_w >> 17) | magic_mask) - magic_bias;\n+\n+  /*\n+   * - Choose either results of conversion of input as a normalized number, or\n+   * as a denormalized number, depending on the input exponent. The variable\n+   * two_w contains input exponent in bits 27-31, therefore if its smaller than\n+   * 2**27, the input is either a denormal number, or zero.\n+   * - Combine the result of conversion of exponent and mantissa with the sign\n+   * of the input number.\n+   */\n+  const uint32_t denormalized_cutoff = UINT32_C(1) << 27;\n+  const uint32_t result =\n+      sign | (two_w < denormalized_cutoff ? fp32_to_bits(denormalized_value)\n+                                          : fp32_to_bits(normalized_value));\n+  return fp32_from_bits(result);\n+}\n+\n+/*\n+ * Convert a 32-bit floating-point number in IEEE single-precision format to a\n+ * 16-bit floating-point number in IEEE half-precision format, in bit\n+ * representation.\n+ *\n+ * @note The implementation relies on IEEE-like (no assumption about rounding\n+ * mode and no operations on denormals) floating-point operations and bitcasts\n+ * between integer and floating-point variables.\n+ */\n+static inline uint16_t fp16_ieee_from_fp32_value(float f) {\n+#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) || \\\n+    defined(__GNUC__) && !defined(__STRICT_ANSI__)\n+  const float scale_to_inf = 0x1.0p+112f;\n+  const float scale_to_zero = 0x1.0p-110f;\n+#else\n+  const float scale_to_inf = fp32_from_bits(UINT32_C(0x77800000));\n+  const float scale_to_zero = fp32_from_bits(UINT32_C(0x08800000));\n+#endif\n+  const uint32_t w = fp32_to_bits(f);\n+  const float abs_f = fp32_from_bits(w & UINT32_C(0x7FFFFFFF));\n+  float base = (abs_f * scale_to_inf) * scale_to_zero;\n+\n+  const uint32_t shl1_w = w + w;\n+  const uint32_t sign = w & UINT32_C(0x80000000);\n+  uint32_t bias = shl1_w & UINT32_C(0xFF000000);\n+  if (bias < UINT32_C(0x71000000)) {\n+    bias = UINT32_C(0x71000000);\n+  }\n+\n+  base = fp32_from_bits((bias >> 1) + UINT32_C(0x07800000)) + base;\n+  const uint32_t bits = fp32_to_bits(base);\n+  const uint32_t exp_bits = (bits >> 13) & UINT32_C(0x00007C00);\n+  const uint32_t mantissa_bits = bits & UINT32_C(0x00000FFF);\n+  const uint32_t nonsign = exp_bits + mantissa_bits;\n+  return (sign >> 16) |\n+         (shl1_w > UINT32_C(0xFF000000) ? UINT16_C(0x7E00) : nonsign);\n+}\n+\n+#endif  // TENSORFLOW_LITE_TYPES_FP16_H_"
        },
        {
            "sha": "13e8662d341b2342630794e2471f4cd21e4b911d",
            "filename": "tensorflow/lite/types/half.h",
            "status": "added",
            "additions": 169,
            "deletions": 0,
            "changes": 169,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2Fhalf.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/304986569a459c9f8ab3b9d922249e796553e5ea/tensorflow%2Flite%2Ftypes%2Fhalf.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftypes%2Fhalf.h?ref=304986569a459c9f8ab3b9d922249e796553e5ea",
            "patch": "@@ -0,0 +1,169 @@\n+/* Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_LITE_TYPES_HALF_H_\n+#define TENSORFLOW_LITE_TYPES_HALF_H_\n+\n+#include <cstdint>\n+\n+// We want to use _Float16 if the compiler supports it fully, but it's\n+// tricky to do this detection; there are compiler versions that define the\n+// type in broken ways. We're only going to bother using it if the support is\n+// known to be at least a robust f16<->f32 conversion, which generally means a\n+// recent version of Clang or GCC, x86 or ARM or RISC-V architectures, and\n+// (in some cases) the right architecture flags specified on the command line.\n+\n+#ifndef TFLITE_ARCH_FLOAT16\n+\n+// Some non-GCC compilers define __GNUC__, but we only want to detect the Real\n+// Thing\n+#if defined(__GNUC__) && !defined(__clang__) && !defined(__INTEL_COMPILER) && \\\n+    !defined(__INTEL_LLVM_COMPILER)\n+#define TFLITE_GNUC_ACTUAL __GNUC__\n+#else\n+#define TFLITE_GNUC_ACTUAL 0\n+#endif\n+\n+#if (defined(__i386__) || defined(__x86_64__)) && defined(__SSE2__) && \\\n+    defined(__FLT16_MAX__) && defined(__F16C__) &&                     \\\n+    ((__clang_major__ >= 15 && !defined(_MSC_VER)) ||                  \\\n+     (TFLITE_GNUC_ACTUAL >= 12))\n+#define TFLITE_ARCH_FLOAT16 1\n+#endif\n+\n+#if ((defined(__arm__) || defined(_M_ARM) || defined(__aarch64__) || \\\n+      defined(_M_ARM64) || defined(_M_ARM64EC)) &&                   \\\n+     !defined(_MSC_VER)) &&                                          \\\n+    defined(__ARM_FEATURE_FP16_SCALAR_ARITHMETIC)\n+#define TFLITE_ARCH_FLOAT16 1\n+#endif\n+\n+#if defined(__riscv) && defined(__riscv_zvfh) && __clang__ >= 1600\n+#define TFLITE_ARCH_FLOAT16 1\n+#endif\n+\n+#ifndef TFLITE_ARCH_FLOAT16\n+#define TFLITE_ARCH_FLOAT16 0\n+#endif\n+\n+#endif  // TFLITE_ARCH_FLOAT16\n+\n+#if TFLITE_ARCH_FLOAT16\n+\n+#include <cmath>\n+\n+#include \"tensorflow/lite/types/bit_cast.h\"\n+\n+namespace tflite {\n+\n+class half {\n+ public:\n+  half() = default;\n+  constexpr half(float x) : value_(static_cast<_Float16>(x)) {}  // NOLINT\n+  constexpr half(int x)\n+      : value_(static_cast<_Float16>(static_cast<float>(x))) {}  // NOLINT\n+\n+  constexpr operator float() const { return value_; }  // NOLINT\n+\n+  static half from_bits(uint16_t bits) {\n+    half result;\n+    result.value_ = bit_cast<_Float16>(bits);\n+    return result;\n+  }\n+\n+  uint16_t to_bits() const { return bit_cast<uint16_t>(value_); }\n+\n+  bool is_zero() const { return value_ == 0.0f; }\n+\n+  // These definitions are imprecise because we want them to be constexpr, and\n+  // the various tools for doing that are not constepxr (bit_cast,\n+  // std::numeric_limits, etc.).\n+  static constexpr half epsilon() { return 0.0009765625f; }\n+  static constexpr half infinity() { return INFINITY; }\n+  static constexpr half min() { return -65504.0f; }\n+  static constexpr half max() { return 65504.0f; }\n+  static constexpr half smallest_normal() { return 0.00006103515625f; }\n+  static constexpr half min_identity() { return INFINITY; }\n+  static constexpr half max_identity() { return -INFINITY; }\n+  static constexpr half sum_identity() { return 0.0f; }\n+\n+  // Not private due to -Werror=class-memaccess, which can't be disabled:\n+  // - via a --copt, because it seems to have no effect.\n+  // - via .bazelrc, because it then applies to C code, and the compiler says\n+  //   this flag is not valid in C.\n+  _Float16 value_;\n+};\n+\n+}  // namespace tflite\n+\n+#else  // TFLITE_ARCH_FLOAT16\n+\n+#include \"tensorflow/lite/types/fp16.h\"\n+\n+namespace tflite {\n+\n+class half {\n+ private:\n+  // We need this hoop jumping to enable implementing a constexpr `from_bits`.\n+  struct zero_initializer {};\n+  explicit constexpr half(zero_initializer) : bits_(0) {}\n+\n+ public:\n+  half() = default;\n+  half(float x) : bits_(fp16_ieee_from_fp32_value(x)) {}  // NOLINT\n+  explicit half(int x)\n+      : bits_(fp16_ieee_from_fp32_value(static_cast<float>(x))) {}\n+\n+  operator float() const { return fp16_ieee_to_fp32_value(bits_); }  // NOLINT\n+\n+  static constexpr half from_bits(uint16_t bits) {\n+    half result{zero_initializer{}};\n+    result.bits_ = bits;\n+    return result;\n+  }\n+\n+  constexpr uint16_t to_bits() const { return bits_; }\n+\n+  bool is_zero() const {\n+    // Check for +/- zero (0x0000/0x8000). uint16 overflow is well defined to\n+    // wrap around.\n+    return static_cast<uint16_t>(bits_ * 2) == 0;\n+  }\n+\n+  static constexpr half epsilon() {\n+    return half::from_bits(0x1400);  // 2^-10 = 0.0009765625\n+  }\n+  static constexpr half infinity() { return from_bits(0x7c00); }\n+  static constexpr half min() { return from_bits(0xfbff); }\n+  static constexpr half max() { return from_bits(0x7bff); }\n+  static constexpr half smallest_normal() {\n+    return from_bits(0x0400);  // 2^-14\n+  }\n+  static constexpr half min_identity() { return from_bits(0x7c00); }\n+  static constexpr half max_identity() { return from_bits(0xfc00); }\n+  static constexpr half sum_identity() { return from_bits(0); }\n+\n+  // Not private due to -Werror=class-memaccess, which can't be disabled:\n+  // - via a --copt, because it seems to have no effect.\n+  // - via .bazelrc, because it then applies to C code, and the compiler says\n+  //   this flag is not valid in C.\n+  uint16_t bits_;\n+};\n+\n+}  // namespace tflite\n+\n+#endif  // TFLITE_ARCH_FLOAT16\n+\n+#endif  // TENSORFLOW_LITE_TYPES_HALF_H_"
        }
    ],
    "stats": {
        "total": 461,
        "additions": 458,
        "deletions": 3
    }
}