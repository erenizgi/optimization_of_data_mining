{
    "author": "derdrdirk",
    "message": "[Autotuner] Use fallback if default algorithms are not available for cuDNN.\n\nPiperOrigin-RevId: 806303066",
    "sha": "40b4981f6cc2f4783286c97c318dcd12e442563a",
    "files": [
        {
            "sha": "4b1f6ede66298a7e584f289de5abd165741c2072",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.cc",
            "status": "modified",
            "additions": 99,
            "deletions": 71,
            "changes": 170,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/40b4981f6cc2f4783286c97c318dcd12e442563a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/40b4981f6cc2f4783286c97c318dcd12e442563a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc?ref=40b4981f6cc2f4783286c97c318dcd12e442563a",
            "patch": "@@ -169,6 +169,83 @@ bool IsSupportedByCudnn(const HloInstruction& instr,\n   return false;\n }\n \n+absl::StatusOr<std::vector<CudnnBackendConfig>> GetAlgorithms(\n+    se::dnn::DnnSupport* dnn, se::dnn::ConvolutionKind conv_kind,\n+    se::dnn::DataType input_type, se::dnn::DataType output_type,\n+    se::Stream* stream, const GpuConvConfig& gpu_conv_config,\n+    const se::NumericOptions& numeric_options, bool use_fallback) {\n+  std::vector<std::unique_ptr<const se::dnn::ConvRunner>> conv_runners;\n+  std::vector<std::unique_ptr<const se::dnn::FusedConvRunner>>\n+      fused_conv_runners;\n+  std::vector<std::unique_ptr<const se::dnn::GraphConvRunner>>\n+      graph_conv_runners;\n+  switch (conv_kind) {\n+    case se::dnn::ConvolutionKind::FORWARD_BIAS_ACTIVATION: {\n+      if (!gpu_conv_config.fusion) {\n+        return absl::InvalidArgumentError(\n+            \"GpuConvConfig had fusion ConvolutionKind but no FusionConfig.\");\n+      }\n+      TF_RETURN_IF_ERROR(dnn->GetFusedConvolveRunners(\n+          se::dnn::ConvolutionKind::FORWARD, input_type,\n+          BiasTypeForInputType(input_type), output_type,\n+          gpu_conv_config.conv_result_scale,\n+          gpu_conv_config.fusion->side_input_scale,\n+          gpu_conv_config.fusion->leakyrelu_alpha, stream,\n+          gpu_conv_config.input_descriptor, gpu_conv_config.filter_descriptor,\n+          gpu_conv_config.bias_descriptor, gpu_conv_config.output_descriptor,\n+          gpu_conv_config.conv_desc, use_fallback, gpu_conv_config.fusion->mode,\n+          numeric_options, &fused_conv_runners));\n+      break;\n+    }\n+    case se::dnn::ConvolutionKind::FORWARD_GRAPH: {\n+      TF_RETURN_IF_ERROR(dnn->GetGraphConvolveRunners(\n+          conv_kind, input_type, output_type, stream,\n+          gpu_conv_config.input_descriptor, gpu_conv_config.filter_descriptor,\n+          gpu_conv_config.output_descriptor, gpu_conv_config.conv_desc,\n+          use_fallback, numeric_options, &graph_conv_runners,\n+          gpu_conv_config.serialized_graph));\n+      break;\n+    }\n+    case se::dnn::ConvolutionKind::FORWARD:\n+    case se::dnn::ConvolutionKind::BACKWARD_DATA:\n+    case se::dnn::ConvolutionKind::BACKWARD_FILTER: {\n+      TF_RETURN_IF_ERROR(dnn->GetConvolveRunners(\n+          conv_kind, input_type, output_type, stream,\n+          gpu_conv_config.input_descriptor,\n+          /*input_data=*/se::DeviceMemoryBase(nullptr),\n+          gpu_conv_config.filter_descriptor,\n+          /*filter_data=*/se::DeviceMemoryBase(nullptr),\n+          gpu_conv_config.output_descriptor,\n+          /*output_data=*/se::DeviceMemoryBase(nullptr),\n+          gpu_conv_config.conv_desc, use_fallback,\n+          /*scratch_allocator=*/nullptr, numeric_options, &conv_runners));\n+      break;\n+    }\n+    default:\n+      return absl::InvalidArgumentError(\n+          \"Cudnn backend doesn't support this convolution kind.\");\n+  }\n+\n+  std::vector<CudnnBackendConfig> configs;\n+  if (!conv_runners.empty()) {\n+    configs.reserve(conv_runners.size());\n+    for (const auto& runner : conv_runners) {\n+      configs.push_back(runner->ToAlgorithmDesc()->ToProto());\n+    }\n+  } else if (!fused_conv_runners.empty()) {\n+    configs.reserve(fused_conv_runners.size());\n+    for (const auto& runner : fused_conv_runners) {\n+      configs.push_back(runner->ToAlgorithmDesc()->ToProto());\n+    }\n+  } else if (!graph_conv_runners.empty()) {\n+    configs.reserve(graph_conv_runners.size());\n+    for (const auto& runner : graph_conv_runners) {\n+      configs.push_back(runner->ToAlgorithmDesc()->ToProto());\n+    }\n+  }\n+  return configs;\n+}\n+\n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n GetCudnnFusionConfigs(const HloInstruction& instr,\n                       se::StreamExecutor* stream_executor) {\n@@ -189,7 +266,6 @@ GetCudnnFusionConfigs(const HloInstruction& instr,\n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n GetConvolutionCustomCallConfigs(const HloCustomCallInstruction* instr,\n                                 se::StreamExecutor* stream_executor) {\n-  std::vector<std::unique_ptr<BackendConfig>> configs;\n   TF_ASSIGN_OR_RETURN(GpuConvConfig gpu_conv_config, GetGpuConvConfig(instr));\n   TF_ASSIGN_OR_RETURN(se::dnn::ConvolutionKind conv_kind,\n                       GetDNNConvKindFromCudnnConvKind(gpu_conv_config.kind));\n@@ -209,76 +285,28 @@ GetConvolutionCustomCallConfigs(const HloCustomCallInstruction* instr,\n       [](int precision) { return precision <= PrecisionConfig::HIGH; });\n   const se::NumericOptions numeric_options{\n       RequireDeterminism(instr->GetModule()->config()), allow_tf32};\n-  switch (conv_kind) {\n-    case se::dnn::ConvolutionKind::FORWARD_BIAS_ACTIVATION: {\n-      if (!gpu_conv_config.fusion) {\n-        return absl::InvalidArgumentError(\n-            \"GpuConvConfig had fusion ConvolutionKind but no FusionConfig.\");\n-      }\n-      std::vector<std::unique_ptr<const se::dnn::FusedConvRunner>> runners;\n-      TF_RETURN_IF_ERROR(dnn->GetFusedConvolveRunners(\n-          // This refers to the kind of convolution op inside the fusion, not\n-          // the whole fused graph.\n-          se::dnn::ConvolutionKind::FORWARD, input_type,\n-          BiasTypeForInputType(input_type), output_type,\n-          /*conv_input_scale=*/gpu_conv_config.conv_result_scale,\n-          /*side_input_scale=*/gpu_conv_config.fusion->side_input_scale,\n-          /*leakyrelu_alpha=*/gpu_conv_config.fusion->leakyrelu_alpha, stream,\n-          gpu_conv_config.input_descriptor, gpu_conv_config.filter_descriptor,\n-          gpu_conv_config.bias_descriptor, gpu_conv_config.output_descriptor,\n-          gpu_conv_config.conv_desc,\n-          /*use_fallback=*/false, gpu_conv_config.fusion->mode, numeric_options,\n-          &runners));\n-      for (const auto& runner : runners) {\n-        auto any = std::make_unique<google::protobuf::Any>();\n-        any->PackFrom(runner->ToAlgorithmDesc()->ToProto());\n-        configs.push_back(std::move(any));\n-      }\n-      return configs;\n-    }\n-    case se::dnn::ConvolutionKind::FORWARD_GRAPH: {\n-      std::vector<std::unique_ptr<const se::dnn::GraphConvRunner>> runners;\n-      // This path is cuDNN-only, where the DeviceMemoryBase arguments and the\n-      // allocator are unused; so, they're all provided as nullptr.\n-      TF_RETURN_IF_ERROR(dnn->GetGraphConvolveRunners(\n-          conv_kind, input_type, output_type, stream,\n-          gpu_conv_config.input_descriptor, gpu_conv_config.filter_descriptor,\n-          gpu_conv_config.output_descriptor, gpu_conv_config.conv_desc,\n-          /*use_fallback=*/false, numeric_options, &runners,\n-          gpu_conv_config.serialized_graph));\n-      for (const auto& runner : runners) {\n-        auto any = std::make_unique<google::protobuf::Any>();\n-        any->PackFrom(runner->ToAlgorithmDesc()->ToProto());\n-        configs.push_back(std::move(any));\n-      }\n-      return configs;\n-    }\n-    case se::dnn::ConvolutionKind::FORWARD:\n-    case se::dnn::ConvolutionKind::BACKWARD_DATA:\n-    case se::dnn::ConvolutionKind::BACKWARD_FILTER: {\n-      std::vector<std::unique_ptr<const se::dnn::ConvRunner>> runners;\n-      // This path is cuDNN-only, where the DeviceMemoryBase arguments and the\n-      // allocator are unused; so, they're all provided as nullptr.\n-      TF_RETURN_IF_ERROR(dnn->GetConvolveRunners(\n-          conv_kind, input_type, output_type, stream,\n-          gpu_conv_config.input_descriptor,\n-          /*input_data=*/se::DeviceMemoryBase(nullptr),\n-          gpu_conv_config.filter_descriptor,\n-          /*filter_data=*/se::DeviceMemoryBase(nullptr),\n-          gpu_conv_config.output_descriptor,\n-          /*output_data=*/se::DeviceMemoryBase(nullptr),\n-          gpu_conv_config.conv_desc,\n-          /*use_fallback=*/false, nullptr, numeric_options, &runners));\n-      for (const auto& runner : runners) {\n-        auto any = std::make_unique<google::protobuf::Any>();\n-        any->PackFrom(runner->ToAlgorithmDesc()->ToProto());\n-        configs.push_back(std::move(any));\n-      }\n-      return configs;\n-    }\n-    default:\n-      return absl::InvalidArgumentError(\n-          \"Cudnn backend doesn't support this convolution kind.\");\n+\n+  // Try to get algorithms without fallback first, as fallback algorithms can be\n+  // very slow.\n+  std::vector<CudnnBackendConfig> algorithm_configs;\n+  TF_ASSIGN_OR_RETURN(\n+      algorithm_configs,\n+      GetAlgorithms(dnn, conv_kind, input_type, output_type, stream,\n+                    gpu_conv_config, numeric_options, /*use_fallback=*/false));\n+\n+  if (algorithm_configs.empty()) {\n+    TF_ASSIGN_OR_RETURN(\n+        algorithm_configs,\n+        GetAlgorithms(dnn, conv_kind, input_type, output_type, stream,\n+                      gpu_conv_config, numeric_options, /*use_fallback=*/true));\n+  }\n+\n+  std::vector<std::unique_ptr<BackendConfig>> configs;\n+  configs.reserve(algorithm_configs.size());\n+  for (const auto& algorithm_config : algorithm_configs) {\n+    auto any = std::make_unique<google::protobuf::Any>();\n+    any->PackFrom(algorithm_config);\n+    configs.push_back(std::move(any));\n   }\n   return configs;\n }"
        }
    ],
    "stats": {
        "total": 170,
        "additions": 99,
        "deletions": 71
    }
}