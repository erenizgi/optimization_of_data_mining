{
    "author": "unknown",
    "message": "[XLA:GPU] Fix BuffersFloatCheckThunk breaking on multi-GPU setups\n\nEach kernel needs to be loaded on every device separately. Attempting to\nrun a kernel handle from one device on another fails with \"invalid\nhandle\" error.\n\nThunk objects are reused across all devices available locally, so they\nneed to use correct kernel. Use a map keyed with StreamExecutor* to\nachieve that.\n\nAlso, make sure to activate the CUDA context when loading a kernel from\nmemory. This happened to work when only one device was used by any\nparticular thread, but not in a test that tried to juggle 2 devices\nwithin one thread. Loading kernels from cubin/ptx already includes\nactivation deeper in the CudaExecutor::LoadKernel call stack.\n\nBug discovered when investigating NaN checker slowdown. Same issue\napplies to BuffersChecksumThunk and BuffersFloatCheckThunk.\nBuffersChecksumThunk fix will follow shortly.\n\nPiperOrigin-RevId: 830441824",
    "sha": "43aae2e9cd074c6473a8e414b56021e070b780b5",
    "files": [
        {
            "sha": "66ae8724a505225ae145a791f5cfe1d2761eea50",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=43aae2e9cd074c6473a8e414b56021e070b780b5",
            "patch": "@@ -3202,6 +3202,7 @@ cc_library(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/synchronization\",\n     ] + if_cuda_is_configured([\n         \"//xla/stream_executor/cuda:buffer_debug_float_check_kernel_cuda\",\n     ]),\n@@ -3210,6 +3211,11 @@ cc_library(\n xla_test(\n     name = \"buffers_float_check_thunk_test\",\n     srcs = [\"buffers_float_check_thunk_test.cc\"],\n+    backend_tags = {\n+        \"gpu\": [\n+            \"multi_gpu\",\n+        ],\n+    },\n     backends = [\"gpu\"],\n     tags = [\n         \"cuda-only\",\n@@ -3234,6 +3240,7 @@ xla_test(\n         \"//xla/stream_executor/gpu:buffer_debug_log\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "0347625e66556f6adfd021c6cb419f3c101e6ce5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 16,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc?ref=43aae2e9cd074c6473a8e414b56021e070b780b5",
            "patch": "@@ -16,11 +16,14 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/buffers_float_check_thunk.h\"\n \n #include <cstdint>\n+#include <memory>\n #include <string>\n+#include <utility>\n \n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -57,15 +60,23 @@ absl::Status BuffersDebugFloatCheckThunk::Initialize(\n     return absl::OkStatus();\n   }\n \n-  se::gpu::GpuKernelRegistry registry =\n-      se::gpu::GpuKernelRegistry::GetGlobalRegistry();\n-  TF_ASSIGN_OR_RETURN(\n-      kernel_f32_, registry.LoadKernel<se::gpu::BufferDebugFloatCheckF32Kernel>(\n-                       params.executor));\n-  TF_ASSIGN_OR_RETURN(\n-      kernel_bf16_,\n-      registry.LoadKernel<se::gpu::BufferDebugFloatCheckBf16Kernel>(\n-          params.executor));\n+  {\n+    absl::MutexLock lock(kernels_mutex_);\n+    if (!kernels_.contains(params.executor)) {\n+      se::gpu::GpuKernelRegistry registry =\n+          se::gpu::GpuKernelRegistry::GetGlobalRegistry();\n+      TF_ASSIGN_OR_RETURN(\n+          auto kernel_f32,\n+          registry.LoadKernel<se::gpu::BufferDebugFloatCheckF32Kernel>(\n+              params.executor));\n+      TF_ASSIGN_OR_RETURN(\n+          auto kernel_bf16,\n+          registry.LoadKernel<se::gpu::BufferDebugFloatCheckBf16Kernel>(\n+              params.executor));\n+      kernels_[params.executor] = std::make_unique<Kernels>(\n+          Kernels{std::move(kernel_f32), std::move(kernel_bf16)});\n+    }\n+  }\n \n   VLOG(1) << \"FloatCheck kernel loaded\";\n   return absl::OkStatus();\n@@ -74,11 +85,19 @@ absl::Status BuffersDebugFloatCheckThunk::Initialize(\n absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   se::StreamExecutor* executor = params.stream->parent();\n-  if (!kernel_f32_.has_value()) {\n-    // Initialize didn't load the kernel. This can happen when we're running on\n-    // an unsupported platform.\n-    VLOG(1) << \"FloatCheck kernel not loaded, skipping\";\n-    return absl::OkStatus();\n+\n+  Kernels* kernels = nullptr;\n+  {\n+    absl::MutexLock lock(kernels_mutex_);\n+    auto kernel_it = kernels_.find(executor);\n+    if (kernel_it == kernels_.end()) {\n+      // Initialize didn't load the kernel. This can happen when we're running\n+      // on an unsupported platform.\n+      VLOG(1) << \"FloatCheck kernels not loaded on device \"\n+              << executor->device_ordinal() << \", skipping\";\n+      return absl::OkStatus();\n+    }\n+    kernels = kernel_it->second.get();\n   }\n \n   VLOG(1) << \"BuffersDebugFloatCheckThunk::ExecuteOnStream\";\n@@ -109,15 +128,15 @@ absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n       VLOG(1) << \"F32 buffer detected with id: \" << entry_id\n               << \" and size: \" << device_buffer.size();\n       se::DeviceMemory<float> f32_buffer(device_buffer);\n-      TF_RETURN_IF_ERROR(kernel_f32_->Launch(\n+      TF_RETURN_IF_ERROR(kernels->f32.Launch(\n           thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n           f32_buffer, f32_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n           buffer_debug_log.GetDeviceEntries<BufferDebugLogEntry>()));\n     } else if (buffer_type == PrimitiveType::BF16) {\n       VLOG(1) << \"BF16 buffer detected with id: \" << entry_id\n               << \" and size: \" << device_buffer.size();\n       se::DeviceMemory<Eigen::bfloat16> bf16_buffer(device_buffer);\n-      TF_RETURN_IF_ERROR(kernel_bf16_->Launch(\n+      TF_RETURN_IF_ERROR(kernels->bf16.Launch(\n           thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n           bf16_buffer, bf16_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n           buffer_debug_log.GetDeviceEntries<BufferDebugLogEntry>()));"
        },
        {
            "sha": "128202622c7ed555daf5ff8f2d9c5e04c558031a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.h",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h?ref=43aae2e9cd074c6473a8e414b56021e070b780b5",
            "patch": "@@ -65,13 +65,20 @@ class BuffersDebugFloatCheckThunk : public Thunk {\n   }\n \n  private:\n-  // Loaded in Initialize.\n-  std::optional<\n-      stream_executor::gpu::BufferDebugFloatCheckF32Kernel::KernelType>\n-      kernel_f32_;\n-  std::optional<\n-      stream_executor::gpu::BufferDebugFloatCheckBf16Kernel::KernelType>\n-      kernel_bf16_;\n+  struct Kernels {\n+    stream_executor::gpu::BufferDebugFloatCheckF32Kernel::KernelType f32;\n+    stream_executor::gpu::BufferDebugFloatCheckBf16Kernel::KernelType bf16;\n+  };\n+  absl::Mutex kernels_mutex_;\n+  // Each loaded kernel is associated with a specific device (represented by its\n+  // StreamExecutor).\n+  //\n+  // ExecuteOnStream implementation requires pointer stability of values, hence\n+  // unique_ptr.\n+  absl::flat_hash_map<stream_executor::StreamExecutor*,\n+                      std::unique_ptr<Kernels>>\n+      kernels_ ABSL_GUARDED_BY(kernels_mutex_);\n+\n   BufferAllocation::Slice log_slice_;\n   ThunkId checked_thunk_id_;\n   absl::flat_hash_map<size_t, BufferAllocation::Slice> checked_thunk_buffers_;"
        },
        {
            "sha": "ae94079b8ade6a3ee28baab14d762ac1d68b0d62",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk_test.cc",
            "status": "modified",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc?ref=43aae2e9cd074c6473a8e414b56021e070b780b5",
            "patch": "@@ -20,10 +20,12 @@ limitations under the License.\n #include <limits>\n #include <memory>\n #include <optional>\n+#include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -191,5 +193,76 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n                       })));\n }\n \n+TEST_F(BuffersDebugFloatCheckThunkTest,\n+       ExecutesCorrectKernelsForDifferentDevices) {\n+  // Loaded kernels are associated with a specific device represented by its\n+  // StreamExecutor. The same Thunk will be Initialized once for each device,\n+  // which will load the kernel onto that device. During ExecuteOnStream, the\n+  // correct kernel needs to be launched.\n+  if (platform_->VisibleDeviceCount() < 2) {\n+    GTEST_SKIP() << \"need at least 2 devices for this test\";\n+  }\n+\n+  static constexpr size_t kLogSizeBytes = 1024;\n+  static constexpr size_t kInputSizeBytes = 1024;\n+\n+  struct TestDevice {\n+    se::StreamExecutor* executor;\n+    std::unique_ptr<se::Stream> stream;\n+    std::unique_ptr<se::StreamExecutorMemoryAllocator> allocator;\n+    BufferAllocations allocations;\n+  };\n+  auto setup_device = [this](int device_ordinal) -> absl::StatusOr<TestDevice> {\n+    TF_ASSIGN_OR_RETURN(se::StreamExecutor * executor,\n+                        platform_->ExecutorForDevice(device_ordinal));\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Stream> stream,\n+                        executor->CreateStream());\n+    auto allocator =\n+        std::make_unique<se::StreamExecutorMemoryAllocator>(executor);\n+    BufferAllocations allocations(\n+        {executor->AllocateArray<uint8_t>(kLogSizeBytes + kInputSizeBytes)},\n+        executor->device_ordinal(), allocator.get());\n+\n+    return TestDevice{std::move(executor), std::move(stream),\n+                      std::move(allocator), std::move(allocations)};\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(TestDevice device0, setup_device(0));\n+  TF_ASSERT_OK_AND_ASSIGN(TestDevice device1, setup_device(1));\n+  BufferAllocation allocation(0, kLogSizeBytes + kInputSizeBytes, 0);\n+  BufferAllocation::Slice log_slice(&allocation, 0, kLogSizeBytes);\n+  BufferAllocation::Slice f32_slice(&allocation, kLogSizeBytes, kInputSizeBytes,\n+                                    PrimitiveType::F32);\n+  BufferAllocation::Slice bf16_slice(&allocation, kLogSizeBytes,\n+                                     kInputSizeBytes, PrimitiveType::BF16);\n+  BuffersDebugFloatCheckThunk thunk(\n+      Thunk::ThunkInfo(), log_slice,\n+      /*checked_thunk_id=*/ThunkId(123),\n+      {{/*buffer_idx=*/0, f32_slice}, {/*buffer_idx=*/1, bf16_slice}},\n+      /*runs_before_checked_thunk=*/true,\n+      std::make_shared<BufferDebugLogEntryMetadataStore>());\n+\n+  // Initialize the Thunk on both devices and run the kernel. An attempt to run\n+  // a kernel on the wrong device will fail with CUDA_ERROR_INVALID_HANDLE. The\n+  // error may be reported from the next operation on the stream, so assert on\n+  // BlockHostUntilDone as well.\n+  TF_ASSERT_OK(\n+      thunk.Initialize(Thunk::InitializeParams{/*executor=*/device0.executor}));\n+  TF_ASSERT_OK(thunk.ExecuteOnStream(Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), device0.allocations, device0.stream.get(),\n+      /*command_buffer_trace_stream=*/device0.stream.get(),\n+      /*collective_params=*/nullptr,\n+      /*collective_cliques=*/nullptr)));\n+  TF_ASSERT_OK(device0.stream->BlockHostUntilDone());\n+\n+  TF_ASSERT_OK(\n+      thunk.Initialize(Thunk::InitializeParams{/*executor=*/device1.executor}));\n+  TF_ASSERT_OK(thunk.ExecuteOnStream(Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), device1.allocations, device1.stream.get(),\n+      /*command_buffer_trace_stream=*/device1.stream.get(),\n+      /*collective_params=*/nullptr,\n+      /*collective_cliques=*/nullptr)));\n+  TF_ASSERT_OK(device1.stream->BlockHostUntilDone());\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "022c1612e2f765cf228282ecc7be059296f77e5b",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43aae2e9cd074c6473a8e414b56021e070b780b5/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=43aae2e9cd074c6473a8e414b56021e070b780b5",
            "patch": "@@ -1162,6 +1162,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n     VLOG(2) << \"[\" << device_ordinal() << \"] Resolve CUDA kernel \"\n             << kernel_name << \" from symbol pointer: \" << symbol;\n     cudaFunction_t func;\n+    std::unique_ptr<ActivateContext> scoped_activation = Activate();\n     TF_RETURN_IF_ERROR(cuda::ToStatus(\n         cudaGetFuncBySymbol(&func, symbol),\n         absl::StrFormat(\"[%d] Failed call to cudaGetFuncBySymbol\","
        }
    ],
    "stats": {
        "total": 153,
        "additions": 130,
        "deletions": 23
    }
}