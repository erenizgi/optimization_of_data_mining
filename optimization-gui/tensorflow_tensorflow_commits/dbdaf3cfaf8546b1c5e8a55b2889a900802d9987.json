{
    "author": "KanishAnand",
    "message": "Delete unused Gather/Scatter functions\n\nPiperOrigin-RevId: 838837583",
    "sha": "dbdaf3cfaf8546b1c5e8a55b2889a900802d9987",
    "files": [
        {
            "sha": "8310c1894d49cf5cbec6e09b209560efb8bfd2fe",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 143,
            "changes": 143,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdaf3cfaf8546b1c5e8a55b2889a900802d9987/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdaf3cfaf8546b1c5e8a55b2889a900802d9987/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=dbdaf3cfaf8546b1c5e8a55b2889a900802d9987",
            "patch": "@@ -1238,58 +1238,6 @@ HloSharding GatherIndexShardingFromOutput(const HloSharding& output_sharding,\n       hlo->operand(1)->shape().dimensions().size());\n }\n \n-HloSharding GatherEffectiveOutputSharding(const HloInstruction& hlo) {\n-  if (hlo.sharding().IsTileMaximal() || hlo.sharding().IsManual()) {\n-    return hlo.sharding();\n-  }\n-\n-  const GatherDimensionNumbers& dnums = hlo.gather_dimension_numbers();\n-  DimensionVector tile_assignment_dims(hlo.shape().dimensions().size());\n-  int64_t num_elements = 1;\n-  for (int64_t i = 0; i < hlo.shape().dimensions().size(); ++i) {\n-    if (!absl::c_binary_search(dnums.offset_dims(), i)) {\n-      tile_assignment_dims[i] = hlo.sharding().dimension(i);\n-      num_elements *= hlo.sharding().dimension(i);\n-    } else {\n-      tile_assignment_dims[i] = 1;\n-    }\n-  }\n-  if (num_elements == hlo.sharding().tile_assignment().num_elements()) {\n-    // Output sharding is only on non offset dimensions. We use output sharding\n-    // to shard this gather op directly.\n-    return hlo.sharding();\n-  }\n-\n-  if (num_elements == 1) {\n-    // Output sharding is only on offset dimensions. We do not shard this gather\n-    // op. Return a tile maximal sharding with the first device in output\n-    // sharding tile assignment.\n-    return HloSharding::AssignDevice(hlo.sharding().tile_assignment().first(),\n-                                     hlo.sharding().metadata());\n-  }\n-\n-  // Output sharding is on both offset and non offset dimensions. We shard the\n-  // gather op only on non offset dimensions.\n-  // For example:\n-  // - the gather op has sharding [2,2]{0,1,2,3},\n-  // - first dimension is non offset dimension,\n-  // - second dimension is offset dimension,\n-  // Then the result sharding will be [2,1]{0,2}.\n-  DimensionVector slice_starts(hlo.shape().dimensions().size(), 0LL),\n-      slice_limits(hlo.shape().dimensions().size());\n-  for (int64_t i = 0; i < hlo.shape().dimensions().size(); ++i) {\n-    if (!absl::c_binary_search(dnums.offset_dims(), i)) {\n-      slice_limits[i] = hlo.sharding().dimension(i);\n-    } else {\n-      slice_limits[i] = 1;\n-    }\n-  }\n-  Array<int64_t> tile_assignment =\n-      hlo.sharding().tile_assignment().array().Slice(slice_starts,\n-                                                     slice_limits);\n-  return HloSharding::Tile(tile_assignment, hlo.sharding().metadata());\n-}\n-\n HloSharding ScatterIndexShardingFromUpdate(\n     const HloSharding& update_sharding, const HloScatterInstruction* scatter) {\n   if (update_sharding.IsTileMaximal() || update_sharding.IsManual()) {\n@@ -1328,97 +1276,6 @@ HloSharding ScatterUpdateShardingFromIndex(\n       scatter->scatter_updates()[0]->shape().dimensions().size());\n }\n \n-HloSharding ScatterEffectiveIndexSharding(\n-    const HloSharding& index_sharding, const HloScatterInstruction& scatter) {\n-  if (index_sharding.IsTileMaximal() || index_sharding.IsManual()) {\n-    return index_sharding;\n-  }\n-\n-  // Only shard on first \"number of scatter_window_dims\" dimensions.\n-  const ScatterDimensionNumbers& dnums = scatter.scatter_dimension_numbers();\n-  int64_t num_elements = 1;\n-  int64_t index_dim = 0;\n-  for (int64_t i = 0; i < scatter.shape().dimensions().size(); ++i) {\n-    if (absl::c_binary_search(dnums.inserted_window_dims(), i)) {\n-      num_elements *= index_sharding.dimension(index_dim);\n-      index_dim++;\n-    }\n-  }\n-  if (num_elements == index_sharding.tile_assignment().num_elements()) {\n-    // Index sharding is only on scatter_window_dims. We use this index sharding\n-    // directly.\n-    return index_sharding;\n-  }\n-\n-  // Index sharding is only on update_window_dims. We do not shard this scatter\n-  // op. Return a tile maximal sharding with the first device in index sharding\n-  // tile assignment.\n-  if (num_elements == 1) {\n-    return HloSharding::AssignDevice(index_sharding.tile_assignment().first(),\n-                                     index_sharding.metadata());\n-  }\n-\n-  const int64_t index_rank =\n-      scatter.scatter_indices()->shape().dimensions().size();\n-  DimensionVector slice_starts(index_rank, 0LL), slice_limits(index_rank);\n-  for (int64_t i = 0; i < index_rank; ++i) {\n-    if (i < index_dim) {\n-      slice_limits[i] = index_sharding.dimension(i);\n-    } else {\n-      slice_limits[i] = 1;\n-    }\n-  }\n-  Array<int64_t> tile_assignment =\n-      index_sharding.tile_assignment().array().Slice(slice_starts,\n-                                                     slice_limits);\n-  return HloSharding::Tile(tile_assignment, index_sharding.metadata());\n-}\n-\n-HloSharding ScatterEffectiveDataSharding(const HloSharding& data_sharding,\n-                                         const HloScatterInstruction& scatter) {\n-  if (data_sharding.IsTileMaximal() || data_sharding.IsManual()) {\n-    return data_sharding;\n-  }\n-\n-  const ScatterDimensionNumbers& dnums = scatter.scatter_dimension_numbers();\n-  const int64_t data_rank =\n-      scatter.scatter_updates()[0]->shape().dimensions().size();\n-  DimensionVector tile_assignment_dims(data_rank, 1LL);\n-  int64_t num_elements = 1;\n-  for (int64_t i = 0; i < scatter.shape().dimensions().size(); ++i) {\n-    if (absl::c_binary_search(dnums.inserted_window_dims(), i)) {\n-      CHECK_LT(i, data_rank);\n-      tile_assignment_dims[i] = data_sharding.dimension(i);\n-      num_elements *= data_sharding.dimension(i);\n-    }\n-  }\n-  if (num_elements == data_sharding.tile_assignment().num_elements()) {\n-    // Data sharding is only on scatter_window_dims. We use this data sharding\n-    // directly.\n-    return data_sharding;\n-  }\n-\n-  if (num_elements == 1) {\n-    // Data sharding is only on update_window_dims. We do not shard this\n-    // scatter op. Return a tile maximal sharding with the first device in\n-    // data sharding tile assignment.\n-    return HloSharding::AssignDevice(data_sharding.tile_assignment().first(),\n-                                     data_sharding.metadata());\n-  }\n-\n-  // Data sharding is on both update_window_dims and scatter_window_dims. We\n-  // shard the scatter op only on scatter_window_dims. For example:\n-  // - the scatter data has sharding [2,2]{0,1,2,3},\n-  // - first dimension is scatter_window_dims,\n-  // - second dimension is update_window_dims,\n-  // Then the result sharding will be [2,1]{0,2}.\n-  DimensionVector slice_starts(data_rank, 0LL);\n-  Array<int64_t> tile_assignment =\n-      data_sharding.tile_assignment().array().Slice(slice_starts,\n-                                                    tile_assignment_dims);\n-  return HloSharding::Tile(tile_assignment, data_sharding.metadata());\n-}\n-\n namespace {\n \n GatherScatterDims GetGatherScatterOperandPassthroughDims("
        },
        {
            "sha": "ff8c3e30e643f6c9e94d71e5983b8acd922c538b",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.h",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbdaf3cfaf8546b1c5e8a55b2889a900802d9987/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbdaf3cfaf8546b1c5e8a55b2889a900802d9987/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h?ref=dbdaf3cfaf8546b1c5e8a55b2889a900802d9987",
            "patch": "@@ -191,12 +191,6 @@ HloSharding GatherOutputShardingFromIndex(const HloSharding& index_sharding,\n HloSharding GatherIndexShardingFromOutput(const HloSharding& output_sharding,\n                                           const HloInstruction* hlo);\n \n-// Returns a new HloSharding for a gather op so that only non offset dimensions\n-// are sharded. Assume \"result\" is returned by this function. It is ensured that\n-// \"GetIndexSharding(result, hlo)\" will have the same number of elements as\n-// \"result\".\n-HloSharding GatherEffectiveOutputSharding(const HloInstruction& hlo);\n-\n // Returns the preferred index sharding for a scatter op based on the sharding\n // of the data.\n HloSharding ScatterIndexShardingFromUpdate(\n@@ -207,20 +201,6 @@ HloSharding ScatterIndexShardingFromUpdate(\n HloSharding ScatterUpdateShardingFromIndex(\n     const HloSharding& index_sharding, const HloScatterInstruction* scatter);\n \n-// Returns a new index sharding for a scatter op so that we only shard on first\n-// \"number of scatter_window_dims\" dimensions. Assume \"result\" is returned by\n-// this function. It is ensured that \"ScatterUpdateShardingFromIndex(result,\n-// hlo)\" will have the same number of elements as \"result\".\n-HloSharding ScatterEffectiveIndexSharding(const HloSharding& index_sharding,\n-                                          const HloScatterInstruction& scatter);\n-\n-// Returns a new data sharding for a scatter op so that we only shard on\n-// scatter_window_dims. Assume \"result\" is returned by this function. It is\n-// ensured that \"ScatterIndexShardingFromUpdate(result, hlo)\" will have the same\n-// number of elements as \"result\".\n-HloSharding ScatterEffectiveDataSharding(const HloSharding& data_sharding,\n-                                         const HloScatterInstruction& scatter);\n-\n // Returns an output sharding of gather by passing through the data operand's\n // sharding.\n std::optional<HloSharding>"
        }
    ],
    "stats": {
        "total": 163,
        "additions": 0,
        "deletions": 163
    }
}