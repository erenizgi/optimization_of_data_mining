{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 829429675",
    "sha": "1f1323c0f4734138091130170927f10e83149ff9",
    "files": [
        {
            "sha": "1e1aaa11483827a50ce93e8af865c3ce43f2c3b3",
            "filename": "tensorflow/core/tpu/graph_rewrite/configure_tpu_embedding_rewrite_pass.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fconfigure_tpu_embedding_rewrite_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fconfigure_tpu_embedding_rewrite_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fconfigure_tpu_embedding_rewrite_pass.cc?ref=1f1323c0f4734138091130170927f10e83149ff9",
            "patch": "@@ -56,7 +56,7 @@ constexpr char kFinalizeOp[] = \"FinalizeTPUEmbedding\";\n constexpr char kEmbeddingConfigurationAttr[] = \"config\";\n \n absl::Status AddSynchronizationNode(\n-    const NodeDef& sync_node_def, const string& device_name,\n+    const NodeDef& sync_node_def, const std::string& device_name,\n     absl::Span<Node* const> end_nodes,\n     absl::Span<const DistributedTPURewriteHelpers::OutputDependency>\n         output_dependencies,\n@@ -88,8 +88,9 @@ absl::Status AddSynchronizationNode(\n }\n \n absl::Status AddSetupPropagationEmbeddingNode(\n-    const string& device_name, const string& node_name, const string& op_name,\n-    absl::Span<Node* const> input_nodes, Graph* graph, Node** node) {\n+    const std::string& device_name, const std::string& node_name,\n+    const std::string& op_name, absl::Span<Node* const> input_nodes,\n+    Graph* graph, Node** node) {\n   NodeDef node_def;\n   node_def.set_name(node_name);\n   node_def.set_op(op_name);\n@@ -109,7 +110,7 @@ absl::Status AddSetupPropagationEmbeddingNode(\n }\n \n absl::Status AddExecutePartitionerNode(\n-    const string& configuration_device_name, const string& config,\n+    const std::string& configuration_device_name, const std::string& config,\n     absl::Span<Node* const> input_dependencies, Graph* graph,\n     Node** partitioner_node) {\n   NodeDef partitioner_def;\n@@ -128,7 +129,7 @@ absl::Status AddExecutePartitionerNode(\n   return absl::OkStatus();\n }\n \n-absl::Status AddConfigureMemoryNode(const string& host_device_name,\n+absl::Status AddConfigureMemoryNode(const std::string& host_device_name,\n                                     Node* partitioner_node, Graph* graph,\n                                     Node** embedding_node) {\n   NodeDef embedding_def;\n@@ -142,7 +143,7 @@ absl::Status AddConfigureMemoryNode(const string& host_device_name,\n   return absl::OkStatus();\n }\n \n-absl::Status AddCollateMemoryNode(const string& configuration_device_name,\n+absl::Status AddCollateMemoryNode(const std::string& configuration_device_name,\n                                   absl::Span<Node* const> memory_nodes,\n                                   Graph* graph, Node** embedding_node) {\n   return AddSetupPropagationEmbeddingNode(\n@@ -153,10 +154,10 @@ absl::Status AddCollateMemoryNode(const string& configuration_device_name,\n       /*node=*/embedding_node);\n }\n \n-absl::Status AddConfigureHostNode(const string& host_device_name,\n-                                  const string& config, Node* partitioner_node,\n-                                  Node* memory_node, Graph* graph,\n-                                  Node** embedding_node) {\n+absl::Status AddConfigureHostNode(const std::string& host_device_name,\n+                                  const std::string& config,\n+                                  Node* partitioner_node, Node* memory_node,\n+                                  Graph* graph, Node** embedding_node) {\n   NodeDef embedding_def;\n   embedding_def.set_name(graph->NewName(\"configure_tpu_embedding_host\"));\n   embedding_def.set_op(kConfigureHostOp);\n@@ -172,7 +173,7 @@ absl::Status AddConfigureHostNode(const string& host_device_name,\n   return absl::OkStatus();\n }\n \n-absl::Status AddConnectHostsNode(const string& host_device_name,\n+absl::Status AddConnectHostsNode(const std::string& host_device_name,\n                                  absl::Span<Node* const> configure_host_nodes,\n                                  Graph* graph, Node** connect_node) {\n   return AddSetupPropagationEmbeddingNode(\n@@ -183,7 +184,7 @@ absl::Status AddConnectHostsNode(const string& host_device_name,\n       /*node=*/connect_node);\n }\n \n-absl::Status AddFinalizeNode(const string& configuration_device_name,\n+absl::Status AddFinalizeNode(const std::string& configuration_device_name,\n                              Node* partitioner_node, Node* memory_node,\n                              Graph* graph, Node** finalize_node) {\n   NodeDef finalize_def;"
        },
        {
            "sha": "2b568d2db40b27f06cff556b38f347dc38728376",
            "filename": "tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_helpers.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fdistributed_tpu_rewrite_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fdistributed_tpu_rewrite_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fdistributed_tpu_rewrite_helpers.cc?ref=1f1323c0f4734138091130170927f10e83149ff9",
            "patch": "@@ -44,7 +44,7 @@ namespace tensorflow {\n \n // LINT.IfChange\n absl::Status DistributedTPURewriteHelpers::GetSystemDevice(\n-    const string& system_spec_string, const DeviceSet& device_set,\n+    const std::string& system_spec_string, const DeviceSet& device_set,\n     DeviceNameUtils::ParsedName* system_spec, Device** system_device) {\n   if (!DeviceNameUtils::ParseFullName(system_spec_string, system_spec)) {\n     system_spec->Clear();\n@@ -72,7 +72,7 @@ absl::Status DistributedTPURewriteHelpers::GetSystemDevice(\n                                    system_spec_string, \"'\");\n   } else if (system_devices.size() > 1) {\n     // Validate that all system devices are part of the same job.\n-    std::unordered_set<string> job_names;\n+    std::unordered_set<std::string> job_names;\n     for (auto device : system_devices) {\n       const auto& parsed_name = device->parsed_name();\n       TF_RET_CHECK(parsed_name.has_job);\n@@ -136,7 +136,7 @@ absl::Status DistributedTPURewriteHelpers::GetHostSystemDevices(\n \n   // Check that all the devices belong to the same job.\n   TF_RET_CHECK((*host_system_devices)[0]->parsed_name().has_job);\n-  const string& job_name = (*host_system_devices)[0]->parsed_name().job;\n+  const std::string& job_name = (*host_system_devices)[0]->parsed_name().job;\n   int replica = (*host_system_devices)[0]->parsed_name().replica;\n   for (const auto host_device : *host_system_devices) {\n     const auto& parsed_name = host_device->parsed_name();\n@@ -215,10 +215,10 @@ absl::Status DistributedTPURewriteHelpers::GetTPUDevices(\n // LINT.ThenChange(//tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc)\n \n absl::Status DistributedTPURewriteHelpers::ForConfigurationNodeMatchingType(\n-    const string& node_type, Graph* graph, const DeviceSet& device_set,\n+    const std::string& node_type, Graph* graph, const DeviceSet& device_set,\n     const std::function<\n         absl::Status(const NodeDef& configuration_node_def,\n-                     const string& configuration_device_name,\n+                     const std::string& configuration_device_name,\n                      const std::vector<Device*>& host_devices,\n                      const std::vector<Node*>& input_dependencies,\n                      const std::vector<OutputDependency>& output_dependencies,\n@@ -232,12 +232,12 @@ absl::Status DistributedTPURewriteHelpers::ForConfigurationNodeMatchingType(\n   }\n \n   for (Node* node : nodes) {\n-    string spec_string = node->requested_device();\n+    std::string spec_string = node->requested_device();\n     DeviceNameUtils::ParsedName spec;\n     Device* device;\n     TF_RETURN_IF_ERROR(\n         GetSystemDevice(spec_string, device_set, &spec, &device));\n-    const string& device_name = device->name();\n+    const std::string& device_name = device->name();\n \n     std::vector<Device*> host_devices;\n     TF_RETURN_IF_ERROR(GetHostSystemDevices(spec, device_set, &host_devices));"
        },
        {
            "sha": "3863f27baffe96606ff96d8e82a389df69bc2eec",
            "filename": "tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_helpers.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fdistributed_tpu_rewrite_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fdistributed_tpu_rewrite_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fdistributed_tpu_rewrite_helpers.h?ref=1f1323c0f4734138091130170927f10e83149ff9",
            "patch": "@@ -44,7 +44,7 @@ class DistributedTPURewriteHelpers {\n   // system_spec_string to identify the TPU_SYSTEM on replica 0, task 0 of the\n   // job that contains the TPU hardware.\n   // TODO(b/110910013): Possibly remove the tpu system device.\n-  static absl::Status GetSystemDevice(const string& system_spec_string,\n+  static absl::Status GetSystemDevice(const std::string& system_spec_string,\n                                       const DeviceSet& device_set,\n                                       DeviceNameUtils::ParsedName* system_spec,\n                                       Device** system_device);\n@@ -91,10 +91,10 @@ class DistributedTPURewriteHelpers {\n     int dst_input;\n   };\n   static absl::Status ForConfigurationNodeMatchingType(\n-      const string& node_type, Graph* graph, const DeviceSet& device_set,\n+      const std::string& node_type, Graph* graph, const DeviceSet& device_set,\n       const std::function<\n           absl::Status(const NodeDef& configuration_node_def,\n-                       const string& configuration_device_name,\n+                       const std::string& configuration_device_name,\n                        const std::vector<Device*>& host_devices,\n                        const std::vector<Node*>& input_dependencies,\n                        const std::vector<OutputDependency>& output_dependencies,"
        },
        {
            "sha": "d6a810b202c0095fadb5a0b7fe5d02dfb8050515",
            "filename": "tensorflow/core/tpu/graph_rewrite/incomplete_nodedef_builder.h",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fincomplete_nodedef_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fincomplete_nodedef_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fincomplete_nodedef_builder.h?ref=1f1323c0f4734138091130170927f10e83149ff9",
            "patch": "@@ -33,23 +33,24 @@ namespace tensorflow {\n // TODO(jpienaar): Clean up NodeDefBuilder and remove this class.\n class IncompleteNodeDefBuilder {\n  public:\n-  IncompleteNodeDefBuilder(const string& name, const string& op,\n+  IncompleteNodeDefBuilder(const std::string& name, const std::string& op,\n                            const NodeDebugInfo& debug);\n \n-  IncompleteNodeDefBuilder& AddAttr(const string& attr, const DataType& type);\n-  IncompleteNodeDefBuilder& AddAttr(const string& attr, int val);\n+  IncompleteNodeDefBuilder& AddAttr(const std::string& attr,\n+                                    const DataType& type);\n+  IncompleteNodeDefBuilder& AddAttr(const std::string& attr, int val);\n \n-  IncompleteNodeDefBuilder& Device(const string& device);\n+  IncompleteNodeDefBuilder& Device(const std::string& device);\n \n   absl::Status Build(Graph* graph, Node** n);\n \n-  static IncompleteNodeDefBuilder Identity(const string& name,\n+  static IncompleteNodeDefBuilder Identity(const std::string& name,\n                                            const DataType& type,\n                                            const NodeDebugInfo& debug);\n-  static IncompleteNodeDefBuilder Merge(const string& name,\n+  static IncompleteNodeDefBuilder Merge(const std::string& name,\n                                         const DataType& type,\n                                         const NodeDebugInfo& debug, int n);\n-  static IncompleteNodeDefBuilder Switch(const string& name,\n+  static IncompleteNodeDefBuilder Switch(const std::string& name,\n                                          const DataType& type,\n                                          const NodeDebugInfo& debug);\n "
        },
        {
            "sha": "b5c287d1726b933663ce123d475a555829a6ec67",
            "filename": "tensorflow/core/tpu/graph_rewrite/tpu_embedding_software_deduplication_rewrite_pass.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Ftpu_embedding_software_deduplication_rewrite_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Ftpu_embedding_software_deduplication_rewrite_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Ftpu_embedding_software_deduplication_rewrite_pass.cc?ref=1f1323c0f4734138091130170927f10e83149ff9",
            "patch": "@@ -51,7 +51,7 @@ namespace {\n // Check the number of outputs for RecvActivationsNode or for number of inputs\n // For SendGradientsNode.\n absl::Status CheckNumInputsOrOutputs(\n-    const int32 num_input_or_outputs, const std::string& attribute_name,\n+    const int32_t num_input_or_outputs, const std::string& attribute_name,\n     const std::string& node_name,\n     const tpu::TPUEmbeddingConfiguration& tpu_embedding_config) {\n   if (tpu_embedding_config.feature_descriptor_size() == 0 &&\n@@ -129,7 +129,7 @@ absl::StatusOr<NodeDef> MakeRecvActivationsNodeDef(\n         \"Malformed config attribute in the RecvTPUEmbeddingActivations node.\");\n   }\n \n-  int32 num_outputs;\n+  int32_t num_outputs;\n   TF_RETURN_IF_ERROR(GetNodeAttr(AttrSlice(old_activations_node_def),\n                                  \"num_outputs\", &num_outputs));\n \n@@ -185,15 +185,15 @@ absl::StatusOr<NodeDef> MakeSendGradientsNodeDef(\n         \"Malformed config attribute in the SendTPUEmbeddingGradients node.\");\n   }\n \n-  int32 num_inputs;\n+  int32_t num_inputs;\n   TF_RETURN_IF_ERROR(\n       GetNodeAttr(AttrSlice(old_gradients_node_def), \"N\", &num_inputs));\n \n   TF_RETURN_IF_ERROR(CheckNumInputsOrOutputs(num_inputs, \"num_inputs\",\n                                              \"SendTPUEmbeddingGradients\",\n                                              tpu_embedding_config));\n \n-  int32 dynamic_inputs_tag_count = 0;\n+  int32_t dynamic_inputs_tag_count = 0;\n   if (!GetNodeAttr(AttrSlice(old_gradients_node_def), \"NN\",\n                    &dynamic_inputs_tag_count)\n            .ok()) {\n@@ -209,7 +209,7 @@ absl::StatusOr<NodeDef> MakeSendGradientsNodeDef(\n         status_or_dynamic_inputs_tag_count.status().message());\n   }\n \n-  const int32 expected_dynamic_inputs_tag_count =\n+  const int32_t expected_dynamic_inputs_tag_count =\n       status_or_dynamic_inputs_tag_count.value();\n \n   if (dynamic_inputs_tag_count != expected_dynamic_inputs_tag_count) {\n@@ -221,7 +221,7 @@ absl::StatusOr<NodeDef> MakeSendGradientsNodeDef(\n   }\n \n   if (data_inputs.size() !=\n-      static_cast<uint64>(num_inputs + dynamic_inputs_tag_count)) {\n+      static_cast<uint64_t>(num_inputs + dynamic_inputs_tag_count)) {\n     return absl::InvalidArgumentError(absl::StrFormat(\n         \"Mismatch in the number of inputs for SendTPUEmbeddingGradients node, \"\n         \"expected: %d, actual: %d\","
        },
        {
            "sha": "36f6fde1ebe817a3386f82ee38c73c2a69b44f2f",
            "filename": "tensorflow/core/tpu/graph_rewrite/update_tpu_embedding_ops_passes.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fupdate_tpu_embedding_ops_passes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fupdate_tpu_embedding_ops_passes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fupdate_tpu_embedding_ops_passes.cc?ref=1f1323c0f4734138091130170927f10e83149ff9",
            "patch": "@@ -109,10 +109,10 @@ absl::Status UpdateTPUEmbeddingEnqueueOrdinalPass::Run(\n   single_tpu_device_spec.has_id = false;\n   options.device_set->FindMatchingDevices(single_tpu_device_spec,\n                                           &task_devices);\n-  int64 num_tpus_per_task = task_devices.size();\n+  int64_t num_tpus_per_task = task_devices.size();\n \n   for (Node* node : embedding_nodes) {\n-    int64 replica_id;\n+    int64_t replica_id;\n     if (TryGetNodeAttr(node->attrs(), kXlaReplicaIdAttrName, &replica_id)) {\n       node->AddAttr(\"device_ordinal\", replica_id % num_tpus_per_task);\n     }\n@@ -128,7 +128,7 @@ absl::Status UpdateMapsForModeOverride(\n     std::map<std::string, N>* enqueue_op,\n     std::map<std::string, bool>* found_recv_op,\n     std::map<std::string, bool>* found_grad_send_op) {\n-  string layer_call_index;\n+  std::string layer_call_index;\n   if (TryGetNodeAttr(attrs, \"_tpu_embedding_layer\", &layer_call_index)) {\n     if ((op == kTPURecvOps[0]) || (op == kTPURecvOps[1])) {\n       // We will prevent users from creating multiple copies of the\n@@ -269,7 +269,7 @@ absl::Status UpdateTPUEmbeddingModePass::UpdateFunctionDefEnqueueOp(\n   TF_RET_CHECK(!node->input(mode_override).empty());\n \n   // Find input node\n-  string select_name = std::vector<std::string>(\n+  std::string select_name = std::vector<std::string>(\n       absl::StrSplit(node->input(mode_override), ':'))[0];\n   int select = 0;\n   while ((select < function->node_def_size()) &&"
        },
        {
            "sha": "ca5c50074b2a0c15ded8b14103b789a5755eb5f0",
            "filename": "tensorflow/core/tpu/graph_rewrite/variable_merger_pass.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fvariable_merger_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1f1323c0f4734138091130170927f10e83149ff9/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fvariable_merger_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fgraph_rewrite%2Fvariable_merger_pass.cc?ref=1f1323c0f4734138091130170927f10e83149ff9",
            "patch": "@@ -50,22 +50,22 @@ namespace {\n // The name of a stateful op is semantically meaningful because ops with the\n // same name will share the same kernel. We therefore form new op names using a\n // deterministic function (a fingerprint) of the old names.\n-uint64 MergedOpFingerprint(absl::Span<Node* const> ops) {\n-  std::vector<string> op_names;\n+uint64_t MergedOpFingerprint(absl::Span<Node* const> ops) {\n+  std::vector<std::string> op_names;\n   op_names.reserve(ops.size());\n   for (const Node* node : ops) {\n     op_names.push_back(node->name());\n   }\n   return Fingerprint64(absl::StrJoin(op_names, \",\"));\n }\n \n-absl::Status MergeVarHandleOps(const string& device,\n+absl::Status MergeVarHandleOps(const std::string& device,\n                                absl::Span<Node* const> nodes, Graph* graph) {\n   int num_var_handles(nodes.size());\n   if (num_var_handles <= 1) return absl::OkStatus();\n \n-  std::vector<string> containers(num_var_handles);\n-  std::vector<string> names(num_var_handles);\n+  std::vector<std::string> containers(num_var_handles);\n+  std::vector<std::string> names(num_var_handles);\n   DataTypeVector dtypes(num_var_handles);\n   std::vector<PartialTensorShape> shapes(num_var_handles);\n   for (int i = 0; i < num_var_handles; ++i) {\n@@ -150,7 +150,7 @@ absl::Status VariableMergerPass::Run(\n \n   // Find VarHandleOps that are graph roots and group them by assigned device.\n   // Also find any ReadVariableOps that are consumers of those handles.\n-  absl::flat_hash_map<string, std::vector<Node*>> var_handle_ops_by_device;\n+  absl::flat_hash_map<std::string, std::vector<Node*>> var_handle_ops_by_device;\n   absl::flat_hash_set<Node*> read_variable_ops;\n \n   for (Node* m : graph->source_node()->out_nodes()) {"
        }
    ],
    "stats": {
        "total": 92,
        "additions": 47,
        "deletions": 45
    }
}