{
    "author": "Tixxx",
    "message": "PR #30984: [NVIDIA GPU] Use both parallel and host threads for processing scheduling passes\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30984\n\nüìù Summary of Changes\nThis is to fix a bug where scheduling pipelines only use the main host execution threads(introduced in this https://github.com/openxla/xla/pull/30487).\nIt should use both main host and parallel threads as there are passes before scheduler that convert computes into async computes that use parallel threads\n\nüéØ Justification\nIf async compute is present, the scheduling pass will error out with\n\"absl::raw_hash_map out of range\"\n\nüöÄ Kind of Contribution\nPlease remove what does not apply: üêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nNA\n\nüß™ Unit Tests:\nAdded scheduler test to verify changes\n\nüß™ Execution Tests:\n\nCopybara import of the project:\n\n--\n9010f07b0b7054346602fedb766c6a215e35e076 by TJ Xu <tjx@nvidia.com>:\n\nUse both parallel and host threads for processing scheduling passes\n\n--\n6de5d532bedaed1a6c296fee3ed54eb8a6049827 by TJ Xu <tjx@nvidia.com>:\n\nadded scheduler test\n\nMerging this change closes #30984\n\nPiperOrigin-RevId: 804385387",
    "sha": "0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe",
    "files": [
        {
            "sha": "07b0ef3df28abeefcca91ce4c7115a636399facc",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe",
            "patch": "@@ -2237,6 +2237,7 @@ cc_library(\n         \"//xla/service/gpu/model:sol_latency_estimator\",\n         \"//xla/service/gpu/transforms:pgle_accuracy_checker\",\n         \"//xla/service/gpu/transforms:scheduling_instruction_annotator\",\n+        \"//xla/service/gpu/transforms:stream_attribute_async_wrapper\",\n         \"//xla/service/gpu/transforms/collectives:async_collective_annotator\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/stream_executor:device_description\","
        },
        {
            "sha": "4dc8bf4dd7aabbdc2874ca796fbbdbea10ed963b",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe",
            "patch": "@@ -64,6 +64,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/gpu/transforms/pgle_accuracy_checker.h\"\n #include \"xla/service/gpu/transforms/scheduling_instruction_annotator.h\"\n+#include \"xla/service/gpu/transforms/stream_attribute_async_wrapper.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/latency_hiding_scheduler.h\"\n #include \"xla/service/legalize_scheduling_annotations.h\"\n@@ -646,7 +647,9 @@ absl::StatusOr<HloSchedule> ScheduleGpuModuleWithMemoryScheduler(\n   return ScheduleModule(\n       module,\n       DefaultMemoryScheduler(alias_info, size_func, PostProcessSchedule),\n-      /*execution_threads=*/{HloInstruction::kMainExecutionThread},\n+      /*execution_threads=*/\n+      {HloInstruction::kMainExecutionThread,\n+       StreamAttributeAsyncWrapper::kParallelExecutionThread},\n       peak_memory_bytes);\n }\n "
        },
        {
            "sha": "b8374d384325fefd6b457587fc6d301c43314320",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc?ref=0b0ee2765fcd5fea83f1c51a1ad2e3f2543146fe",
            "patch": "@@ -1010,5 +1010,32 @@ ENTRY main {\n   EXPECT_FALSE(async_tracker.IsSupportedAsyncStart(*dynamic_slice_done));\n }\n \n+TEST_F(GpuLatencyHidingSchedulerBaseTest, ParallelThreadsShouldBeScheduled) {\n+  absl::string_view kHloModule = R\"(\n+    HloModule Test1\n+\n+    custom_call_F32 {\n+      lhs = f32[2,2]{1,0} parameter(0)\n+      rhs = f32[2,2]{1,0} parameter(1)\n+      ROOT custom_call = f32[2,2]{1,0} custom-call(lhs, rhs), custom_call_target=\"random\"\n+    }\n+\n+    ENTRY Test1 {\n+      a = f32[2,2]{1,0} parameter(0)\n+      b = f32[2,2]{1,0} parameter(1)\n+      start = ((f32[2,2]{1,0}, f32[2,2]{1,0}), f32[2,2]{1,0}) async-start(a, b), calls=custom_call_F32, async_execution_thread=\"parallel\"\n+      ROOT done = f32[2,2]{1,0} async-done(start)\n+    }\n+  )\";\n+\n+  absl::string_view kFdoProfile = \"\";\n+  auto config = GetModuleConfig(kFdoProfile);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule, config));\n+\n+  // It should compile without any issues.\n+  TF_EXPECT_OK(ScheduleModule(module.get()));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 32,
        "deletions": 1
    }
}