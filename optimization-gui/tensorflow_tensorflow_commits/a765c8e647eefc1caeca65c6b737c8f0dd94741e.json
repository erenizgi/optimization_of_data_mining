{
    "author": "sergey-kozub",
    "message": "PR #30843: [XLA:GPU] Fix broadcast shape for non-standard layouts of block scaled dot custom call.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30843\n\nüìù Summary of Changes\nSPMD pass could generate non-default shape layouts for block scaled dot custom call.\nThe XLA builder inherits the layout from the operand, but cannot do that in the case of a broadcast.\nThis PR fixes the layout of the broadcast after building the HLO computation (currently this issue results in a compilation failure).\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nNote: this bug could happen with any code that uses the XLA builder and generates a broadcast.\n(right now it generates a default layout, which could also imply a transpose)\nCopybara import of the project:\n\n--\necd11629daab7c59343d9e45e7191677a872d31f by Sergey Kozub <skozub@nvidia.com>:\n\nFix broadcast layout\n\nMerging this change closes #30843\n\nPiperOrigin-RevId: 802120096",
    "sha": "a765c8e647eefc1caeca65c6b737c8f0dd94741e",
    "files": [
        {
            "sha": "1a9ec1a8817a012138a60aa817d97f40f47f7ce8",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a765c8e647eefc1caeca65c6b737c8f0dd94741e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a765c8e647eefc1caeca65c6b737c8f0dd94741e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc?ref=a765c8e647eefc1caeca65c6b737c8f0dd94741e",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/hlo/builder/lib/constants.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -53,6 +54,34 @@ absl::StatusOr<HloInstruction*> ExpandInstructionUsingBuilder(\n       HloComputation * computation,\n       XlaComputationToHloComputation(xla_computation,\n                                      old_instruction->parent()->parent()));\n+\n+  // Fix broadcast layouts (they cannot be inferred correctly).\n+  for (HloInstruction* instruction : computation->instructions()) {\n+    auto broadcast = DynCast<HloBroadcastInstruction>(instruction);\n+    if (broadcast != nullptr && !LayoutUtil::IsMonotonicWithDim0Major(\n+                                    broadcast->operand(0)->shape().layout())) {\n+      // Previous instruction is a convert, next one is a reshape.\n+      int rank = broadcast->shape().dimensions().size();\n+      const HloInstruction* convert = broadcast->operand(0);\n+      CHECK(convert->opcode() == HloOpcode::kConvert &&\n+            convert->shape().dimensions().size() == rank - 1);\n+      HloInstruction* reshape = broadcast->users()[0];\n+      CHECK(reshape->opcode() == HloOpcode::kReshape &&\n+            reshape->shape().dimensions().size() == rank - 1);\n+\n+      // Increase the layout index of the dimensions after the last one.\n+      // Example: {2,0,1} -> {3,0,2,1}\n+      int last_idx = convert->shape().layout().minor_to_major().back();\n+      auto broadcast_layout = broadcast->mutable_shape()->mutable_layout();\n+      for (int i = 0; i < rank - 1; ++i) {\n+        int idx = convert->shape().layout().minor_to_major(i);\n+        broadcast_layout->set_minor_to_major(i, idx + (idx >= last_idx));\n+      }\n+      broadcast_layout->set_minor_to_major(rank - 1, last_idx);\n+      *reshape->mutable_shape()->mutable_layout() = convert->shape().layout();\n+    }\n+  }\n+\n   return old_instruction->parent()->AddInstruction(HloInstruction::CreateCall(\n       old_instruction->shape(), old_instruction->operands(), computation));\n }"
        },
        {
            "sha": "ccd3e271a2b744aa9cb7b32e11be76bcdd524347",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter_test.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a765c8e647eefc1caeca65c6b737c8f0dd94741e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a765c8e647eefc1caeca65c6b737c8f0dd94741e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc?ref=a765c8e647eefc1caeca65c6b737c8f0dd94741e",
            "patch": "@@ -115,6 +115,41 @@ ENTRY main {\n })\");\n }\n \n+TEST_F(BlockScalingRewriterTest, ExpandBlockScaledDotNonDefaultLayout) {\n+  constexpr absl::string_view hlo_string = R\"(\n+HloModule test\n+\n+ENTRY main {\n+  %lhs = f8e4m3fn[4,16,256]{2,0,1} parameter(0)\n+  %rhs = f8e4m3fn[4,32,256]{0,1,2} parameter(1)\n+  %lhs_scale = f8e5m2[4,16,8]{2,0,1} parameter(2)\n+  %rhs_scale = f8e5m2[4,32,8]{0,1,2} parameter(3)\n+  ROOT %result = f32[4,16,32] custom-call(%lhs, %rhs, %lhs_scale, %rhs_scale),\n+      custom_call_target=\"__op$block_scaled_dot\"\n+})\";\n+\n+  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n+  CHECK: [[lhs_quant:%.+]] = f8e4m3fn[4,16,256]{2,0,1} parameter(0)\n+  CHECK: [[lhs_quant_cvt:%.+]] = f32[4,16,256]{2,0,1} convert([[lhs_quant]])\n+  CHECK: [[lhs_scale:%.+]] = f8e5m2[4,16,8]{2,0,1} parameter(2)\n+  CHECK: [[lhs_scale_cvt:%.+]] = f32[4,16,8]{2,0,1} convert([[lhs_scale]])\n+  CHECK: [[lhs_scale_bc:%.+]] = f32[4,16,8,32]{3,0,2,1} broadcast([[lhs_scale_cvt]])\n+  CHECK: [[lhs_scale_rs:%.+]] = f32[4,16,256]{2,0,1} reshape([[lhs_scale_bc]])\n+  CHECK: [[lhs:%.+]] = f32[4,16,256]{2,0,1} multiply([[lhs_quant_cvt]], [[lhs_scale_rs]])\n+  CHECK: [[rhs_quant:%.+]] = f8e4m3fn[4,32,256]{0,1,2} parameter(1)\n+  CHECK: [[rhs_quant_cvt:%.+]] = f32[4,32,256]{0,1,2} convert([[rhs_quant]])\n+  CHECK: [[rhs_scale:%.+]] = f8e5m2[4,32,8]{0,1,2} parameter(3)\n+  CHECK: [[rhs_scale_cvt:%.+]] = f32[4,32,8]{0,1,2} convert([[rhs_scale]])\n+  CHECK: [[rhs_scale_bc:%.+]] = f32[4,32,8,32]{0,1,3,2} broadcast([[rhs_scale_cvt]])\n+  CHECK: [[rhs_scale_rs:%.+]] = f32[4,32,256]{0,1,2} reshape([[rhs_scale_bc]])\n+  CHECK: [[rhs:%.+]] = f32[4,32,256]{0,1,2} multiply([[rhs_quant_cvt]], [[rhs_scale_rs]])\n+  CHECK: ROOT {{.+}} = f32[4,16,32]{2,1,0} dot([[lhs]], [[rhs]])\n+  CHECK-SAME: lhs_batch_dims={0}, lhs_contracting_dims={2}\n+  CHECK-SAME: rhs_batch_dims={0}, rhs_contracting_dims={2}\n+})\");\n+}\n+\n TEST_F(BlockScalingRewriterTest, ExpandBlockScaledDotQuantizedLhs) {\n   constexpr absl::string_view hlo_string = R\"(\n HloModule test"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 64,
        "deletions": 0
    }
}