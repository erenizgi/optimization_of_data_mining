{
    "author": "hyeontaek",
    "message": "[IFRT] Replace `Client::GetDefaultLayout()` and `Array::layout()` with a new version\n\n`Client::GetDefaultLayout()` and `Array::layout()` are replaced to use\n`CustomLayoutRef`.\n* `Client::GetDefaultLayout()` is functionally equivalent to `Client::GetDefaultPjRtLayout()`, but using IFRT types.\n* `Array::layout()` is slightly semantics changing as it cannot return an error anymore, but it must return some layout (where a compact layout is typically a valid choice). *In the future,* this method will see a further semantics change that a default layout is indicated `nullptr` (i.e., of `LayoutRef` type) instead of its concrete layout.\n\nSubsequent changes will introduce the initial method implementations\nthat simply wrap PjRt layouts using `xla::ifrt::PjRtLayout::Create()`. This\nimplementation needs to be defined in individual runtimes because\n`xla::ifrt::PjRtLayout` is defined in PjRt-IFRT and inaccessible from\ntop-level IFRT.\n\nPiperOrigin-RevId: 813940989",
    "sha": "c1a1cee31087bc1bdd6263e6d1efee355cf271d0",
    "files": [
        {
            "sha": "92b5d5f9172fffa75b30acaab539ec10d5c9e925",
            "filename": "third_party/xla/xla/python/ifrt/array.h",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray.h?ref=c1a1cee31087bc1bdd6263e6d1efee355cf271d0",
            "patch": "@@ -22,11 +22,13 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/attributes.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/Support/ExtensibleRTTI.h\"\n #include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/layout.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/value.h\"\n@@ -79,10 +81,10 @@ class Array : public llvm::RTTIExtends<Array, Value> {\n   // return UNIMPLEMENTED instead.\n   virtual absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> pjrt_layout()\n       const = 0;\n-  // Legacy name for `pjrt_layout()`. Will be removed, and then re-introduced as\n-  // a new signature that returns `xla::ifrt::LayoutRef`.\n-  absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> layout() const {\n-    return pjrt_layout();\n+  virtual CustomLayoutRef layout() const {\n+    // TODO(hyeontaek): Change to a pure virtual method once all implementations\n+    // override this method.\n+    CHECK(false) << \"Placeholder; do not use yet\";\n   }\n \n   // Breaks an array up into per-device arrays. This is the elimination"
        },
        {
            "sha": "96d2f37487e5a5b06e2ff0015c21e7c3367b7309",
            "filename": "third_party/xla/xla/python/ifrt/client.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fclient.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fclient.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fclient.cc?ref=c1a1cee31087bc1bdd6263e6d1efee355cf271d0",
            "patch": "@@ -15,10 +15,28 @@ limitations under the License.\n \n #include \"xla/python/ifrt/client.h\"\n \n+#include <cstdint>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/python/ifrt/device.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/layout.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n+\n namespace xla {\n namespace ifrt {\n \n char Client::ID = 0;\n \n+absl::StatusOr<CustomLayoutRef> Client::GetDefaultLayout(\n+    DType dtype, absl::Span<const int64_t> shard_dims, Device* device,\n+    xla::ifrt::MemoryKind memory_kind) const {\n+  return GetDefaultLayout(dtype, Shape(shard_dims),\n+                          SingleDeviceSharding::Create(device, memory_kind));\n+}\n+\n }  // namespace ifrt\n }  // namespace xla"
        },
        {
            "sha": "03ee9fb7368542bbaf3690a507841ed593196343",
            "filename": "third_party/xla/xla/python/ifrt/client.h",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fclient.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fclient.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fclient.h?ref=c1a1cee31087bc1bdd6263e6d1efee355cf271d0",
            "patch": "@@ -24,8 +24,8 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/macros.h\"\n-#include \"absl/base/nullability.h\"\n #include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/layout.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/remap_plan.h\"\n #include \"xla/python/ifrt/shape.h\"\n@@ -344,15 +345,20 @@ class Client : public llvm::RTTIExtends<Client, llvm::RTTIRoot> {\n                        Device* device,\n                        xla::ifrt::MemoryKind memory_kind) const = 0;\n \n-  // Legacy name for `GetDefaultPjRtLayout()`. Will be removed, and then\n-  // re-introduced as a new signature that returns `xla::ifrt::CustomLayoutRef`.\n-  // TODO(hyeontaek): Change the API to take `Shape` and `Sharding` instead of\n-  // single-shard dimensions and device.\n-  absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> GetDefaultLayout(\n-      DType dtype, absl::Span<const int64_t> dims, Device* device,\n-      xla::ifrt::MemoryKind memory_kind) const {\n-    return GetDefaultPjRtLayout(dtype, dims, device, memory_kind);\n+  // Returns the default layout for an array with `dtype`, `shape`, and\n+  // `sharding`.\n+  virtual absl::StatusOr<CustomLayoutRef> GetDefaultLayout(\n+      DType dtype, const Shape& shape, const ShardingRef& sharding) const {\n+    // TODO(hyeontaek): Change to a pure virtual method once all implementations\n+    // override this method.\n+    CHECK(false) << \"Placeholder; do not use yet\";\n+    return absl::UnimplementedError(\"Not implemented yet\");\n   }\n+  // Helper method for `GetDefaultLayout` for when shard shape dims are known.\n+  // TODO(hyeontaek): Remove this sugar API once the transition is complete.\n+  absl::StatusOr<CustomLayoutRef> GetDefaultLayout(\n+      DType dtype, absl::Span<const int64_t> shard_dims, Device* device,\n+      xla::ifrt::MemoryKind memory_kind) const;\n \n   // Returns a UserContext that captures the current context information such as\n   // the stack trace. IFRT implementations that do not support UserContext will"
        },
        {
            "sha": "b571bf4fced918ea980b1d6b81ea66f2d281bd30",
            "filename": "third_party/xla/xla/python/ifrt/mock.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.cc?ref=c1a1cee31087bc1bdd6263e6d1efee355cf271d0",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/layout.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/remap_plan.h\"\n #include \"xla/python/ifrt/shape.h\"\n@@ -85,6 +86,9 @@ MockArray::MockArray(xla::ifrt::ArrayRef delegated)\n           [this]() -> absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> {\n             return delegated_->pjrt_layout();\n           });\n+  ON_CALL(*this, layout).WillByDefault([this]() -> CustomLayoutRef {\n+    return delegated_->layout();\n+  });\n   ON_CALL(*this, DisassembleIntoSingleDeviceArrays(_, _))\n       .WillByDefault(\n           [this](ArrayCopySemantics array_copy_semantics,\n@@ -231,6 +235,13 @@ MockClient::MockClient(std::unique_ptr<xla::ifrt::Client> delegated)\n             return delegated_->GetDefaultPjRtLayout(dtype, dims, device,\n                                                     memory_kind);\n           });\n+  ON_CALL(*this, GetDefaultLayout)\n+      .WillByDefault(\n+          [this](\n+              DType dtype, const Shape& shape,\n+              const ShardingRef& sharding) -> absl::StatusOr<CustomLayoutRef> {\n+            return delegated_->GetDefaultLayout(dtype, shape, sharding);\n+          });\n   ON_CALL(*this, Attributes).WillByDefault([this]() -> const AttributeMap& {\n     return delegated_->Attributes();\n   });"
        },
        {
            "sha": "738f05b46ad3fa562d16cee3ce133e0365c039db",
            "filename": "third_party/xla/xla/python/ifrt/mock.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1a1cee31087bc1bdd6263e6d1efee355cf271d0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.h?ref=c1a1cee31087bc1bdd6263e6d1efee355cf271d0",
            "patch": "@@ -48,6 +48,7 @@ limitations under the License.\n #include \"xla/python/ifrt/executable_serdes.h\"\n #include \"xla/python/ifrt/host_callback.h\"\n #include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/layout.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/program.h\"\n #include \"xla/python/ifrt/remap_plan.h\"\n@@ -83,6 +84,7 @@ class MockArray : public llvm::RTTIExtends<MockArray, Array> {\n   MOCK_METHOD(ShardingRef, shared_ptr_sharding, (), (const, final));\n   MOCK_METHOD(absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>>,\n               pjrt_layout, (), (const, final));\n+  MOCK_METHOD(CustomLayoutRef, layout, (), (const, final));\n   MOCK_METHOD(UserContextRef, user_context, (), (const, final));\n   MOCK_METHOD(absl::StatusOr<std::vector<ArrayRef>>,\n               DisassembleIntoSingleDeviceArrays,\n@@ -184,6 +186,9 @@ class MockClient : public llvm::RTTIExtends<MockClient, Client> {\n               (xla::ifrt::DType dtype, absl::Span<const int64_t> dims,\n                xla::ifrt::Device* device, xla::ifrt::MemoryKind memory_kind),\n               (const, final));\n+  MOCK_METHOD(absl::StatusOr<CustomLayoutRef>, GetDefaultLayout,\n+              (DType dtype, const Shape& shape, const ShardingRef& sharding),\n+              (const, final));\n   MOCK_METHOD(tsl::RCReference<xla::ifrt::UserContext>, CreateUserContext, (),\n               (final));\n   // LINT.ThenChange(mock.cc:MockClientDelegation)"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 55,
        "deletions": 13
    }
}