{
    "author": "ezhulenev",
    "message": "[xla:ffi] Add example of async custom call in XLA:GPU\n\nPiperOrigin-RevId: 826121283",
    "sha": "9b51864c7b02f3c82dc77cd760233f424a91d06d",
    "files": [
        {
            "sha": "41e52968dd038d91645274654f2b050f7c4d5374",
            "filename": "third_party/xla/xla/ffi/ffi.h",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9b51864c7b02f3c82dc77cd760233f424a91d06d/third_party%2Fxla%2Fxla%2Fffi%2Fffi.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9b51864c7b02f3c82dc77cd760233f424a91d06d/third_party%2Fxla%2Fxla%2Fffi%2Fffi.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi.h?ref=9b51864c7b02f3c82dc77cd760233f424a91d06d",
            "patch": "@@ -249,6 +249,20 @@ struct ArgBinding<Buffer<dtype, rank>> {\n   using Arg = Buffer<dtype, rank>;\n };\n \n+//===----------------------------------------------------------------------===//\n+// Results binding\n+//===----------------------------------------------------------------------===//\n+\n+template <>\n+struct RetBinding<Result<AnyBuffer>> {\n+  using Ret = AnyBuffer;\n+};\n+\n+template <PrimitiveType dtype, size_t rank>\n+struct RetBinding<Result<Buffer<dtype, rank>>> {\n+  using Ret = Buffer<dtype, rank>;\n+};\n+\n //===----------------------------------------------------------------------===//\n // Arguments decoding\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "2968fa49d18c675c8ec8d35ae1760cecab39287a",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9b51864c7b02f3c82dc77cd760233f424a91d06d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9b51864c7b02f3c82dc77cd760233f424a91d06d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=9b51864c7b02f3c82dc77cd760233f424a91d06d",
            "patch": "@@ -196,6 +196,7 @@ xla_test(\n     deps = [\n         \"//xla:debug_options_flags\",\n         \"//xla:literal\",\n+        \"//xla:literal_util\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:xla_data_proto_cc\",\n@@ -223,6 +224,8 @@ xla_test(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\","
        },
        {
            "sha": "0771d1d7a59377546b764a2936f9fb8ed9762cf5",
            "filename": "third_party/xla/xla/service/gpu/custom_call_test.cc",
            "status": "modified",
            "additions": 90,
            "deletions": 0,
            "changes": 90,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9b51864c7b02f3c82dc77cd760233f424a91d06d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9b51864c7b02f3c82dc77cd760233f424a91d06d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc?ref=9b51864c7b02f3c82dc77cd760233f424a91d06d",
            "patch": "@@ -21,6 +21,10 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/no_destructor.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"xla/literal_util.h\"\n+\n #if GOOGLE_CUDA\n #include \"third_party/gpus/cuda/include/cuda.h\"  // IWYU pragma: keep\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n@@ -778,6 +782,92 @@ TEST_F(CustomCallTest, FfiExecutionState) {\n   TF_ASSERT_OK(ExecuteAndTransfer(&b, {}).status());\n }\n \n+//===----------------------------------------------------------------------===//\n+// Asynchronous custom calls example.\n+//===----------------------------------------------------------------------===//\n+\n+// This is an example of how to implement an asynchronous custom call:\n+//\n+//   1. Start custom call initiates async operations and extends the lifetime of\n+//      the input buffer by aliasing it with the output.\n+//   2. Done custom call waits for the async operations to complete and returns\n+//      the result.\n+//\n+// Because HLO type system doesn't allow to express arbitrary values passed\n+// between operations, we rely on a \"side channel\" to communicate between\n+// start and done custom calls. In this example, this side channel is\n+// implemented as a global static map.\n+static absl::NoDestructor<absl::flat_hash_map<int32_t, void*>> async_work_map;\n+\n+static absl::Status AsyncStartCustomCall(ffi::AnyBuffer arg,\n+                                         ffi::Result<ffi::AnyBuffer> ret,\n+                                         int32_t channel) {\n+  // Inside that start custom call we alias input with output and by doing that\n+  // extend the lifetime of the input buffer until the linked done custom call.\n+  EXPECT_EQ(arg.untyped_data(), ret->untyped_data());\n+  EXPECT_EQ(arg.element_type(), F32);\n+  EXPECT_EQ(ret->element_type(), F32);\n+\n+  EXPECT_TRUE(async_work_map->empty());\n+  async_work_map->insert({channel, arg.untyped_data()});\n+\n+  return absl::OkStatus();\n+}\n+\n+static absl::Status AsyncDoneCustomCall(ffi::AnyBuffer arg,\n+                                        ffi::Result<ffi::AnyBuffer> ret,\n+                                        int32_t channel) {\n+  // In done custom call we \"allocate\" real result buffer.\n+  EXPECT_NE(arg.untyped_data(), ret->untyped_data());\n+  EXPECT_EQ(arg.element_type(), F32);\n+\n+  // Chat that argument is the same as the one we put into a map earlier.\n+  EXPECT_EQ(async_work_map->at(channel), arg.untyped_data());\n+\n+  return absl::OkStatus();\n+}\n+\n+XLA_FFI_DEFINE_HANDLER(\n+    kAsyncStartCustomCall, AsyncStartCustomCall,\n+    ffi::Ffi::Bind().Arg<ffi::AnyBuffer>().Ret<ffi::AnyBuffer>().Attr<int32_t>(\n+        \"channel\"));\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), \"xla.gpu.async_start_custom_call\",\n+                         PLATFORM, kAsyncStartCustomCall);\n+\n+XLA_FFI_DEFINE_HANDLER(\n+    kAsyncDoneCustomCall, AsyncDoneCustomCall,\n+    ffi::Ffi::Bind().Arg<ffi::AnyBuffer>().Ret<ffi::AnyBuffer>().Attr<int32_t>(\n+        \"channel\"));\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), \"xla.gpu.async_done_custom_call\",\n+                         PLATFORM, kAsyncDoneCustomCall);\n+\n+TEST_F(CustomCallTest, AsyncCustomCalls) {\n+  auto shape = ShapeUtil::MakeShape(F32, {});\n+\n+  XlaBuilder b(TestName());\n+  auto p0 = Parameter(&b, 0, shape, \"p0\");\n+\n+  auto start = CustomCall(\n+      &b, \"xla.gpu.async_start_custom_call\",\n+      /*operands=*/{Copy(p0)}, ShapeUtil::MakeShape(F32, {}),\n+      /*opaque=*/\"{channel = 0 : i32}\",\n+      /*has_side_effect=*/false,\n+      /*output_operand_aliasing=*/{{{}, {0, {}}}}, /*literal=*/nullptr,\n+      /*schedule=*/CustomCallSchedule::SCHEDULE_NONE,\n+      /*api_version=*/CustomCallApiVersion::API_VERSION_TYPED_FFI);\n+\n+  CustomCall(&b, \"xla.gpu.async_done_custom_call\",\n+             /*operands=*/{start}, ShapeUtil::MakeShape(F32, {}),\n+             /*opaque=*/\"{channel = 0 : i32}\",\n+             /*has_side_effect=*/false,\n+             /*output_operand_aliasing=*/{}, /*literal=*/nullptr,\n+             /*schedule=*/CustomCallSchedule::SCHEDULE_NONE,\n+             /*api_version=*/CustomCallApiVersion::API_VERSION_TYPED_FFI);\n+\n+  Literal literal = LiteralUtil::CreateR0<float>(42.0f);\n+  TF_ASSERT_OK(ExecuteAndTransfer(&b, {&literal}).status());\n+}\n+\n //===----------------------------------------------------------------------===//\n // Testing the use of buffers in custom calls.\n //===----------------------------------------------------------------------===//"
        }
    ],
    "stats": {
        "total": 107,
        "additions": 107,
        "deletions": 0
    }
}