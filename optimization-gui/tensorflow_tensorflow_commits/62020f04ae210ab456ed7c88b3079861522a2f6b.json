{
    "author": "loislo",
    "message": "[XLA:GPU] Refine scaled_dot composite rewriting logic.\n\nThe composite rewriter now supports two types of scaled_dot patterns: FP8 inputs with FP8 scales where the scale factor is a multiple of 32, and BF16 inputs with BF16 scales that are constant tensors of all ones. The test suite has been updated to cover these different scenarios.\n\nPiperOrigin-RevId: 845204253",
    "sha": "62020f04ae210ab456ed7c88b3079861522a2f6b",
    "files": [
        {
            "sha": "89b60223b66765a9d49a290cb5457e61ca02f2f0",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62020f04ae210ab456ed7c88b3079861522a2f6b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62020f04ae210ab456ed7c88b3079861522a2f6b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=62020f04ae210ab456ed7c88b3079861522a2f6b",
            "patch": "@@ -2371,6 +2371,7 @@ cc_library(\n     srcs = [\"composite_rewriter.cc\"],\n     hdrs = [\"composite_rewriter.h\"],\n     deps = [\n+        \"//xla:literal\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n@@ -2397,7 +2398,9 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "dc8e754bf52ae27b41e1c9dacd95762d53087c5b",
            "filename": "third_party/xla/xla/service/gpu/transforms/composite_rewriter.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 20,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62020f04ae210ab456ed7c88b3079861522a2f6b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62020f04ae210ab456ed7c88b3079861522a2f6b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter.cc?ref=62020f04ae210ab456ed7c88b3079861522a2f6b",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/literal.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -143,28 +144,51 @@ absl::StatusOr<bool> CompositeRewriter::RewriteComputation(\n     const HloInstruction* lhs_scale = call->operand(2);\n     const HloInstruction* rhs_scale = call->operand(3);\n \n-    if (lhs->shape().element_type() != BF16) {\n-      int64_t contracting_dim =\n-          dot_dimension_numbers.lhs_contracting_dimensions(0);\n-      int64_t scale_factor = lhs->shape().dimensions(contracting_dim) /\n-                             lhs_scale->shape().dimensions(contracting_dim);\n-      if (scale_factor != 32) {\n-        VLOG(2) << \"LHS scale_factor is not 32: \" << scale_factor\n-                << \" ignore such scaled_dot. It will be inlined later.\";\n-        continue;\n+    int64_t lhs_contracting_dim =\n+        dot_dimension_numbers.lhs_contracting_dimensions(0);\n+    int64_t rhs_contracting_dim =\n+        dot_dimension_numbers.rhs_contracting_dimensions(0);\n+\n+    auto is_supported = [&](const HloInstruction* operand,\n+                            const HloInstruction* scale,\n+                            int64_t contracting_dim) {\n+      auto op_type = operand->shape().element_type();\n+      auto scale_type = scale->shape().element_type();\n+      if ((op_type == F8E4M3FN || op_type == F8E5M2) &&\n+          scale_type == F8E8M0FNU) {\n+        if (contracting_dim >= scale->shape().dimensions_size()) {\n+          return false;\n+        }\n+        int64_t operand_dim_size = operand->shape().dimensions(contracting_dim);\n+        int64_t scale_dim_size = scale->shape().dimensions(contracting_dim);\n+\n+        if (scale_dim_size == 0 || operand_dim_size % scale_dim_size != 0) {\n+          return false;\n+        }\n+        int64_t scale_factor = operand_dim_size / scale_dim_size;\n+        return scale_factor % 32 == 0;\n       }\n-    }\n-\n-    if (rhs->shape().element_type() != BF16) {\n-      int64_t contracting_dim =\n-          dot_dimension_numbers.rhs_contracting_dimensions(0);\n-      int64_t scale_factor = rhs->shape().dimensions(contracting_dim) /\n-                             rhs_scale->shape().dimensions(contracting_dim);\n-      if (scale_factor != 32) {\n-        VLOG(2) << \"RHS scale_factor is not 32: \" << scale_factor\n-                << \" ignore such scaled_dot for now. It will be inlined later.\";\n-        continue;\n+      if (op_type == BF16 && scale_type == BF16) {\n+        if (scale->shape().dimensions_size() !=\n+            operand->shape().dimensions_size()) {\n+          return false;\n+        }\n+        for (int64_t dim : scale->shape().dimensions()) {\n+          if (dim != 1) {\n+            return false;\n+          }\n+        }\n+        if (scale->opcode() != HloOpcode::kConstant) {\n+          return false;\n+        }\n+        return scale->literal().IsAllFloat(1.0);\n       }\n+      return false;\n+    };\n+\n+    if (!is_supported(lhs, lhs_scale, lhs_contracting_dim) ||\n+        !is_supported(rhs, rhs_scale, rhs_contracting_dim)) {\n+      continue;\n     }\n \n     PrecisionConfig precision{};"
        },
        {
            "sha": "e81a5f13a9e842094ac024091670d1633ceb1d3a",
            "filename": "third_party/xla/xla/service/gpu/transforms/composite_rewriter_test.cc",
            "status": "modified",
            "additions": 231,
            "deletions": 32,
            "changes": 263,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62020f04ae210ab456ed7c88b3079861522a2f6b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62020f04ae210ab456ed7c88b3079861522a2f6b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcomposite_rewriter_test.cc?ref=62020f04ae210ab456ed7c88b3079861522a2f6b",
            "patch": "@@ -15,66 +15,265 @@ limitations under the License.\n \n #include \"xla/service/gpu/transforms/composite_rewriter.h\"\n \n+#include <optional>\n #include <string>\n+#include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n #include \"absl/status/status_matchers.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/strings/substitute.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n namespace {\n \n-TEST(CompositeRewriterTest, ScaledDotCompositeRewrite) {\n-  const std::string hlo_string = R\"(\n-    HloModule jit_my_dot\n+struct TestCase {\n+  std::string test_name;\n+  std::string lhs_type;\n+  std::string rhs_type;\n+  std::string lhs_scale_type;\n+  std::string rhs_scale_type;\n+  std::string lhs_scale_shape;\n+  std::string rhs_scale_shape;\n+  std::optional<float> lhs_scale_const_val;\n+  std::optional<float> rhs_scale_const_val;\n+  bool expected_rewrite;\n+};\n+\n+std::string GenerateHlo(const TestCase& test_case) {\n+  // Helper to generate scale definition (either parameter or constant)\n+  // and maintain the list of main parameters.\n+  std::string main_params_decl;\n+  std::vector<std::string> call_operands;\n+  int param_idx = 0;\n+\n+  // LHS operand (always param 0)\n+  main_params_decl +=\n+      absl::Substitute(\"  %lhs = $0[3,128,256]{2,1,0} parameter($1)\\n\",\n+                       test_case.lhs_type, param_idx++);\n+  call_operands.push_back(\"%lhs\");\n+\n+  // RHS operand (always param 1)\n+  main_params_decl +=\n+      absl::Substitute(\"  %rhs = $0[3,256,128]{2,1,0} parameter($1)\\n\",\n+                       test_case.rhs_type, param_idx++);\n+  call_operands.push_back(\"%rhs\");\n+\n+  // LHS Scale\n+  if (test_case.lhs_scale_const_val.has_value()) {\n+    std::string val_str = std::to_string(*test_case.lhs_scale_const_val);\n+    // Remove trailing zeros for cleanliness\n+    val_str.erase(val_str.find_last_not_of('0') + 1, std::string::npos);\n+    if (val_str.back() == '.') {\n+      val_str.pop_back();\n+    }\n+\n+    std::string literal;\n+    if (test_case.lhs_scale_shape == \"3,1,1\") {\n+      literal = absl::Substitute(\"{{{$0}}, {{$0}}, {{$0}}}\", val_str);\n+    } else {\n+      // Assume rank 3 scalar for 1,1,1 or others\n+      literal = absl::Substitute(\"{{{$0}}}\", val_str);\n+    }\n+\n+    main_params_decl += absl::Substitute(\n+        \"  %lhs_scales = $0[$1]{2,1,0} constant($2)\\n\",\n+        test_case.lhs_scale_type, test_case.lhs_scale_shape, literal);\n+  } else {\n+    main_params_decl += absl::Substitute(\n+        \"  %lhs_scales = $0[$1]{2,1,0} parameter($2)\\n\",\n+        test_case.lhs_scale_type, test_case.lhs_scale_shape, param_idx++);\n+  }\n+  call_operands.push_back(\"%lhs_scales\");\n+\n+  // RHS Scale\n+  if (test_case.rhs_scale_const_val.has_value()) {\n+    std::string val_str = std::to_string(*test_case.rhs_scale_const_val);\n+    val_str.erase(val_str.find_last_not_of('0') + 1, std::string::npos);\n+    if (val_str.back() == '.') {\n+      val_str.pop_back();\n+    }\n+\n+    std::string literal;\n+    if (test_case.rhs_scale_shape == \"3,1,1\") {\n+      literal = absl::Substitute(\"{{{$0}}, {{$0}}, {{$0}}}\", val_str);\n+    } else {\n+      literal = absl::Substitute(\"{{{$0}}}\", val_str);\n+    }\n+\n+    main_params_decl += absl::Substitute(\n+        \"  %rhs_scales = $0[$1]{2,1,0} constant($2)\\n\",\n+        test_case.rhs_scale_type, test_case.rhs_scale_shape, literal);\n+  } else {\n+    main_params_decl += absl::Substitute(\n+        \"  %rhs_scales = $0[$1]{2,1,0} parameter($2)\\n\",\n+        test_case.rhs_scale_type, test_case.rhs_scale_shape, param_idx++);\n+  }\n+  call_operands.push_back(\"%rhs_scales\");\n+\n+  // Construct the HLO string\n+  // Note: We use a dummy body for xla.scaled_dot.1 because the rewriter\n+  // currently doesn't inspect it, only the call site.\n+  // We match the parameter types to avoid parser errors.\n+  std::string hlo_template = R\"(\n+    HloModule test_module\n \n     %xla.scaled_dot.1 {\n-      %lhs = f8e4m3fn[3,128,256]{2,1,0} parameter(0)\n-      %lhs_bf16 = bf16[3,128,256]{2,1,0} convert(%lhs)\n-      %lhs_scales = f8e8m0fnu[3,128,8]{2,1,0} parameter(2)\n-      %lhs_scales_bf16 = bf16[3,128,8]{2,1,0} convert(%lhs_scales)\n-      %lhs_scales_bf16_broadcasted = bf16[3,128,8,32]{3,2,1,0} broadcast(%lhs_scales_bf16), dimensions={0,1,2}\n-      %lhs_scales_broadcasted = bf16[3,128,256]{2,1,0} reshape(%lhs_scales_bf16_broadcasted)\n-      %lhs_scaled = bf16[3,128,256]{2,1,0} multiply(%lhs_bf16, %lhs_scales_broadcasted)\n-      %rhs = f8e4m3fn[3,128,256]{2,1,0} parameter(1)\n-      %rhs_bf16 = bf16[3,128,256]{2,1,0} convert(%rhs)\n-      %rhs_scales = f8e8m0fnu[3,128,8]{2,1,0} parameter(3)\n-      %rhs_scales_bf16 = bf16[3,128,8]{2,1,0} convert(%rhs_scales)\n-      %rhs_scales_bf16_broadcasted = bf16[3,128,8,32]{3,2,1,0} broadcast(%rhs_scales_bf16), dimensions={0,1,2}\n-      %rhs_scales_broadcasted = bf16[3,128,256]{2,1,0} reshape(%rhs_scales_bf16_broadcasted)\n-      %rhs_scaled = bf16[3,128,256]{2,1,0} multiply(%rhs_bf16, %rhs_scales_broadcasted)\n-      %rhs_scaled_transposed = bf16[3,256,128]{1,2,0} transpose(%rhs_scaled), dimensions={0,2,1}\n-      ROOT %dot_general.1 = bf16[3,128,128]{2,1,0} dot(%lhs_scaled, %rhs_scaled_transposed),\n-          lhs_batch_dims={0},\n-          lhs_contracting_dims={2},\n-          rhs_batch_dims={0},\n-          rhs_contracting_dims={1}\n+      %p0 = $0[3,128,256]{2,1,0} parameter(0)\n+      %p1 = $1[3,256,128]{2,1,0} parameter(1)\n+      %p2 = $2[$4]{2,1,0} parameter(2)\n+      %p3 = $3[$5]{2,1,0} parameter(3)\n+      // Dummy root with correct shape\n+      ROOT %dummy = bf16[3,128,128]{2,1,0} constant({...})\n     }\n \n     ENTRY %main {\n-      %lhs = f8e4m3fn[3,128,256]{2,1,0} parameter(0)\n-      %rhs = f8e4m3fn[3,256,128]{2,1,0} parameter(1)\n-      %lhs_scales = f8e8m0fnu[3,128,8]{2,1,0} parameter(2)\n-      %rhs_scales = f8e8m0fnu[3,8,128]{2,1,0} parameter(3)\n-      ROOT %call.1 = bf16[3,128,128]{2,1,0} call(%lhs, %rhs, %lhs_scales, %rhs_scales),\n+      $6\n+      ROOT %call = bf16[3,128,128]{2,1,0} call($7),\n           to_apply=%xla.scaled_dot.1,\n           is_composite=true,\n           frontend_attributes={\n             composite.attributes=\"{dimension_numbers=[[[2],[1]],[[0],[0]]]}\",\n             composite.name=\"xla.scaled_dot\",\n             composite.version=\"1\"\n           }\n-    })\";\n+    }\n+  )\";\n+\n+  std::string call_operands_str = absl::StrJoin(call_operands, \", \");\n+\n+  return absl::Substitute(hlo_template, test_case.lhs_type, test_case.rhs_type,\n+                          test_case.lhs_scale_type, test_case.rhs_scale_type,\n+                          test_case.lhs_scale_shape, test_case.rhs_scale_shape,\n+                          main_params_decl, call_operands_str);\n+}\n+\n+class CompositeRewriterParameterizedTest\n+    : public ::testing::TestWithParam<TestCase> {};\n+\n+TEST_P(CompositeRewriterParameterizedTest, Run) {\n+  const TestCase& test_case = GetParam();\n+  std::string hlo_string = GenerateHlo(test_case);\n+  LOG(INFO) << \"HLO string: \\n\" << hlo_string;\n+\n   CompositeRewriter rewriter;\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnUnverifiedModule(hlo_string));\n-  EXPECT_THAT(rewriter.Run(module.get()), absl_testing::IsOkAndHolds(true));\n-  EXPECT_THAT(module->entry_computation()->root_instruction()->opcode(),\n-              HloOpcode::kScaledDot);\n+\n+  auto result = rewriter.Run(module.get());\n+\n+  if (test_case.expected_rewrite) {\n+    EXPECT_THAT(result, absl_testing::IsOkAndHolds(true));\n+    EXPECT_THAT(module->entry_computation()->root_instruction()->opcode(),\n+                HloOpcode::kScaledDot);\n+  } else {\n+    // If it didn't rewrite, it should either be OkAndHolds(false)\n+    // or arguably just check that the opcode is still Call.\n+    // The current implementation returns OkAndHolds(false) if no change.\n+    EXPECT_THAT(result, absl_testing::IsOkAndHolds(false));\n+    EXPECT_THAT(module->entry_computation()->root_instruction()->opcode(),\n+                HloOpcode::kCall);\n+  }\n }\n \n+INSTANTIATE_TEST_SUITE_P(\n+\n+    ScaledDotTests, CompositeRewriterParameterizedTest,\n+\n+    ::testing::Values(\n+        TestCase{\n+            /*test_name=*/\"FP8_Standard_Case\",\n+            /*lhs_type=*/\"f8e4m3fn\",\n+            /*rhs_type=*/\"f8e4m3fn\",\n+            /*lhs_scale_type=*/\"f8e8m0fnu\",\n+            /*rhs_scale_type=*/\"f8e8m0fnu\",\n+            /*lhs_scale_shape=*/\"3,128,8\",\n+            /*rhs_scale_shape=*/\"3,8,128\",\n+            /*lhs_scale_const_val=*/std::nullopt,\n+            /*rhs_scale_const_val=*/std::nullopt,\n+            /*expected_rewrite=*/true,\n+        },\n+        TestCase{\n+            /*test_name=*/\"BF16_Identity_Case\",\n+            /*lhs_type=*/\"bf16\",\n+            /*rhs_type=*/\"bf16\",\n+            /*lhs_scale_type=*/\"bf16\",\n+            /*rhs_scale_type=*/\"bf16\",\n+            /*lhs_scale_shape=*/\"1,1,1\",\n+            /*rhs_scale_shape=*/\"1,1,1\",\n+            /*lhs_scale_const_val=*/1.0f,\n+            /*rhs_scale_const_val=*/1.0f,\n+            /*expected_rewrite=*/true,\n+        },\n+        TestCase{\n+            /*test_name=*/\"BF16_Invalid_Scale_Value\",\n+            /*lhs_type=*/\"bf16\",\n+            /*rhs_type=*/\"bf16\",\n+            /*lhs_scale_type=*/\"bf16\",\n+            /*rhs_scale_type=*/\"bf16\",\n+            /*lhs_scale_shape=*/\"1,1,1\",\n+            /*rhs_scale_shape=*/\"1,1,1\",\n+            /*lhs_scale_const_val=*/1.0f,\n+            /*rhs_scale_const_val=*/2.0f,\n+            /*expected_rewrite=*/false,\n+        },\n+        TestCase{\n+            /*test_name=*/\"BF16_Invalid_Scale_Shape\",\n+            /*lhs_type=*/\"bf16\",\n+            /*rhs_type=*/\"bf16\",\n+            /*lhs_scale_type=*/\"bf16\",\n+            /*rhs_scale_type=*/\"bf16\",\n+            /*lhs_scale_shape=*/\"3,128,1\",\n+            /*rhs_scale_shape=*/\"1,1,1\",\n+            /*lhs_scale_const_val=*/std::nullopt,\n+            /*rhs_scale_const_val=*/1.0f,\n+            /*expected_rewrite=*/false,\n+        },\n+        TestCase{\n+            /*test_name=*/\"Mixed_Type_Fail_BF16_Scale_With_FP8_Op\",\n+            /*lhs_type=*/\"f8e4m3fn\",\n+            /*rhs_type=*/\"f8e4m3fn\",\n+            /*lhs_scale_type=*/\"bf16\",\n+            /*rhs_scale_type=*/\"f8e8m0fnu\",\n+            /*lhs_scale_shape=*/\"3,128,8\",\n+            /*rhs_scale_shape=*/\"3,8,128\",\n+            /*lhs_scale_const_val=*/std::nullopt,\n+            /*rhs_scale_const_val=*/std::nullopt,\n+            /*expected_rewrite=*/false,\n+        },\n+        TestCase{\n+            /*test_name=*/\"FP8_ScaleFactor_16\",\n+            /*lhs_type=*/\"f8e4m3fn\",\n+            /*rhs_type=*/\"f8e4m3fn\",\n+            /*lhs_scale_type=*/\"f8e8m0fnu\",\n+            /*rhs_scale_type=*/\"f8e8m0fnu\",\n+            /*lhs_scale_shape=*/\"3,128,16\",  // 256 / 16 = 16 (not divisible by\n+                                             // 32)\n+            /*rhs_scale_shape=*/\"3,8,128\",\n+            /*lhs_scale_const_val=*/std::nullopt,\n+            /*rhs_scale_const_val=*/std::nullopt,\n+            /*expected_rewrite=*/false,\n+        },\n+        TestCase{\n+            /*test_name=*/\"FP8_ScaleFactor_64\",\n+            /*lhs_type=*/\"f8e4m3fn\",\n+            /*rhs_type=*/\"f8e4m3fn\",\n+            /*lhs_scale_type=*/\"f8e8m0fnu\",\n+            /*rhs_scale_type=*/\"f8e8m0fnu\",\n+            /*lhs_scale_shape=*/\"3,128,4\",  // 256 / 4 = 64 (divisible by 32)\n+            /*rhs_scale_shape=*/\"3,8,128\",\n+            /*lhs_scale_const_val=*/std::nullopt,\n+            /*rhs_scale_const_val=*/std::nullopt,\n+            /*expected_rewrite=*/true,\n+        }),\n+    [](const ::testing::TestParamInfo<TestCase>& info) {\n+      return info.param.test_name;\n+    });\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 330,
        "additions": 278,
        "deletions": 52
    }
}