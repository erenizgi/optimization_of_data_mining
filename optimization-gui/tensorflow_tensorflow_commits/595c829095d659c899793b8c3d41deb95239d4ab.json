{
    "author": "mwhittaker",
    "message": "Allow NCCL communicator creation to be canceled.\n\nRecall that to perform NCCL collectives (e.g., AllReduce, AllGather), a process\ncreates a NCCL communicator and then uses the communicator to perform\ncollectives. If a participant of a collective fails, the collective can hang\nforever. However, we can abort a NCCL communicator which aborts all pending\ncollectives being performed on it.\n\nCreating a communicator can also block if a participant fails, but the code\ndidn't support canceling the creation of the communicator. This change allows\nfor communicator creation to be aborted.\n\nThe change isn't very deep. It's mostly just plumbing around some\n`atomic_bool`s which are set to true when we want to cancel.\n\nPiperOrigin-RevId: 829501509",
    "sha": "595c829095d659c899793b8c3d41deb95239d4ab",
    "files": [
        {
            "sha": "89a1d8dd08a419c6865b54b2a959e32e69ee2c10",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=595c829095d659c899793b8c3d41deb95239d4ab",
            "patch": "@@ -191,7 +191,6 @@ cc_library(\n     srcs = [\"gpu_collectives.cc\"],\n     hdrs = [\"gpu_collectives.h\"],\n     deps = [\n-        \":gpu_communicator\",\n         \"//xla:executable_run_options\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n@@ -200,6 +199,7 @@ cc_library(\n         \"//xla/core/collectives:clique_key\",\n         \"//xla/core/collectives:collectives_registry\",\n         \"//xla/core/collectives:communicator\",\n+        \"//xla/core/collectives:rank_id\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:global_device_id\",\n         \"//xla/stream_executor:device_memory\",\n@@ -210,6 +210,7 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/types:span\",\n         \"@local_tsl//tsl/platform:casts\",\n     ],\n )"
        },
        {
            "sha": "33a11217ae575ff3b3c4d6c56c21791578475d7a",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_cliques.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 18,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc?ref=595c829095d659c899793b8c3d41deb95239d4ab",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n \n+#include <atomic>\n #include <cstdint>\n #include <cstdlib>\n #include <memory>\n@@ -101,7 +102,15 @@ namespace {\n // Container for initialized and ready to use local (in-process) GPU cliques.\n struct ProcessGpuCliques {\n   absl::Mutex mu;\n+\n+  // GpuCliques, keyed by GpuCliqueKey.\n   absl::node_hash_map<GpuCliqueKey, LockableGpuClique> map ABSL_GUARDED_BY(mu);\n+\n+  // Booleans that can be set to cancel the construction of a GpuClique.\n+  absl::node_hash_map<GpuCliqueKey, std::atomic_bool> cancel\n+      ABSL_GUARDED_BY(mu);\n+\n+  // The latest state of every task.\n   std::vector<tensorflow::CoordinatedTaskStateInfo> task_state_infos\n       ABSL_GUARDED_BY(mu);\n };\n@@ -344,24 +353,36 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         clique_ids.fingerprint(), peer_access_enabled);\n \n     ProcessGpuCliques& cliques = GetProcessGpuCliques();\n+    std::atomic_bool* cancel = nullptr;\n     {\n       VLOG(5) << \"Locking cliques.mu\";\n       absl::MutexLock lock(cliques.mu);\n       VLOG(5) << \"Checking clique key \" << clique_key.ToString()\n               << \" for staleness\";\n       TF_RETURN_IF_ERROR(\n           CheckCliqueKeyIsntStaleImpl(cliques.task_state_infos, clique_key));\n+      auto [it, unused_inserted] = cliques.cancel.emplace(clique_key, false);\n+      cancel = &it->second;\n     }\n \n     VLOG(5) << \"Creating communicators\";\n-    TF_ASSIGN_OR_RETURN(\n-        std::vector<std::unique_ptr<Communicator>> created_comms,\n-        collectives->CreateCommunicators(clique_key, clique_ids, ranks,\n-                                         config));\n+    // Don't hold cliques.mu while creating the communicators, because creating\n+    // communicators can block.\n+    absl::StatusOr<std::vector<std::unique_ptr<Communicator>>> created_comms =\n+        collectives->CreateCommunicatorsWithCancel(clique_key, clique_ids,\n+                                                   ranks, config, cancel);\n+\n+    VLOG(5) << \"Locking cliques.mu\";\n+    absl::MutexLock lock(cliques.mu);\n+    cliques.cancel.erase(clique_key);\n+\n+    if (!created_comms.ok()) {\n+      return created_comms.status();\n+    }\n \n     absl::btree_map<RankId, std::unique_ptr<Communicator>> comms;\n     for (size_t i = 0; i < ranks.size(); ++i) {\n-      comms[ranks[i].rank] = std::move(created_comms[i]);\n+      comms[ranks[i].rank] = std::move((*created_comms)[i]);\n     }\n \n     VLOG(3) << absl::StreamFormat(\n@@ -370,14 +391,12 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         clique_key.ToString(), DeviceRanksToString(ranks), nroots,\n         clique_ids.fingerprint(), peer_access_enabled);\n \n-    VLOG(5) << \"Locking cliques.mu\";\n-    absl::MutexLock lock(cliques.mu);\n     if (absl::Status s =\n             CheckCliqueKeyIsntStaleImpl(cliques.task_state_infos, clique_key);\n         !s.ok()) {\n       LOG(WARNING) << \"Clique key \" << clique_key.ToString()\n                    << \" is stale. Aborting recently created communicators.\";\n-      for (std::unique_ptr<Communicator>& comm : created_comms) {\n+      for (auto& [rank, comm] : comms) {\n         TF_RETURN_IF_ERROR(comm->Abort());\n       }\n       return s;\n@@ -538,23 +557,36 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         absl::StrJoin(rank_mapping, \",\", rank_mapping_formatter));\n \n     ProcessGpuCliques& cliques = GetProcessGpuCliques();\n+    std::atomic_bool* cancel = nullptr;\n     {\n       VLOG(5) << \"Locking cliques.mu\";\n       absl::MutexLock lock(cliques.mu);\n       VLOG(5) << \"Checking clique key \" << clique_key.ToString()\n               << \" for staleness\";\n       TF_RETURN_IF_ERROR(\n           CheckCliqueKeyIsntStaleImpl(cliques.task_state_infos, clique_key));\n+      cancel = &cliques.cancel[clique_key];\n+      auto [it, unused_inserted] = cliques.cancel.emplace(clique_key, false);\n+      cancel = &it->second;\n     }\n \n+    // Don't hold cliques.mu while creating the communicators, because creating\n+    // communicators can block.\n     VLOG(5) << \"Splitting communicators\";\n-    TF_ASSIGN_OR_RETURN(auto splitted_comms,\n-                        collectives->SplitCommunicators(parent_comms, color,\n-                                                        keys, config, ranks));\n+    auto splitted_comms = collectives->SplitCommunicatorsWithCancel(\n+        parent_comms, color, keys, config, ranks, cancel);\n+\n+    VLOG(5) << \"Locking cliques.mu\";\n+    absl::MutexLock lock(cliques.mu);\n+    cliques.cancel.erase(clique_key);\n+\n+    if (!splitted_comms.ok()) {\n+      return splitted_comms.status();\n+    }\n \n     absl::btree_map<RankId, std::unique_ptr<Communicator>> comms;\n-    for (size_t i = 0; i < splitted_comms.size(); ++i) {\n-      comms[keys[i]] = std::move(splitted_comms[i]);\n+    for (size_t i = 0; i < splitted_comms->size(); ++i) {\n+      comms[keys[i]] = std::move((*splitted_comms)[i]);\n     }\n \n     VLOG(3) << absl::StreamFormat(\n@@ -565,14 +597,12 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         peer_access_enabled,\n         absl::StrJoin(rank_mapping, \",\", rank_mapping_formatter));\n \n-    VLOG(5) << \"Locking cliques.mu\";\n-    absl::MutexLock lock(cliques.mu);\n     if (absl::Status s =\n             CheckCliqueKeyIsntStaleImpl(cliques.task_state_infos, clique_key);\n         !s.ok()) {\n       LOG(WARNING) << \"Clique key \" << clique_key.ToString()\n                    << \" is stale. Aborting recently split communicators.\";\n-      for (std::unique_ptr<Communicator>& comm : splitted_comms) {\n+      for (auto& [rank, comm] : comms) {\n         TF_RETURN_IF_ERROR(comm->Abort());\n       }\n       return s;\n@@ -713,6 +743,7 @@ bool CliqueKeyContainsIncarnation(\n // REQUIRES: GetProcessGpuCliques().mu held\n static absl::Status AbortCliquesWithIncarnations(\n     absl::node_hash_map<GpuCliqueKey, LockableGpuClique>& map,\n+    absl::node_hash_map<GpuCliqueKey, std::atomic_bool>& cancel,\n     absl::Span<const IncarnationId> incarnations) {\n   VLOG(1) << \"Aborting GPU cliques for incarnations \"\n           << absl::StrJoin(incarnations, \", \",\n@@ -721,6 +752,16 @@ static absl::Status AbortCliquesWithIncarnations(\n                            });\n   const absl::flat_hash_set<IncarnationId> incarnation_set(incarnations.begin(),\n                                                            incarnations.end());\n+\n+  // Cancel pending collectives.\n+  for (auto& [key, b] : cancel) {\n+    if (CliqueKeyContainsIncarnation(key, incarnation_set)) {\n+      VLOG(1) << \"Canceling pending GPU clique \" << key.ToString();\n+      b.store(true);\n+    }\n+  }\n+\n+  // Delete constructed collectives.\n   absl::Status result;\n   for (auto it = map.begin(); it != map.end();) {\n     auto copy = it++;\n@@ -745,6 +786,7 @@ static absl::Status AbortCliquesWithIncarnations(\n // REQUIRES: GetProcessGpuCliques().mu held\n static absl::Status AbortOnFailure(\n     absl::node_hash_map<GpuCliqueKey, LockableGpuClique>& map,\n+    absl::node_hash_map<GpuCliqueKey, std::atomic_bool>& cancel,\n     absl::Span<const tensorflow::CoordinatedTaskStateInfo> previous_state,\n     absl::Span<const tensorflow::CoordinatedTaskStateInfo> current_state) {\n   if (previous_state.empty()) {\n@@ -786,7 +828,7 @@ static absl::Status AbortOnFailure(\n   }\n \n   if (!failed_incarnations.empty()) {\n-    return AbortCliquesWithIncarnations(map, failed_incarnations);\n+    return AbortCliquesWithIncarnations(map, cancel, failed_incarnations);\n   }\n   return absl::OkStatus();\n }\n@@ -795,7 +837,8 @@ absl::Status UpdateGlobalProcessInfo(\n     absl::Span<tensorflow::CoordinatedTaskStateInfo> infos) {\n   ProcessGpuCliques& cliques = GetProcessGpuCliques();\n   absl::MutexLock lock(cliques.mu);\n-  absl::Status s = AbortOnFailure(cliques.map, cliques.task_state_infos, infos);\n+  absl::Status s = AbortOnFailure(cliques.map, cliques.cancel,\n+                                  cliques.task_state_infos, infos);\n   if (!s.ok()) {\n     LOG(WARNING) << s;\n   }"
        },
        {
            "sha": "76062ce505336e52f104953713b283a537682062",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_collectives.h",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_collectives.h?ref=595c829095d659c899793b8c3d41deb95239d4ab",
            "patch": "@@ -16,19 +16,23 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_COLLECTIVES_GPU_COLLECTIVES_H_\n #define XLA_BACKENDS_GPU_COLLECTIVES_GPU_COLLECTIVES_H_\n \n+#include <atomic>\n #include <cstddef>\n #include <cstdint>\n #include <functional>\n #include <memory>\n+#include <optional>\n+#include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/core/collectives/clique_id.h\"\n #include \"xla/core/collectives/clique_key.h\"\n #include \"xla/core/collectives/collectives.h\"\n #include \"xla/core/collectives/communicator.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/global_device_id.h\"\n@@ -95,6 +99,28 @@ class GpuCollectives : public Collectives {\n     bool async_execution = false;\n   };\n \n+  // A cancelable version of Collectives::CreateCommunicators.\n+  virtual absl::StatusOr<std::vector<std::unique_ptr<Communicator>>>\n+  CreateCommunicatorsWithCancel(const CliqueKey& clique_key,\n+                                const std::optional<CliqueIds>& clique_ids,\n+                                absl::Span<const DeviceRank> ranks,\n+                                const Collectives::Config& config,\n+                                std::atomic_bool* cancel) {\n+    // By default, we ignore cancel.\n+    return CreateCommunicators(clique_key, clique_ids, ranks, config);\n+  }\n+\n+  // A cancelable version of Collectives::SplitCommunicators.\n+  virtual absl::StatusOr<std::vector<std::unique_ptr<Communicator>>>\n+  SplitCommunicatorsWithCancel(absl::Span<const Communicator* const> comms,\n+                               int32_t color, absl::Span<const RankId> keys,\n+                               const Collectives::Config& config,\n+                               absl::Span<const DeviceRank> ranks,\n+                               std::atomic_bool* cancel) {\n+    // By default, we ignore cancel.\n+    return SplitCommunicators(comms, color, keys, config, ranks);\n+  }\n+\n   // Returns true if GPU collectives are implemented.\n   virtual bool IsImplemented() const = 0;\n "
        },
        {
            "sha": "2f900d8d119f178c772098b2a9531a4713e70062",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_collectives.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc?ref=595c829095d659c899793b8c3d41deb95239d4ab",
            "patch": "@@ -144,10 +144,10 @@ static absl::StatusOr<ncclUniqueId> AsNcclUniqueId(const CliqueId& clique_id) {\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<Communicator>>>\n-NcclCollectives::CreateCommunicators(const CliqueKey& clique_key,\n-                                     const std::optional<CliqueIds>& clique_ids,\n-                                     absl::Span<const DeviceRank> ranks,\n-                                     const Collectives::Config& config) {\n+NcclCollectives::CreateCommunicatorsWithCancel(\n+    const CliqueKey& clique_key, const std::optional<CliqueIds>& clique_ids,\n+    absl::Span<const DeviceRank> ranks, const Collectives::Config& config,\n+    std::atomic_bool* cancel) {\n   // Validate clique ids. With the NCCL backend, we rely on the host to exchange\n   // unique clique ids.\n   if (!clique_ids.has_value() || clique_ids->data().empty()) {\n@@ -201,7 +201,7 @@ NcclCollectives::CreateCommunicators(const CliqueKey& clique_key,\n       pool.Schedule([&, i]() {\n         absl::StatusOr<std::unique_ptr<NcclCommunicator>> comm =\n             NcclCommunicator::Create(std::bind(make_comm, i),\n-                                     gpu_config.async_execution);\n+                                     gpu_config.async_execution, cancel);\n         if (!comm.ok()) {\n           absl::call_once(once, [&] { status = comm.status(); });\n           return;\n@@ -215,11 +215,10 @@ NcclCollectives::CreateCommunicators(const CliqueKey& clique_key,\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<Communicator>>>\n-NcclCollectives::SplitCommunicators(absl::Span<const Communicator* const> comms,\n-                                    int32_t color,\n-                                    absl::Span<const RankId> keys,\n-                                    const Collectives::Config& config,\n-                                    absl::Span<const DeviceRank> ranks) {\n+NcclCollectives::SplitCommunicatorsWithCancel(\n+    absl::Span<const Communicator* const> comms, int32_t color,\n+    absl::Span<const RankId> keys, const Collectives::Config& config,\n+    absl::Span<const DeviceRank> ranks, std::atomic_bool* cancel) {\n   auto rank_formatter = [](std::string* str, RankId rank) {\n     absl::StrAppend(str, rank.value());\n   };\n@@ -263,7 +262,7 @@ NcclCollectives::SplitCommunicators(absl::Span<const Communicator* const> comms,\n       pool.Schedule([&, i]() {\n         absl::StatusOr<std::unique_ptr<NcclCommunicator>> comm =\n             NcclCommunicator::Create(std::bind(make_comm, i),\n-                                     gpu_config.async_execution);\n+                                     gpu_config.async_execution, cancel);\n         if (!comm.ok()) {\n           absl::call_once(once, [&] { status = comm.status(); });\n           return;"
        },
        {
            "sha": "f12cfb70f5a88e7da2ea16fac3c14aa9afce181b",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_collectives.h",
            "status": "modified",
            "additions": 27,
            "deletions": 5,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.h?ref=595c829095d659c899793b8c3d41deb95239d4ab",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_COLLECTIVES_NCCL_COLLECTIVES_H_\n #define XLA_BACKENDS_GPU_COLLECTIVES_NCCL_COLLECTIVES_H_\n \n+#include <atomic>\n #include <cstdint>\n #include <memory>\n #include <optional>\n@@ -49,15 +50,36 @@ class NcclCollectives : public GpuCollectives {\n   CreateCommunicators(const CliqueKey& clique_key,\n                       const std::optional<CliqueIds>& clique_ids,\n                       absl::Span<const DeviceRank> ranks,\n-                      const Collectives::Config& config) final;\n-\n-  absl::StatusOr<std::unique_ptr<Communicator>> CreateCommunicator() final {\n-    return absl::UnimplementedError(\"Not implemented.\");\n+                      const Collectives::Config& config) final {\n+    return CreateCommunicatorsWithCancel(clique_key, clique_ids, ranks, config,\n+                                         nullptr);\n   }\n+\n+  absl::StatusOr<std::vector<std::unique_ptr<Communicator>>>\n+  CreateCommunicatorsWithCancel(const CliqueKey& clique_key,\n+                                const std::optional<CliqueIds>& clique_ids,\n+                                absl::Span<const DeviceRank> ranks,\n+                                const Collectives::Config& config,\n+                                std::atomic_bool* cancel) final;\n+\n   absl::StatusOr<std::vector<std::unique_ptr<Communicator>>> SplitCommunicators(\n       absl::Span<const Communicator* const> comms, int32_t color,\n       absl::Span<const RankId> keys, const Collectives::Config& config,\n-      absl::Span<const DeviceRank> ranks) final;\n+      absl::Span<const DeviceRank> ranks) final {\n+    return SplitCommunicatorsWithCancel(comms, color, keys, config, ranks,\n+                                        nullptr);\n+  }\n+\n+  absl::StatusOr<std::vector<std::unique_ptr<Communicator>>>\n+  SplitCommunicatorsWithCancel(absl::Span<const Communicator* const> comms,\n+                               int32_t color, absl::Span<const RankId> keys,\n+                               const Collectives::Config& config,\n+                               absl::Span<const DeviceRank> ranks,\n+                               std::atomic_bool* cancel) final;\n+\n+  absl::StatusOr<std::unique_ptr<Communicator>> CreateCommunicator() final {\n+    return absl::UnimplementedError(\"Not implemented.\");\n+  }\n \n   absl::StatusOr<void*> Allocate(uint64_t bytes) final;\n "
        },
        {
            "sha": "428e52862a0f2102f5e5556b391167bfe0ff674f",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=595c829095d659c899793b8c3d41deb95239d4ab",
            "patch": "@@ -225,11 +225,15 @@ class NcclCommunicator::NcclRegisteredBufferHandle\n \n absl::StatusOr<std::unique_ptr<NcclCommunicator>> NcclCommunicator::Create(\n     absl::AnyInvocable<absl::StatusOr<ncclComm_t>()> make_comm, bool is_async,\n-    tsl::Env& env) {\n-  // TODO(mwhittaker): There is currently no way to abort these operations.\n-  auto f = [&make_comm]() -> absl::StatusOr<ncclComm_t> {\n+    std::atomic_bool* cancel, tsl::Env& env) {\n+  auto f = [cancel, &make_comm]() -> absl::StatusOr<ncclComm_t> {\n     TF_ASSIGN_OR_RETURN(ncclComm_t comm, make_comm());\n-    TF_RETURN_IF_ERROR(::xla::gpu::PollUntilDone(comm, std::atomic_bool{}));\n+    if (cancel) {\n+      TF_RETURN_IF_ERROR(::xla::gpu::PollUntilDone(comm, *cancel));\n+    } else {\n+      std::atomic_bool never_cancelled;\n+      TF_RETURN_IF_ERROR(::xla::gpu::PollUntilDone(comm, never_cancelled));\n+    }\n     return comm;\n   };\n "
        },
        {
            "sha": "5620a5aa3130a1b3f4884ed8bedfbda057cddbeb",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/595c829095d659c899793b8c3d41deb95239d4ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.h?ref=595c829095d659c899793b8c3d41deb95239d4ab",
            "patch": "@@ -69,7 +69,8 @@ class NcclCommunicator : public GpuCommunicator {\n   // synchronously on the calling thread.\n   static absl::StatusOr<std::unique_ptr<NcclCommunicator>> Create(\n       absl::AnyInvocable<absl::StatusOr<ncclComm_t>()> make_comm,\n-      bool is_async = false, tsl::Env& env = *tsl::Env::Default());\n+      bool is_async = false, std::atomic_bool* cancel = nullptr,\n+      tsl::Env& env = *tsl::Env::Default());\n \n   ~NcclCommunicator() override;\n "
        }
    ],
    "stats": {
        "total": 178,
        "additions": 137,
        "deletions": 41
    }
}