{
    "author": "Aelphy",
    "message": "Supported int2 in xnnpack_delegate.\n\nPiperOrigin-RevId: 845875757",
    "sha": "fe02aab3112b67dedd407d39e720741a8dcc1282",
    "files": [
        {
            "sha": "f390b8065caac26bedde52e5d8dea639a6248667",
            "filename": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "status": "modified",
            "additions": 67,
            "deletions": 10,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fe02aab3112b67dedd407d39e720741a8dcc1282/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fxnnpack_delegate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fe02aab3112b67dedd407d39e720741a8dcc1282/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fxnnpack_delegate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fxnnpack_delegate.cc?ref=fe02aab3112b67dedd407d39e720741a8dcc1282",
            "patch": "@@ -211,7 +211,7 @@ bool CheckZeroPointForPerChannelQuantization(\n   // be 8.\n   for (int c = 0; c < quantization_zero_point.size; c++) {\n     const int zero_point = quantization_zero_point.data[c];\n-    if (zero_point != 0 && (tensor.type != kTfLiteInt4 && zero_point != 8)) {\n+    if (zero_point != 0 && (tensor.type != kTfLiteInt4 || zero_point != 8)) {\n       TF_LITE_KERNEL_LOG(context,\n                          \"unsupported zero-point value (%d) in channel %d of \"\n                          \"%s tensor %d in XNNPACK delegate\",\n@@ -268,7 +268,8 @@ xnn_datatype GetXNNPackDatatype(TfLiteContext* context,\n       return xnn_datatype_quint8;\n     }\n     case kTfLiteInt8:\n-    case kTfLiteInt4: {\n+    case kTfLiteInt4:\n+    case kTfLiteInt2: {\n       switch (tensor.quantization.type) {\n         case kTfLiteAffineQuantization: {\n           const auto quantization_params =\n@@ -320,6 +321,8 @@ xnn_datatype GetXNNPackDatatype(TfLiteContext* context,\n               return xnn_datatype_qcint8;\n             case kTfLiteInt4:\n               return xnn_datatype_qcint4;\n+            case kTfLiteInt2:\n+              return xnn_datatype_qcint2;\n             default:\n               // Outermost switch prevents this\n               TFL_UNREACHABLE();\n@@ -528,6 +531,22 @@ TfLiteStatus DefineXNNPACKValue(TfLiteContext* context, xnn_subgraph_t subgraph,\n           dims.size(), dims.data(), data, XNN_INVALID_VALUE_ID, flags,\n           xnnpack_id);\n     } break;\n+    case xnn_datatype_qcint2: {\n+      status = xnn_define_channelwise_quantized_tensor_value_v3(\n+          subgraph, datatype,\n+          static_cast<const TfLiteAffineQuantization*>(\n+              tensor.quantization.params)\n+              ->zero_point->data[0],\n+          static_cast<const TfLiteAffineQuantization*>(\n+              tensor.quantization.params)\n+              ->scale->data,\n+          dims.size(),\n+          static_cast<const TfLiteAffineQuantization*>(\n+              tensor.quantization.params)\n+              ->quantized_dimension,\n+          dims.data(), data, XNN_INVALID_VALUE_ID, flags, xnnpack_id,\n+          /*channelwise_zero_point=*/nullptr);\n+    } break;\n     case xnn_datatype_qcint4:\n     case xnn_datatype_qcint8:\n     case xnn_datatype_qcint32:\n@@ -2228,18 +2247,21 @@ class Subgraph {\n     return kTfLiteError;\n   }\n \n-  static TfLiteStatus CheckTensorFloat32OrFloat16OrQCInt4OrQCInt8Type(\n-      const Delegate& delegate, TfLiteContext* context,\n-      const TfLiteTensor& tensor, int expected_quantized_dimension,\n-      int tensor_index, int node_index) {\n+  static TfLiteStatus CheckTensorFilterType(const Delegate& delegate,\n+                                            TfLiteContext* context,\n+                                            const TfLiteTensor& tensor,\n+                                            int expected_quantized_dimension,\n+                                            int tensor_index, int node_index) {\n     switch (tensor.type) {\n       case kTfLiteFloat32:\n       case kTfLiteFloat16:\n         return kTfLiteOk;\n+      case kTfLiteInt2:\n       case kTfLiteInt4:\n       case kTfLiteInt8:\n         if (delegate.support_signed_8bit_quantization() &&\n-            (kTfLiteInt8 == tensor.type || kTfLiteInt4 == tensor.type)) {\n+            (kTfLiteInt8 == tensor.type || kTfLiteInt4 == tensor.type ||\n+             kTfLiteInt2 == tensor.type)) {\n           switch (tensor.quantization.type) {\n             case kTfLiteAffineQuantization: {\n               const TfLiteAffineQuantization* quantization_params =\n@@ -2277,6 +2299,20 @@ class Subgraph {\n                     quantization_params->quantized_dimension, tensor_index,\n                     node_index);\n                 return kTfLiteError;\n+              } else if (tensor.type == kTfLiteInt2 &&\n+                         quantization_params->scale->size !=\n+                             SizeOfDimension(\n+                                 &tensor,\n+                                 quantization_params->quantized_dimension)) {\n+                // Only per channel quantized 2 bit weights are supported.\n+                TF_LITE_MAYBE_KERNEL_LOG(\n+                    context,\n+                    \"2 bit weights must be per channel and not per tensor \"\n+                    \"quantized in channel #%\" PRId32\n+                    \" in tensor #%d in node #%d\",\n+                    quantization_params->quantized_dimension, tensor_index,\n+                    node_index);\n+                return kTfLiteError;\n               }\n               break;\n             }\n@@ -4489,7 +4525,7 @@ class Subgraph {\n     // Dynamic filter is supported, but only for FP32.\n     if (!(delegate.support_dynamic_fully_connected_operator() &&\n           filter_tensor.type == kTfLiteFloat32)) {\n-      TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrFloat16OrQCInt4OrQCInt8Type(\n+      TF_LITE_ENSURE_STATUS(CheckTensorFilterType(\n           delegate, logging_context, filter_tensor,\n           /*expected_quantized_dimension=*/0, filter_tensor_id, node_index));\n       if (quasi_static_tensors.count(filter_tensor_id) == 0) {\n@@ -4543,10 +4579,12 @@ class Subgraph {\n     bool dynamically_quantized =\n         (!delegate.disable_dynamically_quantized_ops() &&\n          (input_tensor.type == kTfLiteFloat32 &&\n-          (filter_tensor.type == kTfLiteInt4 ||\n+          (filter_tensor.type == kTfLiteInt2 ||\n+           filter_tensor.type == kTfLiteInt4 ||\n            filter_tensor.type == kTfLiteInt8)));\n     bool supported_srq = (input_tensor.type == kTfLiteInt8 &&\n-                          (filter_tensor.type == kTfLiteInt4 ||\n+                          (filter_tensor.type == kTfLiteInt2 ||\n+                           filter_tensor.type == kTfLiteInt4 ||\n                            filter_tensor.type == kTfLiteInt8));\n     if (input_tensor.type != output_tensor.type ||\n         ((input_tensor.type != filter_tensor.type) &&\n@@ -4567,6 +4605,15 @@ class Subgraph {\n       return kTfLiteError;\n     }\n \n+    if (filter_tensor.type == kTfLiteInt2 && input_channels % 4 != 0) {\n+      TF_LITE_MAYBE_KERNEL_LOG(\n+          logging_context,\n+          \"unsupported non-multiple of 4 number of inputs channels (%d) in\"\n+          \" FULLY_CONNECTED operator #%d\",\n+          input_channels, node_index);\n+      return kTfLiteError;\n+    }\n+\n     float output_min = -std::numeric_limits<float>::infinity();\n     float output_max = +std::numeric_limits<float>::infinity();\n     TF_LITE_ENSURE_STATUS(ConvertActivationToOutputRange(\n@@ -4644,6 +4691,16 @@ class Subgraph {\n             &filter_tensor.dims->data[NumDimensions(&filter_tensor)]);\n         uint32_t kernel_id = XNN_INVALID_VALUE_ID;\n         switch (filter_datatype) {\n+          case xnn_datatype_qcint2: {\n+            int32_t zero_point_value = filter_params->zero_point->data[0];\n+            status = xnn_define_channelwise_quantized_tensor_value_v3(\n+                subgraph, filter_datatype, zero_point_value,\n+                filter_params->scale->data, filter_dims.size(),\n+                /*channel_dim=*/0, filter_dims.data(),\n+                GetTensorData<int8_t>(&filter_tensor), XNN_INVALID_VALUE_ID,\n+                /*flags=*/0, &kernel_id, /*channelwise_zero_point=*/nullptr);\n+            break;\n+          }\n           case xnn_datatype_qcint4:\n           case xnn_datatype_qcint8: {\n             int32_t zero_point_value = filter_params->zero_point->data[0];"
        }
    ],
    "stats": {
        "total": 77,
        "additions": 67,
        "deletions": 10
    }
}