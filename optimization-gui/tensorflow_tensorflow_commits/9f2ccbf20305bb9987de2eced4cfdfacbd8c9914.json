{
    "author": "mrguenther",
    "message": "Legalize MLIR to StableHLO instead of MHLO before lowering\n\nPiperOrigin-RevId: 842359586",
    "sha": "9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
    "files": [
        {
            "sha": "475bd79849e80e09a7eb3884666a32a6760ae63c",
            "filename": "tensorflow/compiler/mlir/tf2xla/api/v1/compile_mlir_util_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Fapi%2Fv1%2Fcompile_mlir_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Fapi%2Fv1%2Fcompile_mlir_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Fapi%2Fv1%2Fcompile_mlir_util_test.cc?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -85,7 +85,7 @@ TEST(LegalizeMlirTest, LegalizesModule) {\n       /*shape_determination_fns=*/{}, &compilation_result);\n \n   EXPECT_TRUE(status.ok());\n-  EXPECT_THAT(status.value(), HasSubstr(\"mhlo.const\"));\n+  EXPECT_THAT(status.value(), HasSubstr(\"stablehlo.constant\"));\n }\n \n TEST(LegalizeMlirTest, FailsLegalizesModule) {"
        },
        {
            "sha": "5e49416bc43a26f5f729b4942756f2c02979a737",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -982,6 +982,18 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n +  return %0 : !stablehlo.token\n +}\n +\n+diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir b/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n+--- stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n++++ stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n+@@ -316,7 +316,7 @@\n+ // Serialized string:\n+ //   \"\\08\\03\\1A\\02\\01\\02\\22\\02\\00\\01\"\n+ func.func @test_custom_call2(%arg0: tensor<16x16xf32>) -> tensor<16x16xf32> {\n+-  %0 = \"stablehlo.custom_call\"(%arg0) {backend_config = \"\", call_target_name = \"Sharding\", stablehlo.sharding = \"\\08\\03\\1A\\02\\01\\02\\22\\02\\00\\01\"} : (tensor<16x16xf32>) -> tensor<16x16xf32>\n++  %0 = \"stablehlo.custom_call\"(%arg0) {backend_config = \"\", call_target_name = \"Sharding\", mhlo.sharding = \"\\08\\03\\1A\\02\\01\\02\\22\\02\\00\\01\"} : (tensor<16x16xf32>) -> tensor<16x16xf32>\n+   func.return %0 : tensor<16x16xf32>\n+ }\n+ \n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir"
        },
        {
            "sha": "d2a31df67d3b4b0b48ac3a0a90027d6c906b454d",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/attribute_exporter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fattribute_exporter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fattribute_exporter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fattribute_exporter.cc?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -490,7 +490,7 @@ std::optional<xla::OpSharding> ExtractShardyResultShardingFromFrontendAttrs(\n   mlir::Operation* defining_op =\n       mlir::sdy::getBodyTerminatorOperand(function, res_num).getDefiningOp();\n   auto custom_call_op =\n-      mlir::dyn_cast_or_null<mlir::mhlo::CustomCallOp>(defining_op);\n+      mlir::dyn_cast_or_null<mlir::stablehlo::CustomCallOp>(defining_op);\n \n   if (custom_call_op == nullptr ||\n       custom_call_op.getCallTargetName() !="
        },
        {
            "sha": "230922533025f06298254e0cfd582e6f72762de0",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/attribute_exporter_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fattribute_exporter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fattribute_exporter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fattribute_exporter_test.cc?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -184,8 +184,8 @@ TEST_F(AttributeExporterTest, ExtractShardyResultShardingFromFrontendAttrs) {\n       \"{mesh = #sdy.mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>}\"\n     }} {\n       func.func @main(%arg0: tensor<8x8xf32>) -> (tensor<8x8xf32>, tensor<8x8xf32>) {\n-        %0 = mhlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n-        %1 = mhlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n+        %0 = stablehlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n+        %1 = stablehlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n         return %0, %1 : tensor<8x8xf32>, tensor<8x8xf32>\n       }\n     }\n@@ -222,8 +222,8 @@ TEST_F(AttributeExporterTest,\n   constexpr absl::string_view mlir_source = R\"mlir(\n     module @test {\n       func.func @main(%arg0: tensor<8x8xf32>) -> (tensor<8x8xf32>, tensor<8x8xf32>) {\n-        %0 = mhlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n-        %1 = mhlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{}, {}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n+        %0 = stablehlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{\\\"x\\\", \\\"y\\\", ?}, {\\\"z\\\"}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n+        %1 = stablehlo.custom_call @local_xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<mesh<[\\\"x\\\"=2, \\\"y\\\"=4, \\\"z\\\"=4]>, [{}, {}]>]>\"}} : (tensor<8x8xf32>) -> tensor<8x8xf32>\n         return %0, %1 : tensor<8x8xf32>, tensor<8x8xf32>\n       }\n     }"
        },
        {
            "sha": "88fc6bc4f2da29a237f0979d9523b2856d1b49ad",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 37,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -3039,8 +3039,8 @@ mlir::LogicalResult ExportXlaOp(SetDimensionSizeOp op, OpLoweringContext ctx) {\n     return op.emitError(shape_or.status().ToString());\n   }\n   xla::XlaOp xla_result;\n-  if (auto constant =\n-          llvm::dyn_cast_or_null<ConstantOp>(op.getSize().getDefiningOp());\n+  if (auto constant = llvm::dyn_cast_or_null<stablehlo::ConstantOp>(\n+          op.getSize().getDefiningOp());\n       constant != nullptr) {\n     auto value = constant.getValue();\n     auto values = value.getValues<mlir::IntegerAttr>();\n@@ -3340,9 +3340,9 @@ LogicalResult ExportXlaOp(ReduceScatterOp op, OpLoweringContext ctx) {\n   return success();\n }\n \n-LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n+LogicalResult ExportXlaOp(mhlo::AsyncStartOp op, OpLoweringContext ctx) {\n   for (auto* user : op.getResult().getUsers()) {\n-    if (!isa<AsyncUpdateOp, AsyncDoneOp>(user)) {\n+    if (!isa<mhlo::AsyncUpdateOp, mhlo::AsyncDoneOp>(user)) {\n       return op.emitOpError() << \"Users of AsyncStart's return value must be \"\n                               << \"async_update or async_done\";\n     }\n@@ -3357,8 +3357,8 @@ LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n   mlir::func::FuncOp callee = ctx.converter->LookUpSymbol(\n       FlatSymbolRefAttr::get(op->getContext(), op.getCalledComputation()));\n \n-  auto all_gather_op =\n-      dyn_cast_or_null<AllGatherOp>(callee.getBody().front().front());\n+  auto all_gather_op = dyn_cast_or_null<stablehlo::AllGatherOp>(\n+      callee.getBody().front().front());\n   if (all_gather_op && SimplyReturnedOp(all_gather_op)) {\n     TensorType operand_type =\n         mlir::cast<TensorType>(all_gather_op.getOperand(0).getType());\n@@ -3378,8 +3378,8 @@ LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n         Convert_use_global_device_ids(all_gather_op.getUseGlobalDeviceIds()));\n     return success();\n   }\n-  auto all_reduce_op =\n-      dyn_cast_or_null<AllReduceOp>(callee.getBody().front().front());\n+  auto all_reduce_op = dyn_cast_or_null<stablehlo::AllReduceOp>(\n+      callee.getBody().front().front());\n   if (all_reduce_op && SimplyReturnedOp(all_reduce_op)) {\n     xla::XlaComputationId computation;\n     if (failed(ctx.converter->LowerRegionAsComputation(\n@@ -3394,8 +3394,8 @@ LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n         Convert_use_global_device_ids(all_reduce_op.getUseGlobalDeviceIds()));\n     return success();\n   }\n-  auto collective_permute_op =\n-      dyn_cast_or_null<CollectivePermuteOp>(callee.getBody().front().front());\n+  auto collective_permute_op = dyn_cast_or_null<stablehlo::CollectivePermuteOp>(\n+      callee.getBody().front().front());\n   if (collective_permute_op && SimplyReturnedOp(collective_permute_op)) {\n     value_map[result] =\n         xla::internal::XlaBuilderFriend::BuildCollectivePermuteStart(\n@@ -3405,7 +3405,8 @@ LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n             Convert_channel_handle(collective_permute_op.getChannelHandle()));\n     return mlir::success();\n   }\n-  auto copy_op = dyn_cast_or_null<CopyOp>(callee.getBody().front().front());\n+  auto copy_op =\n+      dyn_cast_or_null<mhlo::CopyOp>(callee.getBody().front().front());\n   if (copy_op && SimplyReturnedOp(copy_op)) {\n     std::optional<int> cross_program_prefetch_index =\n         copy_op.getCrossProgramPrefetchIndex()\n@@ -3415,7 +3416,8 @@ LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n         ctx.builder, operands[0], cross_program_prefetch_index);\n     return mlir::success();\n   }\n-  auto send_op = dyn_cast_or_null<SendOp>(callee.getBody().front().front());\n+  auto send_op =\n+      dyn_cast_or_null<stablehlo::SendOp>(callee.getBody().front().front());\n   if (send_op && SimplyReturnedOp(send_op)) {\n     xla::XlaOp operand;\n     if (operands.size() == 2)\n@@ -3432,7 +3434,8 @@ LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n         send_op.getIsHostTransfer());\n     return mlir::success();\n   }\n-  auto recv_op = dyn_cast_or_null<RecvOp>(callee.getBody().front().front());\n+  auto recv_op =\n+      dyn_cast_or_null<stablehlo::RecvOp>(callee.getBody().front().front());\n   if (recv_op && SimplyReturnedOp(recv_op)) {\n     auto result_types =\n         mlir::cast<AsyncBundleType>(result.getType()).getTypes()[1];\n@@ -3466,8 +3469,9 @@ LogicalResult ExportXlaOp(AsyncStartOp op, OpLoweringContext ctx) {\n   return success();\n }\n \n-LogicalResult ExportXlaOp(AsyncUpdateOp op, OpLoweringContext ctx) {\n-  if (!isa<AsyncStartOp, AsyncUpdateOp>(op.getBundle().getDefiningOp())) {\n+LogicalResult ExportXlaOp(mhlo::AsyncUpdateOp op, OpLoweringContext ctx) {\n+  if (!isa<mhlo::AsyncStartOp, mhlo::AsyncUpdateOp>(\n+          op.getBundle().getDefiningOp())) {\n     auto theerror = op.emitError()\n                     << \"Defining op of AsyncUpdate's operand must be \"\n                     << \"async_start or async_update\";\n@@ -3480,7 +3484,7 @@ LogicalResult ExportXlaOp(AsyncUpdateOp op, OpLoweringContext ctx) {\n   }\n \n   for (auto* user : op.getResult().getUsers()) {\n-    if (!isa<AsyncUpdateOp, AsyncDoneOp>(user)) {\n+    if (!isa<mhlo::AsyncUpdateOp, mhlo::AsyncDoneOp>(user)) {\n       return op.emitOpError() << \"Users of AsyncUpdate's return value must be \"\n                               << \"async_update or async_done\";\n     }\n@@ -3497,8 +3501,9 @@ LogicalResult ExportXlaOp(AsyncUpdateOp op, OpLoweringContext ctx) {\n   return success();\n }\n \n-LogicalResult ExportXlaOp(AsyncDoneOp op, OpLoweringContext ctx) {\n-  if (!isa<AsyncStartOp, AsyncUpdateOp>(op.getBundle().getDefiningOp())) {\n+LogicalResult ExportXlaOp(mhlo::AsyncDoneOp op, OpLoweringContext ctx) {\n+  if (!isa<mhlo::AsyncStartOp, mhlo::AsyncUpdateOp>(\n+          op.getBundle().getDefiningOp())) {\n     auto theerror = op.emitError()\n                     << \"Defining op of AsyncDone's operand must be \"\n                     << \"async_start or async_update\";\n@@ -3514,64 +3519,69 @@ LogicalResult ExportXlaOp(AsyncDoneOp op, OpLoweringContext ctx) {\n   if (failed(GetXlaOp(op.getBundle(), value_map, &operand, op)))\n     return failure();\n \n-  // Find the AsyncStartOp that starts the async chain.\n+  // Find the mhlo::AsyncStartOp that starts the async chain.\n   Operation* start = op;\n-  while (start != nullptr && !isa<AsyncStartOp>(start)) {\n+  while (start != nullptr && !isa<mhlo::AsyncStartOp>(start)) {\n     start = start->getOperand(0).getDefiningOp();\n-    if (start == nullptr || !isa<AsyncStartOp, AsyncUpdateOp>(start)) {\n+    if (start == nullptr ||\n+        !isa<mhlo::AsyncStartOp, mhlo::AsyncUpdateOp>(start)) {\n       return op.emitError() << \"Defining op of AsyncDone's operand must be \"\n                             << \"async_start or async_update\";\n     }\n   }\n \n-  if (!isa<AsyncStartOp>(start)) {\n+  if (!isa<mhlo::AsyncStartOp>(start)) {\n     return op.emitError() << \"Could not find async chain start\";\n   }\n \n   mlir::func::FuncOp callee =\n       ctx.converter->LookUpSymbol(FlatSymbolRefAttr::get(\n-          op->getContext(), cast<AsyncStartOp>(start).getCalledComputation()));\n+          op->getContext(),\n+          cast<mhlo::AsyncStartOp>(start).getCalledComputation()));\n \n-  auto all_gather_op =\n-      dyn_cast_or_null<AllGatherOp>(callee.getBody().front().front());\n+  auto all_gather_op = dyn_cast_or_null<stablehlo::AllGatherOp>(\n+      callee.getBody().front().front());\n   if (all_gather_op && SimplyReturnedOp(all_gather_op)) {\n     value_map[op.getResult(0)] =\n         xla::internal::XlaBuilderFriend::BuildAllGatherDone(\n             ctx.builder, operand, xla::TypeToShape(all_gather_op.getType(0)));\n     return success();\n   }\n-  auto all_reduce_op =\n-      dyn_cast_or_null<AllReduceOp>(callee.getBody().front().front());\n+  auto all_reduce_op = dyn_cast_or_null<stablehlo::AllReduceOp>(\n+      callee.getBody().front().front());\n   if (all_reduce_op && SimplyReturnedOp(all_reduce_op)) {\n     value_map[op.getResult(0)] =\n         xla::internal::XlaBuilderFriend::BuildAllReduceDone(\n             ctx.builder, operand, xla::TypeToShape(all_reduce_op.getType(0)));\n     return success();\n   }\n-  auto collective_permute_op =\n-      dyn_cast_or_null<CollectivePermuteOp>(callee.getBody().front().front());\n+  auto collective_permute_op = dyn_cast_or_null<stablehlo::CollectivePermuteOp>(\n+      callee.getBody().front().front());\n   if (collective_permute_op && SimplyReturnedOp(collective_permute_op)) {\n     value_map[op.getResult(0)] =\n         xla::internal::XlaBuilderFriend::BuildCollectivePermuteDone(\n             ctx.builder, operand,\n             xla::TypeToShape(collective_permute_op.getType()));\n     return success();\n   }\n-  auto copy_op = dyn_cast_or_null<CopyOp>(callee.getBody().front().front());\n+  auto copy_op =\n+      dyn_cast_or_null<mhlo::CopyOp>(callee.getBody().front().front());\n   if (copy_op && SimplyReturnedOp(copy_op)) {\n     value_map[op.getResult(0)] = xla::internal::XlaBuilderFriend::BuildCopyDone(\n         ctx.builder, operand, xla::TypeToShape(copy_op.getType()));\n     return success();\n   }\n-  auto send_op = dyn_cast_or_null<SendOp>(callee.getBody().front().front());\n+  auto send_op =\n+      dyn_cast_or_null<stablehlo::SendOp>(callee.getBody().front().front());\n   if (send_op && SimplyReturnedOp(send_op)) {\n     value_map[op.getResult(0)] = xla::internal::XlaBuilderFriend::BuildSendDone(\n         ctx.builder, operand,\n         Convert_channel_handle(send_op.getChannelHandle()),\n         send_op.getIsHostTransfer());\n     return success();\n   }\n-  auto recv_op = dyn_cast_or_null<RecvOp>(callee.getBody().front().front());\n+  auto recv_op =\n+      dyn_cast_or_null<stablehlo::RecvOp>(callee.getBody().front().front());\n   if (recv_op && SimplyReturnedOp(recv_op)) {\n     auto result_types =\n         mlir::cast<AsyncBundleType>(op.getBundle().getType()).getTypes()[1];\n@@ -6427,12 +6437,11 @@ absl::Status ConvertMlirHloToHlo(mlir::ModuleOp module,\n #endif\n   pm.enableVerifier(enableVerifier);\n \n-  mhlo::StablehloLegalizeToHloPassOptions shlo_pass_opts;\n-  shlo_pass_opts.convert_xla_supported_stablehlo_ =\n-      !options.direct_stablehlo_to_hlo;\n-  pm.addPass(mlir::mhlo::createStablehloLegalizeToHloPass(shlo_pass_opts));\n+  mhlo::HloLegalizeToStablehloPassOptions shlo_pass_opts;\n+  shlo_pass_opts.allow_xla_features_ = true;\n+  pm.addPass(mlir::mhlo::createHloLegalizeToStablehloPass(shlo_pass_opts));\n   if (failed(pm.run(module))) {\n-    return absl::InternalError(\"Unable to convert StableHLO to MHLO\");\n+    return absl::InternalError(\"Unable to convert MHLO to StableHLO\");\n   }\n \n   TF_RETURN_IF_ERROR(PrepareForExport(module));"
        },
        {
            "sha": "461307a0b558f8fb66d2905fcfd19e0a5ebfd380",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/opaque_elements_attr.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fopaque_elements_attr.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fopaque_elements_attr.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fopaque_elements_attr.mlir?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -10,7 +10,7 @@ func.func @main() {\n \n // Tests dynamic result shape\n \n-// CHECK: 'mhlo.all_gather' op can't be translated to XLA HLO\n+// CHECK: 'stablehlo.all_gather' op can't be translated to XLA HLO\n func.func @main(%arg0: tensor<128x32xf32>) -> tensor<128x?xf32> {\n   %0 = \"mhlo.all_gather\"(%arg0) {\n     all_gather_dim = 1 : i64,\n@@ -24,7 +24,7 @@ func.func @main(%arg0: tensor<128x32xf32>) -> tensor<128x?xf32> {\n \n // Tests dynamic operand shape\n \n-// CHECK: 'mhlo.all_gather' op can't be translated to XLA HLO\n+// CHECK: 'stablehlo.all_gather' op can't be translated to XLA HLO\n func.func @main(%arg0: tensor<128x32xf32>) -> tensor<128x?xf32> {\n   %0 = \"mhlo.all_gather\"(%arg0) {\n     all_gather_dim = 1 : i64,"
        },
        {
            "sha": "e677e65de672e9070d51fc1a8e6b547219755747",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo_pass.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 8,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -106,13 +106,23 @@ struct HloLegalizeToStablehloPass\n     stablehlo::registerFuncOpsForTypeConversion(target, patterns, converter);\n \n     if (allow_xla_features_) {\n-      // These ops do not exist in StableHLO.\n-      target.addLegalOp<mhlo::AsyncDoneOp, mhlo::AsyncStartOp,\n-                        mhlo::AsyncUpdateOp, mhlo::BitcastOp, mhlo::CopyOp,\n-                        mhlo::DomainOp, mhlo::ErfOp, mhlo::FusionOp,\n-                        mhlo::MinimumBroadcastShapesOp, mhlo::RaggedDotOp,\n-                        mhlo::StochasticConvertOp, mhlo::TopKOp, mhlo::TraceOp,\n-                        mhlo::XlaRngGetAndUpdateStateOp>();\n+      // These ops do not exist in StableHLO. (They do exist in CHLO, a slightly\n+      // higher-level dialect wrapping StableHLO, but we leave them as MHLO here\n+      // since we're specifically legalizing to StableHLO, not to CHLO.)\n+      target.addLegalOp<  //\n+          mhlo::AcosOp, mhlo::AcoshOp, mhlo::AsinOp, mhlo::AsinhOp,\n+          mhlo::AtanhOp, mhlo::CoshOp, mhlo::ErfOp, mhlo::RaggedDotOp,\n+          mhlo::SinhOp, mhlo::TopKOp>();\n+\n+      // These ops do not exist in StableHLO. (They don't exist in CHLO, either;\n+      // MHLO is the appropriate dialect for expressing XLA-specific features\n+      // such as these.)\n+      target.addLegalOp<\n+          mhlo::AsyncDoneOp, mhlo::AsyncStartOp, mhlo::AsyncUpdateOp,\n+          mhlo::BitcastOp, mhlo::CopyOp, mhlo::DomainOp, mhlo::FusionOp,\n+          mhlo::MinimumBroadcastShapesOp, mhlo::StochasticConvertOp,\n+          mhlo::TraceOp, mhlo::XlaRngGetAndUpdateStateOp>();\n+\n       target.addDynamicallyLegalOp<mhlo::AddDependencyOp>(\n           [](mhlo::AddDependencyOp op) {\n             return !hasMhloTypes(op->getOperandTypes());\n@@ -142,8 +152,11 @@ struct HloLegalizeToStablehloPass\n         [](Operation* op) { return !hasMhloTypes(op->getOperandTypes()); });\n     patterns.add<UpdateOperandsInUnknownOp>(converter, &getContext());\n \n+    ConversionConfig config;\n+    config.foldingMode = DialectConversionFoldingMode::Never;\n+\n     if (failed(applyPartialConversion(getOperation(), target,\n-                                      std::move(patterns))))\n+                                      std::move(patterns), config)))\n       return signalPassFailure();\n   }\n };"
        },
        {
            "sha": "5712ad045279b4c73988a925cbdef2d0d9522fd4",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_prepare_for_hlo_export.cpp",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_prepare_for_hlo_export.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2ccbf20305bb9987de2eced4cfdfacbd8c9914/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_prepare_for_hlo_export.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_prepare_for_hlo_export.cpp?ref=9f2ccbf20305bb9987de2eced4cfdfacbd8c9914",
            "patch": "@@ -44,7 +44,7 @@ limitations under the License.\n namespace mlir {\n namespace stablehlo_ext {\n \n-constexpr char kShardingAttr[] = \"stablehlo.sharding\";\n+constexpr char kShardingAttr[] = \"mhlo.sharding\";\n \n #define GEN_PASS_DEF_STABLEHLOPREPAREFORHLOEXPORTPASS\n #include \"stablehlo_ext/transforms/passes.h.inc\""
        }
    ],
    "stats": {
        "total": 142,
        "additions": 88,
        "deletions": 54
    }
}