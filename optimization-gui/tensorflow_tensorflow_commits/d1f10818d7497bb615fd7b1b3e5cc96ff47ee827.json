{
    "author": "hyeontaek",
    "message": "[PjRt-IFRT] Rewrite `PjRtLoadedExecutable` construction\n\nThis change rewrites the construction of `PjRtLoadedExecutable`. The main goal is to prepare for IFRT layout changes and to remove legacy boilerplate code.\n\nIt cleans up the output spec (dtype, shape, shading, layout) calculation logic in `xla::ifrt::PjRtLoadedExecutable` that used to be done in two stages (`::Create()` and `::CreateInternal()`) into one stage instead, where `PjRtLoadedExecutable` built from compilation and `PjRtLoadedExecutable` built from deserialization share most of the logic.\n\nSeveral utility functions are changed to take a return value from `xla::PjRtLoadedExecuable` methods instead of requiring `xla::PjRtLoadedExecuable*` directly. This makes these utility functions reusable for `xla::PjRtExecuable*` in `xla::ifrt::PjRtExecutable` (not part of this change; soon to follow)\n\nThe output shape calculation logic using per-shard shapes embedded in HLO has been removed. All output shapes are computed by applying output shardings on full output shapes.\n\nAll outputs from a compiled executable use `xla::ifrt::HloSharding` (including for token types). `xla::ifrt::ConcreteEvenSharding` are still used for an `xla::ifrt::PjRtLoadedExecutable` created from `xla::PjRtLoadedExecutable` (for deserialization) because there is no correct sharding information available. The use of `xla::ifrt::ConcreteEvenSharding` will also be removed once serialization & deserialization roundtrip preserves IFRT-level metadata. Following an existing convention, non-shardable types like tokens are still considered fully replicated.\n\nPiperOrigin-RevId: 842807805",
    "sha": "d1f10818d7497bb615fd7b1b3e5cc96ff47ee827",
    "files": [
        {
            "sha": "2ea62a8d42fda4c5c043aa8eb807e6b5d3bf151a",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 373,
            "deletions": 444,
            "changes": 817,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1f10818d7497bb615fd7b1b3e5cc96ff47ee827/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1f10818d7497bb615fd7b1b3e5cc96ff47ee827/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=d1f10818d7497bb615fd7b1b3e5cc96ff47ee827",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n@@ -47,7 +46,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/pjrt/utils.h\"\n-#include \"xla/primitive_util.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/attribute_map.h\"\n #include \"xla/python/ifrt/basic_device_list.h\"\n@@ -66,12 +64,10 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_dtype.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_host_callback.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_memory.h\"\n-#include \"xla/python/pjrt_ifrt/xla_compiler.h\"\n #include \"xla/python/pjrt_ifrt/xla_sharding.h\"\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/concurrency/future.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n@@ -85,145 +81,220 @@ namespace ifrt {\n \n namespace {\n \n-// Returns the op sharding of the root instruction in the entry computation.\n-absl::StatusOr<const xla::HloInstructionProto*> FindRootInstruction(\n-    const HloModuleProto& proto) {\n-  for (const auto& computation : proto.computations()) {\n-    if (computation.id() == proto.entry_computation_id()) {\n-      for (const auto& instruction : computation.instructions()) {\n-        if (instruction.id() == computation.root_id()) {\n-          return &instruction;\n-        }\n-      }\n-    }\n-  }\n-  return InvalidArgument(\"Entry computation not found\");\n+constexpr absl::string_view kDefaultMemoryKind = \"device\";\n+\n+// Returns a flat list of IFRT dtypes from element type information that a PjRt\n+// executable returns (per-module lists of primitive of element types).\n+// PjRt-IFRT always uses the first module's information.\n+absl::StatusOr<std::vector<DType>> GetDTypes(\n+    const absl::StatusOr<std::vector<std::vector<xla::PrimitiveType>>>&\n+        pjrt_executable_element_types) {\n+  TF_RETURN_IF_ERROR(pjrt_executable_element_types.status());\n+  if (pjrt_executable_element_types->empty()) {\n+    return FailedPrecondition(\"No module found\");\n+  }\n+  std::vector<DType> dtypes;\n+  dtypes.reserve(pjrt_executable_element_types->front().size());\n+  for (xla::PrimitiveType element_type :\n+       pjrt_executable_element_types->front()) {\n+    TF_ASSIGN_OR_RETURN(DType dtype, ToDType(element_type));\n+    dtypes.push_back(dtype);\n+  }\n+  return dtypes;\n }\n \n-// Returns the output element types of the first module in a\n-// `PjRtLoadedExecutable`.\n-absl::StatusOr<std::vector<xla::PrimitiveType>>\n-GetFirstModuleOutputElementTypes(\n-    xla::PjRtLoadedExecutable* pjrt_loaded_executable) {\n-  auto element_types = pjrt_loaded_executable->GetOutputElementTypes();\n-  TF_RETURN_IF_ERROR(element_types.status());\n-  if (element_types->empty()) {\n-    return FailedPrecondition(\"No output element types found\");\n-  }\n-  return element_types->front();\n-}\n-\n-// Returns the output dimensions of the first module in a\n-// `PjRtLoadedExecutable`.\n-absl::StatusOr<std::vector<xla::DimensionVector>>\n-GetFirstModuleOutputDimensions(\n-    xla::PjRtLoadedExecutable* pjrt_loaded_executable) {\n-  auto dimensions = pjrt_loaded_executable->GetOutputDimensions();\n-  TF_RETURN_IF_ERROR(dimensions.status());\n-  if (dimensions->empty()) {\n-    return FailedPrecondition(\"No output dimensions found\");\n-  }\n-  return dimensions->front();\n+// Returns a flat list of IFRT shapes from the dimension information that a PjRt\n+// executable returns (per-module lists of dimension vectors).\n+// PjRt-IFRT always uses the first module's information.\n+absl::StatusOr<std::vector<Shape>> GetShapes(\n+    const absl::StatusOr<std::vector<std::vector<xla::DimensionVector>>>&\n+        pjrt_executable_dimensions,\n+    absl::Span<const DType> dtypes) {\n+  TF_RETURN_IF_ERROR(pjrt_executable_dimensions.status());\n+  if (pjrt_executable_dimensions->empty()) {\n+    return FailedPrecondition(\"No module found\");\n+  }\n+  if (pjrt_executable_dimensions->front().size() != dtypes.size()) {\n+    return FailedPrecondition(\n+        \"Output dimensions and dtypes have different sizes: %d vs. %d\",\n+        pjrt_executable_dimensions->front().size(), dtypes.size());\n+  }\n+  std::vector<Shape> shapes;\n+  shapes.reserve(pjrt_executable_dimensions->front().size());\n+  for (int i = 0; i < pjrt_executable_dimensions->front().size(); ++i) {\n+    if (dtypes[i].kind() == DType::kToken) {\n+      // Token uses a scalar shape by convention.\n+      shapes.push_back(Shape({}));\n+    } else {\n+      shapes.push_back(Shape(pjrt_executable_dimensions->front()[i]));\n+    }\n+  }\n+  return shapes;\n }\n \n-// Returns the output shardings of the first module in a\n-// `PjRtLoadedExecutable`.\n-absl::StatusOr<std::optional<xla::HloSharding>> GetFirstModuleOutputSharding(\n-    xla::PjRtLoadedExecutable* pjrt_loaded_executable,\n-    const xla::Shape& shape) {\n-  auto output_shardings = pjrt_loaded_executable->GetOutputShardings();\n-  std::optional<xla::HloSharding> result_hlo_sharding;\n-  if (output_shardings.has_value()) {\n-    std::vector<xla::HloSharding> hlo_shardings;\n-    hlo_shardings.reserve(output_shardings->size());\n-    for (const auto& sharding : *output_shardings) {\n-      TF_ASSIGN_OR_RETURN(auto hlo_sharding,\n-                          xla::HloSharding::FromProto(sharding));\n-      hlo_shardings.push_back(hlo_sharding);\n-    }\n-    if (shape.IsTuple()) {\n-      return xla::HloSharding::Tuple(shape, hlo_shardings);\n+// Returns a pair of flat lists of IFRT dtypes and shapes from XLA shapes\n+// extracted from an MLIR module's signature.\n+absl::StatusOr<std::pair<std::vector<DType>, std::vector<Shape>>>\n+GetDTypesAndShapes(absl::Span<const xla::Shape> mlir_module_xla_shapes) {\n+  std::vector<DType> dtypes;\n+  dtypes.reserve(mlir_module_xla_shapes.size());\n+  std::vector<Shape> shapes;\n+  shapes.reserve(mlir_module_xla_shapes.size());\n+  for (const xla::Shape& xla_shape : mlir_module_xla_shapes) {\n+    TF_ASSIGN_OR_RETURN(DType dtype, ToDType(xla_shape.element_type()));\n+    dtypes.push_back(dtype);\n+    if (dtype.kind() == DType::kToken) {\n+      // Token uses a scalar shape by convention.\n+      shapes.push_back(Shape({}));\n     } else {\n-      return hlo_shardings.front();\n+      shapes.push_back(Shape(xla_shape.dimensions()));\n     }\n   }\n-  return std::nullopt;\n+  return std::make_pair(std::move(dtypes), std::move(shapes));\n }\n \n-// Returns the flattened output memory_kinds of the first module in a\n-// `PjRtLoadedExecutable`.\n-// `UnimplementedError` will be converted into `std::nullopt`.\n-absl::StatusOr<std::optional<std::vector<absl::string_view>>>\n-GetFirstModuleOutputMemoryKinds(\n-    xla::PjRtLoadedExecutable* pjrt_loaded_executable) {\n-  auto output_memory_kinds = pjrt_loaded_executable->GetOutputMemoryKinds();\n-  // Gracefully handle an unimplemented error.\n-  if (absl::IsUnimplemented(output_memory_kinds.status())) {\n+// Returns a flat list of HLO shardings from the sharding information that a\n+// PjRt executable returns (a flat list of `OpSharding`s, with some special\n+// cases). Returns `std::nullopt` if the executable does not have sharding\n+// information.\n+absl::StatusOr<std::optional<std::vector<xla::HloSharding>>> GetHloShardings(\n+    const std::optional<std::vector<xla::OpSharding>>&\n+        pjrt_executable_op_shardings,\n+    absl::Span<const DType> dtypes, bool is_output) {\n+  if (!pjrt_executable_op_shardings.has_value()) {\n     return std::nullopt;\n   }\n-  TF_RETURN_IF_ERROR(output_memory_kinds.status());\n-  // Expect `xla::PjRtExecutable::GetOutputMemoryKinds()` to return at least\n-  // one module's output memory_kinds if it returns any non-error result.\n-  if (output_memory_kinds->empty()) {\n-    return FailedPrecondition(\"No output memory kinds found\");\n+  std::vector<xla::HloSharding> hlo_shardings;\n+  if (is_output && dtypes.empty()) {\n+    // If the HLO module output is an empty tuple, the output sharding will have\n+    // a single element for the tuple as a special case. We allow this condition\n+    // by checking this condition specifically.\n+    if (pjrt_executable_op_shardings->size() != 1) {\n+      return FailedPrecondition(\n+          \"HLO module output is an empty tuple, but the output sharding has \"\n+          \"%d elements\",\n+          pjrt_executable_op_shardings->size());\n+    }\n+    return std::vector<xla::HloSharding>();\n+  }\n+  if (pjrt_executable_op_shardings->size() != dtypes.size()) {\n+    return FailedPrecondition(\n+        \"Output shardings and dtypes have different sizes: %d vs. %d\",\n+        pjrt_executable_op_shardings->size(), dtypes.size());\n+  }\n+  hlo_shardings.reserve(pjrt_executable_op_shardings->size());\n+  for (int i = 0; i < pjrt_executable_op_shardings->size(); ++i) {\n+    if (dtypes[i].kind() == DType::kToken) {\n+      // Token uses a fully replicated sharding by convention.\n+      hlo_shardings.push_back(xla::HloSharding::Replicate());\n+    } else {\n+      TF_ASSIGN_OR_RETURN(\n+          auto hlo_sharding,\n+          xla::HloSharding::FromProto((*pjrt_executable_op_shardings)[i]));\n+      hlo_shardings.push_back(hlo_sharding);\n+    }\n   }\n-  return std::move(output_memory_kinds)->front();\n+  return hlo_shardings;\n }\n \n-// Returns the flattened output layouts of the first module in a\n-// `PjRtLoadedExecutable`.\n-// `UnimplementedError` will be converted into a vector of `nullptr`.\n-absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n-GetFirstModuleOutputLayouts(\n-    xla::PjRtLoadedExecutable* pjrt_loaded_executable,\n-    absl::Span<const xla::LayoutMode> output_layout_modes) {\n-  absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n-      executable_output_layouts = pjrt_loaded_executable->GetOutputLayouts();\n-  // An unimplemented error is converted into all-default layouts.\n-  if (absl::IsUnimplemented(executable_output_layouts.status())) {\n-    return std::vector<std::shared_ptr<const xla::PjRtLayout>>(\n-        /*size=*/output_layout_modes.size(), /*value=*/nullptr);\n-  }\n-  TF_RETURN_IF_ERROR(executable_output_layouts.status());\n-  std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts;\n-  if (executable_output_layouts->size() != output_layout_modes.size()) {\n+// Returns a flat list of IFRT memory kinds from the memory kind information\n+// that a PjRt executable returns (per-module lists of memory kind strings).\n+// PjRt-IFRT always uses the first module's information.\n+absl::StatusOr<std::vector<absl::string_view>> GetMemoryKinds(\n+    const absl::StatusOr<std::vector<std::vector<absl::string_view>>>&\n+        pjrt_executable_memory_kinds,\n+    absl::Span<const DType> dtypes) {\n+  std::vector<absl::string_view> memory_kinds;\n+  // An unimplemented error is converted into all-default memory kinds.\n+  if (absl::IsUnimplemented(pjrt_executable_memory_kinds.status())) {\n+    memory_kinds.resize(/*size=*/dtypes.size(), /*value=*/kDefaultMemoryKind);\n+    return memory_kinds;\n+  }\n+  TF_RETURN_IF_ERROR(pjrt_executable_memory_kinds.status());\n+  if (pjrt_executable_memory_kinds->empty()) {\n+    return FailedPrecondition(\"No module found\");\n+  }\n+  if (pjrt_executable_memory_kinds->front().size() != dtypes.size()) {\n     return FailedPrecondition(\n-        \"Output memory kinds and output layout modes have different sizes: %d \"\n-        \"vs. %d\",\n-        executable_output_layouts->size(), output_layout_modes.size());\n-  }\n-  output_layouts.reserve(executable_output_layouts->size());\n-  for (int i = 0; i < executable_output_layouts->size(); ++i) {\n-    if (output_layout_modes[i].mode == xla::LayoutMode::Mode::kDefault) {\n-      output_layouts.push_back(nullptr);\n+        \"Memory kinds and dtypes have different sizes: %d vs. %d\",\n+        pjrt_executable_memory_kinds->front().size(), dtypes.size());\n+  }\n+  memory_kinds.reserve(pjrt_executable_memory_kinds->front().size());\n+  for (int i = 0; i < pjrt_executable_memory_kinds->front().size(); ++i) {\n+    if (dtypes[i].kind() == DType::kToken) {\n+      // Token uses a device memory kind by convention.\n+      memory_kinds.push_back(kDefaultMemoryKind);\n     } else {\n-      output_layouts.push_back(std::move((*executable_output_layouts)[i]));\n+      memory_kinds.push_back(pjrt_executable_memory_kinds->front()[i]);\n     }\n   }\n-  return output_layouts;\n+  return memory_kinds;\n }\n \n-struct ShapePartialInfo {\n-  std::vector<xla::PrimitiveType> element_types;\n-  std::vector<xla::DimensionVector> dimensions;\n-};\n-\n-absl::StatusOr<ShapePartialInfo> CreateShapePartialInfo(\n-    absl::Span<const xla::Shape> shapes) {\n-  ShapePartialInfo partial_info;\n-  partial_info.element_types.reserve(shapes.size());\n-  partial_info.dimensions.reserve(shapes.size());\n-  for (const auto& shape : shapes) {\n-    if (shape.IsTuple()) {\n-      return FailedPrecondition(\n-          \"Tupled shape is not supported in `CreateShapePartialInfo`.\");\n+// Makes IFRT shardings created from HLO shardings and memory kinds.\n+std::vector<ShardingRef> MakeShardings(\n+    absl::Span<const Shape> shapes,\n+    const std::optional<std::vector<xla::HloSharding>>& hlo_shardings,\n+    absl::Span<const absl::string_view> memory_kinds,\n+    const DeviceListRef& executable_devices) {\n+  std::vector<ShardingRef> shardings;\n+  shardings.reserve(memory_kinds.size());\n+  if (hlo_shardings.has_value()) {\n+    for (int i = 0; i < memory_kinds.size(); ++i) {\n+      shardings.push_back(ifrt::HloSharding::Create(executable_devices,\n+                                                    MemoryKind(memory_kinds[i]),\n+                                                    (*hlo_shardings)[i]));\n+    }\n+  } else {\n+    // Assume a traditional replication computation where tile shapes are the\n+    // same as global shapes.\n+    for (int i = 0; i < memory_kinds.size(); ++i) {\n+      shardings.push_back(ifrt::ConcreteEvenSharding::Create(\n+          executable_devices, MemoryKind(memory_kinds[i]),\n+          /*shape=*/shapes[i],\n+          /*shard_shape=*/shapes[i]));\n     }\n-    partial_info.element_types.push_back(shape.element_type());\n-    partial_info.dimensions.push_back(\n-        xla::ShapeUtil::CreateDimensionVectorFromShape(shape));\n   }\n+  return shardings;\n+}\n \n-  return partial_info;\n+// Returns a flat list of layouts by combining layout modes and PjRt executable\n+// layouts.\n+// If any error other than an unimplemented error happens, returns\n+// `std::nullopt`. The layout will be determined at execute time.\n+//\n+// TODO(hyeontaek): Remove the nullopt path once obtaining layout modes and\n+// concrete layouts avoids HLO module serialization/deserialization and always\n+// succeeds.\n+absl::StatusOr<\n+    std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>>\n+GetLayouts(\n+    const absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>&\n+        pjrt_executable_layouts,\n+    absl::Span<const xla::LayoutMode> layout_modes) {\n+  // An unimplemented error is converted into all-default layouts.\n+  if (absl::IsUnimplemented(pjrt_executable_layouts.status())) {\n+    return std::vector<std::shared_ptr<const xla::PjRtLayout>>(\n+        /*size=*/layout_modes.size(), /*value=*/nullptr);\n+  }\n+  if (!pjrt_executable_layouts.ok()) {\n+    return std::nullopt;\n+  }\n+  std::vector<std::shared_ptr<const xla::PjRtLayout>> layouts;\n+  if (pjrt_executable_layouts->size() != layout_modes.size()) {\n+    return FailedPrecondition(\n+        \"Layouts and layout modes have different sizes: %d vs. %d\",\n+        pjrt_executable_layouts->size(), layout_modes.size());\n+  }\n+  layouts.reserve(pjrt_executable_layouts->size());\n+  for (int i = 0; i < pjrt_executable_layouts->size(); ++i) {\n+    if (layout_modes[i].mode == xla::LayoutMode::Mode::kDefault) {\n+      layouts.push_back(nullptr);\n+    } else {\n+      layouts.push_back(std::move((*pjrt_executable_layouts)[i]));\n+    }\n+  }\n+  return layouts;\n }\n \n // Special `xla::GetLayoutModes()` implementation for obtaining layout modes\n@@ -237,9 +308,10 @@ static absl::StatusOr<std::vector<LayoutMode>> GetLayoutModesFromFrontendAttr(\n   std::vector<std::string> str_modes =\n       absl::StrSplit(attr, kDelimiter, absl::SkipEmpty());\n   std::vector<LayoutMode> result;\n+  result.reserve(str_modes.size());\n   for (const std::string& str_mode : str_modes) {\n     TF_ASSIGN_OR_RETURN(LayoutMode mode, LayoutMode::FromString(str_mode));\n-    result.emplace_back(std::move(mode));\n+    result.push_back(std::move(mode));\n   }\n   return result;\n }\n@@ -256,6 +328,89 @@ static absl::StatusOr<std::vector<LayoutMode>> GetLayoutModes(\n   return GetLayoutModesFromFrontendAttr(iter->second);\n }\n \n+// Returns a flat list of output layout modes by examining the HLO modules.\n+//\n+// TODO(hyeontaek): Remove this layout mode discovery method once\n+// deserialization loads layout information from the serialization metadata\n+// instead of from `xla::PjRtExecutable` or `xla::PjRtLoadedExecutable`.\n+absl::StatusOr<std::vector<xla::LayoutMode>> GetOutputLayoutModesFromHloModules(\n+    const absl::StatusOr<std::vector<std::shared_ptr<xla::HloModule>>>&\n+        hlo_modules,\n+    absl::Span<const DType> output_dtypes) {\n+  TF_RETURN_IF_ERROR(hlo_modules.status());\n+  if (hlo_modules->empty()) {\n+    return FailedPrecondition(\"No module found\");\n+  }\n+  return GetLayoutModes(*hlo_modules->front(), \"out_layout_modes\",\n+                        output_dtypes.size());\n+}\n+\n+// Returns a new `DeviceListRef` that contains the addressable devices of the\n+// PjRt executable if the supplied `executable_devices` has an incomplete set of\n+// devices.\n+absl::StatusOr<DeviceListRef> AdjustExecutableDevicesForPmap(\n+    PjRtClient* client, const xla::PjRtLoadedExecutable* pjrt_loaded_executable,\n+    DeviceListRef executable_devices) {\n+  // For jit(pmap(...)), the device assignment (passed as `executable_devices`)\n+  // may contain a single device while the PjRt executable has multiple\n+  // addressable devices. We check for this condition and replace\n+  // `executable_devices` with the executable's addressable devices if\n+  // necessary.\n+  if (pjrt_loaded_executable->num_replicas() > 1 &&\n+      executable_devices->devices().size() == 1) {\n+    if (pjrt_loaded_executable->addressable_devices().size() > 1) {\n+      BasicDeviceList::Devices ds;\n+      ds.reserve(pjrt_loaded_executable->addressable_devices().size());\n+      for (xla::PjRtDevice* device :\n+           pjrt_loaded_executable->addressable_devices()) {\n+        TF_ASSIGN_OR_RETURN(Device * ifrt_device,\n+                            client->LookupPjRtDevice(device));\n+        ds.push_back(ifrt_device);\n+      }\n+      executable_devices = BasicDeviceList::Create(std::move(ds));\n+    } else if (pjrt_loaded_executable->addressable_devices().size() == 1) {\n+      TF_ASSIGN_OR_RETURN(\n+          Device * ifrt_device,\n+          client->LookupPjRtDevice(\n+              pjrt_loaded_executable->addressable_devices().front()));\n+      if (ifrt_device != executable_devices->devices().front()) {\n+        return FailedPrecondition(\n+            \"Addressable device does not match sharding device\");\n+      }\n+    }\n+  }\n+  if (executable_devices->devices().size() <\n+      pjrt_loaded_executable->addressable_devices().size()) {\n+    return FailedPrecondition(\n+        \"Sharding devices must be at least as many as addressable devices\");\n+  }\n+  return executable_devices;\n+}\n+\n+// Gathers all `PjRtHostSendAndRecvLoadedHostCallback` from the given list of\n+// loaded host callbacks.\n+std::vector<PjRtHostSendAndRecvLoadedHostCallback*>\n+GatherHostSendAndRecvCallbacks(\n+    absl::Span<const tsl::RCReference<LoadedHostCallback>>\n+        loaded_host_callbacks) {\n+  std::vector<PjRtHostSendAndRecvLoadedHostCallback*>\n+      host_send_and_recv_callbacks;\n+  host_send_and_recv_callbacks.reserve(loaded_host_callbacks.size());\n+  // Gather all `PjRtLoadedHostCallback` separately, as each execution will\n+  // register `PjRtLoadedHostCallback` for host send and recv. All host\n+  // callbacks will be referenced by the executable and any pending execution to\n+  // guarantee the liveliness of host callbacks during executions.\n+  for (auto& loaded_host_callback : loaded_host_callbacks) {\n+    auto* host_send_and_recv_callback =\n+        llvm::dyn_cast<PjRtHostSendAndRecvLoadedHostCallback>(\n+            loaded_host_callback.get());\n+    if (host_send_and_recv_callback != nullptr) {\n+      host_send_and_recv_callbacks.push_back(host_send_and_recv_callback);\n+    }\n+  }\n+  return host_send_and_recv_callbacks;\n+}\n+\n }  // namespace\n \n char PjRtCompatibleExecutable::ID = 0;\n@@ -292,52 +447,69 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n     std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable,\n     std::vector<tsl::RCReference<LoadedHostCallback>> loaded_host_callbacks,\n     DeviceListRef executable_devices) {\n-  // TODO(hyeontaek): Use a full shape and a sharding rather than a per-shard\n-  // shape.\n   VLOG(3) << \"PjRtLoadedExecutable::Create\";\n-  VLOG(3) << \"Using per-shard shape\";\n+\n+  TF_ASSIGN_OR_RETURN(\n+      executable_devices,\n+      AdjustExecutableDevicesForPmap(client, pjrt_loaded_executable.get(),\n+                                     std::move(executable_devices)));\n+\n   TF_ASSIGN_OR_RETURN(\n-      auto result_element_types,\n-      GetFirstModuleOutputElementTypes(pjrt_loaded_executable.get()));\n+      std::vector<DType> output_dtypes,\n+      GetDTypes(pjrt_loaded_executable->GetOutputElementTypes()));\n   TF_ASSIGN_OR_RETURN(\n-      auto result_dimensions,\n-      GetFirstModuleOutputDimensions(pjrt_loaded_executable.get()));\n+      std::vector<Shape> output_shapes,\n+      GetShapes(pjrt_loaded_executable->GetOutputDimensions(), output_dtypes));\n+  // When creating `xla::ifrt::PjRtLoadedExecutable` from an already compiled\n+  // and loaded `xla::PjRtLoadedExecutable`, we do not have a full shape\n+  // (`xla::PjRtLoadedExecutable::GetOutputDimensions()` returns shard shapes).\n+  // This prevents us from using\n+  // `xla::PjRtLoadedExecutable::GetOutputShardings()` for constructing IFRT\n+  // shardings; otherwise, we would try to apply the shardings to already\n+  // sharded shapes, which will result in incorrect sharded shapes (and layouts\n+  // computed from these shard shapes). Thus, we ignore HLO shardings and use\n+  // `xla::ifrt::ConcreteEvenSharding` that will take the already sharded shapes\n+  // as shard shapes.\n+  //\n+  // TODO(hyeontaek): Remove this special handling once we can preserve full\n+  // output shapes and layouts from the original compilation during\n+  // serialization/deserialization, and remove this `PjRtLoadedExecutable`\n+  // construction path.\n+  std::optional<std::vector<xla::HloSharding>> output_hlo_shardings =\n+      std::nullopt;\n   TF_ASSIGN_OR_RETURN(\n-      auto result_memory_kinds,\n-      GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n-  // Obtaining output layout modes and output layouts directly from\n-  // `PjRtLoadedExecutable` may fail because the currently PjRt implementations\n-  // often fetch and serialize the optimized HLO. For now, we gracefully\n-  // handle it by omitting output layouts at creation time and using output\n-  // `PjRtBuffer`'s concrete layouts.\n-  // TODO(hyeontaek): Add a way to obtain output layout modes and\n-  // `PjRtLoadedExecutable::GetOutputLayouts()` without causing the optimized\n-  // HLO to be serialized and fetched.\n+      std::vector<absl::string_view> output_memory_kinds,\n+      GetMemoryKinds(pjrt_loaded_executable->GetOutputMemoryKinds(),\n+                     output_dtypes));\n+  std::vector<ShardingRef> output_shardings =\n+      MakeShardings(output_shapes, output_hlo_shardings, output_memory_kinds,\n+                    executable_devices);\n+\n+  // Obtaining output layout modes and output layouts directly may fail because\n+  // PjRt implementations often fetch and serialize/deserialize the optimized\n+  // HLO to provide the layout information. For now, we gracefully handle it by\n+  // omitting output layouts at creation time and using output `PjRtBuffer`'s\n+  // concrete layouts.\n+  //\n+  // TODO(hyeontaek): Remove this layout mode discovery method once\n+  // deserialization loads layout information from the serialization metadata\n+  // instead of from `xla::PjRtExecutable` or `xla::PjRtLoadedExecutable`.\n   std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n       output_layouts;\n-  absl::StatusOr<std::vector<std::shared_ptr<HloModule>>> hlo_modules =\n-      pjrt_loaded_executable->GetHloModules();\n-  if (hlo_modules.ok()) {\n-    if (hlo_modules->empty()) {\n-      return FailedPrecondition(\"Requires at least one HloModule.\");\n-    }\n-    absl::StatusOr<std::vector<xla::LayoutMode>> output_layout_modes =\n-        GetLayoutModes(*hlo_modules->front(), \"out_layout_modes\",\n-                       result_element_types.size());\n-    if (output_layout_modes.ok()) {\n-      absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n-          first_module_output_layouts = GetFirstModuleOutputLayouts(\n-              pjrt_loaded_executable.get(), *output_layout_modes);\n-      if (first_module_output_layouts.ok()) {\n-        output_layouts = *std::move(first_module_output_layouts);\n-      }\n-    }\n+  absl::StatusOr<std::vector<xla::LayoutMode>> output_layout_modes =\n+      GetOutputLayoutModesFromHloModules(\n+          pjrt_loaded_executable->GetHloModules(), output_dtypes);\n+  if (output_layout_modes.ok()) {\n+    TF_ASSIGN_OR_RETURN(output_layouts,\n+                        GetLayouts(pjrt_loaded_executable->GetOutputLayouts(),\n+                                   *output_layout_modes));\n   }\n-  return CreateInternal(client, std::move(pjrt_loaded_executable),\n-                        result_element_types, result_dimensions,\n-                        /*result_hlo_sharding=*/std::nullopt,\n-                        result_memory_kinds, output_layouts,\n-                        loaded_host_callbacks, std::move(executable_devices));\n+\n+  return LoadedExecutableRef(new PjRtLoadedExecutable(\n+      client, std::move(pjrt_loaded_executable), std::move(executable_devices),\n+      std::move(loaded_host_callbacks), std::move(output_dtypes),\n+      std::move(output_shapes), std::move(output_shardings),\n+      std::move(output_layouts)));\n }\n \n static absl::StatusOr<std::vector<xla::Shape>> ResultShapesOfModule(\n@@ -366,303 +538,71 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n     module.dump();\n   }\n   VLOG(3) << compile_options.ToProto()->DebugString();\n-  const auto& build_options = compile_options.executable_build_options;\n-  const bool auto_spmd_partitioning =\n-      build_options.use_spmd_partitioning() &&\n-      build_options.num_partitions() > 1 &&\n-      (build_options.use_auto_spmd_partitioning() ||\n-       build_options.any_allow_spmd_sharding_propagation_to_parameters() ||\n-       build_options.any_allow_spmd_sharding_propagation_to_output());\n \n   // We have to do process the MLIR before the compile call, since the latter\n   // will use the MLIR as scratch space, or possibly even deallocate it.\n-  TF_ASSIGN_OR_RETURN(const std::vector<xla::Shape> result_shapes,\n-                      ResultShapesOfModule(module));\n-  absl::StatusOr<std::vector<xla::LayoutMode>> output_layout_modes =\n-      GetOutputLayoutModes(module);\n-\n-  TF_ASSIGN_OR_RETURN(auto pjrt_loaded_executable,\n-                      client->pjrt_client()->CompileAndLoad(\n-                          std::move(module), std::move(compile_options)));\n-\n-  if (auto_spmd_partitioning) {\n-    // TODO(hyeontaek): Use a full shape and a sharding rather than a per-shard\n-    // shape.\n-    VLOG(3) << \"Using per-shard shape\";\n-    TF_ASSIGN_OR_RETURN(\n-        auto result_element_types,\n-        GetFirstModuleOutputElementTypes(pjrt_loaded_executable.get()));\n-    TF_ASSIGN_OR_RETURN(\n-        auto result_dimensions,\n-        GetFirstModuleOutputDimensions(pjrt_loaded_executable.get()));\n-    TF_ASSIGN_OR_RETURN(\n-        auto result_memory_kinds,\n-        GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n-    // Obtaining output layout modes and output layouts directly from\n-    // `PjRtLoadedExecutable` may fail because the currently PjRt\n-    // implementations often fetch and serialize the optimized HLO. For now, we\n-    // gracefully handle it by omitting output layouts at creation time and\n-    // using output `PjRtBuffer`'s concrete layouts.\n-    // TODO(hyeontaek): Add a way to obtain output layout modes and\n-    // `PjRtLoadedExecutable::GetOutputLayouts()` without causing the optimized\n-    // HLO to be serialized and fetched.\n-    std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n-        output_layouts;\n-    if (output_layout_modes.ok()) {\n-      absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n-          first_module_output_layouts = GetFirstModuleOutputLayouts(\n-              pjrt_loaded_executable.get(), *output_layout_modes);\n-      if (first_module_output_layouts.ok()) {\n-        output_layouts = *std::move(first_module_output_layouts);\n-      }\n-    }\n-    return CreateInternal(client, std::move(pjrt_loaded_executable),\n-                          result_element_types, result_dimensions,\n-                          /*result_hlo_sharding=*/std::nullopt,\n-                          result_memory_kinds, output_layouts,\n-                          std::move(loaded_host_callbacks),\n-                          std::move(executable_devices));\n-  } else {\n-    VLOG(3) << \"Using full shape\";\n-    // TODO(yueshengys): Consider getting element types and dimensions directly\n-    // from module.\n-    bool tuple_output = result_shapes.size() != 1;\n-    xla::Shape result_shape;\n-    std::vector<xla::Shape> output_shapes;\n-    if (tuple_output) {\n-      result_shape = xla::ShapeUtil::MakeTupleShape(result_shapes);\n-      output_shapes = std::move(result_shapes);\n-    } else {\n-      result_shape = result_shapes.front();\n-      output_shapes = result_shape.IsTuple()\n-                          ? result_shape.tuple_shapes()\n-                          : std::vector<xla::Shape>{result_shape};\n-    }\n-    TF_ASSIGN_OR_RETURN(auto shape_partial_info,\n-                        CreateShapePartialInfo(output_shapes));\n-    TF_ASSIGN_OR_RETURN(auto result_hlo_sharding,\n-                        GetFirstModuleOutputSharding(\n-                            pjrt_loaded_executable.get(), result_shape));\n-    TF_ASSIGN_OR_RETURN(\n-        auto result_memory_kinds,\n-        GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n-    // Obtaining output layout modes and output layouts directly from\n-    // `PjRtLoadedExecutable` may fail because the currently PjRt\n-    // implementations often fetch and serialize the optimized HLO. For now, we\n-    // gracefully handle it by omitting output layouts at creation time and\n-    // using output `PjRtBuffer`'s concrete layouts.\n-    // TODO(hyeontaek): Add a way to obtain output layout modes and\n-    // `PjRtLoadedExecutable::GetOutputLayouts()` without causing the optimized\n-    // HLO to be serialized and fetched.\n-    std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n-        output_layouts;\n-    if (output_layout_modes.ok()) {\n-      absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n-          first_module_output_layouts = GetFirstModuleOutputLayouts(\n-              pjrt_loaded_executable.get(), *output_layout_modes);\n-      if (first_module_output_layouts.ok()) {\n-        output_layouts = *std::move(first_module_output_layouts);\n-      }\n-    }\n-    return CreateInternal(\n-        client, std::move(pjrt_loaded_executable),\n-        shape_partial_info.element_types, shape_partial_info.dimensions,\n-        result_hlo_sharding, result_memory_kinds, output_layouts,\n-        std::move(loaded_host_callbacks), std::move(executable_devices));\n-  }\n-}\n-\n-absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::CreateInternal(\n-    PjRtClient* client,\n-    std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable,\n-    absl::Span<const xla::PrimitiveType> result_element_types,\n-    absl::Span<const xla::DimensionVector> result_dimensions,\n-    const std::optional<xla::HloSharding>& result_hlo_sharding,\n-    const std::optional<std::vector<absl::string_view>>& result_memory_kinds,\n-    const std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>&\n-        output_layouts,\n-    std::vector<tsl::RCReference<LoadedHostCallback>> loaded_host_callbacks,\n-    DeviceListRef executable_devices) {\n-  // For jit(pmap(...)), the device assignment (passed as `executable_devices`)\n-  // may contain a single device while the PjRt executable has multiple\n-  // addressable devices. We check for this condition and replace\n-  // `executable_devices` with the executable's addressable devices if\n-  // necessary.\n-  if (pjrt_loaded_executable->num_replicas() > 1 &&\n-      executable_devices->devices().size() == 1) {\n-    if (pjrt_loaded_executable->addressable_devices().size() > 1) {\n-      BasicDeviceList::Devices ds;\n-      ds.reserve(pjrt_loaded_executable->addressable_devices().size());\n-      for (xla::PjRtDevice* device :\n-           pjrt_loaded_executable->addressable_devices()) {\n-        TF_ASSIGN_OR_RETURN(Device * ifrt_device,\n-                            client->LookupPjRtDevice(device));\n-        ds.push_back(ifrt_device);\n-      }\n-      executable_devices = BasicDeviceList::Create(std::move(ds));\n-    } else if (pjrt_loaded_executable->addressable_devices().size() == 1) {\n-      TF_ASSIGN_OR_RETURN(\n-          Device * ifrt_device,\n-          client->LookupPjRtDevice(\n-              pjrt_loaded_executable->addressable_devices().front()));\n-      if (ifrt_device != executable_devices->devices().front()) {\n-        return FailedPrecondition(\n-            \"Addressable device does not match sharding device\");\n-      }\n-    }\n-  }\n-  if (executable_devices->devices().size() <\n-      pjrt_loaded_executable->addressable_devices().size()) {\n-    return FailedPrecondition(\n-        \"Sharding devices must be at least as many as addressable devices\");\n-  }\n-  std::vector<DType> output_dtypes;\n-  std::vector<Shape> output_shapes;\n-  std::vector<ShardingRef> output_shardings;\n-\n-  auto append_arg = [&](const xla::PrimitiveType& element_type,\n-                        const xla::DimensionVector& dimensions,\n-                        const xla::HloSharding* sharding,\n-                        MemoryKind memory_kind) -> absl::Status {\n-    TF_ASSIGN_OR_RETURN(auto dtype, ToDType(element_type));\n-    output_dtypes.push_back(dtype);\n-    output_shapes.push_back(Shape(dimensions));\n-\n-    CHECK(xla::primitive_util::IsArrayType(element_type));\n-\n-    if (sharding != nullptr) {\n-      output_shardings.push_back(ifrt::HloSharding::Create(\n-          executable_devices, memory_kind, *sharding));\n-    } else {\n-      // Assume a traditional replication computation where tile shapes are\n-      // the same as global shapes.\n-      const xla::DimensionVector& tile_shape_dimensions = dimensions;\n-      output_shardings.push_back(ifrt::ConcreteEvenSharding::Create(\n-          executable_devices, memory_kind,\n-          /*shape=*/ifrt::Shape(dimensions),\n-          /*shard_shape=*/ifrt::Shape(tile_shape_dimensions)));\n-    }\n-    return absl::OkStatus();\n-  };\n-  auto append_token = [&](MemoryKind memory_kind) {\n-    output_dtypes.push_back(DType(DType::kToken));\n-    output_shapes.push_back(Shape({}));\n-    output_shardings.push_back(\n-        ifrt::ConcreteEvenSharding::Create(executable_devices, memory_kind,\n-                                           /*shape=*/ifrt::Shape({}),\n-                                           /*shard_shape=*/ifrt::Shape({})));\n-  };\n-  auto check_output_sharding_condition =\n-      [](absl::Span<const xla::PrimitiveType> element_types,\n-         const xla::HloSharding& sharding) {\n-        if (sharding.IsTuple()) {\n-          // Check that the HLO sharding of the result has the same number of\n-          // elements as the output tuple shape. If the output is an empty tuple\n-          // then the output sharding will have a single element for the tuple\n-          // as a special case, so we will have to allow that by checking this\n-          // condition specifically.\n-          return element_types.size() == sharding.tuple_elements().size() ||\n-                 (element_types.empty() &&\n-                  sharding.tuple_elements().size() == 1);\n-        }\n-        return element_types.size() == 1;\n-      };\n-\n-  if (result_memory_kinds.has_value() &&\n-      result_memory_kinds->size() != result_element_types.size()) {\n-    return FailedPrecondition(\n-        \"Output memory kinds are inconsistent with the output shape\");\n-  }\n-  if (result_hlo_sharding.has_value() &&\n-      !check_output_sharding_condition(result_element_types,\n-                                       *result_hlo_sharding)) {\n-    return FailedPrecondition(\n-        \"Output sharding is inconsistent with the output shape\");\n-  }\n-\n-  CHECK_EQ(result_element_types.size(), result_dimensions.size());\n-  output_dtypes.reserve(result_element_types.size());\n-  output_shapes.reserve(result_element_types.size());\n-  output_shardings.reserve(result_element_types.size());\n-  for (int i = 0; i < result_element_types.size(); ++i) {\n-    const auto& element_type = result_element_types[i];\n-    MemoryKind element_memory_kind;\n-    if (result_memory_kinds.has_value()) {\n-      element_memory_kind = MemoryKind((*result_memory_kinds)[i]);\n-    }\n-    if (xla::primitive_util::IsArrayType(element_type)) {\n-      const xla::HloSharding* element_hlo_sharding = nullptr;\n-      if (result_hlo_sharding.has_value()) {\n-        element_hlo_sharding = result_hlo_sharding->IsTuple()\n-                                   ? &result_hlo_sharding->tuple_elements()[i]\n-                                   : &*result_hlo_sharding;\n-        if (element_hlo_sharding->IsTuple()) {\n-          return FailedPrecondition(\n-              \"Nested-tupled output sharding is not supported\");\n-        }\n-      }\n-      TF_RETURN_IF_ERROR(append_arg(element_type, result_dimensions[i],\n-                                    element_hlo_sharding, element_memory_kind));\n-    } else if (element_type == TOKEN) {\n-      append_token(element_memory_kind);\n-    } else {\n-      return FailedPrecondition(\n-          \"The element type is not a supported type (array, token)\");\n-    }\n-  }\n+  TF_ASSIGN_OR_RETURN(\n+      const std::vector<xla::Shape> mlir_module_output_xla_shapes,\n+      ResultShapesOfModule(module));\n+  TF_ASSIGN_OR_RETURN(const std::vector<xla::LayoutMode> output_layout_modes,\n+                      GetOutputLayoutModes(module));\n \n-  std::vector<PjRtHostSendAndRecvLoadedHostCallback*>\n-      host_send_and_recv_callbacks;\n-  host_send_and_recv_callbacks.reserve(loaded_host_callbacks.size());\n-  // Gather all `PjRtLoadedHostCallback` separately, as each execution will\n-  // register `PjRtLoadedHostCallback` for host send and recv. All host\n-  // callbacks will be referenced by the executable and any pending execution to\n-  // guarantee the liveliness of host callbacks during executions.\n-  for (auto& loaded_host_callback : loaded_host_callbacks) {\n-    auto* host_send_and_recv_callback =\n-        llvm::dyn_cast<PjRtHostSendAndRecvLoadedHostCallback>(\n-            loaded_host_callback.get());\n-    if (host_send_and_recv_callback != nullptr) {\n-      host_send_and_recv_callbacks.push_back(host_send_and_recv_callback);\n-    }\n-  }\n+  TF_ASSIGN_OR_RETURN(\n+      std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable,\n+      client->pjrt_client()->CompileAndLoad(std::move(module),\n+                                            std::move(compile_options)));\n \n-  std::vector<Device*> addressable_devices;\n-  addressable_devices.reserve(\n-      pjrt_loaded_executable->addressable_devices().size());\n-  for (xla::PjRtDevice* device :\n-       pjrt_loaded_executable->addressable_devices()) {\n-    TF_ASSIGN_OR_RETURN(Device * ifrt_device, client->LookupPjRtDevice(device));\n-    addressable_devices.push_back(ifrt_device);\n-  }\n+  TF_ASSIGN_OR_RETURN(\n+      executable_devices,\n+      AdjustExecutableDevicesForPmap(client, pjrt_loaded_executable.get(),\n+                                     std::move(executable_devices)));\n+\n+  TF_ASSIGN_OR_RETURN(auto output_dtypes_and_shapes,\n+                      GetDTypesAndShapes(mlir_module_output_xla_shapes));\n+  std::vector<DType> output_dtypes = std::move(output_dtypes_and_shapes.first);\n+  std::vector<Shape> output_shapes = std::move(output_dtypes_and_shapes.second);\n+  TF_ASSIGN_OR_RETURN(\n+      std::optional<std::vector<xla::HloSharding>> output_hlo_shardings,\n+      GetHloShardings(pjrt_loaded_executable->GetOutputShardings(),\n+                      output_dtypes, /*is_output=*/true));\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<absl::string_view> output_memory_kinds,\n+      GetMemoryKinds(pjrt_loaded_executable->GetOutputMemoryKinds(),\n+                     output_dtypes));\n+  std::vector<ShardingRef> output_shardings =\n+      MakeShardings(output_shapes, output_hlo_shardings, output_memory_kinds,\n+                    executable_devices);\n+  TF_ASSIGN_OR_RETURN(\n+      std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+          output_layouts,\n+      GetLayouts(pjrt_loaded_executable->GetOutputLayouts(),\n+                 output_layout_modes));\n \n   return LoadedExecutableRef(new PjRtLoadedExecutable(\n       client, std::move(pjrt_loaded_executable), std::move(executable_devices),\n-      std::move(addressable_devices), std::move(loaded_host_callbacks),\n-      std::move(host_send_and_recv_callbacks), std::move(output_dtypes),\n+      std::move(loaded_host_callbacks), std::move(output_dtypes),\n       std::move(output_shapes), std::move(output_shardings),\n       std::move(output_layouts)));\n }\n \n PjRtLoadedExecutable::PjRtLoadedExecutable(\n     PjRtClient* client,\n     std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable,\n-    DeviceListRef devices, std::vector<Device*> addressable_devices,\n+    DeviceListRef devices,\n     std::vector<tsl::RCReference<LoadedHostCallback>> all_loaded_host_callbacks,\n-    std::vector<PjRtHostSendAndRecvLoadedHostCallback*>\n-        host_send_recv_callbacks,\n     std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n     std::vector<ShardingRef> output_shardings,\n     std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n         output_layouts)\n     : client_(client),\n       pjrt_loaded_executable_(std::move(pjrt_loaded_executable)),\n       devices_(std::move(devices)),\n-      addressable_devices_(std::move(addressable_devices)),\n+      addressable_devices_(devices_->AddressableDeviceList()->devices()),\n       all_loaded_host_callbacks_(\n           std::make_shared<std::vector<tsl::RCReference<LoadedHostCallback>>>(\n               std::move(all_loaded_host_callbacks))),\n-      host_send_recv_callbacks_(std::move(host_send_recv_callbacks)),\n+      host_send_recv_callbacks_(\n+          GatherHostSendAndRecvCallbacks(*all_loaded_host_callbacks_)),\n       output_dtypes_(std::move(output_dtypes)),\n       output_shapes_(std::move(output_shapes)),\n       output_shardings_(std::move(output_shardings)),\n@@ -877,20 +817,9 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n     }\n   } else {\n     auto maybe_layouts = GetOutputLayouts();\n+    // An unimplemented error is converted into all-default layouts.\n     if (absl::IsUnimplemented(maybe_layouts.status())) {\n-      for (int i = 0; i < num_outputs; ++i) {\n-        std::shared_ptr<const xla::PjRtLayout> layout;\n-        if (output_dtypes_[i].kind() == xla::ifrt::DType::kToken) {\n-          layout = std::make_shared<xla::PjRtLayout>(xla::Layout());\n-        } else {\n-          TF_ASSIGN_OR_RETURN(layout,\n-                              client_->GetDefaultPjRtLayout(\n-                                  output_dtypes_[i], output_shapes_[i].dims(),\n-                                  devices_->devices().front(),\n-                                  output_shardings_[i]->memory_kind()));\n-        }\n-        layouts.push_back(std::move(layout));\n-      }\n+      layouts.resize(/*size=*/num_outputs, /*value=*/nullptr);\n     } else {\n       TF_RETURN_IF_ERROR(maybe_layouts.status());\n       layouts = *std::move(maybe_layouts);"
        },
        {
            "sha": "c3e4bd2111dc7df0cc37a04bc4a7a3c294b04994",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.h",
            "status": "modified",
            "additions": 8,
            "deletions": 21,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1f10818d7497bb615fd7b1b3e5cc96ff47ee827/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1f10818d7497bb615fd7b1b3e5cc96ff47ee827/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h?ref=d1f10818d7497bb615fd7b1b3e5cc96ff47ee827",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/Support/ExtensibleRTTI.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n-#include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n@@ -350,26 +349,12 @@ class PjRtLoadedExecutable final\n   static char ID;  // NOLINT\n \n  private:\n-  static absl::StatusOr<LoadedExecutableRef> CreateInternal(\n-      PjRtClient* client,\n-      std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable,\n-      absl::Span<const xla::PrimitiveType> result_element_types,\n-      absl::Span<const xla::DimensionVector> result_dimensions,\n-      const std::optional<xla::HloSharding>& result_hlo_sharding,\n-      const std::optional<std::vector<absl::string_view>>& result_memory_kinds,\n-      const std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>&\n-          output_layouts,\n-      std::vector<tsl::RCReference<LoadedHostCallback>> loaded_host_callbacks,\n-      DeviceListRef executable_devices);\n-\n   PjRtLoadedExecutable(\n       PjRtClient* client,\n       std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable,\n-      DeviceListRef devices, std::vector<Device*> addressable_devices,\n+      DeviceListRef devices,\n       std::vector<tsl::RCReference<LoadedHostCallback>>\n           all_loaded_host_callbacks,\n-      std::vector<PjRtHostSendAndRecvLoadedHostCallback*>\n-          host_send_recv_callbacks,\n       std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n       std::vector<ShardingRef> output_shardings,\n       std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n@@ -380,17 +365,19 @@ class PjRtLoadedExecutable final\n   // Devices that `pjrt_loaded_executable_` runs on. Empty if the executable is\n   // portable.\n   DeviceListRef devices_;\n-  std::vector<Device*> addressable_devices_;\n+  // Addressable devices. The underlying device list is owned by\n+  // `devices_->AddressableDeviceList()`.\n+  absl::Span<Device* const> addressable_devices_;\n   std::shared_ptr<std::vector<tsl::RCReference<LoadedHostCallback>>>\n       all_loaded_host_callbacks_;\n   std::vector<PjRtHostSendAndRecvLoadedHostCallback*> host_send_recv_callbacks_;\n \n-  // Output array specs. If the executable is portable, shardings in\n-  // `output_shardings_` will use an arbitrary addressable device, and will be\n-  // overridden by a `SingleDeviceSharding` generated on the fly at execution\n-  // time.\n+  // Output array specs.\n   std::vector<DType> output_dtypes_;\n   std::vector<Shape> output_shapes_;\n+  // If the executable is portable, shardings in `output_shardings_` will use an\n+  // arbitrary addressable device, and will be overridden by a\n+  // `SingleDeviceSharding` generated on the fly at execution time.\n   std::vector<ShardingRef> output_shardings_;\n   std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n       output_layouts_;"
        }
    ],
    "stats": {
        "total": 846,
        "additions": 381,
        "deletions": 465
    }
}