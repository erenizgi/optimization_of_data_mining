{
    "author": "ezhulenev",
    "message": "[stream_executor] Migrate from DeviceMemory to DeviceAddress types inside stream executor folder\n\nPiperOrigin-RevId: 840808277",
    "sha": "064ad759c42134977b7b66e02d49298037353aea",
    "files": [
        {
            "sha": "07960b7271751feab807266971d16c22cba92c60",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -108,7 +108,7 @@ absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n   se::DeviceMemory<uint8_t> log_ptr(\n       params.buffer_allocations->GetDeviceAddress(log_slice_));\n   auto buffer_debug_log =\n-      se::gpu::BufferDebugLog<BufferDebugLogEntry>::FromDeviceMemoryUnchecked(\n+      se::gpu::BufferDebugLog<BufferDebugLogEntry>::FromDeviceAddressUnchecked(\n           log_ptr);\n \n   for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {"
        },
        {
            "sha": "b7ec498a1da13822a93ba2f4a07f955eb96227cc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -109,7 +109,7 @@ absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n       params.buffer_allocations->GetDeviceAddress(log_slice_));\n   se::gpu::BufferDebugLog<BufferDebugFloatCheckEntry> buffer_debug_log =\n       se::gpu::BufferDebugLog<\n-          BufferDebugFloatCheckEntry>::FromDeviceMemoryUnchecked(log_ptr);\n+          BufferDebugFloatCheckEntry>::FromDeviceAddressUnchecked(log_ptr);\n   const uint32_t execution_id = execution_count_.fetch_add(1);\n \n   for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {"
        },
        {
            "sha": "e8d07aaeacad8ba30fe8f3ff35273f49b8d358e0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_checksum.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_checksum.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_checksum.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_checksum.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -151,7 +151,7 @@ absl::Status DumpBufferDebugChecksumLog(\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n \n   auto buffer_debug_log =\n-      se::gpu::BufferDebugLog<BufferDebugLogEntry>::FromDeviceMemoryUnchecked(\n+      se::gpu::BufferDebugLog<BufferDebugLogEntry>::FromDeviceAddressUnchecked(\n           log_buffer.device_memory());\n   TF_ASSIGN_OR_RETURN(std::vector<BufferDebugLogEntry> log_entries,\n                       buffer_debug_log.ReadFromDevice(*stream));"
        },
        {
            "sha": "8241084f52831c8ff65ebda83dba16c0b1effd89",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_float_check.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -176,7 +176,7 @@ absl::Status BufferDebugFloatCheck(\n       DebugOptions::DETECTION_MODE_NONE;\n \n   auto buffer_debug_log = se::gpu::BufferDebugLog<BufferDebugFloatCheckEntry>::\n-      FromDeviceMemoryUnchecked(log_buffer.device_memory());\n+      FromDeviceAddressUnchecked(log_buffer.device_memory());\n   TF_ASSIGN_OR_RETURN(std::vector<BufferDebugFloatCheckEntry> entries,\n                       buffer_debug_log.ReadFromDevice(*stream));\n "
        },
        {
            "sha": "41c825e3599ea2679a543e0d2af94c24027be3b3",
            "filename": "third_party/xla/xla/ffi/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fffi%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fffi%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -182,6 +182,7 @@ cc_library(\n         \"//xla/ffi/api:c_api_internal\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:logging\","
        },
        {
            "sha": "31287ac7587ef433f26beace8a43258d0bab65e0",
            "filename": "third_party/xla/xla/ffi/ffi_api.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"xla/ffi/ffi_structs.h\"\n #include \"xla/ffi/type_registry.h\"\n #include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/chain.h\"\n@@ -760,7 +761,7 @@ static XLA_FFI_Error* XLA_FFI_DeviceMemory_Allocate(\n         InvalidArgument(\"Unsupported alignment: %d\", args->alignment)};\n   }\n \n-  absl::StatusOr<stream_executor::OwningDeviceMemory> memory =\n+  absl::StatusOr<stream_executor::ScopedDeviceAddress<uint8_t>> memory =\n       gpu->allocator->Allocate(args->ctx->device_ordinal, args->size);\n   if (!memory.ok()) {\n     return new XLA_FFI_Error{std::move(memory).status()};"
        },
        {
            "sha": "3827a0241e036a7e37b85955eba77eae1216eea2",
            "filename": "third_party/xla/xla/service/gpu/kernels/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -415,6 +415,7 @@ cc_library(\n         \":custom_kernel\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor:launch_dim\",\n         \"@com_google_absl//absl/status:statusor\","
        },
        {
            "sha": "1b684f72ac40852a8848ecf36a3823635bcca5cd",
            "filename": "third_party/xla/xla/service/gpu/kernels/ptx_custom_kernel.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/kernel.h\"\n+#include \"xla/stream_executor/kernel_args.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n \n@@ -34,7 +35,7 @@ namespace se = ::stream_executor;\n \n absl::StatusOr<std::unique_ptr<se::KernelArgsPackedArrayBase>>\n KernelArgsPacking(const se::Kernel& kernel, const se::KernelArgs& args) {\n-  auto* mem_args = se::Cast<se::KernelArgsDeviceMemoryArray>(&args);\n+  auto* mem_args = se::Cast<se::KernelArgsDeviceAddressArray>(&args);\n \n   return se::PackKernelArgs<se::DeviceMemoryBase>(\n       mem_args->device_memory_args(), mem_args->number_of_shared_bytes());"
        },
        {
            "sha": "98c983ce4f0b395ca5dff31a33a3cbdecef50c3e",
            "filename": "third_party/xla/xla/service/service_executable_run_options.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fservice%2Fservice_executable_run_options.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fservice%2Fservice_executable_run_options.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fservice_executable_run_options.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -55,7 +55,7 @@ class ServiceExecutableRunOptions {\n \n   // Delegate to `ExecutableRunOptions` member.\n   se::Stream* stream() const { return run_options_.stream(); }\n-  se::DeviceMemoryAllocator* allocator() const {\n+  se::DeviceAddressAllocator* allocator() const {\n     return run_options_.allocator();\n   }\n   int device_ordinal() const { return run_options_.device_ordinal(); }"
        },
        {
            "sha": "f00f783257bce678ed97bbe18a7cc74517d4f71e",
            "filename": "third_party/xla/xla/stream_executor/BUILD",
            "status": "modified",
            "additions": 24,
            "deletions": 14,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -125,7 +125,7 @@ cc_library(\n     hdrs = [\"device_memory.h\"],\n     deps = [\n         \":device_address\",\n-        \"//xla/stream_executor/gpu:tensor_map\",\n+        \":tensor_map\",\n         \"@com_google_absl//absl/base:core_headers\",\n     ],\n )\n@@ -173,7 +173,7 @@ cc_library(\n     hdrs = [\"gpu_solver_context.h\"],\n     deps = [\n         \":blas\",\n-        \":device_memory\",\n+        \":device_address\",\n         \":stream\",\n         \"//xla:xla_data_proto_cc\",\n         \"@com_google_absl//absl/status\",\n@@ -213,8 +213,8 @@ cc_library(\n     testonly = True,\n     hdrs = [\"mock_stream.h\"],\n     deps = [\n+        \":device_address\",\n         \":device_description\",\n-        \":device_memory\",\n         \":event\",\n         \":event_based_timer\",\n         \":launch_dim\",\n@@ -237,8 +237,8 @@ cc_library(\n         \":allocator_stats\",\n         \":blas\",\n         \":command_buffer\",\n+        \":device_address\",\n         \":device_description\",\n-        \":device_memory\",\n         \":dnn\",\n         \":event\",\n         \":event_based_timer\",\n@@ -384,7 +384,7 @@ cc_library(\n     deps = [\n         \":blas_proto_cc\",\n         \":data_type\",\n-        \":device_memory\",\n+        \":device_address\",\n         \":engine_options\",\n         \":scratch_allocator\",\n         \":stream\",\n@@ -404,8 +404,8 @@ cc_library(\n     hdrs = [\"dnn.h\"],\n     deps = [\n         \":data_type\",\n+        \":device_address\",\n         \":device_description_proto_cc\",\n-        \":device_memory\",\n         \":engine_options\",\n         \":scratch_allocator\",\n         \":stream\",\n@@ -461,6 +461,7 @@ cc_library(\n         \":allocator_stats\",\n         \":blas\",\n         \":command_buffer\",\n+        \":device_address\",\n         \":device_description\",\n         \":device_memory\",\n         \":dnn\",\n@@ -530,6 +531,7 @@ cc_library(\n     srcs = [\"stream.cc\"],\n     hdrs = [\"stream.h\"],\n     deps = [\n+        \":device_address\",\n         \":device_description\",\n         \":device_memory\",\n         \":event\",\n@@ -673,7 +675,7 @@ cc_library(\n     srcs = [\"kernel_argument_packing_spec.cc\"],\n     hdrs = [\"kernel_argument_packing_spec.h\"],\n     deps = [\n-        \":device_memory\",\n+        \":device_address\",\n         \":kernel_args_packed_vector\",\n         \":kernel_argument_packing_spec_proto_cc\",\n         \"//xla/tsl/platform:statusor\",\n@@ -695,7 +697,7 @@ xla_cc_test(\n     name = \"kernel_argument_packing_spec_test\",\n     srcs = [\"kernel_argument_packing_spec_test.cc\"],\n     deps = [\n-        \":device_memory\",\n+        \":device_address\",\n         \":kernel_args_packed_vector\",\n         \":kernel_argument_packing_spec\",\n         \"//xla/tsl/platform:statusor\",\n@@ -714,6 +716,8 @@ cc_library(\n     name = \"kernel\",\n     hdrs = [\"kernel.h\"],\n     deps = [\n+        \":device_address\",\n+        \":device_memory\",\n         \":kernel_args\",\n         \":kernel_metadata\",\n         \":launch_dim\",\n@@ -777,8 +781,8 @@ cc_library(\n     name = \"scratch_allocator\",\n     hdrs = [\"scratch_allocator.h\"],\n     deps = [\n-        \":device_memory\",\n-        \":device_memory_allocator\",\n+        \":device_address\",\n+        \":device_address_allocator\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@local_tsl//tsl/platform:statusor\",\n@@ -823,7 +827,7 @@ cc_library(\n     visibility = [\":internal\"],\n     deps = [\n         \":bit_pattern\",\n-        \":device_memory\",\n+        \":device_address\",\n         \":dnn\",\n         \":kernel\",\n         \":launch_dim\",\n@@ -913,8 +917,8 @@ cc_library(\n         \":allocator_stats\",\n         \":blas\",  # build_cleaner: keep\n         \":command_buffer\",  # build_cleaner: keep\n+        \":device_address\",\n         \":device_description\",\n-        \":device_memory\",\n         \":dnn\",\n         \":executor_cache\",\n         \":fft\",\n@@ -930,6 +934,11 @@ cc_library(\n     ] + if_oss([\"//xla/tsl/protobuf:dnn_proto_cc_impl\"]),\n )\n \n+cc_library(\n+    name = \"tensor_map\",\n+    hdrs = [\"tensor_map.h\"],\n+)\n+\n #===--------------------------------------------------------------------------------------------===#\n # StreamExecutor tests\n #===--------------------------------------------------------------------------------------------===#\n@@ -980,8 +989,9 @@ cc_library(\n     name = \"kernel_args\",\n     hdrs = [\"kernel_args.h\"],\n     deps = [\n-        \":device_memory\",\n+        \":device_address\",\n         \":kernel_metadata\",\n+        \":tensor_map\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/functional:overload\",\n@@ -1004,7 +1014,7 @@ xla_cc_test(\n     name = \"kernel_args_test\",\n     srcs = [\"kernel_args_test.cc\"],\n     deps = [\n-        \":device_memory\",\n+        \":device_address\",\n         \":kernel_args\",\n         \":kernel_metadata\",\n         \"//xla/stream_executor/host:host_platform\","
        },
        {
            "sha": "3c27ae930a08c8a8969106eecd892436ebdf40a5",
            "filename": "third_party/xla/xla/stream_executor/blas.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fblas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fblas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fblas.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,27 +24,29 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"xla/stream_executor/blas.pb.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace stream_executor {\n namespace blas {\n \n // TODO(ezhulenev): We need a scoped thread local map-like container to make\n // sure that we can have multiple BlasSupport instances that do not overwrite\n // each others workspaces. For not it's ok as we know that this can't happen.\n-static thread_local DeviceMemoryBase* workspace_thread_local = nullptr;\n+static thread_local DeviceAddressBase* workspace_thread_local = nullptr;\n \n BlasSupport::ScopedWorkspace::ScopedWorkspace(BlasSupport* blas,\n-                                              DeviceMemoryBase* workspace)\n+                                              DeviceAddressBase* workspace)\n     : blas_(blas) {\n   blas->SetWorkspace(workspace);\n }\n \n BlasSupport::ScopedWorkspace::~ScopedWorkspace() { blas_->ResetWorkspace(); }\n \n-DeviceMemoryBase* BlasSupport::GetWorkspace() { return workspace_thread_local; }\n+DeviceAddressBase* BlasSupport::GetWorkspace() {\n+  return workspace_thread_local;\n+}\n \n-void BlasSupport::SetWorkspace(DeviceMemoryBase* workspace) {\n+void BlasSupport::SetWorkspace(DeviceAddressBase* workspace) {\n   workspace_thread_local = workspace;\n }\n "
        },
        {
            "sha": "58f3dc53521fff1a5f1c6f097bc88348d6e403b1",
            "filename": "third_party/xla/xla/stream_executor/blas.h",
            "status": "modified",
            "additions": 183,
            "deletions": 181,
            "changes": 364,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fblas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fblas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fblas.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -34,7 +34,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/blas.pb.h\"\n #include \"xla/stream_executor/data_type.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -54,7 +54,7 @@ struct OutputMatrixDescriptor;\n }  // namespace gpu\n \n template <typename T>\n-using DeviceMemorySlice = absl::Span<DeviceMemory<T> *const>;\n+using DeviceAddressSlice = absl::Span<DeviceAddress<T>* const>;\n \n namespace blas {\n \n@@ -234,20 +234,20 @@ class BlasSupport {\n   virtual absl::StatusOr<bool> IsMainStreamSet() const = 0;\n \n   // Computes the product of a vector by a scalar: x <- a*x.\n-  virtual bool DoBlasScal(Stream *stream, uint64_t elem_count, float alpha,\n-                          DeviceMemory<float> *x, int incx) = 0;\n-  virtual bool DoBlasScal(Stream *stream, uint64_t elem_count, double alpha,\n-                          DeviceMemory<double> *x, int incx) = 0;\n-  virtual bool DoBlasScal(Stream *stream, uint64_t elem_count, float alpha,\n-                          DeviceMemory<std::complex<float>> *x, int incx) = 0;\n-  virtual bool DoBlasScal(Stream *stream, uint64_t elem_count, double alpha,\n-                          DeviceMemory<std::complex<double>> *x, int incx) = 0;\n-  virtual bool DoBlasScal(Stream *stream, uint64_t elem_count,\n+  virtual bool DoBlasScal(Stream* stream, uint64_t elem_count, float alpha,\n+                          DeviceAddress<float>* x, int incx) = 0;\n+  virtual bool DoBlasScal(Stream* stream, uint64_t elem_count, double alpha,\n+                          DeviceAddress<double>* x, int incx) = 0;\n+  virtual bool DoBlasScal(Stream* stream, uint64_t elem_count, float alpha,\n+                          DeviceAddress<std::complex<float>>* x, int incx) = 0;\n+  virtual bool DoBlasScal(Stream* stream, uint64_t elem_count, double alpha,\n+                          DeviceAddress<std::complex<double>>* x, int incx) = 0;\n+  virtual bool DoBlasScal(Stream* stream, uint64_t elem_count,\n                           std::complex<float> alpha,\n-                          DeviceMemory<std::complex<float>> *x, int incx) = 0;\n-  virtual bool DoBlasScal(Stream *stream, uint64_t elem_count,\n+                          DeviceAddress<std::complex<float>>* x, int incx) = 0;\n+  virtual bool DoBlasScal(Stream* stream, uint64_t elem_count,\n                           std::complex<double> alpha,\n-                          DeviceMemory<std::complex<double>> *x, int incx) = 0;\n+                          DeviceAddress<std::complex<double>>* x, int incx) = 0;\n \n   // Computes a matrix-vector product using a general matrix.\n   //\n@@ -260,27 +260,28 @@ class BlasSupport {\n   // alpha and beta are scalars; a is an m-by-n general matrix; x is a vector\n   // with n(trans==kNoTranspose)/m(otherwise) elements;\n   // y is a vector with m(trans==kNoTranspose)/n(otherwise) elements.\n-  virtual bool DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n-                          uint64_t n, float alpha, const DeviceMemory<float> &a,\n-                          int lda, const DeviceMemory<float> &x, int incx,\n-                          float beta, DeviceMemory<float> *y, int incy) = 0;\n-  virtual bool DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n+  virtual bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n+                          uint64_t n, float alpha,\n+                          const DeviceAddress<float>& a, int lda,\n+                          const DeviceAddress<float>& x, int incx, float beta,\n+                          DeviceAddress<float>* y, int incy) = 0;\n+  virtual bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n                           uint64_t n, double alpha,\n-                          const DeviceMemory<double> &a, int lda,\n-                          const DeviceMemory<double> &x, int incx, double beta,\n-                          DeviceMemory<double> *y, int incy) = 0;\n-  virtual bool DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n+                          const DeviceAddress<double>& a, int lda,\n+                          const DeviceAddress<double>& x, int incx, double beta,\n+                          DeviceAddress<double>* y, int incy) = 0;\n+  virtual bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n                           uint64_t n, std::complex<float> alpha,\n-                          const DeviceMemory<std::complex<float>> &a, int lda,\n-                          const DeviceMemory<std::complex<float>> &x, int incx,\n+                          const DeviceAddress<std::complex<float>>& a, int lda,\n+                          const DeviceAddress<std::complex<float>>& x, int incx,\n                           std::complex<float> beta,\n-                          DeviceMemory<std::complex<float>> *y, int incy) = 0;\n-  virtual bool DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n+                          DeviceAddress<std::complex<float>>* y, int incy) = 0;\n+  virtual bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n                           uint64_t n, std::complex<double> alpha,\n-                          const DeviceMemory<std::complex<double>> &a, int lda,\n-                          const DeviceMemory<std::complex<double>> &x, int incx,\n-                          std::complex<double> beta,\n-                          DeviceMemory<std::complex<double>> *y, int incy) = 0;\n+                          const DeviceAddress<std::complex<double>>& a, int lda,\n+                          const DeviceAddress<std::complex<double>>& x,\n+                          int incx, std::complex<double> beta,\n+                          DeviceAddress<std::complex<double>>* y, int incy) = 0;\n \n   // Computes a matrix-matrix product with general matrices:\n   //\n@@ -299,9 +300,9 @@ class BlasSupport {\n   virtual absl::Status DoBlasGemm(Stream* stream, blas::Transpose transa,\n                                   blas::Transpose transb, uint64_t m,\n                                   uint64_t n, uint64_t k, DataType dtype,\n-                                  const void* alpha, const DeviceMemoryBase& a,\n-                                  int lda, const DeviceMemoryBase& b, int ldb,\n-                                  const void* beta, DeviceMemoryBase* c,\n+                                  const void* alpha, const DeviceAddressBase& a,\n+                                  int lda, const DeviceAddressBase& b, int ldb,\n+                                  const void* beta, DeviceAddressBase* c,\n                                   int ldc, const EngineOptions& engine_options,\n                                   blas::CallContext context) = 0;\n \n@@ -326,91 +327,91 @@ class BlasSupport {\n   virtual absl::Status DoBlasGemmWithAlgorithm(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, const void* alpha,\n-      const DeviceMemoryBase& a, DataType type_a, int lda,\n-      const DeviceMemoryBase& b, DataType type_b, int ldb, const void* beta,\n-      DeviceMemoryBase* c, DataType type_c, int ldc,\n+      const DeviceAddressBase& a, DataType type_a, int lda,\n+      const DeviceAddressBase& b, DataType type_b, int ldb, const void* beta,\n+      DeviceAddressBase* c, DataType type_c, int ldc,\n       ComputationType computation_type, AlgorithmType algorithm,\n       const EngineOptions& engine_options, ProfileResult* output_profile_result,\n       blas::CallContext context) = 0;\n   virtual absl::Status DoBlasGemmStridedBatchedWithAlgorithm(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, const void* alpha,\n-      const DeviceMemoryBase& a, DataType type_a, int lda, int64_t stride_a,\n-      const DeviceMemoryBase& b, DataType type_b, int ldb, int64_t stride_b,\n-      const void* beta, DeviceMemoryBase* c, DataType type_c, int ldc,\n+      const DeviceAddressBase& a, DataType type_a, int lda, int64_t stride_a,\n+      const DeviceAddressBase& b, DataType type_b, int ldb, int64_t stride_b,\n+      const void* beta, DeviceAddressBase* c, DataType type_c, int ldc,\n       int64_t stride_c, int batch_count, ComputationType computation_type,\n       AlgorithmType algorithm, const EngineOptions& engine_options,\n       ProfileResult* output_profile_result, blas::CallContext context) = 0;\n \n   // Computes a batch of matrix-matrix product with general matrices.\n   // This is a batched version of DoBlasGemm.\n   // The batched GEMM computes matrix product for each input/output in a, b,\n-  // and c, which contain batch_count DeviceMemory objects.\n+  // and c, which contain batch_count DeviceAddress objects.\n   virtual bool DoBlasGemmBatched(Stream* stream, blas::Transpose transa,\n                                  blas::Transpose transb, uint64_t m, uint64_t n,\n                                  uint64_t k, float alpha,\n-                                 DeviceMemorySlice<Eigen::half> a, int lda,\n-                                 DeviceMemorySlice<Eigen::half> b, int ldb,\n-                                 float beta, DeviceMemorySlice<Eigen::half> c,\n+                                 DeviceAddressSlice<Eigen::half> a, int lda,\n+                                 DeviceAddressSlice<Eigen::half> b, int ldb,\n+                                 float beta, DeviceAddressSlice<Eigen::half> c,\n                                  int ldc, int batch_count,\n                                  const EngineOptions& engine_options,\n                                  ScratchAllocator* scratch_allocator,\n                                  blas::CallContext context) = 0;\n   virtual bool DoBlasGemmBatched(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, float alpha,\n-      DeviceMemorySlice<Eigen::bfloat16> a, int lda,\n-      DeviceMemorySlice<Eigen::bfloat16> b, int ldb, float beta,\n-      DeviceMemorySlice<Eigen::bfloat16> c, int ldc, int batch_count,\n+      DeviceAddressSlice<Eigen::bfloat16> a, int lda,\n+      DeviceAddressSlice<Eigen::bfloat16> b, int ldb, float beta,\n+      DeviceAddressSlice<Eigen::bfloat16> c, int ldc, int batch_count,\n       const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n       blas::CallContext context) = 0;\n   virtual bool DoBlasGemmBatched(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, float alpha,\n-      DeviceMemorySlice<float> a, int lda, DeviceMemorySlice<float> b, int ldb,\n-      float beta, DeviceMemorySlice<float> c, int ldc, int batch_count,\n-      const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n-      blas::CallContext context) = 0;\n+      DeviceAddressSlice<float> a, int lda, DeviceAddressSlice<float> b,\n+      int ldb, float beta, DeviceAddressSlice<float> c, int ldc,\n+      int batch_count, const EngineOptions& engine_options,\n+      ScratchAllocator* scratch_allocator, blas::CallContext context) = 0;\n   virtual bool DoBlasGemmBatched(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, double alpha,\n-      DeviceMemorySlice<double> a, int lda, DeviceMemorySlice<double> b,\n-      int ldb, double beta, DeviceMemorySlice<double> c, int ldc,\n+      DeviceAddressSlice<double> a, int lda, DeviceAddressSlice<double> b,\n+      int ldb, double beta, DeviceAddressSlice<double> c, int ldc,\n       int batch_count, const EngineOptions& engine_options,\n       ScratchAllocator* scratch_allocator, blas::CallContext context) = 0;\n   virtual bool DoBlasGemmBatched(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, std::complex<float> alpha,\n-      DeviceMemorySlice<std::complex<float>> a, int lda,\n-      DeviceMemorySlice<std::complex<float>> b, int ldb,\n-      std::complex<float> beta, DeviceMemorySlice<std::complex<float>> c,\n+      DeviceAddressSlice<std::complex<float>> a, int lda,\n+      DeviceAddressSlice<std::complex<float>> b, int ldb,\n+      std::complex<float> beta, DeviceAddressSlice<std::complex<float>> c,\n       int ldc, int batch_count, const EngineOptions& engine_options,\n       ScratchAllocator* scratch_allocator, blas::CallContext context) = 0;\n   virtual bool DoBlasGemmBatched(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, std::complex<double> alpha,\n-      DeviceMemorySlice<std::complex<double>> a, int lda,\n-      DeviceMemorySlice<std::complex<double>> b, int ldb,\n-      std::complex<double> beta, DeviceMemorySlice<std::complex<double>> c,\n+      DeviceAddressSlice<std::complex<double>> a, int lda,\n+      DeviceAddressSlice<std::complex<double>> b, int ldb,\n+      std::complex<double> beta, DeviceAddressSlice<std::complex<double>> c,\n       int ldc, int batch_count, const EngineOptions& engine_options,\n       ScratchAllocator* scratch_allocator, blas::CallContext context) = 0;\n   // Batched gemm with strides instead of pointer arrays.\n   virtual absl::Status DoBlasGemmStridedBatched(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, DataType dtype, const void* alpha,\n-      const DeviceMemoryBase& a, int lda, int64_t stride_a,\n-      const DeviceMemoryBase& b, int ldb, int64_t stride_b, const void* beta,\n-      DeviceMemoryBase* c, int ldc, int64_t stride_c, int batch_count,\n+      const DeviceAddressBase& a, int lda, int64_t stride_a,\n+      const DeviceAddressBase& b, int ldb, int64_t stride_b, const void* beta,\n+      DeviceAddressBase* c, int ldc, int64_t stride_c, int batch_count,\n       const EngineOptions& engine_options, blas::CallContext context) = 0;\n \n   template <typename InputType, typename OutputType, typename ConstantType>\n   absl::Status BlasGemmStridedBatchedWithAlgorithm(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, ConstantType alpha,\n-      const DeviceMemory<InputType>& a, int lda, int64_t stride_a,\n-      const DeviceMemory<InputType>& b, int ldb, int64_t stride_b,\n-      ConstantType beta, DeviceMemory<OutputType>* c, int ldc, int64_t stride_c,\n-      int batch_count, blas::ComputationType computation_type,\n+      const DeviceAddress<InputType>& a, int lda, int64_t stride_a,\n+      const DeviceAddress<InputType>& b, int ldb, int64_t stride_b,\n+      ConstantType beta, DeviceAddress<OutputType>* c, int ldc,\n+      int64_t stride_c, int batch_count, blas::ComputationType computation_type,\n       blas::AlgorithmType algorithm, const EngineOptions& engine_options,\n       blas::ProfileResult* output_profile_result, blas::CallContext context) {\n     TF_RETURN_IF_ERROR(\n@@ -440,10 +441,10 @@ class BlasSupport {\n   absl::Status BlasGemm(Stream* stream, blas::Transpose transa,\n                         blas::Transpose transb, uint64_t m, uint64_t n,\n                         uint64_t k, ConstantType alpha,\n-                        const DeviceMemory<InputType>& a, int lda,\n-                        const DeviceMemory<InputType>& b, int ldb,\n-                        ConstantType beta, DeviceMemory<OutputType>* c, int ldc,\n-                        const EngineOptions& engine_options,\n+                        const DeviceAddress<InputType>& a, int lda,\n+                        const DeviceAddress<InputType>& b, int ldb,\n+                        ConstantType beta, DeviceAddress<OutputType>* c,\n+                        int ldc, const EngineOptions& engine_options,\n                         blas::CallContext context) {\n     static_assert(\n         detail::is_any_of<InputType, int8_t, Eigen::half, Eigen::bfloat16,\n@@ -474,9 +475,9 @@ class BlasSupport {\n   template <typename InputType, typename OutputType>\n   absl::Status BlasGemm(Stream* stream, blas::Transpose transa,\n                         blas::Transpose transb, uint64_t m, uint64_t n,\n-                        uint64_t k, const DeviceMemory<InputType>& a, int lda,\n-                        const DeviceMemory<InputType>& b, int ldb,\n-                        DeviceMemory<OutputType>* c, int ldc,\n+                        uint64_t k, const DeviceAddress<InputType>& a, int lda,\n+                        const DeviceAddress<InputType>& b, int ldb,\n+                        DeviceAddress<OutputType>* c, int ldc,\n                         const EngineOptions& engine_options,\n                         blas::CallContext context) {\n     InputType alpha{1.0};\n@@ -489,9 +490,9 @@ class BlasSupport {\n   absl::Status BlasGemmWithAlgorithm(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, ConstantType alpha,\n-      const DeviceMemory<InputType>& a, int lda,\n-      const DeviceMemory<InputType>& b, int ldb, ConstantType beta,\n-      DeviceMemory<OutputType>* c, int ldc,\n+      const DeviceAddress<InputType>& a, int lda,\n+      const DeviceAddress<InputType>& b, int ldb, ConstantType beta,\n+      DeviceAddress<OutputType>* c, int ldc,\n       blas::ComputationType computation_type, blas::AlgorithmType algorithm,\n       const EngineOptions& engine_options,\n       blas::ProfileResult* output_profile_result, blas::CallContext context) {\n@@ -521,12 +522,12 @@ class BlasSupport {\n \n   template <typename InputType, typename OutputType>\n   absl::Status BlasGemmWithAlgorithm(\n-      Stream *stream, blas::Transpose transa, blas::Transpose transb,\n-      uint64_t m, uint64_t n, uint64_t k, const DeviceMemory<InputType> &a,\n-      int lda, const DeviceMemory<InputType> &b, int ldb,\n-      DeviceMemory<OutputType> *c, int ldc,\n+      Stream* stream, blas::Transpose transa, blas::Transpose transb,\n+      uint64_t m, uint64_t n, uint64_t k, const DeviceAddress<InputType>& a,\n+      int lda, const DeviceAddress<InputType>& b, int ldb,\n+      DeviceAddress<OutputType>* c, int ldc,\n       blas::ComputationType computation_type, blas::AlgorithmType algorithm,\n-      blas::ProfileResult *output_profile_result, blas::CallContext context) {\n+      blas::ProfileResult* output_profile_result, blas::CallContext context) {\n     OutputType alpha{1};\n     OutputType beta{0};\n \n@@ -540,10 +541,10 @@ class BlasSupport {\n   absl::Status BlasGemmStridedBatched(\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,\n       uint64_t m, uint64_t n, uint64_t k, ConstantType alpha,\n-      const DeviceMemory<InputType>& a, int lda, int64_t stride_a,\n-      const DeviceMemory<InputType>& b, int ldb, int64_t stride_b,\n-      ConstantType beta, DeviceMemory<OutputType>* c, int ldc, int64_t stride_c,\n-      int batch_count, const EngineOptions& engine_options,\n+      const DeviceAddress<InputType>& a, int lda, int64_t stride_a,\n+      const DeviceAddress<InputType>& b, int ldb, int64_t stride_b,\n+      ConstantType beta, DeviceAddress<OutputType>* c, int ldc,\n+      int64_t stride_c, int batch_count, const EngineOptions& engine_options,\n       blas::CallContext context) {\n     static_assert(\n         detail::is_any_of<InputType, int8_t, float, Eigen::half,\n@@ -577,58 +578,58 @@ class BlasSupport {\n   // alpha is a scalar; x and b are m-by-n matrices; a is a unit, or non-unit,\n   // upper or lower triangular matrix; op(a) is one of op(a) = a, or op(a) = a',\n   // or op(a) = conj(a').\n-  virtual bool DoBlasTrsm(Stream *stream, blas::Side side,\n+  virtual bool DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n-                          float alpha, const DeviceMemory<float> &a, int lda,\n-                          DeviceMemory<float> *b, int ldb) = 0;\n-  virtual bool DoBlasTrsm(Stream *stream, blas::Side side,\n+                          float alpha, const DeviceAddress<float>& a, int lda,\n+                          DeviceAddress<float>* b, int ldb) = 0;\n+  virtual bool DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n-                          double alpha, const DeviceMemory<double> &a, int lda,\n-                          DeviceMemory<double> *b, int ldb) = 0;\n-  virtual bool DoBlasTrsm(Stream *stream, blas::Side side,\n+                          double alpha, const DeviceAddress<double>& a, int lda,\n+                          DeviceAddress<double>* b, int ldb) = 0;\n+  virtual bool DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n                           std::complex<float> alpha,\n-                          const DeviceMemory<std::complex<float>> &a, int lda,\n-                          DeviceMemory<std::complex<float>> *b, int ldb) = 0;\n-  virtual bool DoBlasTrsm(Stream *stream, blas::Side side,\n+                          const DeviceAddress<std::complex<float>>& a, int lda,\n+                          DeviceAddress<std::complex<float>>* b, int ldb) = 0;\n+  virtual bool DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n                           std::complex<double> alpha,\n-                          const DeviceMemory<std::complex<double>> &a, int lda,\n-                          DeviceMemory<std::complex<double>> *b, int ldb) = 0;\n+                          const DeviceAddress<std::complex<double>>& a, int lda,\n+                          DeviceAddress<std::complex<double>>* b, int ldb) = 0;\n \n   // Same as DoBlasTrsm, but operates over a list of a's and b's.  The lists\n   // `as` and `bs` must have the same length.\n-  virtual bool DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+  virtual bool DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n-                                 float alpha, const DeviceMemory<float *> &as,\n-                                 int lda, DeviceMemory<float *> *bs, int ldb,\n+                                 float alpha, const DeviceAddress<float*>& as,\n+                                 int lda, DeviceAddress<float*>* bs, int ldb,\n                                  int batch_count) = 0;\n-  virtual bool DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+  virtual bool DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n-                                 double alpha, const DeviceMemory<double *> &as,\n-                                 int lda, DeviceMemory<double *> *bs, int ldb,\n+                                 double alpha, const DeviceAddress<double*>& as,\n+                                 int lda, DeviceAddress<double*>* bs, int ldb,\n                                  int batch_count) = 0;\n-  virtual bool DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+  virtual bool DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n                                  std::complex<float> alpha,\n-                                 const DeviceMemory<std::complex<float> *> &as,\n+                                 const DeviceAddress<std::complex<float>*>& as,\n                                  int lda,\n-                                 DeviceMemory<std::complex<float> *> *bs,\n+                                 DeviceAddress<std::complex<float>*>* bs,\n                                  int ldb, int batch_count) = 0;\n-  virtual bool DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+  virtual bool DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n                                  std::complex<double> alpha,\n-                                 const DeviceMemory<std::complex<double> *> &as,\n+                                 const DeviceAddress<std::complex<double>*>& as,\n                                  int lda,\n-                                 DeviceMemory<std::complex<double> *> *bs,\n+                                 DeviceAddress<std::complex<double>*>* bs,\n                                  int ldb, int batch_count) = 0;\n \n   // TODO(ezhulenev): We should never pass ScratchAllocator to any of the APIs\n@@ -641,7 +642,7 @@ class BlasSupport {\n   // allocating scratch memory on demand.\n   class ScopedWorkspace {\n    public:\n-    ScopedWorkspace(BlasSupport *blas, DeviceMemoryBase *workspace);\n+    ScopedWorkspace(BlasSupport* blas, DeviceAddressBase* workspace);\n     ~ScopedWorkspace();\n \n    private:\n@@ -651,7 +652,7 @@ class BlasSupport {\n   virtual absl::Status GetVersion(std::string *version) = 0;\n \n  protected:\n-  DeviceMemoryBase *GetWorkspace();\n+  DeviceAddressBase* GetWorkspace();\n \n   BlasSupport() {}\n \n@@ -663,7 +664,7 @@ class BlasSupport {\n   //\n   // TODO(ezhulenev): This is a giant footgun! We have to remove it and use\n   // explicit workspace memory argument for all BLAS operations.\n-  void SetWorkspace(DeviceMemoryBase *workspace);\n+  void SetWorkspace(DeviceAddressBase* workspace);\n \n   // Resets user-defined workspace memory, so that Blas operations can use their\n   // own memory pool for allocating workspace.\n@@ -743,45 +744,45 @@ class BlasSupport {\n #define TENSORFLOW_STREAM_EXECUTOR_GPU_BLAS_SUPPORT_OVERRIDES                  \\\n   absl::StatusOr<bool> IsMainStreamSet() const override;                       \\\n   bool DoBlasScal(Stream* stream, uint64_t elem_count, float alpha,            \\\n-                  DeviceMemory<float>* x, int incx) override;                  \\\n+                  DeviceAddress<float>* x, int incx) override;                 \\\n   bool DoBlasScal(Stream* stream, uint64_t elem_count, double alpha,           \\\n-                  DeviceMemory<double>* x, int incx) override;                 \\\n+                  DeviceAddress<double>* x, int incx) override;                \\\n   bool DoBlasScal(Stream* stream, uint64_t elem_count, float alpha,            \\\n-                  DeviceMemory<std::complex<float>>* x, int incx) override;    \\\n+                  DeviceAddress<std::complex<float>>* x, int incx) override;   \\\n   bool DoBlasScal(Stream* stream, uint64_t elem_count, double alpha,           \\\n-                  DeviceMemory<std::complex<double>>* x, int incx) override;   \\\n+                  DeviceAddress<std::complex<double>>* x, int incx) override;  \\\n   bool DoBlasScal(Stream* stream, uint64_t elem_count,                         \\\n                   std::complex<float> alpha,                                   \\\n-                  DeviceMemory<std::complex<float>>* x, int incx) override;    \\\n+                  DeviceAddress<std::complex<float>>* x, int incx) override;   \\\n   bool DoBlasScal(Stream* stream, uint64_t elem_count,                         \\\n                   std::complex<double> alpha,                                  \\\n-                  DeviceMemory<std::complex<double>>* x, int incx) override;   \\\n+                  DeviceAddress<std::complex<double>>* x, int incx) override;  \\\n   bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,           \\\n-                  uint64_t n, float alpha, const DeviceMemory<float>& a,       \\\n-                  int lda, const DeviceMemory<float>& x, int incx, float beta, \\\n-                  DeviceMemory<float>* y, int incy) override;                  \\\n+                  uint64_t n, float alpha, const DeviceAddress<float>& a,      \\\n+                  int lda, const DeviceAddress<float>& x, int incx,            \\\n+                  float beta, DeviceAddress<float>* y, int incy) override;     \\\n   bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,           \\\n-                  uint64_t n, double alpha, const DeviceMemory<double>& a,     \\\n-                  int lda, const DeviceMemory<double>& x, int incx,            \\\n-                  double beta, DeviceMemory<double>* y, int incy) override;    \\\n+                  uint64_t n, double alpha, const DeviceAddress<double>& a,    \\\n+                  int lda, const DeviceAddress<double>& x, int incx,           \\\n+                  double beta, DeviceAddress<double>* y, int incy) override;   \\\n   bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,           \\\n                   uint64_t n, std::complex<float> alpha,                       \\\n-                  const DeviceMemory<std::complex<float>>& a, int lda,         \\\n-                  const DeviceMemory<std::complex<float>>& x, int incx,        \\\n+                  const DeviceAddress<std::complex<float>>& a, int lda,        \\\n+                  const DeviceAddress<std::complex<float>>& x, int incx,       \\\n                   std::complex<float> beta,                                    \\\n-                  DeviceMemory<std::complex<float>>* y, int incy) override;    \\\n+                  DeviceAddress<std::complex<float>>* y, int incy) override;   \\\n   bool DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,           \\\n                   uint64_t n, std::complex<double> alpha,                      \\\n-                  const DeviceMemory<std::complex<double>>& a, int lda,        \\\n-                  const DeviceMemory<std::complex<double>>& x, int incx,       \\\n+                  const DeviceAddress<std::complex<double>>& a, int lda,       \\\n+                  const DeviceAddress<std::complex<double>>& x, int incx,      \\\n                   std::complex<double> beta,                                   \\\n-                  DeviceMemory<std::complex<double>>* y, int incy) override;   \\\n+                  DeviceAddress<std::complex<double>>* y, int incy) override;  \\\n   absl::Status DoBlasGemm(                                                     \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, blas::DataType dtype,                \\\n-      const void* alpha, const DeviceMemoryBase& a, int lda,                   \\\n-      const DeviceMemoryBase& b, int ldb, const void* beta,                    \\\n-      DeviceMemoryBase* c, int ldc, const EngineOptions& engine_options,       \\\n+      const void* alpha, const DeviceAddressBase& a, int lda,                  \\\n+      const DeviceAddressBase& b, int ldb, const void* beta,                   \\\n+      DeviceAddressBase* c, int ldc, const EngineOptions& engine_options,      \\\n       blas::CallContext context) override;                                     \\\n   bool GetBlasGemmAlgorithms(                                                  \\\n       Stream* stream, const gpu::MatrixDescriptor& a,                          \\\n@@ -791,124 +792,125 @@ class BlasSupport {\n   absl::Status DoBlasGemmWithAlgorithm(                                        \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, const void* alpha,                   \\\n-      const DeviceMemoryBase& a, blas::DataType type_a, int lda,               \\\n-      const DeviceMemoryBase& b, blas::DataType type_b, int ldb,               \\\n-      const void* beta, DeviceMemoryBase* c, blas::DataType type_c, int ldc,   \\\n+      const DeviceAddressBase& a, blas::DataType type_a, int lda,              \\\n+      const DeviceAddressBase& b, blas::DataType type_b, int ldb,              \\\n+      const void* beta, DeviceAddressBase* c, blas::DataType type_c, int ldc,  \\\n       blas::ComputationType computation_type, blas::AlgorithmType algorithm,   \\\n       const EngineOptions& engine_options,                                     \\\n       blas::ProfileResult* output_profile_result, blas::CallContext context)   \\\n       override;                                                                \\\n   bool DoBlasGemmBatched(                                                      \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, float alpha,                         \\\n-      DeviceMemorySlice<Eigen::half> a, int lda,                               \\\n-      DeviceMemorySlice<Eigen::half> b, int ldb, float beta,                   \\\n-      DeviceMemorySlice<Eigen::half> c, int ldc, int batch_count,              \\\n+      DeviceAddressSlice<Eigen::half> a, int lda,                              \\\n+      DeviceAddressSlice<Eigen::half> b, int ldb, float beta,                  \\\n+      DeviceAddressSlice<Eigen::half> c, int ldc, int batch_count,             \\\n       const EngineOptions& engine_options,                                     \\\n       ScratchAllocator* scratch_allocator, blas::CallContext context)          \\\n       override;                                                                \\\n   bool DoBlasGemmBatched(                                                      \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, float alpha,                         \\\n-      DeviceMemorySlice<Eigen::bfloat16> a, int lda,                           \\\n-      DeviceMemorySlice<Eigen::bfloat16> b, int ldb, float beta,               \\\n-      DeviceMemorySlice<Eigen::bfloat16> c, int ldc, int batch_count,          \\\n+      DeviceAddressSlice<Eigen::bfloat16> a, int lda,                          \\\n+      DeviceAddressSlice<Eigen::bfloat16> b, int ldb, float beta,              \\\n+      DeviceAddressSlice<Eigen::bfloat16> c, int ldc, int batch_count,         \\\n       const EngineOptions& engine_options,                                     \\\n       ScratchAllocator* scratch_allocator, blas::CallContext context)          \\\n       override;                                                                \\\n   bool DoBlasGemmBatched(Stream* stream, blas::Transpose transa,               \\\n                          blas::Transpose transb, uint64_t m, uint64_t n,       \\\n-                         uint64_t k, float alpha, DeviceMemorySlice<float> a,  \\\n-                         int lda, DeviceMemorySlice<float> b, int ldb,         \\\n-                         float beta, DeviceMemorySlice<float> c, int ldc,      \\\n+                         uint64_t k, float alpha, DeviceAddressSlice<float> a, \\\n+                         int lda, DeviceAddressSlice<float> b, int ldb,        \\\n+                         float beta, DeviceAddressSlice<float> c, int ldc,     \\\n                          int batch_count, const EngineOptions& engine_options, \\\n                          ScratchAllocator* scratch_allocator,                  \\\n                          blas::CallContext context) override;                  \\\n   bool DoBlasGemmBatched(                                                      \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, double alpha,                        \\\n-      DeviceMemorySlice<double> a, int lda, DeviceMemorySlice<double> b,       \\\n-      int ldb, double beta, DeviceMemorySlice<double> c, int ldc,              \\\n+      DeviceAddressSlice<double> a, int lda, DeviceAddressSlice<double> b,     \\\n+      int ldb, double beta, DeviceAddressSlice<double> c, int ldc,             \\\n       int batch_count, const EngineOptions& engine_options,                    \\\n       ScratchAllocator* scratch_allocator, blas::CallContext context)          \\\n       override;                                                                \\\n   bool DoBlasGemmBatched(                                                      \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, std::complex<float> alpha,           \\\n-      DeviceMemorySlice<std::complex<float>> a, int lda,                       \\\n-      DeviceMemorySlice<std::complex<float>> b, int ldb,                       \\\n-      std::complex<float> beta, DeviceMemorySlice<std::complex<float>> c,      \\\n+      DeviceAddressSlice<std::complex<float>> a, int lda,                      \\\n+      DeviceAddressSlice<std::complex<float>> b, int ldb,                      \\\n+      std::complex<float> beta, DeviceAddressSlice<std::complex<float>> c,     \\\n       int ldc, int batch_count, const EngineOptions& engine_options,           \\\n       ScratchAllocator* scratch_allocator, blas::CallContext context)          \\\n       override;                                                                \\\n   bool DoBlasGemmBatched(                                                      \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, std::complex<double> alpha,          \\\n-      DeviceMemorySlice<std::complex<double>> a, int lda,                      \\\n-      DeviceMemorySlice<std::complex<double>> b, int ldb,                      \\\n-      std::complex<double> beta, DeviceMemorySlice<std::complex<double>> c,    \\\n+      DeviceAddressSlice<std::complex<double>> a, int lda,                     \\\n+      DeviceAddressSlice<std::complex<double>> b, int ldb,                     \\\n+      std::complex<double> beta, DeviceAddressSlice<std::complex<double>> c,   \\\n       int ldc, int batch_count, const EngineOptions& engine_options,           \\\n       ScratchAllocator* scratch_allocator, blas::CallContext context)          \\\n       override;                                                                \\\n   absl::Status DoBlasGemmStridedBatched(                                       \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, blas::DataType dtype,                \\\n-      const void* alpha, const DeviceMemoryBase& a, int lda, int64_t stride_a, \\\n-      const DeviceMemoryBase& b, int ldb, int64_t stride_b, const void* beta,  \\\n-      DeviceMemoryBase* c, int ldc, int64_t stride_c, int batch_count,         \\\n-      const EngineOptions& engine_options, blas::CallContext context)          \\\n-      override;                                                                \\\n+      const void* alpha, const DeviceAddressBase& a, int lda,                  \\\n+      int64_t stride_a, const DeviceAddressBase& b, int ldb, int64_t stride_b, \\\n+      const void* beta, DeviceAddressBase* c, int ldc, int64_t stride_c,       \\\n+      int batch_count, const EngineOptions& engine_options,                    \\\n+      blas::CallContext context) override;                                     \\\n   absl::Status DoBlasGemmStridedBatchedWithAlgorithm(                          \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, const void* alpha,                   \\\n-      const DeviceMemoryBase& a, blas::DataType type_a, int lda,               \\\n-      int64_t stride_a, const DeviceMemoryBase& b, blas::DataType type_b,      \\\n-      int ldb, int64_t stride_b, const void* beta, DeviceMemoryBase* c,        \\\n+      const DeviceAddressBase& a, blas::DataType type_a, int lda,              \\\n+      int64_t stride_a, const DeviceAddressBase& b, blas::DataType type_b,     \\\n+      int ldb, int64_t stride_b, const void* beta, DeviceAddressBase* c,       \\\n       blas::DataType type_c, int ldc, int64_t stride_c, int batch_count,       \\\n       blas::ComputationType computation_type, blas::AlgorithmType algorithm,   \\\n       const EngineOptions& engine_options,                                     \\\n       blas::ProfileResult* output_profile_result, blas::CallContext context)   \\\n       override;                                                                \\\n   bool DoBlasTrsm(Stream* stream, blas::Side side, blas::UpperLower uplo,      \\\n                   blas::Transpose transa, blas::Diagonal diag, uint64_t m,     \\\n-                  uint64_t n, float alpha, const DeviceMemory<float>& a,       \\\n-                  int lda, DeviceMemory<float>* b, int ldb) override;          \\\n+                  uint64_t n, float alpha, const DeviceAddress<float>& a,      \\\n+                  int lda, DeviceAddress<float>* b, int ldb) override;         \\\n   bool DoBlasTrsm(Stream* stream, blas::Side side, blas::UpperLower uplo,      \\\n                   blas::Transpose transa, blas::Diagonal diag, uint64_t m,     \\\n-                  uint64_t n, double alpha, const DeviceMemory<double>& a,     \\\n-                  int lda, DeviceMemory<double>* b, int ldb) override;         \\\n+                  uint64_t n, double alpha, const DeviceAddress<double>& a,    \\\n+                  int lda, DeviceAddress<double>* b, int ldb) override;        \\\n   bool DoBlasTrsm(Stream* stream, blas::Side side, blas::UpperLower uplo,      \\\n                   blas::Transpose transa, blas::Diagonal diag, uint64_t m,     \\\n                   uint64_t n, std::complex<float> alpha,                       \\\n-                  const DeviceMemory<std::complex<float>>& a, int lda,         \\\n-                  DeviceMemory<std::complex<float>>* b, int ldb) override;     \\\n+                  const DeviceAddress<std::complex<float>>& a, int lda,        \\\n+                  DeviceAddress<std::complex<float>>* b, int ldb) override;    \\\n   bool DoBlasTrsm(Stream* stream, blas::Side side, blas::UpperLower uplo,      \\\n                   blas::Transpose transa, blas::Diagonal diag, uint64_t m,     \\\n                   uint64_t n, std::complex<double> alpha,                      \\\n-                  const DeviceMemory<std::complex<double>>& a, int lda,        \\\n-                  DeviceMemory<std::complex<double>>* b, int ldb) override;    \\\n-  bool DoBlasTrsmBatched(                                                      \\\n-      Stream* stream, blas::Side side, blas::UpperLower uplo,                  \\\n-      blas::Transpose transa, blas::Diagonal diag, uint64_t m, uint64_t n,     \\\n-      float alpha, const DeviceMemory<float*>& as, int lda,                    \\\n-      DeviceMemory<float*>* bs, int ldb, int batch_count) override;            \\\n+                  const DeviceAddress<std::complex<double>>& a, int lda,       \\\n+                  DeviceAddress<std::complex<double>>* b, int ldb) override;   \\\n   bool DoBlasTrsmBatched(                                                      \\\n       Stream* stream, blas::Side side, blas::UpperLower uplo,                  \\\n       blas::Transpose transa, blas::Diagonal diag, uint64_t m, uint64_t n,     \\\n-      double alpha, const DeviceMemory<double*>& as, int lda,                  \\\n-      DeviceMemory<double*>* bs, int ldb, int batch_count) override;           \\\n+      float alpha, const DeviceAddress<float*>& as, int lda,                   \\\n+      DeviceAddress<float*>* bs, int ldb, int batch_count) override;           \\\n   bool DoBlasTrsmBatched(                                                      \\\n       Stream* stream, blas::Side side, blas::UpperLower uplo,                  \\\n       blas::Transpose transa, blas::Diagonal diag, uint64_t m, uint64_t n,     \\\n-      std::complex<float> alpha, const DeviceMemory<std::complex<float>*>& as, \\\n-      int lda, DeviceMemory<std::complex<float>*>* bs, int ldb,                \\\n-      int batch_count) override;                                               \\\n+      double alpha, const DeviceAddress<double*>& as, int lda,                 \\\n+      DeviceAddress<double*>* bs, int ldb, int batch_count) override;          \\\n+  bool DoBlasTrsmBatched(Stream* stream, blas::Side side,                      \\\n+                         blas::UpperLower uplo, blas::Transpose transa,        \\\n+                         blas::Diagonal diag, uint64_t m, uint64_t n,          \\\n+                         std::complex<float> alpha,                            \\\n+                         const DeviceAddress<std::complex<float>*>& as,        \\\n+                         int lda, DeviceAddress<std::complex<float>*>* bs,     \\\n+                         int ldb, int batch_count) override;                   \\\n   bool DoBlasTrsmBatched(Stream* stream, blas::Side side,                      \\\n                          blas::UpperLower uplo, blas::Transpose transa,        \\\n                          blas::Diagonal diag, uint64_t m, uint64_t n,          \\\n                          std::complex<double> alpha,                           \\\n-                         const DeviceMemory<std::complex<double>*>& as,        \\\n-                         int lda, DeviceMemory<std::complex<double>*>* bs,     \\\n+                         const DeviceAddress<std::complex<double>*>& as,       \\\n+                         int lda, DeviceAddress<std::complex<double>*>* bs,    \\\n                          int ldb, int batch_count) override;                   \\\n   absl::Status GetVersion(std::string* version) override;\n "
        },
        {
            "sha": "525fd82c170ffe3701ba5b5bb67ec7e047cc9017",
            "filename": "third_party/xla/xla/stream_executor/command_buffer.h",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/bit_pattern.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -200,23 +200,23 @@ class CommandBuffer {\n \n   // Creates a device-to-device memory copy.\n   virtual absl::StatusOr<const Command*> CreateMemcpyD2D(\n-      DeviceMemoryBase* dst, const DeviceMemoryBase& src, uint64_t size,\n+      DeviceAddressBase* dst, const DeviceAddressBase& src, uint64_t size,\n       absl::Span<const Command* const> dependencies) = 0;\n \n   // Updates a device-to-device memory copy.\n   virtual absl::Status UpdateMemcpyD2D(const Command* command,\n-                                       DeviceMemoryBase* dst,\n-                                       const DeviceMemoryBase& src,\n+                                       DeviceAddressBase* dst,\n+                                       const DeviceAddressBase& src,\n                                        uint64_t size) = 0;\n \n   // Creates a memset command.\n   virtual absl::StatusOr<const Command*> CreateMemset(\n-      DeviceMemoryBase* dst, BitPattern bit_pattern, size_t num_elements,\n+      DeviceAddressBase* dst, BitPattern bit_pattern, size_t num_elements,\n       absl::Span<const Command* const> dependencies) = 0;\n \n   // Updates a memset command.\n   virtual absl::Status UpdateMemset(const Command* command,\n-                                    DeviceMemoryBase* dst,\n+                                    DeviceAddressBase* dst,\n                                     const BitPattern& bit_pattern,\n                                     size_t num_elements) = 0;\n \n@@ -226,13 +226,13 @@ class CommandBuffer {\n \n   // Creates a DNN graph launch command.\n   virtual absl::StatusOr<const Command*> CreateDnnGraphCommand(\n-      dnn::DnnGraph&, Stream&, absl::Span<DeviceMemoryBase> operands,\n+      dnn::DnnGraph&, Stream&, absl::Span<DeviceAddressBase> operands,\n       absl::Span<const Command* const> dependencies) = 0;\n \n   // Updates a DNN graph command.\n   virtual absl::Status UpdateDnnGraphCommand(\n       const Command*, dnn::DnnGraph&, Stream&,\n-      absl::Span<DeviceMemoryBase> operands) = 0;\n+      absl::Span<DeviceAddressBase> operands) = 0;\n \n   //--------------------------------------------------------------------------//\n   // Command buffer condtitional commands API\n@@ -245,20 +245,20 @@ class CommandBuffer {\n   //\n   // See: https://github.com/openxla/stablehlo/blob/main/docs/spec.md#case\n   virtual absl::StatusOr<const Command*> CreateCase(\n-      DeviceMemory<int32_t> index, std::vector<CreateCommands> create_branches,\n+      DeviceAddress<int32_t> index, std::vector<CreateCommands> create_branches,\n       absl::Span<const Command* const> dependencies) = 0;\n \n   virtual absl::StatusOr<const Command*> CreateCase(\n-      DeviceMemory<bool> index, std::vector<CreateCommands> create_branches,\n+      DeviceAddress<bool> index, std::vector<CreateCommands> create_branches,\n       absl::Span<const Command* const> dependencies) = 0;\n \n   // Updates a Case command.\n   virtual absl::Status UpdateCase(\n-      const Command* command, DeviceMemory<int32_t> index,\n+      const Command* command, DeviceAddress<int32_t> index,\n       std::vector<UpdateCommands> update_branches) = 0;\n \n   virtual absl::Status UpdateCase(\n-      const Command* command, DeviceMemory<bool> index,\n+      const Command* command, DeviceAddress<bool> index,\n       std::vector<UpdateCommands> update_branches) = 0;\n \n   // Creates a conditional operation that will execute a command buffer\n@@ -275,13 +275,13 @@ class CommandBuffer {\n   //     cond_builder()\n   //\n   virtual absl::StatusOr<const Command*> CreateWhile(\n-      DeviceMemory<bool> pred, CreateCommands create_cond,\n+      DeviceAddress<bool> pred, CreateCommands create_cond,\n       CreateCommands create_body,\n       absl::Span<const Command* const> dependencies) = 0;\n \n   // Updates a While command.\n   virtual absl::Status UpdateWhile(const Command* command,\n-                                   DeviceMemory<bool> pred,\n+                                   DeviceAddress<bool> pred,\n                                    UpdateCommands update_cond,\n                                    UpdateCommands update_body) = 0;\n "
        },
        {
            "sha": "24a305d31a76ce278f5cbad3d8ae2ec4e0072855",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -276,8 +276,8 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:event_based_timer\",\n         \"//xla/stream_executor:plugin_registry\",\n@@ -349,7 +349,7 @@ cc_library(\n         \":cuda_helpers\",\n         \":cuda_platform_id\",\n         \"//xla/stream_executor:activate_context\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:fft\",\n         \"//xla/stream_executor:plugin_registry\",\n         \"//xla/stream_executor:scratch_allocator\",\n@@ -420,7 +420,7 @@ xla_test(\n     deps = [\n         \":buffer_debug_xor_checksum_kernel_cuda\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -474,7 +474,7 @@ xla_test(\n         \"//xla:types\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n         \"//xla/backends/gpu/runtime:thunk_id\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -517,7 +517,7 @@ cc_library(\n         \":cudnn_sdpa_score_mod\",\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:data_type\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:event_based_timer\",\n@@ -1142,8 +1142,8 @@ cc_library(\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:command_buffer\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:event_based_timer\",\n@@ -1211,8 +1211,8 @@ xla_test(\n         \":cuda_executor\",\n         \":cuda_platform\",\n         \":cuda_platform_id\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor:memory_allocation\",\n@@ -1257,7 +1257,7 @@ xla_test(\n         \":cuda_executor\",\n         \":cuda_executor_multigpu_test_kernels\",\n         \":cuda_platform\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -1388,7 +1388,7 @@ cc_library(\n         \":cuda_event\",\n         \":cuda_status\",\n         \"//xla/stream_executor:activate_context\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:event_based_timer\",\n         \"//xla/stream_executor:launch_dim\",\n@@ -1424,7 +1424,7 @@ xla_test(\n         \":cuda_executor\",\n         \":cuda_platform_id\",\n         \":cuda_stream\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n@@ -1480,7 +1480,7 @@ xla_test(\n         \":cuda_executor\",\n         \":cuda_platform_id\",\n         \":cuda_timer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n@@ -1512,7 +1512,7 @@ cc_library(\n         \":cuda_status\",\n         \"//xla/stream_executor:bit_pattern\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n@@ -1550,7 +1550,7 @@ xla_test(\n         \":cudnn_plugin\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:platform\",\n@@ -2365,7 +2365,7 @@ cuda_library(\n     ],\n     deps = [\n         \":cuda_platform_id\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n@@ -2385,7 +2385,7 @@ xla_test(\n         \"//xla:shape_util\",\n         \"//xla:types\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\","
        },
        {
            "sha": "56ec5d18289bedeec4081f1af6ddef82434d8d06",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_float_check_kernel_cuda_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -28,7 +28,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_float_check_kernel.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -73,8 +73,8 @@ class FloatCheckKernelTest : public ::testing::Test {\n   }\n \n   template <typename T>\n-  absl::StatusOr<se::DeviceMemory<T>> CheckNotNull(\n-      se::DeviceMemory<T> device_memory, absl::string_view name) {\n+  absl::StatusOr<se::DeviceAddress<T>> CheckNotNull(\n+      se::DeviceAddress<T> device_memory, absl::string_view name) {\n     if (device_memory.is_null()) {\n       return absl::InternalError(\n           absl::StrFormat(\"Device memory for %s is null\", name));\n@@ -94,7 +94,7 @@ class FloatCheckKernelTest : public ::testing::Test {\n \n     // Setup device buffers\n     TF_ASSIGN_OR_RETURN(\n-        se::DeviceMemory<InputType> device_input,\n+        se::DeviceAddress<InputType> device_input,\n         CheckNotNull(executor_->AllocateArray<InputType>(input.size()),\n                      \"input\"));\n     auto cleanup_input =\n@@ -121,7 +121,7 @@ class FloatCheckKernelTest : public ::testing::Test {\n };\n \n TEST_F(FloatCheckKernelTest, ChecksFloatsForF32) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   std::vector<float> input(1024, 1.0f);\n   input[100] = std::numeric_limits<float>::quiet_NaN();\n   input[200] = std::numeric_limits<float>::quiet_NaN();\n@@ -153,7 +153,7 @@ TEST_F(FloatCheckKernelTest, ChecksFloatsForBf16) {\n   input[50] = xla::bfloat16(std::numeric_limits<float>::infinity());\n   input[60] = xla::bfloat16(std::numeric_limits<float>::infinity());\n \n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto device_log,\n       se::gpu::BufferDebugLog<BufferDebugFloatCheckEntry>::CreateOnDevice(\n@@ -170,7 +170,7 @@ TEST_F(FloatCheckKernelTest, ChecksFloatsForBf16) {\n }\n \n TEST_F(FloatCheckKernelTest, ChecksFloatsInParallel) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   std::vector<float> input(1024, 1.0f);\n   input[100] = std::numeric_limits<float>::quiet_NaN();\n   input[200] = std::numeric_limits<float>::quiet_NaN();"
        },
        {
            "sha": "26638e74e65d8e747cc544180de36c9f8c4058f4",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_xor_checksum_kernel_cuda_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_xor_checksum_kernel_cuda_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -71,8 +71,8 @@ class ChecksumKernelTest : public ::testing::Test {\n   }\n \n   template <typename T>\n-  absl::StatusOr<se::DeviceMemory<T>> CheckNotNull(\n-      se::DeviceMemory<T> device_memory, absl::string_view name) {\n+  absl::StatusOr<se::DeviceAddress<T>> CheckNotNull(\n+      se::DeviceAddress<T> device_memory, absl::string_view name) {\n     if (device_memory.is_null()) {\n       return absl::InternalError(\n           absl::StrFormat(\"Device memory for %s is null\", name));\n@@ -93,7 +93,7 @@ class ChecksumKernelTest : public ::testing::Test {\n         registry.LoadKernel<gpu::BufferDebugXorChecksumKernel>(executor_));\n \n     // Setup device buffers\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemory<uint8_t> device_input,\n+    TF_ASSIGN_OR_RETURN(se::DeviceAddress<uint8_t> device_input,\n                         CheckNotNull(executor_->AllocateArray<uint8_t>(\n                                          input.size() * sizeof(input[0])),\n                                      \"input\"));\n@@ -121,7 +121,7 @@ class ChecksumKernelTest : public ::testing::Test {\n };\n \n TEST_F(ChecksumKernelTest, ComputesCorrectChecksumForMultipleOf32Bit) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   std::vector<uint8_t> input = std::vector<uint8_t>(1024, 0x55);\n   // Xor with the expected checksum value.\n   // Assumes the device uses little-endian byte order.\n@@ -146,7 +146,7 @@ TEST_F(ChecksumKernelTest, ComputesCorrectChecksumForMultipleOf32Bit) {\n \n TEST_F(ChecksumKernelTest,\n        PadsMostSignifantBitsOfIncomplete32BitInputWordWithZeros) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   const std::vector<uint8_t> kInput = std::vector<uint8_t>(1023, 0x55);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto device_log,\n@@ -163,7 +163,7 @@ TEST_F(ChecksumKernelTest,\n }\n \n TEST_F(ChecksumKernelTest, ComputesCorrectChecksumInParallel) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   std::vector<uint32_t> input =\n       std::vector<uint32_t>(64 * 1024 / sizeof(uint32_t), 0x55aa55aa);\n   // Xor with the expected checksum value.\n@@ -183,7 +183,7 @@ TEST_F(ChecksumKernelTest, ComputesCorrectChecksumInParallel) {\n }\n \n TEST_F(ChecksumKernelTest, ComputesCorrectChecksumInParallelWithMaxThreads) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   std::vector<uint32_t> input =\n       std::vector<uint32_t>(64 * 1024 / sizeof(uint32_t), 0x55aa55aa);\n   // Xor with the expected checksum value.\n@@ -203,7 +203,7 @@ TEST_F(ChecksumKernelTest, ComputesCorrectChecksumInParallelWithMaxThreads) {\n }\n \n TEST_F(ChecksumKernelTest, AppendsChecksumsToLog) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n   constexpr std::array<uint32_t, 1> kInput123 = {0x01230123};\n   constexpr std::array<uint32_t, 1> kInput456 = {0x04560456};\n   constexpr std::array<uint32_t, 1> kInput789 = {0x07890789};\n@@ -230,7 +230,7 @@ TEST_F(ChecksumKernelTest, AppendsChecksumsToLog) {\n }\n \n TEST_F(ChecksumKernelTest, DiscardsOverflowingChecksums) {\n-  se::DeviceMemory<uint8_t> mem = executor_->AllocateArray<uint8_t>(\n+  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(\n       sizeof(BufferDebugLogHeader) + sizeof(BufferDebugLogEntry) * 2);\n   constexpr std::array<uint32_t, 1> kInput123 = {0x01230123};\n   constexpr std::array<uint32_t, 1> kInput456 = {0x04560456};"
        },
        {
            "sha": "d85c55114372c4b6155bceb63c09e6ca6ad0993f",
            "filename": "third_party/xla/xla/stream_executor/cuda/cub_prefix_sum_kernel_cuda_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/primitive_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/gpu/prefix_sum_kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -67,8 +67,8 @@ class CubPrefixSumKernelCudaTest\n   }\n \n   template <typename T>\n-  absl::StatusOr<se::DeviceMemory<T>> CheckNotNull(\n-      se::DeviceMemory<T> device_memory, absl::string_view name) {\n+  absl::StatusOr<se::DeviceAddress<T>> CheckNotNull(\n+      se::DeviceAddress<T> device_memory, absl::string_view name) {\n     if (device_memory.is_null()) {\n       return absl::InternalError(\n           absl::StrFormat(\"Device memory for %s is null\", name));\n@@ -87,9 +87,9 @@ class CubPrefixSumKernelCudaTest\n \n     // Setup device buffers\n     TF_ASSIGN_OR_RETURN(\n-        se::DeviceMemory<T> device_input,\n+        se::DeviceAddress<T> device_input,\n         CheckNotNull(executor_->AllocateArray<T>(input.size()), \"input\"));\n-    se::DeviceMemory<T> device_output;\n+    se::DeviceAddress<T> device_output;\n     if (in_place) {\n       device_output = device_input;\n     } else {"
        },
        {
            "sha": "cf946b68cae9153d250eae4b5ee95749a39647b3",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_blas.cc",
            "status": "modified",
            "additions": 102,
            "deletions": 99,
            "changes": 201,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -47,7 +47,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_helpers.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/gpu/gpu_helpers.h\"\n@@ -420,77 +420,78 @@ absl::Status CUDABlas::DoBlasInternalImpl(FuncT cublas_func, Stream *stream,\n     return func(std::forward<decltype(args)>(args)...);            \\\n   }\n \n-bool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, float alpha,\n-                          DeviceMemory<float> *x, int incx) {\n+bool CUDABlas::DoBlasScal(Stream* stream, uint64_t elem_count, float alpha,\n+                          DeviceAddress<float>* x, int incx) {\n   return DoBlasInternal(cublasSscal, stream, true /* = pointer_mode_host */,\n                         elem_count, &alpha, GpuMemoryMutable(x), incx);\n }\n \n-bool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, double alpha,\n-                          DeviceMemory<double> *x, int incx) {\n+bool CUDABlas::DoBlasScal(Stream* stream, uint64_t elem_count, double alpha,\n+                          DeviceAddress<double>* x, int incx) {\n   return DoBlasInternal(cublasDscal, stream, true /* = pointer_mode_host */,\n                         elem_count, &alpha, GpuMemoryMutable(x), incx);\n }\n \n-bool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, float alpha,\n-                          DeviceMemory<std::complex<float>> *x, int incx) {\n+bool CUDABlas::DoBlasScal(Stream* stream, uint64_t elem_count, float alpha,\n+                          DeviceAddress<std::complex<float>>* x, int incx) {\n   return DoBlasInternal(cublasCsscal, stream, true /* = pointer_mode_host */,\n                         elem_count, &alpha, CUDAComplex(GpuMemoryMutable(x)),\n                         incx);\n }\n \n-bool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, double alpha,\n-                          DeviceMemory<std::complex<double>> *x, int incx) {\n+bool CUDABlas::DoBlasScal(Stream* stream, uint64_t elem_count, double alpha,\n+                          DeviceAddress<std::complex<double>>* x, int incx) {\n   return DoBlasInternal(cublasZdscal, stream, true /* = pointer_mode_host */,\n                         elem_count, &alpha, CUDAComplex(GpuMemoryMutable(x)),\n                         incx);\n }\n \n-bool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count,\n+bool CUDABlas::DoBlasScal(Stream* stream, uint64_t elem_count,\n                           std::complex<float> alpha,\n-                          DeviceMemory<std::complex<float>> *x, int incx) {\n+                          DeviceAddress<std::complex<float>>* x, int incx) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   return DoBlasInternal(cublasCscal, stream, true /* = pointer_mode_host */,\n                         elem_count, CUDAComplex(&cb_alpha),\n                         CUDAComplex(GpuMemoryMutable(x)), incx);\n }\n \n-bool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count,\n+bool CUDABlas::DoBlasScal(Stream* stream, uint64_t elem_count,\n                           std::complex<double> alpha,\n-                          DeviceMemory<std::complex<double>> *x, int incx) {\n+                          DeviceAddress<std::complex<double>>* x, int incx) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   return DoBlasInternal(cublasZscal, stream, true /* = pointer_mode_host */,\n                         elem_count, CUDAComplex(&cb_alpha),\n                         CUDAComplex(GpuMemoryMutable(x)), incx);\n }\n \n-bool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n-                          uint64_t n, float alpha, const DeviceMemory<float> &a,\n-                          int lda, const DeviceMemory<float> &x, int incx,\n-                          float beta, DeviceMemory<float> *y, int incy) {\n+bool CUDABlas::DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n+                          uint64_t n, float alpha,\n+                          const DeviceAddress<float>& a, int lda,\n+                          const DeviceAddress<float>& x, int incx, float beta,\n+                          DeviceAddress<float>* y, int incy) {\n   return DoBlasInternal(cublasSgemv, stream, true /* = pointer_mode_host */,\n                         AsCublasOperation(trans), m, n, &alpha, GpuMemory(a),\n                         lda, GpuMemory(x), incx, &beta, GpuMemoryMutable(y),\n                         incy);\n }\n \n-bool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n+bool CUDABlas::DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n                           uint64_t n, double alpha,\n-                          const DeviceMemory<double> &a, int lda,\n-                          const DeviceMemory<double> &x, int incx, double beta,\n-                          DeviceMemory<double> *y, int incy) {\n+                          const DeviceAddress<double>& a, int lda,\n+                          const DeviceAddress<double>& x, int incx, double beta,\n+                          DeviceAddress<double>* y, int incy) {\n   return DoBlasInternal(cublasDgemv, stream, true /* = pointer_mode_host */,\n                         AsCublasOperation(trans), m, n, &alpha, GpuMemory(a),\n                         lda, GpuMemory(x), incx, &beta, GpuMemoryMutable(y),\n                         incy);\n }\n \n-bool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n+bool CUDABlas::DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n                           uint64_t n, std::complex<float> alpha,\n-                          const DeviceMemory<std::complex<float>> &a, int lda,\n-                          const DeviceMemory<std::complex<float>> &x, int incx,\n+                          const DeviceAddress<std::complex<float>>& a, int lda,\n+                          const DeviceAddress<std::complex<float>>& x, int incx,\n                           std::complex<float> beta,\n-                          DeviceMemory<std::complex<float>> *y, int incy) {\n+                          DeviceAddress<std::complex<float>>* y, int incy) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   auto cb_beta = CUDAComplexValue(beta);\n   return DoBlasInternal(cublasCgemv, stream, true /* = pointer_mode_host */,\n@@ -500,12 +501,12 @@ bool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n                         CUDAComplex(GpuMemoryMutable(y)), incy);\n }\n \n-bool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n+bool CUDABlas::DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m,\n                           uint64_t n, std::complex<double> alpha,\n-                          const DeviceMemory<std::complex<double>> &a, int lda,\n-                          const DeviceMemory<std::complex<double>> &x, int incx,\n-                          std::complex<double> beta,\n-                          DeviceMemory<std::complex<double>> *y, int incy) {\n+                          const DeviceAddress<std::complex<double>>& a, int lda,\n+                          const DeviceAddress<std::complex<double>>& x,\n+                          int incx, std::complex<double> beta,\n+                          DeviceAddress<std::complex<double>>* y, int incy) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   auto cb_beta = CUDAComplexValue(beta);\n   return DoBlasInternal(cublasZgemv, stream, true /* = pointer_mode_host */,\n@@ -518,9 +519,9 @@ bool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n absl::Status CUDABlas::DoBlasGemm(Stream* stream, blas::Transpose transa,\n                                   blas::Transpose transb, uint64_t m,\n                                   uint64_t n, uint64_t k, blas::DataType dtype,\n-                                  const void* alpha, const DeviceMemoryBase& a,\n-                                  int lda, const DeviceMemoryBase& b, int ldb,\n-                                  const void* beta, DeviceMemoryBase* c,\n+                                  const void* alpha, const DeviceAddressBase& a,\n+                                  int lda, const DeviceAddressBase& b, int ldb,\n+                                  const void* beta, DeviceAddressBase* c,\n                                   int ldc, const EngineOptions& engine_options,\n                                   blas::CallContext context) {\n   cublasMath_t math_type = CUBLAS_DEFAULT_MATH;\n@@ -712,9 +713,9 @@ static absl::Status PopulateProfileFromTimer(\n \n absl::Status CUDABlas::DoBlasGemmWithAlgorithm(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, const void* alpha, const DeviceMemoryBase& a,\n-    blas::DataType type_a, int lda, const DeviceMemoryBase& b,\n-    blas::DataType type_b, int ldb, const void* beta, DeviceMemoryBase* c,\n+    uint64_t n, uint64_t k, const void* alpha, const DeviceAddressBase& a,\n+    blas::DataType type_a, int lda, const DeviceAddressBase& b,\n+    blas::DataType type_b, int ldb, const void* beta, DeviceAddressBase* c,\n     blas::DataType type_c, int ldc, blas::ComputationType computation_type,\n     blas::AlgorithmType algorithm, const EngineOptions& engine_options,\n     blas::ProfileResult* output_profile_result, blas::CallContext context) {\n@@ -747,12 +748,13 @@ absl::Status CUDABlas::DoBlasGemmWithAlgorithm(\n \n absl::Status CUDABlas::DoBlasGemmStridedBatchedWithAlgorithm(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, const void* alpha, const DeviceMemoryBase& a,\n-    blas::DataType type_a, int lda, int64_t stride_a, const DeviceMemoryBase& b,\n-    blas::DataType type_b, int ldb, int64_t stride_b, const void* beta,\n-    DeviceMemoryBase* c, blas::DataType type_c, int ldc, int64_t stride_c,\n-    int batch_count, blas::ComputationType computation_type,\n-    blas::AlgorithmType algorithm, const EngineOptions& engine_options,\n+    uint64_t n, uint64_t k, const void* alpha, const DeviceAddressBase& a,\n+    blas::DataType type_a, int lda, int64_t stride_a,\n+    const DeviceAddressBase& b, blas::DataType type_b, int ldb,\n+    int64_t stride_b, const void* beta, DeviceAddressBase* c,\n+    blas::DataType type_c, int ldc, int64_t stride_c, int batch_count,\n+    blas::ComputationType computation_type, blas::AlgorithmType algorithm,\n+    const EngineOptions& engine_options,\n     blas::ProfileResult* output_profile_result, blas::CallContext context) {\n   TF_ASSIGN_OR_RETURN(\n       cublasMath_t math_type,\n@@ -916,9 +918,9 @@ template <typename T, typename Scalar, typename FuncT>\n absl::Status CUDABlas::DoBlasGemmBatchedInternal(\n     FuncT cublas_func, Stream* stream, blas::Transpose transa,\n     blas::Transpose transb, uint64_t m, uint64_t n, uint64_t k, Scalar alpha,\n-    const DeviceMemorySlice<T>& a_ptrs_to_wrappers, int lda,\n-    const DeviceMemorySlice<T>& b_ptrs_to_wrappers, int ldb, Scalar beta,\n-    const DeviceMemorySlice<T>& c_ptrs_to_wrappers, int ldc, int batch_count,\n+    const DeviceAddressSlice<T>& a_ptrs_to_wrappers, int lda,\n+    const DeviceAddressSlice<T>& b_ptrs_to_wrappers, int ldb, Scalar beta,\n+    const DeviceAddressSlice<T>& c_ptrs_to_wrappers, int ldc, int batch_count,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator) {\n   std::vector<T *> a_raw_ptrs, b_raw_ptrs, c_raw_ptrs;\n   for (int i = 0; i < batch_count; ++i) {\n@@ -934,15 +936,15 @@ absl::Status CUDABlas::DoBlasGemmBatchedInternal(\n   if (scratch_allocator == nullptr) {\n     return absl::InternalError(\"scratch_allocator is null\");\n   }\n-  TF_ASSIGN_OR_RETURN(DeviceMemory<uint8_t> a_bytes,\n+  TF_ASSIGN_OR_RETURN(DeviceAddress<uint8_t> a_bytes,\n                       scratch_allocator->AllocateBytes(size));\n-  TF_ASSIGN_OR_RETURN(DeviceMemory<uint8_t> b_bytes,\n+  TF_ASSIGN_OR_RETURN(DeviceAddress<uint8_t> b_bytes,\n                       scratch_allocator->AllocateBytes(size));\n-  TF_ASSIGN_OR_RETURN(DeviceMemory<uint8_t> c_bytes,\n+  TF_ASSIGN_OR_RETURN(DeviceAddress<uint8_t> c_bytes,\n                       scratch_allocator->AllocateBytes(size));\n-  DeviceMemory<CUDA_T *> a(a_bytes);\n-  DeviceMemory<CUDA_T *> b(b_bytes);\n-  DeviceMemory<CUDA_T *> c(c_bytes);\n+  DeviceAddress<CUDA_T*> a(a_bytes);\n+  DeviceAddress<CUDA_T*> b(b_bytes);\n+  DeviceAddress<CUDA_T*> c(c_bytes);\n \n   TF_RETURN_IF_ERROR(stream->Memcpy(&a, a_raw_ptrs.data(), size));\n   TF_RETURN_IF_ERROR(stream->Memcpy(&b, b_raw_ptrs.data(), size));\n@@ -1012,9 +1014,9 @@ absl::Status CUDABlas::DoBlasGemmBatchedInternal(\n   } else {\n     // Fall back to a loop for fp16\n     for (int b = 0; b < batch_count; ++b) {\n-      const DeviceMemory<T> &a_matrix = *a_ptrs_to_wrappers[b];\n-      const DeviceMemory<T> &b_matrix = *b_ptrs_to_wrappers[b];\n-      DeviceMemory<T> *c_matrix = c_ptrs_to_wrappers[b];\n+      const DeviceAddress<T>& a_matrix = *a_ptrs_to_wrappers[b];\n+      const DeviceAddress<T>& b_matrix = *b_ptrs_to_wrappers[b];\n+      DeviceAddress<T>* c_matrix = c_ptrs_to_wrappers[b];\n       TF_RETURN_IF_ERROR(DoBlasGemm(\n           stream, transa, transb, m, n, k, blas::ToDataType<T>::value, &alpha,\n           a_matrix, lda, b_matrix, ldb, &beta, c_matrix, ldc, engine_options,\n@@ -1026,9 +1028,10 @@ absl::Status CUDABlas::DoBlasGemmBatchedInternal(\n \n bool CUDABlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, float alpha, DeviceMemorySlice<Eigen::half> a_array,\n-    int lda, DeviceMemorySlice<Eigen::half> b_array, int ldb, float beta,\n-    DeviceMemorySlice<Eigen::half> c_array, int ldc, int batch_count,\n+    uint64_t n, uint64_t k, float alpha,\n+    DeviceAddressSlice<Eigen::half> a_array, int lda,\n+    DeviceAddressSlice<Eigen::half> b_array, int ldb, float beta,\n+    DeviceAddressSlice<Eigen::half> c_array, int ldc, int batch_count,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n     blas::CallContext context) {\n   // Note: The func passed here (cublasSgemmBatched) is not actually called,\n@@ -1046,9 +1049,9 @@ bool CUDABlas::DoBlasGemmBatched(\n bool CUDABlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n     uint64_t n, uint64_t k, float alpha,\n-    DeviceMemorySlice<Eigen::bfloat16> a_array, int lda,\n-    DeviceMemorySlice<Eigen::bfloat16> b_array, int ldb, float beta,\n-    DeviceMemorySlice<Eigen::bfloat16> c_array, int ldc, int batch_count,\n+    DeviceAddressSlice<Eigen::bfloat16> a_array, int lda,\n+    DeviceAddressSlice<Eigen::bfloat16> b_array, int ldb, float beta,\n+    DeviceAddressSlice<Eigen::bfloat16> c_array, int ldc, int batch_count,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n     blas::CallContext context) {\n   // Note: The func passed here (cublasSgemmBatched) is not actually called,\n@@ -1065,9 +1068,9 @@ bool CUDABlas::DoBlasGemmBatched(\n \n bool CUDABlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, float alpha, DeviceMemorySlice<float> a_array,\n-    int lda, DeviceMemorySlice<float> b_array, int ldb, float beta,\n-    DeviceMemorySlice<float> c_array, int ldc, int batch_count,\n+    uint64_t n, uint64_t k, float alpha, DeviceAddressSlice<float> a_array,\n+    int lda, DeviceAddressSlice<float> b_array, int ldb, float beta,\n+    DeviceAddressSlice<float> c_array, int ldc, int batch_count,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n     blas::CallContext context) {\n   absl::Status status = DoBlasGemmBatchedInternal(\n@@ -1082,9 +1085,9 @@ bool CUDABlas::DoBlasGemmBatched(\n \n bool CUDABlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, double alpha, DeviceMemorySlice<double> a_array,\n-    int lda, DeviceMemorySlice<double> b_array, int ldb, double beta,\n-    DeviceMemorySlice<double> c_array, int ldc, int batch_count,\n+    uint64_t n, uint64_t k, double alpha, DeviceAddressSlice<double> a_array,\n+    int lda, DeviceAddressSlice<double> b_array, int ldb, double beta,\n+    DeviceAddressSlice<double> c_array, int ldc, int batch_count,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n     blas::CallContext context) {\n   absl::Status status = DoBlasGemmBatchedInternal(\n@@ -1101,9 +1104,9 @@ bool CUDABlas::DoBlasGemmBatched(\n bool CUDABlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n     uint64_t n, uint64_t k, std::complex<float> alpha,\n-    DeviceMemorySlice<std::complex<float>> a_array, int lda,\n-    DeviceMemorySlice<std::complex<float>> b_array, int ldb,\n-    std::complex<float> beta, DeviceMemorySlice<std::complex<float>> c_array,\n+    DeviceAddressSlice<std::complex<float>> a_array, int lda,\n+    DeviceAddressSlice<std::complex<float>> b_array, int ldb,\n+    std::complex<float> beta, DeviceAddressSlice<std::complex<float>> c_array,\n     int ldc, int batch_count, const EngineOptions& engine_options,\n     ScratchAllocator* scratch_allocator, blas::CallContext context) {\n   absl::Status status = DoBlasGemmBatchedInternal(\n@@ -1120,9 +1123,9 @@ bool CUDABlas::DoBlasGemmBatched(\n bool CUDABlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n     uint64_t n, uint64_t k, std::complex<double> alpha,\n-    DeviceMemorySlice<std::complex<double>> a_array, int lda,\n-    DeviceMemorySlice<std::complex<double>> b_array, int ldb,\n-    std::complex<double> beta, DeviceMemorySlice<std::complex<double>> c_array,\n+    DeviceAddressSlice<std::complex<double>> a_array, int lda,\n+    DeviceAddressSlice<std::complex<double>> b_array, int ldb,\n+    std::complex<double> beta, DeviceAddressSlice<std::complex<double>> c_array,\n     int ldc, int batch_count, const EngineOptions& engine_options,\n     ScratchAllocator* scratch_allocator, blas::CallContext context) {\n   absl::Status status = DoBlasGemmBatchedInternal(\n@@ -1138,9 +1141,9 @@ bool CUDABlas::DoBlasGemmBatched(\n absl::Status CUDABlas::DoBlasGemmStridedBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n     uint64_t n, uint64_t k, blas::DataType dtype, const void* alpha,\n-    const DeviceMemoryBase& a, int lda, int64_t stride_a,\n-    const DeviceMemoryBase& b, int ldb, int64_t stride_b, const void* beta,\n-    DeviceMemoryBase* c, int ldc, int64_t stride_c, int batch_count,\n+    const DeviceAddressBase& a, int lda, int64_t stride_a,\n+    const DeviceAddressBase& b, int ldb, int64_t stride_b, const void* beta,\n+    DeviceAddressBase* c, int ldc, int64_t stride_c, int batch_count,\n     const EngineOptions& engine_options, blas::CallContext context) {\n   cublasMath_t math_type = CUBLAS_DEFAULT_MATH;\n #if CUDA_VERSION < 11000\n@@ -1273,34 +1276,34 @@ absl::Status CUDABlas::DoBlasGemmStridedBatched(\n   }\n }\n \n-bool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n-                          float alpha, const DeviceMemory<float> &a, int lda,\n-                          DeviceMemory<float> *b, int ldb) {\n+                          float alpha, const DeviceAddress<float>& a, int lda,\n+                          DeviceAddress<float>* b, int ldb) {\n   return DoBlasInternal(cublasStrsm, stream, true /* = pointer_mode_host */,\n                         CUDABlasSide(side), CUDABlasUpperLower(uplo),\n                         AsCublasOperation(transa), CUDABlasDiagonal(diag), m, n,\n                         &alpha, GpuMemory(a), lda, GpuMemoryMutable(b), ldb);\n }\n \n-bool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n-                          double alpha, const DeviceMemory<double> &a, int lda,\n-                          DeviceMemory<double> *b, int ldb) {\n+                          double alpha, const DeviceAddress<double>& a, int lda,\n+                          DeviceAddress<double>* b, int ldb) {\n   return DoBlasInternal(cublasDtrsm, stream, true /* = pointer_mode_host */,\n                         CUDABlasSide(side), CUDABlasUpperLower(uplo),\n                         AsCublasOperation(transa), CUDABlasDiagonal(diag), m, n,\n                         &alpha, GpuMemory(a), lda, GpuMemoryMutable(b), ldb);\n }\n \n-bool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n                           std::complex<float> alpha,\n-                          const DeviceMemory<std::complex<float>> &a, int lda,\n-                          DeviceMemory<std::complex<float>> *b, int ldb) {\n+                          const DeviceAddress<std::complex<float>>& a, int lda,\n+                          DeviceAddress<std::complex<float>>* b, int ldb) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   return DoBlasInternal(cublasCtrsm, stream, true /* = pointer_mode_host */,\n                         CUDABlasSide(side), CUDABlasUpperLower(uplo),\n@@ -1309,12 +1312,12 @@ bool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n                         CUDAComplex(GpuMemoryMutable(b)), ldb);\n }\n \n-bool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsm(Stream* stream, blas::Side side,\n                           blas::UpperLower uplo, blas::Transpose transa,\n                           blas::Diagonal diag, uint64_t m, uint64_t n,\n                           std::complex<double> alpha,\n-                          const DeviceMemory<std::complex<double>> &a, int lda,\n-                          DeviceMemory<std::complex<double>> *b, int ldb) {\n+                          const DeviceAddress<std::complex<double>>& a, int lda,\n+                          DeviceAddress<std::complex<double>>* b, int ldb) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   return DoBlasInternal(cublasZtrsm, stream, true /* = pointer_mode_host */,\n                         CUDABlasSide(side), CUDABlasUpperLower(uplo),\n@@ -1323,11 +1326,11 @@ bool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n                         CUDAComplex(GpuMemoryMutable(b)), ldb);\n }\n \n-bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n-                                 float alpha, const DeviceMemory<float *> &as,\n-                                 int lda, DeviceMemory<float *> *bs, int ldb,\n+                                 float alpha, const DeviceAddress<float*>& as,\n+                                 int lda, DeviceAddress<float*>* bs, int ldb,\n                                  int batch_count) {\n   return DoBlasInternal(cublasStrsmBatched, stream,\n                         true /* = pointer_mode_host */, CUDABlasSide(side),\n@@ -1336,11 +1339,11 @@ bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n                         lda, GpuMemoryMutable(bs), ldb, batch_count);\n }\n \n-bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n-                                 double alpha, const DeviceMemory<double *> &as,\n-                                 int lda, DeviceMemory<double *> *bs, int ldb,\n+                                 double alpha, const DeviceAddress<double*>& as,\n+                                 int lda, DeviceAddress<double*>* bs, int ldb,\n                                  int batch_count) {\n   return DoBlasInternal(cublasDtrsmBatched, stream,\n                         true /* = pointer_mode_host */, CUDABlasSide(side),\n@@ -1349,13 +1352,13 @@ bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n                         lda, GpuMemoryMutable(bs), ldb, batch_count);\n }\n \n-bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n                                  std::complex<float> alpha,\n-                                 const DeviceMemory<std::complex<float> *> &as,\n+                                 const DeviceAddress<std::complex<float>*>& as,\n                                  int lda,\n-                                 DeviceMemory<std::complex<float> *> *bs,\n+                                 DeviceAddress<std::complex<float>*>* bs,\n                                  int ldb, int batch_count) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   return DoBlasInternal(\n@@ -1366,13 +1369,13 @@ bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n       reinterpret_cast<float2 **>(GpuMemoryMutable(bs)), ldb, batch_count);\n }\n \n-bool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n+bool CUDABlas::DoBlasTrsmBatched(Stream* stream, blas::Side side,\n                                  blas::UpperLower uplo, blas::Transpose transa,\n                                  blas::Diagonal diag, uint64_t m, uint64_t n,\n                                  std::complex<double> alpha,\n-                                 const DeviceMemory<std::complex<double> *> &as,\n+                                 const DeviceAddress<std::complex<double>*>& as,\n                                  int lda,\n-                                 DeviceMemory<std::complex<double> *> *bs,\n+                                 DeviceAddress<std::complex<double>*>* bs,\n                                  int ldb, int batch_count) {\n   auto cb_alpha = CUDAComplexValue(alpha);\n   return DoBlasInternal("
        },
        {
            "sha": "77e6a35504f96d4163f9951220d1502c094527b3",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_blas.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -99,9 +99,9 @@ class CUDABlas : public blas::BlasSupport {\n   absl::Status DoBlasGemmBatchedInternal(\n       FuncT cublas_func, Stream* stream, blas::Transpose transa,\n       blas::Transpose transb, uint64_t m, uint64_t n, uint64_t k, Scalar alpha,\n-      const DeviceMemorySlice<T>& a_array, int lda,\n-      const DeviceMemorySlice<T>& b_array, int ldb, Scalar beta,\n-      const DeviceMemorySlice<T>& c_array, int ldc, int batch_count,\n+      const DeviceAddressSlice<T>& a_array, int lda,\n+      const DeviceAddressSlice<T>& b_array, int ldb, Scalar beta,\n+      const DeviceAddressSlice<T>& c_array, int ldc, int batch_count,\n       const EngineOptions& engine_options, ScratchAllocator* scratch_allocator);\n \n   // Guards the cuBLAS handle for this device."
        },
        {
            "sha": "831a7424404b93ce3ed229d42ee2c115174004bc",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_blas_lt.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas_lt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas_lt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_blas_lt.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/cuda/cuda_blas_utils.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/gpu/gpu_helpers.h\"\n@@ -356,8 +356,8 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n     return absl::InternalError(\n         \"Algorithm must be set before calling DoMatMul!\");\n   }\n-  DeviceMemoryBase a = args.a, b = args.b;\n-  DeviceMemoryBase a_scale = args.a_scale, b_scale = args.b_scale;\n+  DeviceAddressBase a = args.a, b = args.b;\n+  DeviceAddressBase a_scale = args.a_scale, b_scale = args.b_scale;\n   if (must_swap_operands_) {\n     std::swap(a, b);\n     std::swap(a_scale, b_scale);\n@@ -377,7 +377,7 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n   if (workspace_size > 0) {\n     if (args.scratch_allocator != nullptr) {\n       TF_ASSIGN_OR_RETURN(\n-          DeviceMemory<uint8_t> alloc,\n+          DeviceAddress<uint8_t> alloc,\n           args.scratch_allocator->AllocateBytes(workspace_size));\n       workspace_addr = gpu::GpuMemoryMutable(&alloc);\n     } else {"
        },
        {
            "sha": "3047d033059a1db386749c65612661cf4514842e",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_command_buffer.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -38,7 +38,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_context.h\"\n #include \"xla/stream_executor/cuda/cuda_kernel.h\"\n #include \"xla/stream_executor/cuda/cuda_status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/gpu/gpu_command_buffer.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -65,7 +65,7 @@ absl::StatusOr<CUgraph> CreateGraph() {\n   return graph;\n }\n \n-CUdeviceptr AsDevicePtr(const DeviceMemoryBase& mem) {\n+CUdeviceptr AsDevicePtr(const DeviceAddressBase& mem) {\n   return absl::bit_cast<CUdeviceptr>(mem.opaque());\n }\n \n@@ -144,7 +144,7 @@ absl::StatusOr<std::unique_ptr<CudaCommandBuffer>> CudaCommandBuffer::Create(\n //===----------------------------------------------------------------------===//\n \n absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateSetWhileConditionNode(\n-    GraphConditionalHandle conditional, DeviceMemory<bool> predicate,\n+    GraphConditionalHandle conditional, DeviceAddress<bool> predicate,\n     absl::Span<const GraphNodeHandle> dependencies) {\n   if (!set_while_condition_kernel_) {\n     TF_ASSIGN_OR_RETURN(auto spec,\n@@ -163,7 +163,7 @@ absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateSetWhileConditionNode(\n \n absl::Status CudaCommandBuffer::UpdateSetWhileConditionNode(\n     GraphNodeHandle handle, GraphConditionalHandle conditional,\n-    DeviceMemory<bool> predicate) {\n+    DeviceAddress<bool> predicate) {\n   auto kernel_args = PackKernelArgs(set_while_condition_kernel_,\n                                     ToCudaGraphHandle(conditional), predicate);\n   return UpdateKernelNode(handle, ThreadDim(), BlockDim(),\n@@ -174,7 +174,7 @@ template <typename... Params>\n static std::unique_ptr<KernelArgsPackedArrayBase> PackCaseConditionKernelArgs(\n     const TypedKernel<Params...>& kernel,\n     absl::Span<const GraphConditionalHandle> conditionals,\n-    DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+    DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n     bool enable_conditional_default) {\n   constexpr int kCaseBranchBatchSize = 8;\n   CHECK(conditionals.size() <= kCaseBranchBatchSize);\n@@ -196,7 +196,7 @@ static std::unique_ptr<KernelArgsPackedArrayBase> PackCaseConditionKernelArgs(\n \n absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateSetCaseConditionNode(\n     absl::Span<const GraphConditionalHandle> conditionals,\n-    DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+    DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n     bool enable_conditional_default,\n     absl::Span<const GraphNodeHandle> dependencies) {\n   if (!set_case_condition_kernel_) {\n@@ -217,7 +217,7 @@ absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateSetCaseConditionNode(\n absl::Status CudaCommandBuffer::UpdateSetCaseConditionNode(\n     GraphNodeHandle handle,\n     absl::Span<const GraphConditionalHandle> conditionals,\n-    DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+    DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n     bool enable_conditional_default) {\n   auto kernel_args = PackCaseConditionKernelArgs(\n       set_case_condition_kernel_, conditionals, index, index_is_bool,\n@@ -288,7 +288,8 @@ CudaCommandBuffer::CreateConditionalNode(\n \n absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateMemsetNode(\n     absl::Span<const GraphNodeHandle> dependencies,\n-    DeviceMemoryBase destination, BitPattern bit_pattern, size_t num_elements) {\n+    DeviceAddressBase destination, BitPattern bit_pattern,\n+    size_t num_elements) {\n   VLOG(2) << \"Add memset node to a graph \" << graph_\n           << \"; dst: \" << destination.opaque()\n           << \"; bit_pattern: \" << bit_pattern.ToString()\n@@ -316,7 +317,7 @@ absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateMemsetNode(\n }\n \n absl::Status CudaCommandBuffer::UpdateMemsetNode(GraphNodeHandle node_handle,\n-                                                 DeviceMemoryBase destination,\n+                                                 DeviceAddressBase destination,\n                                                  BitPattern bit_pattern,\n                                                  size_t num_elements) {\n   VLOG(2) << \"Set memset node params \" << node_handle << \" in graph executable \"\n@@ -341,7 +342,7 @@ absl::Status CudaCommandBuffer::UpdateMemsetNode(GraphNodeHandle node_handle,\n \n absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateMemcpyD2DNode(\n     absl::Span<const GraphNodeHandle> dependencies,\n-    DeviceMemoryBase destination, DeviceMemoryBase source, uint64_t size) {\n+    DeviceAddressBase destination, DeviceAddressBase source, uint64_t size) {\n   VLOG(2) << \"Add memcpy d2d node to a graph \" << graph_\n           << \"; dst: \" << destination.opaque() << \"; src: \" << source.opaque()\n           << \"; size: \" << size << \"; context: \" << cuda_context_->context()\n@@ -367,8 +368,8 @@ absl::StatusOr<GraphNodeHandle> CudaCommandBuffer::CreateMemcpyD2DNode(\n }\n \n absl::Status CudaCommandBuffer::UpdateMemcpyD2DNode(\n-    GraphNodeHandle node_handle, DeviceMemoryBase destination,\n-    DeviceMemoryBase source, uint64_t size) {\n+    GraphNodeHandle node_handle, DeviceAddressBase destination,\n+    DeviceAddressBase source, uint64_t size) {\n   VLOG(2) << \"Set memcpy d2d node params \" << node_handle\n           << \" in graph executable \" << graph_exec()\n           << \"; dst: \" << destination.opaque() << \"; src: \" << source.opaque()\n@@ -390,14 +391,14 @@ absl::Status CudaCommandBuffer::UpdateMemcpyD2DNode(\n \n absl::Status CudaCommandBuffer::PopulateDnnGraphNode(\n     dnn::DnnGraph& dnn_graph, Stream& stream,\n-    absl::Span<DeviceMemoryBase> operands) {\n+    absl::Span<DeviceAddressBase> operands) {\n   return dnn_graph.PopulateOrUpdateRawCommandBuffer(stream, operands, graph_,\n                                                     false);\n }\n \n absl::Status CudaCommandBuffer::UpdateDnnGraphNode(\n     dnn::DnnGraph& dnn_graph, Stream& stream,\n-    absl::Span<DeviceMemoryBase> operands, GraphNodeHandle node_handle) {\n+    absl::Span<DeviceAddressBase> operands, GraphNodeHandle node_handle) {\n   CUgraph child_graph;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cuGraphChildGraphNodeGetGraph(\n       ToCudaGraphHandle(node_handle), &child_graph)));"
        },
        {
            "sha": "19367f09da5645d80795504c7c57dc48a9adfdc6",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_command_buffer.h",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"xla/stream_executor/bit_pattern.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/cuda/cuda_context.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_command_buffer.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -77,23 +77,23 @@ class CudaCommandBuffer final : public GpuCommandBuffer {\n \n   absl::StatusOr<GraphNodeHandle> CreateSetCaseConditionNode(\n       absl::Span<const GraphConditionalHandle> conditionals,\n-      DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+      DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n       bool enable_conditional_default,\n       absl::Span<const GraphNodeHandle> dependencies) override;\n \n   absl::Status UpdateSetCaseConditionNode(\n       GraphNodeHandle handle,\n       absl::Span<const GraphConditionalHandle> conditionals,\n-      DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+      DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n       bool enable_conditional_default) override;\n \n   absl::StatusOr<GraphNodeHandle> CreateSetWhileConditionNode(\n-      GraphConditionalHandle conditional, DeviceMemory<bool> predicate,\n+      GraphConditionalHandle conditional, DeviceAddress<bool> predicate,\n       absl::Span<const GraphNodeHandle> dependencies) override;\n \n   absl::Status UpdateSetWhileConditionNode(\n       GraphNodeHandle handle, GraphConditionalHandle conditional,\n-      DeviceMemory<bool> predicate) override;\n+      DeviceAddress<bool> predicate) override;\n \n   //===--------------------------------------------------------------------===//\n \n@@ -107,29 +107,29 @@ class CudaCommandBuffer final : public GpuCommandBuffer {\n \n   absl::StatusOr<GraphNodeHandle> CreateMemsetNode(\n       absl::Span<const GraphNodeHandle> dependencies,\n-      DeviceMemoryBase destination, BitPattern bit_pattern,\n+      DeviceAddressBase destination, BitPattern bit_pattern,\n       size_t num_elements) override;\n \n   absl::Status UpdateMemsetNode(GraphNodeHandle node_handle,\n-                                DeviceMemoryBase destination,\n+                                DeviceAddressBase destination,\n                                 BitPattern bit_pattern,\n                                 size_t num_elements) override;\n \n   absl::StatusOr<GraphNodeHandle> CreateMemcpyD2DNode(\n       absl::Span<const GraphNodeHandle> dependencies,\n-      DeviceMemoryBase destination, DeviceMemoryBase source,\n+      DeviceAddressBase destination, DeviceAddressBase source,\n       uint64_t size) override;\n \n   absl::Status UpdateMemcpyD2DNode(GraphNodeHandle node_handle,\n-                                   DeviceMemoryBase destination,\n-                                   DeviceMemoryBase source,\n+                                   DeviceAddressBase destination,\n+                                   DeviceAddressBase source,\n                                    uint64_t size) override;\n \n   absl::Status PopulateDnnGraphNode(\n-      dnn::DnnGraph&, Stream&, absl::Span<DeviceMemoryBase> operands) override;\n+      dnn::DnnGraph&, Stream&, absl::Span<DeviceAddressBase> operands) override;\n \n   absl::Status UpdateDnnGraphNode(dnn::DnnGraph&, Stream&,\n-                                  absl::Span<DeviceMemoryBase> operands,\n+                                  absl::Span<DeviceAddressBase> operands,\n                                   GraphNodeHandle) override;\n \n   absl::StatusOr<GraphNodeHandle> CreateChildNode(\n@@ -184,10 +184,10 @@ class CudaCommandBuffer final : public GpuCommandBuffer {\n                   CUgraphConditionalHandle, CUgraphConditionalHandle,\n                   CUgraphConditionalHandle, CUgraphConditionalHandle,\n                   CUgraphConditionalHandle, CUgraphConditionalHandle,\n-                  DeviceMemory<uint8_t>, bool, int32_t, int32_t, bool>;\n+                  DeviceAddress<uint8_t>, bool, int32_t, int32_t, bool>;\n \n   using SetWhileConditionKernel =\n-      TypedKernel<CUgraphConditionalHandle, DeviceMemory<bool>>;\n+      TypedKernel<CUgraphConditionalHandle, DeviceAddress<bool>>;\n \n   // Lazy loaded auxiliary kernels required for building CUDA graphs (no-op\n   // barriers, updating conditional handles, etc.)."
        },
        {
            "sha": "c30ca5242b337e3f8c162f2a0424e5ac6b09cf96",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_command_buffer_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_dnn.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -101,12 +101,12 @@ TEST(CudaCommandBufferTest, CuDnnExplicitConstructionAndUpdateWork) {\n   EXPECT_THAT(graph.SupportsExplicitCommandBufferConstruction(),\n               absl_testing::IsOkAndHolds(true));\n \n-  DeviceMemory<int8_t> input = executor->AllocateArray<int8_t>(kTotalElements);\n+  DeviceAddress<int8_t> input = executor->AllocateArray<int8_t>(kTotalElements);\n   TF_ASSERT_OK(stream->MemZero(&input, input.size()));\n-  DeviceMemory<int32_t> output0 =\n+  DeviceAddress<int32_t> output0 =\n       executor->AllocateArray<int32_t>(kTotalElements);\n-  DeviceMemoryBase workspace;\n-  std::vector<DeviceMemoryBase> operands;\n+  DeviceAddressBase workspace;\n+  std::vector<DeviceAddressBase> operands;\n   operands.reserve(4);\n   operands.push_back(input);  // multiplying the input by itself\n   operands.push_back(input);\n@@ -120,7 +120,7 @@ TEST(CudaCommandBufferTest, CuDnnExplicitConstructionAndUpdateWork) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto* dnn_command,\n       cmd_buffer->CreateDnnGraphCommand(\n-          graph, *stream, absl::Span<DeviceMemoryBase>(operands), {}));\n+          graph, *stream, absl::Span<DeviceAddressBase>(operands), {}));\n   TF_ASSERT_OK(cmd_buffer->Finalize());\n \n   std::vector<int32_t> host_buffer(output0.ElementCount());\n@@ -140,7 +140,7 @@ TEST(CudaCommandBufferTest, CuDnnExplicitConstructionAndUpdateWork) {\n   EXPECT_THAT(host_buffer, Each(0));\n \n   // Swap the output buffer.\n-  DeviceMemory<int32_t> output1 =\n+  DeviceAddress<int32_t> output1 =\n       executor->AllocateArray<int32_t>(kTotalElements);\n   operands[2] = output1;\n   executor->Deallocate(&output0);\n@@ -154,7 +154,7 @@ TEST(CudaCommandBufferTest, CuDnnExplicitConstructionAndUpdateWork) {\n   // Update the command buffer to write into the new output buffer.\n   TF_ASSERT_OK(cmd_buffer->Update());\n   TF_ASSERT_OK(cmd_buffer->UpdateDnnGraphCommand(\n-      dnn_command, graph, *stream, absl::Span<DeviceMemoryBase>(operands)));\n+      dnn_command, graph, *stream, absl::Span<DeviceAddressBase>(operands)));\n   TF_ASSERT_OK(cmd_buffer->Finalize());\n \n   // Run the computation."
        },
        {
            "sha": "631b8489c22c7eab7acae4689078138d9fc0c5fc",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 233,
            "deletions": 230,
            "changes": 463,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -60,7 +60,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cudnn_frontend_helpers.h\"\n #include \"xla/stream_executor/cuda/cudnn_sdpa_score_mod.h\"\n #include \"xla/stream_executor/data_type.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -1203,7 +1203,7 @@ class CudnnDropoutDescriptor {\n       return CudnnDropoutDescriptor(std::move(handle));\n     }\n \n-    DeviceMemory<uint8_t> state_memory;\n+    DeviceAddress<uint8_t> state_memory;\n     if (state_allocator) {\n       size_t state_sizes_in_bytes = 0;\n       RETURN_IF_CUDNN_ERROR(\n@@ -1770,17 +1770,17 @@ template <class T>\n absl::StatusOr<RnnModelDims> ExtractAndCheckRnnForward(\n     const CudnnRnnDescriptor& rnn_desc,\n     const CudnnRnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<T>& input_data,\n+    const DeviceAddress<T>& input_data,\n     const CudnnRnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<T>& input_h_data,\n+    const DeviceAddress<T>& input_h_data,\n     const CudnnRnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+    const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n     const CudnnRnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<T>& output_data,\n+    const DeviceAddress<T>& output_data,\n     const CudnnRnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<T>& output_h_data,\n+    const DeviceAddress<T>& output_h_data,\n     const CudnnRnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<T>& output_c_data) {\n+    const DeviceAddress<T>& output_c_data) {\n   // extract model parameters\n   RnnModelDims model_dims;\n   model_dims.num_layers = rnn_desc.num_layers();\n@@ -1846,7 +1846,7 @@ absl::Status CreateRnnTempSpace(\n     const CudnnRnnSequenceTensorDescriptor& input_desc,\n     ScratchAllocator* workspace_allocator,\n     ScratchAllocator* reserve_space_allocator, bool is_fwd_training,\n-    DeviceMemory<uint8_t>* workspace, DeviceMemory<uint8_t>* reserve_space) {\n+    DeviceAddress<uint8_t>* workspace, DeviceAddress<uint8_t>* reserve_space) {\n   size_t reserve_space_size_in_bytes = 0;\n   size_t workspace_size_in_bytes = 0;\n   if (input_desc.is_var_seq_lengths()) {\n@@ -1893,7 +1893,7 @@ absl::Status CreateRnnTempSpace(\n   return absl::OkStatus();\n }\n \n-absl::StatusOr<DeviceMemory<uint8_t>> CreateBatchNormForwardWorkspace(\n+absl::StatusOr<DeviceAddress<uint8_t>> CreateBatchNormForwardWorkspace(\n     Stream* stream, const CudnnHandle& cudnn, const cudnnBatchNormMode_t& mode,\n     const cudnnBatchNormOps_t& bn_ops,\n     const cudnnActivationDescriptor_t& activation_desc,\n@@ -1912,12 +1912,12 @@ absl::StatusOr<DeviceMemory<uint8_t>> CreateBatchNormForwardWorkspace(\n           /*sizeInBytes=*/&workspace_size_in_bytes));\n   // Allocate the workspace.\n   if (workspace_size_in_bytes == 0) {\n-    return DeviceMemory<uint8_t>();\n+    return DeviceAddress<uint8_t>();\n   }\n   return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n }\n \n-absl::StatusOr<DeviceMemory<uint8_t>> CreateBatchNormBackwardWorkspace(\n+absl::StatusOr<DeviceAddress<uint8_t>> CreateBatchNormBackwardWorkspace(\n     Stream* stream, const CudnnHandle& cudnn, const cudnnBatchNormMode_t& mode,\n     const cudnnBatchNormOps_t& bn_ops,\n     const cudnnActivationDescriptor_t& activation_desc,\n@@ -1938,7 +1938,7 @@ absl::StatusOr<DeviceMemory<uint8_t>> CreateBatchNormBackwardWorkspace(\n       /*sizeInBytes=*/&workspace_size_in_bytes));\n   // Allocate the workspace.\n   if (workspace_size_in_bytes == 0) {\n-    return DeviceMemory<uint8_t>();\n+    return DeviceAddress<uint8_t>();\n   }\n   return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n }\n@@ -1966,18 +1966,18 @@ template <class T>\n absl::Status CudnnSupport::DoRnnForwardImpl(\n     Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n     const CudnnRnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<T>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<T>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const CudnnRnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<T>& input_h_data,\n+    const DeviceAddress<T>& input_h_data,\n     const CudnnRnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+    const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n     const CudnnRnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<T>* output_data,\n+    DeviceAddress<T>* output_data,\n     const CudnnRnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<T>* output_h_data,\n+    DeviceAddress<T>* output_h_data,\n     const CudnnRnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<T>* output_c_data, bool is_training,\n+    DeviceAddress<T>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -1992,8 +1992,8 @@ absl::Status CudnnSupport::DoRnnForwardImpl(\n \n   TF_RETURN_IF_ERROR(CheckRNNParameterSize(cudnn, rnn_desc, input_desc));\n \n-  DeviceMemory<uint8_t> reserve_space;\n-  DeviceMemory<uint8_t> workspace;\n+  DeviceAddress<uint8_t> reserve_space;\n+  DeviceAddress<uint8_t> workspace;\n   TF_RETURN_IF_ERROR(CreateRnnTempSpace(\n       stream, cudnn, rnn_desc, model_dims, input_desc, workspace_allocator,\n       reserve_space_allocator, is_training, &workspace, &reserve_space));\n@@ -2074,26 +2074,26 @@ template <class T>\n absl::Status CudnnSupport::DoRnnBackwardImpl(\n     Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n     const CudnnRnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<T>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<T>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const CudnnRnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<T>& input_h_data,\n+    const DeviceAddress<T>& input_h_data,\n     const CudnnRnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+    const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n     const CudnnRnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<T>& output_data,\n+    const DeviceAddress<T>& output_data,\n     const CudnnRnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<T>& output_h_data,\n+    const DeviceAddress<T>& output_h_data,\n     const CudnnRnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<T>& output_c_data,\n-    const DeviceMemory<T>& output_backprop_data,\n-    const DeviceMemory<T>& output_h_backprop_data,\n-    const DeviceMemory<T>& output_c_backprop_data,\n-    DeviceMemory<T>* input_backprop_data,\n-    DeviceMemory<T>* input_h_backprop_data,\n-    DeviceMemory<T>* input_c_backprop_data,\n-    DeviceMemory<T>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<T>& output_c_data,\n+    const DeviceAddress<T>& output_backprop_data,\n+    const DeviceAddress<T>& output_h_backprop_data,\n+    const DeviceAddress<T>& output_c_backprop_data,\n+    DeviceAddress<T>* input_backprop_data,\n+    DeviceAddress<T>* input_h_backprop_data,\n+    DeviceAddress<T>* input_c_backprop_data,\n+    DeviceAddress<T>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   TF_ASSIGN_OR_RETURN(\n@@ -2107,7 +2107,7 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n \n   TF_RETURN_IF_ERROR(CheckRNNParameterSize(cudnn, rnn_desc, input_desc));\n \n-  DeviceMemory<uint8_t> workspace;\n+  DeviceAddress<uint8_t> workspace;\n   TF_RETURN_IF_ERROR(CreateRnnTempSpace(stream, cudnn, rnn_desc, model_dims,\n                                         input_desc, workspace_allocator,\n                                         nullptr, true, &workspace, nullptr));\n@@ -2223,12 +2223,12 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n \n absl::Status CudnnSupport::DoCtcLossImpl(\n     Stream* stream, const CudnnRnnStateTensorDescriptor& probs_desc,\n-    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+    const DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n     absl::Span<const int> labels_lengths_data,\n-    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n+    absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n     const CudnnRnnStateTensorDescriptor& grads_desc,\n-    DeviceMemoryBase grads_data, const CudnnCtcLossDescriptor& ctc_loss_desc,\n-    DeviceMemory<uint8_t> scratch_memory, int ctc_loss_algo_id) {\n+    DeviceAddressBase grads_data, const CudnnCtcLossDescriptor& ctc_loss_desc,\n+    DeviceAddress<uint8_t> scratch_memory, int ctc_loss_algo_id) {\n   auto cudnn = cudnn_->GetHandle(parent_, stream);\n \n   int kNumTimestamps = probs_desc.num_layers();\n@@ -2315,19 +2315,19 @@ CudnnSupport::CreateRnnStateTensorDescriptor(int num_layer, int batch_size,\n bool CudnnSupport::DoRnnForward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<Eigen::half>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<Eigen::half>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<Eigen::half>& input_h_data,\n+    const DeviceAddress<Eigen::half>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<Eigen::half>& input_c_data,\n-    const DeviceMemory<Eigen::half>& params,\n+    const DeviceAddress<Eigen::half>& input_c_data,\n+    const DeviceAddress<Eigen::half>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<Eigen::half>* output_data,\n+    DeviceAddress<Eigen::half>* output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<Eigen::half>* output_h_data,\n+    DeviceAddress<Eigen::half>* output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n+    DeviceAddress<Eigen::half>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -2359,18 +2359,19 @@ bool CudnnSupport::DoRnnForward(\n bool CudnnSupport::DoRnnForward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<float>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<float>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<float>& input_h_data,\n+    const DeviceAddress<float>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n+    const DeviceAddress<float>& input_c_data,\n+    const DeviceAddress<float>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<float>* output_data,\n+    DeviceAddress<float>* output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<float>* output_h_data,\n+    DeviceAddress<float>* output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<float>* output_c_data, bool is_training,\n+    DeviceAddress<float>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -2402,19 +2403,19 @@ bool CudnnSupport::DoRnnForward(\n bool CudnnSupport::DoRnnForward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<double>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<double>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<double>& input_h_data,\n+    const DeviceAddress<double>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<double>& input_c_data,\n-    const DeviceMemory<double>& params,\n+    const DeviceAddress<double>& input_c_data,\n+    const DeviceAddress<double>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<double>* output_data,\n+    DeviceAddress<double>* output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<double>* output_h_data,\n+    DeviceAddress<double>* output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<double>* output_c_data, bool is_training,\n+    DeviceAddress<double>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -2446,27 +2447,27 @@ bool CudnnSupport::DoRnnForward(\n bool CudnnSupport::DoRnnBackward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<Eigen::half>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<Eigen::half>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<Eigen::half>& input_h_data,\n+    const DeviceAddress<Eigen::half>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<Eigen::half>& input_c_data,\n-    const DeviceMemory<Eigen::half>& params,\n+    const DeviceAddress<Eigen::half>& input_c_data,\n+    const DeviceAddress<Eigen::half>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<Eigen::half>& output_data,\n+    const DeviceAddress<Eigen::half>& output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<Eigen::half>& output_h_data,\n+    const DeviceAddress<Eigen::half>& output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<Eigen::half>& output_c_data,\n-    const DeviceMemory<Eigen::half>& output_backprop_data,\n-    const DeviceMemory<Eigen::half>& output_h_backprop_data,\n-    const DeviceMemory<Eigen::half>& output_c_backprop_data,\n-    DeviceMemory<Eigen::half>* input_backprop_data,\n-    DeviceMemory<Eigen::half>* input_h_backprop_data,\n-    DeviceMemory<Eigen::half>* input_c_backprop_data,\n-    DeviceMemory<Eigen::half>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<Eigen::half>& output_c_data,\n+    const DeviceAddress<Eigen::half>& output_backprop_data,\n+    const DeviceAddress<Eigen::half>& output_h_backprop_data,\n+    const DeviceAddress<Eigen::half>& output_c_backprop_data,\n+    DeviceAddress<Eigen::half>* input_backprop_data,\n+    DeviceAddress<Eigen::half>* input_h_backprop_data,\n+    DeviceAddress<Eigen::half>* input_c_backprop_data,\n+    DeviceAddress<Eigen::half>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   const CudnnRnnDescriptor& cudnn_rnn_desc =\n@@ -2499,26 +2500,27 @@ bool CudnnSupport::DoRnnBackward(\n bool CudnnSupport::DoRnnBackward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<float>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<float>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<float>& input_h_data,\n+    const DeviceAddress<float>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n+    const DeviceAddress<float>& input_c_data,\n+    const DeviceAddress<float>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<float>& output_data,\n+    const DeviceAddress<float>& output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<float>& output_h_data,\n+    const DeviceAddress<float>& output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<float>& output_c_data,\n-    const DeviceMemory<float>& output_backprop_data,\n-    const DeviceMemory<float>& output_h_backprop_data,\n-    const DeviceMemory<float>& output_c_backprop_data,\n-    DeviceMemory<float>* input_backprop_data,\n-    DeviceMemory<float>* input_h_backprop_data,\n-    DeviceMemory<float>* input_c_backprop_data,\n-    DeviceMemory<float>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<float>& output_c_data,\n+    const DeviceAddress<float>& output_backprop_data,\n+    const DeviceAddress<float>& output_h_backprop_data,\n+    const DeviceAddress<float>& output_c_backprop_data,\n+    DeviceAddress<float>* input_backprop_data,\n+    DeviceAddress<float>* input_h_backprop_data,\n+    DeviceAddress<float>* input_c_backprop_data,\n+    DeviceAddress<float>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   const CudnnRnnDescriptor& cudnn_rnn_desc =\n@@ -2551,27 +2553,27 @@ bool CudnnSupport::DoRnnBackward(\n bool CudnnSupport::DoRnnBackward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<double>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<double>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<double>& input_h_data,\n+    const DeviceAddress<double>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<double>& input_c_data,\n-    const DeviceMemory<double>& params,\n+    const DeviceAddress<double>& input_c_data,\n+    const DeviceAddress<double>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<double>& output_data,\n+    const DeviceAddress<double>& output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<double>& output_h_data,\n+    const DeviceAddress<double>& output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<double>& output_c_data,\n-    const DeviceMemory<double>& output_backprop_data,\n-    const DeviceMemory<double>& output_h_backprop_data,\n-    const DeviceMemory<double>& output_c_backprop_data,\n-    DeviceMemory<double>* input_backprop_data,\n-    DeviceMemory<double>* input_h_backprop_data,\n-    DeviceMemory<double>* input_c_backprop_data,\n-    DeviceMemory<double>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<double>& output_c_data,\n+    const DeviceAddress<double>& output_backprop_data,\n+    const DeviceAddress<double>& output_h_backprop_data,\n+    const DeviceAddress<double>& output_c_backprop_data,\n+    DeviceAddress<double>* input_backprop_data,\n+    DeviceAddress<double>* input_h_backprop_data,\n+    DeviceAddress<double>* input_c_backprop_data,\n+    DeviceAddress<double>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   const CudnnRnnDescriptor& cudnn_rnn_desc =\n@@ -5198,7 +5200,7 @@ class CudnnExecutionPlanRunner<void(Args...)>\n   }\n \n   absl::Status operator()(Stream* stream, dnn::ProfileResult* profile_result,\n-                          DeviceMemoryBase scratch_memory,\n+                          DeviceAddressBase scratch_memory,\n                           Args... inputs) const override {\n     if (parent_ != stream->parent()) {\n       return absl::InternalError(\n@@ -5219,11 +5221,11 @@ class CudnnExecutionPlanRunner<void(Args...)>\n \n     // The operands of ForwardGraph convolutions and norm Custom Calls are\n     // gathered dynamically. In these cases, Args... is\n-    // std::vector<DeviceMemoryBase>.\n+    // std::vector<DeviceAddressBase>.\n     if constexpr (sizeof...(Args) == 1 &&\n                   std::is_same_v<std::tuple_element_t<0, std::tuple<Args...>>,\n-                                 std::vector<DeviceMemoryBase>>) {\n-      for (DeviceMemoryBase input : std::get<0>(std::tie(inputs...))) {\n+                                 std::vector<DeviceAddressBase>>) {\n+      for (DeviceAddressBase input : std::get<0>(std::tie(inputs...))) {\n         data_ptrs_vec.push_back(input.opaque());\n       }\n     } else {\n@@ -5457,11 +5459,11 @@ absl::Status CudnnSupport::GetConvolveRunners(\n     dnn::ConvolutionKind kind, dnn::DataType input_type,\n     dnn::DataType output_type, Stream* stream,\n     const dnn::BatchDescriptor& input_descriptor,\n-    DeviceMemoryBase /*input_data*/,\n+    DeviceAddressBase /*input_data*/,\n     const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase /*filter_data*/,\n+    DeviceAddressBase /*filter_data*/,\n     const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase /*output_data*/,\n+    DeviceAddressBase /*output_data*/,\n     const dnn::ConvolutionDescriptor& convolution_descriptor, bool use_fallback,\n     ScratchAllocator* /*scratch_allocator*/,\n     const EngineOptions& engine_options,\n@@ -5958,16 +5960,16 @@ bool CudnnSupport::GetConvolveBackwardFilterAlgorithms(\n }\n \n bool CudnnSupport::DoBatchNormalizationForward(\n-    Stream* stream, const DeviceMemory<float>& x,\n-    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-    const DeviceMemory<float>& estimated_mean,\n-    const DeviceMemory<float>& estimated_variance,\n-    const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n+    Stream* stream, const DeviceAddress<float>& x,\n+    const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+    const DeviceAddress<float>& estimated_mean,\n+    const DeviceAddress<float>& estimated_variance,\n+    const DeviceAddress<float>& side_input, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<float>* y,\n-    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<float>* y,\n+    DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+    DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n     bool is_training, ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator) {\n   return IsStatusOk(\n@@ -5981,17 +5983,17 @@ bool CudnnSupport::DoBatchNormalizationForward(\n }\n \n bool CudnnSupport::DoBatchNormalizationForward(\n-    Stream* stream, const DeviceMemory<Eigen::half>& x,\n-    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-    const DeviceMemory<float>& estimated_mean,\n-    const DeviceMemory<float>& estimated_variance,\n-    const DeviceMemory<Eigen::half>& side_input,\n+    Stream* stream, const DeviceAddress<Eigen::half>& x,\n+    const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+    const DeviceAddress<float>& estimated_mean,\n+    const DeviceAddress<float>& estimated_variance,\n+    const DeviceAddress<Eigen::half>& side_input,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y,\n-    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<Eigen::half>* y,\n+    DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+    DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n     bool is_training, ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator) {\n   return IsStatusOk(\n@@ -6005,17 +6007,17 @@ bool CudnnSupport::DoBatchNormalizationForward(\n }\n \n bool CudnnSupport::DoBatchNormalizationForward(\n-    Stream* stream, const DeviceMemory<Eigen::bfloat16>& x,\n-    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-    const DeviceMemory<float>& estimated_mean,\n-    const DeviceMemory<float>& estimated_variance,\n-    const DeviceMemory<Eigen::bfloat16>& side_input,\n+    Stream* stream, const DeviceAddress<Eigen::bfloat16>& x,\n+    const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+    const DeviceAddress<float>& estimated_mean,\n+    const DeviceAddress<float>& estimated_variance,\n+    const DeviceAddress<Eigen::bfloat16>& side_input,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::bfloat16>* y,\n-    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<Eigen::bfloat16>* y,\n+    DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+    DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n     bool is_training, ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator) {\n   return IsStatusOk(\n@@ -6031,16 +6033,16 @@ bool CudnnSupport::DoBatchNormalizationForward(\n template <class T, class U>\n absl::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n     Stream* stream, dnn::DataType input_data_type,\n-    dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n-    const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n-    const DeviceMemory<U>& estimated_mean,\n-    const DeviceMemory<U>& estimated_variance,\n-    const DeviceMemory<T>& side_input, const dnn::BatchDescriptor& x_desc,\n+    dnn::DataType scale_data_type, const DeviceAddress<T>& x,\n+    const DeviceAddress<U>& scale, const DeviceAddress<U>& offset,\n+    const DeviceAddress<U>& estimated_mean,\n+    const DeviceAddress<U>& estimated_variance,\n+    const DeviceAddress<T>& side_input, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<T>* y,\n-    DeviceMemory<U>* batch_mean, DeviceMemory<U>* batch_var,\n-    DeviceMemory<U>* saved_mean, DeviceMemory<U>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<T>* y,\n+    DeviceAddress<U>* batch_mean, DeviceAddress<U>* batch_var,\n+    DeviceAddress<U>* saved_mean, DeviceAddress<U>* saved_inv_var,\n     bool is_training, ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator) {\n   CudnnTensorDescriptor x_descriptor(x_desc, ToCudnnDataType(input_data_type));\n@@ -6054,8 +6056,8 @@ absl::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n   float zero = 0.0;\n   auto cudnn = cudnn_->GetHandle(parent_, stream);\n \n-  DeviceMemory<uint8_t> workspace;\n-  DeviceMemory<uint8_t> reserve_space;\n+  DeviceAddress<uint8_t> workspace;\n+  DeviceAddress<uint8_t> reserve_space;\n \n   const auto get_bn_ops = [&]() -> cudnnBatchNormOps_t {\n     if (side_input.is_null()) {\n@@ -6174,16 +6176,16 @@ absl::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n }\n \n bool CudnnSupport::DoBatchNormalizationBackward(\n-    Stream* stream, const DeviceMemory<float>& y_backprop,\n-    const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n-    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-    const DeviceMemory<float>& inv_var, const DeviceMemory<float>& y,\n+    Stream* stream, const DeviceAddress<float>& y_backprop,\n+    const DeviceAddress<float>& x, const DeviceAddress<float>& scale,\n+    const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+    const DeviceAddress<float>& inv_var, const DeviceAddress<float>& y,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-    dnn::ActivationMode activation_mode, DeviceMemory<float>* x_backprop,\n-    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-    DeviceMemory<float>* side_input_backprop,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    dnn::ActivationMode activation_mode, DeviceAddress<float>* x_backprop,\n+    DeviceAddress<float>* scale_backprop, DeviceAddress<float>* offset_backprop,\n+    DeviceAddress<float>* side_input_backprop,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator) {\n   return IsStatusOk(\n       DoBatchNormalizationBackwardImpl(\n@@ -6195,16 +6197,16 @@ bool CudnnSupport::DoBatchNormalizationBackward(\n }\n \n bool CudnnSupport::DoBatchNormalizationBackward(\n-    Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n-    const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n-    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-    const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::half>& y,\n+    Stream* stream, const DeviceAddress<Eigen::half>& y_backprop,\n+    const DeviceAddress<Eigen::half>& x, const DeviceAddress<float>& scale,\n+    const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+    const DeviceAddress<float>& inv_var, const DeviceAddress<Eigen::half>& y,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* x_backprop,\n-    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-    DeviceMemory<Eigen::half>* side_input_backprop,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    dnn::ActivationMode activation_mode, DeviceAddress<Eigen::half>* x_backprop,\n+    DeviceAddress<float>* scale_backprop, DeviceAddress<float>* offset_backprop,\n+    DeviceAddress<Eigen::half>* side_input_backprop,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator) {\n   return IsStatusOk(\n       DoBatchNormalizationBackwardImpl(\n@@ -6216,17 +6218,17 @@ bool CudnnSupport::DoBatchNormalizationBackward(\n }\n \n bool CudnnSupport::DoBatchNormalizationBackward(\n-    Stream* stream, const DeviceMemory<Eigen::bfloat16>& y_backprop,\n-    const DeviceMemory<Eigen::bfloat16>& x, const DeviceMemory<float>& scale,\n-    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-    const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::bfloat16>& y,\n-    const dnn::BatchDescriptor& x_desc,\n+    Stream* stream, const DeviceAddress<Eigen::bfloat16>& y_backprop,\n+    const DeviceAddress<Eigen::bfloat16>& x, const DeviceAddress<float>& scale,\n+    const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+    const DeviceAddress<float>& inv_var,\n+    const DeviceAddress<Eigen::bfloat16>& y, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     dnn::ActivationMode activation_mode,\n-    DeviceMemory<Eigen::bfloat16>* x_backprop,\n-    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-    DeviceMemory<Eigen::bfloat16>* side_input_backprop,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    DeviceAddress<Eigen::bfloat16>* x_backprop,\n+    DeviceAddress<float>* scale_backprop, DeviceAddress<float>* offset_backprop,\n+    DeviceAddress<Eigen::bfloat16>* side_input_backprop,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator) {\n   return IsStatusOk(\n       DoBatchNormalizationBackwardImpl(\n@@ -6240,15 +6242,15 @@ bool CudnnSupport::DoBatchNormalizationBackward(\n template <class T, class U>\n absl::Status CudnnSupport::DoBatchNormalizationBackwardImpl(\n     Stream* stream, int cudnn_input_type, int cudnn_scale_type,\n-    const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n-    const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n-    const DeviceMemory<U>& mean, const DeviceMemory<U>& inv_var,\n-    const DeviceMemory<T>& y, const dnn::BatchDescriptor& x_desc,\n+    const DeviceAddress<T>& y_backprop, const DeviceAddress<T>& x,\n+    const DeviceAddress<U>& scale, const DeviceAddress<U>& offset,\n+    const DeviceAddress<U>& mean, const DeviceAddress<U>& inv_var,\n+    const DeviceAddress<T>& y, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-    dnn::ActivationMode activation_mode, DeviceMemory<T>* x_backprop,\n-    DeviceMemory<U>* scale_backprop, DeviceMemory<U>* offset_backprop,\n-    DeviceMemory<T>* side_input_backprop,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    dnn::ActivationMode activation_mode, DeviceAddress<T>* x_backprop,\n+    DeviceAddress<U>* scale_backprop, DeviceAddress<U>* offset_backprop,\n+    DeviceAddress<T>* side_input_backprop,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator) {\n   CudnnTensorDescriptor x_descriptor(\n       x_desc, static_cast<cudnnDataType_t>(cudnn_input_type));\n@@ -6282,7 +6284,7 @@ absl::Status CudnnSupport::DoBatchNormalizationBackwardImpl(\n         activation_mode, CUDNN_PROPAGATE_NAN, x_desc.value_max());\n \n     TF_ASSIGN_OR_RETURN(\n-        DeviceMemory<uint8_t> workspace,\n+        DeviceAddress<uint8_t> workspace,\n         CreateBatchNormBackwardWorkspace(\n             stream, cudnn, mode, bn_ops, activation_desc.handle(), x_descriptor,\n             scale_offset_descriptor, workspace_allocator));\n@@ -6346,15 +6348,15 @@ absl::Status CudnnSupport::DoFusedConvolve(\n     Stream* stream, dnn::DataType input_type, dnn::DataType side_input_type,\n     dnn::DataType bias_type, dnn::DataType output_type,\n     const dnn::BatchDescriptor& conv_input_descriptor,\n-    DeviceMemoryBase conv_input_data, double conv_scale,\n+    DeviceAddressBase conv_input_data, double conv_scale,\n     const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data,\n+    DeviceAddressBase filter_data,\n     const dnn::ConvolutionDescriptor& convolution_descriptor,\n-    DeviceMemoryBase side_input_data, double side_input_scale,\n-    const dnn::BatchDescriptor& bias_descriptor, DeviceMemoryBase biases,\n+    DeviceAddressBase side_input_data, double side_input_scale,\n+    const dnn::BatchDescriptor& bias_descriptor, DeviceAddressBase biases,\n     dnn::ActivationMode activation_mode,\n-    const dnn::BatchDescriptor& output_descriptor, DeviceMemoryBase output_data,\n-    ScratchAllocator* scratch_allocator,\n+    const dnn::BatchDescriptor& output_descriptor,\n+    DeviceAddressBase output_data, ScratchAllocator* scratch_allocator,\n     const dnn::AlgorithmConfig& algorithm_config,\n     dnn::ProfileResult* output_profile_result) {\n   return absl::UnimplementedError(\n@@ -6363,10 +6365,10 @@ absl::Status CudnnSupport::DoFusedConvolve(\n \n absl::Status CudnnSupport::CudnnReorderConvolutionFilterAndBias(\n     Stream* stream, const dnn::FilterDescriptor& filter_descriptor,\n-    const DeviceMemory<int8_t>& filter_input,\n-    DeviceMemory<int8_t>* filter_output,\n-    std::optional<const DeviceMemory<float>> bias_input,\n-    std::optional<DeviceMemory<float>> bias_output) {\n+    const DeviceAddress<int8_t>& filter_input,\n+    DeviceAddress<int8_t>* filter_output,\n+    std::optional<const DeviceAddress<float>> bias_input,\n+    std::optional<DeviceAddress<float>> bias_output) {\n   bool has_bias = bias_input.has_value();\n   CHECK(!has_bias || bias_output.has_value());\n \n@@ -6394,7 +6396,7 @@ absl::Status CudnnSupport::DoPrepareForCtcLoss(\n     absl::Span<const int> labels_lengths_data,\n     absl::Span<const int> input_lengths_data,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n-    DeviceMemory<uint8_t>* scratch_memory, int* ctc_loss_algo_id) {\n+    DeviceAddress<uint8_t>* scratch_memory, int* ctc_loss_algo_id) {\n   auto cudnn = cudnn_->GetHandle(parent_, stream);\n   // Query the workspace size.\n   size_t workspace_size_in_bytes = 0;\n@@ -6438,7 +6440,7 @@ absl::Status CudnnSupport::DoPrepareForCtcLoss(\n   *ctc_loss_algo_id = algo;\n   // Allocate the workspace.\n   if (workspace_size_in_bytes == 0) {\n-    *scratch_memory = DeviceMemory<uint8_t>();\n+    *scratch_memory = DeviceAddress<uint8_t>();\n     return absl::OkStatus();\n   }\n   const auto scratch_or =\n@@ -6454,11 +6456,11 @@ absl::Status CudnnSupport::DoPrepareForCtcLoss(\n absl::Status CudnnSupport::DoCtcLoss(\n     Stream* stream, dnn::DataType element_type,\n     const dnn::RnnStateTensorDescriptor& probs_desc,\n-    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+    const DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n     absl::Span<const int> labels_lengths_data,\n-    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n+    absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n     const dnn::RnnStateTensorDescriptor& grads_desc,\n-    DeviceMemoryBase grads_data, DeviceMemory<uint8_t> scratch_memory,\n+    DeviceAddressBase grads_data, DeviceAddress<uint8_t> scratch_memory,\n     int ctc_loss_algo_id) {\n   // Current cuDNN CTC Loss only supports the float datatype\n   if (element_type != dnn::DataType::kFloat) {\n@@ -6480,10 +6482,10 @@ absl::Status CudnnSupport::DoCtcLoss(\n bool CudnnSupport::DoTransformTensor(Stream* stream,\n                                      const dnn::BatchDescriptor& input_desc,\n                                      dnn::DataType input_type,\n-                                     const DeviceMemoryBase& input_data,\n+                                     const DeviceAddressBase& input_data,\n                                      const dnn::BatchDescriptor& output_desc,\n                                      dnn::DataType output_type, float scale,\n-                                     DeviceMemoryBase* output_data) {\n+                                     DeviceAddressBase* output_data) {\n   float beta = 0.0f;\n   CudnnTensorDescriptor input_tensor_desc(\n       input_desc, ToCudnnDataType(input_type, input_desc.layout()));\n@@ -6560,9 +6562,9 @@ absl::StatusOr<std::vector<PoolingSplitsSpec>> GetTensorSplits(\n absl::Status CudnnSupport::DoPoolForward(\n     dnn::DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, ScratchAllocator* workspace_allocator) {\n   return DoPoolForward(element_type, stream, pooling_dimensions,\n                        EngineOptions{}, input_dimensions, input_data,\n                        output_dimensions, output_data, workspace_allocator);\n@@ -6572,9 +6574,9 @@ absl::Status CudnnSupport::DoPoolForward(\n     dnn::DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n     const EngineOptions& engine_options,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, ScratchAllocator* workspace_allocator) {\n   // Alpha is the scaling factor for input.\n   const float alpha_f = 1.0f;\n   const double alpha_d = 1.0;\n@@ -6638,10 +6640,10 @@ absl::Status CudnnSupport::DoPoolForward(\n absl::Status CudnnSupport::DoPoolBackward(\n     dnn::DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    DeviceMemoryBase input_diff_data, DeviceMemoryBase output_diff_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, DeviceAddressBase input_diff_data,\n+    DeviceAddressBase output_diff_data, ScratchAllocator* workspace_allocator) {\n   return DoPoolBackward(element_type, stream, pooling_dimensions,\n                         EngineOptions{}, input_dimensions, input_data,\n                         output_dimensions, output_data, input_diff_data,\n@@ -6652,10 +6654,10 @@ absl::Status CudnnSupport::DoPoolBackward(\n     dnn::DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n     const EngineOptions& engine_options,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    DeviceMemoryBase input_diff_data, DeviceMemoryBase output_diff_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, DeviceAddressBase input_diff_data,\n+    DeviceAddressBase output_diff_data, ScratchAllocator* workspace_allocator) {\n   // Alpha is the scaling factor for input.\n   const float alpha_f = 1.0f;\n   const double alpha_d = 1.0;\n@@ -6726,7 +6728,7 @@ absl::Status CudnnSupport::DoPoolBackward(\n bool CudnnSupport::DoNormalizeWithDimensions(\n     Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n     const dnn::BatchDescriptor& dimensions,\n-    const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n+    const DeviceAddress<float>& input_data, DeviceAddress<float>* output_data) {\n   // Check for unsupported modes.\n   if (normalize_descriptor.wrap_around()) {\n     LOG(ERROR) << \"CUDA LRN does not support cudnn-around mode\";\n@@ -6760,10 +6762,11 @@ bool CudnnSupport::DoNormalizeWithDimensions(\n \n bool CudnnSupport::DoNormalizeBackwardWithDimensions(\n     Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n-    const dnn::BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n-    const DeviceMemory<float>& normalized_data,\n-    const DeviceMemory<float>& normalized_variable_gradient,\n-    DeviceMemory<float>* raw_variable_gradient,\n+    const dnn::BatchDescriptor& dimensions,\n+    const DeviceAddress<float>& raw_data,\n+    const DeviceAddress<float>& normalized_data,\n+    const DeviceAddress<float>& normalized_variable_gradient,\n+    DeviceAddress<float>* raw_variable_gradient,\n     ScratchAllocator* workspace_allocator) {\n   // Check for unsupported modes.\n   if (normalize_descriptor.wrap_around()) {\n@@ -6864,10 +6867,10 @@ absl::Status CudnnGraph::Build(dnn::DnnSupport& dnn_support,\n }\n \n CudnnGraph::VariantPack CudnnGraph::PackOperands(\n-    absl::Span<DeviceMemoryBase> operands, DeviceMemoryBase& workspace,\n+    absl::Span<DeviceAddressBase> operands, DeviceAddressBase& workspace,\n     std::optional<int64_t> local_device_ordinal) const {\n   CudnnGraph::VariantPack tensor_to_ptr_map;\n-  absl::Span<DeviceMemoryBase> operands_without_workspace = operands;\n+  absl::Span<DeviceAddressBase> operands_without_workspace = operands;\n   if (graph_.get_workspace_size() > 0) {\n     workspace = operands.back();\n     CHECK_EQ(graph_.get_workspace_size(), workspace.size());\n@@ -6876,7 +6879,7 @@ CudnnGraph::VariantPack CudnnGraph::PackOperands(\n     operands_without_workspace = operands.first(operands.size() - 1);\n   }\n   auto next_uid = [uid = 0]() mutable -> int { return CuDnnTensorUID(uid++); };\n-  for (DeviceMemoryBase operand : operands_without_workspace) {\n+  for (DeviceAddressBase operand : operands_without_workspace) {\n     tensor_to_ptr_map[next_uid()] = operand.opaque();\n   }\n \n@@ -6892,9 +6895,9 @@ CudnnGraph::VariantPack CudnnGraph::PackOperands(\n }\n \n absl::Status CudnnGraph::Execute(Stream& stream,\n-                                 absl::Span<DeviceMemoryBase> operands,\n+                                 absl::Span<DeviceAddressBase> operands,\n                                  int64_t local_device_ordinal) const {\n-  DeviceMemoryBase workspace;\n+  DeviceAddressBase workspace;\n   VariantPack tensor_to_ptr_map =\n       PackOperands(operands, workspace, local_device_ordinal);\n \n@@ -6921,9 +6924,9 @@ absl::StatusOr<bool> CudnnGraph::SupportsExplicitCommandBufferConstruction()\n }\n \n absl::Status CudnnGraph::PopulateOrUpdateRawCommandBuffer(\n-    Stream& stream, absl::Span<DeviceMemoryBase> operands,\n+    Stream& stream, absl::Span<DeviceAddressBase> operands,\n     RawCommandBufferHandle cuda_graph, bool do_update) {\n-  DeviceMemoryBase workspace;\n+  DeviceAddressBase workspace;\n   VariantPack tensor_to_ptr_map = PackOperands(operands, workspace);\n \n   const CudnnSupport& dnn_support ="
        },
        {
            "sha": "fb9ad380986787da3d092030e24c386df57c47ef",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.h",
            "status": "modified",
            "additions": 210,
            "deletions": 205,
            "changes": 415,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"third_party/cudnn_frontend/include/cudnn_frontend.h\"\n #include \"xla/stream_executor/cuda/cudnn_sdpa_score_mod.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n@@ -51,7 +51,7 @@ class CudnnCtcLossDescriptor;\n using BatchDescriptorSlice = absl::Span<const dnn::BatchDescriptor>;\n \n template <typename T>\n-using DeviceMemorySlice = absl::Span<const DeviceMemory<T>* const>;\n+using DeviceAddressSlice = absl::Span<const DeviceAddress<T>* const>;\n \n class CudnnGraph : public dnn::DnnGraph {\n  public:\n@@ -62,7 +62,7 @@ class CudnnGraph : public dnn::DnnGraph {\n   // Builds single plan of the graph with given ID.\n   absl::Status Build(dnn::DnnSupport&, std::optional<int64_t> plan_id) override;\n   // Builds all the plans\n-  absl::Status Execute(Stream& stream, absl::Span<DeviceMemoryBase> operands,\n+  absl::Status Execute(Stream& stream, absl::Span<DeviceAddressBase> operands,\n                        int64_t local_device_ordinal) const override;\n   const cudnn_frontend::graph::Graph& Graph() const { return graph_; }\n   void InitDropoutState(int64_t local_device_count, int64_t seed,\n@@ -78,7 +78,7 @@ class CudnnGraph : public dnn::DnnGraph {\n   absl::StatusOr<bool> SupportsExplicitCommandBufferConstruction()\n       const override;\n   absl::Status PopulateOrUpdateRawCommandBuffer(\n-      Stream&, absl::Span<DeviceMemoryBase> operands, RawCommandBufferHandle,\n+      Stream&, absl::Span<DeviceAddressBase> operands, RawCommandBufferHandle,\n       bool do_update) override;\n \n  private:\n@@ -88,7 +88,7 @@ class CudnnGraph : public dnn::DnnGraph {\n   int64_t dropout_rng_offset_increment_ = 0;\n   using VariantPack = std::unordered_map<int64_t, void*>;\n   VariantPack PackOperands(\n-      absl::Span<DeviceMemoryBase> operands, DeviceMemoryBase& workspace,\n+      absl::Span<DeviceAddressBase> operands, DeviceAddressBase& workspace,\n       std::optional<int64_t> local_device_ordinal = std::nullopt) const;\n };\n \n@@ -127,147 +127,148 @@ class CudnnSupport : public dnn::DnnSupport {\n \n   bool DoRnnForward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                    const DeviceMemory<Eigen::half>& input_data,\n-                    const DeviceMemory<int>& seq_lengths_data,\n+                    const DeviceAddress<Eigen::half>& input_data,\n+                    const DeviceAddress<int>& seq_lengths_data,\n                     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                    const DeviceMemory<Eigen::half>& input_h_data,\n+                    const DeviceAddress<Eigen::half>& input_h_data,\n                     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                    const DeviceMemory<Eigen::half>& input_c_data,\n-                    const DeviceMemory<Eigen::half>& params,\n+                    const DeviceAddress<Eigen::half>& input_c_data,\n+                    const DeviceAddress<Eigen::half>& params,\n                     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                    DeviceMemory<Eigen::half>* output_data,\n+                    DeviceAddress<Eigen::half>* output_data,\n                     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                    DeviceMemory<Eigen::half>* output_h_data,\n+                    DeviceAddress<Eigen::half>* output_h_data,\n                     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n+                    DeviceAddress<Eigen::half>* output_c_data, bool is_training,\n                     ScratchAllocator* reserve_space_allocator,\n                     ScratchAllocator* workspace_allocator,\n                     dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnForward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                    const DeviceMemory<float>& input_data,\n-                    const DeviceMemory<int>& seq_lengths_data,\n+                    const DeviceAddress<float>& input_data,\n+                    const DeviceAddress<int>& seq_lengths_data,\n                     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                    const DeviceMemory<float>& input_h_data,\n+                    const DeviceAddress<float>& input_h_data,\n                     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                    const DeviceMemory<float>& input_c_data,\n-                    const DeviceMemory<float>& params,\n+                    const DeviceAddress<float>& input_c_data,\n+                    const DeviceAddress<float>& params,\n                     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                    DeviceMemory<float>* output_data,\n+                    DeviceAddress<float>* output_data,\n                     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                    DeviceMemory<float>* output_h_data,\n+                    DeviceAddress<float>* output_h_data,\n                     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                    DeviceMemory<float>* output_c_data, bool is_training,\n+                    DeviceAddress<float>* output_c_data, bool is_training,\n                     ScratchAllocator* reserve_space_allocator,\n                     ScratchAllocator* workspace_allocator,\n                     dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnForward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                    const DeviceMemory<double>& input_data,\n-                    const DeviceMemory<int>& seq_lengths_data,\n+                    const DeviceAddress<double>& input_data,\n+                    const DeviceAddress<int>& seq_lengths_data,\n                     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                    const DeviceMemory<double>& input_h_data,\n+                    const DeviceAddress<double>& input_h_data,\n                     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                    const DeviceMemory<double>& input_c_data,\n-                    const DeviceMemory<double>& params,\n+                    const DeviceAddress<double>& input_c_data,\n+                    const DeviceAddress<double>& params,\n                     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                    DeviceMemory<double>* output_data,\n+                    DeviceAddress<double>* output_data,\n                     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                    DeviceMemory<double>* output_h_data,\n+                    DeviceAddress<double>* output_h_data,\n                     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                    DeviceMemory<double>* output_c_data, bool is_training,\n+                    DeviceAddress<double>* output_c_data, bool is_training,\n                     ScratchAllocator* reserve_space_allocator,\n                     ScratchAllocator* workspace_allocator,\n                     dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnBackward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                      const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                     const DeviceMemory<Eigen::half>& input_data,\n-                     const DeviceMemory<int>& seq_lengths_data,\n+                     const DeviceAddress<Eigen::half>& input_data,\n+                     const DeviceAddress<int>& seq_lengths_data,\n                      const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                     const DeviceMemory<Eigen::half>& input_h_data,\n+                     const DeviceAddress<Eigen::half>& input_h_data,\n                      const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                     const DeviceMemory<Eigen::half>& input_c_data,\n-                     const DeviceMemory<Eigen::half>& params,\n+                     const DeviceAddress<Eigen::half>& input_c_data,\n+                     const DeviceAddress<Eigen::half>& params,\n                      const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                     const DeviceMemory<Eigen::half>& output_data,\n+                     const DeviceAddress<Eigen::half>& output_data,\n                      const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                     const DeviceMemory<Eigen::half>& output_h_data,\n+                     const DeviceAddress<Eigen::half>& output_h_data,\n                      const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                     const DeviceMemory<Eigen::half>& output_c_data,\n-                     const DeviceMemory<Eigen::half>& output_backprop_data,\n-                     const DeviceMemory<Eigen::half>& output_h_backprop_data,\n-                     const DeviceMemory<Eigen::half>& output_c_backprop_data,\n-                     DeviceMemory<Eigen::half>* input_backprop_data,\n-                     DeviceMemory<Eigen::half>* input_h_backprop_data,\n-                     DeviceMemory<Eigen::half>* input_c_backprop_data,\n-                     DeviceMemory<Eigen::half>* params_backprop_data,\n-                     DeviceMemory<uint8_t>* reserve_space_data,\n+                     const DeviceAddress<Eigen::half>& output_c_data,\n+                     const DeviceAddress<Eigen::half>& output_backprop_data,\n+                     const DeviceAddress<Eigen::half>& output_h_backprop_data,\n+                     const DeviceAddress<Eigen::half>& output_c_backprop_data,\n+                     DeviceAddress<Eigen::half>* input_backprop_data,\n+                     DeviceAddress<Eigen::half>* input_h_backprop_data,\n+                     DeviceAddress<Eigen::half>* input_c_backprop_data,\n+                     DeviceAddress<Eigen::half>* params_backprop_data,\n+                     DeviceAddress<uint8_t>* reserve_space_data,\n                      ScratchAllocator* workspace_allocator,\n                      dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnBackward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                      const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                     const DeviceMemory<float>& input_data,\n-                     const DeviceMemory<int>& seq_lengths_data,\n+                     const DeviceAddress<float>& input_data,\n+                     const DeviceAddress<int>& seq_lengths_data,\n                      const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                     const DeviceMemory<float>& input_h_data,\n+                     const DeviceAddress<float>& input_h_data,\n                      const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                     const DeviceMemory<float>& input_c_data,\n-                     const DeviceMemory<float>& params,\n+                     const DeviceAddress<float>& input_c_data,\n+                     const DeviceAddress<float>& params,\n                      const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                     const DeviceMemory<float>& output_data,\n+                     const DeviceAddress<float>& output_data,\n                      const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                     const DeviceMemory<float>& output_h_data,\n+                     const DeviceAddress<float>& output_h_data,\n                      const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                     const DeviceMemory<float>& output_c_data,\n-                     const DeviceMemory<float>& output_backprop_data,\n-                     const DeviceMemory<float>& output_h_backprop_data,\n-                     const DeviceMemory<float>& output_c_backprop_data,\n-                     DeviceMemory<float>* input_backprop_data,\n-                     DeviceMemory<float>* input_h_backprop_data,\n-                     DeviceMemory<float>* input_c_backprop_data,\n-                     DeviceMemory<float>* params_backprop_data,\n-                     DeviceMemory<uint8_t>* reserve_space_data,\n+                     const DeviceAddress<float>& output_c_data,\n+                     const DeviceAddress<float>& output_backprop_data,\n+                     const DeviceAddress<float>& output_h_backprop_data,\n+                     const DeviceAddress<float>& output_c_backprop_data,\n+                     DeviceAddress<float>* input_backprop_data,\n+                     DeviceAddress<float>* input_h_backprop_data,\n+                     DeviceAddress<float>* input_c_backprop_data,\n+                     DeviceAddress<float>* params_backprop_data,\n+                     DeviceAddress<uint8_t>* reserve_space_data,\n                      ScratchAllocator* workspace_allocator,\n                      dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnBackward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                      const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                     const DeviceMemory<double>& input_data,\n-                     const DeviceMemory<int>& seq_lengths_data,\n+                     const DeviceAddress<double>& input_data,\n+                     const DeviceAddress<int>& seq_lengths_data,\n                      const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                     const DeviceMemory<double>& input_h_data,\n+                     const DeviceAddress<double>& input_h_data,\n                      const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                     const DeviceMemory<double>& input_c_data,\n-                     const DeviceMemory<double>& params,\n+                     const DeviceAddress<double>& input_c_data,\n+                     const DeviceAddress<double>& params,\n                      const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                     const DeviceMemory<double>& output_data,\n+                     const DeviceAddress<double>& output_data,\n                      const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                     const DeviceMemory<double>& output_h_data,\n+                     const DeviceAddress<double>& output_h_data,\n                      const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                     const DeviceMemory<double>& output_c_data,\n-                     const DeviceMemory<double>& output_backprop_data,\n-                     const DeviceMemory<double>& output_h_backprop_data,\n-                     const DeviceMemory<double>& output_c_backprop_data,\n-                     DeviceMemory<double>* input_backprop_data,\n-                     DeviceMemory<double>* input_h_backprop_data,\n-                     DeviceMemory<double>* input_c_backprop_data,\n-                     DeviceMemory<double>* params_backprop_data,\n-                     DeviceMemory<uint8_t>* reserve_space_data,\n+                     const DeviceAddress<double>& output_c_data,\n+                     const DeviceAddress<double>& output_backprop_data,\n+                     const DeviceAddress<double>& output_h_backprop_data,\n+                     const DeviceAddress<double>& output_c_backprop_data,\n+                     DeviceAddress<double>* input_backprop_data,\n+                     DeviceAddress<double>* input_h_backprop_data,\n+                     DeviceAddress<double>* input_c_backprop_data,\n+                     DeviceAddress<double>* params_backprop_data,\n+                     DeviceAddress<uint8_t>* reserve_space_data,\n                      ScratchAllocator* workspace_allocator,\n                      dnn::ProfileResult* output_profile_result) override;\n \n   absl::Status GetConvolveRunners(\n       dnn::ConvolutionKind kind, dnn::DataType input_type,\n       dnn::DataType output_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+      const dnn::BatchDescriptor& input_descriptor,\n+      DeviceAddressBase input_data,\n       const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n+      DeviceAddressBase filter_data,\n       const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n+      DeviceAddressBase output_data,\n       const dnn::ConvolutionDescriptor& convolution_descriptor,\n       bool use_fallback, ScratchAllocator* scratch_allocator,\n       const EngineOptions& engine_options,\n@@ -356,165 +357,169 @@ class CudnnSupport : public dnn::DnnSupport {\n       std::vector<dnn::AlgorithmDesc>* out_algorithms) override;\n \n   bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<float>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_var_iance,\n-      const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n+      Stream* stream, const DeviceAddress<float>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_var_iance,\n+      const DeviceAddress<float>& side_input,\n+      const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n       double exponential_average_factor, dnn::ActivationMode activation_mode,\n-      DeviceMemory<float>* y, DeviceMemory<float>* batch_mean,\n-      DeviceMemory<float>* batch_var, DeviceMemory<float>* saved_mean,\n-      DeviceMemory<float>* saved_inv_var, bool is_training,\n+      DeviceAddress<float>* y, DeviceAddress<float>* batch_mean,\n+      DeviceAddress<float>* batch_var, DeviceAddress<float>* saved_mean,\n+      DeviceAddress<float>* saved_inv_var, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<Eigen::half>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<Eigen::half>& side_input,\n+      Stream* stream, const DeviceAddress<Eigen::half>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<Eigen::half>& side_input,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n       double exponential_average_factor, dnn::ActivationMode activation_mode,\n-      DeviceMemory<Eigen::half>* y, DeviceMemory<float>* batch_mean,\n-      DeviceMemory<float>* batch_var, DeviceMemory<float>* saved_mean,\n-      DeviceMemory<float>* saved_inv_var, bool is_training,\n+      DeviceAddress<Eigen::half>* y, DeviceAddress<float>* batch_mean,\n+      DeviceAddress<float>* batch_var, DeviceAddress<float>* saved_mean,\n+      DeviceAddress<float>* saved_inv_var, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<Eigen::bfloat16>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<Eigen::bfloat16>& side_input,\n+      Stream* stream, const DeviceAddress<Eigen::bfloat16>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<Eigen::bfloat16>& side_input,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n       double exponential_average_factor, dnn::ActivationMode activation_mode,\n-      DeviceMemory<Eigen::bfloat16>* y, DeviceMemory<float>* batch_mean,\n-      DeviceMemory<float>* batch_var, DeviceMemory<float>* saved_mean,\n-      DeviceMemory<float>* saved_inv_var, bool is_training,\n+      DeviceAddress<Eigen::bfloat16>* y, DeviceAddress<float>* batch_mean,\n+      DeviceAddress<float>* batch_var, DeviceAddress<float>* saved_mean,\n+      DeviceAddress<float>* saved_inv_var, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<float>& y_backprop,\n-      const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var, const DeviceMemory<float>& y,\n+      Stream* stream, const DeviceAddress<float>& y_backprop,\n+      const DeviceAddress<float>& x, const DeviceAddress<float>& scale,\n+      const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+      const DeviceAddress<float>& inv_var, const DeviceAddress<float>& y,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n-      dnn::ActivationMode activation_mode, DeviceMemory<float>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<float>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      dnn::ActivationMode activation_mode, DeviceAddress<float>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<float>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n-      const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::half>& y,\n+      Stream* stream, const DeviceAddress<Eigen::half>& y_backprop,\n+      const DeviceAddress<Eigen::half>& x, const DeviceAddress<float>& scale,\n+      const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+      const DeviceAddress<float>& inv_var, const DeviceAddress<Eigen::half>& y,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n       dnn::ActivationMode activation_mode,\n-      DeviceMemory<Eigen::half>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<Eigen::half>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      DeviceAddress<Eigen::half>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<Eigen::half>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<Eigen::bfloat16>& y_backprop,\n-      const DeviceMemory<Eigen::bfloat16>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var,\n-      const DeviceMemory<Eigen::bfloat16>& y,\n+      Stream* stream, const DeviceAddress<Eigen::bfloat16>& y_backprop,\n+      const DeviceAddress<Eigen::bfloat16>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& mean, const DeviceAddress<float>& inv_var,\n+      const DeviceAddress<Eigen::bfloat16>& y,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n       dnn::ActivationMode activation_mode,\n-      DeviceMemory<Eigen::bfloat16>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<Eigen::bfloat16>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      DeviceAddress<Eigen::bfloat16>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<Eigen::bfloat16>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n   absl::Status DoFusedConvolve(\n       Stream* stream, dnn::DataType input_type, dnn::DataType side_input_type,\n       dnn::DataType bias_type, dnn::DataType output_type,\n       const dnn::BatchDescriptor& conv_input_descriptor,\n-      DeviceMemoryBase conv_input_data, double conv_scale,\n+      DeviceAddressBase conv_input_data, double conv_scale,\n       const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n+      DeviceAddressBase filter_data,\n       const dnn::ConvolutionDescriptor& convolution_descriptor,\n-      DeviceMemoryBase side_input_data, double side_input_scale,\n-      const dnn::BatchDescriptor& bias_descriptor, DeviceMemoryBase biases,\n+      DeviceAddressBase side_input_data, double side_input_scale,\n+      const dnn::BatchDescriptor& bias_descriptor, DeviceAddressBase biases,\n       dnn::ActivationMode activation_mode,\n       const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data, ScratchAllocator* scratch_allocator,\n+      DeviceAddressBase output_data, ScratchAllocator* scratch_allocator,\n       const dnn::AlgorithmConfig& algorithm_config,\n       dnn::ProfileResult* output_profile_result) override;\n \n   absl::Status CudnnReorderConvolutionFilterAndBias(\n       Stream* stream, const dnn::FilterDescriptor& filter_descriptor,\n-      const DeviceMemory<int8_t>& filter_input,\n-      DeviceMemory<int8_t>* filter_output,\n-      std::optional<const DeviceMemory<float>> bias_input,\n-      std::optional<DeviceMemory<float>> bias_output) override;\n+      const DeviceAddress<int8_t>& filter_input,\n+      DeviceAddress<int8_t>* filter_output,\n+      std::optional<const DeviceAddress<float>> bias_input,\n+      std::optional<DeviceAddress<float>> bias_output) override;\n \n   absl::Status DoPoolForward(dnn::DataType element_type, Stream* stream,\n                              const dnn::PoolingDescriptor& pooling_dimensions,\n                              const dnn::BatchDescriptor& input_dimensions,\n-                             DeviceMemoryBase input_data,\n+                             DeviceAddressBase input_data,\n                              const dnn::BatchDescriptor& output_dimensions,\n-                             DeviceMemoryBase output_data,\n+                             DeviceAddressBase output_data,\n                              ScratchAllocator* workspace_allocator) override;\n \n   absl::Status DoPoolForward(dnn::DataType element_type, Stream* stream,\n                              const dnn::PoolingDescriptor& pooling_dimensions,\n                              const EngineOptions& engine_options,\n                              const dnn::BatchDescriptor& input_dimensions,\n-                             DeviceMemoryBase input_data,\n+                             DeviceAddressBase input_data,\n                              const dnn::BatchDescriptor& output_dimensions,\n-                             DeviceMemoryBase output_data,\n+                             DeviceAddressBase output_data,\n                              ScratchAllocator* workspace_allocator) override;\n \n   absl::Status DoPoolBackward(dnn::DataType element_type, Stream* stream,\n                               const dnn::PoolingDescriptor& pooling_dimensions,\n                               const dnn::BatchDescriptor& input_dimensions,\n-                              DeviceMemoryBase input_data,\n+                              DeviceAddressBase input_data,\n                               const dnn::BatchDescriptor& output_dimensions,\n-                              DeviceMemoryBase output_data,\n-                              DeviceMemoryBase input_diff_data,\n-                              DeviceMemoryBase output_diff_data,\n+                              DeviceAddressBase output_data,\n+                              DeviceAddressBase input_diff_data,\n+                              DeviceAddressBase output_diff_data,\n                               ScratchAllocator* workspace_allocator) override;\n \n   absl::Status DoPoolBackward(dnn::DataType element_type, Stream* stream,\n                               const dnn::PoolingDescriptor& pooling_dimensions,\n                               const EngineOptions& engine_options,\n                               const dnn::BatchDescriptor& input_dimensions,\n-                              DeviceMemoryBase input_data,\n+                              DeviceAddressBase input_data,\n                               const dnn::BatchDescriptor& output_dimensions,\n-                              DeviceMemoryBase output_data,\n-                              DeviceMemoryBase input_diff_data,\n-                              DeviceMemoryBase output_diff_data,\n+                              DeviceAddressBase output_data,\n+                              DeviceAddressBase input_diff_data,\n+                              DeviceAddressBase output_diff_data,\n                               ScratchAllocator* workspace_allocator) override;\n \n   bool DoNormalizeWithDimensions(\n       Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n       const dnn::BatchDescriptor& dimensions,\n-      const DeviceMemory<float>& input_data,\n-      DeviceMemory<float>* output_data) override;\n+      const DeviceAddress<float>& input_data,\n+      DeviceAddress<float>* output_data) override;\n \n   bool DoNormalizeBackwardWithDimensions(\n       Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n       const dnn::BatchDescriptor& dimensions,\n-      const DeviceMemory<float>& raw_data,\n-      const DeviceMemory<float>& normalized_data,\n-      const DeviceMemory<float>& normalized_variable_gradient,\n-      DeviceMemory<float>* raw_variable_gradient,\n+      const DeviceAddress<float>& raw_data,\n+      const DeviceAddress<float>& normalized_data,\n+      const DeviceAddress<float>& normalized_variable_gradient,\n+      DeviceAddress<float>* raw_variable_gradient,\n       ScratchAllocator* workspace_allocator) override;\n \n   // Derives an output batch descriptor from an input batch and convolution\n@@ -527,22 +532,22 @@ class CudnnSupport : public dnn::DnnSupport {\n \n   absl::Status DoCtcLoss(Stream* stream, dnn::DataType element_type,\n                          const dnn::RnnStateTensorDescriptor& probs_desc,\n-                         DeviceMemoryBase probs_data,\n+                         DeviceAddressBase probs_data,\n                          absl::Span<const int> labels_data,\n                          absl::Span<const int> labels_lengths_data,\n                          absl::Span<const int> input_lengths_data,\n-                         DeviceMemoryBase costs_data,\n+                         DeviceAddressBase costs_data,\n                          const dnn::RnnStateTensorDescriptor& grads_desc,\n-                         DeviceMemoryBase grads_data,\n-                         DeviceMemory<uint8_t> scratch_memory,\n+                         DeviceAddressBase grads_data,\n+                         DeviceAddress<uint8_t> scratch_memory,\n                          int ctc_loss_algo_id) override;\n \n   bool DoTransformTensor(Stream* stream, const dnn::BatchDescriptor& input_desc,\n                          dnn::DataType input_type,\n-                         const DeviceMemoryBase& input_data,\n+                         const DeviceAddressBase& input_data,\n                          const dnn::BatchDescriptor& output_desc,\n                          dnn::DataType output_type, float scale,\n-                         DeviceMemoryBase* output_data) override;\n+                         DeviceAddressBase* output_data) override;\n \n   void NotifyStreamDestroyed(Stream* stream) override;\n \n@@ -577,49 +582,49 @@ class CudnnSupport : public dnn::DnnSupport {\n   template <class T, class U>\n   absl::Status DoBatchNormalizationForwardImpl(\n       Stream* stream, dnn::DataType input_data_type,\n-      dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n-      const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n-      const DeviceMemory<U>& estimated_mean,\n-      const DeviceMemory<U>& estimated_variance,\n-      const DeviceMemory<T>& side_input, const dnn::BatchDescriptor& x_desc,\n+      dnn::DataType scale_data_type, const DeviceAddress<T>& x,\n+      const DeviceAddress<U>& scale, const DeviceAddress<U>& offset,\n+      const DeviceAddress<U>& estimated_mean,\n+      const DeviceAddress<U>& estimated_variance,\n+      const DeviceAddress<T>& side_input, const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n       double exponential_average_factor, dnn::ActivationMode activation_mode,\n-      DeviceMemory<T>* y, DeviceMemory<U>* batch_mean,\n-      DeviceMemory<U>* batch_var, DeviceMemory<U>* saved_mean,\n-      DeviceMemory<U>* saved_inv_var, bool is_training,\n+      DeviceAddress<T>* y, DeviceAddress<U>* batch_mean,\n+      DeviceAddress<U>* batch_var, DeviceAddress<U>* saved_mean,\n+      DeviceAddress<U>* saved_inv_var, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator);\n \n   template <class T, class U>\n   absl::Status DoBatchNormalizationBackwardImpl(\n       Stream* stream, int cudnn_input_type, int cudnn_scale_type,\n-      const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n-      const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n-      const DeviceMemory<U>& mean, const DeviceMemory<U>& inv_var,\n-      const DeviceMemory<T>& y, const dnn::BatchDescriptor& x_desc,\n+      const DeviceAddress<T>& y_backprop, const DeviceAddress<T>& x,\n+      const DeviceAddress<U>& scale, const DeviceAddress<U>& offset,\n+      const DeviceAddress<U>& mean, const DeviceAddress<U>& inv_var,\n+      const DeviceAddress<T>& y, const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, double epsilon,\n-      dnn::ActivationMode activation_mode, DeviceMemory<T>* x_backprop,\n-      DeviceMemory<U>* scale_backprop, DeviceMemory<U>* offset_backprop,\n-      DeviceMemory<T>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      dnn::ActivationMode activation_mode, DeviceAddress<T>* x_backprop,\n+      DeviceAddress<U>* scale_backprop, DeviceAddress<U>* offset_backprop,\n+      DeviceAddress<T>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator);\n \n   template <class T>\n   absl::Status DoRnnForwardImpl(\n       Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n       const CudnnRnnSequenceTensorDescriptor& input_desc,\n-      const DeviceMemory<T>& input_data,\n-      const DeviceMemory<int>& seq_lengths_data,\n+      const DeviceAddress<T>& input_data,\n+      const DeviceAddress<int>& seq_lengths_data,\n       const CudnnRnnStateTensorDescriptor& input_h_desc,\n-      const DeviceMemory<T>& input_h_data,\n+      const DeviceAddress<T>& input_h_data,\n       const CudnnRnnStateTensorDescriptor& input_c_desc,\n-      const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+      const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n       const CudnnRnnSequenceTensorDescriptor& output_desc,\n-      DeviceMemory<T>* output_data,\n+      DeviceAddress<T>* output_data,\n       const CudnnRnnStateTensorDescriptor& output_h_desc,\n-      DeviceMemory<T>* output_h_data,\n+      DeviceAddress<T>* output_h_data,\n       const CudnnRnnStateTensorDescriptor& output_c_desc,\n-      DeviceMemory<T>* output_c_data, bool is_training,\n+      DeviceAddress<T>* output_c_data, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator,\n       dnn::ProfileResult* output_profile_result);\n@@ -628,37 +633,37 @@ class CudnnSupport : public dnn::DnnSupport {\n   absl::Status DoRnnBackwardImpl(\n       Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n       const CudnnRnnSequenceTensorDescriptor& input_desc,\n-      const DeviceMemory<T>& input_data,\n-      const DeviceMemory<int>& seq_lengths_data,\n+      const DeviceAddress<T>& input_data,\n+      const DeviceAddress<int>& seq_lengths_data,\n       const CudnnRnnStateTensorDescriptor& input_h_desc,\n-      const DeviceMemory<T>& input_h_data,\n+      const DeviceAddress<T>& input_h_data,\n       const CudnnRnnStateTensorDescriptor& input_c_desc,\n-      const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+      const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n       const CudnnRnnSequenceTensorDescriptor& output_desc,\n-      const DeviceMemory<T>& output_data,\n+      const DeviceAddress<T>& output_data,\n       const CudnnRnnStateTensorDescriptor& output_h_desc,\n-      const DeviceMemory<T>& output_h_data,\n+      const DeviceAddress<T>& output_h_data,\n       const CudnnRnnStateTensorDescriptor& output_c_desc,\n-      const DeviceMemory<T>& output_c_data,\n-      const DeviceMemory<T>& output_backprop_data,\n-      const DeviceMemory<T>& output_h_backprop_data,\n-      const DeviceMemory<T>& output_c_backprop_data,\n-      DeviceMemory<T>* input_backprop_data,\n-      DeviceMemory<T>* input_h_backprop_data,\n-      DeviceMemory<T>* input_c_backprop_data,\n-      DeviceMemory<T>* params_backprop_data,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      const DeviceAddress<T>& output_c_data,\n+      const DeviceAddress<T>& output_backprop_data,\n+      const DeviceAddress<T>& output_h_backprop_data,\n+      const DeviceAddress<T>& output_c_backprop_data,\n+      DeviceAddress<T>* input_backprop_data,\n+      DeviceAddress<T>* input_h_backprop_data,\n+      DeviceAddress<T>* input_c_backprop_data,\n+      DeviceAddress<T>* params_backprop_data,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator,\n       dnn::ProfileResult* output_profile_result);\n \n   absl::Status DoCtcLossImpl(\n       Stream* stream, const CudnnRnnStateTensorDescriptor& probs_desc,\n-      DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+      DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n       absl::Span<const int> labels_lengths_data,\n-      absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n+      absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n       const CudnnRnnStateTensorDescriptor& grads_desc,\n-      DeviceMemoryBase grads_data, const CudnnCtcLossDescriptor& ctc_loss_desc,\n-      DeviceMemory<uint8_t> scratch_memory, int ctc_loss_algo_id);\n+      DeviceAddressBase grads_data, const CudnnCtcLossDescriptor& ctc_loss_desc,\n+      DeviceAddress<uint8_t> scratch_memory, int ctc_loss_algo_id);\n \n  private:\n   absl::Status DoPrepareForCtcLoss(\n@@ -669,7 +674,7 @@ class CudnnSupport : public dnn::DnnSupport {\n       absl::Span<const int> labels_lengths_data,\n       absl::Span<const int> input_lengths_data,\n       const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n-      DeviceMemory<uint8_t>* scratch_memory, int* ctc_loss_algo_id) override;\n+      DeviceAddress<uint8_t>* scratch_memory, int* ctc_loss_algo_id) override;\n \n   CudnnSupport(const CudnnSupport&) = delete;\n   void operator=(const CudnnSupport&) = delete;"
        },
        {
            "sha": "488e0f465f594a967b1e04135b27dd0317db8f99",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -68,8 +68,8 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_version_parser.h\"\n #include \"xla/stream_executor/cuda/cudnn_api_wrappers.h\"\n #include \"xla/stream_executor/cuda/tma_util.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -828,22 +828,22 @@ bool CudaExecutor::MemoryTracker::Remove(CUdeviceptr ptr) {\n // N.B. we must lose constness in order to pass a suitable type to the existing\n // libcuda APIs, so the caller should take care to only pass the result of const\n // GPU memory conversions to libcuda functions which will honor constness.\n-static CUdeviceptr AsCudaDevicePtr(const DeviceMemoryBase& gpu_mem) {\n+static CUdeviceptr AsCudaDevicePtr(const DeviceAddressBase& gpu_mem) {\n   return reinterpret_cast<CUdeviceptr>(gpu_mem.opaque());\n }\n \n // See description on const version above.\n-static CUdeviceptr AsCudaDevicePtr(DeviceMemoryBase* gpu_mem) {\n+static CUdeviceptr AsCudaDevicePtr(DeviceAddressBase* gpu_mem) {\n   return AsCudaDevicePtr(*gpu_mem);\n }\n \n-absl::StatusOr<DeviceMemoryBase> CudaExecutor::GetMemoryRange(\n-    const DeviceMemoryBase& location) const {\n+absl::StatusOr<DeviceAddressBase> CudaExecutor::GetMemoryRange(\n+    const DeviceAddressBase& location) const {\n   CUdeviceptr device_pointer;\n   size_t size;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuMemGetAddressRange(&device_pointer, &size, AsCudaDevicePtr(location))));\n-  return DeviceMemoryBase(reinterpret_cast<void*>(device_pointer), size);\n+  return DeviceAddressBase(reinterpret_cast<void*>(device_pointer), size);\n }\n \n std::unique_ptr<ActivateContext> CudaExecutor::Activate() {\n@@ -1239,7 +1239,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n         std::get<KernelArgumentsPackingSpec>(spec.kernel_args_packing());\n     cuda_kernel->set_args_packing(\n         [packing_spec](const Kernel& kernel, const KernelArgs& args) {\n-          const auto& mem_args = Cast<KernelArgsDeviceMemoryArray>(&args);\n+          const auto& mem_args = Cast<KernelArgsDeviceAddressArray>(&args);\n           return packing_spec.BuildArguments(mem_args->device_memory_args(),\n                                              args.number_of_shared_bytes());\n         });\n@@ -1348,7 +1348,7 @@ int fpus_per_core(int cc_major, int cc_minor) {\n \n }  // namespace\n \n-absl::StatusOr<std::shared_ptr<DeviceMemoryBase>>\n+absl::StatusOr<std::shared_ptr<DeviceAddressBase>>\n CudaExecutor::CreateOrShareConstant(Stream* stream,\n                                     absl::Span<const uint8_t> content) {\n   absl::MutexLock lock{shared_constants_mu_};\n@@ -1360,10 +1360,10 @@ CudaExecutor::CreateOrShareConstant(Stream* stream,\n       reinterpret_cast<const char*>(content.data()), content.size()));\n   // Must insert nullptr first to get an iterator to the insertion point.\n   auto insert_result = shared_constants_.insert(\n-      {fingerprint, std::weak_ptr<DeviceMemoryBase>()});\n+      {fingerprint, std::weak_ptr<DeviceAddressBase>()});\n   auto it = insert_result.first;\n   bool was_already_in_cache = !insert_result.second;\n-  std::shared_ptr<DeviceMemoryBase> shared_constant;\n+  std::shared_ptr<DeviceAddressBase> shared_constant;\n \n   if (was_already_in_cache) {\n     shared_constant = it->second.lock();\n@@ -1372,7 +1372,7 @@ CudaExecutor::CreateOrShareConstant(Stream* stream,\n   if (shared_constant == nullptr) {\n     // Either the constant wasn't found in the cache, or it was but its\n     // weak_ptr had expired.\n-    auto new_constant = std::make_unique<DeviceMemoryBase>(\n+    auto new_constant = std::make_unique<DeviceAddressBase>(\n         Allocate(content.size(), /*memory_space=*/0));\n     if (new_constant->opaque() == nullptr) {\n       return absl::InternalError(absl::StrFormat(\n@@ -1391,18 +1391,18 @@ CudaExecutor::CreateOrShareConstant(Stream* stream,\n \n     // Capturing 'this' in the custom deleter means this executor must\n     // outlive all shared uses of this constant.\n-    shared_constant = std::shared_ptr<DeviceMemoryBase>(\n-        new_constant.release(), [this](DeviceMemoryBase* p) {\n+    shared_constant = std::shared_ptr<DeviceAddressBase>(\n+        new_constant.release(), [this](DeviceAddressBase* p) {\n           Deallocate(p);\n           delete p;\n         });\n-    it->second = std::weak_ptr<DeviceMemoryBase>(shared_constant);\n+    it->second = std::weak_ptr<DeviceAddressBase>(shared_constant);\n   }\n \n   return shared_constant;\n }\n \n-DeviceMemoryBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n+DeviceAddressBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n   XLA_VLOG_DEVICE(1, device_ordinal())\n       << \"CudaExecutor::Allocate size: \" << size\n       << \" memory_space: \" << memory_space;\n@@ -1415,33 +1415,33 @@ DeviceMemoryBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n     }\n     XLA_VLOG_DEVICE(1, device_ordinal())\n         << \"CudaExecutor::Allocate returns \" << result.value();\n-    return DeviceMemoryBase(result.value(), size);\n+    return DeviceAddressBase(result.value(), size);\n   }\n \n   if (memory_space == static_cast<int64_t>(MemoryType::kHost)) {\n     auto result = HostAllocate(cuda_context_, numa_node_, size);\n     if (!result.ok()) {\n       XLA_LOG_DEVICE(ERROR, device_ordinal())\n           << \"Failed to allocate host memory: \" << result.status();\n-      return DeviceMemoryBase(nullptr, 0);\n+      return DeviceAddressBase(nullptr, 0);\n     }\n     XLA_VLOG_DEVICE(1, device_ordinal())\n         << \"CudaExecutor::Allocate returns \" << result.value();\n-    return DeviceMemoryBase(result.value(), size);\n+    return DeviceAddressBase(result.value(), size);\n   }\n \n   if (memory_space == static_cast<int64_t>(MemoryType::kP2P) &&\n       is_vmm_supported_) {\n     auto device_buf_base = VmmAllocateMemory(size);\n \n     if (device_buf_base.ok()) {\n-      return DeviceMemoryBase(device_buf_base.value(), size);\n+      return DeviceAddressBase(device_buf_base.value(), size);\n     }\n \n     XLA_LOG_DEVICE(ERROR, device_ordinal())\n         << \"Failed to allocate memory with VMM: \" << device_buf_base.status();\n \n-    return DeviceMemoryBase(nullptr, 0);\n+    return DeviceAddressBase(nullptr, 0);\n   }\n \n   CHECK(memory_space == static_cast<int64_t>(MemoryType::kDevice) ||\n@@ -1450,15 +1450,15 @@ DeviceMemoryBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n   auto device_buf_base = DeviceAllocate(cuda_context_, size);\n   XLA_VLOG_DEVICE(1, device_ordinal())\n       << \"CudaExecutor::Allocate returns \" << device_buf_base;\n-  return DeviceMemoryBase(device_buf_base, size);\n+  return DeviceAddressBase(device_buf_base, size);\n }\n \n absl::StatusOr<std::unique_ptr<MemoryAllocation>>\n CudaExecutor::HostMemoryAllocate(uint64_t size) {\n   return AllocateHostMemory(cuda_context_, numa_node_, size);\n }\n \n-void CudaExecutor::Deallocate(DeviceMemoryBase* mem) {\n+void CudaExecutor::Deallocate(DeviceAddressBase* mem) {\n   XLA_VLOG_DEVICE(1, device_ordinal())\n       << \"CudaExecutor::Deallocate mem: \" << mem->opaque();\n \n@@ -1500,7 +1500,7 @@ bool CudaExecutor::HostMemoryUnregister(void* location) {\n   return HostUnregister(cuda_context_, location);\n }\n \n-absl::Status CudaExecutor::SynchronousMemZero(DeviceMemoryBase* location,\n+absl::Status CudaExecutor::SynchronousMemZero(DeviceAddressBase* location,\n                                               uint64_t size) {\n   std::unique_ptr<ActivateContext> activation = Activate();\n   CUdeviceptr cuda_location = AsCudaDevicePtr(location);\n@@ -1514,7 +1514,7 @@ absl::Status CudaExecutor::SynchronousMemZero(DeviceMemoryBase* location,\n                         \"Failed to memset memory\");\n }\n \n-absl::Status CudaExecutor::SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n+absl::Status CudaExecutor::SynchronousMemcpy(DeviceAddressBase* gpu_dst,\n                                              const void* host_src,\n                                              uint64_t size) {\n   std::unique_ptr<ActivateContext> activation = Activate();\n@@ -1531,7 +1531,7 @@ absl::Status CudaExecutor::SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n }\n \n absl::Status CudaExecutor::SynchronousMemcpy(void* host_dst,\n-                                             const DeviceMemoryBase& gpu_src,\n+                                             const DeviceAddressBase& gpu_src,\n                                              uint64_t size) {\n   std::unique_ptr<ActivateContext> activation = Activate();\n   TF_RETURN_IF_ERROR(cuda::ToStatus(\n@@ -1643,7 +1643,7 @@ bool CudaExecutor::DeviceMemoryUsage(int64_t* free_out,\n   return true;\n }\n \n-absl::StatusOr<DeviceMemoryBase> CudaExecutor::GetSymbol(\n+absl::StatusOr<DeviceAddressBase> CudaExecutor::GetSymbol(\n     const std::string& symbol_name, ModuleHandle module_handle) {\n   void* mem = nullptr;\n   size_t bytes = 0;\n@@ -1659,7 +1659,7 @@ absl::StatusOr<DeviceMemoryBase> CudaExecutor::GetSymbol(\n     TF_RETURN_IF_ERROR(\n         GetModuleSymbol(cuda_context_, gpu_module_handle, symbol_name.c_str(),\n                         reinterpret_cast<CUdeviceptr*>(&mem), &bytes));\n-    return DeviceMemoryBase(mem, bytes);\n+    return DeviceAddressBase(mem, bytes);\n   }\n \n   return absl::NotFoundError(\n@@ -2087,7 +2087,7 @@ absl::Status CudaExecutor::CudaMulticastMemory::SubscribeDevice(\n }\n \n absl::StatusOr<void*> CudaExecutor::CudaMulticastMemory::MapMemory(\n-    const DeviceMemoryBase& location, const GpuExecutor* gpu_executor) {\n+    const DeviceAddressBase& location, const GpuExecutor* gpu_executor) {\n   const CudaExecutor* cuda_executor =\n       dynamic_cast<const CudaExecutor*>(gpu_executor);\n   if (cuda_executor == nullptr) {"
        },
        {
            "sha": "4c92d0eac36255c52c89acf3de4439ad67b314ce",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -40,8 +40,8 @@ limitations under the License.\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/cuda/cuda_context.h\"\n #include \"xla/stream_executor/cuda/cuda_kernel.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -68,18 +68,18 @@ class CudaExecutor : public GpuExecutor {\n   std::unique_ptr<ActivateContext> Activate() override;\n   absl::Status Init() override;\n   bool SynchronizeAllActivity() override;\n-  absl::StatusOr<DeviceMemoryBase> GetMemoryRange(\n-      const DeviceMemoryBase& location) const override;\n+  absl::StatusOr<DeviceAddressBase> GetMemoryRange(\n+      const DeviceAddressBase& location) const override;\n   absl::StatusOr<std::unique_ptr<EventBasedTimer>> CreateEventBasedTimer(\n       Stream* stream, bool use_delay_kernel) override;\n-  absl::StatusOr<DeviceMemoryBase> GetSymbol(\n+  absl::StatusOr<DeviceAddressBase> GetSymbol(\n       const std::string& symbol_name, ModuleHandle module_handle) override;\n-  absl::Status SynchronousMemZero(DeviceMemoryBase* location,\n+  absl::Status SynchronousMemZero(DeviceAddressBase* location,\n                                   uint64_t size) override;\n-  absl::Status SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n+  absl::Status SynchronousMemcpy(DeviceAddressBase* gpu_dst,\n                                  const void* host_src, uint64_t size) override;\n   absl::Status SynchronousMemcpy(void* host_dst,\n-                                 const DeviceMemoryBase& gpu_src,\n+                                 const DeviceAddressBase& gpu_src,\n                                  uint64_t size) override;\n   void DeallocateStream(Stream* stream) override;\n   absl::Status EnablePeerAccessTo(StreamExecutor* other) override;\n@@ -91,10 +91,10 @@ class CudaExecutor : public GpuExecutor {\n   absl::StatusOr<ModuleHandle> LoadModule(\n       const MultiModuleLoaderSpec& spec) override;\n   bool UnloadModule(ModuleHandle module_handle) override;\n-  absl::StatusOr<std::shared_ptr<DeviceMemoryBase>> CreateOrShareConstant(\n+  absl::StatusOr<std::shared_ptr<DeviceAddressBase>> CreateOrShareConstant(\n       Stream* stream, absl::Span<const uint8_t> content) override;\n-  DeviceMemoryBase Allocate(uint64_t size, int64_t memory_space) override;\n-  void Deallocate(DeviceMemoryBase* mem) override;\n+  DeviceAddressBase Allocate(uint64_t size, int64_t memory_space) override;\n+  void Deallocate(DeviceAddressBase* mem) override;\n   blas::BlasSupport* AsBlas() override;\n   fft::FftSupport* AsFft() override;\n   dnn::DnnSupport* AsDnn() override;\n@@ -176,7 +176,7 @@ class CudaExecutor : public GpuExecutor {\n \n     absl::Status SubscribeDevice(int device_number) override;\n \n-    absl::StatusOr<void*> MapMemory(const DeviceMemoryBase& location,\n+    absl::StatusOr<void*> MapMemory(const DeviceAddressBase& location,\n                                     const GpuExecutor* gpu_executor) override;\n \n    private:\n@@ -238,7 +238,7 @@ class CudaExecutor : public GpuExecutor {\n   // On-device constants that can be shared between multiple executables. A\n   // pointer for a given constant will expire when no executables require use\n   // of that constant anymore.\n-  std::map<const absl::uint128, std::weak_ptr<DeviceMemoryBase>>\n+  std::map<const absl::uint128, std::weak_ptr<DeviceAddressBase>>\n       shared_constants_ ABSL_GUARDED_BY(shared_constants_mu_);\n \n   // Kernel -> loaded GPU module. Many kernels may load the same binary."
        },
        {
            "sha": "1bd00cb53a35bbe7f1243ad81ceccd30ddc2a288",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/stream_executor/cuda/cuda_executor.h\"\n #include \"xla/stream_executor/cuda/cuda_executor_multigpu_test_kernels.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n #include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -42,9 +42,9 @@ using ::absl_testing::StatusIs;\n using ::testing::NotNull;\n \n template <typename T>\n-absl::StatusOr<stream_executor::DeviceMemoryBase> AllocateInitializedMemory(\n+absl::StatusOr<stream_executor::DeviceAddressBase> AllocateInitializedMemory(\n     CudaExecutor* executor, size_t size, size_t offset, T value) {\n-  stream_executor::DeviceMemoryBase device_memory = executor->Allocate(\n+  stream_executor::DeviceAddressBase device_memory = executor->Allocate(\n       size + offset, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n   if (device_memory.opaque() == nullptr) {\n     return absl::InternalError(\"Failed to allocate memory.\");\n@@ -61,7 +61,7 @@ absl::StatusOr<stream_executor::DeviceMemoryBase> AllocateInitializedMemory(\n \n template <typename T>\n absl::Status CheckMemory(CudaExecutor* executor,\n-                         stream_executor::DeviceMemoryBase device_memory,\n+                         stream_executor::DeviceAddressBase device_memory,\n                          T expected_value) {\n   size_t num_elements = device_memory.size() / sizeof(T);\n   std::vector<T> device_memory_vector(num_elements, 0);\n@@ -108,7 +108,7 @@ TEST(CudaExecutorMultiGpuTest, AllDevicesMustBeSubscribedBeforeMapping) {\n   TF_ASSERT_OK_AND_ASSIGN(multicast_memory,\n                           executors[0]->CreateMulticastMemory(1024, 2));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n-  DeviceMemoryBase device_memory(reinterpret_cast<void*>(1), 1);\n+  DeviceAddressBase device_memory(reinterpret_cast<void*>(1), 1);\n   EXPECT_THAT(multicast_memory->MapMemory(device_memory, executors[0]),\n               StatusIs(absl::StatusCode::kFailedPrecondition,\n                        \"All devices should be subscribed.\"));\n@@ -147,7 +147,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingNonVmmMemory) {\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n   EXPECT_THAT(multicast_memory->SubscribeDevice(1), IsOk());\n \n-  DeviceMemoryBase device_memory = executors[0]->Allocate(8, 0);\n+  DeviceAddressBase device_memory = executors[0]->Allocate(8, 0);\n   EXPECT_THAT(\n       multicast_memory->MapMemory(device_memory, executors[0]),\n       StatusIs(absl::StatusCode::kInternal,\n@@ -172,17 +172,17 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingVmmMemory) {\n   EXPECT_THAT(multicast_memory->SubscribeDevice(1), IsOk());\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      stream_executor::DeviceMemoryBase first_device_memory,\n+      stream_executor::DeviceAddressBase first_device_memory,\n       AllocateInitializedMemory(executors[0], kMemorySize, 0, kValue));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      stream_executor::DeviceMemoryBase output_device_memory,\n+      stream_executor::DeviceAddressBase output_device_memory,\n       AllocateInitializedMemory(executors[0], kMemorySize, 0, 0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       void* first_device_multicast_ptr,\n       multicast_memory->MapMemory(first_device_memory, executors[0]));\n   TF_ASSERT_OK_AND_ASSIGN(\n-      stream_executor::DeviceMemoryBase second_device_memory,\n+      stream_executor::DeviceAddressBase second_device_memory,\n       AllocateInitializedMemory(executors[1], kMemorySize, 0, kValue));\n   EXPECT_THAT(multicast_memory->MapMemory(second_device_memory, executors[1]),\n               IsOkAndHolds(NotNull()));\n@@ -219,7 +219,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryMapDifferentSlicesUnaligned) {\n                           executors[0]->GetVmmGranularity());\n   // Allocate memory with unaligned offset.\n   TF_ASSERT_OK_AND_ASSIGN(\n-      stream_executor::DeviceMemoryBase first_device_mapped_memory,\n+      stream_executor::DeviceAddressBase first_device_mapped_memory,\n       AllocateInitializedMemory(\n           executors[0],\n           // Add granularity to make sure that there is\n@@ -253,18 +253,18 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryMapDifferentSlices) {\n   TF_ASSERT_OK_AND_ASSIGN(size_t vmm_granularity,\n                           executors[0]->GetVmmGranularity());\n   TF_ASSERT_OK_AND_ASSIGN(\n-      stream_executor::DeviceMemoryBase first_device_mapped_memory,\n+      stream_executor::DeviceAddressBase first_device_mapped_memory,\n       AllocateInitializedMemory(executors[0], kMappedMemorySize,\n                                 vmm_granularity, kValue));\n   TF_ASSERT_OK_AND_ASSIGN(\n-      stream_executor::DeviceMemoryBase output_device_memory,\n+      stream_executor::DeviceAddressBase output_device_memory,\n       AllocateInitializedMemory(executors[0], kMappedMemorySize, 0, 0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       void* first_device_multicast_ptr,\n       multicast_memory->MapMemory(first_device_mapped_memory, executors[0]));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      stream_executor::DeviceMemoryBase second_device_mapped_memory,\n+      stream_executor::DeviceAddressBase second_device_mapped_memory,\n       AllocateInitializedMemory(executors[1], kMappedMemorySize, 0, kValue));\n   EXPECT_THAT(\n       multicast_memory->MapMemory(second_device_mapped_memory, executors[1]),"
        },
        {
            "sha": "076495e91cf41e482144900d37682149fecc1e0a",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,8 +25,8 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n@@ -205,13 +205,13 @@ TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithHostMemory) {\n               absl_testing::IsOkAndHolds(MemoryType::kHost));\n }\n \n-TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithDeviceMemory) {\n+TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithDeviceAddress) {\n   TF_ASSERT_OK_AND_ASSIGN(Platform * platform,\n                           PlatformManager::PlatformWithName(\"CUDA\"));\n   TF_ASSERT_OK_AND_ASSIGN(StreamExecutor * executor,\n                           platform->ExecutorForDevice(0));\n \n-  DeviceMemoryBase allocation = executor->Allocate(256);\n+  DeviceAddressBase allocation = executor->Allocate(256);\n   EXPECT_NE(allocation.opaque(), nullptr);\n   EXPECT_THAT(executor->GetPointerMemorySpace(allocation.opaque()),\n               absl_testing::IsOkAndHolds(MemoryType::kDevice));\n@@ -225,7 +225,7 @@ TEST(CudaExecutorTest, AllocateMemoryWithVmmApi) {\n \n   auto cuda_executor = dynamic_cast<CudaExecutor*>(executor);\n   ASSERT_NE(cuda_executor, nullptr);\n-  DeviceMemoryBase ptr =\n+  DeviceAddressBase ptr =\n       cuda_executor->Allocate(1024, static_cast<int>(MemoryType::kP2P));\n \n   EXPECT_NE(ptr.opaque(), nullptr);\n@@ -247,7 +247,7 @@ TEST(CudaExecutorTest,\n \n   auto cuda_executor = dynamic_cast<CudaExecutor*>(executor);\n   ASSERT_NE(cuda_executor, nullptr);\n-  DeviceMemoryBase ptr =\n+  DeviceAddressBase ptr =\n       cuda_executor->Allocate(1024, static_cast<int>(MemoryType::kDevice));\n \n   EXPECT_NE(ptr.opaque(), nullptr);"
        },
        {
            "sha": "5500b2c4586cdeaf0027a91edf1af925e80da967",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_fft.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_fft.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_fft.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_fft.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/cuda/cuda_helpers.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/fft.h\"\n #include \"xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"xla/stream_executor/platform/initialize.h\"\n@@ -349,12 +349,12 @@ void CUDAFft::UpdatePlanWithScratchAllocator(\n }\n \n template <typename FuncT, typename InputT, typename OutputT>\n-bool CUDAFft::DoFftInternal(Stream *stream, fft::Plan *plan, FuncT cufftExec,\n-                            const DeviceMemory<InputT> &input,\n-                            DeviceMemory<OutputT> *output) {\n+bool CUDAFft::DoFftInternal(Stream* stream, fft::Plan* plan, FuncT cufftExec,\n+                            const DeviceAddress<InputT>& input,\n+                            DeviceAddress<OutputT>* output) {\n   CUDAFftPlan *cuda_fft_plan = dynamic_cast<CUDAFftPlan *>(plan);\n \n-  DeviceMemory<InputT> input_maybe_copy = input;\n+  DeviceAddress<InputT> input_maybe_copy = input;\n \n   if (cuda_fft_plan == nullptr) {\n     LOG(ERROR) << \"The passed-in plan is not a CUDAFftPlan object.\";\n@@ -380,7 +380,7 @@ bool CUDAFft::DoFftInternal(Stream *stream, fft::Plan *plan, FuncT cufftExec,\n       auto allocated = allocator->AllocateBytes(input.size());\n       if (allocated.ok()) {\n         if (stream->Memcpy(&allocated.value(), input, input.size()).ok()) {\n-          input_maybe_copy = DeviceMemory<InputT>(allocated.value());\n+          input_maybe_copy = DeviceAddress<InputT>(allocated.value());\n         }\n       }\n       // Keep going even the workaround fails, since we don't have a good\n@@ -405,10 +405,10 @@ bool CUDAFft::DoFftInternal(Stream *stream, fft::Plan *plan, FuncT cufftExec,\n }\n \n template <typename FuncT, typename InputT, typename OutputT>\n-bool CUDAFft::DoFftWithDirectionInternal(Stream *stream, fft::Plan *plan,\n+bool CUDAFft::DoFftWithDirectionInternal(Stream* stream, fft::Plan* plan,\n                                          FuncT cufftExec,\n-                                         const DeviceMemory<InputT> &input,\n-                                         DeviceMemory<OutputT> *output) {\n+                                         const DeviceAddress<InputT>& input,\n+                                         DeviceAddress<OutputT>* output) {\n   CUDAFftPlan *cuda_fft_plan = dynamic_cast<CUDAFftPlan *>(plan);\n   if (cuda_fft_plan == nullptr) {\n     LOG(ERROR) << \"The passed-in plan is not a CUDAFftPlan object.\";\n@@ -435,20 +435,20 @@ bool CUDAFft::DoFftWithDirectionInternal(Stream *stream, fft::Plan *plan,\n \n #define STREAM_EXECUTOR_CUDA_DEFINE_FFT(__type, __fft_type1, __fft_type2,      \\\n                                         __fft_type3)                           \\\n-  bool CUDAFft::DoFft(Stream *stream, fft::Plan *plan,                         \\\n-                      const DeviceMemory<std::complex<__type>> &input,         \\\n-                      DeviceMemory<std::complex<__type>> *output) {            \\\n+  bool CUDAFft::DoFft(Stream* stream, fft::Plan* plan,                         \\\n+                      const DeviceAddress<std::complex<__type>>& input,        \\\n+                      DeviceAddress<std::complex<__type>>* output) {           \\\n     return DoFftWithDirectionInternal(stream, plan, cufftExec##__fft_type1,    \\\n                                       input, output);                          \\\n   }                                                                            \\\n-  bool CUDAFft::DoFft(Stream *stream, fft::Plan *plan,                         \\\n-                      const DeviceMemory<__type> &input,                       \\\n-                      DeviceMemory<std::complex<__type>> *output) {            \\\n+  bool CUDAFft::DoFft(Stream* stream, fft::Plan* plan,                         \\\n+                      const DeviceAddress<__type>& input,                      \\\n+                      DeviceAddress<std::complex<__type>>* output) {           \\\n     return DoFftInternal(stream, plan, cufftExec##__fft_type2, input, output); \\\n   }                                                                            \\\n-  bool CUDAFft::DoFft(Stream *stream, fft::Plan *plan,                         \\\n-                      const DeviceMemory<std::complex<__type>> &input,         \\\n-                      DeviceMemory<__type> *output) {                          \\\n+  bool CUDAFft::DoFft(Stream* stream, fft::Plan* plan,                         \\\n+                      const DeviceAddress<std::complex<__type>>& input,        \\\n+                      DeviceAddress<__type>* output) {                         \\\n     return DoFftInternal(stream, plan, cufftExec##__fft_type3, input, output); \\\n   }\n "
        },
        {
            "sha": "b3c0e5590a4c1344433fe8721f9a6bf454921cde",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_fft.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_fft.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_fft.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_fft.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -81,7 +81,7 @@ class CUDAFftPlan : public fft::Plan {\n   StreamExecutor* parent_;\n   cufftHandle plan_;\n   fft::Type fft_type_;\n-  DeviceMemory<uint8_t> scratch_;\n+  DeviceAddress<uint8_t> scratch_;\n   size_t scratch_size_bytes_;\n   bool is_initialized_;\n   ScratchAllocator* scratch_allocator_;\n@@ -114,15 +114,15 @@ class CUDAFft : public fft::FftSupport {\n   template <typename FuncT, typename InputT, typename OutputT>\n   bool DoFftWithDirectionInternal(Stream* stream, fft::Plan* plan,\n                                   FuncT cufft_exec,\n-                                  const DeviceMemory<InputT>& input,\n-                                  DeviceMemory<OutputT>* output);\n+                                  const DeviceAddress<InputT>& input,\n+                                  DeviceAddress<OutputT>* output);\n \n   // This is for complex to real or real to complex FFT, when the direction\n   // is implied.\n   template <typename FuncT, typename InputT, typename OutputT>\n   bool DoFftInternal(Stream* stream, fft::Plan* plan, FuncT cufft_exec,\n-                     const DeviceMemory<InputT>& input,\n-                     DeviceMemory<OutputT>* output);\n+                     const DeviceAddress<InputT>& input,\n+                     DeviceAddress<OutputT>* output);\n \n   CUDAFft(const CUDAFft&) = delete;\n   void operator=(const CUDAFft&) = delete;"
        },
        {
            "sha": "2099b542336a3ee107770247fbb4df5c01c4a263",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_kernel.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_kernel.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -124,7 +124,7 @@ absl::Status CudaKernel::Launch(const ThreadDim& thread_dims,\n   }\n \n   // For device memory array we rely on a custom kernel arguments packing.\n-  if (auto* device_mem = DynCast<KernelArgsDeviceMemoryArray>(&args)) {\n+  if (auto* device_mem = DynCast<KernelArgsDeviceAddressArray>(&args)) {\n     auto& pack = args_packing();\n     if (!pack) {\n       return absl::InternalError("
        },
        {
            "sha": "40978d58251749ab67c58d2cdb5c2e4121325586",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_stream.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_context.h\"\n #include \"xla/stream_executor/cuda/cuda_event.h\"\n #include \"xla/stream_executor/cuda/cuda_status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -262,7 +262,7 @@ absl::Status CudaStream::BlockHostUntilDone() {\n   return absl::OkStatus();\n }\n \n-absl::Status CudaStream::Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+absl::Status CudaStream::Memset32(DeviceAddressBase* location, uint32_t pattern,\n                                   uint64_t size) {\n   if (absl::bit_cast<uintptr_t>(location->opaque()) % alignof(uint32_t) != 0) {\n     return absl::InvalidArgumentError(\"location must be 4 byte aligned.\");\n@@ -277,7 +277,7 @@ absl::Status CudaStream::Memset32(DeviceMemoryBase* location, uint32_t pattern,\n       \"Failed to enqueue async memset operation\");\n }\n \n-absl::Status CudaStream::MemZero(DeviceMemoryBase* location, uint64_t size) {\n+absl::Status CudaStream::MemZero(DeviceAddressBase* location, uint64_t size) {\n   if (reinterpret_cast<uintptr_t>(location->opaque()) % alignof(uint32_t) ==\n           0 &&\n       size % sizeof(uint32_t) == 0) {\n@@ -291,22 +291,23 @@ absl::Status CudaStream::MemZero(DeviceMemoryBase* location, uint64_t size) {\n   }\n }\n \n-absl::Status CudaStream::Memcpy(DeviceMemoryBase* gpu_dst,\n-                                const DeviceMemoryBase& gpu_src,\n+absl::Status CudaStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   return AsynchronousMemcpyD2D(\n       executor_, absl::bit_cast<CUdeviceptr>(gpu_dst->opaque()),\n       absl::bit_cast<CUdeviceptr>(gpu_src.opaque()), size, stream_handle_);\n }\n \n-absl::Status CudaStream::Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n-                                uint64_t size) {\n+absl::Status CudaStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const void* host_src, uint64_t size) {\n   return AsynchronousMemcpyH2D(executor_,\n                                absl::bit_cast<CUdeviceptr>(gpu_dst->opaque()),\n                                host_src, size, stream_handle_);\n }\n \n-absl::Status CudaStream::Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+absl::Status CudaStream::Memcpy(void* host_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   return AsynchronousMemcpyD2H(executor_, host_dst,\n                                absl::bit_cast<CUdeviceptr>(gpu_src.opaque()),"
        },
        {
            "sha": "614142053bd7fa44456055adb8b95ed127edd0a4",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_stream.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"third_party/gpus/cuda/include/cuda.h\"\n #include \"xla/stream_executor/cuda/cuda_event.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -49,15 +49,15 @@ class CudaStream : public StreamCommon {\n   absl::Status RecordEvent(Event* event) override;\n   absl::Status WaitFor(Event* event) override;\n \n-  absl::Status Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+  absl::Status Memset32(DeviceAddressBase* location, uint32_t pattern,\n                         uint64_t size) override;\n-  absl::Status MemZero(DeviceMemoryBase* location, uint64_t size) override;\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n+  absl::Status MemZero(DeviceAddressBase* location, uint64_t size) override;\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst, const void* host_src,\n                       uint64_t size) override;\n-  absl::Status Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+  absl::Status Memcpy(void* host_dst, const DeviceAddressBase& gpu_src,\n                       uint64_t size) override;\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst,\n-                      const DeviceMemoryBase& gpu_src, uint64_t size) override;\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst,\n+                      const DeviceAddressBase& gpu_src, uint64_t size) override;\n   absl::Status DoHostCallbackWithStatus(\n       absl::AnyInvocable<absl::Status() &&> callback) override;\n   absl::Status BlockHostUntilDone() override;"
        },
        {
            "sha": "f25ec5a2996f578c22858e19fd528da829a8308c",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_stream_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_event.h\"\n #include \"xla/stream_executor/cuda/cuda_executor.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -67,7 +67,7 @@ class CudaStreamTest : public ::testing::Test {\n \n TEST_F(CudaStreamTest, Memset32) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer =\n+  DeviceAddress<uint32_t> buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<CudaStream> stream,\n@@ -80,7 +80,7 @@ TEST_F(CudaStreamTest, Memset32) {\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n \n   // Should fail due to the non-4-byte-aligned pointer.\n-  DeviceMemoryBase unaligned_pointer =\n+  DeviceAddressBase unaligned_pointer =\n       buffer.GetByteSlice(/*offset_bytes=*/1, /*size_bytes=*/0);\n   EXPECT_THAT(stream->Memset32(&unaligned_pointer, 0xDEADBEEF,\n                                kBufferNumElements * sizeof(uint32_t) + 1),\n@@ -101,7 +101,7 @@ TEST_F(CudaStreamTest, Memset32) {\n \n TEST_F(CudaStreamTest, MemZero) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer =\n+  DeviceAddress<uint32_t> buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<CudaStream> stream,\n@@ -134,7 +134,7 @@ TEST_F(CudaStreamTest, MemZero) {\n \n TEST_F(CudaStreamTest, MemcpyHostToDeviceAndBack) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer =\n+  DeviceAddress<uint32_t> buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<CudaStream> stream,\n@@ -158,9 +158,9 @@ TEST_F(CudaStreamTest, MemcpyHostToDeviceAndBack) {\n \n TEST_F(CudaStreamTest, MemcpyDeviceToDevice) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer1 =\n+  DeviceAddress<uint32_t> buffer1 =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n-  DeviceMemory<uint32_t> buffer2 =\n+  DeviceAddress<uint32_t> buffer2 =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<CudaStream> stream,\n@@ -207,9 +207,9 @@ TEST_F(CudaStreamTest, LaunchKernel) {\n   constexpr int64_t kByteLength = sizeof(int32_t) * kLength;\n \n   // Prepare arguments: a=1, b=2, c=0\n-  DeviceMemory<int32_t> a = executor_->AllocateArray<int32_t>(kLength, 0);\n-  DeviceMemory<int32_t> b = executor_->AllocateArray<int32_t>(kLength, 0);\n-  DeviceMemory<int32_t> c = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> a = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> b = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> c = executor_->AllocateArray<int32_t>(kLength, 0);\n \n   EXPECT_THAT(stream->Memset32(&a, 1, kByteLength), absl_testing::IsOk());\n   EXPECT_THAT(stream->Memset32(&b, 2, kByteLength), absl_testing::IsOk());"
        },
        {
            "sha": "92ed18b883a96d478a6d2b7c97f809a742797a7b",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_timer_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_timer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_timer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_timer_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/time/time.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -47,9 +47,9 @@ class CudaTimerTest : public ::testing::TestWithParam<CudaTimer::TimerType> {\n     int64_t byte_length = sizeof(int32_t) * length;\n \n     // Prepare arguments: a=1, b=2, c=0\n-    DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-    DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-    DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n \n     ASSERT_THAT(stream->Memset32(&a, 1, byte_length), absl_testing::IsOk());\n     ASSERT_THAT(stream->Memset32(&b, 2, byte_length), absl_testing::IsOk());"
        },
        {
            "sha": "b06499c152547fbec5073e78f9a9f9de0cd202d6",
            "filename": "third_party/xla/xla/stream_executor/cuda/delay_kernel_cuda.cu.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fdelay_kernel_cuda.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fdelay_kernel_cuda.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fdelay_kernel_cuda.cu.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -60,7 +60,7 @@ absl::StatusOr<GpuSemaphore> LaunchDelayKernel(Stream* stream) {\n   // multiple GpuTimer objects.\n   TF_ASSIGN_OR_RETURN(\n       auto kernel,\n-      (TypedKernelFactory<DeviceMemory<GpuSemaphoreState>,\n+      (TypedKernelFactory<DeviceAddress<GpuSemaphoreState>,\n                           GpuSemaphoreState>::Create(executor, \"DelayKernel\",\n                                                      reinterpret_cast<void*>(\n                                                          DelayKernel))));"
        },
        {
            "sha": "c4409f8732f4ba0c1bf55ea2e7507c4989a412ce",
            "filename": "third_party/xla/xla/stream_executor/cuda/gpu_test_kernels_cuda.cu.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fgpu_test_kernels_cuda.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fgpu_test_kernels_cuda.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fgpu_test_kernels_cuda.cu.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -78,9 +78,10 @@ GPU_KERNEL_REGISTRY_REGISTER_KERNEL_STATICALLY(\n           \"AddI32Ptrs3\", arity,\n           [&](const stream_executor::Kernel& kernel,\n               const stream_executor::KernelArgs& args) {\n-            auto bufs = stream_executor::Cast<\n-                            stream_executor::KernelArgsDeviceMemoryArray>(&args)\n-                            ->device_memory_args();\n+            auto bufs =\n+                stream_executor::Cast<\n+                    stream_executor::KernelArgsDeviceAddressArray>(&args)\n+                    ->device_memory_args();\n             auto cast = [](auto m) {\n               return reinterpret_cast<int32_t*>(m.opaque());\n             };"
        },
        {
            "sha": "2def3569a28d54c935b4430cae501215750379ed",
            "filename": "third_party/xla/xla/stream_executor/device_memory.h",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_memory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_memory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_memory.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -28,7 +28,7 @@ limitations under the License.\n \n #include \"absl/base/macros.h\"\n #include \"xla/stream_executor/device_address.h\"\n-#include \"xla/stream_executor/gpu/tensor_map.h\"\n+#include \"xla/stream_executor/tensor_map.h\"  // IWYU pragma: keep\n \n namespace stream_executor {\n \n@@ -39,8 +39,6 @@ template <typename T>\n using DeviceMemory ABSL_DEPRECATE_AND_INLINE() =\n     ::stream_executor::DeviceAddress<T>;\n \n-using TensorMap ABSL_DEPRECATE_AND_INLINE() = ::stream_executor::gpu::TensorMap;\n-\n }  // namespace stream_executor\n \n #endif  // XLA_STREAM_EXECUTOR_DEVICE_MEMORY_H_"
        },
        {
            "sha": "f38a2597972d75b819da8e872703d21fd441ddb7",
            "filename": "third_party/xla/xla/stream_executor/dnn.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -39,7 +39,7 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/data_type.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -137,11 +137,11 @@ absl::Status DnnSupport::GetConvolveRunners(\n     dnn::ConvolutionKind /*kind*/, dnn::DataType /*input_type*/,\n     dnn::DataType /*output_type*/, Stream* /*stream*/,\n     const dnn::BatchDescriptor& /*input_descriptor*/,\n-    DeviceMemoryBase /*input_data*/,\n+    DeviceAddressBase /*input_data*/,\n     const dnn::FilterDescriptor& /*filter_descriptor*/,\n-    DeviceMemoryBase /*filter_data*/,\n+    DeviceAddressBase /*filter_data*/,\n     const dnn::BatchDescriptor& /*output_descriptor*/,\n-    DeviceMemoryBase /*output_data*/,\n+    DeviceAddressBase /*output_data*/,\n     const dnn::ConvolutionDescriptor& /*convolution_descriptor*/,\n     bool /*use_fallback*/, ScratchAllocator* /*scratch_allocator*/,\n     const EngineOptions& /*engine_options*/,\n@@ -247,11 +247,11 @@ bool DnnSupport::GetMIOpenConvolveAlgorithms(\n     dnn::ConvolutionKind /*kind*/, dnn::DataType /*element_type*/,\n     dnn::DataType /*output_type*/, Stream* /*stream*/,\n     const dnn::BatchDescriptor& /*input_descriptor*/,\n-    DeviceMemoryBase input_data,\n+    DeviceAddressBase input_data,\n     const dnn::FilterDescriptor& /*filter_descriptor*/,\n-    DeviceMemoryBase filter_data,\n+    DeviceAddressBase filter_data,\n     const dnn::BatchDescriptor& /*output_descriptor*/,\n-    DeviceMemoryBase output_data,\n+    DeviceAddressBase output_data,\n     const dnn::ConvolutionDescriptor& /*convolution_descriptor*/,\n     ScratchAllocator* scratch_allocator,\n     std::vector<ProfileResult>* /*out_algorithms*/) {\n@@ -266,9 +266,9 @@ absl::Status DnnSupport::DoPoolForward(\n     DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n     const EngineOptions& engine_options,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, ScratchAllocator* workspace_allocator) {\n   // Ignore numeric options. Subclasses can override this method to use it.\n   return DoPoolForward(element_type, stream, pooling_dimensions,\n                        input_dimensions, input_data, output_dimensions,\n@@ -279,10 +279,10 @@ absl::Status DnnSupport::DoPoolBackward(\n     DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n     const EngineOptions& engine_options,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    DeviceMemoryBase input_diff_data, DeviceMemoryBase output_diff_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, DeviceAddressBase input_diff_data,\n+    DeviceAddressBase output_diff_data, ScratchAllocator* workspace_allocator) {\n   // Ignore numeric options. Subclasses can override this method to use it.\n   return DoPoolBackward(element_type, stream, pooling_dimensions,\n                         input_dimensions, input_data, output_dimensions,\n@@ -940,11 +940,11 @@ bool DnnSupport::IsStatusOk(const absl::Status& status, bool report_error) {\n absl::Status DnnSupport::DoCtcLoss(\n     Stream* stream, dnn::DataType element_type,\n     const RnnStateTensorDescriptor& probs_desc,\n-    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+    const DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n     absl::Span<const int> labels_lengths_data,\n-    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n-    const RnnStateTensorDescriptor& grads_desc, DeviceMemoryBase grads_data,\n-    DeviceMemory<uint8_t> scratch_memory, int ctc_loss_algo_id) {\n+    absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n+    const RnnStateTensorDescriptor& grads_desc, DeviceAddressBase grads_data,\n+    DeviceAddress<uint8_t> scratch_memory, int ctc_loss_algo_id) {\n   return absl::UnimplementedError(\"CtcLoss not implemented\");\n }\n "
        },
        {
            "sha": "f9e7edb5e45fd08317f926fd3f633c1761441092",
            "filename": "third_party/xla/xla/stream_executor/dnn.h",
            "status": "modified",
            "additions": 211,
            "deletions": 206,
            "changes": 417,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -41,8 +41,8 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/data_type.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -842,32 +842,32 @@ class OpRunner<void(Args...)> {\n \n   // Launch the operation, with the signature determined by `Sig`.\n   virtual absl::Status operator()(Stream*, ProfileResult*,\n-                                  DeviceMemoryBase scratch_memory,\n+                                  DeviceAddressBase scratch_memory,\n                                   Args... args) const = 0;\n };\n \n-using ConvSignature = void(DeviceMemoryBase /* input_data */,\n-                           DeviceMemoryBase /* filter_data */,\n-                           DeviceMemoryBase /* output_data */);\n+using ConvSignature = void(DeviceAddressBase /* input_data */,\n+                           DeviceAddressBase /* filter_data */,\n+                           DeviceAddressBase /* output_data */);\n using ConvRunner = OpRunner<ConvSignature>;\n \n-using GraphConvSignature = void(std::vector<DeviceMemoryBase>);\n+using GraphConvSignature = void(std::vector<DeviceAddressBase>);\n using GraphConvRunner = OpRunner<GraphConvSignature>;\n \n-using FusedConvSignature = void(DeviceMemoryBase /* input_data */,\n-                                DeviceMemoryBase /* filter_data */,\n-                                DeviceMemoryBase /* side_input_data */,\n-                                DeviceMemoryBase /* bias_data */,\n-                                DeviceMemoryBase /* output_data */);\n+using FusedConvSignature = void(DeviceAddressBase /* input_data */,\n+                                DeviceAddressBase /* filter_data */,\n+                                DeviceAddressBase /* side_input_data */,\n+                                DeviceAddressBase /* bias_data */,\n+                                DeviceAddressBase /* output_data */);\n using FusedConvRunner = OpRunner<FusedConvSignature>;\n \n-using FusedMatmulSignature = void(DeviceMemoryBase /* a_data */,\n-                                  DeviceMemoryBase /* b_data */,\n-                                  DeviceMemoryBase /* bias_data */,\n-                                  DeviceMemoryBase /* c_data */);\n+using FusedMatmulSignature = void(DeviceAddressBase /* a_data */,\n+                                  DeviceAddressBase /* b_data */,\n+                                  DeviceAddressBase /* bias_data */,\n+                                  DeviceAddressBase /* c_data */);\n using FusedMatmulRunner = OpRunner<FusedMatmulSignature>;\n \n-using NormSignature = void(std::vector<DeviceMemoryBase>);\n+using NormSignature = void(std::vector<DeviceAddressBase>);\n using NormRunner = OpRunner<NormSignature>;\n \n // Describes the configuration for the algorithms that will used.\n@@ -1095,15 +1095,15 @@ class DnnGraph {\n   virtual absl::Status Prepare(DnnSupport&, const EngineOptions&) = 0;\n   virtual absl::Status Build(DnnSupport&, std::optional<int64_t> plan_id) = 0;\n   virtual absl::Status Execute(Stream& stream,\n-                               absl::Span<DeviceMemoryBase> operands,\n+                               absl::Span<DeviceAddressBase> operands,\n                                int64_t local_device_ordinal) const = 0;\n   virtual void InitDropoutState(int64_t local_device_count, int64_t seed,\n                                 int64_t increment) = 0;\n   virtual absl::StatusOr<bool> SupportsExplicitCommandBufferConstruction()\n       const = 0;\n   using RawCommandBufferHandle = void*;\n   virtual absl::Status PopulateOrUpdateRawCommandBuffer(\n-      Stream&, absl::Span<DeviceMemoryBase> operands, RawCommandBufferHandle,\n+      Stream&, absl::Span<DeviceAddressBase> operands, RawCommandBufferHandle,\n       bool do_update) = 0;\n };\n \n@@ -1169,16 +1169,16 @@ class DnnSupport {\n   //    in the backward gradient computation.\n   //  is_training: Set to true for training, false for inference.\n   virtual bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<float>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<float>& side_input, const BatchDescriptor& x_desc,\n+      Stream* stream, const DeviceAddress<float>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<float>& side_input, const BatchDescriptor& x_desc,\n       const BatchDescriptor& scale_offset_desc, const double epsilon,\n       const double exponential_average_factor, ActivationMode activation_mode,\n-      DeviceMemory<float>* y, DeviceMemory<float>* batch_mean,\n-      DeviceMemory<float>* batch_var, DeviceMemory<float>* reserve_space_1,\n-      DeviceMemory<float>* reserve_space_2, bool is_training,\n+      DeviceAddress<float>* y, DeviceAddress<float>* batch_mean,\n+      DeviceAddress<float>* batch_var, DeviceAddress<float>* reserve_space_1,\n+      DeviceAddress<float>* reserve_space_2, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) {\n     return false;\n@@ -1187,17 +1187,17 @@ class DnnSupport {\n   // Performs a half-precision forwards batch normalization operation onto the\n   // stream. See DoBatchNormalizationForward above for argument details.\n   virtual bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<Eigen::half>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<Eigen::half>& side_input,\n+      Stream* stream, const DeviceAddress<Eigen::half>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<Eigen::half>& side_input,\n       const BatchDescriptor& x_desc, const BatchDescriptor& scale_offset_desc,\n       const double epsilon, const double exponential_average_factor,\n-      ActivationMode activation_mode, DeviceMemory<Eigen::half>* y,\n-      DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-      DeviceMemory<float>* reserve_space_1,\n-      DeviceMemory<float>* reserve_space_2, bool is_training,\n+      ActivationMode activation_mode, DeviceAddress<Eigen::half>* y,\n+      DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+      DeviceAddress<float>* reserve_space_1,\n+      DeviceAddress<float>* reserve_space_2, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) {\n     return false;\n@@ -1206,17 +1206,17 @@ class DnnSupport {\n   // Performs a bfloat16 forward batch normalization operation onto the\n   // stream. See DoBatchNormalizationForward above for argument details.\n   virtual bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<Eigen::bfloat16>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<Eigen::bfloat16>& side_input,\n+      Stream* stream, const DeviceAddress<Eigen::bfloat16>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<Eigen::bfloat16>& side_input,\n       const BatchDescriptor& x_desc, const BatchDescriptor& scale_offset_desc,\n       const double epsilon, const double exponential_average_factor,\n-      ActivationMode activation_mode, DeviceMemory<Eigen::bfloat16>* y,\n-      DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-      DeviceMemory<float>* reserve_space_1,\n-      DeviceMemory<float>* reserve_space_2, bool is_training,\n+      ActivationMode activation_mode, DeviceAddress<Eigen::bfloat16>* y,\n+      DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+      DeviceAddress<float>* reserve_space_1,\n+      DeviceAddress<float>* reserve_space_2, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) {\n     return false;\n@@ -1240,16 +1240,16 @@ class DnnSupport {\n   //  scale_backprop: gradient with respect to scale.\n   //  offset_backprop: gradient with respect to offset.\n   virtual bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<float>& y_backprop,\n-      const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var, const DeviceMemory<float>& y,\n+      Stream* stream, const DeviceAddress<float>& y_backprop,\n+      const DeviceAddress<float>& x, const DeviceAddress<float>& scale,\n+      const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+      const DeviceAddress<float>& inv_var, const DeviceAddress<float>& y,\n       const BatchDescriptor& x_desc, const BatchDescriptor& scale_offset_desc,\n       const double epsilon, ActivationMode activation_mode,\n-      DeviceMemory<float>* x_backprop, DeviceMemory<float>* scale_backprop,\n-      DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<float>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      DeviceAddress<float>* x_backprop, DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<float>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) {\n     return false;\n   }\n@@ -1258,16 +1258,17 @@ class DnnSupport {\n   // operation onto the stream. See DoBatchNormalizationBackward above for\n   // argument details.\n   virtual bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n-      const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::half>& y,\n+      Stream* stream, const DeviceAddress<Eigen::half>& y_backprop,\n+      const DeviceAddress<Eigen::half>& x, const DeviceAddress<float>& scale,\n+      const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+      const DeviceAddress<float>& inv_var, const DeviceAddress<Eigen::half>& y,\n       const BatchDescriptor& x_desc, const BatchDescriptor& scale_offset_desc,\n       const double epsilon, ActivationMode activation_mode,\n-      DeviceMemory<Eigen::half>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<Eigen::half>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      DeviceAddress<Eigen::half>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<Eigen::half>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) {\n     return false;\n   }\n@@ -1276,16 +1277,18 @@ class DnnSupport {\n   // operation onto the stream. See DoBatchNormalizationBackward above for\n   // argument details.\n   virtual bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<Eigen::bfloat16>& y_backprop,\n-      const DeviceMemory<Eigen::bfloat16>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var,\n-      const DeviceMemory<Eigen::bfloat16>& y, const BatchDescriptor& x_desc,\n+      Stream* stream, const DeviceAddress<Eigen::bfloat16>& y_backprop,\n+      const DeviceAddress<Eigen::bfloat16>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& mean, const DeviceAddress<float>& inv_var,\n+      const DeviceAddress<Eigen::bfloat16>& y, const BatchDescriptor& x_desc,\n       const BatchDescriptor& scale_offset_desc, const double epsilon,\n-      ActivationMode activation_mode, DeviceMemory<Eigen::bfloat16>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<Eigen::bfloat16>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      ActivationMode activation_mode,\n+      DeviceAddress<Eigen::bfloat16>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<Eigen::bfloat16>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) {\n     return false;\n   }\n@@ -1344,13 +1347,13 @@ class DnnSupport {\n       Stream* stream, DataType input_type, DataType side_input_type,\n       DataType bias_type, DataType output_type,\n       const BatchDescriptor& conv_input_descriptor,\n-      DeviceMemoryBase conv_input_data, double conv_input_scale,\n-      const FilterDescriptor& filter_descriptor, DeviceMemoryBase filter_data,\n+      DeviceAddressBase conv_input_data, double conv_input_scale,\n+      const FilterDescriptor& filter_descriptor, DeviceAddressBase filter_data,\n       const ConvolutionDescriptor& convolution_descriptor,\n-      DeviceMemoryBase side_input_data, double side_input_scale,\n-      const BatchDescriptor& bias_descriptor, DeviceMemoryBase biases,\n+      DeviceAddressBase side_input_data, double side_input_scale,\n+      const BatchDescriptor& bias_descriptor, DeviceAddressBase biases,\n       ActivationMode activation_mode, const BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data, ScratchAllocator* scratch_allocator,\n+      DeviceAddressBase output_data, ScratchAllocator* scratch_allocator,\n       const AlgorithmConfig& algorithm_config,\n       ProfileResult* output_profile_result) {\n     return absl::UnimplementedError(\n@@ -1361,14 +1364,15 @@ class DnnSupport {\n             typename BiasT, typename OutputT>\n   absl::Status FusedConvolveWithAlgorithm(\n       Stream* stream, const BatchDescriptor& conv_input_descriptor,\n-      const DeviceMemory<InputT>& conv_input_data, ScaleT conv_input_scale,\n+      const DeviceAddress<InputT>& conv_input_data, ScaleT conv_input_scale,\n       const FilterDescriptor& filter_descriptor,\n-      const DeviceMemory<InputT>& filter_data,\n+      const DeviceAddress<InputT>& filter_data,\n       const ConvolutionDescriptor& convolution_descriptor,\n-      const DeviceMemory<SideInputT>& side_input_data, ScaleT side_input_scale,\n-      const BatchDescriptor& bias_descriptor, const DeviceMemory<BiasT>& biases,\n-      ActivationMode activation_mode, const BatchDescriptor& output_descriptor,\n-      DeviceMemory<OutputT>* output, ScratchAllocator* scratch_allocator,\n+      const DeviceAddress<SideInputT>& side_input_data, ScaleT side_input_scale,\n+      const BatchDescriptor& bias_descriptor,\n+      const DeviceAddress<BiasT>& biases, ActivationMode activation_mode,\n+      const BatchDescriptor& output_descriptor, DeviceAddress<OutputT>* output,\n+      ScratchAllocator* scratch_allocator,\n       const AlgorithmConfig& algorithm_config,\n       ProfileResult* output_profile_result) {\n     return DoFusedConvolve(\n@@ -1385,10 +1389,10 @@ class DnnSupport {\n   // convolutions faster using Tensor Core IMMA instruction.\n   virtual absl::Status CudnnReorderConvolutionFilterAndBias(\n       Stream* stream, const FilterDescriptor& filter_descriptor,\n-      const DeviceMemory<int8_t>& filter_input,\n-      DeviceMemory<int8_t>* filter_output,\n-      std::optional<const DeviceMemory<float>> bias_input,\n-      std::optional<DeviceMemory<float>> bias_output) {\n+      const DeviceAddress<int8_t>& filter_input,\n+      DeviceAddress<int8_t>* filter_output,\n+      std::optional<const DeviceAddress<float>> bias_input,\n+      std::optional<DeviceAddress<float>> bias_output) {\n     return absl::UnimplementedError(\n         \"DnnSupport::CudnnReorderConvolutionFilterAndBias is specific to CUDA \"\n         \"convolution implementation.\");\n@@ -1397,9 +1401,9 @@ class DnnSupport {\n   virtual absl::Status GetConvolveRunners(\n       ConvolutionKind kind, DataType input_type, DataType output_type,\n       Stream* stream, const BatchDescriptor& input_descriptor,\n-      DeviceMemoryBase input_data, const FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data, const BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n+      DeviceAddressBase input_data, const FilterDescriptor& filter_descriptor,\n+      DeviceAddressBase filter_data, const BatchDescriptor& output_descriptor,\n+      DeviceAddressBase output_data,\n       const ConvolutionDescriptor& convolution_descriptor, bool use_fallback,\n       ScratchAllocator* scratch_allocator, const EngineOptions& engine_options,\n       std::vector<std::unique_ptr<const ConvRunner>>* out_exec_plans);\n@@ -1487,9 +1491,9 @@ class DnnSupport {\n   virtual bool GetMIOpenConvolveAlgorithms(\n       ConvolutionKind kind, DataType element_type, DataType output_type,\n       Stream* stream, const BatchDescriptor& input_descriptor,\n-      DeviceMemoryBase input_data, const FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data, const BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n+      DeviceAddressBase input_data, const FilterDescriptor& filter_descriptor,\n+      DeviceAddressBase filter_data, const BatchDescriptor& output_descriptor,\n+      DeviceAddressBase output_data,\n       const ConvolutionDescriptor& convolution_descriptor,\n       ScratchAllocator* scratch_allocator,\n       std::vector<ProfileResult>* out_algorithms);\n@@ -1502,9 +1506,9 @@ class DnnSupport {\n                            const PoolingDescriptor& pooling_dimensions,\n                            const EngineOptions& engine_options,\n                            const BatchDescriptor& input_dimensions,\n-                           const DeviceMemory<ElementType>& input_data,\n+                           const DeviceAddress<ElementType>& input_data,\n                            const BatchDescriptor& output_dimensions,\n-                           DeviceMemory<ElementType>* output_data,\n+                           DeviceAddress<ElementType>* output_data,\n                            ScratchAllocator* workspace_allocator = nullptr) {\n     return DoPoolForward(ToDataType<ElementType>::value, stream,\n                          pooling_dimensions, engine_options, input_dimensions,\n@@ -1517,11 +1521,11 @@ class DnnSupport {\n                             const PoolingDescriptor& pooling_dimensions,\n                             const EngineOptions& engine_options,\n                             const BatchDescriptor& input_dimensions,\n-                            const DeviceMemory<ElementType>& input_data,\n+                            const DeviceAddress<ElementType>& input_data,\n                             const BatchDescriptor& output_dimensions,\n-                            const DeviceMemory<ElementType>& output_data,\n-                            const DeviceMemory<ElementType>& input_diff_data,\n-                            DeviceMemory<ElementType>* output_diff_data,\n+                            const DeviceAddress<ElementType>& output_data,\n+                            const DeviceAddress<ElementType>& input_diff_data,\n+                            DeviceAddress<ElementType>* output_diff_data,\n                             ScratchAllocator* workspace_allocator = nullptr) {\n     return DoPoolBackward(\n         ToDataType<ElementType>::value, stream, pooling_dimensions,\n@@ -1543,34 +1547,34 @@ class DnnSupport {\n   virtual absl::Status DoPoolForward(\n       DataType element_type, Stream* stream,\n       const PoolingDescriptor& pooling_dimensions,\n-      const BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-      const BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n+      const BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+      const BatchDescriptor& output_dimensions, DeviceAddressBase output_data,\n       ScratchAllocator* workspace_allocator) = 0;\n \n   virtual absl::Status DoPoolForward(\n       DataType element_type, Stream* stream,\n       const PoolingDescriptor& pooling_dimensions,\n       const EngineOptions& engine_options,\n-      const BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-      const BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n+      const BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+      const BatchDescriptor& output_dimensions, DeviceAddressBase output_data,\n       ScratchAllocator* workspace_allocator);\n \n   // Performs differentiation of the pooling operation.\n   virtual absl::Status DoPoolBackward(\n       DataType element_type, Stream* stream,\n       const PoolingDescriptor& pooling_dimensions,\n-      const BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-      const BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-      DeviceMemoryBase input_diff_data, DeviceMemoryBase output_diff_data,\n+      const BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+      const BatchDescriptor& output_dimensions, DeviceAddressBase output_data,\n+      DeviceAddressBase input_diff_data, DeviceAddressBase output_diff_data,\n       ScratchAllocator* workspace_allocator) = 0;\n \n   virtual absl::Status DoPoolBackward(\n       DataType element_type, Stream* stream,\n       const PoolingDescriptor& pooling_dimensions,\n       const EngineOptions& engine_options,\n-      const BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-      const BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-      DeviceMemoryBase input_diff_data, DeviceMemoryBase output_diff_data,\n+      const BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+      const BatchDescriptor& output_dimensions, DeviceAddressBase output_data,\n+      DeviceAddressBase input_diff_data, DeviceAddressBase output_diff_data,\n       ScratchAllocator* workspace_allocator);\n \n   // Applies local response normalization to the values from input_data and\n@@ -1580,8 +1584,8 @@ class DnnSupport {\n   // normalization.\n   virtual bool DoNormalizeWithDimensions(\n       Stream* stream, const NormalizeDescriptor& normalize_descriptor,\n-      const BatchDescriptor& dimensions, const DeviceMemory<float>& input_data,\n-      DeviceMemory<float>* output_data) {\n+      const BatchDescriptor& dimensions, const DeviceAddress<float>& input_data,\n+      DeviceAddress<float>* output_data) {\n     return false;\n   }\n \n@@ -1599,10 +1603,10 @@ class DnnSupport {\n   // normalization.\n   virtual bool DoNormalizeBackwardWithDimensions(\n       Stream* stream, const NormalizeDescriptor& normalize_descriptor,\n-      const BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n-      const DeviceMemory<float>& normalized_data,\n-      const DeviceMemory<float>& normalized_variable_gradient,\n-      DeviceMemory<float>* raw_variable_gradient,\n+      const BatchDescriptor& dimensions, const DeviceAddress<float>& raw_data,\n+      const DeviceAddress<float>& normalized_data,\n+      const DeviceAddress<float>& normalized_variable_gradient,\n+      DeviceAddress<float>* raw_variable_gradient,\n       ScratchAllocator* workspace_allocator) {\n     return false;\n   }\n@@ -1704,19 +1708,19 @@ class DnnSupport {\n   //    enough for the lifespan of this operation, and recycles afterwards.\n   virtual bool DoRnnForward(Stream* stream, const RnnDescriptor& rnn_desc,\n                             const RnnSequenceTensorDescriptor& input_desc,\n-                            const DeviceMemory<Eigen::half>& input_data,\n-                            const DeviceMemory<int>& seq_lengths_data,\n+                            const DeviceAddress<Eigen::half>& input_data,\n+                            const DeviceAddress<int>& seq_lengths_data,\n                             const RnnStateTensorDescriptor& input_h_desc,\n-                            const DeviceMemory<Eigen::half>& input_h_data,\n+                            const DeviceAddress<Eigen::half>& input_h_data,\n                             const RnnStateTensorDescriptor& input_c_desc,\n-                            const DeviceMemory<Eigen::half>& input_c_data,\n-                            const DeviceMemory<Eigen::half>& params,\n+                            const DeviceAddress<Eigen::half>& input_c_data,\n+                            const DeviceAddress<Eigen::half>& params,\n                             const RnnSequenceTensorDescriptor& output_desc,\n-                            DeviceMemory<Eigen::half>* output_data,\n+                            DeviceAddress<Eigen::half>* output_data,\n                             const RnnStateTensorDescriptor& output_h_desc,\n-                            DeviceMemory<Eigen::half>* output_h_data,\n+                            DeviceAddress<Eigen::half>* output_h_data,\n                             const RnnStateTensorDescriptor& output_c_desc,\n-                            DeviceMemory<Eigen::half>* output_c_data,\n+                            DeviceAddress<Eigen::half>* output_c_data,\n                             bool is_training,\n                             ScratchAllocator* reserve_space_allocator,\n                             ScratchAllocator* workspace_allocator,\n@@ -1726,19 +1730,19 @@ class DnnSupport {\n \n   virtual bool DoRnnForward(Stream* stream, const RnnDescriptor& rnn_desc,\n                             const RnnSequenceTensorDescriptor& input_desc,\n-                            const DeviceMemory<float>& input_data,\n-                            const DeviceMemory<int>& seq_lengths_data,\n+                            const DeviceAddress<float>& input_data,\n+                            const DeviceAddress<int>& seq_lengths_data,\n                             const RnnStateTensorDescriptor& input_h_desc,\n-                            const DeviceMemory<float>& input_h_data,\n+                            const DeviceAddress<float>& input_h_data,\n                             const RnnStateTensorDescriptor& input_c_desc,\n-                            const DeviceMemory<float>& input_c_data,\n-                            const DeviceMemory<float>& params,\n+                            const DeviceAddress<float>& input_c_data,\n+                            const DeviceAddress<float>& params,\n                             const RnnSequenceTensorDescriptor& output_desc,\n-                            DeviceMemory<float>* output_data,\n+                            DeviceAddress<float>* output_data,\n                             const RnnStateTensorDescriptor& output_h_desc,\n-                            DeviceMemory<float>* output_h_data,\n+                            DeviceAddress<float>* output_h_data,\n                             const RnnStateTensorDescriptor& output_c_desc,\n-                            DeviceMemory<float>* output_c_data,\n+                            DeviceAddress<float>* output_c_data,\n                             bool is_training,\n                             ScratchAllocator* reserve_space_allocator,\n                             ScratchAllocator* workspace_allocator,\n@@ -1748,19 +1752,19 @@ class DnnSupport {\n \n   virtual bool DoRnnForward(Stream* stream, const RnnDescriptor& rnn_desc,\n                             const RnnSequenceTensorDescriptor& input_desc,\n-                            const DeviceMemory<double>& input_data,\n-                            const DeviceMemory<int>& seq_lengths_data,\n+                            const DeviceAddress<double>& input_data,\n+                            const DeviceAddress<int>& seq_lengths_data,\n                             const RnnStateTensorDescriptor& input_h_desc,\n-                            const DeviceMemory<double>& input_h_data,\n+                            const DeviceAddress<double>& input_h_data,\n                             const RnnStateTensorDescriptor& input_c_desc,\n-                            const DeviceMemory<double>& input_c_data,\n-                            const DeviceMemory<double>& params,\n+                            const DeviceAddress<double>& input_c_data,\n+                            const DeviceAddress<double>& params,\n                             const RnnSequenceTensorDescriptor& output_desc,\n-                            DeviceMemory<double>* output_data,\n+                            DeviceAddress<double>* output_data,\n                             const RnnStateTensorDescriptor& output_h_desc,\n-                            DeviceMemory<double>* output_h_data,\n+                            DeviceAddress<double>* output_h_data,\n                             const RnnStateTensorDescriptor& output_c_desc,\n-                            DeviceMemory<double>* output_c_data,\n+                            DeviceAddress<double>* output_c_data,\n                             bool is_training,\n                             ScratchAllocator* reserve_space_allocator,\n                             ScratchAllocator* workspace_allocator,\n@@ -1811,99 +1815,100 @@ class DnnSupport {\n   virtual bool DoRnnBackward(\n       Stream* stream, const RnnDescriptor& rnn_desc,\n       const RnnSequenceTensorDescriptor& input_desc,\n-      const DeviceMemory<Eigen::half>& input_data,\n-      const DeviceMemory<int>& seq_lengths_data,\n+      const DeviceAddress<Eigen::half>& input_data,\n+      const DeviceAddress<int>& seq_lengths_data,\n       const RnnStateTensorDescriptor& input_h_desc,\n-      const DeviceMemory<Eigen::half>& input_h_data,\n+      const DeviceAddress<Eigen::half>& input_h_data,\n       const RnnStateTensorDescriptor& input_c_desc,\n-      const DeviceMemory<Eigen::half>& input_c_data,\n-      const DeviceMemory<Eigen::half>& params,\n+      const DeviceAddress<Eigen::half>& input_c_data,\n+      const DeviceAddress<Eigen::half>& params,\n       const RnnSequenceTensorDescriptor& output_desc,\n-      const DeviceMemory<Eigen::half>& output_data,\n+      const DeviceAddress<Eigen::half>& output_data,\n       const RnnStateTensorDescriptor& output_h_desc,\n-      const DeviceMemory<Eigen::half>& output_h_data,\n+      const DeviceAddress<Eigen::half>& output_h_data,\n       const RnnStateTensorDescriptor& output_c_desc,\n-      const DeviceMemory<Eigen::half>& output_c_data,\n-      const DeviceMemory<Eigen::half>& output_backprop_data,\n-      const DeviceMemory<Eigen::half>& output_h_backprop_data,\n-      const DeviceMemory<Eigen::half>& output_c_backprop_data,\n-      DeviceMemory<Eigen::half>* input_backprop_data,\n-      DeviceMemory<Eigen::half>* input_h_backprop_data,\n-      DeviceMemory<Eigen::half>* input_c_backprop_data,\n-      DeviceMemory<Eigen::half>* params_backprop_data,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      const DeviceAddress<Eigen::half>& output_c_data,\n+      const DeviceAddress<Eigen::half>& output_backprop_data,\n+      const DeviceAddress<Eigen::half>& output_h_backprop_data,\n+      const DeviceAddress<Eigen::half>& output_c_backprop_data,\n+      DeviceAddress<Eigen::half>* input_backprop_data,\n+      DeviceAddress<Eigen::half>* input_h_backprop_data,\n+      DeviceAddress<Eigen::half>* input_c_backprop_data,\n+      DeviceAddress<Eigen::half>* params_backprop_data,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator,\n       ProfileResult* output_profile_result) {\n     return false;\n   }\n \n   virtual bool DoRnnBackward(Stream* stream, const RnnDescriptor& rnn_desc,\n                              const RnnSequenceTensorDescriptor& input_desc,\n-                             const DeviceMemory<float>& input_data,\n-                             const DeviceMemory<int>& seq_lengths_data,\n+                             const DeviceAddress<float>& input_data,\n+                             const DeviceAddress<int>& seq_lengths_data,\n                              const RnnStateTensorDescriptor& input_h_desc,\n-                             const DeviceMemory<float>& input_h_data,\n+                             const DeviceAddress<float>& input_h_data,\n                              const RnnStateTensorDescriptor& input_c_desc,\n-                             const DeviceMemory<float>& input_c_data,\n-                             const DeviceMemory<float>& params,\n+                             const DeviceAddress<float>& input_c_data,\n+                             const DeviceAddress<float>& params,\n                              const RnnSequenceTensorDescriptor& output_desc,\n-                             const DeviceMemory<float>& output_data,\n+                             const DeviceAddress<float>& output_data,\n                              const RnnStateTensorDescriptor& output_h_desc,\n-                             const DeviceMemory<float>& output_h_data,\n+                             const DeviceAddress<float>& output_h_data,\n                              const RnnStateTensorDescriptor& output_c_desc,\n-                             const DeviceMemory<float>& output_c_data,\n-                             const DeviceMemory<float>& output_backprop_data,\n-                             const DeviceMemory<float>& output_h_backprop_data,\n-                             const DeviceMemory<float>& output_c_backprop_data,\n-                             DeviceMemory<float>* input_backprop_data,\n-                             DeviceMemory<float>* input_h_backprop_data,\n-                             DeviceMemory<float>* input_c_backprop_data,\n-                             DeviceMemory<float>* params_backprop_data,\n-                             DeviceMemory<uint8_t>* reserve_space_data,\n+                             const DeviceAddress<float>& output_c_data,\n+                             const DeviceAddress<float>& output_backprop_data,\n+                             const DeviceAddress<float>& output_h_backprop_data,\n+                             const DeviceAddress<float>& output_c_backprop_data,\n+                             DeviceAddress<float>* input_backprop_data,\n+                             DeviceAddress<float>* input_h_backprop_data,\n+                             DeviceAddress<float>* input_c_backprop_data,\n+                             DeviceAddress<float>* params_backprop_data,\n+                             DeviceAddress<uint8_t>* reserve_space_data,\n                              ScratchAllocator* workspace_allocator,\n                              ProfileResult* output_profile_result) {\n     return false;\n   }\n \n-  virtual bool DoRnnBackward(Stream* stream, const RnnDescriptor& rnn_desc,\n-                             const RnnSequenceTensorDescriptor& input_desc,\n-                             const DeviceMemory<double>& input_data,\n-                             const DeviceMemory<int>& seq_lengths_data,\n-                             const RnnStateTensorDescriptor& input_h_desc,\n-                             const DeviceMemory<double>& input_h_data,\n-                             const RnnStateTensorDescriptor& input_c_desc,\n-                             const DeviceMemory<double>& input_c_data,\n-                             const DeviceMemory<double>& params,\n-                             const RnnSequenceTensorDescriptor& output_desc,\n-                             const DeviceMemory<double>& output_data,\n-                             const RnnStateTensorDescriptor& output_h_desc,\n-                             const DeviceMemory<double>& output_h_data,\n-                             const RnnStateTensorDescriptor& output_c_desc,\n-                             const DeviceMemory<double>& output_c_data,\n-                             const DeviceMemory<double>& output_backprop_data,\n-                             const DeviceMemory<double>& output_h_backprop_data,\n-                             const DeviceMemory<double>& output_c_backprop_data,\n-                             DeviceMemory<double>* input_backprop_data,\n-                             DeviceMemory<double>* input_h_backprop_data,\n-                             DeviceMemory<double>* input_c_backprop_data,\n-                             DeviceMemory<double>* params_backprop_data,\n-                             DeviceMemory<uint8_t>* reserve_space_data,\n-                             ScratchAllocator* workspace_allocator,\n-                             ProfileResult* output_profile_result) {\n+  virtual bool DoRnnBackward(\n+      Stream* stream, const RnnDescriptor& rnn_desc,\n+      const RnnSequenceTensorDescriptor& input_desc,\n+      const DeviceAddress<double>& input_data,\n+      const DeviceAddress<int>& seq_lengths_data,\n+      const RnnStateTensorDescriptor& input_h_desc,\n+      const DeviceAddress<double>& input_h_data,\n+      const RnnStateTensorDescriptor& input_c_desc,\n+      const DeviceAddress<double>& input_c_data,\n+      const DeviceAddress<double>& params,\n+      const RnnSequenceTensorDescriptor& output_desc,\n+      const DeviceAddress<double>& output_data,\n+      const RnnStateTensorDescriptor& output_h_desc,\n+      const DeviceAddress<double>& output_h_data,\n+      const RnnStateTensorDescriptor& output_c_desc,\n+      const DeviceAddress<double>& output_c_data,\n+      const DeviceAddress<double>& output_backprop_data,\n+      const DeviceAddress<double>& output_h_backprop_data,\n+      const DeviceAddress<double>& output_c_backprop_data,\n+      DeviceAddress<double>* input_backprop_data,\n+      DeviceAddress<double>* input_h_backprop_data,\n+      DeviceAddress<double>* input_c_backprop_data,\n+      DeviceAddress<double>* params_backprop_data,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n+      ScratchAllocator* workspace_allocator,\n+      ProfileResult* output_profile_result) {\n     return false;\n   }\n \n   template <typename ElementType>\n   absl::Status PrepareForCtcLoss(Stream* stream,\n                                  const RnnStateTensorDescriptor& probs_desc,\n-                                 DeviceMemory<ElementType> probs_data,\n+                                 DeviceAddress<ElementType> probs_data,\n                                  const RnnStateTensorDescriptor& grads_desc,\n                                  absl::Span<const int> labels_data,\n                                  absl::Span<const int> labels_lengths_data,\n                                  absl::Span<const int> input_lengths_data,\n                                  const EngineOptions& engine_options,\n                                  ScratchAllocator* workspace_allocator,\n-                                 DeviceMemory<uint8_t>* scratch_memory,\n+                                 DeviceAddress<uint8_t>* scratch_memory,\n                                  int* ctc_loss_algo_id) {\n     return DoPrepareForCtcLoss(\n         stream, ToDataType<ElementType>::value, probs_desc, grads_desc,\n@@ -1935,22 +1940,22 @@ class DnnSupport {\n   virtual absl::Status DoCtcLoss(\n       Stream* stream, DataType element_type,\n       const RnnStateTensorDescriptor& probs_desc,\n-      const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+      const DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n       absl::Span<const int> labels_lengths_data,\n-      absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n-      const RnnStateTensorDescriptor& grads_desc, DeviceMemoryBase grads_data,\n-      DeviceMemory<uint8_t> scratch_memory, int ctc_loss_algo_id);\n+      absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n+      const RnnStateTensorDescriptor& grads_desc, DeviceAddressBase grads_data,\n+      DeviceAddress<uint8_t> scratch_memory, int ctc_loss_algo_id);\n \n   template <typename ElementType>\n   bool DoCtcLoss(Stream* stream, const RnnStateTensorDescriptor& probs_desc,\n-                 const DeviceMemory<ElementType>& probs_data,\n+                 const DeviceAddress<ElementType>& probs_data,\n                  absl::Span<const int> labels_data,\n                  absl::Span<const int> labels_lengths_data,\n                  absl::Span<const int> input_lengths_data,\n-                 DeviceMemory<ElementType>* costs_data,\n+                 DeviceAddress<ElementType>* costs_data,\n                  const RnnStateTensorDescriptor& grads_desc,\n-                 DeviceMemory<ElementType>* grads_data,\n-                 DeviceMemory<uint8_t>* scratch_memory, int ctc_loss_algo_id) {\n+                 DeviceAddress<ElementType>* grads_data,\n+                 DeviceAddress<uint8_t>* scratch_memory, int ctc_loss_algo_id) {\n     return IsStatusOk(\n         DoCtcLoss(stream, ToDataType<ElementType>::value, probs_desc,\n                   probs_data, labels_data, labels_lengths_data,\n@@ -1973,8 +1978,8 @@ class DnnSupport {\n   //  output_data: the device memory region that contains the output tensor.\n   virtual bool DoTransformTensor(\n       Stream* stream, const BatchDescriptor& input_desc, DataType input_type,\n-      const DeviceMemoryBase& input_data, const BatchDescriptor& output_desc,\n-      DataType output_type, float scale, DeviceMemoryBase* output_data) {\n+      const DeviceAddressBase& input_data, const BatchDescriptor& output_desc,\n+      DataType output_type, float scale, DeviceAddressBase* output_data) {\n     return false;\n   }\n \n@@ -1997,7 +2002,7 @@ class DnnSupport {\n       absl::Span<const int> labels_lengths_data,\n       absl::Span<const int> input_lengths_data,\n       const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n-      DeviceMemory<uint8_t>* scratch_memory, int* ctc_loss_algo_id) {\n+      DeviceAddress<uint8_t>* scratch_memory, int* ctc_loss_algo_id) {\n     *scratch_memory = {};\n     return absl::OkStatus();\n   }"
        },
        {
            "sha": "ddabc85bce9692f9643216fac428c0e8492e0717",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 28,
            "deletions": 33,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -141,7 +141,7 @@ cc_library(\n         \"//xla/service:dump\",\n         \"//xla/stream_executor:bit_pattern\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_spec\",\n@@ -171,7 +171,7 @@ cc_library(\n     hdrs = [\"gpu_executor.h\"],\n     deps = [\n         \":multicast_memory\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_common\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -186,7 +186,7 @@ cc_library(\n     name = \"multicast_memory\",\n     hdrs = [\"multicast_memory.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n     ],\n@@ -196,7 +196,7 @@ cc_library(\n     name = \"gpu_helpers_header\",\n     hdrs = [\"gpu_helpers.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n     ],\n )\n \n@@ -260,7 +260,7 @@ cc_library(\n     srcs = [\"gpu_semaphore.cc\"],\n     hdrs = [\"gpu_semaphore.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -333,16 +333,11 @@ cc_library(\n         \"redzone_allocator_kernel.h\",\n     ],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n \n-cc_library(\n-    name = \"tensor_map\",\n-    hdrs = [\"tensor_map.h\"],\n-)\n-\n cc_library(\n     name = \"redzone_allocator\",\n     srcs = [\n@@ -355,9 +350,9 @@ cc_library(\n         \":redzone_allocator_kernel\",\n         \"//xla:shape_util\",\n         \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n-        \"//xla/stream_executor:device_memory_handle\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n+        \"//xla/stream_executor:device_address_handle\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:scratch_allocator\",\n         \"//xla/stream_executor:stream\",\n@@ -383,8 +378,8 @@ xla_test(\n     deps = [\n         \":gpu_init\",\n         \":redzone_allocator\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n@@ -460,7 +455,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/service:algorithm_util\",\n         \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n@@ -515,7 +510,7 @@ cc_library(\n     name = \"gpu_test_kernel_traits\",\n     hdrs = [\"gpu_test_kernel_traits.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -594,7 +589,7 @@ xla_test(\n         \":tma_metadata_proto_cc\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_args\",\n         \"//xla/stream_executor:kernel_spec\",\n@@ -626,7 +621,7 @@ xla_test(\n         \":gpu_test_kernels\",\n         \"//xla/service:platform_util\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor:launch_dim\",\n@@ -657,7 +652,7 @@ xla_test(\n     backends = [\"gpu\"],\n     deps = [\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -830,7 +825,7 @@ cc_library(\n     name = \"buffer_comparator_kernel\",\n     hdrs = [\"buffer_comparator_kernel.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -848,7 +843,7 @@ cc_library(\n     name = \"make_batch_pointers_kernel\",\n     hdrs = [\"make_batch_pointers_kernel.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -857,7 +852,7 @@ cc_library(\n     name = \"ragged_all_to_all_kernel\",\n     hdrs = [\"ragged_all_to_all_kernel.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -881,7 +876,7 @@ cc_library(\n     name = \"topk_kernel\",\n     hdrs = [\"topk_kernel.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -890,7 +885,7 @@ cc_library(\n     name = \"repeat_buffer_kernel\",\n     hdrs = [\"repeat_buffer_kernel.h\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -904,7 +899,7 @@ xla_test(\n         \":gpu_kernel_registry\",\n         \":repeat_buffer_kernel\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -952,8 +947,8 @@ cc_library(\n     deps = [\n         \"//xla/backends/gpu/runtime:buffer_debug_log_proto_cc\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -972,7 +967,7 @@ xla_test(\n         \"//xla/backends/gpu/runtime:buffer_debug_log_proto_cc\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n         \"//xla/backends/gpu/runtime:thunk_id\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -990,7 +985,7 @@ cc_library(\n     hdrs = [\"buffer_debug_xor_checksum_kernel.h\"],\n     deps = [\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -1001,7 +996,7 @@ cc_library(\n     deps = [\n         \"//xla:types\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n@@ -1011,7 +1006,7 @@ cc_library(\n     hdrs = [\"prefix_sum_kernel.h\"],\n     deps = [\n         \"//xla:types\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n     ],\n )"
        },
        {
            "sha": "50799bdb350b54a38c363bd6d14af47af29e5f19",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_comparator_kernel.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n \n #include <cstdint>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n@@ -30,10 +30,10 @@ namespace stream_executor::gpu {\n template <typename ElementT>\n struct BufferComparatorKernel {\n   using KernelType =\n-      stream_executor::TypedKernel<stream_executor::DeviceMemory<ElementT>,\n-                                   stream_executor::DeviceMemory<ElementT>,\n+      stream_executor::TypedKernel<stream_executor::DeviceAddress<ElementT>,\n+                                   stream_executor::DeviceAddress<ElementT>,\n                                    float, uint64_t,\n-                                   stream_executor::DeviceMemory<uint64_t>>;\n+                                   stream_executor::DeviceAddress<uint64_t>>;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "421a1a08b7d547d429469d7a78c72ae713fce267",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_float_check_kernel.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_float_check_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_float_check_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_float_check_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n #include <cstdint>\n \n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/types.h\"\n \n@@ -31,17 +31,17 @@ namespace stream_executor::gpu {\n // This kernel MUST execute on a single thread block.\n struct BufferDebugFloatCheckF32Kernel {\n   using KernelType =\n-      TypedKernel<xla::gpu::BufferDebugLogEntryId, DeviceMemory<float>,\n-                  uint64_t, DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n-                  DeviceMemory<xla::gpu::BufferDebugFloatCheckEntry>>;\n+      TypedKernel<xla::gpu::BufferDebugLogEntryId, DeviceAddress<float>,\n+                  uint64_t, DeviceAddress<xla::gpu::BufferDebugLogHeader>,\n+                  DeviceAddress<xla::gpu::BufferDebugFloatCheckEntry>>;\n };\n \n struct BufferDebugFloatCheckBf16Kernel {\n   using KernelType =\n       TypedKernel<xla::gpu::BufferDebugLogEntryId,\n-                  DeviceMemory<Eigen::bfloat16>, uint64_t,\n-                  DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n-                  DeviceMemory<xla::gpu::BufferDebugFloatCheckEntry>>;\n+                  DeviceAddress<Eigen::bfloat16>, uint64_t,\n+                  DeviceAddress<xla::gpu::BufferDebugLogHeader>,\n+                  DeviceAddress<xla::gpu::BufferDebugFloatCheckEntry>>;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "650bc51333990e486ebb7aed89942bdcfe4227f4",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_log.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,17 +25,17 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n \n namespace stream_executor::gpu {\n \n using ::xla::gpu::BufferDebugLogHeader;\n \n-absl::StatusOr<DeviceMemory<uint8_t>> BufferDebugLogBase::CreateOnDevice(\n-    Stream& stream, DeviceMemory<uint8_t> memory, size_t entry_size) {\n+absl::StatusOr<DeviceAddress<uint8_t>> BufferDebugLogBase::CreateOnDevice(\n+    Stream& stream, DeviceAddress<uint8_t> memory, size_t entry_size) {\n   if (memory.is_null()) {\n     return absl::InvalidArgumentError(\"Log buffer must be non-null\");\n   }\n@@ -60,15 +60,15 @@ absl::StatusOr<DeviceMemory<uint8_t>> BufferDebugLogBase::CreateOnDevice(\n }\n \n absl::StatusOr<BufferDebugLogHeader> BufferDebugLogBase::ReadHeaderFromDevice(\n-    Stream& stream, DeviceMemory<uint8_t> memory) const {\n+    Stream& stream, DeviceAddress<uint8_t> memory) const {\n   BufferDebugLogHeader header;\n   TF_RETURN_IF_ERROR(stream.Memcpy(&header, memory, sizeof(header)));\n   TF_RETURN_IF_ERROR(stream.BlockHostUntilDone());\n   return header;\n }\n \n absl::StatusOr<size_t> BufferDebugLogBase::ReadFromDevice(\n-    Stream& stream, DeviceMemory<uint8_t> memory, size_t entry_size,\n+    Stream& stream, DeviceAddress<uint8_t> memory, size_t entry_size,\n     void* entries_data) const {\n   std::vector<uint8_t> buffer(memory.size());\n   TF_RETURN_IF_ERROR(stream.Memcpy(buffer.data(), memory, memory.size()));"
        },
        {
            "sha": "8e172ef694160c5076f4fba50cf812e1cf3dd3fe",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_log.h",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log.pb.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -35,15 +35,15 @@ namespace stream_executor::gpu {\n class BufferDebugLogBase {\n  protected:\n   absl::StatusOr<xla::gpu::BufferDebugLogHeader> ReadHeaderFromDevice(\n-      Stream& stream, DeviceMemory<uint8_t> memory) const;\n+      Stream& stream, DeviceAddress<uint8_t> memory) const;\n \n   absl::StatusOr<size_t> ReadFromDevice(Stream& stream,\n-                                        DeviceMemory<uint8_t> memory,\n+                                        DeviceAddress<uint8_t> memory,\n                                         size_t entry_size,\n                                         void* entries_data) const;\n \n-  static absl::StatusOr<DeviceMemory<uint8_t>> CreateOnDevice(\n-      Stream& stream, DeviceMemory<uint8_t> memory, size_t entry_size);\n+  static absl::StatusOr<DeviceAddress<uint8_t>> CreateOnDevice(\n+      Stream& stream, DeviceAddress<uint8_t> memory, size_t entry_size);\n };\n \n // A wrapper over a device memory buffer used to store debug info about contents\n@@ -70,7 +70,7 @@ class BufferDebugLog : public BufferDebugLogBase {\n   // Fails with `absl::StatusCode::kInvalidArgument` if `log_buffer` is too\n   // small to hold any entries.\n   static absl::StatusOr<BufferDebugLog<Entry>> CreateOnDevice(\n-      Stream& stream, DeviceMemory<uint8_t> log_buffer) {\n+      Stream& stream, DeviceAddress<uint8_t> log_buffer) {\n     TF_ASSIGN_OR_RETURN(auto memory, BufferDebugLogBase::CreateOnDevice(\n                                          stream, log_buffer, sizeof(Entry)));\n     return BufferDebugLog<Entry>(memory);\n@@ -80,8 +80,8 @@ class BufferDebugLog : public BufferDebugLogBase {\n   // buffer.\n   //\n   // `log_buffer` must contain an initialized `BufferDebugLogHeader`.\n-  static BufferDebugLog FromDeviceMemoryUnchecked(\n-      DeviceMemory<uint8_t> memory) {\n+  static BufferDebugLog FromDeviceAddressUnchecked(\n+      DeviceAddress<uint8_t> memory) {\n     return BufferDebugLog<Entry>(memory);\n   }\n \n@@ -112,27 +112,27 @@ class BufferDebugLog : public BufferDebugLogBase {\n \n   // Returns a view of the `BufferDebugLogHeader`.\n   //\n-  // The returned `DeviceMemory` gets invalidated when the `BufferDebugLog` is\n+  // The returned `DeviceAddress` gets invalidated when the `BufferDebugLog` is\n   // destroyed.\n-  DeviceMemory<xla::gpu::BufferDebugLogHeader> GetDeviceHeader() const {\n-    return DeviceMemory<xla::gpu::BufferDebugLogHeader>(\n+  DeviceAddress<xla::gpu::BufferDebugLogHeader> GetDeviceHeader() const {\n+    return DeviceAddress<xla::gpu::BufferDebugLogHeader>(\n         memory_.GetByteSlice(0, sizeof(xla::gpu::BufferDebugLogHeader)));\n   }\n \n   // Returns a view of the `Entry` array.\n   //\n-  // The returned `DeviceMemory` gets invalidated when the `BufferDebugLog` is\n+  // The returned `DeviceAddress` gets invalidated when the `BufferDebugLog` is\n   // destroyed.\n-  DeviceMemory<Entry> GetDeviceEntries() const {\n-    return DeviceMemory<Entry>(memory_.GetByteSlice(\n+  DeviceAddress<Entry> GetDeviceEntries() const {\n+    return DeviceAddress<Entry>(memory_.GetByteSlice(\n         sizeof(xla::gpu::BufferDebugLogHeader),\n         memory_.size() - sizeof(xla::gpu::BufferDebugLogHeader)));\n   }\n \n  private:\n-  explicit BufferDebugLog(DeviceMemory<uint8_t> memory) : memory_(memory) {}\n+  explicit BufferDebugLog(DeviceAddress<uint8_t> memory) : memory_(memory) {}\n \n-  DeviceMemory<uint8_t> memory_;\n+  DeviceAddress<uint8_t> memory_;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "87f4ad79ef016e01acb17cfbc26515df398d25c2",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_log_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_log_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/buffer_debug_log.pb.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -62,7 +62,7 @@ class BufferDebugLogTest : public ::testing::Test {\n };\n \n TEST_F(BufferDebugLogTest, CreateBufferDebugLogOnDevice_InitializesEmptyLog) {\n-  DeviceMemory<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(1024);\n+  DeviceAddress<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(1024);\n \n   TF_ASSERT_OK_AND_ASSIGN(auto device_log,\n                           BufferDebugLog<BufferDebugLogEntry>::CreateOnDevice(\n@@ -78,7 +78,7 @@ TEST_F(BufferDebugLogTest,\n   constexpr size_t kExpectedHeaderSize = sizeof(BufferDebugLogHeader);\n   constexpr size_t kExpectedEntriesSize =\n       sizeof(BufferDebugLogEntry) * kMaxEntries;\n-  DeviceMemory<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(\n+  DeviceAddress<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(\n       kExpectedHeaderSize + kExpectedEntriesSize);\n \n   TF_ASSERT_OK_AND_ASSIGN(auto device_log,\n@@ -91,7 +91,7 @@ TEST_F(BufferDebugLogTest,\n \n TEST_F(BufferDebugLogTest, CreateBufferDebugLogOnDevice_InitializesHeader) {\n   constexpr size_t kMaxEntries = 123;\n-  DeviceMemory<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(\n+  DeviceAddress<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(\n       BufferDebugLog<BufferDebugLogEntry>::RequiredSizeForEntries(kMaxEntries));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto device_log,\n@@ -106,13 +106,13 @@ TEST_F(BufferDebugLogTest, CreateBufferDebugLogOnDevice_InitializesHeader) {\n \n TEST_F(BufferDebugLogTest, CreateBufferDebugLogOnDevice_FailsForNullBuffer) {\n   EXPECT_THAT(BufferDebugLog<BufferDebugLogEntry>::CreateOnDevice(\n-                  *stream_, DeviceMemory<uint8_t>()),\n+                  *stream_, DeviceAddress<uint8_t>()),\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n TEST_F(BufferDebugLogTest,\n        CreateBufferDebugLogOnDevice_FailsForTooSmallBuffer) {\n-  DeviceMemory<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(\n+  DeviceAddress<uint8_t> log_buffer = executor_->AllocateArray<uint8_t>(\n       BufferDebugLog<BufferDebugLogEntry>::RequiredSizeForEntries(1) - 1);\n \n   EXPECT_THAT("
        },
        {
            "sha": "d1fbcd1117768ae3edbbaa9b01f90a847d33e314",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_xor_checksum_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_xor_checksum_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_xor_checksum_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n #include <cstdint>\n \n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n@@ -30,9 +30,9 @@ namespace stream_executor::gpu {\n // This kernel MUST execute on a single thread block.\n struct BufferDebugXorChecksumKernel {\n   using KernelType =\n-      TypedKernel<xla::gpu::BufferDebugLogEntryId, DeviceMemory<uint8_t>,\n-                  uint64_t, DeviceMemory<xla::gpu::BufferDebugLogHeader>,\n-                  DeviceMemory<xla::gpu::BufferDebugLogEntry>>;\n+      TypedKernel<xla::gpu::BufferDebugLogEntryId, DeviceAddress<uint8_t>,\n+                  uint64_t, DeviceAddress<xla::gpu::BufferDebugLogHeader>,\n+                  DeviceAddress<xla::gpu::BufferDebugLogEntry>>;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "8b6043b8bf8b64cd2803aa66c3f57557d0dda12b",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_blas_lt.h",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_blas_lt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_blas_lt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_blas_lt.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.pb.h\"\n #include \"xla/types.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -86,15 +86,15 @@ struct MatrixLayout {  // plain MatrixLayout which is extended with create\n // compact version of the matrix layout to be used to pass matrices\n // to underlying blas API\n struct MatrixDescriptor {\n-  DeviceMemoryBase data;\n+  DeviceAddressBase data;\n   int64_t leading_dim_stride = 0;\n   int64_t batch_stride = 0;\n   blas::DataType type{};\n   blas::Transpose transpose{};\n \n   template <typename T>\n-  DeviceMemory<T> cast() const {\n-    return DeviceMemory<T>(data);\n+  DeviceAddress<T> cast() const {\n+    return DeviceAddress<T>(data);\n   }\n };\n \n@@ -165,25 +165,25 @@ struct BlasLt {\n   };\n \n   struct MemoryArgs {\n-    DeviceMemoryBase a, b, c, d;                          // these are mandatory\n-    DeviceMemoryBase bias, aux;                           // these may be null\n-    DeviceMemoryBase a_scale, b_scale, c_scale, d_scale;  // these may be null\n-    DeviceMemoryBase d_amax;                              // this may be null\n-    DeviceMemoryBase workspace;                           // either workspace or\n+    DeviceAddressBase a, b, c, d;  // these are mandatory\n+    DeviceAddressBase bias, aux;   // these may be null\n+    DeviceAddressBase a_scale, b_scale, c_scale, d_scale;  // these may be null\n+    DeviceAddressBase d_amax;                              // this may be null\n+    DeviceAddressBase workspace;          // either workspace or\n     ScratchAllocator* scratch_allocator;  // scratch_allocator must not be null\n   };\n \n   struct MatmulPlan {\n     // This function is to be removed once TF interface is fixed,\n     // see tensorflow/core/kernels/matmul_util.cc\n     absl::Status ExecuteOnStream(\n-        Stream* stream, DeviceMemoryBase a, DeviceMemoryBase b,\n-        DeviceMemoryBase c, DeviceMemoryBase d,\n-        DeviceMemoryBase bias,  // may be null\n-        DeviceMemoryBase aux,   // may be null\n-        DeviceMemoryBase a_scale, DeviceMemoryBase b_scale,\n-        DeviceMemoryBase c_scale, DeviceMemoryBase d_scale,\n-        DeviceMemoryBase d_amax, const MatmulAlgorithm& algorithm,\n+        Stream* stream, DeviceAddressBase a, DeviceAddressBase b,\n+        DeviceAddressBase c, DeviceAddressBase d,\n+        DeviceAddressBase bias,  // may be null\n+        DeviceAddressBase aux,   // may be null\n+        DeviceAddressBase a_scale, DeviceAddressBase b_scale,\n+        DeviceAddressBase c_scale, DeviceAddressBase d_scale,\n+        DeviceAddressBase d_amax, const MatmulAlgorithm& algorithm,\n         ScratchAllocator& scratch_allocator,\n         blas::ProfileResult* profile_result = nullptr) const {\n       // Temporary hack until Tensorflow side is fixed\n@@ -192,37 +192,37 @@ struct BlasLt {\n       return ExecuteOnStream(\n           stream,\n           MemoryArgs{a, b, c, d, bias, aux, a_scale, b_scale, c_scale, d_scale,\n-                     d_amax, DeviceMemoryBase{}, &scratch_allocator},\n+                     d_amax, DeviceAddressBase{}, &scratch_allocator},\n           profile_result);\n     }\n \n     // API that uses scratch_allocator to allocate workspace.\n     // This version is used by TF: see tensorflow/core/kernels/matmul_util.cc\n     absl::Status ExecuteOnStream(\n-        Stream* stream, DeviceMemoryBase a, DeviceMemoryBase b,\n-        DeviceMemoryBase c, DeviceMemoryBase d,\n-        DeviceMemoryBase bias,  // may be null\n-        DeviceMemoryBase aux,   // may be null\n-        DeviceMemoryBase a_scale, DeviceMemoryBase b_scale,\n-        DeviceMemoryBase c_scale, DeviceMemoryBase d_scale,\n-        DeviceMemoryBase d_amax, ScratchAllocator& scratch_allocator,\n+        Stream* stream, DeviceAddressBase a, DeviceAddressBase b,\n+        DeviceAddressBase c, DeviceAddressBase d,\n+        DeviceAddressBase bias,  // may be null\n+        DeviceAddressBase aux,   // may be null\n+        DeviceAddressBase a_scale, DeviceAddressBase b_scale,\n+        DeviceAddressBase c_scale, DeviceAddressBase d_scale,\n+        DeviceAddressBase d_amax, ScratchAllocator& scratch_allocator,\n         blas::ProfileResult* profile_result = nullptr) const {\n       return ExecuteOnStream(\n           stream,\n           MemoryArgs{a, b, c, d, bias, aux, a_scale, b_scale, c_scale, d_scale,\n-                     d_amax, DeviceMemoryBase{}, &scratch_allocator},\n+                     d_amax, DeviceAddressBase{}, &scratch_allocator},\n           profile_result);\n     }\n \n     // API that uses pre-allocated buffer as workspace.\n     absl::Status ExecuteOnStream(\n-        Stream* stream, DeviceMemoryBase a, DeviceMemoryBase b,\n-        DeviceMemoryBase c, DeviceMemoryBase d,\n-        DeviceMemoryBase bias,  // may be null\n-        DeviceMemoryBase aux,   // may be null\n-        DeviceMemoryBase a_scale, DeviceMemoryBase b_scale,\n-        DeviceMemoryBase c_scale, DeviceMemoryBase d_scale,\n-        DeviceMemoryBase d_amax, DeviceMemoryBase workspace,\n+        Stream* stream, DeviceAddressBase a, DeviceAddressBase b,\n+        DeviceAddressBase c, DeviceAddressBase d,\n+        DeviceAddressBase bias,  // may be null\n+        DeviceAddressBase aux,   // may be null\n+        DeviceAddressBase a_scale, DeviceAddressBase b_scale,\n+        DeviceAddressBase c_scale, DeviceAddressBase d_scale,\n+        DeviceAddressBase d_amax, DeviceAddressBase workspace,\n         blas::ProfileResult* profile_result = nullptr) const {\n       return ExecuteOnStream(\n           stream,"
        },
        {
            "sha": "6e055ba515e80bf184bf554d14416312884a4d8d",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_command_buffer.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -37,7 +37,7 @@ limitations under the License.\n #include \"xla/service/dump.h\"\n #include \"xla/stream_executor/bit_pattern.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n@@ -177,7 +177,7 @@ absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateLaunch(\n   }\n \n   // For device memory array we rely on a custom kernel arguments packing.\n-  if (auto* device_mem = DynCast<KernelArgsDeviceMemoryArray>(&args)) {\n+  if (auto* device_mem = DynCast<KernelArgsDeviceAddressArray>(&args)) {\n     auto& pack = kernel.args_packing();\n     if (!pack) {\n       return absl::InternalError(\n@@ -207,7 +207,7 @@ absl::Status GpuCommandBuffer::UpdateLaunch(const Command* command,\n   }\n \n   // For device memory array we rely on a custom kernel arguments packing.\n-  if (auto* device_mem = DynCast<KernelArgsDeviceMemoryArray>(&args)) {\n+  if (auto* device_mem = DynCast<KernelArgsDeviceAddressArray>(&args)) {\n     auto& pack = kernel.args_packing();\n     if (!pack) {\n       return absl::InternalError(\n@@ -269,7 +269,7 @@ absl::Status GpuCommandBuffer::UpdateChildCommand(\n }\n \n absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateMemcpyD2D(\n-    DeviceMemoryBase* dst, const DeviceMemoryBase& src, uint64_t size,\n+    DeviceAddressBase* dst, const DeviceAddressBase& src, uint64_t size,\n     absl::Span<const Command* const> dependencies) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kCreate));\n \n@@ -281,16 +281,16 @@ absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateMemcpyD2D(\n }\n \n absl::Status GpuCommandBuffer::UpdateMemcpyD2D(const Command* command,\n-                                               DeviceMemoryBase* dst,\n-                                               const DeviceMemoryBase& src,\n+                                               DeviceAddressBase* dst,\n+                                               const DeviceAddressBase& src,\n                                                uint64_t size) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kUpdate));\n   auto* gpu_command = tsl::down_cast<const GpuCommand*>(command);\n   return UpdateMemcpyD2DNode(gpu_command->handle, *dst, src, size);\n }\n \n absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateMemset(\n-    DeviceMemoryBase* dst, BitPattern bit_pattern, size_t num_elements,\n+    DeviceAddressBase* dst, BitPattern bit_pattern, size_t num_elements,\n     absl::Span<const Command* const> dependencies) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kCreate));\n \n@@ -302,7 +302,7 @@ absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateMemset(\n }\n \n absl::Status GpuCommandBuffer::UpdateMemset(const Command* command,\n-                                            DeviceMemoryBase* dst,\n+                                            DeviceAddressBase* dst,\n                                             const BitPattern& bit_pattern,\n                                             size_t num_elements) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kUpdate));\n@@ -317,7 +317,7 @@ absl::Status GpuCommandBuffer::UpdateMemset(const Command* command,\n absl::StatusOr<const CommandBuffer::Command*>\n GpuCommandBuffer::CreateDnnGraphCommand(\n     dnn::DnnGraph& dnn_graph, Stream& stream,\n-    absl::Span<DeviceMemoryBase> operands,\n+    absl::Span<DeviceAddressBase> operands,\n     absl::Span<const Command* const> dependencies) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kCreate));\n \n@@ -338,7 +338,7 @@ GpuCommandBuffer::CreateDnnGraphCommand(\n \n absl::Status GpuCommandBuffer::UpdateDnnGraphCommand(\n     const Command* command, dnn::DnnGraph& dnn_graph, Stream& stream,\n-    absl::Span<DeviceMemoryBase> operands) {\n+    absl::Span<DeviceAddressBase> operands) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kUpdate));\n   return UpdateDnnGraphNode(dnn_graph, stream, operands,\n                             tsl::down_cast<const GpuCommand*>(command)->handle);\n@@ -359,7 +359,7 @@ GpuCommandBuffer::CreateConditionalHandles(size_t num_handles) {\n }\n \n absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateCase(\n-    DeviceMemory<uint8_t> index, bool index_is_bool,\n+    DeviceAddress<uint8_t> index, bool index_is_bool,\n     std::vector<CreateCommands> create_branches,\n     absl::Span<const Command* const> dependencies) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kCreate));\n@@ -429,7 +429,7 @@ absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateCase(\n }\n \n absl::Status GpuCommandBuffer::UpdateCase(\n-    const Command* command, DeviceMemory<uint8_t> index, bool index_is_bool,\n+    const Command* command, DeviceAddress<uint8_t> index, bool index_is_bool,\n     std::vector<UpdateCommands> update_branches) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kUpdate));\n \n@@ -475,41 +475,41 @@ absl::Status GpuCommandBuffer::UpdateCase(\n }\n \n absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateCase(\n-    DeviceMemory<int32_t> index, std::vector<CreateCommands> create_branches,\n+    DeviceAddress<int32_t> index, std::vector<CreateCommands> create_branches,\n     absl::Span<const Command* const> dependencies) {\n   return CreateCase(\n-      DeviceMemory<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n+      DeviceAddress<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n       /*index_is_bool=*/false, std::move(create_branches), dependencies);\n }\n \n absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateCase(\n-    DeviceMemory<bool> index, std::vector<CreateCommands> create_branches,\n+    DeviceAddress<bool> index, std::vector<CreateCommands> create_branches,\n     absl::Span<const Command* const> dependencies) {\n   return CreateCase(\n-      DeviceMemory<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n+      DeviceAddress<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n       /*index_is_bool=*/true, std::move(create_branches), dependencies);\n }\n \n absl::Status GpuCommandBuffer::UpdateCase(\n-    const Command* command, DeviceMemory<int32_t> index,\n+    const Command* command, DeviceAddress<int32_t> index,\n     std::vector<UpdateCommands> update_branches) {\n   return UpdateCase(\n       command,\n-      DeviceMemory<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n+      DeviceAddress<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n       /*index_is_bool=*/false, std::move(update_branches));\n }\n \n absl::Status GpuCommandBuffer::UpdateCase(\n-    const Command* command, DeviceMemory<bool> index,\n+    const Command* command, DeviceAddress<bool> index,\n     std::vector<UpdateCommands> update_branches) {\n   return UpdateCase(\n       command,\n-      DeviceMemory<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n+      DeviceAddress<uint8_t>::MakeFromByteSize(index.opaque(), index.size()),\n       /*index_is_bool=*/true, std::move(update_branches));\n }\n \n absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateWhile(\n-    DeviceMemory<bool> pred, CreateCommands create_cond,\n+    DeviceAddress<bool> pred, CreateCommands create_cond,\n     CreateCommands create_body, absl::Span<const Command* const> dependencies) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kCreate));\n \n@@ -541,7 +541,7 @@ absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateWhile(\n }\n \n absl::Status GpuCommandBuffer::UpdateWhile(const Command* command,\n-                                           DeviceMemory<bool> pred,\n+                                           DeviceAddress<bool> pred,\n                                            UpdateCommands update_cond,\n                                            UpdateCommands update_body) {\n   TF_RETURN_IF_ERROR(CheckInState(State::kUpdate));"
        },
        {
            "sha": "300d626fdfcc63a97c2db363e568ee3329a604c9",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_command_buffer.h",
            "status": "modified",
            "additions": 30,
            "deletions": 29,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/bit_pattern.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -153,49 +153,49 @@ class GpuCommandBuffer : public CommandBuffer {\n           record_fn) override;\n \n   absl::StatusOr<const Command*> CreateMemcpyD2D(\n-      DeviceMemoryBase* dst, const DeviceMemoryBase& src, uint64_t size,\n+      DeviceAddressBase* dst, const DeviceAddressBase& src, uint64_t size,\n       absl::Span<const Command* const> dependencies) override;\n \n-  absl::Status UpdateMemcpyD2D(const Command* command, DeviceMemoryBase* dst,\n-                               const DeviceMemoryBase& src,\n+  absl::Status UpdateMemcpyD2D(const Command* command, DeviceAddressBase* dst,\n+                               const DeviceAddressBase& src,\n                                uint64_t size) override;\n \n   absl::StatusOr<const Command*> CreateMemset(\n-      DeviceMemoryBase* dst, BitPattern bit_pattern, size_t num_elements,\n+      DeviceAddressBase* dst, BitPattern bit_pattern, size_t num_elements,\n       absl::Span<const Command* const> dependencies) override;\n \n-  absl::Status UpdateMemset(const Command* command, DeviceMemoryBase* dst,\n+  absl::Status UpdateMemset(const Command* command, DeviceAddressBase* dst,\n                             const BitPattern& bit_pattern,\n                             size_t num_elements) override;\n \n   absl::StatusOr<const Command*> CreateDnnGraphCommand(\n-      dnn::DnnGraph&, Stream&, absl::Span<DeviceMemoryBase> operands,\n+      dnn::DnnGraph&, Stream&, absl::Span<DeviceAddressBase> operands,\n       absl::Span<const Command* const> dependencies) override;\n \n   absl::Status UpdateDnnGraphCommand(\n       const Command*, dnn::DnnGraph&, Stream&,\n-      absl::Span<DeviceMemoryBase> operands) override;\n+      absl::Span<DeviceAddressBase> operands) override;\n \n   absl::StatusOr<const Command*> CreateCase(\n-      DeviceMemory<int32_t> index, std::vector<CreateCommands> create_branches,\n+      DeviceAddress<int32_t> index, std::vector<CreateCommands> create_branches,\n       absl::Span<const Command* const> dependencies) override;\n \n   absl::StatusOr<const Command*> CreateCase(\n-      DeviceMemory<bool> index, std::vector<CreateCommands> create_branches,\n+      DeviceAddress<bool> index, std::vector<CreateCommands> create_branches,\n       absl::Span<const Command* const> dependencies) override;\n \n-  absl::Status UpdateCase(const Command* command, DeviceMemory<int32_t> index,\n+  absl::Status UpdateCase(const Command* command, DeviceAddress<int32_t> index,\n                           std::vector<UpdateCommands> update_branches) override;\n \n-  absl::Status UpdateCase(const Command* command, DeviceMemory<bool> index,\n+  absl::Status UpdateCase(const Command* command, DeviceAddress<bool> index,\n                           std::vector<UpdateCommands> update_branches) override;\n \n   absl::StatusOr<const Command*> CreateWhile(\n-      DeviceMemory<bool> pred, CreateCommands create_cond,\n+      DeviceAddress<bool> pred, CreateCommands create_cond,\n       CreateCommands create_body,\n       absl::Span<const Command* const> dependencies) override;\n \n-  absl::Status UpdateWhile(const Command* command, DeviceMemory<bool> pred,\n+  absl::Status UpdateWhile(const Command* command, DeviceAddress<bool> pred,\n                            UpdateCommands update_cond,\n                            UpdateCommands update_body) override;\n \n@@ -234,26 +234,26 @@ class GpuCommandBuffer : public CommandBuffer {\n   // to 8 conditionals.\n   virtual absl::StatusOr<GraphNodeHandle> CreateSetCaseConditionNode(\n       absl::Span<const GraphConditionalHandle> conditionals,\n-      DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+      DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n       bool enable_conditional_default,\n       absl::Span<const GraphNodeHandle> dependencies) = 0;\n \n   virtual absl::Status UpdateSetCaseConditionNode(\n       GraphNodeHandle handle,\n       absl::Span<const GraphConditionalHandle> conditionals,\n-      DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+      DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n       bool enable_conditional_default) = 0;\n \n   // Launches a kernel that updates the state of the given graph conditional\n   // based on the predicate. If the predicate is true, `conditional` is set to\n   // 1, otherwise to 0.\n   virtual absl::StatusOr<GraphNodeHandle> CreateSetWhileConditionNode(\n-      GraphConditionalHandle conditional, DeviceMemory<bool> predicate,\n+      GraphConditionalHandle conditional, DeviceAddress<bool> predicate,\n       absl::Span<const GraphNodeHandle> dependencies) = 0;\n \n   virtual absl::Status UpdateSetWhileConditionNode(\n       GraphNodeHandle handle, GraphConditionalHandle conditional,\n-      DeviceMemory<bool> predicate) = 0;\n+      DeviceAddress<bool> predicate) = 0;\n \n   //===--------------------------------------------------------------------===//\n \n@@ -283,11 +283,11 @@ class GpuCommandBuffer : public CommandBuffer {\n \n  private:\n   absl::StatusOr<const Command*> CreateCase(\n-      DeviceMemory<uint8_t> index, bool index_is_bool,\n+      DeviceAddress<uint8_t> index, bool index_is_bool,\n       std::vector<CreateCommands> create_branches,\n       absl::Span<const Command* const> dependencies);\n \n-  absl::Status UpdateCase(const Command* command, DeviceMemory<uint8_t> index,\n+  absl::Status UpdateCase(const Command* command, DeviceAddress<uint8_t> index,\n                           bool index_is_bool,\n                           std::vector<UpdateCommands> update_branches);\n \n@@ -321,32 +321,33 @@ class GpuCommandBuffer : public CommandBuffer {\n   // Adds a new memset node to the underlying graph.\n   virtual absl::StatusOr<GraphNodeHandle> CreateMemsetNode(\n       absl::Span<const GraphNodeHandle> dependencies,\n-      DeviceMemoryBase destination, BitPattern bit_pattern,\n+      DeviceAddressBase destination, BitPattern bit_pattern,\n       size_t num_elements) = 0;\n \n   // Updates an existing memset node. Note that `node_handle` needs to refer\n   // to a node created by `CreateMemsetNode`.\n   virtual absl::Status UpdateMemsetNode(GraphNodeHandle node_handle,\n-                                        DeviceMemoryBase destination,\n+                                        DeviceAddressBase destination,\n                                         BitPattern bit_pattern,\n                                         size_t num_elements) = 0;\n \n   // Adds a new memcpy node to the graph.\n   virtual absl::StatusOr<GraphNodeHandle> CreateMemcpyD2DNode(\n       absl::Span<const GraphNodeHandle> dependencies,\n-      DeviceMemoryBase destination, DeviceMemoryBase source, uint64_t size) = 0;\n+      DeviceAddressBase destination, DeviceAddressBase source,\n+      uint64_t size) = 0;\n \n   virtual absl::Status UpdateMemcpyD2DNode(GraphNodeHandle node_handle,\n-                                           DeviceMemoryBase destination,\n-                                           DeviceMemoryBase source,\n+                                           DeviceAddressBase destination,\n+                                           DeviceAddressBase source,\n                                            uint64_t size) = 0;\n \n   virtual absl::Status PopulateDnnGraphNode(\n-      dnn::DnnGraph&, Stream&, absl::Span<DeviceMemoryBase> operands) = 0;\n+      dnn::DnnGraph&, Stream&, absl::Span<DeviceAddressBase> operands) = 0;\n \n-  virtual absl::Status UpdateDnnGraphNode(dnn::DnnGraph&, Stream&,\n-                                          absl::Span<DeviceMemoryBase> operands,\n-                                          GraphNodeHandle) = 0;\n+  virtual absl::Status UpdateDnnGraphNode(\n+      dnn::DnnGraph&, Stream&, absl::Span<DeviceAddressBase> operands,\n+      GraphNodeHandle) = 0;\n \n   // Adds a new nested command buffer node to the graph.\n   virtual absl::StatusOr<GraphNodeHandle> CreateChildNode("
        },
        {
            "sha": "67c5ea2c425b14a5a978a59506fd3de660d03732",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_command_buffer_test.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 41,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n@@ -85,9 +85,9 @@ TEST(GpuCommandBufferTest, LaunchSingleKernel) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=1, b=2, c=0\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 1, byte_length));\n   TF_ASSERT_OK(stream->Memset32(&b, 2, byte_length));\n@@ -111,7 +111,7 @@ TEST(GpuCommandBufferTest, LaunchSingleKernel) {\n   ASSERT_EQ(dst, expected);\n \n   // Prepare argument for graph update: d = 0\n-  DeviceMemory<int32_t> d = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> d = executor->AllocateArray<int32_t>(length, 0);\n   TF_ASSERT_OK(stream->MemZero(&d, byte_length));\n \n   // Update command buffer to write into `d` buffer.\n@@ -143,16 +143,16 @@ TEST(GpuCommandBufferTest, TraceSingleKernel) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=1, b=2, c=0\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 1, byte_length));\n   TF_ASSERT_OK(stream->Memset32(&b, 2, byte_length));\n   TF_ASSERT_OK(stream->MemZero(&c, byte_length));\n \n   // Use an array of device memory base pointers as argument to test packing.\n-  KernelArgsDeviceMemoryArray args({a, b, c}, 0);\n+  KernelArgsDeviceAddressArray args({a, b, c}, 0);\n \n   // Create a command buffer by tracing kernel launch operations.\n   TF_ASSERT_OK_AND_ASSIGN(auto cmd_buffer, TraceCommandBufferFactory::Create(\n@@ -186,9 +186,9 @@ TEST(GpuCommandBufferTest, LaunchNestedCommandBuffer) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=1, b=2, c=0\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 1, byte_length));\n   TF_ASSERT_OK(stream->Memset32(&b, 2, byte_length));\n@@ -217,7 +217,7 @@ TEST(GpuCommandBufferTest, LaunchNestedCommandBuffer) {\n   ASSERT_EQ(dst, expected);\n \n   // Prepare argument for graph update: d = 0\n-  DeviceMemory<int32_t> d = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> d = executor->AllocateArray<int32_t>(length, 0);\n   TF_ASSERT_OK(stream->MemZero(&d, byte_length));\n \n   // Update command buffer to write into `d` buffer by creating a new nested\n@@ -248,8 +248,8 @@ TEST(GpuCommandBufferTest, MemcpyDeviceToDevice) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=42, b=uninitialized\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&a, 42, byte_length));\n \n@@ -294,7 +294,7 @@ TEST(GpuCommandBufferTest, Memset) {\n   int64_t length = 4;\n   int64_t byte_length = sizeof(int32_t) * length;\n \n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n \n   // Create a command buffer with a single memset command.\n   auto cmd_buffer = executor->CreateCommandBuffer(primary).value();\n@@ -344,10 +344,10 @@ TEST(GpuCommandBufferTest, ConditionalCaseEmptyGraph) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=2, b=3, c=0, index=0\n-  DeviceMemory<int32_t> index = executor->AllocateArray<int32_t>(1, 0);\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> index = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&index, 0, sizeof(int32_t)));\n   TF_ASSERT_OK(stream->Memset32(&a, 2, byte_length));\n@@ -438,12 +438,12 @@ TEST_P(GpuCommandBufferCaseTest, ConditionalMultiCase) {\n   int64_t byte_length = sizeof(int32_t) * kLength;\n \n   // Prepare arguments: index=0\n-  DeviceMemory<int32_t> index = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> index = executor->AllocateArray<int32_t>(1, 0);\n   TF_ASSERT_OK(stream->Memset32(&index, 0, sizeof(int32_t)));\n \n   const int kNumCases = GetNumCases();\n-  std::vector<DeviceMemory<int32_t>> values;\n-  std::vector<DeviceMemory<int32_t>> results;\n+  std::vector<DeviceAddress<int32_t>> values;\n+  std::vector<DeviceAddress<int32_t>> results;\n   std::vector<CommandBuffer::CreateCommands> branches;\n   values.resize(kNumCases);\n   results.resize(kNumCases);\n@@ -521,10 +521,10 @@ TEST(GpuCommandBufferTest, ConditionalCase) {\n   int64_t byte_length = sizeof(int32_t) * length;\n \n   // Prepare arguments: a=2, b=3, c=0, index=0\n-  DeviceMemory<int32_t> index = executor->AllocateArray<int32_t>(1, 0);\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> index = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n \n   TF_ASSERT_OK(stream->Memset32(&index, 0, sizeof(int32_t)));\n   TF_ASSERT_OK(stream->Memset32(&a, 2, byte_length));\n@@ -608,11 +608,11 @@ TEST(GpuCommandBufferTest, ConditionalWhile) {\n   // Prepare arguments: a=1, b=0, loop_counter=0, pred=false\n   // Value of `pred` is not important, as it will be updated by `cond_builder`\n   // below.\n-  DeviceMemory<bool> pred = executor->AllocateArray<bool>(1, 0);\n-  DeviceMemory<int32_t> loop_counter = executor->AllocateArray<int32_t>(1, 0);\n-  DeviceMemory<int32_t> num_iters = executor->AllocateArray<int32_t>(1, 0);\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<bool> pred = executor->AllocateArray<bool>(1, 0);\n+  DeviceAddress<int32_t> loop_counter = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> num_iters = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n \n   static constexpr bool kFalse = false;\n   TF_ASSERT_OK(stream->Memcpy(&pred, &kFalse, 1));\n@@ -670,12 +670,12 @@ TEST(GpuCommandBufferTest, DISABLED_WhileNestedConditional) {\n   // Prepare arguments: a=1, b=0, loop_counter=0, pred=false\n   // Value of `pred` is not important, as it will be updated by `cond_builder`\n   // below.\n-  DeviceMemory<bool> pred = executor->AllocateArray<bool>(1, 0);\n-  DeviceMemory<bool> pred_then = executor->AllocateArray<bool>(1, 0);\n-  DeviceMemory<int32_t> loop_counter = executor->AllocateArray<int32_t>(1, 0);\n-  DeviceMemory<int32_t> num_iters = executor->AllocateArray<int32_t>(1, 0);\n-  DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<bool> pred = executor->AllocateArray<bool>(1, 0);\n+  DeviceAddress<bool> pred_then = executor->AllocateArray<bool>(1, 0);\n+  DeviceAddress<int32_t> loop_counter = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> num_iters = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n \n   static constexpr bool kFalse = false;\n   static constexpr bool kTrue = true;\n@@ -753,7 +753,7 @@ static void BM_CreateCommandBuffer(benchmark::State& state) {\n   StreamExecutor* executor = platform->ExecutorForDevice(0).value();\n   TF_ASSERT_OK_AND_ASSIGN(auto add, LoadAddI32TestKernel(executor));\n \n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(1, 0);\n \n   for (auto s : state) {\n     auto cmd_buffer = executor->CreateCommandBuffer(nested).value();\n@@ -774,7 +774,7 @@ static void BM_TraceCommandBuffer(benchmark::State& state) {\n   TF_ASSERT_OK_AND_ASSIGN(auto stream, executor->CreateStream());\n   TF_ASSERT_OK_AND_ASSIGN(auto add, LoadAddI32TestKernel(executor));\n \n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(1, 0);\n \n   for (auto s : state) {\n     auto launch_kernels = [&](Stream* stream) {\n@@ -796,7 +796,7 @@ static void BM_UpdateCommandBuffer(benchmark::State& state) {\n   StreamExecutor* executor = platform->ExecutorForDevice(0).value();\n   TF_ASSERT_OK_AND_ASSIGN(auto add, LoadAddI32TestKernel(executor));\n \n-  DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(1, 0);\n+  DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(1, 0);\n \n   auto cmd_buffer = executor->CreateCommandBuffer(primary).value();\n   for (int i = 1; i < state.range(0); ++i) {"
        },
        {
            "sha": "4d0bb1b1e3c711cfc8f33f850837fc3bdeeaa39c",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_executor_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -49,7 +49,7 @@ TEST_F(GetPointerMemorySpaceTest, Host) {\n \n TEST_F(GetPointerMemorySpaceTest, HostAllocatedWithMemoryKind) {\n   StreamExecutor* executor = GetPlatform()->ExecutorForDevice(0).value();\n-  DeviceMemoryBase host_ptr = executor->Allocate(\n+  DeviceAddressBase host_ptr = executor->Allocate(\n       64, static_cast<int64_t>(stream_executor::MemoryType::kHost));\n   EXPECT_FALSE(host_ptr.is_null());\n   TF_ASSERT_OK_AND_ASSIGN(MemoryType memory_space,"
        },
        {
            "sha": "c2bba886530d089cce6d37c6ff81f023db93b8b5",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_helpers.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_helpers.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -23,23 +23,23 @@ limitations under the License.\n \n #include <stddef.h>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace stream_executor {\n \n namespace gpu {\n \n-// Converts a const DeviceMemory reference to its underlying typed pointer in\n+// Converts a const DeviceAddress reference to its underlying typed pointer in\n // CUDA device memory.\n template <typename T>\n-const T* GpuMemory(const DeviceMemory<T>& mem) {\n+const T* GpuMemory(const DeviceAddress<T>& mem) {\n   return static_cast<const T*>(mem.opaque());\n }\n \n-// Converts a (non-const) DeviceMemory pointer reference to its underlying typed\n-// pointer in CUDA device memory.\n+// Converts a (non-const) DeviceAddress pointer reference to its underlying\n+// typed pointer in CUDA device memory.\n template <typename T>\n-T* GpuMemoryMutable(DeviceMemory<T>* mem) {\n+T* GpuMemoryMutable(DeviceAddress<T>* mem) {\n   return static_cast<T*>(mem->opaque());\n }\n "
        },
        {
            "sha": "0a0ac9f862f9c954ee05e769b293327a15deb0cc",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_kernel_registry.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_registry.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_registry.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_registry.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -40,9 +40,9 @@ namespace stream_executor::gpu {\n //\n //   struct MyKernelTrait {\n //     using KernelType =\n-//         stream_executor::TypedKernel<stream_executor::DeviceMemoryBase,\n+//         stream_executor::TypedKernel<stream_executor::DeviceAddressBase,\n //                                      size_t, size_t,\n-//                                      stream_executor::DeviceMemoryBase>;\n+//                                      stream_executor::DeviceAddressBase>;\n //   };\n //\n // The registry is thread-safe. Registered kernels are immutable and cannot be"
        },
        {
            "sha": "122d62df87d8a0acf184ebf4612e503579b43381",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_kernel_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_kernel_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels_fatbin.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n@@ -55,8 +55,8 @@ using ::testing::Each;\n using tsl::proto_testing::ParseTextProtoOrDie;\n \n using AddI32Kernel =\n-    TypedKernelFactory<DeviceMemory<int32_t>, DeviceMemory<int32_t>,\n-                       DeviceMemory<int32_t>>;\n+    TypedKernelFactory<DeviceAddress<int32_t>, DeviceAddress<int32_t>,\n+                       DeviceAddress<int32_t>>;\n using TmaKernel = TypedKernelFactory<TensorMap, TensorMap, TensorMap>;\n \n class GpuKernelTest : public ::testing::Test {\n@@ -76,9 +76,9 @@ class GpuKernelTest : public ::testing::Test {\n     int64_t byte_length = sizeof(int32_t) * length;\n \n     // Prepare arguments: a=1, b=2, c=0\n-    DeviceMemory<int32_t> a = executor_->AllocateArray<int32_t>(length, 0);\n-    DeviceMemory<int32_t> b = executor_->AllocateArray<int32_t>(length, 0);\n-    DeviceMemory<int32_t> c = executor_->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> a = executor_->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> b = executor_->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> c = executor_->AllocateArray<int32_t>(length, 0);\n \n     TF_ASSERT_OK(stream->Memset32(&a, 1, byte_length));\n     TF_ASSERT_OK(stream->Memset32(&b, 2, byte_length));\n@@ -128,9 +128,9 @@ TEST_F(GpuKernelTest, LoadAndRunKernelFromSymbolWithCustomArgsPacking) {\n   constexpr int64_t kArraySizeBytes = sizeof(int32_t) * kArraySize;\n \n   // Prepare arguments: in=10, out=0\n-  DeviceMemory<int32_t> in =\n+  DeviceAddress<int32_t> in =\n       executor_->AllocateArray<int32_t>(kArraySize, /*memory_space=*/0);\n-  DeviceMemory<int32_t> out =\n+  DeviceAddress<int32_t> out =\n       executor_->AllocateArray<int32_t>(kArraySize, /*memory_space=*/0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Stream> stream,\n@@ -145,7 +145,7 @@ TEST_F(GpuKernelTest, LoadAndRunKernelFromSymbolWithCustomArgsPacking) {\n   TF_ASSERT_OK(kernel->Launch(\n       ThreadDim(), BlockDim(4),\n       /*cluster_dims=*/std::nullopt, stream.get(),\n-      KernelArgsDeviceMemoryArray({in, out}, /*shared_memory_bytes=*/0)));\n+      KernelArgsDeviceAddressArray({in, out}, /*shared_memory_bytes=*/0)));\n \n   // Copy data back to host and verify that the output is 5 + 10 = 15.\n   std::vector<int32_t> dst(4, 0);\n@@ -159,7 +159,7 @@ TEST_F(GpuKernelTest, ArrayArgByValue) {\n \n   constexpr int64_t kLength = 16;\n \n-  DeviceMemory<char> dst = executor_->AllocateArray<char>(kLength, 0);\n+  DeviceAddress<char> dst = executor_->AllocateArray<char>(kLength, 0);\n   TF_ASSERT_OK(stream->MemZero(&dst, kLength));\n \n   std::array<std::byte, 16> storage;\n@@ -240,9 +240,9 @@ TEST_F(GpuKernelTest, TmaLoadAndRunKernelFromPtx) {\n                                 l2_promotion: L2_PROMOTION_BYTES128\n                               )pb\"));\n \n-  DeviceMemory<int16_t> mem0 = executor_->AllocateArray<int16_t>(512 * 1024);\n-  DeviceMemory<int16_t> mem1 = executor_->AllocateArray<int16_t>(1024 * 512);\n-  DeviceMemory<int32_t> mem2 = executor_->AllocateArray<int32_t>(1024 * 1024);\n+  DeviceAddress<int16_t> mem0 = executor_->AllocateArray<int16_t>(512 * 1024);\n+  DeviceAddress<int16_t> mem1 = executor_->AllocateArray<int16_t>(1024 * 512);\n+  DeviceAddress<int32_t> mem2 = executor_->AllocateArray<int32_t>(1024 * 1024);\n \n   TF_ASSERT_OK_AND_ASSIGN(auto tma0,\n                           executor_->CreateTensorMap(arg0_desc, mem0.opaque()));"
        },
        {
            "sha": "0294d3dfbeb21f31b466c683f0a68206bc8ac5af",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_semaphore.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_semaphore.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_semaphore.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_semaphore.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"tsl/platform/statusor.h\"\n \n@@ -31,10 +31,10 @@ absl::StatusOr<GpuSemaphore> GpuSemaphore::Create(StreamExecutor* executor) {\n   return GpuSemaphore{std::move(alloc)};\n }\n \n-DeviceMemory<GpuSemaphoreState> GpuSemaphore::device() {\n+DeviceAddress<GpuSemaphoreState> GpuSemaphore::device() {\n   // This assumes unified addressing, as we do not explicitly translate the\n   // host pointer into a device pointer.\n-  return DeviceMemory<GpuSemaphoreState>::MakeFromByteSize(\n+  return DeviceAddress<GpuSemaphoreState>::MakeFromByteSize(\n       ptr_->opaque(), sizeof(GpuSemaphoreState));\n }\n }  // namespace stream_executor"
        },
        {
            "sha": "010bde955cfd2f36f020c3e91f51ac52baf98a7e",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_semaphore.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_semaphore.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_semaphore.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_semaphore.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n@@ -44,7 +44,7 @@ class GpuSemaphore {\n   GpuSemaphoreState& operator*() {\n     return *static_cast<GpuSemaphoreState*>(ptr_->opaque());\n   }\n-  DeviceMemory<GpuSemaphoreState> device();\n+  DeviceAddress<GpuSemaphoreState> device();\n \n  private:\n   explicit GpuSemaphore(std::unique_ptr<MemoryAllocation> alloc)"
        },
        {
            "sha": "915bc3604e8e6e3f8f64dd2fa855a21da91a596b",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_test_kernel_traits.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernel_traits.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernel_traits.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernel_traits.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n@@ -34,22 +34,22 @@ struct Ptrs3 {\n namespace internal {\n \n struct AddI32Kernel {\n-  using KernelType = TypedKernel<DeviceMemory<int32_t>, DeviceMemory<int32_t>,\n-                                 DeviceMemory<int32_t>>;\n+  using KernelType = TypedKernel<DeviceAddress<int32_t>, DeviceAddress<int32_t>,\n+                                 DeviceAddress<int32_t>>;\n };\n \n struct IncrementBy5I32KernelWithCustomArgsPacking {\n-  using KernelType = TypedKernel<DeviceMemory<int32_t>>;\n+  using KernelType = TypedKernel<DeviceAddress<int32_t>>;\n };\n \n struct MulI32Kernel {\n-  using KernelType = TypedKernel<DeviceMemory<int32_t>, DeviceMemory<int32_t>,\n-                                 DeviceMemory<int32_t>>;\n+  using KernelType = TypedKernel<DeviceAddress<int32_t>, DeviceAddress<int32_t>,\n+                                 DeviceAddress<int32_t>>;\n };\n \n struct IncAndCmpKernel {\n-  using KernelType = TypedKernel<DeviceMemory<int32_t>, DeviceMemory<bool>,\n-                                 DeviceMemory<int32_t>>;\n+  using KernelType = TypedKernel<DeviceAddress<int32_t>, DeviceAddress<bool>,\n+                                 DeviceAddress<int32_t>>;\n };\n \n struct AddI32Ptrs3Kernel {\n@@ -58,7 +58,7 @@ struct AddI32Ptrs3Kernel {\n \n struct CopyKernel {\n   using KernelType =\n-      TypedKernel<DeviceMemory<std::byte>, std::array<std::byte, 16>>;\n+      TypedKernel<DeviceAddress<std::byte>, std::array<std::byte, 16>>;\n };\n \n }  // namespace internal"
        },
        {
            "sha": "8da9c72ebbe6e0eab1a5270db287c85823139570",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_test_kernels.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_test_kernels.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -53,8 +53,8 @@ absl::StatusOr<KernelLoaderSpec> GetAddI32TestKernelSpec(\n // int32_t* b, int32_t* c)` under the hood and implements `c[i] = a + b[i]`.\n // It uses a custom argument packing that supplies a constant scalar value of 5\n // to the kernel for `a`, therefore it appears as if the the kernel had the\n-// function signature `void IncI32(DeviceMemory<int32_t> in,\n-// DeviceMemory<int32_t> out)`.\n+// function signature `void IncI32(DeviceAddress<int32_t> in,\n+// DeviceAddress<int32_t> out)`.\n //\n // The main purpose is the testing of the custom argument packing feature.\n absl::StatusOr<KernelLoaderSpec>"
        },
        {
            "sha": "ab0dcdf51c872570b2ec04f8896bad24f3da4852",
            "filename": "third_party/xla/xla/stream_executor/gpu/make_batch_pointers_kernel.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmake_batch_pointers_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmake_batch_pointers_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmake_batch_pointers_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n #include <cstddef>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n@@ -27,8 +27,8 @@ namespace stream_executor::gpu {\n // and look up the kernel in the GPU kernel registry.\n struct MakeBatchPointersKernel {\n   using KernelType =\n-      stream_executor::TypedKernel<stream_executor::DeviceMemoryBase, size_t,\n-                                   size_t, stream_executor::DeviceMemoryBase>;\n+      stream_executor::TypedKernel<stream_executor::DeviceAddressBase, size_t,\n+                                   size_t, stream_executor::DeviceAddressBase>;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "641b96adaa51a1d95551fb60ac319c74d19e1c85",
            "filename": "third_party/xla/xla/stream_executor/gpu/memcpy_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmemcpy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmemcpy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmemcpy_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -15,7 +15,7 @@ limitations under the License.\n \n #include <gtest/gtest.h>\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -33,7 +33,7 @@ TEST(MemcpyTest, PinnedHostMemory) {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto d_ptr,\n                           executor->HostMemoryAllocate(sizeof(int)));\n-  DeviceMemoryBase d_mem(d_ptr->opaque(), sizeof(int));\n+  DeviceAddressBase d_mem(d_ptr->opaque(), sizeof(int));\n \n   int h_ptr;\n   TF_ASSERT_OK(stream->Memcpy(&h_ptr, d_mem, d_mem.size()));"
        },
        {
            "sha": "958c6a5410f14a12b6589077963ef8a317f4560b",
            "filename": "third_party/xla/xla/stream_executor/gpu/multicast_memory.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmulticast_memory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmulticast_memory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmulticast_memory.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n \n namespace stream_executor::gpu {\n \n@@ -33,7 +33,7 @@ class MulticastMemory {\n     return absl::UnimplementedError(\"SubscribeDevice is not implemented.\");\n   }\n \n-  virtual absl::StatusOr<void*> MapMemory(const DeviceMemoryBase& location,\n+  virtual absl::StatusOr<void*> MapMemory(const DeviceAddressBase& location,\n                                           const GpuExecutor* gpu_executor) {\n     return absl::UnimplementedError(\"MapMemory is not implemented.\");\n   }"
        },
        {
            "sha": "2a9d052a3e2112f4078fdea3ffc4b163a121f7e4",
            "filename": "third_party/xla/xla/stream_executor/gpu/prefix_sum_kernel.h",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fprefix_sum_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fprefix_sum_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fprefix_sum_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -19,58 +19,58 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/types.h\"\n \n namespace stream_executor::gpu {\n struct PrefixSumBF16Kernel {\n-  using KernelType = TypedKernel<const DeviceMemory<xla::bfloat16>,\n-                                 DeviceMemory<xla::bfloat16>, size_t>;\n+  using KernelType = TypedKernel<const DeviceAddress<xla::bfloat16>,\n+                                 DeviceAddress<xla::bfloat16>, size_t>;\n };\n struct PrefixSumF16Kernel {\n-  using KernelType = TypedKernel<const DeviceMemory<xla::half>,\n-                                 DeviceMemory<xla::half>, size_t>;\n+  using KernelType = TypedKernel<const DeviceAddress<xla::half>,\n+                                 DeviceAddress<xla::half>, size_t>;\n };\n struct PrefixSumF32Kernel {\n   using KernelType =\n-      TypedKernel<const DeviceMemory<float>, DeviceMemory<float>, size_t>;\n+      TypedKernel<const DeviceAddress<float>, DeviceAddress<float>, size_t>;\n };\n struct PrefixSumF64Kernel {\n   using KernelType =\n-      TypedKernel<const DeviceMemory<double>, DeviceMemory<double>, size_t>;\n+      TypedKernel<const DeviceAddress<double>, DeviceAddress<double>, size_t>;\n };\n struct PrefixSumS8Kernel {\n   using KernelType =\n-      TypedKernel<const DeviceMemory<int8_t>, DeviceMemory<int8_t>, size_t>;\n+      TypedKernel<const DeviceAddress<int8_t>, DeviceAddress<int8_t>, size_t>;\n };\n struct PrefixSumS16Kernel {\n   using KernelType =\n-      TypedKernel<const DeviceMemory<int16_t>, DeviceMemory<int16_t>, size_t>;\n+      TypedKernel<const DeviceAddress<int16_t>, DeviceAddress<int16_t>, size_t>;\n };\n struct PrefixSumS32Kernel {\n   using KernelType =\n-      TypedKernel<const DeviceMemory<int32_t>, DeviceMemory<int32_t>, size_t>;\n+      TypedKernel<const DeviceAddress<int32_t>, DeviceAddress<int32_t>, size_t>;\n };\n struct PrefixSumS64Kernel {\n   using KernelType =\n-      TypedKernel<const DeviceMemory<int64_t>, DeviceMemory<int64_t>, size_t>;\n+      TypedKernel<const DeviceAddress<int64_t>, DeviceAddress<int64_t>, size_t>;\n };\n struct PrefixSumU8Kernel {\n   using KernelType =\n-      TypedKernel<const DeviceMemory<uint8_t>, DeviceMemory<uint8_t>, size_t>;\n+      TypedKernel<const DeviceAddress<uint8_t>, DeviceAddress<uint8_t>, size_t>;\n };\n struct PrefixSumU16Kernel {\n-  using KernelType =\n-      TypedKernel<const DeviceMemory<uint16_t>, DeviceMemory<uint16_t>, size_t>;\n+  using KernelType = TypedKernel<const DeviceAddress<uint16_t>,\n+                                 DeviceAddress<uint16_t>, size_t>;\n };\n struct PrefixSumU32Kernel {\n-  using KernelType =\n-      TypedKernel<const DeviceMemory<uint32_t>, DeviceMemory<uint32_t>, size_t>;\n+  using KernelType = TypedKernel<const DeviceAddress<uint32_t>,\n+                                 DeviceAddress<uint32_t>, size_t>;\n };\n struct PrefixSumU64Kernel {\n-  using KernelType =\n-      TypedKernel<const DeviceMemory<uint64_t>, DeviceMemory<uint64_t>, size_t>;\n+  using KernelType = TypedKernel<const DeviceAddress<uint64_t>,\n+                                 DeviceAddress<uint64_t>, size_t>;\n };\n }  // namespace stream_executor::gpu\n "
        },
        {
            "sha": "88ee3a927aaf27ccf3e55d863ae4115258cb53b0",
            "filename": "third_party/xla/xla/stream_executor/gpu/ragged_all_to_all_kernel.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -21,7 +21,7 @@ limitations under the License.\n #include <array>\n #include <cstdint>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n@@ -32,10 +32,10 @@ inline constexpr int64_t kMaxNumRaggedAllToAllOutputPtrs = 8;\n template <int64_t kVectorSize>\n struct RaggedAllToAllKernel {\n   using KernelType = stream_executor::TypedKernel<\n-      stream_executor::DeviceMemoryBase,\n+      stream_executor::DeviceAddressBase,\n       std::array<void*, kMaxNumRaggedAllToAllOutputPtrs>,\n-      stream_executor::DeviceMemoryBase, stream_executor::DeviceMemoryBase,\n-      stream_executor::DeviceMemoryBase, int64_t, int64_t>;\n+      stream_executor::DeviceAddressBase, stream_executor::DeviceAddressBase,\n+      stream_executor::DeviceAddressBase, int64_t, int64_t>;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "60559910852beda0a9013ae8f1665a61ee4c1079",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 25,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -33,9 +33,9 @@ limitations under the License.\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n-#include \"xla/stream_executor/device_memory_handle.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n+#include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator_kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -62,7 +62,7 @@ constexpr int64_t kRhsRedzoneAlign = 4;\n using RedzoneCheckStatus = RedzoneAllocator::RedzoneCheckStatus;\n \n RedzoneAllocator::RedzoneAllocator(Stream* stream,\n-                                   DeviceMemoryAllocator* memory_allocator,\n+                                   DeviceAddressAllocator* memory_allocator,\n                                    int64_t memory_limit, int64_t redzone_size,\n                                    uint8_t redzone_pattern)\n     : device_ordinal_(stream->parent()->device_ordinal()),\n@@ -74,7 +74,7 @@ RedzoneAllocator::RedzoneAllocator(Stream* stream,\n       redzone_pattern_(redzone_pattern),\n       memory_allocator_(memory_allocator) {}\n \n-absl::StatusOr<DeviceMemory<uint8_t>> RedzoneAllocator::AllocateBytes(\n+absl::StatusOr<DeviceAddress<uint8_t>> RedzoneAllocator::AllocateBytes(\n     int64_t byte_size) {\n   CHECK_GE(byte_size, 0) << \"byte_size must be positive.\";\n   if (byte_size > GetMemoryLimitInBytes()) {\n@@ -85,30 +85,30 @@ absl::StatusOr<DeviceMemory<uint8_t>> RedzoneAllocator::AllocateBytes(\n \n   int64_t rhs_slop = RoundUpToNearest(byte_size, kRhsRedzoneAlign) - byte_size;\n   TF_ASSIGN_OR_RETURN(\n-      OwningDeviceMemory allocated_buffer,\n+      OwningDeviceAddress allocated_buffer,\n       memory_allocator_->Allocate(device_ordinal_,\n                                   byte_size + 2 * redzone_size_ + rhs_slop,\n                                   /*retry_on_failure=*/false));\n   allocated_bytes_excluding_redzones_ += byte_size;\n \n   static_assert(sizeof(uint8_t) == 1, \"Unexpected size\");\n-  DeviceMemory<uint8_t> allocated_buffer_memory(*allocated_buffer);\n+  DeviceAddress<uint8_t> allocated_buffer_memory(*allocated_buffer);\n \n-  DeviceMemory<uint8_t> lhs_redzone =\n+  DeviceAddress<uint8_t> lhs_redzone =\n       allocated_buffer_memory.GetSlice(0, redzone_size_);\n \n-  DeviceMemory<uint8_t> data_chunk =\n+  DeviceAddress<uint8_t> data_chunk =\n       allocated_buffer_memory.GetSlice(redzone_size_, byte_size);\n \n   // Split up the RHS redzone into two pieces:\n   //  - 0 to kRhsRedzoneAlign bytes adjacent to the user buffer, followed by\n   //  - redzone_size_ bytes.\n   // We do this because Stream::Memset32 requires the buffer address and\n   // size to be aligned to 4 bytes.\n-  DeviceMemory<uint8_t> rhs_redzone_slop =\n+  DeviceAddress<uint8_t> rhs_redzone_slop =\n       allocated_buffer_memory.GetSlice(redzone_size_ + byte_size, rhs_slop);\n \n-  DeviceMemory<uint8_t> rhs_redzone_nonslop = allocated_buffer_memory.GetSlice(\n+  DeviceAddress<uint8_t> rhs_redzone_nonslop = allocated_buffer_memory.GetSlice(\n       redzone_size_ + byte_size + rhs_slop, redzone_size_);\n \n   uint8_t pattern_arr[] = {redzone_pattern_, redzone_pattern_, redzone_pattern_,\n@@ -131,7 +131,7 @@ absl::StatusOr<DeviceMemory<uint8_t>> RedzoneAllocator::AllocateBytes(\n //\n // Slower, but gives a more useful error message.\n static absl::StatusOr<RedzoneCheckStatus> CheckRedzoneHost(\n-    DeviceMemoryBase redzone, DeviceMemoryBase user_allocation,\n+    DeviceAddressBase redzone, DeviceAddressBase user_allocation,\n     absl::string_view name, Stream* stream, uint8_t redzone_pattern) {\n   uint64_t size = redzone.size();\n   auto redzone_data = std::make_unique<uint8_t[]>(size);\n@@ -165,8 +165,8 @@ static absl::StatusOr<RedzoneCheckStatus> CheckRedzoneHost(\n //\n // Increment out_param if mismatch occurs.\n static absl::Status RunRedzoneChecker(\n-    Stream* stream, const DeviceMemory<uint8_t>& redzone,\n-    uint8_t redzone_pattern, const DeviceMemory<uint64_t>& out_param,\n+    Stream* stream, const DeviceAddress<uint8_t>& redzone,\n+    uint8_t redzone_pattern, const DeviceAddress<uint64_t>& out_param,\n     gpu::RedzoneAllocatorKernel::KernelType& comparison_kernel) {\n   StreamExecutor* executor = stream->parent();\n \n@@ -192,7 +192,7 @@ static absl::Status RunRedzoneChecker(\n //\n // This function is blocking, since redzone failing is a rare event.\n static absl::Status ReinitializeRedzone(Stream* stream,\n-                                        DeviceMemoryBase redzone,\n+                                        DeviceAddressBase redzone,\n                                         uint8_t redzone_pattern) {\n   absl::FixedArray<uint8_t> redzone_array(redzone.size());\n   redzone_array.fill(redzone_pattern);\n@@ -206,8 +206,8 @@ static absl::Status ReinitializeRedzone(Stream* stream,\n //\n // Precondition: the memory pointed out by out_param is zeroed.\n static absl::StatusOr<RedzoneCheckStatus> CheckRedzonesForBuffer(\n-    Stream* stream, DeviceMemoryBase memory,\n-    const DeviceMemory<uint64_t>& out_param,\n+    Stream* stream, DeviceAddressBase memory,\n+    const DeviceAddress<uint64_t>& out_param,\n     gpu::RedzoneAllocatorKernel::KernelType& comparison_kernel,\n     int64_t user_allocation_size, uint64_t redzone_size,\n     uint8_t redzone_pattern) {\n@@ -216,14 +216,14 @@ static absl::StatusOr<RedzoneCheckStatus> CheckRedzonesForBuffer(\n       user_allocation_size;\n   CHECK_EQ(memory.size(), user_allocation_size + rhs_slop + 2 * redzone_size);\n \n-  DeviceMemory<uint8_t> buffer_uint8(memory);\n-  DeviceMemory<uint8_t> lhs_redzone =\n+  DeviceAddress<uint8_t> buffer_uint8(memory);\n+  DeviceAddress<uint8_t> lhs_redzone =\n       buffer_uint8.GetSlice(0,\n                             /*element_count=*/redzone_size);\n-  DeviceMemory<uint8_t> user_allocation =\n+  DeviceAddress<uint8_t> user_allocation =\n       buffer_uint8.GetSlice(redzone_size,\n                             /*element_count=*/user_allocation_size);\n-  DeviceMemory<uint8_t> rhs_redzone =\n+  DeviceAddress<uint8_t> rhs_redzone =\n       buffer_uint8.GetSlice(redzone_size + user_allocation_size,\n                             /*element_count=*/redzone_size + rhs_slop);\n \n@@ -257,9 +257,9 @@ static absl::StatusOr<RedzoneCheckStatus> CheckRedzonesForBuffer(\n   return RedzoneCheckStatus::OK();\n }\n \n-absl::StatusOr<DeviceMemoryBase> RedzoneAllocator::CreateBuffer(\n+absl::StatusOr<DeviceAddressBase> RedzoneAllocator::CreateBuffer(\n     const xla::Shape& shape, bool initialize_buffers, int64_t& rng_state) {\n-  TF_ASSIGN_OR_RETURN(stream_executor::DeviceMemoryBase buffer,\n+  TF_ASSIGN_OR_RETURN(stream_executor::DeviceAddressBase buffer,\n                       AllocateBytes(xla::ShapeUtil::ByteSizeOf(shape)));\n   if (initialize_buffers) {\n     xla::gpu::InitializeBuffer(stream(), shape.element_type(), &rng_state,\n@@ -275,15 +275,15 @@ absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {\n                       gpu::GpuKernelRegistry::GetGlobalRegistry()\n                           .LoadKernel<gpu::RedzoneAllocatorKernel>(executor));\n \n-  DeviceMemoryHandle out_param(executor, executor->AllocateScalar<uint64_t>());\n+  DeviceAddressHandle out_param(executor, executor->AllocateScalar<uint64_t>());\n   TF_RETURN_IF_ERROR(\n       stream_->MemZero(out_param.memory_ptr(), sizeof(uint64_t)));\n \n   for (const auto& buf_and_size : allocated_buffers_) {\n     TF_ASSIGN_OR_RETURN(\n         RedzoneCheckStatus redzone_status,\n         CheckRedzonesForBuffer(stream_, *buf_and_size.first,\n-                               DeviceMemory<uint64_t>(out_param.memory()),\n+                               DeviceAddress<uint64_t>(out_param.memory()),\n                                kernel, buf_and_size.second, redzone_size_,\n                                redzone_pattern_));\n     if (!redzone_status.ok()) {"
        },
        {
            "sha": "ab8ccb8d2d94adcb9b16534ba26baa126eee6f05",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator.h",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,8 +24,8 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n \n@@ -50,7 +50,7 @@ class RedzoneAllocator : public ScratchAllocator {\n   // Maximum number of thread blocks to be used for redzone checker kernel\n   static constexpr int64_t kMaxNumThreadBlocksForKernel = 32768;\n \n-  RedzoneAllocator(Stream* stream, DeviceMemoryAllocator* memory_allocator,\n+  RedzoneAllocator(Stream* stream, DeviceAddressAllocator* memory_allocator,\n                    int64_t memory_limit = (1LL << 32),  // 4GB\n                    int64_t redzone_size = kDefaultRedzoneSize,\n                    uint8_t redzone_pattern = kDefaultRedzonePattern);\n@@ -62,7 +62,7 @@ class RedzoneAllocator : public ScratchAllocator {\n     return allocated_bytes_excluding_redzones_;\n   }\n \n-  absl::StatusOr<DeviceMemory<uint8_t>> AllocateBytes(\n+  absl::StatusOr<DeviceAddress<uint8_t>> AllocateBytes(\n       int64_t byte_size) override;\n \n   // Non-empty redzone check status implies that there was a write into a\n@@ -109,9 +109,9 @@ class RedzoneAllocator : public ScratchAllocator {\n \n   // Create a buffer for a given operation using redzone checker, initialize\n   // based on a given rng state.\n-  absl::StatusOr<DeviceMemoryBase> CreateBuffer(const xla::Shape& shape,\n-                                                bool initialize_buffers,\n-                                                int64_t& rng_state);\n+  absl::StatusOr<DeviceAddressBase> CreateBuffer(const xla::Shape& shape,\n+                                                 bool initialize_buffers,\n+                                                 int64_t& rng_state);\n \n  private:\n   const int device_ordinal_;\n@@ -127,16 +127,16 @@ class RedzoneAllocator : public ScratchAllocator {\n   const int64_t redzone_size_;\n \n   const uint8_t redzone_pattern_;\n-  DeviceMemoryAllocator* memory_allocator_;\n+  DeviceAddressAllocator* memory_allocator_;\n \n   // The second element of the pair is the size of the user allocation.  This\n   // isn't necessarily just first.size() - 2 * redzone_size_ because when the\n   // user allocation size is not a multiple of 4 bytes, we round up the size of\n   // the RHS redzone.\n   //\n   // ScratchAllocators need to free all allocated memory on destruction so we\n-  // use `OwningDeviceMemory` here.\n-  std::vector<std::pair<OwningDeviceMemory, int64_t>> allocated_buffers_;\n+  // use `OwningDeviceAddress` here.\n+  std::vector<std::pair<OwningDeviceAddress, int64_t>> allocated_buffers_;\n \n   int64_t allocated_bytes_excluding_redzones_ = 0;\n };"
        },
        {
            "sha": "682e1efb19bd1536a762d7282311ad51eaad643d",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator_kernel.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -18,14 +18,14 @@ limitations under the License.\n \n #include <cstdint>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n \n struct RedzoneAllocatorKernel {\n-  using KernelType = TypedKernel<DeviceMemory<uint8_t>, uint8_t, uint64_t,\n-                                 DeviceMemory<uint64_t>>;\n+  using KernelType = TypedKernel<DeviceAddress<uint8_t>, uint8_t, uint64_t,\n+                                 DeviceAddress<uint64_t>>;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "b18c5025d6839194b97dc0a01327c05bcfb0cd5a",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n@@ -67,16 +67,16 @@ TEST(RedzoneAllocatorTest, WriteToRedzone) {\n                              /*memory_limit=*/(1LL << 32),\n                              /*redzone_size=*/kRedzoneSize,\n                              /*redzone_pattern=*/kRedzonePattern);\n-  TF_ASSERT_OK_AND_ASSIGN(DeviceMemory<uint8_t> buf,\n+  TF_ASSERT_OK_AND_ASSIGN(DeviceAddress<uint8_t> buf,\n                           allocator.AllocateBytes(/*byte_size=*/kAllocSize));\n   EXPECT_REDZONE_OK(allocator.CheckRedzones());\n \n   char* buf_addr = reinterpret_cast<char*>(buf.opaque());\n-  DeviceMemoryBase lhs_redzone(buf_addr - kRedzoneSize, kRedzoneSize);\n-  DeviceMemoryBase rhs_redzone(buf_addr + kAllocSize, kRedzoneSize);\n+  DeviceAddressBase lhs_redzone(buf_addr - kRedzoneSize, kRedzoneSize);\n+  DeviceAddressBase rhs_redzone(buf_addr + kAllocSize, kRedzoneSize);\n \n   // Check that the redzones are in fact filled with kRedzonePattern.\n-  auto check_redzone = [&](DeviceMemoryBase redzone, absl::string_view name) {\n+  auto check_redzone = [&](DeviceAddressBase redzone, absl::string_view name) {\n     std::vector<uint8_t> host_buf(kRedzoneSize);\n     TF_ASSERT_OK(stream->Memcpy(host_buf.data(), redzone, kRedzoneSize));\n     TF_ASSERT_OK(stream->BlockHostUntilDone());\n@@ -100,13 +100,15 @@ TEST(RedzoneAllocatorTest, WriteToRedzone) {\n   // Modifies a redzone, checks that RedzonesAreUnmodified returns false, then\n   // reverts it back to its original value and checks that RedzonesAreUnmodified\n   // returns true.\n-  auto modify_redzone = [&](DeviceMemoryBase redzone, int64_t offset,\n+  auto modify_redzone = [&](DeviceAddressBase redzone, int64_t offset,\n                             absl::string_view name) {\n     SCOPED_TRACE(absl::StrCat(name, \", offset=\", offset));\n-    DeviceMemoryBase redzone_at_offset(\n+    DeviceAddressBase redzone_at_offset(\n         reinterpret_cast<char*>(redzone.opaque()) + offset, 1);\n     char old_redzone_value = 0;\n-    { EXPECT_REDZONE_OK(allocator.CheckRedzones()); }\n+    {\n+      EXPECT_REDZONE_OK(allocator.CheckRedzones());\n+    }\n     TF_ASSERT_OK(stream->Memcpy(&old_redzone_value, redzone_at_offset, 1));\n     TF_ASSERT_OK(stream->MemZero(&redzone_at_offset, 1));\n     EXPECT_REDZONE_VIOLATION(allocator.CheckRedzones());"
        },
        {
            "sha": "3c749d68f52f83e1145471826cad62055aab5692",
            "filename": "third_party/xla/xla/stream_executor/gpu/repeat_buffer_kernel.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Frepeat_buffer_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Frepeat_buffer_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Frepeat_buffer_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n #include <cstdint>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n \n namespace stream_executor::gpu {\n@@ -27,7 +27,7 @@ namespace stream_executor::gpu {\n // and look up the kernel in the GPU kernel registry.\n struct RepeatBufferKernel {\n   using KernelType =\n-      stream_executor::TypedKernel<stream_executor::DeviceMemoryBase, int64_t,\n+      stream_executor::TypedKernel<stream_executor::DeviceAddressBase, int64_t,\n                                    int64_t>;\n };\n "
        },
        {
            "sha": "fbae8ef7a74c91500ab8ca82c2b7d33841bed1da",
            "filename": "third_party/xla/xla/stream_executor/gpu/repeat_buffer_kernel_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Frepeat_buffer_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Frepeat_buffer_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Frepeat_buffer_kernel_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"absl/strings/ascii.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -64,7 +64,7 @@ TEST_F(RepeatBufferKernelTest, CreateRepeatedBufferAndTestResult) {\n   // We use a non-dividing number of elements here to ensure that also the last\n   // non-complete section is handled correctly.\n   constexpr int kNumberOfTotalElements = 129;\n-  DeviceMemory<float> buffer =\n+  DeviceAddress<float> buffer =\n       executor_->AllocateArray<float>(kNumberOfTotalElements);\n \n   CHECK_OK(stream->MemcpyH2D(absl::MakeConstSpan(kInitialBuf), &buffer));\n@@ -78,7 +78,7 @@ TEST_F(RepeatBufferKernelTest, CreateRepeatedBufferAndTestResult) {\n       kernel.Launch(\n           ThreadDim{kNumberOfRepeatedElements * sizeof(float), 1, 1},\n           BlockDim{1, 1, 1}, stream.get(),\n-          static_cast<const DeviceMemoryBase&>(buffer),\n+          static_cast<const DeviceAddressBase&>(buffer),\n           static_cast<int64_t>(kNumberOfRepeatedElements * sizeof(float)),\n           static_cast<int64_t>(kNumberOfTotalElements * sizeof(float))),\n       absl_testing::IsOk());"
        },
        {
            "sha": "8e30e4b04e2f10608936e3af654c7ad2d143a14d",
            "filename": "third_party/xla/xla/stream_executor/gpu/topk_kernel.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftopk_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftopk_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftopk_kernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n \n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel.h\"\n namespace stream_executor::gpu {\n \n@@ -32,9 +32,9 @@ static constexpr size_t kTopKMaxThreadsPerBlock = 1024;\n template <size_t K, typename KT, typename VT>\n struct TopKKernel {\n   using KernelType =\n-      stream_executor::TypedKernel<stream_executor::DeviceMemory<KT>, size_t,\n-                                   stream_executor::DeviceMemory<KT>,\n-                                   stream_executor::DeviceMemory<uint32_t>,\n+      stream_executor::TypedKernel<stream_executor::DeviceAddress<KT>, size_t,\n+                                   stream_executor::DeviceAddress<KT>,\n+                                   stream_executor::DeviceAddress<uint32_t>,\n                                    size_t>;\n };\n "
        },
        {
            "sha": "580889b3432ed0ea33414b3ae59ae51ad83ad678",
            "filename": "third_party/xla/xla/stream_executor/gpu_solver_context.h",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu_solver_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu_solver_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu_solver_context.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -46,38 +46,38 @@ class GpuSolverContext {\n   // `as` is a list of pointers to the batch_size individual n x n matrices\n   // that make up the input array.\n   virtual absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                                    DeviceMemory<float*> as, int lda,\n-                                    DeviceMemory<int> lapack_info,\n+                                    DeviceAddress<float*> as, int lda,\n+                                    DeviceAddress<int> lapack_info,\n                                     int batch_size) = 0;\n   virtual absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                                    DeviceMemory<double*> as, int lda,\n-                                    DeviceMemory<int> lapack_info,\n+                                    DeviceAddress<double*> as, int lda,\n+                                    DeviceAddress<int> lapack_info,\n                                     int batch_size) = 0;\n   virtual absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                                    DeviceMemory<std::complex<float>*> as,\n-                                    int lda, DeviceMemory<int> lapack_info,\n+                                    DeviceAddress<std::complex<float>*> as,\n+                                    int lda, DeviceAddress<int> lapack_info,\n                                     int batch_size) = 0;\n   virtual absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                                    DeviceMemory<std::complex<double>*> as,\n-                                    int lda, DeviceMemory<int> lapack_info,\n+                                    DeviceAddress<std::complex<double>*> as,\n+                                    int lda, DeviceAddress<int> lapack_info,\n                                     int batch_size) = 0;\n \n   virtual absl::Status Potrf(blas::UpperLower uplo, int n,\n-                             DeviceMemory<float> a, int lda,\n-                             DeviceMemory<int> lapack_info,\n-                             DeviceMemory<float> workspace) = 0;\n+                             DeviceAddress<float> a, int lda,\n+                             DeviceAddress<int> lapack_info,\n+                             DeviceAddress<float> workspace) = 0;\n   virtual absl::Status Potrf(blas::UpperLower uplo, int n,\n-                             DeviceMemory<double> a, int lda,\n-                             DeviceMemory<int> lapack_info,\n-                             DeviceMemory<double> workspace) = 0;\n+                             DeviceAddress<double> a, int lda,\n+                             DeviceAddress<int> lapack_info,\n+                             DeviceAddress<double> workspace) = 0;\n   virtual absl::Status Potrf(blas::UpperLower uplo, int n,\n-                             DeviceMemory<std::complex<float>> a, int lda,\n-                             DeviceMemory<int> lapack_info,\n-                             DeviceMemory<std::complex<float>> workspace) = 0;\n+                             DeviceAddress<std::complex<float>> a, int lda,\n+                             DeviceAddress<int> lapack_info,\n+                             DeviceAddress<std::complex<float>> workspace) = 0;\n   virtual absl::Status Potrf(blas::UpperLower uplo, int n,\n-                             DeviceMemory<std::complex<double>> a, int lda,\n-                             DeviceMemory<int> lapack_info,\n-                             DeviceMemory<std::complex<double>> workspace) = 0;\n+                             DeviceAddress<std::complex<double>> a, int lda,\n+                             DeviceAddress<int> lapack_info,\n+                             DeviceAddress<std::complex<double>> workspace) = 0;\n \n   // Returns the max size of the `workspace` required by Potrf and PotrfBatched,\n   // in number of elements of `type`."
        },
        {
            "sha": "90f35bd0eb33eb5151a1710aaab3357fee84050f",
            "filename": "third_party/xla/xla/stream_executor/host/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -75,7 +75,7 @@ cc_library(\n     ],\n     deps = [\n         \":host_event\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_common\",\n@@ -114,8 +114,8 @@ cc_library(\n         \":host_event\",\n         \":host_stream\",\n         \":host_stream_factory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:generic_memory_allocation\",\n         \"//xla/stream_executor:generic_memory_allocator\","
        },
        {
            "sha": "a8f70bd25ccac0bf549b4152d72eae66bb804abc",
            "filename": "third_party/xla/xla/stream_executor/host/host_executor.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -33,8 +33,8 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/generic_memory_allocation.h\"\n #include \"xla/stream_executor/generic_memory_allocator.h\"\n@@ -79,34 +79,34 @@ bool HostExecutor::DeviceMemoryUsage(int64_t* free, int64_t* total) const {\n   return true;\n }\n \n-DeviceMemoryBase HostExecutor::Allocate(uint64_t size, int64_t memory_space) {\n+DeviceAddressBase HostExecutor::Allocate(uint64_t size, int64_t memory_space) {\n   CHECK_EQ(memory_space, 0);\n   // Use a minimum alignment of 64 bytes to be friendly to AVX512 code.\n   // This should probably be kept in sync with\n   // tsl::Allocator::kAllocatorAlignment.\n-  return DeviceMemoryBase(\n+  return DeviceAddressBase(\n       tsl::port::AlignedMalloc(size, /*minimum_alignment=*/64), size);\n }\n \n-void HostExecutor::Deallocate(DeviceMemoryBase* mem) {\n+void HostExecutor::Deallocate(DeviceAddressBase* mem) {\n   tsl::port::AlignedFree(mem->opaque());\n }\n \n-absl::Status HostExecutor::SynchronousMemZero(DeviceMemoryBase* location,\n+absl::Status HostExecutor::SynchronousMemZero(DeviceAddressBase* location,\n                                               uint64_t size) {\n   memset(location->opaque(), 0, size);\n   return absl::OkStatus();\n }\n \n-absl::Status HostExecutor::SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n+absl::Status HostExecutor::SynchronousMemcpy(DeviceAddressBase* gpu_dst,\n                                              const void* host_src,\n                                              uint64_t size) {\n   memcpy(gpu_dst->opaque(), host_src, size);\n   return absl::OkStatus();\n }\n \n absl::Status HostExecutor::SynchronousMemcpy(void* host_dst,\n-                                             const DeviceMemoryBase& gpu_src,\n+                                             const DeviceAddressBase& gpu_src,\n                                              uint64_t size) {\n   memcpy(host_dst, gpu_src.opaque(), size);\n   return absl::OkStatus();"
        },
        {
            "sha": "50475bb0116296ae542b5b99d0dfdff2d9395ad2",
            "filename": "third_party/xla/xla/stream_executor/host/host_executor.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -23,8 +23,8 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/generic_memory_allocation.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -56,8 +56,8 @@ class HostExecutor : public StreamExecutorCommon {\n   absl::StatusOr<std::unique_ptr<Kernel>> LoadKernel(\n       const KernelLoaderSpec& spec) override;\n \n-  DeviceMemoryBase Allocate(uint64_t size, int64_t memory_space) override;\n-  void Deallocate(DeviceMemoryBase* mem) override;\n+  DeviceAddressBase Allocate(uint64_t size, int64_t memory_space) override;\n+  void Deallocate(DeviceAddressBase* mem) override;\n \n   absl::StatusOr<std::unique_ptr<MemoryAllocation>> HostMemoryAllocate(\n       uint64_t size) override {\n@@ -68,13 +68,13 @@ class HostExecutor : public StreamExecutorCommon {\n   }\n \n   bool SynchronizeAllActivity() override { return true; }\n-  absl::Status SynchronousMemZero(DeviceMemoryBase* location,\n+  absl::Status SynchronousMemZero(DeviceAddressBase* location,\n                                   uint64_t size) override;\n \n-  absl::Status SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n+  absl::Status SynchronousMemcpy(DeviceAddressBase* gpu_dst,\n                                  const void* host_src, uint64_t size) override;\n   absl::Status SynchronousMemcpy(void* host_dst,\n-                                 const DeviceMemoryBase& gpu_src,\n+                                 const DeviceAddressBase& gpu_src,\n                                  uint64_t size) override;\n \n   void DeallocateStream(Stream* stream) override;"
        },
        {
            "sha": "39b70eed0412b8e8359d22dd4e7f3c3ee5e4f539",
            "filename": "third_party/xla/xla/stream_executor/host/host_stream.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_stream.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_stream.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_stream.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/synchronization/notification.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/host/host_event.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -40,37 +40,38 @@ HostStream::HostStream(StreamExecutor* executor) : StreamCommon(executor) {}\n \n HostStream::~HostStream() { parent()->DeallocateStream(this); }\n \n-absl::Status HostStream::Memcpy(DeviceMemoryBase* gpu_dst,\n-                                const DeviceMemoryBase& gpu_src,\n+absl::Status HostStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   void* dst_mem = gpu_dst->opaque();\n   void* src_mem = const_cast<void*>(gpu_src.opaque());\n   memcpy(dst_mem, src_mem, size);\n   return absl::OkStatus();\n }\n \n-absl::Status HostStream::Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+absl::Status HostStream::Memcpy(void* host_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   void* src_mem = const_cast<void*>(gpu_src.opaque());\n   memcpy(host_dst, src_mem, size);\n   return absl::OkStatus();\n }\n \n-absl::Status HostStream::Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n-                                uint64_t size) {\n+absl::Status HostStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const void* host_src, uint64_t size) {\n   void* dst_mem = gpu_dst->opaque();\n   memcpy(dst_mem, host_src, size);\n   return absl::OkStatus();\n }\n \n-absl::Status HostStream::Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+absl::Status HostStream::Memset32(DeviceAddressBase* location, uint32_t pattern,\n                                   uint64_t size) {\n   void* gpu_mem = location->opaque();\n   memset(gpu_mem, pattern, size);\n   return absl::OkStatus();\n }\n \n-absl::Status HostStream::MemZero(DeviceMemoryBase* location, uint64_t size) {\n+absl::Status HostStream::MemZero(DeviceAddressBase* location, uint64_t size) {\n   void* gpu_mem = location->opaque();\n   memset(gpu_mem, 0, size);\n   return absl::OkStatus();"
        },
        {
            "sha": "e8395aad2989c9ad6e605dac6e235257ba3e72d6",
            "filename": "third_party/xla/xla/stream_executor/host/host_stream.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_stream.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_stream.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_stream.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n \n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_common.h\"\n@@ -42,14 +42,14 @@ class HostStream : public StreamCommon {\n   absl::Status WaitFor(Stream* other) override;\n   absl::Status WaitFor(Event* event) override;\n   absl::Status RecordEvent(Event* event) override;\n-  absl::Status MemZero(DeviceMemoryBase* location, uint64_t size) override;\n-  absl::Status Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+  absl::Status MemZero(DeviceAddressBase* location, uint64_t size) override;\n+  absl::Status Memset32(DeviceAddressBase* location, uint32_t pattern,\n                         uint64_t size) override;\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst, const void* host_src,\n                       uint64_t size) override;\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst,\n-                      const DeviceMemoryBase& gpu_src, uint64_t size) override;\n-  absl::Status Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst,\n+                      const DeviceAddressBase& gpu_src, uint64_t size) override;\n+  absl::Status Memcpy(void* host_dst, const DeviceAddressBase& gpu_src,\n                       uint64_t size) override;\n   absl::Status DoHostCallbackWithStatus(\n       absl::AnyInvocable<absl::Status() &&> callback) override;"
        },
        {
            "sha": "77726488f69bdda7d8b1258bba3bda66caae98bf",
            "filename": "third_party/xla/xla/stream_executor/integrations/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -44,8 +44,8 @@ cc_library(\n     hdrs = [\"tf_allocator_adapter.h\"],\n     deps = [\n         \"//xla:shape_util\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -120,7 +120,7 @@ xla_cc_test(\n         \":tf_allocator_adapter\",\n         \"//xla/service:cpu_plugin\",\n         \"//xla/service:platform_util\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\","
        },
        {
            "sha": "733d1a7058c6b2c29acf1eb5e74c6776aa017be9",
            "filename": "third_party/xla/xla/stream_executor/integrations/device_mem_allocator.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fdevice_mem_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fdevice_mem_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fdevice_mem_allocator.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -61,7 +61,7 @@ class DeviceMemAllocator : public tsl::SubAllocator {\n \n     if (ptr != nullptr) {\n       VisitFree(ptr, device_id_.value(), num_bytes);\n-      DeviceMemoryBase device_ptr(ptr);\n+      DeviceAddressBase device_ptr(ptr);\n       stream_exec_->Deallocate(&device_ptr);\n     }\n   }"
        },
        {
            "sha": "84e2580aff9d90dd9f51380f5b4150e670eeee56",
            "filename": "third_party/xla/xla/stream_executor/integrations/stream_executor_allocator_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -74,7 +74,7 @@ TEST(StreamExecutorAllocatorTest, GetMemoryTypeReturnsHostPinnedForHostMemory) {\n             stream_executor_allocator.GetMemoryType());\n }\n \n-TEST(StreamExecutorAllocatorTest, GetMemoryTypeReturnsDeviceForDeviceMemory) {\n+TEST(StreamExecutorAllocatorTest, GetMemoryTypeReturnsDeviceForDeviceAddress) {\n   auto allocator = std::make_unique<GenericMemoryAllocator>(\n       [](uint64_t size) -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n         return absl::InternalError(\"Failed to allocate memory\");"
        },
        {
            "sha": "c6fa05eddfdbf9dfa44bac94b6d00029bff10a07",
            "filename": "third_party/xla/xla/stream_executor/integrations/tf_allocator_adapter.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,8 +24,8 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/layout.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -35,18 +35,18 @@ limitations under the License.\n \n namespace stream_executor {\n \n-TfAllocatorAdapter::TfAllocatorAdapter(tsl::Allocator *wrapped, Stream *stream)\n-    : DeviceMemoryAllocator(CHECK_NOTNULL(stream)->parent()->GetPlatform()),\n+TfAllocatorAdapter::TfAllocatorAdapter(tsl::Allocator* wrapped, Stream* stream)\n+    : DeviceAddressAllocator(CHECK_NOTNULL(stream)->parent()->GetPlatform()),\n       wrapped_(wrapped),\n       stream_(stream) {}\n \n-TfAllocatorAdapter::TfAllocatorAdapter(tsl::Allocator *wrapped,\n-                                       const Platform *platform)\n-    : DeviceMemoryAllocator(platform), wrapped_(wrapped), stream_(nullptr) {}\n+TfAllocatorAdapter::TfAllocatorAdapter(tsl::Allocator* wrapped,\n+                                       const Platform* platform)\n+    : DeviceAddressAllocator(platform), wrapped_(wrapped), stream_(nullptr) {}\n \n TfAllocatorAdapter::~TfAllocatorAdapter() {}\n \n-absl::StatusOr<OwningDeviceMemory> TfAllocatorAdapter::Allocate(\n+absl::StatusOr<OwningDeviceAddress> TfAllocatorAdapter::Allocate(\n     int device_ordinal, uint64_t size, bool retry_on_failure,\n     int64_t memory_space) {\n   tsl::AllocationAttributes attrs;\n@@ -60,11 +60,12 @@ absl::StatusOr<OwningDeviceMemory> TfAllocatorAdapter::Allocate(\n           size, memory_space == xla::Layout::kHostMemorySpace);\n     }\n   }\n-  return OwningDeviceMemory(DeviceMemoryBase(data, size), device_ordinal, this);\n+  return OwningDeviceAddress(DeviceAddressBase(data, size), device_ordinal,\n+                             this);\n }\n \n absl::Status TfAllocatorAdapter::Deallocate(int device_ordinal,\n-                                            DeviceMemoryBase mem) {\n+                                            DeviceAddressBase mem) {\n   wrapped_->DeallocateRaw(mem.opaque());\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "8f54fc429afc3a7769bc00e43171a8279a595855",
            "filename": "third_party/xla/xla/stream_executor/integrations/tf_allocator_adapter.h",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -28,8 +28,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -41,7 +41,7 @@ namespace stream_executor {\n //\n // Assumes that the Tensorflow allocator permits asynchronous deallocation:\n // see comment on `AllowsAsynchronousDeallocation()`.\n-class TfAllocatorAdapter : public DeviceMemoryAllocator {\n+class TfAllocatorAdapter : public DeviceAddressAllocator {\n  public:\n   // stream: a Stream on which the allocator can only be used. If non-null, the\n   // allocator can not be used on any other stream.\n@@ -52,11 +52,12 @@ class TfAllocatorAdapter : public DeviceMemoryAllocator {\n \n   ~TfAllocatorAdapter() override;\n \n-  absl::StatusOr<OwningDeviceMemory> Allocate(int device_ordinal, uint64_t size,\n-                                              bool retry_on_failure,\n-                                              int64_t memory_space) override;\n+  absl::StatusOr<OwningDeviceAddress> Allocate(int device_ordinal,\n+                                               uint64_t size,\n+                                               bool retry_on_failure,\n+                                               int64_t memory_space) override;\n \n-  absl::Status Deallocate(int device_ordinal, DeviceMemoryBase mem) override;\n+  absl::Status Deallocate(int device_ordinal, DeviceAddressBase mem) override;\n \n   // The Tensorflow BFC allocator used on GPU allows host-side deallocation\n   // before GPU execution takes place. Tensorflow uses the ordering of the main\n@@ -79,7 +80,7 @@ class TfAllocatorAdapter : public DeviceMemoryAllocator {\n // Adapter class that wraps per-device TF allocators with corresponding streams\n // as a TfAllocatorAdapter. Assumes that the Tensorflow allocator permits\n // asynchronous deallocation; see comment on `AllowsAsynchronousDeallocation()`.\n-class MultiDeviceAdapter : public DeviceMemoryAllocator {\n+class MultiDeviceAdapter : public DeviceAddressAllocator {\n  public:\n   struct AllocatorInfo {\n     std::unique_ptr<tsl::Allocator> allocator;\n@@ -99,9 +100,9 @@ class MultiDeviceAdapter : public DeviceMemoryAllocator {\n           platform(platform) {}\n   };\n \n-  MultiDeviceAdapter(const Platform *platform,\n+  MultiDeviceAdapter(const Platform* platform,\n                      std::vector<AllocatorInfo> tf_allocators)\n-      : DeviceMemoryAllocator(platform) {\n+      : DeviceAddressAllocator(platform) {\n     tf_allocators_.reserve(tf_allocators.size());\n     for (AllocatorInfo &info : tf_allocators) {\n       auto &per_device_allocators =\n@@ -127,9 +128,10 @@ class MultiDeviceAdapter : public DeviceMemoryAllocator {\n     }\n   }\n \n-  absl::StatusOr<OwningDeviceMemory> Allocate(int device_ordinal, uint64_t size,\n-                                              bool retry_on_failure,\n-                                              int64_t memory_space) override {\n+  absl::StatusOr<OwningDeviceAddress> Allocate(int device_ordinal,\n+                                               uint64_t size,\n+                                               bool retry_on_failure,\n+                                               int64_t memory_space) override {\n     // memory_space is used here to select allocator. This isn't a need to pass\n     // it any lower to TfAllocatorAdapter.\n     auto it = memory_space_to_per_device_allocators_.find(memory_space);\n@@ -144,7 +146,7 @@ class MultiDeviceAdapter : public DeviceMemoryAllocator {\n     return result;\n   }\n \n-  absl::Status Deallocate(int device_ordinal, DeviceMemoryBase mem) override {\n+  absl::Status Deallocate(int device_ordinal, DeviceAddressBase mem) override {\n     if (mem.opaque() == nullptr) return absl::OkStatus();\n     // Memory space is not passed to deallocate, look up in\n     // buffer_memory_spaces_."
        },
        {
            "sha": "ecf2f97402c3f6fd6543696b994880599ce40404",
            "filename": "third_party/xla/xla/stream_executor/integrations/tf_allocator_adapter_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Ftf_allocator_adapter_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -87,27 +87,27 @@ TEST(MultiDeviceAdapter, UsesCorrectAllocator) {\n                      /*memory_space=*/1, /*device_ordinal=*/0);\n   infos.emplace_back(std::make_unique<TestAllocator>(0x4000), stream.get(),\n                      /*memory_space=*/1, /*device_ordinal=*/1);\n-  std::unique_ptr<DeviceMemoryAllocator> allocator =\n+  std::unique_ptr<DeviceAddressAllocator> allocator =\n       std::make_unique<MultiDeviceAdapter>(platform, std::move(infos));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      OwningDeviceMemory buff0,\n+      OwningDeviceAddress buff0,\n       allocator->Allocate(/*device_ordinal=*/0, 4, false, /*memory_space=*/0));\n   CHECK_EQ(reinterpret_cast<size_t>(buff0->opaque()), 0x1001);\n   TF_ASSERT_OK_AND_ASSIGN(\n-      OwningDeviceMemory buff1,\n+      OwningDeviceAddress buff1,\n       allocator->Allocate(/*device_ordinal=*/0, 4, false, /*memory_space=*/0));\n   CHECK_EQ(reinterpret_cast<size_t>(buff1->opaque()), 0x1002);\n   TF_ASSERT_OK_AND_ASSIGN(\n-      OwningDeviceMemory buff2,\n+      OwningDeviceAddress buff2,\n       allocator->Allocate(/*device_ordinal=*/0, 4, false, /*memory_space=*/1));\n   CHECK_EQ(reinterpret_cast<size_t>(buff2->opaque()), 0x3001);\n   TF_ASSERT_OK_AND_ASSIGN(\n-      OwningDeviceMemory buff3,\n+      OwningDeviceAddress buff3,\n       allocator->Allocate(/*device_ordinal=*/1, 4, false, /*memory_space=*/0));\n   CHECK_EQ(reinterpret_cast<size_t>(buff3->opaque()), 0x2001);\n   TF_ASSERT_OK_AND_ASSIGN(\n-      OwningDeviceMemory buff4,\n+      OwningDeviceAddress buff4,\n       allocator->Allocate(/*device_ordinal=*/1, 4, false, /*memory_space=*/1));\n   CHECK_EQ(reinterpret_cast<size_t>(buff4->opaque()), 0x4001);\n }\n@@ -126,27 +126,27 @@ TEST(MultiDeviceAdapter, DeallocationWithDifferentAllocator) {\n       std::make_unique<TestAllocator>(0x1000, allocations), stream.get(),\n       /*memory_space=*/0, /*device_ordinal=*/0);\n \n-  std::unique_ptr<DeviceMemoryAllocator> allocator =\n+  std::unique_ptr<DeviceAddressAllocator> allocator =\n       std::make_unique<MultiDeviceAdapter>(platform, std::move(info_allocator));\n \n   std::vector<MultiDeviceAdapter::AllocatorInfo> info_deallocator;\n   info_deallocator.emplace_back(\n       std::make_unique<TestAllocator>(0x1000, allocations), stream.get(),\n       /*memory_space=*/0, /*device_ordinal=*/0);\n-  std::unique_ptr<DeviceMemoryAllocator> deallocator =\n+  std::unique_ptr<DeviceAddressAllocator> deallocator =\n       std::make_unique<MultiDeviceAdapter>(platform,\n                                            std::move(info_deallocator));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      OwningDeviceMemory buff0,\n+      OwningDeviceAddress buff0,\n       allocator->Allocate(/*device_ordinal=*/0, 4, false, /*memory_space=*/0));\n   CHECK_EQ(allocations->size(), 1);\n   CHECK_EQ(reinterpret_cast<size_t>(buff0->opaque()), 0x1001);\n \n   CHECK_OK(deallocator->Deallocate(/*device_ordinal=*/0, buff0.cref()));\n   CHECK_EQ(allocations->size(), 0);\n \n-  // Place back memory pointer to remove it during with ScopedDeviceMemory\n+  // Place back memory pointer to remove it during with ScopedDeviceAddress\n   // destruction.\n   allocations->insert(buff0->opaque());\n }"
        },
        {
            "sha": "991ec39f5b4d97899edfcebff3c8e812c88c41e6",
            "filename": "third_party/xla/xla/stream_executor/kernel.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -23,15 +23,15 @@ limitations under the License.\n // compile time. TypedKernels express their argument types via template\n // parameters like so:\n //\n-//  TypedKernel<DeviceMemory<int>*, int>\n+//  TypedKernel<DeviceAddress<int>*, int>\n //\n // Which expresses a data parallel kernel signature for:\n //\n //  void(int*, int);\n //\n // And for a const memory region:\n //\n-//  TypedKernel<const DeviceMemory<int>&, int>\n+//  TypedKernel<const DeviceAddress<int>&, int>\n //\n // Corresponds to a data parallel kernel signature for:\n //\n@@ -41,7 +41,7 @@ limitations under the License.\n // be memcpy'ied from device memory to the host.\n //\n // Also note that a scalar integer residing in device memory and an array of\n-// integers residing in device memory have the same signature: DeviceMemory<T>.\n+// integers residing in device memory have the same signature: DeviceAddress<T>.\n // However, in the future, checks may be added for additional safety that arrays\n // of minimum sizes are passed when those minimum sizes are contractually\n // expected by the kernel.\n@@ -51,13 +51,13 @@ limitations under the License.\n // defined types are similarly permitted to be expressed as residing in device\n // memory:\n //\n-//  TypedKernel<DeviceMemory<MyUserDefinedStructure>>\n+//  TypedKernel<DeviceAddress<MyUserDefinedStructure>>\n //\n // And, when the alignment and padding are agreed upon, POD types will also be\n // able to be passed by value; for example, it is a common idiom to specify a\n // bunch of options simultaneously with a structure:\n //\n-//  TypedKernel<MyOptionsStructurePassedByValue, DeviceMemory<float>>\n+//  TypedKernel<MyOptionsStructurePassedByValue, DeviceAddress<float>>\n //\n // Which corresponds to a data parallel kernel signature like:\n //\n@@ -185,7 +185,7 @@ class TypedKernel {\n \n   // Launches a kernel with the given (variadic) parameters for the invocation\n   // onto the specified stream. These arguments can be things\n-  // like DeviceMemory or primitive types such as int. What arguments you may\n+  // like DeviceAddress or primitive types such as int. What arguments you may\n   // pass to a given kernel are noted as the template parameters to the\n   // TypedKernel type that the compiler generates.\n   //\n@@ -196,8 +196,8 @@ class TypedKernel {\n   //\n   // Implementation: A compile-time compatibility check is performed that has\n   // some leniency versus an exact parameter pack match -- for example,\n-  // `const DeviceMemory<T>` is considered \"pack compatible\" with a\n-  // `const DeviceMemory<T>&` formal parameter; in part, because we don't have\n+  // `const DeviceAddress<T>` is considered \"pack compatible\" with a\n+  // `const DeviceAddress<T>&` formal parameter; in part, because we don't have\n   // perfect forwarding support without rvalue references. It also attempts to\n   // spit out helpful static_assert error traces with information as to the\n   // argument number and types that were mismatched."
        },
        {
            "sha": "8cf87425f38dfd334db1f0b1ee61370efbce0691",
            "filename": "third_party/xla/xla/stream_executor/kernel_args.h",
            "status": "modified",
            "additions": 43,
            "deletions": 34,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_args.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_args.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_args.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include <utility>\n #include <variant>\n \n+#include \"absl/base/macros.h\"\n #include \"absl/base/optimization.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/overload.h\"\n@@ -36,8 +37,9 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel_metadata.h\"\n+#include \"xla/stream_executor/tensor_map.h\"\n \n namespace stream_executor {\n \n@@ -52,12 +54,13 @@ class KernelArgs {\n   using IsKernelArgs = std::enable_if_t<std::is_base_of<KernelArgs, T>::value>;\n \n   enum class Kind {\n-    // A list of type-erased DeviceMemoryBase pointers to on-device memory. This\n+    // A list of type-erased DeviceAddressBase pointers to on-device memory.\n+    // This\n     // type of kernel arguments used only when the kernel has to do its own\n     // custom packing, e.g. wrap all device pointers into a custom\n     // structure, but can't be implemented as a TypedKernel because it has to be\n     // passed around as a generic Kernel.\n-    kDeviceMemoryArray,\n+    kDeviceAddressArray,\n \n     // A list of kernel arguments packed into a storage that can be passed\n     // directly to device kernel as void** kernel parameters.\n@@ -134,29 +137,29 @@ const T* DynCastOrNull(const KernelArgs* args) {\n }\n \n //===----------------------------------------------------------------------===//\n-// Kernel arguments device memory array\n+// Kernel arguments device address array\n //===----------------------------------------------------------------------===//\n \n-class KernelArgsDeviceMemoryArray : public KernelArgs {\n+class KernelArgsDeviceAddressArray : public KernelArgs {\n  public:\n-  KernelArgsDeviceMemoryArray(absl::Span<const DeviceMemoryBase> args,\n-                              size_t shared_memory_bytes)\n+  KernelArgsDeviceAddressArray(absl::Span<const DeviceAddressBase> args,\n+                               size_t shared_memory_bytes)\n       : device_memory_args_(args.begin(), args.end()),\n         shared_memory_bytes_(shared_memory_bytes) {}\n \n   static bool classof(const KernelArgs* args) {\n-    return args->kind() == Kind::kDeviceMemoryArray;\n+    return args->kind() == Kind::kDeviceAddressArray;\n   }\n \n-  Kind kind() const final { return Kind::kDeviceMemoryArray; }\n+  Kind kind() const final { return Kind::kDeviceAddressArray; }\n \n   size_t number_of_arguments() const final {\n     return device_memory_args_.size() + (shared_memory_bytes_ > 0);\n   }\n \n   uint64_t number_of_shared_bytes() const final { return shared_memory_bytes_; }\n \n-  absl::Span<const DeviceMemoryBase> device_memory_args() const {\n+  absl::Span<const DeviceAddressBase> device_memory_args() const {\n     return device_memory_args_;\n   }\n \n@@ -169,15 +172,19 @@ class KernelArgsDeviceMemoryArray : public KernelArgs {\n   }\n \n  private:\n-  absl::InlinedVector<DeviceMemoryBase, 4> device_memory_args_;\n+  absl::InlinedVector<DeviceAddressBase, 4> device_memory_args_;\n   size_t shared_memory_bytes_ = 0;\n };\n \n+// TODO(ezhulenev): Remove this alias once all users are migrated.\n+using KernelArgsDeviceMemoryArray ABSL_DEPRECATE_AND_INLINE() =\n+    KernelArgsDeviceAddressArray;\n+\n //===----------------------------------------------------------------------===//\n // Kernel arguments packing for device memory and POD args\n //===----------------------------------------------------------------------===//\n \n-// KernelArgsPackedArray is optimized for packing DeviceMemoryBase pointers\n+// KernelArgsPackedArray is optimized for packing DeviceAddressBase pointers\n // and POD arguments (i.e. scalars) when the number and type of arguments are\n // not known at compile time.\n \n@@ -255,7 +262,7 @@ class KernelArgsPackedArray : public KernelArgsPackedArrayBase, ArgsStorage {\n   }\n \n   // Adds a device memory argument to the list.\n-  void add_device_memory_argument(const DeviceMemoryBase& arg) {\n+  void add_device_memory_argument(const DeviceAddressBase& arg) {\n     const void** copy_ptr =\n         &device_memory_opaque_pointers_[number_of_argument_addresses_];\n     *copy_ptr = arg.opaque();\n@@ -300,14 +307,14 @@ class KernelArgsPackedArray : public KernelArgsPackedArrayBase, ArgsStorage {\n   size_t number_of_argument_addresses_ = 0;\n };\n \n-using KernelArgument = std::variant<DeviceMemoryBase, TensorMap, int64_t>;\n+using KernelArgument = std::variant<DeviceAddressBase, TensorMap, int64_t>;\n \n namespace internal {\n template <int n>\n std::unique_ptr<KernelArgsPackedArrayBase> PackKernelArgs(\n-    absl::Span<const DeviceMemoryBase> args, uint32_t shared_mem_bytes) {\n+    absl::Span<const DeviceAddressBase> args, uint32_t shared_mem_bytes) {\n   auto packed = std::make_unique<KernelArgsPackedArray<n, EmptyArgs>>();\n-  for (const DeviceMemoryBase& buf : args) {\n+  for (const DeviceAddressBase& buf : args) {\n     packed->add_device_memory_argument(buf);\n   }\n   if (shared_mem_bytes > 0) {\n@@ -323,7 +330,7 @@ std::unique_ptr<KernelArgsPackedArray<n, ArgsStorage>> PackKernelArgsImpl(\n   for (const auto& arg : args) {\n     std::visit(\n         absl::Overload{\n-            [&](const DeviceMemoryBase& device_memory) {\n+            [&](const DeviceAddressBase& device_memory) {\n               packed->add_device_memory_argument(device_memory);\n             },\n             [&](int64_t int_arg) {\n@@ -435,8 +442,8 @@ namespace internal {\n //   (1) We always strip references and store a copy of an argument.\n //   (2) We do not support pointer arguments, as we should not be passing a\n //       pointers to host memory to device kernels.\n-//   (3) DeviceMemory passed as an opaque `void*` pointer.\n-//   (4) We have a special case for passing pointers to DeviceMemory where we\n+//   (3) DeviceAddress passed as an opaque `void*` pointer.\n+//   (4) We have a special case for passing pointers to DeviceAddress where we\n //       also pass it as an opaque device pointer.\n template <typename T>\n struct PackedArgType {\n@@ -445,33 +452,33 @@ struct PackedArgType {\n };\n \n template <>\n-struct PackedArgType<DeviceMemoryBase> {\n+struct PackedArgType<DeviceAddressBase> {\n   using Type = const void*;\n };\n \n template <typename T>\n-struct PackedArgType<DeviceMemory<T>> {\n-  using Type = typename PackedArgType<DeviceMemoryBase>::Type;\n+struct PackedArgType<DeviceAddress<T>> {\n+  using Type = typename PackedArgType<DeviceAddressBase>::Type;\n };\n \n template <>\n-struct PackedArgType<DeviceMemoryBase*> {\n-  using Type = typename PackedArgType<DeviceMemoryBase>::Type;\n+struct PackedArgType<DeviceAddressBase*> {\n+  using Type = typename PackedArgType<DeviceAddressBase>::Type;\n };\n \n template <>\n-struct PackedArgType<const DeviceMemoryBase*> {\n-  using Type = typename PackedArgType<DeviceMemoryBase>::Type;\n+struct PackedArgType<const DeviceAddressBase*> {\n+  using Type = typename PackedArgType<DeviceAddressBase>::Type;\n };\n \n template <typename T>\n-struct PackedArgType<DeviceMemory<T>*> {\n-  using Type = typename PackedArgType<DeviceMemoryBase>::Type;\n+struct PackedArgType<DeviceAddress<T>*> {\n+  using Type = typename PackedArgType<DeviceAddressBase>::Type;\n };\n \n template <typename T>\n-struct PackedArgType<const DeviceMemory<T>*> {\n-  using Type = typename PackedArgType<DeviceMemoryBase>::Type;\n+struct PackedArgType<const DeviceAddress<T>*> {\n+  using Type = typename PackedArgType<DeviceAddressBase>::Type;\n };\n \n // Overload set for packing kernel arguments. This overload set matches\n@@ -481,18 +488,20 @@ T PackArg(const T& arg) {\n   return arg;\n }\n \n-inline const void* PackArg(const DeviceMemoryBase& arg) { return arg.opaque(); }\n-inline const void* PackArg(const DeviceMemoryBase* arg) {\n+inline const void* PackArg(const DeviceAddressBase& arg) {\n+  return arg.opaque();\n+}\n+inline const void* PackArg(const DeviceAddressBase* arg) {\n   return PackArg(*arg);\n }\n \n template <typename T>\n-const void* PackArg(const DeviceMemory<T>& arg) {\n+const void* PackArg(const DeviceAddress<T>& arg) {\n   return arg.opaque();\n }\n \n template <typename T>\n-const void* PackArg(const DeviceMemory<T>* arg) {\n+const void* PackArg(const DeviceAddress<T>* arg) {\n   return PackArg(*arg);\n }\n "
        },
        {
            "sha": "3bcba14e8be63db56367e61dba085fc8732232bb",
            "filename": "third_party/xla/xla/stream_executor/kernel_args_test.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_args_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_args_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_args_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/types/span.h\"\n #include \"benchmark/benchmark.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel_metadata.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -47,28 +47,29 @@ static_assert(\n static_assert(std::is_same_v<ArgsStorage<Data, const Data, Data&, const Data>,\n                              std::tuple<Data, Data, Data, Data>>);\n \n-// We pass DeviceMemoryBase as an opaque pointer.\n+// We pass DeviceAddressBase as an opaque pointer.\n static_assert(std::is_same_v<\n-              ArgsStorage<DeviceMemoryBase, const DeviceMemoryBase,\n-                          DeviceMemoryBase&, const DeviceMemoryBase&>,\n+              ArgsStorage<DeviceAddressBase, const DeviceAddressBase,\n+                          DeviceAddressBase&, const DeviceAddressBase&>,\n               std::tuple<const void*, const void*, const void*, const void*>>);\n \n-// We pass DeviceMemory<T> as an opaque pointer.\n+// We pass DeviceAddress<T> as an opaque pointer.\n static_assert(std::is_same_v<\n-              ArgsStorage<DeviceMemory<float>, const DeviceMemory<float>,\n-                          DeviceMemory<float>&, const DeviceMemory<float>&>,\n+              ArgsStorage<DeviceAddress<float>, const DeviceAddress<float>,\n+                          DeviceAddress<float>&, const DeviceAddress<float>&>,\n               std::tuple<const void*, const void*, const void*, const void*>>);\n \n-// We accept pointers to DeviceMemoryBase and extract opaque pointers from them.\n+// We accept pointers to DeviceAddressBase and extract opaque pointers from\n+// them.\n static_assert(\n-    std::is_same_v<ArgsStorage<DeviceMemoryBase*, const DeviceMemoryBase*>,\n+    std::is_same_v<ArgsStorage<DeviceAddressBase*, const DeviceAddressBase*>,\n                    std::tuple<const void*, const void*>>);\n \n-TEST(KernelTest, PackDeviceMemoryArguments) {\n-  DeviceMemoryBase a(reinterpret_cast<void*>(0x12345678));\n-  DeviceMemoryBase b(reinterpret_cast<void*>(0x87654321));\n+TEST(KernelTest, PackDeviceAddressArguments) {\n+  DeviceAddressBase a(reinterpret_cast<void*>(0x12345678));\n+  DeviceAddressBase b(reinterpret_cast<void*>(0x87654321));\n \n-  auto args = PackKernelArgs<DeviceMemoryBase>({a, b}, 0).value();\n+  auto args = PackKernelArgs<DeviceAddressBase>({a, b}, 0).value();\n   ASSERT_EQ(args->number_of_arguments(), 2);\n \n   auto packed = args->argument_addresses();\n@@ -113,7 +114,7 @@ TEST(KernelTest, PackTupleArguments) {\n \n TEST(KernelTest, PackArgumentsWithInt64) {\n   std::vector<KernelArgument> args;\n-  DeviceMemoryBase somemem(reinterpret_cast<void*>(0x12345678));\n+  DeviceAddressBase somemem(reinterpret_cast<void*>(0x12345678));\n   int64_t someint64 = 1234;\n   args.emplace_back(somemem);\n   args.emplace_back(someint64);\n@@ -131,19 +132,19 @@ TEST(KernelTest, PackArgumentsWithInt64) {\n // Performance benchmarks below\n //===----------------------------------------------------------------------===//\n \n-static void BM_PackDeviceMemoryArgs(benchmark::State& state) {\n-  std::vector<DeviceMemoryBase> args(state.range(0));\n+static void BM_PackDeviceAddressArgs(benchmark::State& state) {\n+  std::vector<DeviceAddressBase> args(state.range(0));\n   for (int i = 0; i < state.range(0); ++i) {\n-    args[i] = DeviceMemoryBase(reinterpret_cast<void*>(0x12345678), 42);\n+    args[i] = DeviceAddressBase(reinterpret_cast<void*>(0x12345678), 42);\n   }\n \n   for (auto s : state) {\n-    auto packed = PackKernelArgs<DeviceMemoryBase>(args, 0);\n+    auto packed = PackKernelArgs<DeviceAddressBase>(args, 0);\n     benchmark::DoNotOptimize(packed);\n   }\n }\n \n-BENCHMARK(BM_PackDeviceMemoryArgs)\n+BENCHMARK(BM_PackDeviceAddressArgs)\n     ->Arg(4)\n     ->Arg(8)\n     ->Arg(32)"
        },
        {
            "sha": "6bc96cbf658ce47fa3a30ca76dc9de3d3e3bee2f",
            "filename": "third_party/xla/xla/stream_executor/kernel_argument_packing_spec.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel_args_packed_vector.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/safe_reinterpret_cast.h\"\n@@ -53,7 +53,7 @@ absl::StatusOr<ArgumentPackingRelocation::Type> FromProtoType(\n }  // namespace\n \n absl::StatusOr<std::vector<char>> SingleArgumentPackingSpec::BuildArgument(\n-    absl::Span<const DeviceMemoryBase> args) const {\n+    absl::Span<const DeviceAddressBase> args) const {\n   auto argument = storage_;\n \n   for (const ArgumentPackingRelocation& relocation : relocations_) {\n@@ -95,7 +95,7 @@ void SingleArgumentPackingSpec::WriteArgumentAddress(int argument_index) {\n \n absl::StatusOr<std::unique_ptr<KernelArgsPackedVector>>\n KernelArgumentsPackingSpec::BuildArguments(\n-    absl::Span<const DeviceMemoryBase> thunk_arguments,\n+    absl::Span<const DeviceAddressBase> thunk_arguments,\n     size_t shared_memory_bytes) const {\n   std::vector<std::vector<char>> result;\n   result.reserve(kernel_arguments_.size());"
        },
        {
            "sha": "a7a238720058070646eec53054fd2ea027a0b5bf",
            "filename": "third_party/xla/xla/stream_executor/kernel_argument_packing_spec.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel_args_packed_vector.h\"\n #include \"xla/stream_executor/kernel_argument_packing_spec.pb.h\"\n \n@@ -73,7 +73,7 @@ class SingleArgumentPackingSpec {\n   // must contain at least the number of arguments referenced in the packing\n   // spec, otherwise an error will be returned.\n   absl::StatusOr<std::vector<char>> BuildArgument(\n-      absl::Span<const DeviceMemoryBase> args) const;\n+      absl::Span<const DeviceAddressBase> args) const;\n \n   // Writes a placeholder to the argument packing spec that will be replaced\n   // by the runtime with the address of the argument `argument_index`th\n@@ -170,7 +170,7 @@ class KernelArgumentsPackingSpec {\n   // must contain at least the number of arguments referenced in the packing\n   // spec, otherwise an error will be returned.\n   absl::StatusOr<std::unique_ptr<KernelArgsPackedVector>> BuildArguments(\n-      absl::Span<const DeviceMemoryBase> thunk_arguments,\n+      absl::Span<const DeviceAddressBase> thunk_arguments,\n       size_t shared_memory_bytes) const;\n \n   absl::StatusOr<KernelArgumentsPackingSpecProto> ToProto() const;"
        },
        {
            "sha": "13e68655f6b9fd189075ebce4532d69126ce1bda",
            "filename": "third_party/xla/xla/stream_executor/kernel_argument_packing_spec_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fkernel_argument_packing_spec_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/kernel_args_packed_vector.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/parse_text_proto.h\"\n@@ -41,16 +41,16 @@ using ::testing::SizeIs;\n using tsl::proto_testing::EqualsProto;\n using tsl::proto_testing::ParseTextProtoOrDie;\n \n-// This function creates a `DeviceMemoryBase` with an opaque pointer that\n+// This function creates a `DeviceAddressBase` with an opaque pointer that\n // contains the given value. The size of the device memory is set to 0 since\n // it's unused.\n // Note that this device pointer is not a valid pointer to device memory, it\n // is only used for testing and can't be dereferenced.\n-DeviceMemoryBase MakeDevicePointer(uint32_t value) {\n+DeviceAddressBase MakeDevicePointer(uint32_t value) {\n   // To construct a pointer that works both on 32bit and 64bit platforms and\n   // does not invoke undefined behaviour, we first cast our integer to uintptr_t\n   // and then cast it to void*.\n-  return DeviceMemoryBase(\n+  return DeviceAddressBase(\n       tsl::safe_reinterpret_cast<void*>(static_cast<uintptr_t>(value)),\n       /*size=*/0);\n }"
        },
        {
            "sha": "652871597330f03d30c55946a01a79f6f8cc5f14",
            "filename": "third_party/xla/xla/stream_executor/mock_stream.h",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,8 +27,8 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/testlib/test.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -51,19 +51,20 @@ class MockStream : public Stream {\n   MOCK_METHOD(absl::Status, WaitFor, (Event * event), (override));\n   MOCK_METHOD(absl::Status, RecordEvent, (Event * event), (override));\n   MOCK_METHOD(absl::Status, Memcpy,\n-              (void *host_dst, const DeviceMemoryBase &gpu_src, uint64_t size),\n+              (void* host_dst, const DeviceAddressBase& gpu_src, uint64_t size),\n               (override));\n   MOCK_METHOD(absl::Status, Memcpy,\n-              (DeviceMemoryBase * gpu_dst, const void *host_src, uint64_t size),\n+              (DeviceAddressBase * gpu_dst, const void* host_src,\n+               uint64_t size),\n               (override));\n   MOCK_METHOD(absl::Status, Memcpy,\n-              (DeviceMemoryBase * gpu_dst, const DeviceMemoryBase &gpu_src,\n+              (DeviceAddressBase * gpu_dst, const DeviceAddressBase& gpu_src,\n                uint64_t size),\n               (override));\n   MOCK_METHOD(absl::Status, MemZero,\n-              (DeviceMemoryBase * location, uint64_t size), (override));\n+              (DeviceAddressBase * location, uint64_t size), (override));\n   MOCK_METHOD(absl::Status, Memset32,\n-              (DeviceMemoryBase * location, uint32_t pattern, uint64_t size),\n+              (DeviceAddressBase * location, uint32_t pattern, uint64_t size),\n               (override));\n   MOCK_METHOD(absl::Status, BlockHostUntilDone, (), (override));\n   MOCK_METHOD(absl::Status, DoHostCallbackWithStatus,"
        },
        {
            "sha": "231ddf297e3e6655db09840f4ffc2325a66ef9b2",
            "filename": "third_party/xla/xla/stream_executor/mock_stream_executor.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream_executor.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -30,8 +30,8 @@ limitations under the License.\n #include \"xla/stream_executor/allocator_stats.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -59,23 +59,23 @@ class MockStreamExecutor : public StreamExecutor {\n   MOCK_METHOD(bool, UnloadModule, (ModuleHandle module_handle), (override));\n   MOCK_METHOD(absl::StatusOr<ModuleHandle>, LoadModule,\n               (const MultiModuleLoaderSpec& spec), (override));\n-  MOCK_METHOD(absl::StatusOr<std::shared_ptr<DeviceMemoryBase>>,\n+  MOCK_METHOD(absl::StatusOr<std::shared_ptr<DeviceAddressBase>>,\n               CreateOrShareConstant,\n               (Stream * stream, absl::Span<const uint8_t> content), (override));\n-  MOCK_METHOD(DeviceMemoryBase, Allocate, (uint64_t size, int64_t memory_space),\n-              (override));\n-  MOCK_METHOD(void, Deallocate, (DeviceMemoryBase * mem), (override));\n+  MOCK_METHOD(DeviceAddressBase, Allocate,\n+              (uint64_t size, int64_t memory_space), (override));\n+  MOCK_METHOD(void, Deallocate, (DeviceAddressBase * mem), (override));\n   MOCK_METHOD(absl::StatusOr<std::unique_ptr<MemoryAllocation>>,\n               HostMemoryAllocate, (uint64_t size), (override));\n   MOCK_METHOD(bool, SynchronizeAllActivity, (), (override));\n   MOCK_METHOD(absl::Status, SynchronousMemZero,\n-              (DeviceMemoryBase * location, uint64_t size), (override));\n+              (DeviceAddressBase * location, uint64_t size), (override));\n   MOCK_METHOD(absl::Status, SynchronousMemcpy,\n-              (DeviceMemoryBase * device_dst, const void* host_src,\n+              (DeviceAddressBase * device_dst, const void* host_src,\n                uint64_t size),\n               (override));\n   MOCK_METHOD(absl::Status, SynchronousMemcpy,\n-              (void* host_dst, const DeviceMemoryBase& device_src,\n+              (void* host_dst, const DeviceAddressBase& device_src,\n                uint64_t size),\n               (override));\n   MOCK_METHOD(void, DeallocateStream, (Stream * stream), (override));\n@@ -85,7 +85,7 @@ class MockStreamExecutor : public StreamExecutor {\n               (override));\n   MOCK_METHOD(bool, DeviceMemoryUsage, (int64_t* free, int64_t* total),\n               (const, override));\n-  MOCK_METHOD(absl::StatusOr<DeviceMemoryBase>, GetSymbol,\n+  MOCK_METHOD(absl::StatusOr<DeviceAddressBase>, GetSymbol,\n               (const std::string& symbol_name, ModuleHandle module_handle),\n               (override));\n   MOCK_METHOD(absl::StatusOr<std::unique_ptr<DeviceDescription>>,"
        },
        {
            "sha": "ab3b73b0fc8fa9e3b0f539a6a81796d3c8e6d3fe",
            "filename": "third_party/xla/xla/stream_executor/rocm/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -156,8 +156,8 @@ cc_library(\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:command_buffer\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:event_based_timer\",\n@@ -378,8 +378,8 @@ cc_library(\n         \":rocm_platform_id\",\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:event_based_timer\",\n         \"//xla/stream_executor:plugin_registry\",\n@@ -438,7 +438,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:gpu_solver_context\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n@@ -466,7 +466,7 @@ cc_library(\n         \":rocm_complex_converters\",\n         \":rocm_platform_id\",\n         \"//xla/stream_executor:activate_context\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:fft\",\n         \"//xla/stream_executor:plugin_registry\",\n         \"//xla/stream_executor:scratch_allocator\",\n@@ -515,8 +515,8 @@ cc_library(\n         \":rocm_platform_id\",\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:engine_options\",\n         \"//xla/stream_executor:event_based_timer\",\n@@ -685,7 +685,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event_based_timer\",\n         \"//xla/stream_executor:scratch_allocator\",\n         \"//xla/stream_executor:stream\",\n@@ -726,7 +726,7 @@ cc_library(\n     deps = [\n         \"//xla:types\",\n         \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt\",\n         \"//xla/tsl/platform:env\",\n@@ -899,7 +899,7 @@ cc_library(\n         \":rocm_kernel\",\n         \":rocm_status\",\n         \"//xla/stream_executor:activate_context\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:event_based_timer\",\n         \"//xla/stream_executor:kernel\",\n@@ -932,7 +932,7 @@ xla_test(\n         \":rocm_executor\",\n         \":rocm_platform_id\",\n         \":rocm_stream\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n@@ -983,7 +983,7 @@ xla_test(\n         \":rocm_executor\",\n         \":rocm_platform_id\",\n         \":rocm_timer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n@@ -1050,7 +1050,7 @@ cc_library(\n         \":rocm_status\",\n         \"//xla/stream_executor:bit_pattern\",\n         \"//xla/stream_executor:command_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:kernel\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\","
        },
        {
            "sha": "5010c9d64fcd85364a3f6ef43ace096d0f4f0b8e",
            "filename": "third_party/xla/xla/stream_executor/rocm/gpu_test_kernels_rocm.cu.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fgpu_test_kernels_rocm.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fgpu_test_kernels_rocm.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fgpu_test_kernels_rocm.cu.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -76,9 +76,10 @@ GPU_KERNEL_REGISTRY_REGISTER_KERNEL_STATICALLY(\n           \"AddI32Ptrs3\", arity,\n           [&](const stream_executor::Kernel& kernel,\n               const stream_executor::KernelArgs& args) {\n-            auto bufs = stream_executor::Cast<\n-                            stream_executor::KernelArgsDeviceMemoryArray>(&args)\n-                            ->device_memory_args();\n+            auto bufs =\n+                stream_executor::Cast<\n+                    stream_executor::KernelArgsDeviceAddressArray>(&args)\n+                    ->device_memory_args();\n             auto cast = [](auto m) {\n               return reinterpret_cast<int32_t*>(m.opaque());\n             };"
        },
        {
            "sha": "a304edaecf9b24817143cc54d8be18bcc42dd9f9",
            "filename": "third_party/xla/xla/stream_executor/rocm/hip_blas_lt.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/gpu/gpu_helpers.h\"\n@@ -390,7 +390,7 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n     return absl::InternalError(\n         \"Algorithm must be set before calling DoMatMul!\");\n   }\n-  DeviceMemoryBase a = args.a, b = args.b;\n+  DeviceAddressBase a = args.a, b = args.b;\n   if (must_swap_operands_) {\n     std::swap(a, b);\n   }\n@@ -413,7 +413,7 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n   if (workspace_size > 0) {\n     if (args.scratch_allocator != nullptr) {\n       TF_ASSIGN_OR_RETURN(\n-          DeviceMemory<uint8_t> alloc,\n+          DeviceAddress<uint8_t> alloc,\n           args.scratch_allocator->AllocateBytes(workspace_size));\n       workspace_addr = gpu::GpuMemoryMutable(&alloc);\n     } else {"
        },
        {
            "sha": "efe8b84b6741680ffed950cbae3efdf620997f4a",
            "filename": "third_party/xla/xla/stream_executor/rocm/hip_blas_lt.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Fhip_blas_lt.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"rocm/rocm_config.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/types.h\""
        },
        {
            "sha": "9802e07fdd503121dc5d34fcee66d85fa9480124",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_blas.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 57,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -40,8 +40,8 @@ limitations under the License.\n #include \"rocm/rocm_config.h\"\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n@@ -71,17 +71,17 @@ extern void rocm_Broadcast_fp32(void *stream, float *dst, int dst_stride,\n                                 int size);\n \n template <class T>\n-const RocBlasType_t<T> *const *complex_cast(const DeviceMemory<T *> &a) {\n+const RocBlasType_t<T>* const* complex_cast(const DeviceAddress<T*>& a) {\n   return reinterpret_cast<const RocBlasType_t<T> *const *>(GpuMemory(a));\n }\n \n template <class T>\n-RocBlasType_t<T> *const *complex_cast(DeviceMemory<T *> &a) {\n+RocBlasType_t<T>* const* complex_cast(DeviceAddress<T*>& a) {\n   return reinterpret_cast<RocBlasType_t<T> *const *>(GpuMemory(a));\n }\n \n template <class T>\n-const RocBlasType_t<T> *complex_cast(const DeviceMemory<T> &a) {\n+const RocBlasType_t<T>* complex_cast(const DeviceAddress<T>& a) {\n   return reinterpret_cast<const RocBlasType_t<T> *>(GpuMemory(a));\n }\n \n@@ -90,7 +90,7 @@ const RocBlasType_t<T> *complex_cast(const T &a) {\n   return reinterpret_cast<const RocBlasType_t<T> *>(&a);\n }\n template <class T>\n-RocBlasType_t<T> *complex_cast(DeviceMemory<T> *a) {\n+RocBlasType_t<T>* complex_cast(DeviceAddress<T>* a) {\n   return reinterpret_cast<RocBlasType_t<T> *>(GpuMemoryMutable(a));\n }\n \n@@ -414,8 +414,8 @@ absl::Status ROCMBlas::DoBlasInternalImpl(FuncT rocblas_func, Stream *stream,\n }\n \n #define Impl_DoBlasScal(Fun, T, Ta)                                         \\\n-  bool ROCMBlas::DoBlasScal(Stream *stream, uint64_t elem_count, Ta alpha,  \\\n-                            DeviceMemory<T> *x, int incx) {                 \\\n+  bool ROCMBlas::DoBlasScal(Stream* stream, uint64_t elem_count, Ta alpha,  \\\n+                            DeviceAddress<T>* x, int incx) {                \\\n     return DoBlasInternal(Fun, stream, /* pointer_mode_host = */ true,      \\\n                           elem_count, complex_cast(alpha), complex_cast(x), \\\n                           incx);                                            \\\n@@ -430,10 +430,10 @@ Impl_DoBlasScal(wrap::rocblas_sscal, float, float)\n                     Impl_DoBlasScal(wrap::rocblas_zscal, std::complex<double>,\n                                     std::complex<double>)\n #define Impl_DoBlasGemv(fun, T)                                                \\\n-  bool ROCMBlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m, \\\n-                            uint64_t n, T alpha, const DeviceMemory<T> &a,     \\\n-                            int lda, const DeviceMemory<T> &x, int incx,       \\\n-                            T beta, DeviceMemory<T> *y, int incy) {            \\\n+  bool ROCMBlas::DoBlasGemv(Stream* stream, blas::Transpose trans, uint64_t m, \\\n+                            uint64_t n, T alpha, const DeviceAddress<T>& a,    \\\n+                            int lda, const DeviceAddress<T>& x, int incx,      \\\n+                            T beta, DeviceAddress<T>* y, int incy) {           \\\n     return DoBlasInternal(fun, stream, /* pointer_mode_host = */ true,         \\\n                           ROCMBlasTranspose(trans), m, n, complex_cast(alpha), \\\n                           complex_cast(a), lda, complex_cast(x), incx,         \\\n@@ -474,9 +474,9 @@ void ROCMBlas::MaybeLogGemmOp(GemmCallTrace::GemmType op,\n absl::Status ROCMBlas::DoBlasGemm(Stream* stream, blas::Transpose transa,\n                                   blas::Transpose transb, uint64_t m,\n                                   uint64_t n, uint64_t k, blas::DataType dtype,\n-                                  const void* alpha, const DeviceMemoryBase& a,\n-                                  int lda, const DeviceMemoryBase& b, int ldb,\n-                                  const void* beta, DeviceMemoryBase* c,\n+                                  const void* alpha, const DeviceAddressBase& a,\n+                                  int lda, const DeviceAddressBase& b, int ldb,\n+                                  const void* beta, DeviceAddressBase* c,\n                                   int ldc, const EngineOptions& engine_options,\n                                   blas::CallContext context) {\n   MaybeLogGemmOp(GemmCallTrace::GemmType::kPlain, context,\n@@ -560,9 +560,9 @@ absl::Status ROCMBlas::DoBlasGemm(Stream* stream, blas::Transpose transa,\n \n absl::Status ROCMBlas::DoBlasGemmWithAlgorithm(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, const void* alpha, const DeviceMemoryBase& a,\n-    blas::DataType type_a, int lda, const DeviceMemoryBase& b,\n-    blas::DataType type_b, int ldb, const void* beta, DeviceMemoryBase* c,\n+    uint64_t n, uint64_t k, const void* alpha, const DeviceAddressBase& a,\n+    blas::DataType type_a, int lda, const DeviceAddressBase& b,\n+    blas::DataType type_b, int ldb, const void* beta, DeviceAddressBase* c,\n     blas::DataType type_c, int ldc, blas::ComputationType computation_type,\n     blas::AlgorithmType algorithm, const EngineOptions& engine_options,\n     blas::ProfileResult* profile_result, blas::CallContext context) {\n@@ -620,13 +620,14 @@ absl::Status ROCMBlas::DoBlasGemmWithAlgorithm(\n \n absl::Status ROCMBlas::DoBlasGemmStridedBatchedWithAlgorithm(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, const void* alpha, const DeviceMemoryBase& a,\n-    blas::DataType type_a, int lda, int64_t stride_a, const DeviceMemoryBase& b,\n-    blas::DataType type_b, int ldb, int64_t stride_b, const void* beta,\n-    DeviceMemoryBase* c, blas::DataType type_c, int ldc, int64_t stride_c,\n-    int batch_count, blas::ComputationType computation_type,\n-    blas::AlgorithmType algorithm, const EngineOptions& engine_options,\n-    blas::ProfileResult* profile_result, blas::CallContext context) {\n+    uint64_t n, uint64_t k, const void* alpha, const DeviceAddressBase& a,\n+    blas::DataType type_a, int lda, int64_t stride_a,\n+    const DeviceAddressBase& b, blas::DataType type_b, int ldb,\n+    int64_t stride_b, const void* beta, DeviceAddressBase* c,\n+    blas::DataType type_c, int ldc, int64_t stride_c, int batch_count,\n+    blas::ComputationType computation_type, blas::AlgorithmType algorithm,\n+    const EngineOptions& engine_options, blas::ProfileResult* profile_result,\n+    blas::CallContext context) {\n   if (type_a != type_b) {\n     return absl::InternalError(absl::StrFormat(\n         \"DoBlasGemmStridedBatchedWithAlgorithm: different \"\n@@ -820,9 +821,9 @@ bool MemCopyOpsFold(MemoryCopyOp &y, const MemoryCopyOp &x) {\n // The below algorithm tries to minimize the number of memcpy by consolidating\n // neighboring memcpy into a single request.\n template <typename MAPPED_T>\n-absl::Status ReorganizeMemory(Stream *stream,\n-                              DeviceMemory<MAPPED_T> *device_memory,\n-                              const std::vector<MAPPED_T *> &raw_ptrs,\n+absl::Status ReorganizeMemory(Stream* stream,\n+                              DeviceAddress<MAPPED_T>* device_memory,\n+                              const std::vector<MAPPED_T*>& raw_ptrs,\n                               int batch_count, uint64_t batch_stride,\n                               bool gather) {\n   if (gather == false) {\n@@ -869,8 +870,8 @@ absl::Status ReorganizeMemory(Stream *stream,\n           reinterpret_cast<float *>(x.dst_ptr), x.dst_stride >> 2, x.count,\n           x.src_count, reinterpret_cast<float *>(x.src_ptr), x.size >> 2);\n     } else {\n-      DeviceMemoryBase src_mem = DeviceMemoryBase(x.src_ptr, x.size);\n-      DeviceMemoryBase target_mem = DeviceMemoryBase(x.dst_ptr, x.size);\n+      DeviceAddressBase src_mem = DeviceAddressBase(x.src_ptr, x.size);\n+      DeviceAddressBase target_mem = DeviceAddressBase(x.dst_ptr, x.size);\n       TF_RETURN_IF_ERROR(stream->Memcpy(&target_mem, src_mem, x.size));\n     }\n     i++;\n@@ -881,7 +882,7 @@ absl::Status ReorganizeMemory(Stream *stream,\n template <typename T>\n struct AllocateStridedResult {\n   using Type = RocBlasType_t<T>;\n-  DeviceMemory<Type> device_mem;\n+  DeviceAddress<Type> device_mem;\n   bool reallocated;\n };\n \n@@ -909,18 +910,18 @@ absl::StatusOr<AllocateStridedResult<T>> AllocateStridedBuffer(\n \n   // No need to do re-allocation, take the short cut and return\n   if (!needs_allocate_strided) {\n-    res.device_mem = DeviceMemory<MAPPED_T>(\n-        DeviceMemoryBase(raw_ptrs[0], matrix_batch_byte_size));\n+    res.device_mem = DeviceAddress<MAPPED_T>(\n+        DeviceAddressBase(raw_ptrs[0], matrix_batch_byte_size));\n     res.reallocated = false;\n     return res;\n   }\n \n   if (scratch_allocator == nullptr) {\n     return absl::InternalError(\"scratch_allocator is null\");\n   }\n-  TF_ASSIGN_OR_RETURN(DeviceMemory<uint8_t> batch_matrix_bytes,\n+  TF_ASSIGN_OR_RETURN(DeviceAddress<uint8_t> batch_matrix_bytes,\n                       scratch_allocator->AllocateBytes(matrix_batch_byte_size));\n-  res.device_mem = DeviceMemory<MAPPED_T>(batch_matrix_bytes);\n+  res.device_mem = DeviceAddress<MAPPED_T>(batch_matrix_bytes);\n   res.reallocated = true;\n   if (copy_data) {\n     TF_RETURN_IF_ERROR(ReorganizeMemory(stream, &res.device_mem, raw_ptrs,\n@@ -933,12 +934,12 @@ absl::StatusOr<AllocateStridedResult<T>> AllocateStridedBuffer(\n \n template <typename T, typename FuncT>\n absl::Status ROCMBlas::DoBlasGemmBatchedInternal(\n-    FuncT rocblas_func, Stream *stream, blas::Transpose transa,\n+    FuncT rocblas_func, Stream* stream, blas::Transpose transa,\n     blas::Transpose transb, uint64_t m, uint64_t n, uint64_t k, T alpha,\n-    DeviceMemorySlice<T> a_ptrs_to_wrappers, int lda,\n-    DeviceMemorySlice<T> b_ptrs_to_wrappers, int ldb, T beta,\n-    DeviceMemorySlice<T> c_ptrs_to_wrappers, int ldc, int batch_count,\n-    ScratchAllocator *scratch_allocator) {\n+    DeviceAddressSlice<T> a_ptrs_to_wrappers, int lda,\n+    DeviceAddressSlice<T> b_ptrs_to_wrappers, int ldb, T beta,\n+    DeviceAddressSlice<T> c_ptrs_to_wrappers, int ldc, int batch_count,\n+    ScratchAllocator* scratch_allocator) {\n   using MAPPED_T = RocBlasType_t<T>;\n \n   // Sanity checks before making any further progress\n@@ -1073,9 +1074,9 @@ const char *rocblas_gemm_strided_batched_bf16::kName =\n     \"rocblas_gemm_strided_batched_bf16\";\n bool ROCMBlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n-    uint64_t n, uint64_t k, float alpha, DeviceMemorySlice<Eigen::half> a,\n-    int lda, DeviceMemorySlice<Eigen::half> b, int ldb, float beta,\n-    DeviceMemorySlice<Eigen::half> c, int ldc, int batch_count,\n+    uint64_t n, uint64_t k, float alpha, DeviceAddressSlice<Eigen::half> a,\n+    int lda, DeviceAddressSlice<Eigen::half> b, int ldb, float beta,\n+    DeviceAddressSlice<Eigen::half> c, int ldc, int batch_count,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n     blas::CallContext context) {\n   MaybeLogGemmOp(GemmCallTrace::GemmType::kBatched, context, a.size(),\n@@ -1109,9 +1110,9 @@ bool ROCMBlas::DoBlasGemmBatched(\n bool ROCMBlas::DoBlasGemmBatched(\n     Stream* stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n     uint64_t n, uint64_t k, float alpha,\n-    DeviceMemorySlice<Eigen::bfloat16> a_array, int lda,\n-    DeviceMemorySlice<Eigen::bfloat16> b_array, int ldb, float beta,\n-    DeviceMemorySlice<Eigen::bfloat16> c_array, int ldc, int batch_count,\n+    DeviceAddressSlice<Eigen::bfloat16> a_array, int lda,\n+    DeviceAddressSlice<Eigen::bfloat16> b_array, int ldb, float beta,\n+    DeviceAddressSlice<Eigen::bfloat16> c_array, int ldc, int batch_count,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n     blas::CallContext context) {\n   MaybeLogGemmOp(GemmCallTrace::GemmType::kBatched, context, a_array.size(),\n@@ -1133,9 +1134,9 @@ bool ROCMBlas::DoBlasGemmBatched(\n   bool ROCMBlas::DoBlasGemmBatched(                                            \\\n       Stream* stream, blas::Transpose transa, blas::Transpose transb,          \\\n       uint64_t m, uint64_t n, uint64_t k, T alpha,                             \\\n-      DeviceMemorySlice<T> a_array, int lda, DeviceMemorySlice<T> b_array,     \\\n-      int ldb, T beta, DeviceMemorySlice<T> c_array, int ldc, int batch_count, \\\n-      const EngineOptions& engine_options,                                     \\\n+      DeviceAddressSlice<T> a_array, int lda, DeviceAddressSlice<T> b_array,   \\\n+      int ldb, T beta, DeviceAddressSlice<T> c_array, int ldc,                 \\\n+      int batch_count, const EngineOptions& engine_options,                    \\\n       ScratchAllocator* scratch_allocator, blas::CallContext context) {        \\\n     MaybeLogGemmOp(GemmCallTrace::GemmType::kBatched, context, a_array.size(), \\\n                    b_array.size());                                            \\\n@@ -1155,11 +1156,11 @@ IMPL_DoBlasGemmBatched(float, wrap::rocblas_sgemm_strided_batched)\n             IMPL_DoBlasGemmBatched(std::complex<double>,\n                                    wrap::rocblas_zgemm_strided_batched)\n #define IMPL_DoBlasTrsm(T, Fun, Fun2)                                        \\\n-  bool ROCMBlas::DoBlasTrsm(Stream *stream, blas::Side side,                 \\\n+  bool ROCMBlas::DoBlasTrsm(Stream* stream, blas::Side side,                 \\\n                             blas::UpperLower uplo, blas::Transpose transa,   \\\n                             blas::Diagonal diag, uint64_t m, uint64_t n,     \\\n-                            T alpha, const DeviceMemory<T> &a, int lda,      \\\n-                            DeviceMemory<T> *b, int ldb) {                   \\\n+                            T alpha, const DeviceAddress<T>& a, int lda,     \\\n+                            DeviceAddress<T>* b, int ldb) {                  \\\n     return DoBlasInternal(Fun, stream, /* pointer_mode_host = */ true,       \\\n                           ROCMBlasSide(side), ROCMBlasUpperLower(uplo),      \\\n                           ROCMBlasTranspose(transa), ROCMBlasDiagonal(diag), \\\n@@ -1168,9 +1169,9 @@ IMPL_DoBlasGemmBatched(float, wrap::rocblas_sgemm_strided_batched)\n   }                                                                          \\\n                                                                              \\\n   bool ROCMBlas::DoBlasTrsmBatched(                                          \\\n-      Stream *stream, blas::Side side, blas::UpperLower uplo,                \\\n+      Stream* stream, blas::Side side, blas::UpperLower uplo,                \\\n       blas::Transpose transa, blas::Diagonal diag, uint64_t m, uint64_t n,   \\\n-      T alpha, const DeviceMemory<T *> &as, int lda, DeviceMemory<T *> *bs,  \\\n+      T alpha, const DeviceAddress<T*>& as, int lda, DeviceAddress<T*>* bs,  \\\n       int ldb, int batch_count) {                                            \\\n     return DoBlasInternal(Fun2, stream, true /* = pointer_mode_host */,      \\\n                           ROCMBlasSide(side), ROCMBlasUpperLower(uplo),      \\\n@@ -1194,10 +1195,11 @@ IMPL_DoBlasGemmBatched(float, wrap::rocblas_sgemm_strided_batched)\n     ROCMBlas::DoBlasGemmStridedBatched(\n         Stream* stream, blas::Transpose transa, blas::Transpose transb,\n         uint64_t m, uint64_t n, uint64_t k, blas::DataType dtype,\n-        const void* alpha, const DeviceMemoryBase& a, int lda, int64_t stride_a,\n-        const DeviceMemoryBase& b, int ldb, int64_t stride_b, const void* beta,\n-        DeviceMemoryBase* c, int ldc, int64_t stride_c, int batch_count,\n-        const EngineOptions& engine_options, blas::CallContext context) {\n+        const void* alpha, const DeviceAddressBase& a, int lda,\n+        int64_t stride_a, const DeviceAddressBase& b, int ldb, int64_t stride_b,\n+        const void* beta, DeviceAddressBase* c, int ldc, int64_t stride_c,\n+        int batch_count, const EngineOptions& engine_options,\n+        blas::CallContext context) {\n   VLOG(1) << absl::StreamFormat(\n       \"doing rocBLAS GEMM Strided Batched: at=%d bt=%d m=%u n=%u \"\n       \"k=%llu alpha=%p a=%p lda=%d b=%p ldb=%d beta=%p \""
        },
        {
            "sha": "80dde539d47ec92bff0050b7934521b76ae2b5bf",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_blas.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_blas.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -174,12 +174,12 @@ class ROCMBlas : public blas::BlasSupport {\n   // reallocate the memory layout to be strided batched.\n   template <typename T, typename FuncT>\n   absl::Status DoBlasGemmBatchedInternal(\n-      FuncT rocblas_func, Stream *stream, blas::Transpose transa,\n+      FuncT rocblas_func, Stream* stream, blas::Transpose transa,\n       blas::Transpose transb, uint64_t m, uint64_t n, uint64_t k, T alpha,\n-      DeviceMemorySlice<T> a_ptrs_to_wrappers, int lda,\n-      DeviceMemorySlice<T> b_ptrs_to_wrappers, int ldb, T beta,\n-      DeviceMemorySlice<T> c_ptrs_to_wrappers, int ldc, int batch_count,\n-      ScratchAllocator *scratch_allocator);\n+      DeviceAddressSlice<T> a_ptrs_to_wrappers, int lda,\n+      DeviceAddressSlice<T> b_ptrs_to_wrappers, int ldb, T beta,\n+      DeviceAddressSlice<T> c_ptrs_to_wrappers, int ldc, int batch_count,\n+      ScratchAllocator* scratch_allocator);\n \n   // mutex that guards the rocBLAS handle for this device.\n   mutable absl::Mutex mu_;"
        },
        {
            "sha": "528f4febfcd2e42ec0d42119b5d87e727a32f1e9",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_command_buffer.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n #include \"rocm/include/hip/hip_runtime.h\"\n #include \"xla/stream_executor/bit_pattern.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_command_buffer.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -61,7 +61,7 @@ absl::StatusOr<hipGraph_t> CreateGraph() {\n   return graph;\n }\n \n-hipDeviceptr_t AsDevicePtr(const DeviceMemoryBase& mem) {\n+hipDeviceptr_t AsDevicePtr(const DeviceAddressBase& mem) {\n   return absl::bit_cast<hipDeviceptr_t>(mem.opaque());\n }\n \n@@ -109,7 +109,7 @@ RocmCommandBuffer::CreateConditionalNode(\n \n absl::StatusOr<GraphNodeHandle> RocmCommandBuffer::CreateSetCaseConditionNode(\n     absl::Span<const GraphConditionalHandle> conditionals,\n-    DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+    DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n     bool enable_conditional_default,\n     absl::Span<const GraphNodeHandle> dependencies) {\n   return absl::UnimplementedError(\"Conditionals are not supported on ROCM.\");\n@@ -118,26 +118,27 @@ absl::StatusOr<GraphNodeHandle> RocmCommandBuffer::CreateSetCaseConditionNode(\n absl::Status RocmCommandBuffer::UpdateSetCaseConditionNode(\n     GraphNodeHandle handle,\n     absl::Span<const GraphConditionalHandle> conditionals,\n-    DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+    DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n     bool enable_conditional_default) {\n   return absl::UnimplementedError(\"Conditionals are not supported on ROCM.\");\n }\n \n absl::StatusOr<GraphNodeHandle> RocmCommandBuffer::CreateSetWhileConditionNode(\n-    GraphConditionalHandle conditional, DeviceMemory<bool> predicate,\n+    GraphConditionalHandle conditional, DeviceAddress<bool> predicate,\n     absl::Span<const GraphNodeHandle> dependencies) {\n   return absl::UnimplementedError(\"Conditionals are not supported on ROCM.\");\n }\n \n absl::Status RocmCommandBuffer::UpdateSetWhileConditionNode(\n     GraphNodeHandle handle, GraphConditionalHandle conditional,\n-    DeviceMemory<bool> predicate) {\n+    DeviceAddress<bool> predicate) {\n   return absl::UnimplementedError(\"Conditionals are not supported on ROCM.\");\n }\n \n absl::StatusOr<GraphNodeHandle> RocmCommandBuffer::CreateMemsetNode(\n     absl::Span<const GraphNodeHandle> dependencies,\n-    DeviceMemoryBase destination, BitPattern bit_pattern, size_t num_elements) {\n+    DeviceAddressBase destination, BitPattern bit_pattern,\n+    size_t num_elements) {\n   VLOG(2) << \"Add memset node to a graph \" << graph_\n           << \"; dst: \" << destination.opaque()\n           << \"; bit_pattern: \" << bit_pattern.ToString()\n@@ -163,7 +164,7 @@ absl::StatusOr<GraphNodeHandle> RocmCommandBuffer::CreateMemsetNode(\n }\n \n absl::Status RocmCommandBuffer::UpdateMemsetNode(GraphNodeHandle node_handle,\n-                                                 DeviceMemoryBase destination,\n+                                                 DeviceAddressBase destination,\n                                                  BitPattern bit_pattern,\n                                                  size_t num_elements) {\n   VLOG(2) << \"Set memset node params \" << node_handle << \" in graph executable \"\n@@ -186,7 +187,7 @@ absl::Status RocmCommandBuffer::UpdateMemsetNode(GraphNodeHandle node_handle,\n \n absl::StatusOr<GraphNodeHandle> RocmCommandBuffer::CreateMemcpyD2DNode(\n     absl::Span<const GraphNodeHandle> dependencies,\n-    DeviceMemoryBase destination, DeviceMemoryBase source, uint64_t size) {\n+    DeviceAddressBase destination, DeviceAddressBase source, uint64_t size) {\n   VLOG(2) << \"Add memcpy d2d node to a graph \" << graph_\n           << \"; dst: \" << destination.opaque() << \"; src: \" << source.opaque()\n           << \"; size: \" << size << \"; deps: \" << dependencies.size();\n@@ -204,8 +205,8 @@ absl::StatusOr<GraphNodeHandle> RocmCommandBuffer::CreateMemcpyD2DNode(\n }\n \n absl::Status RocmCommandBuffer::UpdateMemcpyD2DNode(\n-    GraphNodeHandle node_handle, DeviceMemoryBase destination,\n-    DeviceMemoryBase source, uint64_t size) {\n+    GraphNodeHandle node_handle, DeviceAddressBase destination,\n+    DeviceAddressBase source, uint64_t size) {\n   VLOG(2) << \"Set memcpy d2d node params \" << node_handle\n           << \" in graph executable \" << exec_\n           << \"; dst: \" << destination.opaque() << \"; src: \" << source.opaque()"
        },
        {
            "sha": "5f69669f93daa5b5205376fb3c0603ac49da7705",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_command_buffer.h",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"rocm/include/hip/hip_runtime.h\"\n #include \"xla/stream_executor/bit_pattern.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_command_buffer.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -63,23 +63,23 @@ class RocmCommandBuffer : public GpuCommandBuffer {\n \n   absl::StatusOr<GraphNodeHandle> CreateSetCaseConditionNode(\n       absl::Span<const GraphConditionalHandle> conditionals,\n-      DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+      DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n       bool enable_conditional_default,\n       absl::Span<const GraphNodeHandle> dependencies) override;\n \n   absl::Status UpdateSetCaseConditionNode(\n       GraphNodeHandle handle,\n       absl::Span<const GraphConditionalHandle> conditionals,\n-      DeviceMemory<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n+      DeviceAddress<uint8_t> index, bool index_is_bool, int32_t batch_offset,\n       bool enable_conditional_default) override;\n \n   absl::StatusOr<GraphNodeHandle> CreateSetWhileConditionNode(\n-      GraphConditionalHandle conditional, DeviceMemory<bool> predicate,\n+      GraphConditionalHandle conditional, DeviceAddress<bool> predicate,\n       absl::Span<const GraphNodeHandle> dependencies) override;\n \n   absl::Status UpdateSetWhileConditionNode(\n       GraphNodeHandle handle, GraphConditionalHandle conditional,\n-      DeviceMemory<bool> predicate) override;\n+      DeviceAddress<bool> predicate) override;\n \n   //===--------------------------------------------------------------------===//\n \n@@ -89,31 +89,32 @@ class RocmCommandBuffer : public GpuCommandBuffer {\n \n   absl::StatusOr<GraphNodeHandle> CreateMemsetNode(\n       absl::Span<const GraphNodeHandle> dependencies,\n-      DeviceMemoryBase destination, BitPattern bit_pattern,\n+      DeviceAddressBase destination, BitPattern bit_pattern,\n       size_t num_elements) override;\n \n   absl::Status UpdateMemsetNode(GraphNodeHandle node_handle,\n-                                DeviceMemoryBase destination,\n+                                DeviceAddressBase destination,\n                                 BitPattern bit_pattern,\n                                 size_t num_elements) override;\n \n   absl::StatusOr<GraphNodeHandle> CreateMemcpyD2DNode(\n       absl::Span<const GraphNodeHandle> dependencies,\n-      DeviceMemoryBase destination, DeviceMemoryBase source,\n+      DeviceAddressBase destination, DeviceAddressBase source,\n       uint64_t size) override;\n \n   absl::Status UpdateMemcpyD2DNode(GraphNodeHandle node_handle,\n-                                   DeviceMemoryBase destination,\n-                                   DeviceMemoryBase source,\n+                                   DeviceAddressBase destination,\n+                                   DeviceAddressBase source,\n                                    uint64_t size) override;\n \n   absl::Status PopulateDnnGraphNode(\n-      dnn::DnnGraph&, Stream&, absl::Span<DeviceMemoryBase> operands) override {\n+      dnn::DnnGraph&, Stream&,\n+      absl::Span<DeviceAddressBase> operands) override {\n     return absl::UnimplementedError(\"Not implemented.\");\n   }\n \n   absl::Status UpdateDnnGraphNode(dnn::DnnGraph&, Stream&,\n-                                  absl::Span<DeviceMemoryBase> operands,\n+                                  absl::Span<DeviceAddressBase> operands,\n                                   GraphNodeHandle) override {\n     return absl::UnimplementedError(\"Not implemented.\");\n   }"
        },
        {
            "sha": "3c272a578d345572a4a65f0948248ddd0ba47825",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_dnn.cc",
            "status": "modified",
            "additions": 268,
            "deletions": 261,
            "changes": 529,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -49,8 +49,8 @@ limitations under the License.\n #include \"rocm/rocm_config.h\"\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -2056,7 +2056,7 @@ class MIOpenDropoutDescriptor {\n     }\n \n     if (dropout > 0.0f) {\n-      DeviceMemory<uint8_t> state_memory;\n+      DeviceAddress<uint8_t> state_memory;\n       if (state_allocator) {\n         size_t state_sizes_in_bytes = 0;\n         status = wrap::miopenDropoutGetStatesSize(miopen_handle,\n@@ -2335,17 +2335,17 @@ template <class T>\n bool ExtractAndCheckRnnForward(\n     const MIOpenRnnDescriptor& rnn_desc,\n     const MIOpenRnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<T>& input_data,\n+    const DeviceAddress<T>& input_data,\n     const MIOpenRnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<T>& input_h_data,\n+    const DeviceAddress<T>& input_h_data,\n     const MIOpenRnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+    const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n     const MIOpenRnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<T>& output_data,\n+    const DeviceAddress<T>& output_data,\n     const MIOpenRnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<T>& output_h_data,\n+    const DeviceAddress<T>& output_h_data,\n     const MIOpenRnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<T>& output_c_data, RnnModelDims* model_dims) {\n+    const DeviceAddress<T>& output_c_data, RnnModelDims* model_dims) {\n   // extract model parameters\n   model_dims->num_layers = rnn_desc.num_layers();\n   model_dims->batch_size = input_desc.batch_size();\n@@ -2412,7 +2412,7 @@ bool CreateRnnWorkspace(Stream* stream, miopenHandle_t miopen_handle,\n                         const MIOpenRnnDescriptor& rnn_desc,\n                         const MIOpenRnnSequenceTensorDescriptor& input_desc,\n                         ScratchAllocator* workspace_allocator,\n-                        DeviceMemory<uint8_t>* workspace) {\n+                        DeviceAddress<uint8_t>* workspace) {\n   // Query the workspace size.\n   size_t workspace_size_in_bytes = 0;\n   auto status = wrap::miopenGetRNNWorkspaceSize(\n@@ -2436,7 +2436,7 @@ bool CreateRnnWorkspace(Stream* stream, miopenHandle_t miopen_handle,\n       return false;\n     }\n   } else {\n-    *workspace = DeviceMemory<uint8_t>();\n+    *workspace = DeviceAddress<uint8_t>();\n   }\n   return true;\n }\n@@ -2447,17 +2447,17 @@ template <class T>\n absl::Status MIOpenSupport::DoRnnForwardImpl(\n     Stream* stream, const MIOpenRnnDescriptor& rnn_desc,\n     const MIOpenRnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<T>& input_data,\n+    const DeviceAddress<T>& input_data,\n     const MIOpenRnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<T>& input_h_data,\n+    const DeviceAddress<T>& input_h_data,\n     const MIOpenRnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+    const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n     const MIOpenRnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<T>* output_data,\n+    DeviceAddress<T>* output_data,\n     const MIOpenRnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<T>* output_h_data,\n+    DeviceAddress<T>* output_h_data,\n     const MIOpenRnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<T>* output_c_data, bool is_training,\n+    DeviceAddress<T>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -2483,7 +2483,7 @@ absl::Status MIOpenSupport::DoRnnForwardImpl(\n   }\n \n   // create the workspace\n-  DeviceMemory<uint8_t> workspace;\n+  DeviceAddress<uint8_t> workspace;\n   if (!CreateRnnWorkspace(stream, miopen.handle(), rnn_desc, input_desc,\n                           workspace_allocator, &workspace)) {\n     LOG(ERROR) << \"Unable to create rnn workspace\";\n@@ -2492,7 +2492,7 @@ absl::Status MIOpenSupport::DoRnnForwardImpl(\n \n   // query the reserve space size\n   // allocate the reserve space\n-  DeviceMemory<uint8_t> reserve_space;\n+  DeviceAddress<uint8_t> reserve_space;\n   if (is_training) {\n     size_t reserve_space_size_in_bytes = 0;\n     auto status = wrap::miopenGetRNNTrainingReserveSize(\n@@ -2579,25 +2579,25 @@ template <class T>\n absl::Status MIOpenSupport::DoRnnBackwardImpl(\n     Stream* stream, const MIOpenRnnDescriptor& rnn_desc,\n     const MIOpenRnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<T>& input_data,\n+    const DeviceAddress<T>& input_data,\n     const MIOpenRnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<T>& input_h_data,\n+    const DeviceAddress<T>& input_h_data,\n     const MIOpenRnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+    const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n     const MIOpenRnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<T>& output_data,\n+    const DeviceAddress<T>& output_data,\n     const MIOpenRnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<T>& output_h_data,\n+    const DeviceAddress<T>& output_h_data,\n     const MIOpenRnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<T>& output_c_data,\n-    const DeviceMemory<T>& output_backprop_data,\n-    const DeviceMemory<T>& output_h_backprop_data,\n-    const DeviceMemory<T>& output_c_backprop_data,\n-    DeviceMemory<T>* input_backprop_data,\n-    DeviceMemory<T>* input_h_backprop_data,\n-    DeviceMemory<T>* input_c_backprop_data,\n-    DeviceMemory<T>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<T>& output_c_data,\n+    const DeviceAddress<T>& output_backprop_data,\n+    const DeviceAddress<T>& output_h_backprop_data,\n+    const DeviceAddress<T>& output_c_backprop_data,\n+    DeviceAddress<T>* input_backprop_data,\n+    DeviceAddress<T>* input_h_backprop_data,\n+    DeviceAddress<T>* input_c_backprop_data,\n+    DeviceAddress<T>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   // extract model parameters\n@@ -2621,7 +2621,7 @@ absl::Status MIOpenSupport::DoRnnBackwardImpl(\n   }\n \n   // create the workspace\n-  DeviceMemory<uint8_t> workspace;\n+  DeviceAddress<uint8_t> workspace;\n   if (!CreateRnnWorkspace(stream, miopen.handle(), rnn_desc, input_desc,\n                           workspace_allocator, &workspace)) {\n     LOG(ERROR) << \"Unable to create rnn workspace\";\n@@ -2798,7 +2798,7 @@ absl::Status MIOpenSupport::DoPrepareForCtcLoss(\n     absl::Span<const int> labels_lengths_data,\n     absl::Span<const int> input_lengths_data,\n     const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n-    DeviceMemory<uint8_t>* scratch_memory, int* ctc_loss_algo_id) {\n+    DeviceAddress<uint8_t>* scratch_memory, int* ctc_loss_algo_id) {\n   auto miopen = miopen_->GetHandle(parent_, stream);\n \n   MIOpenCTCLossDescriptor miopen_ctc_loss_desc(ToMIOpenDataType(element_type));\n@@ -2825,7 +2825,7 @@ absl::Status MIOpenSupport::DoPrepareForCtcLoss(\n         \"Failed to determine scratch memory size for MIOpen CTC Loss\");\n   }\n \n-  *scratch_memory = DeviceMemory<uint8_t>();\n+  *scratch_memory = DeviceAddress<uint8_t>();\n \n   // Allocate the workspace.\n   if (workspace_size_in_bytes != 0) {\n@@ -2855,12 +2855,12 @@ absl::Status MIOpenSupport::DoPrepareForCtcLoss(\n \n absl::Status MIOpenSupport::DoCtcLossImpl(\n     Stream* stream, const MIOpenRnnStateTensorDescriptor& probs_desc,\n-    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+    const DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n     absl::Span<const int> labels_lengths_data,\n-    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n+    absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n     const MIOpenRnnStateTensorDescriptor& grads_desc,\n-    DeviceMemoryBase grads_data, const MIOpenCTCLossDescriptor& ctc_loss_desc,\n-    DeviceMemory<uint8_t> scratch_memory, int ctc_loss_algo_id) {\n+    DeviceAddressBase grads_data, const MIOpenCTCLossDescriptor& ctc_loss_desc,\n+    DeviceAddress<uint8_t> scratch_memory, int ctc_loss_algo_id) {\n   auto miopen = miopen_->GetHandle(parent_, stream);\n \n   int kNumTimestamps = probs_desc.num_layers();\n@@ -2886,11 +2886,11 @@ absl::Status MIOpenSupport::DoCtcLossImpl(\n absl::Status MIOpenSupport::DoCtcLoss(\n     Stream* stream, dnn::DataType element_type,\n     const dnn::RnnStateTensorDescriptor& probs_desc,\n-    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+    const DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n     absl::Span<const int> labels_lengths_data,\n-    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n+    absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n     const dnn::RnnStateTensorDescriptor& grads_desc,\n-    DeviceMemoryBase grads_data, DeviceMemory<uint8_t> scratch_memory,\n+    DeviceAddressBase grads_data, DeviceAddress<uint8_t> scratch_memory,\n     int ctc_loss_algo_id) {\n   // Current MIOPen CTC Loss only supports the float datatype\n   if (element_type != dnn::DataType::kFloat) {\n@@ -2979,19 +2979,19 @@ MIOpenSupport::CreateRnnStateTensorDescriptor(int num_layer, int batch_size,\n bool MIOpenSupport::DoRnnForward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<Eigen::half>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<Eigen::half>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<Eigen::half>& input_h_data,\n+    const DeviceAddress<Eigen::half>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<Eigen::half>& input_c_data,\n-    const DeviceMemory<Eigen::half>& params,\n+    const DeviceAddress<Eigen::half>& input_c_data,\n+    const DeviceAddress<Eigen::half>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<Eigen::half>* output_data,\n+    DeviceAddress<Eigen::half>* output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<Eigen::half>* output_h_data,\n+    DeviceAddress<Eigen::half>* output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n+    DeviceAddress<Eigen::half>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -3023,18 +3023,19 @@ bool MIOpenSupport::DoRnnForward(\n bool MIOpenSupport::DoRnnForward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<float>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<float>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<float>& input_h_data,\n+    const DeviceAddress<float>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n+    const DeviceAddress<float>& input_c_data,\n+    const DeviceAddress<float>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<float>* output_data,\n+    DeviceAddress<float>* output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<float>* output_h_data,\n+    DeviceAddress<float>* output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<float>* output_c_data, bool is_training,\n+    DeviceAddress<float>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -3066,19 +3067,19 @@ bool MIOpenSupport::DoRnnForward(\n bool MIOpenSupport::DoRnnForward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<double>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<double>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<double>& input_h_data,\n+    const DeviceAddress<double>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<double>& input_c_data,\n-    const DeviceMemory<double>& params,\n+    const DeviceAddress<double>& input_c_data,\n+    const DeviceAddress<double>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    DeviceMemory<double>* output_data,\n+    DeviceAddress<double>* output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    DeviceMemory<double>* output_h_data,\n+    DeviceAddress<double>* output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    DeviceMemory<double>* output_c_data, bool is_training,\n+    DeviceAddress<double>* output_c_data, bool is_training,\n     ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n@@ -3089,27 +3090,27 @@ bool MIOpenSupport::DoRnnForward(\n bool MIOpenSupport::DoRnnBackward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<Eigen::half>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<Eigen::half>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<Eigen::half>& input_h_data,\n+    const DeviceAddress<Eigen::half>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<Eigen::half>& input_c_data,\n-    const DeviceMemory<Eigen::half>& params,\n+    const DeviceAddress<Eigen::half>& input_c_data,\n+    const DeviceAddress<Eigen::half>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<Eigen::half>& output_data,\n+    const DeviceAddress<Eigen::half>& output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<Eigen::half>& output_h_data,\n+    const DeviceAddress<Eigen::half>& output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<Eigen::half>& output_c_data,\n-    const DeviceMemory<Eigen::half>& output_backprop_data,\n-    const DeviceMemory<Eigen::half>& output_h_backprop_data,\n-    const DeviceMemory<Eigen::half>& output_c_backprop_data,\n-    DeviceMemory<Eigen::half>* input_backprop_data,\n-    DeviceMemory<Eigen::half>* input_h_backprop_data,\n-    DeviceMemory<Eigen::half>* input_c_backprop_data,\n-    DeviceMemory<Eigen::half>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<Eigen::half>& output_c_data,\n+    const DeviceAddress<Eigen::half>& output_backprop_data,\n+    const DeviceAddress<Eigen::half>& output_h_backprop_data,\n+    const DeviceAddress<Eigen::half>& output_c_backprop_data,\n+    DeviceAddress<Eigen::half>* input_backprop_data,\n+    DeviceAddress<Eigen::half>* input_h_backprop_data,\n+    DeviceAddress<Eigen::half>* input_c_backprop_data,\n+    DeviceAddress<Eigen::half>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   const MIOpenRnnDescriptor& miopen_rnn_desc =\n@@ -3143,26 +3144,27 @@ bool MIOpenSupport::DoRnnBackward(\n bool MIOpenSupport::DoRnnBackward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<float>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<float>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<float>& input_h_data,\n+    const DeviceAddress<float>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n+    const DeviceAddress<float>& input_c_data,\n+    const DeviceAddress<float>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<float>& output_data,\n+    const DeviceAddress<float>& output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<float>& output_h_data,\n+    const DeviceAddress<float>& output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<float>& output_c_data,\n-    const DeviceMemory<float>& output_backprop_data,\n-    const DeviceMemory<float>& output_h_backprop_data,\n-    const DeviceMemory<float>& output_c_backprop_data,\n-    DeviceMemory<float>* input_backprop_data,\n-    DeviceMemory<float>* input_h_backprop_data,\n-    DeviceMemory<float>* input_c_backprop_data,\n-    DeviceMemory<float>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<float>& output_c_data,\n+    const DeviceAddress<float>& output_backprop_data,\n+    const DeviceAddress<float>& output_h_backprop_data,\n+    const DeviceAddress<float>& output_c_backprop_data,\n+    DeviceAddress<float>* input_backprop_data,\n+    DeviceAddress<float>* input_h_backprop_data,\n+    DeviceAddress<float>* input_c_backprop_data,\n+    DeviceAddress<float>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   const MIOpenRnnDescriptor& miopen_rnn_desc =\n@@ -3196,27 +3198,27 @@ bool MIOpenSupport::DoRnnBackward(\n bool MIOpenSupport::DoRnnBackward(\n     Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-    const DeviceMemory<double>& input_data,\n-    const DeviceMemory<int>& seq_lengths_data,\n+    const DeviceAddress<double>& input_data,\n+    const DeviceAddress<int>& seq_lengths_data,\n     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-    const DeviceMemory<double>& input_h_data,\n+    const DeviceAddress<double>& input_h_data,\n     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-    const DeviceMemory<double>& input_c_data,\n-    const DeviceMemory<double>& params,\n+    const DeviceAddress<double>& input_c_data,\n+    const DeviceAddress<double>& params,\n     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-    const DeviceMemory<double>& output_data,\n+    const DeviceAddress<double>& output_data,\n     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-    const DeviceMemory<double>& output_h_data,\n+    const DeviceAddress<double>& output_h_data,\n     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-    const DeviceMemory<double>& output_c_data,\n-    const DeviceMemory<double>& output_backprop_data,\n-    const DeviceMemory<double>& output_h_backprop_data,\n-    const DeviceMemory<double>& output_c_backprop_data,\n-    DeviceMemory<double>* input_backprop_data,\n-    DeviceMemory<double>* input_h_backprop_data,\n-    DeviceMemory<double>* input_c_backprop_data,\n-    DeviceMemory<double>* params_backprop_data,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    const DeviceAddress<double>& output_c_data,\n+    const DeviceAddress<double>& output_backprop_data,\n+    const DeviceAddress<double>& output_h_backprop_data,\n+    const DeviceAddress<double>& output_c_backprop_data,\n+    DeviceAddress<double>* input_backprop_data,\n+    DeviceAddress<double>* input_h_backprop_data,\n+    DeviceAddress<double>* input_c_backprop_data,\n+    DeviceAddress<double>* params_backprop_data,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator,\n     dnn::ProfileResult* output_profile_result) {\n   LOG(ERROR) << \"miopen does not support half type RNN bwd yet\";\n@@ -3236,7 +3238,7 @@ void* MIOpenAllocatorCallback(void* ctx, size_t size_in_bytes) {\n   auto* mac = static_cast<MIOpenAllocatorContext*>(ctx);\n   auto allocated = mac->scratch_allocator_->AllocateBytes(size_in_bytes);\n \n-  DeviceMemory<uint8_t> scratch;\n+  DeviceAddress<uint8_t> scratch;\n   if (allocated.ok()) {\n     scratch = allocated.value();\n     return scratch.opaque();\n@@ -3292,10 +3294,10 @@ class RocmConvRunner : public dnn::ConvRunner {\n \n   absl::Status operator()(Stream* stream,\n                           dnn::ProfileResult* output_profile_result,\n-                          DeviceMemoryBase scratch_memory,\n-                          DeviceMemoryBase input_data,\n-                          DeviceMemoryBase filter_data,\n-                          DeviceMemoryBase output_data) const override {\n+                          DeviceAddressBase scratch_memory,\n+                          DeviceAddressBase input_data,\n+                          DeviceAddressBase filter_data,\n+                          DeviceAddressBase output_data) const override {\n     auto miopen = miopen_->GetHandle(parent_, stream);\n     // Alpha is the scaling factor for input.\n     float alpha = 1.0;\n@@ -3413,10 +3415,11 @@ class RocmConvRunner : public dnn::ConvRunner {\n absl::Status MIOpenSupport::GetConvolveRunners(\n     dnn::ConvolutionKind kind, dnn::DataType input_type,\n     dnn::DataType output_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+    const dnn::BatchDescriptor& input_descriptor, DeviceAddressBase input_data,\n     const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n+    DeviceAddressBase filter_data,\n+    const dnn::BatchDescriptor& output_descriptor,\n+    DeviceAddressBase output_data,\n     const dnn::ConvolutionDescriptor& convolution_descriptor, bool use_fallback,\n     ScratchAllocator* scratch_allocator, const EngineOptions& engine_options,\n     std::vector<std::unique_ptr<const dnn::ConvRunner>>* out_runners) {\n@@ -3472,10 +3475,11 @@ MIOpenSupport::ConvolveRunnerFromDesc(\n bool MIOpenSupport::GetMIOpenConvolveAlgorithms(\n     dnn::ConvolutionKind kind, dnn::DataType input_type,\n     dnn::DataType output_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+    const dnn::BatchDescriptor& input_descriptor, DeviceAddressBase input_data,\n     const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n+    DeviceAddressBase filter_data,\n+    const dnn::BatchDescriptor& output_descriptor,\n+    DeviceAddressBase output_data,\n     const dnn::ConvolutionDescriptor& convolution_descriptor,\n     ScratchAllocator* scratch_allocator,\n     std::vector<dnn::ProfileResult>* out_algorithms) {\n@@ -3497,10 +3501,11 @@ bool MIOpenSupport::GetMIOpenConvolveAlgorithms(\n absl::Status MIOpenSupport::GetMIOpenConvolveAlgorithmsImmediateMode(\n     dnn::ConvolutionKind kind, dnn::DataType input_type,\n     dnn::DataType output_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+    const dnn::BatchDescriptor& input_descriptor, DeviceAddressBase input_data,\n     const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n+    DeviceAddressBase filter_data,\n+    const dnn::BatchDescriptor& output_descriptor,\n+    DeviceAddressBase output_data,\n     const dnn::ConvolutionDescriptor& convolution_descriptor,\n     ScratchAllocator* scratch_allocator,\n     std::vector<dnn::ProfileResult>* out_algorithms) {\n@@ -3710,10 +3715,11 @@ absl::Status MIOpenSupport::GetMIOpenConvolveAlgorithmsImmediateMode(\n absl::Status MIOpenSupport::GetMIOpenConvolveAlgorithmsFindMode(\n     dnn::ConvolutionKind kind, dnn::DataType input_type,\n     dnn::DataType output_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+    const dnn::BatchDescriptor& input_descriptor, DeviceAddressBase input_data,\n     const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n+    DeviceAddressBase filter_data,\n+    const dnn::BatchDescriptor& output_descriptor,\n+    DeviceAddressBase output_data,\n     const dnn::ConvolutionDescriptor& convolution_descriptor,\n     ScratchAllocator* scratch_allocator,\n     std::vector<dnn::ProfileResult>* out_algorithms) {\n@@ -3785,7 +3791,7 @@ absl::Status MIOpenSupport::GetMIOpenConvolveAlgorithmsFindMode(\n   }\n \n   // allocate scratch memory\n-  DeviceMemory<uint8_t> scratch_memory;\n+  DeviceAddress<uint8_t> scratch_memory;\n   if (scratch_memory_size != 0) {\n     if (scratch_allocator == nullptr) {\n       return absl::InternalError(\n@@ -3892,17 +3898,17 @@ bool MIOpenSupport::GetRnnAlgorithms(\n }\n \n bool MIOpenSupport::DoBatchNormalizationForward(\n-    Stream* stream, const DeviceMemory<Eigen::bfloat16>& x,\n-    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-    const DeviceMemory<float>& estimated_mean,\n-    const DeviceMemory<float>& estimated_variance,\n-    const DeviceMemory<Eigen::bfloat16>& side_input,\n+    Stream* stream, const DeviceAddress<Eigen::bfloat16>& x,\n+    const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+    const DeviceAddress<float>& estimated_mean,\n+    const DeviceAddress<float>& estimated_variance,\n+    const DeviceAddress<Eigen::bfloat16>& side_input,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::bfloat16>* y,\n-    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<Eigen::bfloat16>* y,\n+    DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+    DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n     bool is_training, ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator) {\n   return DoBatchNormalizationForwardImpl<Eigen::bfloat16, float>(\n@@ -3915,17 +3921,17 @@ bool MIOpenSupport::DoBatchNormalizationForward(\n }\n \n bool MIOpenSupport::DoBatchNormalizationForward(\n-    Stream* stream, const DeviceMemory<Eigen::half>& x,\n-    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-    const DeviceMemory<float>& estimated_mean,\n-    const DeviceMemory<float>& estimated_variance,\n-    const DeviceMemory<Eigen::half>& side_input,\n+    Stream* stream, const DeviceAddress<Eigen::half>& x,\n+    const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+    const DeviceAddress<float>& estimated_mean,\n+    const DeviceAddress<float>& estimated_variance,\n+    const DeviceAddress<Eigen::half>& side_input,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y,\n-    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<Eigen::half>* y,\n+    DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+    DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n     bool is_training, ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator) {\n   return DoBatchNormalizationForwardImpl<Eigen::half, float>(\n@@ -3938,16 +3944,16 @@ bool MIOpenSupport::DoBatchNormalizationForward(\n }\n \n bool MIOpenSupport::DoBatchNormalizationForward(\n-    Stream* stream, const DeviceMemory<float>& x,\n-    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-    const DeviceMemory<float>& estimated_mean,\n-    const DeviceMemory<float>& estimated_variance,\n-    const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n+    Stream* stream, const DeviceAddress<float>& x,\n+    const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+    const DeviceAddress<float>& estimated_mean,\n+    const DeviceAddress<float>& estimated_variance,\n+    const DeviceAddress<float>& side_input, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<float>* y,\n-    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<float>* y,\n+    DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+    DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n     bool is_training, ScratchAllocator* reserve_space_allocator,\n     ScratchAllocator* workspace_allocator) {\n   return DoBatchNormalizationForwardImpl<float, float>(\n@@ -3962,16 +3968,16 @@ bool MIOpenSupport::DoBatchNormalizationForward(\n template <class T, class U>\n absl::Status MIOpenSupport::DoBatchNormalizationForwardImpl(\n     Stream* stream, dnn::DataType input_data_type,\n-    dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n-    const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n-    const DeviceMemory<U>& estimated_mean,\n-    const DeviceMemory<U>& estimated_variance,\n-    const DeviceMemory<T>& side_input, const dnn::BatchDescriptor& x_desc,\n+    dnn::DataType scale_data_type, const DeviceAddress<T>& x,\n+    const DeviceAddress<U>& scale, const DeviceAddress<U>& offset,\n+    const DeviceAddress<U>& estimated_mean,\n+    const DeviceAddress<U>& estimated_variance,\n+    const DeviceAddress<T>& side_input, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     const double exponential_average_factor,\n-    dnn::ActivationMode activation_mode, DeviceMemory<T>* y,\n-    DeviceMemory<U>* batch_mean, DeviceMemory<U>* batch_var,\n-    DeviceMemory<U>* saved_mean, DeviceMemory<U>* saved_inv_var,\n+    dnn::ActivationMode activation_mode, DeviceAddress<T>* y,\n+    DeviceAddress<U>* batch_mean, DeviceAddress<U>* batch_var,\n+    DeviceAddress<U>* saved_mean, DeviceAddress<U>* saved_inv_var,\n     bool is_training) {\n   auto miopen = miopen_->GetHandle(parent_, stream);\n \n@@ -4010,17 +4016,17 @@ absl::Status MIOpenSupport::DoBatchNormalizationForwardImpl(\n }\n \n bool MIOpenSupport::DoBatchNormalizationBackward(\n-    Stream* stream, const DeviceMemory<Eigen::bfloat16>& y_backprop,\n-    const DeviceMemory<Eigen::bfloat16>& x, const DeviceMemory<float>& scale,\n-    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-    const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::bfloat16>& y,\n-    const dnn::BatchDescriptor& x_desc,\n+    Stream* stream, const DeviceAddress<Eigen::bfloat16>& y_backprop,\n+    const DeviceAddress<Eigen::bfloat16>& x, const DeviceAddress<float>& scale,\n+    const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+    const DeviceAddress<float>& inv_var,\n+    const DeviceAddress<Eigen::bfloat16>& y, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n     dnn::ActivationMode activation_mode,\n-    DeviceMemory<Eigen::bfloat16>* x_backprop,\n-    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-    DeviceMemory<Eigen::bfloat16>* side_input_backprop,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    DeviceAddress<Eigen::bfloat16>* x_backprop,\n+    DeviceAddress<float>* scale_backprop, DeviceAddress<float>* offset_backprop,\n+    DeviceAddress<Eigen::bfloat16>* side_input_backprop,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator) {\n   return DoBatchNormalizationBackwardImpl<Eigen::bfloat16, float>(\n              stream, miopenBFloat16, miopenFloat, y_backprop, x, scale, mean,\n@@ -4030,16 +4036,16 @@ bool MIOpenSupport::DoBatchNormalizationBackward(\n }\n \n bool MIOpenSupport::DoBatchNormalizationBackward(\n-    Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n-    const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n-    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-    const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::half>& y,\n+    Stream* stream, const DeviceAddress<Eigen::half>& y_backprop,\n+    const DeviceAddress<Eigen::half>& x, const DeviceAddress<float>& scale,\n+    const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+    const DeviceAddress<float>& inv_var, const DeviceAddress<Eigen::half>& y,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* x_backprop,\n-    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-    DeviceMemory<Eigen::half>* side_input_backprop,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    dnn::ActivationMode activation_mode, DeviceAddress<Eigen::half>* x_backprop,\n+    DeviceAddress<float>* scale_backprop, DeviceAddress<float>* offset_backprop,\n+    DeviceAddress<Eigen::half>* side_input_backprop,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator) {\n   return DoBatchNormalizationBackwardImpl<Eigen::half, float>(\n              stream, miopenHalf, miopenFloat, y_backprop, x, scale, mean,\n@@ -4049,16 +4055,16 @@ bool MIOpenSupport::DoBatchNormalizationBackward(\n }\n \n bool MIOpenSupport::DoBatchNormalizationBackward(\n-    Stream* stream, const DeviceMemory<float>& y_backprop,\n-    const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n-    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-    const DeviceMemory<float>& variance, const DeviceMemory<float>& y,\n+    Stream* stream, const DeviceAddress<float>& y_backprop,\n+    const DeviceAddress<float>& x, const DeviceAddress<float>& scale,\n+    const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+    const DeviceAddress<float>& variance, const DeviceAddress<float>& y,\n     const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-    dnn::ActivationMode activation_mode, DeviceMemory<float>* x_backprop,\n-    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-    DeviceMemory<float>* side_input_backprop,\n-    DeviceMemory<uint8_t>* reserve_space_data,\n+    dnn::ActivationMode activation_mode, DeviceAddress<float>* x_backprop,\n+    DeviceAddress<float>* scale_backprop, DeviceAddress<float>* offset_backprop,\n+    DeviceAddress<float>* side_input_backprop,\n+    DeviceAddress<uint8_t>* reserve_space_data,\n     ScratchAllocator* workspace_allocator) {\n   return DoBatchNormalizationBackwardImpl<float, float>(\n              stream, miopenFloat, miopenFloat, y_backprop, x, scale, mean,\n@@ -4070,12 +4076,12 @@ bool MIOpenSupport::DoBatchNormalizationBackward(\n template <class T, class U>\n absl::Status MIOpenSupport::DoBatchNormalizationBackwardImpl(\n     Stream* stream, int miopen_input_type, int miopen_scale_type,\n-    const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n-    const DeviceMemory<U>& scale, const DeviceMemory<U>& mean,\n-    const DeviceMemory<U>& variance, const dnn::BatchDescriptor& x_desc,\n+    const DeviceAddress<T>& y_backprop, const DeviceAddress<T>& x,\n+    const DeviceAddress<U>& scale, const DeviceAddress<U>& mean,\n+    const DeviceAddress<U>& variance, const dnn::BatchDescriptor& x_desc,\n     const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-    DeviceMemory<T>* x_backprop, DeviceMemory<U>* scale_backprop,\n-    DeviceMemory<U>* offset_backprop) {\n+    DeviceAddress<T>* x_backprop, DeviceAddress<U>* scale_backprop,\n+    DeviceAddress<U>* offset_backprop) {\n   auto miopen = miopen_->GetHandle(parent_, stream);\n   TF_ASSIGN_OR_RETURN(\n       auto x_descriptor,\n@@ -4112,9 +4118,9 @@ void launchInplaceBiasActivation(hipStream_t stream, void* c_data,\n \n class ROCmFusedMatmulRunner : public dnn::FusedMatmulRunner {\n   template <typename T>\n-  absl::Status gemm(Stream*, DeviceMemoryBase /* a_data */,\n-                    DeviceMemoryBase /* b_data */,\n-                    DeviceMemoryBase /* c_data */) const;\n+  absl::Status gemm(Stream*, DeviceAddressBase /* a_data */,\n+                    DeviceAddressBase /* b_data */,\n+                    DeviceAddressBase /* c_data */) const;\n \n   Stream* _stream;\n   dnn::DataType _input_type, _bias_type, _output_type;\n@@ -4130,11 +4136,11 @@ class ROCmFusedMatmulRunner : public dnn::FusedMatmulRunner {\n   absl::StatusOr<AlgorithmDesc> ToAlgorithmDesc() const override;\n   // Launch the operation, with the signature determined by `Sig`.\n   absl::Status operator()(Stream*, dnn::ProfileResult*,\n-                          DeviceMemoryBase scratch_memory,\n-                          DeviceMemoryBase /* a_data */,\n-                          DeviceMemoryBase /* b_data */,\n-                          DeviceMemoryBase /* bias_data */,\n-                          DeviceMemoryBase /* c_data */) const override;\n+                          DeviceAddressBase scratch_memory,\n+                          DeviceAddressBase /* a_data */,\n+                          DeviceAddressBase /* b_data */,\n+                          DeviceAddressBase /* bias_data */,\n+                          DeviceAddressBase /* c_data */) const override;\n \n   ROCmFusedMatmulRunner(Stream* stream, dnn::DataType input_type,\n                         dnn::DataType bias_type, dnn::DataType output_type,\n@@ -4184,9 +4190,9 @@ std::string ROCmFusedMatmulRunner::ToString() const {\n \n template <typename T>\n absl::Status ROCmFusedMatmulRunner::gemm(Stream* stream,\n-                                         DeviceMemoryBase a_data,\n-                                         DeviceMemoryBase b_data,\n-                                         DeviceMemoryBase c_data) const {\n+                                         DeviceAddressBase a_data,\n+                                         DeviceAddressBase b_data,\n+                                         DeviceAddressBase c_data) const {\n   blas::Transpose ta =\n       _trans_a ? blas::Transpose::kTranspose : blas::Transpose::kNoTranspose;\n   blas::Transpose tb =\n@@ -4197,16 +4203,16 @@ absl::Status ROCmFusedMatmulRunner::gemm(Stream* stream,\n     return absl::InternalError(\"No Blas support for stream\");\n   }\n   return blas->BlasGemm<T, T>(stream, tb, ta, _n, _m, _k,\n-                              static_cast<DeviceMemory<T>>(b_data), _ldb,\n-                              static_cast<DeviceMemory<T>>(a_data), _lda,\n-                              static_cast<DeviceMemory<T>*>(&c_data), _ldc,\n+                              static_cast<DeviceAddress<T>>(b_data), _ldb,\n+                              static_cast<DeviceAddress<T>>(a_data), _lda,\n+                              static_cast<DeviceAddress<T>*>(&c_data), _ldc,\n                               EngineOptions{}, blas::CallContext::kNone);\n }\n \n template <typename T, typename Tbias = T>\n absl::Status InplaceBiasActivation(\n-    Stream* stream, DeviceMemoryBase c_data, DeviceMemoryBase bias_data,\n-    DeviceMemoryBase side_input_data, float side_input_scale,\n+    Stream* stream, DeviceAddressBase c_data, DeviceAddressBase bias_data,\n+    DeviceAddressBase side_input_data, float side_input_scale,\n     dnn::ActivationMode activation_mode, uint64_t batch, uint64_t m, uint64_t n,\n     int64_t ldc, float param, bool transpose = false) {\n   typedef typename std::conditional<\n@@ -4227,17 +4233,17 @@ absl::Status InplaceBiasActivation(\n }\n \n template <typename Ta, typename Tb, typename... Args>\n-absl::Status InplaceBiasActivation(Stream* stream, DeviceMemory<Ta> c_data,\n-                                   DeviceMemory<Tb> bias_data, Args... args) {\n-  return InplaceBiasActivation<Ta, Tb>(stream, DeviceMemoryBase(c_data),\n-                                       DeviceMemoryBase(bias_data), args...);\n+absl::Status InplaceBiasActivation(Stream* stream, DeviceAddress<Ta> c_data,\n+                                   DeviceAddress<Tb> bias_data, Args... args) {\n+  return InplaceBiasActivation<Ta, Tb>(stream, DeviceAddressBase(c_data),\n+                                       DeviceAddressBase(bias_data), args...);\n }\n \n // Launch the operation, with the signature determined by `Sig`.\n absl::Status ROCmFusedMatmulRunner::operator()(\n-    Stream* stream, dnn::ProfileResult* prof, DeviceMemoryBase scratch_memory,\n-    DeviceMemoryBase a_data, DeviceMemoryBase b_data,\n-    DeviceMemoryBase bias_data, DeviceMemoryBase c_data) const {\n+    Stream* stream, dnn::ProfileResult* prof, DeviceAddressBase scratch_memory,\n+    DeviceAddressBase a_data, DeviceAddressBase b_data,\n+    DeviceAddressBase bias_data, DeviceAddressBase c_data) const {\n   absl::Status status;\n   if (_input_type == dnn::DataType::kFloat)\n     status = gemm<float>(stream, a_data, b_data, c_data);\n@@ -4252,7 +4258,7 @@ absl::Status ROCmFusedMatmulRunner::operator()(\n \n   if (!status.ok()) return status;\n \n-  DeviceMemory<uint8_t> side_input;\n+  DeviceAddress<uint8_t> side_input;\n   if (_input_type == dnn::DataType::kFloat)\n     return InplaceBiasActivation<float>(stream, c_data, bias_data, side_input,\n                                         0.0f, _activation_mode, 1, _m, _n, _ldc,\n@@ -4302,15 +4308,15 @@ absl::Status MIOpenSupport::DoFusedConvolve(\n     Stream* stream, dnn::DataType input_type, dnn::DataType side_input_type,\n     dnn::DataType bias_type, dnn::DataType output_type,\n     const dnn::BatchDescriptor& conv_input_descriptor,\n-    DeviceMemoryBase conv_input_data, double conv_input_scale,\n+    DeviceAddressBase conv_input_data, double conv_input_scale,\n     const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data,\n+    DeviceAddressBase filter_data,\n     const dnn::ConvolutionDescriptor& convolution_descriptor,\n-    DeviceMemoryBase side_input_data, double side_input_scale,\n-    const dnn::BatchDescriptor& bias_descriptor, DeviceMemoryBase biases,\n+    DeviceAddressBase side_input_data, double side_input_scale,\n+    const dnn::BatchDescriptor& bias_descriptor, DeviceAddressBase biases,\n     dnn::ActivationMode activation_mode,\n-    const dnn::BatchDescriptor& output_descriptor, DeviceMemoryBase output_data,\n-    ScratchAllocator* scratch_allocator,\n+    const dnn::BatchDescriptor& output_descriptor,\n+    DeviceAddressBase output_data, ScratchAllocator* scratch_allocator,\n     const dnn::AlgorithmConfig& algorithm_config,\n     dnn::ProfileResult* output_profile_result) {\n   return absl::UnimplementedError(\"fused convolve not implemented yet\");\n@@ -4319,10 +4325,10 @@ absl::Status MIOpenSupport::DoFusedConvolve(\n bool MIOpenSupport::DoTransformTensor(Stream* stream,\n                                       const dnn::BatchDescriptor& input_desc,\n                                       dnn::DataType input_type,\n-                                      const DeviceMemoryBase& input_data,\n+                                      const DeviceAddressBase& input_data,\n                                       const dnn::BatchDescriptor& output_desc,\n                                       dnn::DataType output_type, float scale,\n-                                      DeviceMemoryBase* output_data) {\n+                                      DeviceAddressBase* output_data) {\n   // ROCM TODO implement this operation\n   LOG(ERROR) << \"transform tensor not implemented yet\";\n   return false;\n@@ -4331,9 +4337,9 @@ bool MIOpenSupport::DoTransformTensor(Stream* stream,\n absl::Status MIOpenSupport::DoPoolForward(\n     dnn::DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, ScratchAllocator* workspace_allocator) {\n   if (element_type == dnn::DataType::kDouble) {\n     return absl::InvalidArgumentError(\n         \"MIOpen does not support pooling for double type yet\");\n@@ -4431,7 +4437,7 @@ void PoolingWorkspaceCache::insert(\n     const void* p, const dnn::BatchDescriptor& input_dimensions,\n     const dnn::BatchDescriptor& output_dimensions,\n     const dnn::PoolingDescriptor& pooling_dimensions, int _type,\n-    ScopedDeviceMemory<uint8_t>& workspace, size_t wsp_size,\n+    ScopedDeviceAddress<uint8_t>& workspace, size_t wsp_size,\n     hipStream_t hip_stream) {\n   PoolingWorkspaceDescriptor* desc = nullptr;\n   auto it = cache.find(p);\n@@ -4484,10 +4490,10 @@ void PoolingWorkspaceCache::trim(hipStream_t hip_stream) {\n absl::Status MIOpenSupport::DoPoolBackward(\n     dnn::DataType element_type, Stream* stream,\n     const dnn::PoolingDescriptor& pooling_dimensions,\n-    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n-    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n-    DeviceMemoryBase input_diff_data, DeviceMemoryBase output_diff_data,\n-    ScratchAllocator* workspace_allocator) {\n+    const dnn::BatchDescriptor& input_dimensions, DeviceAddressBase input_data,\n+    const dnn::BatchDescriptor& output_dimensions,\n+    DeviceAddressBase output_data, DeviceAddressBase input_diff_data,\n+    DeviceAddressBase output_diff_data, ScratchAllocator* workspace_allocator) {\n   if (element_type == dnn::DataType::kDouble) {\n     return absl::InvalidArgumentError(\n         \"MIOpen does not support pooling for double type yet\");\n@@ -4508,7 +4514,7 @@ absl::Status MIOpenSupport::DoPoolBackward(\n   TF_ASSIGN_OR_RETURN(auto pooling_desc, scope(pooling_dimensions));\n \n   uint8_t* workspace_ptr = nullptr;\n-  DeviceMemory<uint8_t> workspace;\n+  DeviceAddress<uint8_t> workspace;\n   PoolingWorkspaceDescriptor* pdesc = nullptr;\n \n   size_t workspace_size_in_bytes = 0;\n@@ -4540,7 +4546,7 @@ absl::Status MIOpenSupport::DoPoolBackward(\n         return absl::InternalError(\n             \"Failed to allocate backward pooling workspace\");\n       }\n-      DeviceMemory<uint8_t> dest2;  // duplicated dest from forward:\n+      DeviceAddress<uint8_t> dest2;  // duplicated dest from forward:\n       int64_t dest2_size = 0;\n \n       // miopen requires the strides and dims to be ordered as BDYX.\n@@ -4607,7 +4613,7 @@ absl::Status MIOpenSupport::DoPoolBackward(\n bool MIOpenSupport::DoNormalizeWithDimensions(\n     Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n     const dnn::BatchDescriptor& dimensions,\n-    const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n+    const DeviceAddress<float>& input_data, DeviceAddress<float>* output_data) {\n   // Check for unsupported modes.\n   if (normalize_descriptor.wrap_around()) {\n     LOG(ERROR) << \"MIOpen LRN does not support wrap-around mode\";\n@@ -4642,10 +4648,11 @@ bool MIOpenSupport::DoNormalizeWithDimensions(\n \n bool MIOpenSupport::DoNormalizeBackwardWithDimensions(\n     Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n-    const dnn::BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n-    const DeviceMemory<float>& normalized_data,\n-    const DeviceMemory<float>& normalized_variable_gradient,\n-    DeviceMemory<float>* raw_variable_gradient,\n+    const dnn::BatchDescriptor& dimensions,\n+    const DeviceAddress<float>& raw_data,\n+    const DeviceAddress<float>& normalized_data,\n+    const DeviceAddress<float>& normalized_variable_gradient,\n+    DeviceAddress<float>* raw_variable_gradient,\n     ScratchAllocator* workspace_allocator) {\n   // Check for unsupported modes.\n   if (normalize_descriptor.wrap_around()) {\n@@ -4665,7 +4672,7 @@ bool MIOpenSupport::DoNormalizeBackwardWithDimensions(\n   float alpha = 1.0f;\n   float beta = 0.0f;\n \n-  DeviceMemory<uint8_t> workspace;\n+  DeviceAddress<uint8_t> workspace;\n   size_t workspace_size_in_bytes = 0;\n   auto status =\n       wrap::miopenLRNGetWorkSpaceSize(dims.handle(), &workspace_size_in_bytes);\n@@ -4686,7 +4693,7 @@ bool MIOpenSupport::DoNormalizeBackwardWithDimensions(\n     }\n   }\n \n-  DeviceMemory<uint8_t> dest2;  // duplicated dest from forward:\n+  DeviceAddress<uint8_t> dest2;  // duplicated dest from forward:\n   int dest2_size = 0;\n \n   // miopen requires the strides and dims to be ordered as BDYX.\n@@ -4782,12 +4789,12 @@ class RocmFusedConvRunner : public dnn::FusedConvRunner {\n   }\n \n   absl::Status operator()(Stream* stream, dnn::ProfileResult* profile_result,\n-                          DeviceMemoryBase scratch_memory,\n-                          DeviceMemoryBase input_data,\n-                          DeviceMemoryBase filter_data,\n-                          DeviceMemoryBase side_input_data,\n-                          DeviceMemoryBase bias_data,\n-                          DeviceMemoryBase output_data) const override {\n+                          DeviceAddressBase scratch_memory,\n+                          DeviceAddressBase input_data,\n+                          DeviceAddressBase filter_data,\n+                          DeviceAddressBase side_input_data,\n+                          DeviceAddressBase bias_data,\n+                          DeviceAddressBase output_data) const override {\n     VLOG(2) << \"RocmFusedConvRunner()\";\n     if (parent_ != stream->parent()) {\n       return absl::InternalError(\n@@ -4960,9 +4967,9 @@ class RocmFusedConvRunner : public dnn::FusedConvRunner {\n \n   absl::Status execute_unfused(\n       Stream* stream, dnn::ProfileResult* profile_result,\n-      DeviceMemoryBase scratch_memory, DeviceMemoryBase input_data,\n-      DeviceMemoryBase filter_data, DeviceMemoryBase side_input_data,\n-      DeviceMemoryBase bias_data, DeviceMemoryBase output_data) const {\n+      DeviceAddressBase scratch_memory, DeviceAddressBase input_data,\n+      DeviceAddressBase filter_data, DeviceAddressBase side_input_data,\n+      DeviceAddressBase bias_data, DeviceAddressBase output_data) const {\n     auto miopen = miopen_->GetHandle(parent_, stream);\n     auto status = wrap::miopenConvolutionForwardImmediate(\n         miopen.handle(), filter_.handle(), filter_data.opaque(),\n@@ -5005,24 +5012,24 @@ class RocmFusedConvRunner : public dnn::FusedConvRunner {\n     absl::Status biasActStatus;\n     if (input_type_ == dnn::DataType::kFloat &&\n         bias_type_ == dnn::DataType::kFloat)\n-      biasActStatus = inplace_call(DeviceMemory<float>(output_data),\n-                                   DeviceMemory<float>(bias_data));\n+      biasActStatus = inplace_call(DeviceAddress<float>(output_data),\n+                                   DeviceAddress<float>(bias_data));\n     else if (input_type_ == dnn::DataType::kHalf &&\n              bias_type_ == dnn::DataType::kFloat)\n-      biasActStatus = inplace_call(DeviceMemory<Eigen::half>(output_data),\n-                                   DeviceMemory<float>(bias_data));\n+      biasActStatus = inplace_call(DeviceAddress<Eigen::half>(output_data),\n+                                   DeviceAddress<float>(bias_data));\n     else if (input_type_ == dnn::DataType::kHalf &&\n              bias_type_ == dnn::DataType::kHalf)\n-      biasActStatus = inplace_call(DeviceMemory<Eigen::half>(output_data),\n-                                   DeviceMemory<Eigen::half>(bias_data));\n+      biasActStatus = inplace_call(DeviceAddress<Eigen::half>(output_data),\n+                                   DeviceAddress<Eigen::half>(bias_data));\n     else if (input_type_ == dnn::DataType::kBF16 &&\n              bias_type_ == dnn::DataType::kFloat)\n-      biasActStatus = inplace_call(DeviceMemory<Eigen::bfloat16>(output_data),\n-                                   DeviceMemory<float>(bias_data));\n+      biasActStatus = inplace_call(DeviceAddress<Eigen::bfloat16>(output_data),\n+                                   DeviceAddress<float>(bias_data));\n     else if (input_type_ == dnn::DataType::kBF16 &&\n              bias_type_ == dnn::DataType::kBF16)\n-      biasActStatus = inplace_call(DeviceMemory<Eigen::bfloat16>(output_data),\n-                                   DeviceMemory<Eigen::bfloat16>(bias_data));\n+      biasActStatus = inplace_call(DeviceAddress<Eigen::bfloat16>(output_data),\n+                                   DeviceAddress<Eigen::bfloat16>(bias_data));\n     else\n       return absl::InternalError(\"Unsupported data type\");\n "
        },
        {
            "sha": "1922c006b60deb912b1e85bf4214b1b157a74db2",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_dnn.h",
            "status": "modified",
            "additions": 207,
            "deletions": 198,
            "changes": 405,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -29,8 +29,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/types/span.h\"\n #include \"rocm/include/miopen/miopen.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/engine_options.h\"\n #include \"xla/stream_executor/plugin_registry.h\"\n@@ -51,7 +51,7 @@ struct PoolingWorkspaceDescriptor {\n   dnn::PoolingDescriptor op;\n   int dtype;\n   uint64_t timestamp;\n-  ScopedDeviceMemory<uint8_t> workspace;\n+  ScopedDeviceAddress<uint8_t> workspace;\n   size_t workspace_size;\n   bool IsSame(const dnn::BatchDescriptor& input_dimensions,\n               const dnn::BatchDescriptor& output_dimensions,\n@@ -71,7 +71,7 @@ struct PoolingWorkspaceCache {\n   void insert(const void* p, const dnn::BatchDescriptor& input_dimensions,\n               const dnn::BatchDescriptor& output_dimensions,\n               const dnn::PoolingDescriptor& pooling_dimensions, int _type,\n-              ScopedDeviceMemory<uint8_t>& workspace, size_t wsp_size,\n+              ScopedDeviceAddress<uint8_t>& workspace, size_t wsp_size,\n               hipStream_t hip_stream);\n \n  private:\n@@ -106,147 +106,148 @@ class MIOpenSupport : public dnn::DnnSupport {\n \n   bool DoRnnForward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                    const DeviceMemory<Eigen::half>& input_data,\n-                    const DeviceMemory<int>& seq_lengths_data,\n+                    const DeviceAddress<Eigen::half>& input_data,\n+                    const DeviceAddress<int>& seq_lengths_data,\n                     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                    const DeviceMemory<Eigen::half>& input_h_data,\n+                    const DeviceAddress<Eigen::half>& input_h_data,\n                     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                    const DeviceMemory<Eigen::half>& input_c_data,\n-                    const DeviceMemory<Eigen::half>& params,\n+                    const DeviceAddress<Eigen::half>& input_c_data,\n+                    const DeviceAddress<Eigen::half>& params,\n                     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                    DeviceMemory<Eigen::half>* output_data,\n+                    DeviceAddress<Eigen::half>* output_data,\n                     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                    DeviceMemory<Eigen::half>* output_h_data,\n+                    DeviceAddress<Eigen::half>* output_h_data,\n                     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n+                    DeviceAddress<Eigen::half>* output_c_data, bool is_training,\n                     ScratchAllocator* reserve_space_allocator,\n                     ScratchAllocator* workspace_allocator,\n                     dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnForward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                    const DeviceMemory<float>& input_data,\n-                    const DeviceMemory<int>& seq_lengths_data,\n+                    const DeviceAddress<float>& input_data,\n+                    const DeviceAddress<int>& seq_lengths_data,\n                     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                    const DeviceMemory<float>& input_h_data,\n+                    const DeviceAddress<float>& input_h_data,\n                     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                    const DeviceMemory<float>& input_c_data,\n-                    const DeviceMemory<float>& params,\n+                    const DeviceAddress<float>& input_c_data,\n+                    const DeviceAddress<float>& params,\n                     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                    DeviceMemory<float>* output_data,\n+                    DeviceAddress<float>* output_data,\n                     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                    DeviceMemory<float>* output_h_data,\n+                    DeviceAddress<float>* output_h_data,\n                     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                    DeviceMemory<float>* output_c_data, bool is_training,\n+                    DeviceAddress<float>* output_c_data, bool is_training,\n                     ScratchAllocator* reserve_space_allocator,\n                     ScratchAllocator* workspace_allocator,\n                     dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnForward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                     const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                    const DeviceMemory<double>& input_data,\n-                    const DeviceMemory<int>& seq_lengths_data,\n+                    const DeviceAddress<double>& input_data,\n+                    const DeviceAddress<int>& seq_lengths_data,\n                     const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                    const DeviceMemory<double>& input_h_data,\n+                    const DeviceAddress<double>& input_h_data,\n                     const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                    const DeviceMemory<double>& input_c_data,\n-                    const DeviceMemory<double>& params,\n+                    const DeviceAddress<double>& input_c_data,\n+                    const DeviceAddress<double>& params,\n                     const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                    DeviceMemory<double>* output_data,\n+                    DeviceAddress<double>* output_data,\n                     const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                    DeviceMemory<double>* output_h_data,\n+                    DeviceAddress<double>* output_h_data,\n                     const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                    DeviceMemory<double>* output_c_data, bool is_training,\n+                    DeviceAddress<double>* output_c_data, bool is_training,\n                     ScratchAllocator* reserve_space_allocator,\n                     ScratchAllocator* workspace_allocator,\n                     dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnBackward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                      const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                     const DeviceMemory<Eigen::half>& input_data,\n-                     const DeviceMemory<int>& seq_lengths_data,\n+                     const DeviceAddress<Eigen::half>& input_data,\n+                     const DeviceAddress<int>& seq_lengths_data,\n                      const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                     const DeviceMemory<Eigen::half>& input_h_data,\n+                     const DeviceAddress<Eigen::half>& input_h_data,\n                      const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                     const DeviceMemory<Eigen::half>& input_c_data,\n-                     const DeviceMemory<Eigen::half>& params,\n+                     const DeviceAddress<Eigen::half>& input_c_data,\n+                     const DeviceAddress<Eigen::half>& params,\n                      const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                     const DeviceMemory<Eigen::half>& output_data,\n+                     const DeviceAddress<Eigen::half>& output_data,\n                      const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                     const DeviceMemory<Eigen::half>& output_h_data,\n+                     const DeviceAddress<Eigen::half>& output_h_data,\n                      const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                     const DeviceMemory<Eigen::half>& output_c_data,\n-                     const DeviceMemory<Eigen::half>& output_backprop_data,\n-                     const DeviceMemory<Eigen::half>& output_h_backprop_data,\n-                     const DeviceMemory<Eigen::half>& output_c_backprop_data,\n-                     DeviceMemory<Eigen::half>* input_backprop_data,\n-                     DeviceMemory<Eigen::half>* input_h_backprop_data,\n-                     DeviceMemory<Eigen::half>* input_c_backprop_data,\n-                     DeviceMemory<Eigen::half>* params_backprop_data,\n-                     DeviceMemory<uint8_t>* reserve_space_data,\n+                     const DeviceAddress<Eigen::half>& output_c_data,\n+                     const DeviceAddress<Eigen::half>& output_backprop_data,\n+                     const DeviceAddress<Eigen::half>& output_h_backprop_data,\n+                     const DeviceAddress<Eigen::half>& output_c_backprop_data,\n+                     DeviceAddress<Eigen::half>* input_backprop_data,\n+                     DeviceAddress<Eigen::half>* input_h_backprop_data,\n+                     DeviceAddress<Eigen::half>* input_c_backprop_data,\n+                     DeviceAddress<Eigen::half>* params_backprop_data,\n+                     DeviceAddress<uint8_t>* reserve_space_data,\n                      ScratchAllocator* workspace_allocator,\n                      dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnBackward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                      const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                     const DeviceMemory<float>& input_data,\n-                     const DeviceMemory<int>& seq_lengths_data,\n+                     const DeviceAddress<float>& input_data,\n+                     const DeviceAddress<int>& seq_lengths_data,\n                      const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                     const DeviceMemory<float>& input_h_data,\n+                     const DeviceAddress<float>& input_h_data,\n                      const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                     const DeviceMemory<float>& input_c_data,\n-                     const DeviceMemory<float>& params,\n+                     const DeviceAddress<float>& input_c_data,\n+                     const DeviceAddress<float>& params,\n                      const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                     const DeviceMemory<float>& output_data,\n+                     const DeviceAddress<float>& output_data,\n                      const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                     const DeviceMemory<float>& output_h_data,\n+                     const DeviceAddress<float>& output_h_data,\n                      const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                     const DeviceMemory<float>& output_c_data,\n-                     const DeviceMemory<float>& output_backprop_data,\n-                     const DeviceMemory<float>& output_h_backprop_data,\n-                     const DeviceMemory<float>& output_c_backprop_data,\n-                     DeviceMemory<float>* input_backprop_data,\n-                     DeviceMemory<float>* input_h_backprop_data,\n-                     DeviceMemory<float>* input_c_backprop_data,\n-                     DeviceMemory<float>* params_backprop_data,\n-                     DeviceMemory<uint8_t>* reserve_space_data,\n+                     const DeviceAddress<float>& output_c_data,\n+                     const DeviceAddress<float>& output_backprop_data,\n+                     const DeviceAddress<float>& output_h_backprop_data,\n+                     const DeviceAddress<float>& output_c_backprop_data,\n+                     DeviceAddress<float>* input_backprop_data,\n+                     DeviceAddress<float>* input_h_backprop_data,\n+                     DeviceAddress<float>* input_c_backprop_data,\n+                     DeviceAddress<float>* params_backprop_data,\n+                     DeviceAddress<uint8_t>* reserve_space_data,\n                      ScratchAllocator* workspace_allocator,\n                      dnn::ProfileResult* output_profile_result) override;\n \n   bool DoRnnBackward(Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n                      const dnn::RnnSequenceTensorDescriptor& input_desc,\n-                     const DeviceMemory<double>& input_data,\n-                     const DeviceMemory<int>& seq_lengths_data,\n+                     const DeviceAddress<double>& input_data,\n+                     const DeviceAddress<int>& seq_lengths_data,\n                      const dnn::RnnStateTensorDescriptor& input_h_desc,\n-                     const DeviceMemory<double>& input_h_data,\n+                     const DeviceAddress<double>& input_h_data,\n                      const dnn::RnnStateTensorDescriptor& input_c_desc,\n-                     const DeviceMemory<double>& input_c_data,\n-                     const DeviceMemory<double>& params,\n+                     const DeviceAddress<double>& input_c_data,\n+                     const DeviceAddress<double>& params,\n                      const dnn::RnnSequenceTensorDescriptor& output_desc,\n-                     const DeviceMemory<double>& output_data,\n+                     const DeviceAddress<double>& output_data,\n                      const dnn::RnnStateTensorDescriptor& output_h_desc,\n-                     const DeviceMemory<double>& output_h_data,\n+                     const DeviceAddress<double>& output_h_data,\n                      const dnn::RnnStateTensorDescriptor& output_c_desc,\n-                     const DeviceMemory<double>& output_c_data,\n-                     const DeviceMemory<double>& output_backprop_data,\n-                     const DeviceMemory<double>& output_h_backprop_data,\n-                     const DeviceMemory<double>& output_c_backprop_data,\n-                     DeviceMemory<double>* input_backprop_data,\n-                     DeviceMemory<double>* input_h_backprop_data,\n-                     DeviceMemory<double>* input_c_backprop_data,\n-                     DeviceMemory<double>* params_backprop_data,\n-                     DeviceMemory<uint8_t>* reserve_space_data,\n+                     const DeviceAddress<double>& output_c_data,\n+                     const DeviceAddress<double>& output_backprop_data,\n+                     const DeviceAddress<double>& output_h_backprop_data,\n+                     const DeviceAddress<double>& output_c_backprop_data,\n+                     DeviceAddress<double>* input_backprop_data,\n+                     DeviceAddress<double>* input_h_backprop_data,\n+                     DeviceAddress<double>* input_c_backprop_data,\n+                     DeviceAddress<double>* params_backprop_data,\n+                     DeviceAddress<uint8_t>* reserve_space_data,\n                      ScratchAllocator* workspace_allocator,\n                      dnn::ProfileResult* output_profile_result) override;\n \n   absl::Status GetConvolveRunners(\n       dnn::ConvolutionKind kind, dnn::DataType input_type,\n       dnn::DataType output_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+      const dnn::BatchDescriptor& input_descriptor,\n+      DeviceAddressBase input_data,\n       const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n+      DeviceAddressBase filter_data,\n       const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n+      DeviceAddressBase output_data,\n       const dnn::ConvolutionDescriptor& convolution_descriptor,\n       bool use_fallback, ScratchAllocator* scratch_allocator,\n       const EngineOptions& engine_options,\n@@ -277,35 +278,38 @@ class MIOpenSupport : public dnn::DnnSupport {\n   bool GetMIOpenConvolveAlgorithms(\n       dnn::ConvolutionKind kind, dnn::DataType input_type,\n       dnn::DataType output_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+      const dnn::BatchDescriptor& input_descriptor,\n+      DeviceAddressBase input_data,\n       const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n+      DeviceAddressBase filter_data,\n       const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n+      DeviceAddressBase output_data,\n       const dnn::ConvolutionDescriptor& convolution_descriptor,\n       ScratchAllocator* scratch_allocator,\n       std::vector<dnn::ProfileResult>* out_algorithms) override;\n \n   absl::Status GetMIOpenConvolveAlgorithmsImmediateMode(\n       dnn::ConvolutionKind kind, dnn::DataType input_type,\n       dnn::DataType output_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+      const dnn::BatchDescriptor& input_descriptor,\n+      DeviceAddressBase input_data,\n       const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n+      DeviceAddressBase filter_data,\n       const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n+      DeviceAddressBase output_data,\n       const dnn::ConvolutionDescriptor& convolution_descriptor,\n       ScratchAllocator* scratch_allocator,\n       std::vector<dnn::ProfileResult>* out_algorithms);\n \n   absl::Status GetMIOpenConvolveAlgorithmsFindMode(\n       dnn::ConvolutionKind kind, dnn::DataType input_type,\n       dnn::DataType output_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n+      const dnn::BatchDescriptor& input_descriptor,\n+      DeviceAddressBase input_data,\n       const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n+      DeviceAddressBase filter_data,\n       const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n+      DeviceAddressBase output_data,\n       const dnn::ConvolutionDescriptor& convolution_descriptor,\n       ScratchAllocator* scratch_allocator,\n       std::vector<dnn::ProfileResult>* out_algorithms);\n@@ -314,104 +318,108 @@ class MIOpenSupport : public dnn::DnnSupport {\n       std::vector<dnn::AlgorithmDesc>* out_algorithms) override;\n \n   bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<float>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n+      Stream* stream, const DeviceAddress<float>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<float>& side_input,\n+      const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n       const double exponential_average_factor,\n-      dnn::ActivationMode activation_mode, DeviceMemory<float>* y,\n-      DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-      DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+      dnn::ActivationMode activation_mode, DeviceAddress<float>* y,\n+      DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+      DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n       bool is_training, ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<Eigen::half>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<Eigen::half>& side_input,\n+      Stream* stream, const DeviceAddress<Eigen::half>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<Eigen::half>& side_input,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n       const double exponential_average_factor,\n-      dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y,\n-      DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-      DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+      dnn::ActivationMode activation_mode, DeviceAddress<Eigen::half>* y,\n+      DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+      DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n       bool is_training, ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationForward(\n-      Stream* stream, const DeviceMemory<Eigen::bfloat16>& x,\n-      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n-      const DeviceMemory<float>& estimated_mean,\n-      const DeviceMemory<float>& estimated_variance,\n-      const DeviceMemory<Eigen::bfloat16>& side_input,\n+      Stream* stream, const DeviceAddress<Eigen::bfloat16>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& estimated_mean,\n+      const DeviceAddress<float>& estimated_variance,\n+      const DeviceAddress<Eigen::bfloat16>& side_input,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n       const double exponential_average_factor,\n-      dnn::ActivationMode activation_mode, DeviceMemory<Eigen::bfloat16>* y,\n-      DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n-      DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n+      dnn::ActivationMode activation_mode, DeviceAddress<Eigen::bfloat16>* y,\n+      DeviceAddress<float>* batch_mean, DeviceAddress<float>* batch_var,\n+      DeviceAddress<float>* saved_mean, DeviceAddress<float>* saved_inv_var,\n       bool is_training, ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<float>& y_backprop,\n-      const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& variance, const DeviceMemory<float>& y,\n+      Stream* stream, const DeviceAddress<float>& y_backprop,\n+      const DeviceAddress<float>& x, const DeviceAddress<float>& scale,\n+      const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+      const DeviceAddress<float>& variance, const DeviceAddress<float>& y,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-      dnn::ActivationMode activation_mode, DeviceMemory<float>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<float>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      dnn::ActivationMode activation_mode, DeviceAddress<float>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<float>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n-      const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::half>& y,\n+      Stream* stream, const DeviceAddress<Eigen::half>& y_backprop,\n+      const DeviceAddress<Eigen::half>& x, const DeviceAddress<float>& scale,\n+      const DeviceAddress<float>& offset, const DeviceAddress<float>& mean,\n+      const DeviceAddress<float>& inv_var, const DeviceAddress<Eigen::half>& y,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n       dnn::ActivationMode activation_mode,\n-      DeviceMemory<Eigen::half>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<Eigen::half>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      DeviceAddress<Eigen::half>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<Eigen::half>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n   bool DoBatchNormalizationBackward(\n-      Stream* stream, const DeviceMemory<Eigen::bfloat16>& y_backprop,\n-      const DeviceMemory<Eigen::bfloat16>& x, const DeviceMemory<float>& scale,\n-      const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n-      const DeviceMemory<float>& inv_var,\n-      const DeviceMemory<Eigen::bfloat16>& y,\n+      Stream* stream, const DeviceAddress<Eigen::bfloat16>& y_backprop,\n+      const DeviceAddress<Eigen::bfloat16>& x,\n+      const DeviceAddress<float>& scale, const DeviceAddress<float>& offset,\n+      const DeviceAddress<float>& mean, const DeviceAddress<float>& inv_var,\n+      const DeviceAddress<Eigen::bfloat16>& y,\n       const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n       dnn::ActivationMode activation_mode,\n-      DeviceMemory<Eigen::bfloat16>* x_backprop,\n-      DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n-      DeviceMemory<Eigen::bfloat16>* side_input_backprop,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      DeviceAddress<Eigen::bfloat16>* x_backprop,\n+      DeviceAddress<float>* scale_backprop,\n+      DeviceAddress<float>* offset_backprop,\n+      DeviceAddress<Eigen::bfloat16>* side_input_backprop,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n   absl::Status DoFusedConvolve(\n       Stream* stream, dnn::DataType input_type, dnn::DataType side_input_type,\n       dnn::DataType bias_type, dnn::DataType output_type,\n       const dnn::BatchDescriptor& conv_input_descriptor,\n-      DeviceMemoryBase conv_input_data, double conv_input_scale,\n+      DeviceAddressBase conv_input_data, double conv_input_scale,\n       const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n+      DeviceAddressBase filter_data,\n       const dnn::ConvolutionDescriptor& convolution_descriptor,\n-      DeviceMemoryBase side_input_data, double side_input_scale,\n-      const dnn::BatchDescriptor& bias_descriptor, DeviceMemoryBase biases,\n+      DeviceAddressBase side_input_data, double side_input_scale,\n+      const dnn::BatchDescriptor& bias_descriptor, DeviceAddressBase biases,\n       dnn::ActivationMode activation_mode,\n       const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data, ScratchAllocator* scratch_allocator,\n+      DeviceAddressBase output_data, ScratchAllocator* scratch_allocator,\n       const dnn::AlgorithmConfig& algorithm_config,\n       dnn::ProfileResult* output_profile_result) override;\n \n@@ -441,34 +449,34 @@ class MIOpenSupport : public dnn::DnnSupport {\n   absl::Status DoPoolForward(dnn::DataType element_type, Stream* stream,\n                              const dnn::PoolingDescriptor& pooling_dimensions,\n                              const dnn::BatchDescriptor& input_dimensions,\n-                             DeviceMemoryBase input_data,\n+                             DeviceAddressBase input_data,\n                              const dnn::BatchDescriptor& output_dimensions,\n-                             DeviceMemoryBase output_data,\n+                             DeviceAddressBase output_data,\n                              ScratchAllocator* workspace_allocator) override;\n \n   absl::Status DoPoolBackward(dnn::DataType element_type, Stream* stream,\n                               const dnn::PoolingDescriptor& pooling_dimensions,\n                               const dnn::BatchDescriptor& input_dimensions,\n-                              DeviceMemoryBase input_data,\n+                              DeviceAddressBase input_data,\n                               const dnn::BatchDescriptor& output_dimensions,\n-                              DeviceMemoryBase output_data,\n-                              DeviceMemoryBase input_diff_data,\n-                              DeviceMemoryBase output_diff_data,\n+                              DeviceAddressBase output_data,\n+                              DeviceAddressBase input_diff_data,\n+                              DeviceAddressBase output_diff_data,\n                               ScratchAllocator* workspace_allocator) override;\n \n   bool DoNormalizeWithDimensions(\n       Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n       const dnn::BatchDescriptor& dimensions,\n-      const DeviceMemory<float>& input_data,\n-      DeviceMemory<float>* output_data) override;\n+      const DeviceAddress<float>& input_data,\n+      DeviceAddress<float>* output_data) override;\n \n   bool DoNormalizeBackwardWithDimensions(\n       Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n       const dnn::BatchDescriptor& dimensions,\n-      const DeviceMemory<float>& raw_data,\n-      const DeviceMemory<float>& normalized_data,\n-      const DeviceMemory<float>& normalized_variable_gradient,\n-      DeviceMemory<float>* raw_variable_gradient,\n+      const DeviceAddress<float>& raw_data,\n+      const DeviceAddress<float>& normalized_data,\n+      const DeviceAddress<float>& normalized_variable_gradient,\n+      DeviceAddress<float>* raw_variable_gradient,\n       ScratchAllocator* workspace_allocator = nullptr) override;\n \n   // Derives an output batch descriptor from an input batch and convolution\n@@ -481,23 +489,23 @@ class MIOpenSupport : public dnn::DnnSupport {\n \n   bool DoTransformTensor(Stream* stream, const dnn::BatchDescriptor& input_desc,\n                          dnn::DataType input_type,\n-                         const DeviceMemoryBase& input_data,\n+                         const DeviceAddressBase& input_data,\n                          const dnn::BatchDescriptor& output_desc,\n                          dnn::DataType output_type, float scale,\n-                         DeviceMemoryBase* output_data) override;\n+                         DeviceAddressBase* output_data) override;\n \n   StreamExecutor* GetParentExecutor() { return parent_; }\n \n   absl::Status DoCtcLoss(Stream* stream, dnn::DataType element_type,\n                          const dnn::RnnStateTensorDescriptor& probs_desc,\n-                         const DeviceMemoryBase probs_data,\n+                         const DeviceAddressBase probs_data,\n                          absl::Span<const int> labels_data,\n                          absl::Span<const int> labels_lengths_data,\n                          absl::Span<const int> input_lengths_data,\n-                         DeviceMemoryBase costs_data,\n+                         DeviceAddressBase costs_data,\n                          const dnn::RnnStateTensorDescriptor& grads_desc,\n-                         DeviceMemoryBase grads_data,\n-                         DeviceMemory<uint8_t> scratch_memory,\n+                         DeviceAddressBase grads_data,\n+                         DeviceAddress<uint8_t> scratch_memory,\n                          int ctc_loss_algo_id) override;\n \n  private:\n@@ -520,80 +528,81 @@ class MIOpenSupport : public dnn::DnnSupport {\n   template <class T, class U>\n   absl::Status DoBatchNormalizationForwardImpl(\n       Stream* stream, dnn::DataType input_data_type,\n-      dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n-      const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n-      const DeviceMemory<U>& estimated_mean,\n-      const DeviceMemory<U>& estimated_variance,\n-      const DeviceMemory<T>& side_input, const dnn::BatchDescriptor& x_desc,\n+      dnn::DataType scale_data_type, const DeviceAddress<T>& x,\n+      const DeviceAddress<U>& scale, const DeviceAddress<U>& offset,\n+      const DeviceAddress<U>& estimated_mean,\n+      const DeviceAddress<U>& estimated_variance,\n+      const DeviceAddress<T>& side_input, const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n       const double exponential_average_factor,\n-      dnn::ActivationMode activation_mode, DeviceMemory<T>* y,\n-      DeviceMemory<U>* batch_mean, DeviceMemory<U>* batch_var,\n-      DeviceMemory<U>* saved_mean, DeviceMemory<U>* saved_inv_var,\n+      dnn::ActivationMode activation_mode, DeviceAddress<T>* y,\n+      DeviceAddress<U>* batch_mean, DeviceAddress<U>* batch_var,\n+      DeviceAddress<U>* saved_mean, DeviceAddress<U>* saved_inv_var,\n       bool is_training);\n \n   template <class T, class U>\n   absl::Status DoBatchNormalizationBackwardImpl(\n       Stream* stream, int miopen_input_type, int miopen_scale_type,\n-      const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n-      const DeviceMemory<U>& scale, const DeviceMemory<U>& mean,\n-      const DeviceMemory<U>& variance, const dnn::BatchDescriptor& x_desc,\n+      const DeviceAddress<T>& y_backprop, const DeviceAddress<T>& x,\n+      const DeviceAddress<U>& scale, const DeviceAddress<U>& mean,\n+      const DeviceAddress<U>& variance, const dnn::BatchDescriptor& x_desc,\n       const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n-      DeviceMemory<T>* x_backprop, DeviceMemory<U>* scale_backprop,\n-      DeviceMemory<U>* offset_backprop);\n+      DeviceAddress<T>* x_backprop, DeviceAddress<U>* scale_backprop,\n+      DeviceAddress<U>* offset_backprop);\n \n   template <class T>\n   absl::Status DoRnnForwardImpl(\n       Stream* stream, const MIOpenRnnDescriptor& rnn_desc,\n       const MIOpenRnnSequenceTensorDescriptor& input_desc,\n-      const DeviceMemory<T>& input_data,\n+      const DeviceAddress<T>& input_data,\n       const MIOpenRnnStateTensorDescriptor& input_h_desc,\n-      const DeviceMemory<T>& input_h_data,\n+      const DeviceAddress<T>& input_h_data,\n       const MIOpenRnnStateTensorDescriptor& input_c_desc,\n-      const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+      const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n       const MIOpenRnnSequenceTensorDescriptor& output_desc,\n-      DeviceMemory<T>* output_data,\n+      DeviceAddress<T>* output_data,\n       const MIOpenRnnStateTensorDescriptor& output_h_desc,\n-      DeviceMemory<T>* output_h_data,\n+      DeviceAddress<T>* output_h_data,\n       const MIOpenRnnStateTensorDescriptor& output_c_desc,\n-      DeviceMemory<T>* output_c_data, bool is_training,\n+      DeviceAddress<T>* output_c_data, bool is_training,\n       ScratchAllocator* reserve_space_allocator,\n       ScratchAllocator* workspace_allocator,\n       dnn::ProfileResult* output_profile_result);\n   template <class T>\n   absl::Status DoRnnBackwardImpl(\n       Stream* stream, const MIOpenRnnDescriptor& rnn_desc,\n       const MIOpenRnnSequenceTensorDescriptor& input_desc,\n-      const DeviceMemory<T>& input_data,\n+      const DeviceAddress<T>& input_data,\n       const MIOpenRnnStateTensorDescriptor& input_h_desc,\n-      const DeviceMemory<T>& input_h_data,\n+      const DeviceAddress<T>& input_h_data,\n       const MIOpenRnnStateTensorDescriptor& input_c_desc,\n-      const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n+      const DeviceAddress<T>& input_c_data, const DeviceAddress<T>& params,\n       const MIOpenRnnSequenceTensorDescriptor& output_desc,\n-      const DeviceMemory<T>& output_data,\n+      const DeviceAddress<T>& output_data,\n       const MIOpenRnnStateTensorDescriptor& output_h_desc,\n-      const DeviceMemory<T>& output_h_data,\n+      const DeviceAddress<T>& output_h_data,\n       const MIOpenRnnStateTensorDescriptor& output_c_desc,\n-      const DeviceMemory<T>& output_c_data,\n-      const DeviceMemory<T>& output_backprop_data,\n-      const DeviceMemory<T>& output_h_backprop_data,\n-      const DeviceMemory<T>& output_c_backprop_data,\n-      DeviceMemory<T>* input_backprop_data,\n-      DeviceMemory<T>* input_h_backprop_data,\n-      DeviceMemory<T>* input_c_backprop_data,\n-      DeviceMemory<T>* params_backprop_data,\n-      DeviceMemory<uint8_t>* reserve_space_data,\n+      const DeviceAddress<T>& output_c_data,\n+      const DeviceAddress<T>& output_backprop_data,\n+      const DeviceAddress<T>& output_h_backprop_data,\n+      const DeviceAddress<T>& output_c_backprop_data,\n+      DeviceAddress<T>* input_backprop_data,\n+      DeviceAddress<T>* input_h_backprop_data,\n+      DeviceAddress<T>* input_c_backprop_data,\n+      DeviceAddress<T>* params_backprop_data,\n+      DeviceAddress<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator,\n       dnn::ProfileResult* output_profile_result);\n \n   absl::Status DoCtcLossImpl(\n       Stream* stream, const MIOpenRnnStateTensorDescriptor& probs_desc,\n-      const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n+      const DeviceAddressBase probs_data, absl::Span<const int> labels_data,\n       absl::Span<const int> labels_lengths_data,\n-      absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n+      absl::Span<const int> input_lengths_data, DeviceAddressBase costs_data,\n       const MIOpenRnnStateTensorDescriptor& grads_desc,\n-      DeviceMemoryBase grads_data, const MIOpenCTCLossDescriptor& ctc_loss_desc,\n-      DeviceMemory<uint8_t> scratch_memory, int ctc_loss_algo_id);\n+      DeviceAddressBase grads_data,\n+      const MIOpenCTCLossDescriptor& ctc_loss_desc,\n+      DeviceAddress<uint8_t> scratch_memory, int ctc_loss_algo_id);\n \n   absl::Status DoPrepareForCtcLoss(\n       Stream* stream, dnn::DataType element_type,\n@@ -603,7 +612,7 @@ class MIOpenSupport : public dnn::DnnSupport {\n       absl::Span<const int> labels_lengths_data,\n       absl::Span<const int> input_lengths_data,\n       const EngineOptions& engine_options, ScratchAllocator* scratch_allocator,\n-      DeviceMemory<uint8_t>* scratch_memory, int* ctc_loss_algo_id) override;\n+      DeviceAddress<uint8_t>* scratch_memory, int* ctc_loss_algo_id) override;\n \n   MIOpenSupport(const MIOpenSupport&) = delete;\n   void operator=(const MIOpenSupport&) = delete;"
        },
        {
            "sha": "316028d7b4109f06344c1de344582d88793f658a",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -50,8 +50,8 @@ limitations under the License.\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -105,12 +105,12 @@ namespace {\n // N.B. we must lose constness in order to pass a suitable type to the existing\n // librocm APIs, so the caller should take care to only pass the result of const\n // GPU memory conversions to librocm functions which will honor constness.\n-hipDeviceptr_t AsROCmDevicePtr(const DeviceMemoryBase& gpu_mem) {\n+hipDeviceptr_t AsROCmDevicePtr(const DeviceAddressBase& gpu_mem) {\n   return const_cast<hipDeviceptr_t>(gpu_mem.opaque());\n }\n \n // See description on const version above.\n-hipDeviceptr_t AsROCmDevicePtr(DeviceMemoryBase* gpu_mem) {\n+hipDeviceptr_t AsROCmDevicePtr(DeviceAddressBase* gpu_mem) {\n   return AsROCmDevicePtr(*gpu_mem);\n }\n \n@@ -530,14 +530,14 @@ bool RocmExecutor::UnloadModule(ModuleHandle module_handle) {\n   return UnloadGpuBinary(module_handle);\n }\n \n-absl::StatusOr<DeviceMemoryBase> RocmExecutor::GetMemoryRange(\n-    const DeviceMemoryBase& location) const {\n+absl::StatusOr<DeviceAddressBase> RocmExecutor::GetMemoryRange(\n+    const DeviceAddressBase& location) const {\n   hipDeviceptr_t device_pointer;\n   size_t size;\n   hipError_t result = wrap::hipMemGetAddressRange(\n       &device_pointer, &size, const_cast<void*>(location.opaque()));\n   if (result == hipSuccess) {\n-    return DeviceMemoryBase(device_pointer, size);\n+    return DeviceAddressBase(device_pointer, size);\n   } else if (result == hipErrorNotFound) {\n     // We differentiate between \"this pointer is unknown\" (return here) and\n     // \"there was an internal error while performing this operation\" (return\n@@ -552,7 +552,7 @@ absl::StatusOr<DeviceMemoryBase> RocmExecutor::GetMemoryRange(\n                       location.opaque(), ToString(result).c_str()));\n }\n \n-absl::StatusOr<std::shared_ptr<DeviceMemoryBase>>\n+absl::StatusOr<std::shared_ptr<DeviceAddressBase>>\n RocmExecutor::CreateOrShareConstant(Stream* stream,\n                                     absl::Span<const uint8_t> content) {\n   absl::MutexLock lock{shared_constants_mu_};\n@@ -564,10 +564,10 @@ RocmExecutor::CreateOrShareConstant(Stream* stream,\n       reinterpret_cast<const char*>(content.data()), content.size()));\n   // Must insert nullptr first to get an iterator to the insertion point.\n   auto insert_result = shared_constants_.insert(\n-      {fingerprint, std::weak_ptr<DeviceMemoryBase>()});\n+      {fingerprint, std::weak_ptr<DeviceAddressBase>()});\n   auto it = insert_result.first;\n   bool was_already_in_cache = !insert_result.second;\n-  std::shared_ptr<DeviceMemoryBase> shared_constant;\n+  std::shared_ptr<DeviceAddressBase> shared_constant;\n \n   if (was_already_in_cache) {\n     shared_constant = it->second.lock();\n@@ -576,8 +576,8 @@ RocmExecutor::CreateOrShareConstant(Stream* stream,\n   if (shared_constant == nullptr) {\n     // Either the constant wasn't found in the cache, or it was but its\n     // weak_ptr had expired.\n-    DeviceMemoryBase* new_constant =\n-        new DeviceMemoryBase(Allocate(content.size(), /*memory_space=*/0));\n+    DeviceAddressBase* new_constant =\n+        new DeviceAddressBase(Allocate(content.size(), /*memory_space=*/0));\n     if (new_constant->opaque() == nullptr) {\n       return absl::InternalError(absl::StrFormat(\n           \"Failed to allocate %d bytes for new constant\", content.size()));\n@@ -595,12 +595,12 @@ RocmExecutor::CreateOrShareConstant(Stream* stream,\n \n     // Capturing 'this' in the custom deleter means this executor must\n     // outlive all shared uses of this constant.\n-    shared_constant = std::shared_ptr<DeviceMemoryBase>(\n-        new_constant, [this](DeviceMemoryBase* p) {\n+    shared_constant = std::shared_ptr<DeviceAddressBase>(\n+        new_constant, [this](DeviceAddressBase* p) {\n           Deallocate(p);\n           delete p;\n         });\n-    it->second = std::weak_ptr<DeviceMemoryBase>(shared_constant);\n+    it->second = std::weak_ptr<DeviceAddressBase>(shared_constant);\n   }\n \n   return shared_constant;\n@@ -733,7 +733,7 @@ absl::StatusOr<std::unique_ptr<Kernel>> RocmExecutor::LoadKernel(\n     rocm_kernel->set_args_packing([packing_spec](const Kernel& kernel,\n                                                  const KernelArgs& args) {\n       const auto& mem_args =\n-          stream_executor::Cast<stream_executor::KernelArgsDeviceMemoryArray>(\n+          stream_executor::Cast<stream_executor::KernelArgsDeviceAddressArray>(\n               &args);\n       return packing_spec.BuildArguments(mem_args->device_memory_args(),\n                                          args.number_of_shared_bytes());\n@@ -778,25 +778,25 @@ absl::StatusOr<ModuleHandle> RocmExecutor::LoadModuleFromHsaco(\n   return module_handle;\n }\n \n-DeviceMemoryBase RocmExecutor::Allocate(uint64_t size, int64_t memory_space) {\n+DeviceAddressBase RocmExecutor::Allocate(uint64_t size, int64_t memory_space) {\n   switch (static_cast<MemoryType>(memory_space)) {\n     case MemoryType::kCollective:\n     case MemoryType::kDevice:\n-      return DeviceMemoryBase(\n+      return DeviceAddressBase(\n           DeviceAllocate(rocm_context_, size, /*is_fine_grained*/ false), size);\n     case MemoryType::kP2P:\n       // On the ROCm platform, differences in cache design (e.g., coherence\n       // protocol) can cause cache coherence issues for some archs (e.g., MI200)\n       // when using normal device memory. To avoid these problems, we use\n       // fine-grained memory in P2P communication for all archs to make sure of\n       // the correctness.\n-      return DeviceMemoryBase(\n+      return DeviceAddressBase(\n           DeviceAllocate(rocm_context_, size, /*is_fine_grained*/ true), size);\n     case MemoryType::kHost:\n       if (auto result = HostAllocate(rocm_context_, size); result.ok()) {\n-        return DeviceMemoryBase(*result, size);\n+        return DeviceAddressBase(*result, size);\n       }\n-      return DeviceMemoryBase(nullptr, 0);\n+      return DeviceAddressBase(nullptr, 0);\n     default:\n       LOG(FATAL) << \"Unsupported memory space: \" << memory_space;\n   }\n@@ -806,7 +806,7 @@ RocmExecutor::HostMemoryAllocate(uint64_t size) {\n   return AllocateHostMemory(rocm_context_, size);\n }\n \n-void RocmExecutor::Deallocate(DeviceMemoryBase* mem) {\n+void RocmExecutor::Deallocate(DeviceAddressBase* mem) {\n   DeviceDeallocate(rocm_context_, mem->opaque());\n }\n \n@@ -911,7 +911,7 @@ bool RocmExecutor::HostMemoryUnregister(void* location) {\n   return true;\n }\n \n-absl::Status RocmExecutor::SynchronousMemZero(DeviceMemoryBase* location,\n+absl::Status RocmExecutor::SynchronousMemZero(DeviceAddressBase* location,\n                                               uint64_t size) {\n   std::unique_ptr<ActivateContext> activation = Activate();\n   hipDeviceptr_t rocm_location = AsROCmDevicePtr(location);\n@@ -925,7 +925,7 @@ absl::Status RocmExecutor::SynchronousMemZero(DeviceMemoryBase* location,\n                   \"Failed to memset memory\");\n }\n \n-absl::Status RocmExecutor::SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n+absl::Status RocmExecutor::SynchronousMemcpy(DeviceAddressBase* gpu_dst,\n                                              const void* host_src,\n                                              uint64_t size) {\n   std::unique_ptr<ActivateContext> activation = Activate();\n@@ -941,7 +941,7 @@ absl::Status RocmExecutor::SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n }\n \n absl::Status RocmExecutor::SynchronousMemcpy(void* host_dst,\n-                                             const DeviceMemoryBase& gpu_src,\n+                                             const DeviceAddressBase& gpu_src,\n                                              uint64_t size) {\n   std::unique_ptr<ActivateContext> activation = Activate();\n   TF_RETURN_IF_ERROR(ToStatus(\n@@ -1036,7 +1036,7 @@ bool RocmExecutor::DeviceMemoryUsage(int64_t* free, int64_t* total) const {\n   return rocm_context_->GetDeviceMemoryUsage(free, total);\n }\n \n-absl::StatusOr<DeviceMemoryBase> RocmExecutor::GetSymbol(\n+absl::StatusOr<DeviceAddressBase> RocmExecutor::GetSymbol(\n     const std::string& symbol_name, ModuleHandle module_handle) {\n   void* mem = nullptr;\n   size_t bytes = 0;\n@@ -1048,14 +1048,14 @@ absl::StatusOr<DeviceMemoryBase> RocmExecutor::GetSymbol(\n     TF_RETURN_IF_ERROR(\n         GetModuleSymbol(rocm_context_, it->second.first, symbol_name.c_str(),\n                         reinterpret_cast<hipDeviceptr_t*>(&mem), &bytes));\n-    return DeviceMemoryBase(mem, bytes);\n+    return DeviceAddressBase(mem, bytes);\n   }\n \n   for (auto& it : gpu_binary_to_module_) {\n     TF_RETURN_IF_ERROR(\n         GetModuleSymbol(rocm_context_, it.second.first, symbol_name.c_str(),\n                         reinterpret_cast<hipDeviceptr_t*>(&mem), &bytes));\n-    return DeviceMemoryBase(mem, bytes);\n+    return DeviceAddressBase(mem, bytes);\n   }\n \n   LOG(INFO) << \"Falied to find symbol in any modules: \" << symbol_name;"
        },
        {
            "sha": "71bec6a2376f8ca1d83aee37ad961e7cbe574bd1",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.h",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -36,8 +36,8 @@ limitations under the License.\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -80,23 +80,23 @@ class RocmExecutor : public GpuExecutor {\n   absl::StatusOr<ModuleHandle> LoadModule(\n       const MultiModuleLoaderSpec& spec) override;\n   bool UnloadModule(ModuleHandle module_handle) override;\n-  absl::StatusOr<std::shared_ptr<DeviceMemoryBase>> CreateOrShareConstant(\n+  absl::StatusOr<std::shared_ptr<DeviceAddressBase>> CreateOrShareConstant(\n       Stream* stream, absl::Span<const uint8_t> content) override;\n-  DeviceMemoryBase Allocate(uint64_t size, int64_t memory_space) override;\n-  absl::StatusOr<DeviceMemoryBase> GetMemoryRange(\n-      const DeviceMemoryBase& location) const override;\n-  void Deallocate(DeviceMemoryBase* mem) override;\n+  DeviceAddressBase Allocate(uint64_t size, int64_t memory_space) override;\n+  absl::StatusOr<DeviceAddressBase> GetMemoryRange(\n+      const DeviceAddressBase& location) const override;\n+  void Deallocate(DeviceAddressBase* mem) override;\n   bool SynchronizeAllActivity() override;\n   absl::StatusOr<std::unique_ptr<EventBasedTimer>> CreateEventBasedTimer(\n       Stream* stream, bool use_delay_kernel) override;\n-  absl::StatusOr<DeviceMemoryBase> GetSymbol(\n+  absl::StatusOr<DeviceAddressBase> GetSymbol(\n       const std::string& symbol_name, ModuleHandle module_handle) override;\n-  absl::Status SynchronousMemZero(DeviceMemoryBase* location,\n+  absl::Status SynchronousMemZero(DeviceAddressBase* location,\n                                   uint64_t size) override;\n-  absl::Status SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n+  absl::Status SynchronousMemcpy(DeviceAddressBase* gpu_dst,\n                                  const void* host_src, uint64_t size) override;\n   absl::Status SynchronousMemcpy(void* host_dst,\n-                                 const DeviceMemoryBase& gpu_src,\n+                                 const DeviceAddressBase& gpu_src,\n                                  uint64_t size) override;\n   void DeallocateStream(Stream* stream) override;\n   absl::Status EnablePeerAccessTo(StreamExecutor* other) override;\n@@ -159,7 +159,7 @@ class RocmExecutor : public GpuExecutor {\n   // On-device constants that can be shared between multiple executables. A\n   // pointer for a given constant will expire when no executables require use\n   // of that constant anymore.\n-  std::map<const absl::uint128, std::weak_ptr<DeviceMemoryBase>>\n+  std::map<const absl::uint128, std::weak_ptr<DeviceAddressBase>>\n       shared_constants_ ABSL_GUARDED_BY(shared_constants_mu_);\n \n   // Kernel -> loaded GPU binary. Many kernels may load the same binary."
        },
        {
            "sha": "bb05dfef804154f437e82048d3d83263482d2a34",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_fft.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_fft.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_fft.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_fft.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"rocm/include/hipfft/hipfft.h\"\n #include \"xla/stream_executor/activate_context.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/fft.h\"\n #include \"xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"xla/stream_executor/platform/initialize.h\"\n@@ -409,9 +409,9 @@ void ROCMFft::UpdatePlanWithScratchAllocator(\n }\n \n template <typename FuncT, typename InputT, typename OutputT>\n-bool ROCMFft::DoFftInternal(Stream *stream, fft::Plan *plan, FuncT hipfftExec,\n-                            const DeviceMemory<InputT> &input,\n-                            DeviceMemory<OutputT> *output) {\n+bool ROCMFft::DoFftInternal(Stream* stream, fft::Plan* plan, FuncT hipfftExec,\n+                            const DeviceAddress<InputT>& input,\n+                            DeviceAddress<OutputT>* output) {\n   ROCMFftPlan *rocm_fft_plan = dynamic_cast<ROCMFftPlan *>(plan);\n   if (rocm_fft_plan == nullptr) {\n     LOG(ERROR) << \"the passed-in plan is not a ROCMFftPlan object.\";\n@@ -431,14 +431,14 @@ bool ROCMFft::DoFftInternal(Stream *stream, fft::Plan *plan, FuncT hipfftExec,\n   // see ROCm TF issue # 1150\n   //\n   // Hence for all those transforms, copy the input buffer\n-  DeviceMemory<InputT> input_maybe_copy = input;\n+  DeviceAddress<InputT> input_maybe_copy = input;\n   if (input.opaque() != output->opaque() && (input.size() > 0)) {\n     auto *allocator = rocm_fft_plan->GetScratchAllocator();\n     if (allocator) {\n       auto allocated = allocator->AllocateBytes(input.size());\n       if (allocated.ok()) {\n         if (stream->Memcpy(&allocated.value(), input, input.size()).ok()) {\n-          input_maybe_copy = DeviceMemory<InputT>(allocated.value());\n+          input_maybe_copy = DeviceAddress<InputT>(allocated.value());\n         } else {\n           LOG(ERROR) << \"failed to copy input buffer for rocFFT.\";\n         }\n@@ -459,10 +459,10 @@ bool ROCMFft::DoFftInternal(Stream *stream, fft::Plan *plan, FuncT hipfftExec,\n }\n \n template <typename FuncT, typename InputT, typename OutputT>\n-bool ROCMFft::DoFftWithDirectionInternal(Stream *stream, fft::Plan *plan,\n+bool ROCMFft::DoFftWithDirectionInternal(Stream* stream, fft::Plan* plan,\n                                          FuncT hipfftExec,\n-                                         const DeviceMemory<InputT> &input,\n-                                         DeviceMemory<OutputT> *output) {\n+                                         const DeviceAddress<InputT>& input,\n+                                         DeviceAddress<OutputT>* output) {\n   ROCMFftPlan *rocm_fft_plan = dynamic_cast<ROCMFftPlan *>(plan);\n   if (rocm_fft_plan == nullptr) {\n     LOG(ERROR) << \"the passed-in plan is not a ROCMFftPlan object.\";\n@@ -488,21 +488,21 @@ bool ROCMFft::DoFftWithDirectionInternal(Stream *stream, fft::Plan *plan,\n \n #define STREAM_EXECUTOR_ROCM_DEFINE_FFT(__type, __fft_type1, __fft_type2,    \\\n                                         __fft_type3)                         \\\n-  bool ROCMFft::DoFft(Stream *stream, fft::Plan *plan,                       \\\n-                      const DeviceMemory<std::complex<__type>> &input,       \\\n-                      DeviceMemory<std::complex<__type>> *output) {          \\\n+  bool ROCMFft::DoFft(Stream* stream, fft::Plan* plan,                       \\\n+                      const DeviceAddress<std::complex<__type>>& input,      \\\n+                      DeviceAddress<std::complex<__type>>* output) {         \\\n     return DoFftWithDirectionInternal(                                       \\\n         stream, plan, wrap::hipfftExec##__fft_type1, input, output);         \\\n   }                                                                          \\\n-  bool ROCMFft::DoFft(Stream *stream, fft::Plan *plan,                       \\\n-                      const DeviceMemory<__type> &input,                     \\\n-                      DeviceMemory<std::complex<__type>> *output) {          \\\n+  bool ROCMFft::DoFft(Stream* stream, fft::Plan* plan,                       \\\n+                      const DeviceAddress<__type>& input,                    \\\n+                      DeviceAddress<std::complex<__type>>* output) {         \\\n     return DoFftInternal(stream, plan, wrap::hipfftExec##__fft_type2, input, \\\n                          output);                                            \\\n   }                                                                          \\\n-  bool ROCMFft::DoFft(Stream *stream, fft::Plan *plan,                       \\\n-                      const DeviceMemory<std::complex<__type>> &input,       \\\n-                      DeviceMemory<__type> *output) {                        \\\n+  bool ROCMFft::DoFft(Stream* stream, fft::Plan* plan,                       \\\n+                      const DeviceAddress<std::complex<__type>>& input,      \\\n+                      DeviceAddress<__type>* output) {                       \\\n     return DoFftInternal(stream, plan, wrap::hipfftExec##__fft_type3, input, \\\n                          output);                                            \\\n   }"
        },
        {
            "sha": "3174ebaa18a63478dd37122ff46a9bf26308e573",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_fft.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_fft.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_fft.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_fft.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -91,7 +91,7 @@ class ROCMFftPlan : public fft::Plan {\n   StreamExecutor *parent_;\n   hipfftHandle plan_;\n   fft::Type fft_type_;\n-  DeviceMemory<uint8_t> scratch_;\n+  DeviceAddress<uint8_t> scratch_;\n   size_t scratch_size_bytes_;\n   bool is_initialized_;\n };\n@@ -121,17 +121,17 @@ class ROCMFft : public fft::FftSupport {\n \n   // This is for complex to complex FFT, when the direction is required.\n   template <typename FuncT, typename InputT, typename OutputT>\n-  bool DoFftWithDirectionInternal(Stream *stream, fft::Plan *plan,\n+  bool DoFftWithDirectionInternal(Stream* stream, fft::Plan* plan,\n                                   FuncT hipfft_exec,\n-                                  const DeviceMemory<InputT> &input,\n-                                  DeviceMemory<OutputT> *output);\n+                                  const DeviceAddress<InputT>& input,\n+                                  DeviceAddress<OutputT>* output);\n \n   // This is for complex to real or real to complex FFT, when the direction\n   // is implied.\n   template <typename FuncT, typename InputT, typename OutputT>\n-  bool DoFftInternal(Stream *stream, fft::Plan *plan, FuncT hipfft_exec,\n-                     const DeviceMemory<InputT> &input,\n-                     DeviceMemory<OutputT> *output);\n+  bool DoFftInternal(Stream* stream, fft::Plan* plan, FuncT hipfft_exec,\n+                     const DeviceAddress<InputT>& input,\n+                     DeviceAddress<OutputT>* output);\n \n   ROCMFft(const ROCMFft &) = delete;\n   void operator=(const ROCMFft &) = delete;"
        },
        {
            "sha": "565d9f4ff547f64df198bb38140d0fc4cc5be24f",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_kernel.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_kernel.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -110,7 +110,7 @@ absl::Status RocmKernel::Launch(const ThreadDim& thread_dims,\n   }\n \n   // For device memory array we rely on a custom kernel arguments packing.\n-  if (auto* device_mem = DynCast<KernelArgsDeviceMemoryArray>(&args)) {\n+  if (auto* device_mem = DynCast<KernelArgsDeviceAddressArray>(&args)) {\n     auto& pack = args_packing();\n     if (!pack) {\n       return absl::InternalError("
        },
        {
            "sha": "b99bbcf84cc42ba4e3ab9cc756ed89cebb0cba0b",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_solver_context.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_solver_context.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_solver_context.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_solver_context.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"rocm/include/hiprand/hiprand.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu_solver_context.h\"\n #include \"xla/stream_executor/platform/platform_object_registry.h\"\n #include \"xla/stream_executor/rocm/rocm_platform_id.h\"\n@@ -94,7 +94,7 @@ struct GpuComplexT<std::complex<double>*> {\n #endif  // TF_ROCM_VERSION >= 40500\n \n template <typename T>\n-inline typename GpuComplexT<T>::type* ToDevicePointer(DeviceMemory<T> p) {\n+inline typename GpuComplexT<T>::type* ToDevicePointer(DeviceAddress<T> p) {\n   return static_cast<typename GpuComplexT<T>::type*>(p.opaque());\n }\n \n@@ -309,8 +309,8 @@ absl::StatusOr<int64_t> RocmSolverContext::PotrfBufferSize(\n }\n \n absl::Status RocmSolverContext::PotrfBatched(blas::UpperLower uplo, int n,\n-                                             DeviceMemory<float*> as, int lda,\n-                                             DeviceMemory<int> lapack_info,\n+                                             DeviceAddress<float*> as, int lda,\n+                                             DeviceAddress<int> lapack_info,\n                                              int batch_size) {\n   return ConvertStatus(GpuSolverSpotrfBatched(\n       handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n@@ -321,8 +321,8 @@ absl::Status RocmSolverContext::PotrfBatched(blas::UpperLower uplo, int n,\n }\n \n absl::Status RocmSolverContext::PotrfBatched(blas::UpperLower uplo, int n,\n-                                             DeviceMemory<double*> as, int lda,\n-                                             DeviceMemory<int> lapack_info,\n+                                             DeviceAddress<double*> as, int lda,\n+                                             DeviceAddress<int> lapack_info,\n                                              int batch_size) {\n   return ConvertStatus(GpuSolverDpotrfBatched(\n       handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n@@ -333,8 +333,8 @@ absl::Status RocmSolverContext::PotrfBatched(blas::UpperLower uplo, int n,\n }\n \n absl::Status RocmSolverContext::PotrfBatched(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<float>*> as,\n-    int lda, DeviceMemory<int> lapack_info, int batch_size) {\n+    blas::UpperLower uplo, int n, DeviceAddress<std::complex<float>*> as,\n+    int lda, DeviceAddress<int> lapack_info, int batch_size) {\n   return ConvertStatus(GpuSolverCpotrfBatched(\n       handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n #if TENSORFLOW_USE_HIPSOLVER\n@@ -344,8 +344,8 @@ absl::Status RocmSolverContext::PotrfBatched(\n }\n \n absl::Status RocmSolverContext::PotrfBatched(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<double>*> as,\n-    int lda, DeviceMemory<int> lapack_info, int batch_size) {\n+    blas::UpperLower uplo, int n, DeviceAddress<std::complex<double>*> as,\n+    int lda, DeviceAddress<int> lapack_info, int batch_size) {\n   return ConvertStatus(GpuSolverZpotrfBatched(\n       handle_, GpuBlasUpperLower(uplo), n, ToDevicePointer(as), lda,\n #if TENSORFLOW_USE_HIPSOLVER\n@@ -356,36 +356,36 @@ absl::Status RocmSolverContext::PotrfBatched(\n \n #if TENSORFLOW_USE_HIPSOLVER\n absl::Status RocmSolverContext::Potrf(blas::UpperLower uplo, int n,\n-                                      DeviceMemory<double> a, int lda,\n-                                      DeviceMemory<int> lapack_info,\n-                                      DeviceMemory<double> workspace) {\n+                                      DeviceAddress<double> a, int lda,\n+                                      DeviceAddress<int> lapack_info,\n+                                      DeviceAddress<double> workspace) {\n   return ConvertStatus(GpuSolverDpotrf(handle_, GpuBlasUpperLower(uplo), n,\n                                        ToDevicePointer(a), lda, nullptr, 0,\n                                        ToDevicePointer(lapack_info)));\n }\n \n absl::Status RocmSolverContext::Potrf(blas::UpperLower uplo, int n,\n-                                      DeviceMemory<float> a, int lda,\n-                                      DeviceMemory<int> lapack_info,\n-                                      DeviceMemory<float> workspace) {\n+                                      DeviceAddress<float> a, int lda,\n+                                      DeviceAddress<int> lapack_info,\n+                                      DeviceAddress<float> workspace) {\n   return ConvertStatus(GpuSolverSpotrf(handle_, GpuBlasUpperLower(uplo), n,\n                                        ToDevicePointer(a), lda, nullptr, 0,\n                                        ToDevicePointer(lapack_info)));\n }\n \n absl::Status RocmSolverContext::Potrf(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<float>> a, int lda,\n-    DeviceMemory<int> lapack_info,\n-    DeviceMemory<std::complex<float>> workspace) {\n+    blas::UpperLower uplo, int n, DeviceAddress<std::complex<float>> a, int lda,\n+    DeviceAddress<int> lapack_info,\n+    DeviceAddress<std::complex<float>> workspace) {\n   return ConvertStatus(GpuSolverCpotrf(handle_, GpuBlasUpperLower(uplo), n,\n                                        ToDevicePointer(a), lda, nullptr, 0,\n                                        ToDevicePointer(lapack_info)));\n }\n \n absl::Status RocmSolverContext::Potrf(\n-    blas::UpperLower uplo, int n, DeviceMemory<std::complex<double>> a, int lda,\n-    DeviceMemory<int> lapack_info,\n-    DeviceMemory<std::complex<double>> workspace) {\n+    blas::UpperLower uplo, int n, DeviceAddress<std::complex<double>> a,\n+    int lda, DeviceAddress<int> lapack_info,\n+    DeviceAddress<std::complex<double>> workspace) {\n   return ConvertStatus(GpuSolverZpotrf(handle_, GpuBlasUpperLower(uplo), n,\n                                        ToDevicePointer(a), lda, nullptr, 0,\n                                        ToDevicePointer(lapack_info)));"
        },
        {
            "sha": "313806bfb724f89159ebe5326b93bc3cd3fbb486",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_solver_context.h",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_solver_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_solver_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_solver_context.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -38,7 +38,7 @@ using gpusolverHandle_t = rocblas_handle;\n #endif  // TF_ROCM_VERSION >= 40500\n \n #include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu_solver_context.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -52,35 +52,35 @@ class RocmSolverContext : public GpuSolverContext {\n \n   absl::Status SetStream(Stream* stream) override;\n   absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<float*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n+                            DeviceAddress<float*> as, int lda,\n+                            DeviceAddress<int> lapack_info,\n                             int batch_size) override;\n   absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<double*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n+                            DeviceAddress<double*> as, int lda,\n+                            DeviceAddress<int> lapack_info,\n                             int batch_size) override;\n   absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<std::complex<float>*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n+                            DeviceAddress<std::complex<float>*> as, int lda,\n+                            DeviceAddress<int> lapack_info,\n                             int batch_size) override;\n   absl::Status PotrfBatched(blas::UpperLower uplo, int n,\n-                            DeviceMemory<std::complex<double>*> as, int lda,\n-                            DeviceMemory<int> lapack_info,\n+                            DeviceAddress<std::complex<double>*> as, int lda,\n+                            DeviceAddress<int> lapack_info,\n                             int batch_size) override;\n-  absl::Status Potrf(blas::UpperLower uplo, int n, DeviceMemory<float> a,\n-                     int lda, DeviceMemory<int> lapack_info,\n-                     DeviceMemory<float> workspace) override;\n-  absl::Status Potrf(blas::UpperLower uplo, int n, DeviceMemory<double> a,\n-                     int lda, DeviceMemory<int> lapack_info,\n-                     DeviceMemory<double> workspace) override;\n+  absl::Status Potrf(blas::UpperLower uplo, int n, DeviceAddress<float> a,\n+                     int lda, DeviceAddress<int> lapack_info,\n+                     DeviceAddress<float> workspace) override;\n+  absl::Status Potrf(blas::UpperLower uplo, int n, DeviceAddress<double> a,\n+                     int lda, DeviceAddress<int> lapack_info,\n+                     DeviceAddress<double> workspace) override;\n   absl::Status Potrf(blas::UpperLower uplo, int n,\n-                     DeviceMemory<std::complex<float>> a, int lda,\n-                     DeviceMemory<int> lapack_info,\n-                     DeviceMemory<std::complex<float>> workspace) override;\n+                     DeviceAddress<std::complex<float>> a, int lda,\n+                     DeviceAddress<int> lapack_info,\n+                     DeviceAddress<std::complex<float>> workspace) override;\n   absl::Status Potrf(blas::UpperLower uplo, int n,\n-                     DeviceMemory<std::complex<double>> a, int lda,\n-                     DeviceMemory<int> lapack_info,\n-                     DeviceMemory<std::complex<double>> workspace) override;\n+                     DeviceAddress<std::complex<double>> a, int lda,\n+                     DeviceAddress<int> lapack_info,\n+                     DeviceAddress<std::complex<double>> workspace) override;\n   absl::StatusOr<int64_t> PotrfBufferSize(xla::PrimitiveType type,\n                                           blas::UpperLower uplo, int n, int lda,\n                                           int batch_size) override;"
        },
        {
            "sha": "a8428c7fb3cb60a23f98050d246c04a38c91d717",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_stream.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -35,7 +35,7 @@ limitations under the License.\n #include \"rocm/include/hip/hip_runtime.h\"\n #include \"rocm/rocm_config.h\"\n #include \"xla/stream_executor/activate_context.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -238,7 +238,7 @@ RocmStream::~RocmStream() {\n   DestroyStream(executor_, stream_handle_);\n }\n \n-absl::Status RocmStream::Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+absl::Status RocmStream::Memset32(DeviceAddressBase* location, uint32_t pattern,\n                                   uint64_t size) {\n   if (absl::bit_cast<uintptr_t>(location->opaque()) % alignof(uint32_t) != 0) {\n     return absl::InvalidArgumentError(\"location must be 4 byte aligned.\");\n@@ -251,7 +251,7 @@ absl::Status RocmStream::Memset32(DeviceMemoryBase* location, uint32_t pattern,\n                   \"Failed to memset memory\");\n }\n \n-absl::Status RocmStream::MemZero(DeviceMemoryBase* location, uint64_t size) {\n+absl::Status RocmStream::MemZero(DeviceAddressBase* location, uint64_t size) {\n   if (absl::bit_cast<uintptr_t>(location->opaque()) % alignof(uint32_t) == 0 &&\n       size % sizeof(uint32_t) == 0) {\n     return Memset32(location, 0x0, size);\n@@ -263,22 +263,23 @@ absl::Status RocmStream::MemZero(DeviceMemoryBase* location, uint64_t size) {\n   }\n }\n \n-absl::Status RocmStream::Memcpy(DeviceMemoryBase* gpu_dst,\n-                                const DeviceMemoryBase& gpu_src,\n+absl::Status RocmStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   return AsynchronousMemcpyD2D(\n       executor_, absl::bit_cast<hipDeviceptr_t>(gpu_dst->opaque()),\n       absl::bit_cast<hipDeviceptr_t>(gpu_src.opaque()), size, stream_handle_);\n }\n \n-absl::Status RocmStream::Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n-                                uint64_t size) {\n+absl::Status RocmStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const void* host_src, uint64_t size) {\n   return AsynchronousMemcpyH2D(\n       executor_, absl::bit_cast<hipDeviceptr_t>(gpu_dst->opaque()), host_src,\n       size, stream_handle_);\n }\n \n-absl::Status RocmStream::Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+absl::Status RocmStream::Memcpy(void* host_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   return AsynchronousMemcpyD2H(executor_, host_dst,\n                                absl::bit_cast<hipDeviceptr_t>(gpu_src.opaque()),"
        },
        {
            "sha": "a16a3bb53055598ccbd4c5ea01a938f5125ae396",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_stream.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/status.h\"\n #include \"rocm/include/hip/hip_runtime.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/kernel.h\"\n@@ -43,15 +43,15 @@ class RocmStream : public StreamCommon {\n   absl::Status RecordEvent(Event* event) override;\n   absl::Status WaitFor(Event* event) override;\n \n-  absl::Status Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+  absl::Status Memset32(DeviceAddressBase* location, uint32_t pattern,\n                         uint64_t size) override;\n-  absl::Status MemZero(DeviceMemoryBase* location, uint64_t size) override;\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n+  absl::Status MemZero(DeviceAddressBase* location, uint64_t size) override;\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst, const void* host_src,\n                       uint64_t size) override;\n-  absl::Status Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+  absl::Status Memcpy(void* host_dst, const DeviceAddressBase& gpu_src,\n                       uint64_t size) override;\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst,\n-                      const DeviceMemoryBase& gpu_src, uint64_t size) override;\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst,\n+                      const DeviceAddressBase& gpu_src, uint64_t size) override;\n   absl::Status DoHostCallbackWithStatus(\n       absl::AnyInvocable<absl::Status() &&> callback) override;\n   absl::Status BlockHostUntilDone() override;"
        },
        {
            "sha": "0a6b29a5a7055c3814bbb3a040c46776d76874a7",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_stream_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_stream_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -29,7 +29,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -65,7 +65,7 @@ class RocmStreamTest : public ::testing::Test {\n \n TEST_F(RocmStreamTest, Memset32) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer =\n+  DeviceAddress<uint32_t> buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<RocmStream> stream,\n@@ -78,7 +78,7 @@ TEST_F(RocmStreamTest, Memset32) {\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n \n   // Should fail due to the non-4-byte-aligned pointer.\n-  DeviceMemoryBase unaligned_pointer =\n+  DeviceAddressBase unaligned_pointer =\n       buffer.GetByteSlice(/*offset_bytes=*/1, /*size_bytes=*/0);\n   EXPECT_THAT(stream->Memset32(&unaligned_pointer, 0xDEADBEEF,\n                                kBufferNumElements * sizeof(uint32_t) + 1),\n@@ -99,7 +99,7 @@ TEST_F(RocmStreamTest, Memset32) {\n \n TEST_F(RocmStreamTest, MemZero) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer =\n+  DeviceAddress<uint32_t> buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<RocmStream> stream,\n@@ -132,7 +132,7 @@ TEST_F(RocmStreamTest, MemZero) {\n \n TEST_F(RocmStreamTest, MemcpyHostToDeviceAndBack) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer =\n+  DeviceAddress<uint32_t> buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<RocmStream> stream,\n@@ -156,9 +156,9 @@ TEST_F(RocmStreamTest, MemcpyHostToDeviceAndBack) {\n \n TEST_F(RocmStreamTest, MemcpyDeviceToDevice) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> buffer1 =\n+  DeviceAddress<uint32_t> buffer1 =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n-  DeviceMemory<uint32_t> buffer2 =\n+  DeviceAddress<uint32_t> buffer2 =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<RocmStream> stream,\n@@ -206,9 +206,9 @@ TEST_F(RocmStreamTest, LaunchKernel) {\n   constexpr int64_t kByteLength = sizeof(int32_t) * kLength;\n \n   // Prepare arguments: a=1, b=2, c=0\n-  DeviceMemory<int32_t> a = executor_->AllocateArray<int32_t>(kLength, 0);\n-  DeviceMemory<int32_t> b = executor_->AllocateArray<int32_t>(kLength, 0);\n-  DeviceMemory<int32_t> c = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> a = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> b = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> c = executor_->AllocateArray<int32_t>(kLength, 0);\n \n   EXPECT_THAT(stream->Memset32(&a, 1, kByteLength), absl_testing::IsOk());\n   EXPECT_THAT(stream->Memset32(&b, 2, kByteLength), absl_testing::IsOk());"
        },
        {
            "sha": "ea35fdb0bf7d307448d685ed3464ab24ad234c2a",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_timer_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_timer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_timer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_timer_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/time/time.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/gpu/gpu_test_kernels.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -49,9 +49,9 @@ class RocmTimerTest : public ::testing::Test {\n     int64_t byte_length = sizeof(int32_t) * length;\n \n     // Prepare arguments: a=1, b=2, c=0\n-    DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n-    DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n-    DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+    DeviceAddress<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n \n     ASSERT_THAT(stream->Memset32(&a, 1, byte_length), absl_testing::IsOk());\n     ASSERT_THAT(stream->Memset32(&b, 2, byte_length), absl_testing::IsOk());"
        },
        {
            "sha": "0c50a4686dc1dc365215851723855f2731589f2c",
            "filename": "third_party/xla/xla/stream_executor/scratch_allocator.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fscratch_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fscratch_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fscratch_allocator.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n \n #include \"absl/container/inlined_vector.h\"\n #include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"tsl/platform/statusor.h\"\n \n namespace stream_executor {\n@@ -50,7 +50,7 @@ class ScratchAllocator {\n   //\n   // This is a temporary allocation, and the caller is responsible for\n   // deallocating at some known-safe point. See the class comment above.\n-  virtual absl::StatusOr<DeviceMemory<uint8_t>> AllocateBytes(\n+  virtual absl::StatusOr<DeviceAddress<uint8_t>> AllocateBytes(\n       int64_t byte_size) = 0;\n };\n \n@@ -62,17 +62,17 @@ class ScratchAllocator {\n template <size_t N = 1>\n class OwningScratchAllocator : public ScratchAllocator {\n  public:\n-  OwningScratchAllocator(int device_ordinal, DeviceMemoryAllocator* allocator)\n+  OwningScratchAllocator(int device_ordinal, DeviceAddressAllocator* allocator)\n       : device_ordinal_(device_ordinal), allocator_(allocator) {}\n \n   OwningScratchAllocator(OwningScratchAllocator&&) = default;\n   OwningScratchAllocator& operator=(OwningScratchAllocator&&) = default;\n \n   int64_t GetMemoryLimitInBytes() override { return -1; }\n \n-  absl::StatusOr<DeviceMemory<uint8_t>> AllocateBytes(\n+  absl::StatusOr<DeviceAddress<uint8_t>> AllocateBytes(\n       int64_t byte_size) override {\n-    TF_ASSIGN_OR_RETURN(OwningDeviceMemory buffer,\n+    TF_ASSIGN_OR_RETURN(OwningDeviceAddress buffer,\n                         allocator_->Allocate(device_ordinal_, byte_size,\n                                              /*retry_on_failure=*/false));\n     buffers_.push_back(std::move(buffer));\n@@ -81,8 +81,8 @@ class OwningScratchAllocator : public ScratchAllocator {\n \n  private:\n   int device_ordinal_;\n-  DeviceMemoryAllocator* allocator_;\n-  absl::InlinedVector<OwningDeviceMemory, N> buffers_;\n+  DeviceAddressAllocator* allocator_;\n+  absl::InlinedVector<OwningDeviceAddress, N> buffers_;\n };\n \n }  // namespace stream_executor"
        },
        {
            "sha": "cf596ee79e3b348040a95c7126f3d2bae91b5156",
            "filename": "third_party/xla/xla/stream_executor/stream.h",
            "status": "modified",
            "additions": 14,
            "deletions": 11,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -38,14 +38,17 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/tsl/lib/gtl/int_type.h\"\n \n+// TODO(ezhulenev): Remove this once transitive dependencies are fixed.\n+#include \"xla/stream_executor/device_memory.h\"\n+\n namespace stream_executor {\n \n class StreamExecutor;\n@@ -143,20 +146,20 @@ class Stream {\n   // Entrain onto the stream: a memcpy to a host destination from a GPU source\n   // of the given target size. host_dst must be a pointer to host memory\n   // allocated by StreamExecutor::HostMemoryAllocate.\n-  virtual absl::Status Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+  virtual absl::Status Memcpy(void* host_dst, const DeviceAddressBase& gpu_src,\n                               uint64_t size) = 0;\n \n   // Entrain onto the stream: a memcpy to a GPU destination from a host source\n   // of the given target size. host_src must be a pointer to host memory\n   // allocated by StreamExecutor::HostMemoryAllocate.\n-  virtual absl::Status Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n+  virtual absl::Status Memcpy(DeviceAddressBase* gpu_dst, const void* host_src,\n                               uint64_t size) = 0;\n \n   // Alternative interface for memcpying from device to host that takes an\n   // array slice. Checks that the destination size can accommodate the host\n   // slice size.\n   template <typename T>\n-  absl::Status MemcpyD2H(const DeviceMemory<T>& gpu_src,\n+  absl::Status MemcpyD2H(const DeviceAddress<T>& gpu_src,\n                          absl::Span<T> host_dst) {\n     auto host_size = host_dst.size() * sizeof(T);\n     if (gpu_src.size() == 0 || host_size >= gpu_src.size()) {\n@@ -170,7 +173,7 @@ class Stream {\n   // slice size.\n   template <typename T>\n   absl::Status MemcpyH2D(absl::Span<const T> host_src,\n-                         DeviceMemory<T>* gpu_dst) {\n+                         DeviceAddress<T>* gpu_dst) {\n     auto host_size = host_src.size() * sizeof(T);\n     if (gpu_dst->size() == 0 || gpu_dst->size() >= host_size) {\n       return Memcpy(gpu_dst, host_src.begin(), host_size);\n@@ -181,28 +184,28 @@ class Stream {\n   // Entrain onto the stream: a memcpy to a GPU destination from a GPU source\n   // of the given target size. gpu_src/dst must be pointers to GPU memory and\n   // peer access must be enabled between their owning StreamExecutors.\n-  virtual absl::Status Memcpy(DeviceMemoryBase* gpu_dst,\n-                              const DeviceMemoryBase& gpu_src, uint64_t size) {\n+  virtual absl::Status Memcpy(DeviceAddressBase* gpu_dst,\n+                              const DeviceAddressBase& gpu_src, uint64_t size) {\n     return absl::UnimplementedError(\n         \"Memcpy from device to device is not implemented for this \"\n         \"stream.\");\n   }\n \n-  absl::Status MemcpyD2D(DeviceMemoryBase* gpu_dst,\n-                         const DeviceMemoryBase& gpu_src, uint64_t size) {\n+  absl::Status MemcpyD2D(DeviceAddressBase* gpu_dst,\n+                         const DeviceAddressBase& gpu_src, uint64_t size) {\n     return Memcpy(gpu_dst, gpu_src, size);\n   }\n \n   // Entrain onto the stream: a memset of zero at a device location of size\n   // bytes. The location must not be null.\n-  virtual absl::Status MemZero(DeviceMemoryBase* location, uint64_t size) {\n+  virtual absl::Status MemZero(DeviceAddressBase* location, uint64_t size) {\n     return absl::UnimplementedError(\"MemZero is not supported on this stream.\");\n   }\n \n   // Entrain onto the stream: a memset of a 32-bit pattern at device location of\n   // size bytes, where bytes must be evenly 32-bit sized (i.e. evenly divisible\n   // by 4). The location must not be null.\n-  virtual absl::Status Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+  virtual absl::Status Memset32(DeviceAddressBase* location, uint32_t pattern,\n                                 uint64_t size) {\n     return absl::UnimplementedError(\n         \"Memset32 is not supported on this stream.\");"
        },
        {
            "sha": "8b43c676ea7014af90aa8d5ba6b0ba7b0137e545",
            "filename": "third_party/xla/xla/stream_executor/stream_executor.h",
            "status": "modified",
            "additions": 29,
            "deletions": 26,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -35,8 +35,8 @@ limitations under the License.\n #include \"xla/stream_executor/allocator_stats.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n@@ -51,6 +51,9 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/lib/gtl/int_type.h\"\n \n+// TODO(ezhulenev): Remove this once transitive dependencies are fixed.\n+#include \"xla/stream_executor/device_memory.h\"\n+\n namespace stream_executor {\n \n // Identifies the memory space where an allocation resides.\n@@ -117,13 +120,13 @@ class StreamExecutor {\n   // Synchronously allocates an array on the device of type T with element_count\n   // elements.\n   template <typename T>\n-  DeviceMemory<T> AllocateArray(uint64_t element_count,\n-                                int64_t memory_space = 0);\n+  DeviceAddress<T> AllocateArray(uint64_t element_count,\n+                                 int64_t memory_space = 0);\n \n   // Convenience wrapper that allocates space for a single element of type T in\n   // device memory.\n   template <typename T>\n-  DeviceMemory<T> AllocateScalar() {\n+  DeviceAddress<T> AllocateScalar() {\n     return AllocateArray<T>(1);\n   }\n \n@@ -153,21 +156,21 @@ class StreamExecutor {\n   }\n \n   // Creates a shared constant using the content provided.\n-  virtual absl::StatusOr<std::shared_ptr<DeviceMemoryBase>>\n+  virtual absl::StatusOr<std::shared_ptr<DeviceAddressBase>>\n   CreateOrShareConstant(Stream* stream, absl::Span<const uint8_t> content) {\n     return absl::UnimplementedError(\"Not Implemented\");\n   }\n \n   // Synchronously allocates size bytes on the underlying platform and returns\n-  // a DeviceMemoryBase representing that allocation. In the case of failure,\n+  // a DeviceAddressBase representing that allocation. In the case of failure,\n   // nullptr is returned.\n-  virtual DeviceMemoryBase Allocate(uint64_t size, int64_t memory_space) = 0;\n-  DeviceMemoryBase Allocate(uint64_t size) {\n+  virtual DeviceAddressBase Allocate(uint64_t size, int64_t memory_space) = 0;\n+  DeviceAddressBase Allocate(uint64_t size) {\n     return Allocate(size, /*memory_space=*/0);\n   }\n-  // Deallocates the DeviceMemory previously allocated via this interface.\n+  // Deallocates the DeviceAddress previously allocated via this interface.\n   // Deallocation of a nullptr-representative value is permitted.\n-  virtual void Deallocate(DeviceMemoryBase* mem) = 0;\n+  virtual void Deallocate(DeviceAddressBase* mem) = 0;\n \n   // Allocates a region of host memory and registers it with the platform API.\n   // Memory allocated in this manner is required for use in asynchronous memcpy\n@@ -185,14 +188,14 @@ class StreamExecutor {\n \n   // Blocks the caller while \"size\" bytes are zeroed out (in POD fashion) at the\n   // given location in device memory.\n-  virtual absl::Status SynchronousMemZero(DeviceMemoryBase* location,\n+  virtual absl::Status SynchronousMemZero(DeviceAddressBase* location,\n                                           uint64_t size) = 0;\n \n-  // Returns a DeviceMemoryBase representing the range [base, base + size)\n-  // for the given DeviceMemoryBase, such that location is contained within the\n+  // Returns a DeviceAddressBase representing the range [base, base + size)\n+  // for the given DeviceAddressBase, such that location is contained within the\n   // returned range.\n-  virtual absl::StatusOr<DeviceMemoryBase> GetMemoryRange(\n-      const DeviceMemoryBase& location) const {\n+  virtual absl::StatusOr<DeviceAddressBase> GetMemoryRange(\n+      const DeviceAddressBase& location) const {\n     return absl::UnimplementedError(\"Not implemented for this executor.\");\n   }\n \n@@ -203,20 +206,20 @@ class StreamExecutor {\n \n   // Blocks the caller while \"size\" bytes are copied to the given location in\n   // device memory.\n-  virtual absl::Status SynchronousMemcpy(DeviceMemoryBase* device_dst,\n+  virtual absl::Status SynchronousMemcpy(DeviceAddressBase* device_dst,\n                                          const void* host_src,\n                                          uint64_t size) = 0;\n   absl::Status SynchronousMemcpyH2D(const void* host_src, int64_t size,\n-                                    DeviceMemoryBase* device_dst) {\n+                                    DeviceAddressBase* device_dst) {\n     return SynchronousMemcpy(device_dst, host_src, size);\n   }\n \n   // Blocks the caller while \"size\" bytes are copied to the given location\n   // in host memory.\n   virtual absl::Status SynchronousMemcpy(void* host_dst,\n-                                         const DeviceMemoryBase& device_src,\n+                                         const DeviceAddressBase& device_src,\n                                          uint64_t size) = 0;\n-  absl::Status SynchronousMemcpyD2H(const DeviceMemoryBase& device_src,\n+  absl::Status SynchronousMemcpyD2H(const DeviceAddressBase& device_src,\n                                     int64_t size, void* host_dst) {\n     return SynchronousMemcpy(host_dst, device_src, size);\n   }\n@@ -241,13 +244,13 @@ class StreamExecutor {\n   }\n \n   // Retrieves device pointer and size for a symbol. To use\n-  // constant memory in CUDA, GetSymbol has to be used. Returns DeviceMemoryBase\n-  // describing the symbol in memory if symbol is found.\n+  // constant memory in CUDA, GetSymbol has to be used. Returns\n+  // DeviceAddressBase describing the symbol in memory if symbol is found.\n   //\n   // If ModuleHandle is set then we search for `symbol_name` only within the\n   // module corresponding to `module_handle`.  Otherwise all loaded modules are\n   // searched.\n-  virtual absl::StatusOr<DeviceMemoryBase> GetSymbol(\n+  virtual absl::StatusOr<DeviceAddressBase> GetSymbol(\n       const std::string& symbol_name, ModuleHandle module_handle) {\n     return absl::UnimplementedError(\"Not implemented\");\n   }\n@@ -399,8 +402,8 @@ class StreamExecutor {\n };\n \n template <typename T>\n-inline DeviceMemory<T> StreamExecutor::AllocateArray(uint64_t element_count,\n-                                                     int64_t memory_space) {\n+inline DeviceAddress<T> StreamExecutor::AllocateArray(uint64_t element_count,\n+                                                      int64_t memory_space) {\n   uint64_t bytes = sizeof(T) * element_count;\n   auto memory_limit_bytes = GetMemoryLimitBytes();\n   if (memory_limit_bytes > 0 &&\n@@ -409,9 +412,9 @@ inline DeviceMemory<T> StreamExecutor::AllocateArray(uint64_t element_count,\n                  << device_ordinal()\n                  << \" within provided limit.  limit=\" << memory_limit_bytes\n                  << \"]\";\n-    return DeviceMemory<T>();\n+    return DeviceAddress<T>();\n   }\n-  return DeviceMemory<T>(Allocate(bytes, memory_space));\n+  return DeviceAddress<T>(Allocate(bytes, memory_space));\n }\n \n }  // namespace stream_executor"
        },
        {
            "sha": "fa5408ada6d3298d0522660063f71da02c6e66b2",
            "filename": "third_party/xla/xla/stream_executor/sycl/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -134,7 +134,7 @@ sycl_library(\n     deps = [\n         \":sycl_context\",\n         \":sycl_event\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:event_based_timer\",\n         \"//xla/stream_executor:stream\",\n@@ -159,7 +159,7 @@ xla_test(\n         \":sycl_stream\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n         \"//xla/service/gpu:gpu_executable\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:typed_kernel_factory\",\n         \"//xla/tests:llvm_irgen_test_base\","
        },
        {
            "sha": "6e21a73c6704a8d9841756274e0ab7ee42185f78",
            "filename": "third_party/xla/xla/stream_executor/sycl/sycl_kernel.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_kernel.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -104,7 +104,7 @@ absl::Status SyclKernel::Launch(const ThreadDim& thread_dims,\n   }\n \n   // For device memory array we rely on a custom kernel arguments packing.\n-  if (auto* device_mem = DynCast<KernelArgsDeviceMemoryArray>(&args)) {\n+  if (auto* device_mem = DynCast<KernelArgsDeviceAddressArray>(&args)) {\n     auto& pack = args_packing();\n     if (!pack) {\n       return absl::InternalError("
        },
        {
            "sha": "c6dd8fefdcb4bbc7c79a1c1dcca277d5b96f5287",
            "filename": "third_party/xla/xla/stream_executor/sycl/sycl_stream.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -141,7 +141,7 @@ absl::Status SyclStream::WaitFor(Event* event) {\n       static_cast<SyclEvent*>(event)->GetEvent());\n }\n \n-absl::Status SyclStream::Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+absl::Status SyclStream::Memset32(DeviceAddressBase* location, uint32_t pattern,\n                                   uint64_t size) {\n   VLOG(2) << \"Enqueuing memset32 operation onto stream \" << stream_handle_.get()\n           << \" at location \" << reinterpret_cast<const void*>(location)\n@@ -163,7 +163,7 @@ absl::Status SyclStream::Memset32(DeviceMemoryBase* location, uint32_t pattern,\n   return absl::OkStatus();\n }\n \n-absl::Status SyclStream::MemZero(DeviceMemoryBase* location, uint64_t size) {\n+absl::Status SyclStream::MemZero(DeviceAddressBase* location, uint64_t size) {\n   if (absl::bit_cast<uintptr_t>(location->opaque()) % alignof(uint32_t) == 0 &&\n       size % sizeof(uint32_t) == 0) {\n     return SyclStream::Memset32(location, 0x0, size);\n@@ -175,8 +175,8 @@ absl::Status SyclStream::MemZero(DeviceMemoryBase* location, uint64_t size) {\n   return absl::OkStatus();\n }\n \n-absl::Status SyclStream::Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n-                                uint64_t size) {\n+absl::Status SyclStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const void* host_src, uint64_t size) {\n   TF_RETURN_IF_ERROR(SyclMemcpyHostToDeviceAsync(\n       stream_handle_.get(), const_cast<void*>(gpu_dst->opaque()), host_src,\n       size));\n@@ -186,7 +186,8 @@ absl::Status SyclStream::Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n   return absl::OkStatus();\n }\n \n-absl::Status SyclStream::Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+absl::Status SyclStream::Memcpy(void* host_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   TF_RETURN_IF_ERROR(\n       SyclMemcpyDeviceToHostAsync(stream_handle_.get(), host_dst,\n@@ -197,8 +198,8 @@ absl::Status SyclStream::Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n   return absl::OkStatus();\n }\n \n-absl::Status SyclStream::Memcpy(DeviceMemoryBase* gpu_dst,\n-                                const DeviceMemoryBase& gpu_src,\n+absl::Status SyclStream::Memcpy(DeviceAddressBase* gpu_dst,\n+                                const DeviceAddressBase& gpu_src,\n                                 uint64_t size) {\n   TF_RETURN_IF_ERROR(SyclMemcpyDeviceToDeviceAsync(\n       stream_handle_.get(), const_cast<void*>(gpu_dst->opaque()),"
        },
        {
            "sha": "87e544dc3709690f12d48a0c0e5f54008ff4b3cd",
            "filename": "third_party/xla/xla/stream_executor/sycl/sycl_stream.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -45,24 +45,24 @@ class SyclStream : public StreamCommon {\n \n   // Enqueues an asynchronous operation to set the specified device memory\n   // region to the given value.\n-  absl::Status Memset32(DeviceMemoryBase* location, uint32_t pattern,\n+  absl::Status Memset32(DeviceAddressBase* location, uint32_t pattern,\n                         uint64_t size) override;\n \n   // Enqueues an asynchronous operation to zero out the specified device memory\n   // region.\n-  absl::Status MemZero(DeviceMemoryBase* location, uint64_t size) override;\n+  absl::Status MemZero(DeviceAddressBase* location, uint64_t size) override;\n \n   // Enqueues an asynchronous copy from host memory to device memory.\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst, const void* host_src,\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst, const void* host_src,\n                       uint64_t size) override;\n \n   // Enqueues an asynchronous copy from device memory to host memory.\n-  absl::Status Memcpy(void* host_dst, const DeviceMemoryBase& gpu_src,\n+  absl::Status Memcpy(void* host_dst, const DeviceAddressBase& gpu_src,\n                       uint64_t size) override;\n \n   // Enqueues an asynchronous copy from one device memory region to another.\n-  absl::Status Memcpy(DeviceMemoryBase* gpu_dst,\n-                      const DeviceMemoryBase& gpu_src, uint64_t size) override;\n+  absl::Status Memcpy(DeviceAddressBase* gpu_dst,\n+                      const DeviceAddressBase& gpu_src, uint64_t size) override;\n \n   // Enqueues a host callback to be executed after all previously enqueued\n   // operations on the current stream have completed."
        },
        {
            "sha": "cb4d745b8c982e6458a4dc05f4b4ad1a98ec4c33",
            "filename": "third_party/xla/xla/stream_executor/sycl/sycl_stream_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_stream_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/service/gpu/gpu_executable.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/sycl/sycl_event.h\"\n #include \"xla/stream_executor/sycl/sycl_platform_id.h\"\n@@ -76,7 +76,7 @@ TEST_F(SyclStreamTest, CreateWithNonDefaultPriority) {\n \n TEST_F(SyclStreamTest, Memset32) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> device_buffer =\n+  DeviceAddress<uint32_t> device_buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<SyclStream> stream,\n@@ -92,7 +92,7 @@ TEST_F(SyclStreamTest, Memset32) {\n       absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n \n   // Should fail due to the non-4-byte-aligned pointer.\n-  DeviceMemoryBase unaligned_device_memory =\n+  DeviceAddressBase unaligned_device_memory =\n       device_buffer.GetByteSlice(/*offset_bytes=*/1, /*size_bytes=*/0);\n   EXPECT_THAT(stream->Memset32(&unaligned_device_memory, 0xDEADBEEF,\n                                kBufferSizeBytes + 1),\n@@ -113,7 +113,7 @@ TEST_F(SyclStreamTest, Memset32) {\n \n TEST_F(SyclStreamTest, MemZero) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> device_buffer =\n+  DeviceAddress<uint32_t> device_buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<SyclStream> stream,\n@@ -149,7 +149,7 @@ TEST_F(SyclStreamTest, MemZero) {\n \n TEST_F(SyclStreamTest, MemcpyHostToDeviceAndBack) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> device_buffer =\n+  DeviceAddress<uint32_t> device_buffer =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<SyclStream> stream,\n@@ -178,9 +178,9 @@ TEST_F(SyclStreamTest, MemcpyHostToDeviceAndBack) {\n \n TEST_F(SyclStreamTest, MemcpyDeviceToDevice) {\n   constexpr int kBufferNumElements = 42;\n-  DeviceMemory<uint32_t> device_buffer1 =\n+  DeviceAddress<uint32_t> device_buffer1 =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n-  DeviceMemory<uint32_t> device_buffer2 =\n+  DeviceAddress<uint32_t> device_buffer2 =\n       executor_->AllocateArray<uint32_t>(kBufferNumElements, 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<SyclStream> stream,\n@@ -227,8 +227,8 @@ TEST_F(SyclStreamTest, LaunchKernel) {\n                                              /*priority=*/std::nullopt));\n \n   using AddKernel =\n-      TypedKernelFactory<DeviceMemory<int32_t>, DeviceMemory<int32_t>,\n-                         DeviceMemory<int32_t>>;\n+      TypedKernelFactory<DeviceAddress<int32_t>, DeviceAddress<int32_t>,\n+                         DeviceAddress<int32_t>>;\n \n   absl::string_view hlo_ir = R\"(\n     ENTRY e {\n@@ -273,9 +273,9 @@ TEST_F(SyclStreamTest, LaunchKernel) {\n   constexpr int64_t kByteLength = sizeof(int32_t) * kLength;\n \n   // Prepare arguments: a=3, b=2, c=0\n-  DeviceMemory<int32_t> a = executor_->AllocateArray<int32_t>(kLength, 0);\n-  DeviceMemory<int32_t> b = executor_->AllocateArray<int32_t>(kLength, 0);\n-  DeviceMemory<int32_t> c = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> a = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> b = executor_->AllocateArray<int32_t>(kLength, 0);\n+  DeviceAddress<int32_t> c = executor_->AllocateArray<int32_t>(kLength, 0);\n \n   EXPECT_THAT(stream->Memset32(&a, 3, kByteLength), absl_testing::IsOk());\n   EXPECT_THAT(stream->Memset32(&b, 2, kByteLength), absl_testing::IsOk());"
        },
        {
            "sha": "676d8175f77f854009c5479c1c7a6f3393f5c067",
            "filename": "third_party/xla/xla/stream_executor/sycl/sycl_timer_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_timer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_timer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fsycl%2Fsycl_timer_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -34,8 +34,8 @@ class SyclTimerTest : public ::testing::Test {\n  public:\n   void LaunchSomeKernel(StreamExecutor* executor, Stream* stream) {\n     using AddKernel =\n-        TypedKernelFactory<DeviceMemory<float>, DeviceMemory<float>,\n-                           DeviceMemory<float>>;\n+        TypedKernelFactory<DeviceAddress<float>, DeviceAddress<float>,\n+                           DeviceAddress<float>>;\n \n     // TODO(intel-tf): This is a temporary workaround to get the test working.\n     // This will be replaced with hlo-based spv binary generation once MLIR\n@@ -196,9 +196,9 @@ class SyclTimerTest : public ::testing::Test {\n     const int64_t kByteLength = sizeof(float) * kLength;\n \n     // Prepare arguments: a=1.0, b=2.0, c=0.0\n-    DeviceMemory<float> a = executor->AllocateArray<float>(kLength, 0);\n-    DeviceMemory<float> b = executor->AllocateArray<float>(kLength, 0);\n-    DeviceMemory<float> c = executor->AllocateArray<float>(kLength, 0);\n+    DeviceAddress<float> a = executor->AllocateArray<float>(kLength, 0);\n+    DeviceAddress<float> b = executor->AllocateArray<float>(kLength, 0);\n+    DeviceAddress<float> c = executor->AllocateArray<float>(kLength, 0);\n \n     ASSERT_THAT(stream->Memset32(&a, 1.0, kByteLength), IsOk());\n     ASSERT_THAT(stream->Memset32(&b, 2.0, kByteLength), IsOk());"
        },
        {
            "sha": "d6181338c395795e0abfebb5a6a56b301d94c99b",
            "filename": "third_party/xla/xla/stream_executor/tensor_map.h",
            "status": "renamed",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftensor_map.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftensor_map.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftensor_map.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -13,19 +13,19 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#ifndef XLA_STREAM_EXECUTOR_GPU_TENSOR_MAP_H_\n-#define XLA_STREAM_EXECUTOR_GPU_TENSOR_MAP_H_\n+#ifndef XLA_STREAM_EXECUTOR_TENSOR_MAP_H_\n+#define XLA_STREAM_EXECUTOR_TENSOR_MAP_H_\n \n #include <cstddef>\n \n-namespace stream_executor::gpu {\n+namespace stream_executor {\n \n // TensorMap is a wrapper around a 128 bytes of storage. It is used to pass TMA\n // descriptors to the kernel.\n struct TensorMap {\n   alignas(64) std::byte storage[128];\n };\n \n-}  // namespace stream_executor::gpu\n+}  // namespace stream_executor\n \n-#endif  // XLA_STREAM_EXECUTOR_GPU_TENSOR_MAP_H_\n+#endif  // XLA_STREAM_EXECUTOR_TENSOR_MAP_H_",
            "previous_filename": "third_party/xla/xla/stream_executor/gpu/tensor_map.h"
        },
        {
            "sha": "c0e4af83f39ddc9d3f15a58a6aad89d7055c513f",
            "filename": "third_party/xla/xla/stream_executor/tpu/BUILD",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2FBUILD?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -74,8 +74,8 @@ cc_library(\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:maybe_owning_device_memory\",\n         \"//xla/service:shaped_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -202,8 +202,8 @@ cc_library(\n         \":tpu_stream_interface\",\n         \":tpu_topology_external\",\n         \"//xla/stream_executor:allocator_stats\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:executor_cache\",\n         \"//xla/stream_executor:memory_allocation\",\n@@ -273,8 +273,8 @@ cc_library(\n         \":tpu_executor_c_api_hdrs\",\n         \":tpu_topology_external\",\n         \"//xla/stream_executor:allocator_stats\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:executor_cache\",\n         \"//xla/stream_executor:memory_allocation\",\n@@ -329,8 +329,8 @@ cc_library(\n         \":tpu_stream_interface\",\n         \":tpu_topology_external\",\n         \"//xla/stream_executor:allocator_stats\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:executor_cache\",\n         \"//xla/stream_executor:memory_allocation\",\n@@ -389,7 +389,7 @@ cc_library(\n         \"//xla/service\",\n         \"//xla/service:backend\",\n         \"//xla/service:stream_pool\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -458,7 +458,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:transfer_manager\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -489,7 +489,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:executable\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n@@ -529,7 +529,7 @@ cc_library(\n     deps = [\n         \":tpu_platform_interface\",\n         \":tpu_topology_external\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_common\",\n         \"@com_google_absl//absl/status\",\n@@ -589,7 +589,7 @@ cc_library(\n     hdrs = [\"tpu_stream_interface.h\"],\n     visibility = [\"//visibility:public\"],\n     deps = [\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream_common\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"@com_google_absl//absl/status\",\n@@ -611,8 +611,8 @@ cc_library(\n         \"//xla/service:maybe_owning_device_memory\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service:transfer_manager\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_address_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:errors\",\n@@ -645,7 +645,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:executable\",\n         \"//xla/service:shaped_buffer\",\n-        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_address\",\n         \"//xla/stream_executor:stream\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:inlined_vector\","
        },
        {
            "sha": "4f06ee508fd8fbf5cadf7d372bd3e90b60cff8d5",
            "filename": "third_party/xla/xla/stream_executor/tpu/c_api_conversions.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 30,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -40,8 +40,8 @@ limitations under the License.\n #include \"xla/shape_layout.h\"\n #include \"xla/shape_tree.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/tpu/c_api_decl.h\"\n #include \"xla/stream_executor/tpu/c_api_defn.h\"  // IWYU pragma: keep\n #include \"xla/stream_executor/tpu/proto_helper.h\"\n@@ -144,7 +144,7 @@ xla::ShapedBuffer FromC(XLA_ShapedBuffer* c_buffer) {\n   xla::Shape xla_on_device_shape =\n       ApiConverter::FromC(&c_buffer->on_device_shape);\n \n-  xla::ShapeTree<stream_executor::DeviceMemoryBase> xla_shape_tree(\n+  xla::ShapeTree<stream_executor::DeviceAddressBase> xla_shape_tree(\n       xla_on_device_shape);\n   size_t i = 0;\n   for (auto& pair : xla_shape_tree) {\n@@ -164,7 +164,7 @@ SE_MaybeOwningDeviceMemory ToC(xla::MaybeOwningDeviceMemory& mem,\n   se_mem.owned = mem.HasOwnership();\n   se_mem.memory = ApiConverter::ToC(mem.AsDeviceMemoryBase());\n   if (mem.HasOwnership()) {\n-    const stream_executor::OwningDeviceMemory* owned =\n+    const stream_executor::OwningDeviceAddress* owned =\n         mem.AsOwningDeviceMemory();\n     se_mem.device_ordinal = owned->device_ordinal();\n     se_mem.allocator = ApiConverter::ToC(owned->allocator());\n@@ -174,27 +174,27 @@ SE_MaybeOwningDeviceMemory ToC(xla::MaybeOwningDeviceMemory& mem,\n     }\n   } else {\n     se_mem.allocator =\n-        ToC(static_cast<stream_executor::DeviceMemoryAllocator*>(nullptr));\n+        ToC(static_cast<stream_executor::DeviceAddressAllocator*>(nullptr));\n     se_mem.device_ordinal = -1;\n   }\n   return se_mem;\n }\n \n xla::MaybeOwningDeviceMemory FromC(\n     SE_MaybeOwningDeviceMemory* se_mem,\n-    stream_executor::DeviceMemoryAllocator* allocator) {\n+    stream_executor::DeviceAddressAllocator* allocator) {\n   if (se_mem->owned) {\n-    return xla::MaybeOwningDeviceMemory(\n-        stream_executor::OwningDeviceMemory(ApiConverter::FromC(se_mem->memory),\n-                                            se_mem->device_ordinal, allocator));\n+    return xla::MaybeOwningDeviceMemory(stream_executor::OwningDeviceAddress(\n+        ApiConverter::FromC(se_mem->memory), se_mem->device_ordinal,\n+        allocator));\n   } else {\n     return xla::MaybeOwningDeviceMemory(ApiConverter::FromC(se_mem->memory));\n   }\n }\n \n-SE_DeviceMemoryAllocator ToC(\n-    stream_executor::DeviceMemoryAllocator* allocator) {\n-  SE_DeviceMemoryAllocator se_allocator;\n+SE_DeviceAddressAllocator ToC(\n+    stream_executor::DeviceAddressAllocator* allocator) {\n+  SE_DeviceAddressAllocator se_allocator;\n   if (allocator == nullptr) {\n     se_allocator.ctx = nullptr;\n     se_allocator.platform = nullptr;\n@@ -207,10 +207,10 @@ SE_DeviceMemoryAllocator ToC(\n   se_allocator.ctx = allocator;\n   se_allocator.allocate = [](void* ctx, int device_ordinal, uint64_t size,\n                              bool retry_on_failure, int64_t memory_space,\n-                             SE_ScopedDeviceMemory* memory,\n+                             SE_ScopedDeviceAddress* memory,\n                              TF_Status* se_status) {\n     auto allocation =\n-        reinterpret_cast<stream_executor::DeviceMemoryAllocator*>(ctx)\n+        reinterpret_cast<stream_executor::DeviceAddressAllocator*>(ctx)\n             ->Allocate(device_ordinal, size, retry_on_failure, memory_space);\n     if (!allocation.ok()) {\n       auto status = allocation.status();\n@@ -224,10 +224,11 @@ SE_DeviceMemoryAllocator ToC(\n     }\n   };\n \n-  se_allocator.deallocate = [](void* ctx, SE_DeviceMemoryBase* base,\n+  se_allocator.deallocate = [](void* ctx, SE_DeviceAddressBase* base,\n                                int device_ordinal, TF_Status* se_status) {\n-    auto status = reinterpret_cast<stream_executor::DeviceMemoryAllocator*>(ctx)\n-                      ->Deallocate(device_ordinal, ApiConverter::FromC(*base));\n+    auto status =\n+        reinterpret_cast<stream_executor::DeviceAddressAllocator*>(ctx)\n+            ->Deallocate(device_ordinal, ApiConverter::FromC(*base));\n     if (!status.ok()) {\n       auto message = status.message();\n       stream_executor::tpu::ExecutorApiFn()->TpuStatus_SetFn(\n@@ -237,13 +238,13 @@ SE_DeviceMemoryAllocator ToC(\n   return se_allocator;\n }\n \n-stream_executor::DeviceMemoryAllocator* FromC(\n-    const SE_DeviceMemoryAllocator& c_allocator) {\n-  return reinterpret_cast<stream_executor::DeviceMemoryAllocator*>(\n+stream_executor::DeviceAddressAllocator* FromC(\n+    const SE_DeviceAddressAllocator& c_allocator) {\n+  return reinterpret_cast<stream_executor::DeviceAddressAllocator*>(\n       c_allocator.ctx);\n }\n \n-SE_MaybeOwningDeviceMemory ToC(stream_executor::OwningDeviceMemory* mem) {\n+SE_MaybeOwningDeviceMemory ToC(stream_executor::OwningDeviceAddress* mem) {\n   SE_MaybeOwningDeviceMemory se_mem;\n   se_mem.device_ordinal = mem->device_ordinal();\n   se_mem.memory = ApiConverter::ToC(mem->Release());\n@@ -252,21 +253,21 @@ SE_MaybeOwningDeviceMemory ToC(stream_executor::OwningDeviceMemory* mem) {\n   return se_mem;\n }\n \n-void ToC(const stream_executor::DeviceMemoryBase& base,\n-         SE_DeviceMemoryBase* se_base) {\n+void ToC(const stream_executor::DeviceAddressBase& base,\n+         SE_DeviceAddressBase* se_base) {\n   se_base->opaque = const_cast<void*>(base.opaque());\n   se_base->payload = base.payload();\n   se_base->size = base.size();\n }\n \n-SE_DeviceMemoryBase ToC(const stream_executor::DeviceMemoryBase& base) {\n-  SE_DeviceMemoryBase se_base;\n+SE_DeviceAddressBase ToC(const stream_executor::DeviceAddressBase& base) {\n+  SE_DeviceAddressBase se_base;\n   ToC(base, &se_base);\n   return se_base;\n }\n \n-stream_executor::DeviceMemoryBase FromC(const SE_DeviceMemoryBase& se_base) {\n-  stream_executor::DeviceMemoryBase base(se_base.opaque, se_base.size);\n+stream_executor::DeviceAddressBase FromC(const SE_DeviceAddressBase& se_base) {\n+  stream_executor::DeviceAddressBase base(se_base.opaque, se_base.size);\n   base.SetPayload(se_base.payload);\n   return base;\n }\n@@ -435,12 +436,12 @@ void ToC(const xla::ShapedBuffer& buffer, XLA_ShapedBuffer* c_device_buffer) {\n   ApiConverter::ToC(buffer.on_device_shape(),\n                     &c_device_buffer->on_device_shape);\n   c_device_buffer->device_ordinal = buffer.device_ordinal();\n-  absl::InlinedVector<SE_DeviceMemoryBase, 2> bases;\n+  absl::InlinedVector<SE_DeviceAddressBase, 2> bases;\n   for (auto& pair : buffer.buffers()) {\n     bases.push_back(ApiConverter::ToC(pair.second));\n   }\n   c_device_buffer->count = bases.size();\n-  c_device_buffer->bases = new SE_DeviceMemoryBase[bases.size()];\n+  c_device_buffer->bases = new SE_DeviceAddressBase[bases.size()];\n   for (int i = 0; i < bases.size(); ++i) {\n     c_device_buffer->bases[i] = bases[i];\n   }\n@@ -457,7 +458,7 @@ std::unique_ptr<TpuEmbeddingEngineParametersData> Create(int num_tables) {\n }\n \n void Destroy(XLA_ShapeIndex* shape_index) { delete[] shape_index; }\n-void Destroy(SE_DeviceMemoryBase*) {}\n+void Destroy(SE_DeviceAddressBase*) {}\n \n void Destroy(XLA_Literal* c_literal) {\n   delete[] c_literal->buffers;"
        },
        {
            "sha": "a3b7c716996b34960ec47e4317fd89342e15fca2",
            "filename": "third_party/xla/xla/stream_executor/tpu/c_api_conversions.h",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -32,8 +32,8 @@ limitations under the License.\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/tpu/c_api_decl.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -56,12 +56,12 @@ void CreateVector(absl::Span<const bool> src, BoolList* dst);\n \n void CreateVector(absl::Span<const xla::DimLevelType> src, IntList* dst);\n \n-// se::DeviceMemoryBase\n-SE_DeviceMemoryBase ToC(const stream_executor::DeviceMemoryBase& base);\n-void ToC(const stream_executor::DeviceMemoryBase& base,\n-         SE_DeviceMemoryBase* se_base);\n-stream_executor::DeviceMemoryBase FromC(const SE_DeviceMemoryBase& se_base);\n-void Destroy(SE_DeviceMemoryBase*);\n+// se::DeviceAddressBase\n+SE_DeviceAddressBase ToC(const stream_executor::DeviceAddressBase& base);\n+void ToC(const stream_executor::DeviceAddressBase& base,\n+         SE_DeviceAddressBase* se_base);\n+stream_executor::DeviceAddressBase FromC(const SE_DeviceAddressBase& se_base);\n+void Destroy(SE_DeviceAddressBase*);\n \n // xla::Tile\n xla::Tile FromC(const XLA_Tile* c_tile);\n@@ -93,10 +93,10 @@ void ToC(const xla::ShapedBuffer& buffer, XLA_ShapedBuffer* c_device_buffer);\n xla::ShapedBuffer FromC(XLA_ShapedBuffer* c_buffer);\n void Destroy(XLA_ShapedBuffer* c_buffer);\n \n-// se::DeviceMemoryBase\n-SE_DeviceMemoryBase ToC(const stream_executor::DeviceMemoryBase& base);\n-stream_executor::DeviceMemoryBase FromC(const SE_DeviceMemoryBase& se_base);\n-void Destroy(SE_DeviceMemoryBase*);\n+// se::DeviceAddressBase\n+SE_DeviceAddressBase ToC(const stream_executor::DeviceAddressBase& base);\n+stream_executor::DeviceAddressBase FromC(const SE_DeviceAddressBase& se_base);\n+void Destroy(SE_DeviceAddressBase*);\n \n // Literal\n void ToC(const xla::LiteralSlice& literal, XLA_Literal* c_literal);\n@@ -119,15 +119,16 @@ std::unique_ptr<TpuEmbeddingEngineParametersData> Create(int num_tables);\n \n xla::MaybeOwningDeviceMemory FromC(\n     SE_MaybeOwningDeviceMemory* se_mem,\n-    stream_executor::DeviceMemoryAllocator* allocator);\n+    stream_executor::DeviceAddressAllocator* allocator);\n \n-// DeviceMemoryAllocator\n-SE_DeviceMemoryAllocator ToC(stream_executor::DeviceMemoryAllocator* allocator);\n-stream_executor::DeviceMemoryAllocator* FromC(\n-    const SE_DeviceMemoryAllocator& c_allocator);\n+// DeviceAddressAllocator\n+SE_DeviceAddressAllocator ToC(\n+    stream_executor::DeviceAddressAllocator* allocator);\n+stream_executor::DeviceAddressAllocator* FromC(\n+    const SE_DeviceAddressAllocator& c_allocator);\n \n-// OwningDeviceMemory\n-SE_MaybeOwningDeviceMemory ToC(stream_executor::OwningDeviceMemory* mem);\n+// OwningDeviceAddress\n+SE_MaybeOwningDeviceMemory ToC(stream_executor::OwningDeviceAddress* mem);\n // mem.HasOwnership() may be true if the buffer is aliased and shouldn't be\n // released. 'aliased' should be true in this case. 'aliased' has no effect if\n // 'mem' is unowned."
        },
        {
            "sha": "05ec51c5e79ea8e048ff771e336b726053a8c470",
            "filename": "third_party/xla/xla/stream_executor/tpu/c_api_conversions_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_conversions_test.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -307,7 +307,7 @@ TEST(XlaHloModule, ToAndFromC) {\n   Destroy(&c_module);\n }\n \n-// TODO(b/290654348): SE_DeviceMemoryBase, SE_DeviceMemoryAllocator,\n+// TODO(b/290654348): SE_DeviceAddressBase, SE_DeviceAddressAllocator,\n // SE_MaybeOwningDeviceMemory\n \n }  // namespace"
        },
        {
            "sha": "096f265acaec793aacbd8750c041ac5e1f514afc",
            "filename": "third_party/xla/xla/stream_executor/tpu/c_api_decl.h",
            "status": "modified",
            "additions": 21,
            "deletions": 14,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_decl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_decl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Fc_api_decl.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -66,16 +66,20 @@ typedef struct SE_PlatformId {\n } SE_PlatformId;\n typedef TF_Status* (*SE_StatusCallback)(void*);\n \n-typedef struct SE_DeviceMemoryBase {\n+typedef struct SE_DeviceAddressBase {\n   void* opaque;\n   uint64_t size;\n   uint64_t payload;\n-} SE_DeviceMemoryBase;\n+} SE_DeviceAddressBase;\n \n-typedef struct SE_ScopedDeviceMemory {\n-  SE_DeviceMemoryBase wrapped;\n+typedef SE_DeviceAddressBase SE_DeviceMemoryBase;\n+\n+typedef struct SE_ScopedDeviceAddress {\n+  SE_DeviceAddressBase wrapped;\n   int device_ordinal;\n-} SE_ScopedDeviceMemory;\n+} SE_ScopedDeviceAddress;\n+\n+typedef SE_ScopedDeviceAddress SE_ScopedDeviceMemory;\n \n typedef struct SE_AllocatorStats {\n   int64_t num_allocs;\n@@ -95,22 +99,25 @@ typedef struct SE_AllocatorStats {\n   int64_t largest_free_block_bytes;\n } SE_AllocatorStats;\n \n-// Note, due to the... odd way in which DeviceMemoryAllocator is used in TF, we\n+// Note, due to the... odd way in which DeviceAddressAllocator is used in TF, we\n // cannot simply wrap an underlying pointer. Instead, we reverse the call\n // direction and request memory via a callback.\n typedef void (*SE_AllocateFn)(void* ctx, int device_ordinal, uint64_t size,\n                               bool retry_on_failure, int64_t memory_space,\n-                              SE_ScopedDeviceMemory* result, TF_Status* status);\n+                              SE_ScopedDeviceAddress* result,\n+                              TF_Status* status);\n \n-typedef void (*SE_DeallocateFn)(void* ctx, SE_DeviceMemoryBase* base,\n+typedef void (*SE_DeallocateFn)(void* ctx, SE_DeviceAddressBase* base,\n                                 int device_ordinal, TF_Status* status);\n \n-typedef struct SE_DeviceMemoryAllocator {\n+typedef struct SE_DeviceAddressAllocator {\n   SE_Platform* platform;\n   void* ctx;\n   SE_AllocateFn allocate;\n   SE_DeallocateFn deallocate;\n-} SE_DeviceMemoryAllocator;\n+} SE_DeviceAddressAllocator;\n+\n+typedef SE_DeviceAddressAllocator SE_DeviceMemoryAllocator;\n \n typedef struct SE_DeviceDescription {\n   char* device_vendor;\n@@ -155,7 +162,7 @@ typedef struct Tpu_Compiler Tpu_Compiler;\n typedef struct SE_Executable SE_Executable;\n \n typedef struct SE_ExecutableRunOptions {\n-  SE_DeviceMemoryAllocator allocator;\n+  SE_DeviceAddressAllocator allocator;\n   int device_ordinal;\n   SE_Stream* stream;\n   SE_Stream* host_to_device_stream;\n@@ -169,12 +176,12 @@ typedef struct SE_ExecutableSerializationHandle\n     SE_ExecutableSerializationHandle;\n \n typedef struct SE_MaybeOwningDeviceMemory {\n-  SE_DeviceMemoryBase memory;\n+  SE_DeviceAddressBase memory;\n   bool owned;\n \n   // Set if owned\n   int device_ordinal;\n-  SE_DeviceMemoryAllocator allocator;\n+  SE_DeviceAddressAllocator allocator;\n } SE_MaybeOwningDeviceMemory;\n \n typedef struct IntList {\n@@ -258,7 +265,7 @@ typedef struct XLA_ShapedBuffer {\n   XLA_Shape on_device_shape;\n   int device_ordinal;\n \n-  SE_DeviceMemoryBase* bases;\n+  SE_DeviceAddressBase* bases;\n   size_t count;\n } XLA_ShapedBuffer;\n "
        },
        {
            "sha": "b34fb2e611dce57ad40d53151606a90b4a1e4e0d",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_executable.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/service_executable_run_options.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/tpu/c_api_decl.h\"\n #include \"xla/stream_executor/tpu/tpu_executable_interface.h\"\n #include \"xla/stream_executor/tpu/tpu_executor_c_api.h\"\n@@ -62,9 +62,9 @@ class TpuExecutable : public TpuExecutableInterface {\n  private:\n   absl::Status LoadProgramAndEnqueueToStream(\n       const ServiceExecutableRunOptions& run_options,\n-      absl::Span<const stream_executor::DeviceMemoryBase> arguments,\n-      stream_executor::DeviceMemoryBase result,\n-      const std::vector<stream_executor::DeviceMemoryBase>&\n+      absl::Span<const stream_executor::DeviceAddressBase> arguments,\n+      stream_executor::DeviceAddressBase result,\n+      const std::vector<stream_executor::DeviceAddressBase>&\n           cross_program_prefetch_addrs,\n       const std::vector<uint32_t>& cross_program_prefetch_offsets) override {\n     LOG(FATAL) << \"LoadProgramAndEnqueueToStream unimplemented\";"
        },
        {
            "sha": "0b4c4db98728d2475b45414af16cd3e5e1daf44e",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_executable_interface.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable_interface.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable_interface.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable_interface.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -38,8 +38,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -74,7 +74,7 @@ static absl::Status PopulateResultTupleBuffers(const ShapedBuffer& result,\n absl::StatusOr<ExecutionOutput>\n TpuExecutableInterface::AllocateOutputMemoryWithInputReuse(\n     const Shape& shape, const HloInputOutputAliasConfig& alias_config,\n-    se::DeviceMemoryAllocator* allocator,\n+    se::DeviceAddressAllocator* allocator,\n     std::vector<ExecutionInput>* arguments, se::Stream* stream,\n     se::Stream* transfer_stream) {\n   auto stream_exec = stream->parent();\n@@ -136,7 +136,7 @@ TpuExecutableInterface::AllocateOutputMemoryWithInputReuse(\n   int64_t total_result_buffer_bytes = 0;\n   for (auto& pair : result.MutableResult()->buffers()) {\n     const ShapeIndex& result_index = pair.first;\n-    se::DeviceMemoryBase& result_buffer = pair.second;\n+    se::DeviceAddressBase& result_buffer = pair.second;\n     int64_t allocation_bytes = shape_size_fn(ShapeUtil::GetSubshape(\n         result.Result().on_device_shape(), result_index));\n     total_result_buffer_bytes += allocation_bytes;\n@@ -159,7 +159,7 @@ TpuExecutableInterface::AllocateOutputMemoryWithInputReuse(\n         // as the output buffer. It is up to the caller whether or not to\n         // donate a buffer; the aliasing information describes which buffers\n         // may alias, not buffers that must alias.\n-        se::DeviceMemoryBase device_memory_base = owning->Release();\n+        se::DeviceAddressBase device_memory_base = owning->Release();\n         *device_memory = device_memory_base;\n         result_buffer = device_memory_base;\n         reused_buffer_bytes += allocation_bytes;\n@@ -209,7 +209,7 @@ TpuExecutableInterface::AllocateOutputMemoryWithInputReuse(\n absl::StatusOr<ExecutionOutput> TpuExecutableInterface::ExecuteAsyncOnStream(\n     const ServiceExecutableRunOptions* run_options,\n     std::vector<ExecutionInput> arguments) {\n-  std::vector<se::DeviceMemoryBase> memory_bases;\n+  std::vector<se::DeviceAddressBase> memory_bases;\n   memory_bases.reserve(arguments.size());\n   for (auto& argument : arguments) {\n     memory_bases.push_back(argument.Buffer({}).AsDeviceMemoryBase());\n@@ -228,7 +228,7 @@ absl::StatusOr<ExecutionOutput> TpuExecutableInterface::ExecuteAsyncOnStream(\n           run_options->run_options().host_to_device_stream()));\n \n   // Address of the buffer in TPU memory that is being speculated.\n-  std::vector<se::DeviceMemoryBase> cross_program_prefetch_addrs;\n+  std::vector<se::DeviceAddressBase> cross_program_prefetch_addrs;\n   std::vector<uint32_t> cross_program_prefetch_offsets;\n   if (has_module()) {\n     for (const auto& [parameter, index, offset] :\n@@ -248,7 +248,7 @@ absl::StatusOr<ExecutionOutput> TpuExecutableInterface::ExecuteAsyncOnStream(\n                 it->second.AsDeviceMemoryBase());\n           });\n       cross_program_prefetch_addrs.emplace_back(\n-          is_prefetch_output_alias ? stream_executor::DeviceMemoryBase()\n+          is_prefetch_output_alias ? stream_executor::DeviceAddressBase()\n                                    : it->second.AsDeviceMemoryBase());\n       cross_program_prefetch_offsets.emplace_back(\n           is_prefetch_output_alias ? std::numeric_limits<uint32_t>::max()"
        },
        {
            "sha": "31ba6f5f214e7ac91775ad5d8dbb50b52144e5aa",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_executable_interface.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executable_interface.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -30,8 +30,8 @@ limitations under the License.\n #include \"xla/service/executable.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla::legacy {\n@@ -63,15 +63,15 @@ class TpuExecutableInterface : public Executable {\n   // tables) to be performed on a separate stream to 'stream'.\n   absl::StatusOr<ExecutionOutput> AllocateOutputMemoryWithInputReuse(\n       const Shape& shape, const HloInputOutputAliasConfig& alias_config,\n-      se::DeviceMemoryAllocator* allocator,\n+      se::DeviceAddressAllocator* allocator,\n       std::vector<ExecutionInput>* arguments, se::Stream* stream,\n       se::Stream* transfer_stream = nullptr);\n \n   virtual absl::Status LoadProgramAndEnqueueToStream(\n       const ServiceExecutableRunOptions& run_options,\n-      absl::Span<const stream_executor::DeviceMemoryBase> arguments,\n-      stream_executor::DeviceMemoryBase result,\n-      const std::vector<stream_executor::DeviceMemoryBase>&\n+      absl::Span<const stream_executor::DeviceAddressBase> arguments,\n+      stream_executor::DeviceAddressBase result,\n+      const std::vector<stream_executor::DeviceAddressBase>&\n           cross_program_prefetch_addrs,\n       const std::vector<uint32_t>& cross_program_prefetch_offsets) = 0;\n "
        },
        {
            "sha": "f5b89cf2e0949936e187d634065b0499ff302e2d",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_executor.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,8 +27,8 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/allocator_stats.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -96,19 +96,19 @@ absl::StatusOr<std::unique_ptr<Event>> TpuExecutor::CreateEvent() {\n   return std::move(tpu_event);\n }\n \n-DeviceMemoryBase TpuExecutor::Allocate(uint64_t size, int64_t memory_space) {\n-  SE_DeviceMemoryBase se_base =\n+DeviceAddressBase TpuExecutor::Allocate(uint64_t size, int64_t memory_space) {\n+  SE_DeviceAddressBase se_base =\n       ExecutorApiFn()->TpuExecutor_AllocateFn(executor_, size, memory_space);\n   return ApiConverter::FromC(se_base);\n }\n \n-void TpuExecutor::Deallocate(const DeviceMemoryBase& memory) {\n-  SE_DeviceMemoryBase se_base = ApiConverter::ToC(memory);\n+void TpuExecutor::Deallocate(const DeviceAddressBase& memory) {\n+  SE_DeviceAddressBase se_base = ApiConverter::ToC(memory);\n   ExecutorApiFn()->TpuExecutor_DeallocateFn(executor_, &se_base);\n }\n \n-void TpuExecutor::Deallocate(DeviceMemoryBase* memory) {\n-  SE_DeviceMemoryBase se_base = ApiConverter::ToC(*memory);\n+void TpuExecutor::Deallocate(DeviceAddressBase* memory) {\n+  SE_DeviceAddressBase se_base = ApiConverter::ToC(*memory);\n   ExecutorApiFn()->TpuExecutor_DeallocateFn(executor_, &se_base);\n }\n \n@@ -167,20 +167,20 @@ absl::Status TpuExecutor::EnqueueInfeed(int32_t infeed_queue_index,\n }\n \n absl::Status TpuExecutor::SynchronousMemcpy(\n-    ::stream_executor::DeviceMemoryBase* device_dst, const void* host_src,\n+    ::stream_executor::DeviceAddressBase* device_dst, const void* host_src,\n     uint64_t size) {\n   StatusHelper status;\n-  SE_DeviceMemoryBase se_base = ApiConverter::ToC(*device_dst);\n+  SE_DeviceAddressBase se_base = ApiConverter::ToC(*device_dst);\n   ExecutorApiFn()->TpuExecutor_SynchronousMemcpyFromHostFn(\n       executor_, &se_base, host_src, size, status.c_status);\n   return status.status();\n }\n \n absl::Status TpuExecutor::SynchronousMemcpy(\n-    void* host_dst, const ::stream_executor::DeviceMemoryBase& device_src,\n+    void* host_dst, const ::stream_executor::DeviceAddressBase& device_src,\n     uint64_t size) {\n   StatusHelper status;\n-  SE_DeviceMemoryBase se_base = ApiConverter::ToC(device_src);\n+  SE_DeviceAddressBase se_base = ApiConverter::ToC(device_src);\n   ExecutorApiFn()->TpuExecutor_SynchronousMemcpyToHostFn(\n       executor_, host_dst, &se_base, size, status.c_status);\n   return status.status();"
        },
        {
            "sha": "8209ec55e0b12c4b9b55a4cf8ef3b80e5526150b",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_executor.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -30,8 +30,8 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/stream_executor/allocator_stats.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -65,16 +65,16 @@ class TpuExecutor : public tensorflow::tpu::TpuExecutorInterface {\n \n   absl::Status Init() override;\n \n-  DeviceMemoryBase Allocate(uint64_t size, int64_t memory_space) override;\n+  DeviceAddressBase Allocate(uint64_t size, int64_t memory_space) override;\n \n   absl::StatusOr<std::unique_ptr<DeviceDescription>> CreateDeviceDescription()\n       const override;\n \n   void DeallocateStream(Stream* stream) override;\n \n-  void Deallocate(const DeviceMemoryBase& memory);\n+  void Deallocate(const DeviceAddressBase& memory);\n \n-  void Deallocate(DeviceMemoryBase* memory) override;\n+  void Deallocate(DeviceAddressBase* memory) override;\n \n   bool DeviceMemoryUsage(int64_t* free, int64_t* total) const override;\n \n@@ -96,10 +96,10 @@ class TpuExecutor : public tensorflow::tpu::TpuExecutorInterface {\n \n   bool SynchronizeAllActivity() override;\n \n-  absl::Status SynchronousMemcpy(DeviceMemoryBase* device_dst,\n+  absl::Status SynchronousMemcpy(DeviceAddressBase* device_dst,\n                                  const void* host_src, uint64_t size) override;\n   absl::Status SynchronousMemcpy(void* host_dst,\n-                                 const DeviceMemoryBase& device_src,\n+                                 const DeviceAddressBase& device_src,\n                                  uint64_t size) override;\n   absl::Status UnloadAllPrograms() override;\n \n@@ -117,7 +117,7 @@ class TpuExecutor : public tensorflow::tpu::TpuExecutorInterface {\n   // TODO(henrytan): convert this to override once the base interface is changed\n   // to TpuExecutorInterface.\n   absl::StatusOr<std::unique_ptr<\n-      tensorflow::tpu::TpuExecutorInterface::TemporaryDeviceMemory>>\n+      tensorflow::tpu::TpuExecutorInterface::TemporaryDeviceAddress>>\n   CreateTemporaryDeviceMemory(int64_t memory_space, int64_t byte_offset,\n                               int64_t size) override {\n     LOG(FATAL) << \"Unimplemented.\";\n@@ -136,7 +136,7 @@ class TpuExecutor : public tensorflow::tpu::TpuExecutorInterface {\n       uint64_t size) override {\n     LOG(FATAL) << \"not yet implemented\";\n   }\n-  absl::Status SynchronousMemZero(DeviceMemoryBase* location,\n+  absl::Status SynchronousMemZero(DeviceAddressBase* location,\n                                   uint64_t size) override {\n     LOG(FATAL) << \"not yet implemented\";\n   }"
        },
        {
            "sha": "ce57d254450d4e0a1f5b21dfc90a1f3abc98db03",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_executor_c_api.h",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor_c_api.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor_c_api.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor_c_api.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -40,10 +40,10 @@ TpuRuntimeVersion TpuPlatform_GetRuntimeVersion(SE_Platform* platform);\n void TpuExecutor_Init(SE_StreamExecutor* executor, TF_Status* status);\n void TpuExecutor_Free(SE_StreamExecutor* executor);\n \n-SE_DeviceMemoryBase TpuExecutor_Allocate(SE_StreamExecutor* executor,\n-                                         uint64_t size, int64_t memory_space);\n+SE_DeviceAddressBase TpuExecutor_Allocate(SE_StreamExecutor* executor,\n+                                          uint64_t size, int64_t memory_space);\n void TpuExecutor_Deallocate(SE_StreamExecutor* executor,\n-                            SE_DeviceMemoryBase* memory);\n+                            SE_DeviceAddressBase* memory);\n bool TpuExecutor_GetAllocatorStats(SE_StreamExecutor* executor,\n                                    SE_AllocatorStats* stats);\n bool TpuExecutor_DeviceMemoryUsage(SE_StreamExecutor* executor, int64_t* free,\n@@ -68,19 +68,19 @@ void TpuExecutor_WaitForEvent(SE_StreamExecutor* executor, SE_Stream* stream,\n \n void TpuExecutor_SynchronousMemcpyToHost(SE_StreamExecutor* executor,\n                                          void* host_dst,\n-                                         const SE_DeviceMemoryBase* device_src,\n+                                         const SE_DeviceAddressBase* device_src,\n                                          uint64_t size, TF_Status* status);\n void TpuExecutor_SynchronousMemcpyFromHost(SE_StreamExecutor* executor,\n-                                           SE_DeviceMemoryBase* device_dst,\n+                                           SE_DeviceAddressBase* device_dst,\n                                            const void* host_src, uint64_t size,\n                                            TF_Status* status);\n void TpuExecutor_MemcpyToHost(SE_StreamExecutor* executor, SE_Stream* stream,\n                               void* host_dst,\n-                              const SE_DeviceMemoryBase* device_src,\n+                              const SE_DeviceAddressBase* device_src,\n                               uint64_t size, TF_Status* status);\n \n void TpuExecutor_MemcpyFromHost(SE_StreamExecutor* executor, SE_Stream* stream,\n-                                SE_DeviceMemoryBase* device_dst,\n+                                SE_DeviceAddressBase* device_dst,\n                                 const void* host_src, uint64_t size,\n                                 TF_Status* status);\n \n@@ -107,16 +107,16 @@ void* TpuStream_Stream(SE_Stream*);\n bool TpuStream_Status(SE_Stream*);\n bool TpuStream_IsSameSharedMemoryLocation(SE_Stream*, SE_Stream*);\n void TpuStream_EnqueueTransferHostToDevice(SE_Stream* stream,\n-                                           SE_DeviceMemoryBase device_dst,\n+                                           SE_DeviceAddressBase device_dst,\n                                            void* host_src, uint64_t size,\n                                            TF_Status* status);\n void TpuStream_EnqueueTransferDeviceToHost(SE_Stream* stream,\n-                                           SE_DeviceMemoryBase device_src,\n+                                           SE_DeviceAddressBase device_src,\n                                            void* host_dst, uint64_t size,\n                                            TF_Status* status);\n void TpuStream_TpuEnqueueOnDeviceSendRecvLocal(SE_Stream* stream,\n-                                               SE_DeviceMemoryBase send_buffer,\n-                                               SE_DeviceMemoryBase recv_buffer,\n+                                               SE_DeviceAddressBase send_buffer,\n+                                               SE_DeviceAddressBase recv_buffer,\n                                                TF_Status* status);\n \n SE_Event* TpuEvent_New(SE_StreamExecutor* parent);\n@@ -163,11 +163,11 @@ bool TpuTransferManager_CanShapedBufferBeAccessedNow(\n     XLA_ShapedBuffer* device_buffer);\n bool TpuTransferManager_CanBufferBeAccessedNow(\n     XLA_TransferManager* manager, SE_StreamExecutor* executor,\n-    SE_DeviceMemoryBase* device_buffer);\n+    SE_DeviceAddressBase* device_buffer);\n void TpuTransferManager_WriteSingleTupleIndexTable(\n     XLA_TransferManager* manager, SE_Stream* stream,\n-    SE_DeviceMemoryBase* elements, size_t elements_len, XLA_Shape* shape,\n-    SE_DeviceMemoryBase* region, TF_Status* status);\n+    SE_DeviceAddressBase* elements, size_t elements_len, XLA_Shape* shape,\n+    SE_DeviceAddressBase* region, TF_Status* status);\n void TpuTransferManager_GetInfeedLayout(XLA_Shape* shape,\n                                         XLA_Shape* infeed_shape);\n void TpuTransferManager_LinearizeToBuffers(\n@@ -268,18 +268,18 @@ TFTPU_CAPI_EXPORT void TpuCompiler_Free(Tpu_Compiler* compiler);\n \n TFTPU_CAPI_EXPORT void TpuCompiler_RunHloPasses(\n     Tpu_Compiler* compiler, XLA_HloModule* se_hlo_module,\n-    SE_StreamExecutor* stream_executor, SE_DeviceMemoryAllocator* allocator,\n+    SE_StreamExecutor* stream_executor, SE_DeviceAddressAllocator* allocator,\n     XLA_HloModule* result, TF_Status* status);\n \n TFTPU_CAPI_EXPORT void TpuCompiler_RunBackend(\n     Tpu_Compiler* compiler, XLA_HloModule* se_hlo_module,\n-    SE_StreamExecutor* stream_executor, SE_DeviceMemoryAllocator* allocator,\n+    SE_StreamExecutor* stream_executor, SE_DeviceAddressAllocator* allocator,\n     SE_Executable** result, TF_Status* status);\n \n TFTPU_CAPI_EXPORT void TpuCompiler_Compile(\n     Tpu_Compiler* compiler, XLA_HloModuleGroup* se_hlo_module_group,\n     SE_StreamExecutorList* stream_exec_lists, int num_lists,\n-    SE_DeviceMemoryAllocator* allocator, SE_Executable** executables,\n+    SE_DeviceAddressAllocator* allocator, SE_Executable** executables,\n     TF_Status* status);\n \n TFTPU_CAPI_EXPORT int64_t TpuCompiler_ShapeSize(Tpu_Compiler* compiler,"
        },
        {
            "sha": "6012bb3752dd4f8aeb45d0c7974db69da05bc1b4",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_executor_interface.h",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_executor_interface.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -1,4 +1,3 @@\n-#include \"xla/stream_executor/platform.h\"\n /* Copyright 2020 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -22,7 +21,8 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor_common.h\"\n #include \"xla/stream_executor/tpu/tpu_platform_interface.h\"\n #include \"xla/stream_executor/tpu/tpu_topology.h\"\n@@ -39,13 +39,16 @@ class TpuExecutorInterface : public stream_executor::StreamExecutorCommon {\n   explicit TpuExecutorInterface(stream_executor::Platform* platform)\n       : StreamExecutorCommon(platform) {}\n \n-  class TemporaryDeviceMemory {\n+  class TemporaryDeviceAddress {\n    public:\n-    virtual ~TemporaryDeviceMemory() {}\n-    virtual stream_executor::DeviceMemoryBase AsDeviceMemoryBase() const = 0;\n+    virtual ~TemporaryDeviceAddress() {}\n+    virtual stream_executor::DeviceAddressBase AsDeviceMemoryBase() const = 0;\n   };\n \n-  virtual absl::StatusOr<std::unique_ptr<TemporaryDeviceMemory>>\n+  using TemporaryDeviceMemory ABSL_DEPRECATE_AND_INLINE() =\n+      TemporaryDeviceAddress;\n+\n+  virtual absl::StatusOr<std::unique_ptr<TemporaryDeviceAddress>>\n   CreateTemporaryDeviceMemory(int64_t memory_space, int64_t byte_offset,\n                               int64_t size) {\n     LOG(FATAL) << \"Unimplemented.\";"
        },
        {
            "sha": "05c2570fcb93338711bc1171dfab0b58174571cc",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_node_context.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_node_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_node_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_node_context.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -24,7 +24,7 @@ limitations under the License.\n #include \"xla/executable_run_options.h\"\n #include \"xla/service/backend.h\"\n #include \"xla/service/stream_pool.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/tpu/tpu_ops_c_api.h\"\n #include \"xla/stream_executor/tpu/tpu_platform_interface.h\"\n #include \"tsl/platform/macros.h\""
        },
        {
            "sha": "d63966f02427e2404c826f0ae313442dd5c9bedd",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_on_demand_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_on_demand_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_on_demand_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_on_demand_compiler.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -140,7 +140,7 @@ class TpuCompiler : public Compiler {\n               ->se_executor();\n     }\n \n-    SE_DeviceMemoryAllocator allocator =\n+    SE_DeviceAddressAllocator allocator =\n         ApiConverter::ToC(options.device_allocator);\n \n     SE_Executable** se_executables = new SE_Executable*[1];"
        },
        {
            "sha": "1602db47a4465fce7dc3006edb000d0009df943f",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_op_executable.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_op_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_op_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_op_executable.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/service_executable_run_options.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/tpu/c_api_conversions.h\"  // IWYU pragma: keep\n #include \"xla/stream_executor/tpu/c_api_decl.h\"\n #include \"xla/stream_executor/tpu/proto_helper.h\"\n@@ -54,25 +54,25 @@ TpuOpExecutable::TpuOpExecutable(\n \n absl::Status TpuOpExecutable::LoadProgramAndEnqueueToStream(\n     const xla::ServiceExecutableRunOptions& run_options,\n-    absl::Span<const se::DeviceMemoryBase> arguments,\n-    se::DeviceMemoryBase result,\n-    const std::vector<se::DeviceMemoryBase>& cross_program_prefetch_addrs,\n+    absl::Span<const se::DeviceAddressBase> arguments,\n+    se::DeviceAddressBase result,\n+    const std::vector<se::DeviceAddressBase>& cross_program_prefetch_addrs,\n     const std::vector<uint32_t>& cross_program_prefetch_offsets) {\n-  auto DeviceMemoryBaseToC = [](const se::DeviceMemoryBase& addr) {\n-    return SE_DeviceMemoryBase{const_cast<void*>(addr.opaque()), addr.size(),\n-                               addr.payload()};\n+  auto DeviceAddressBaseToC = [](const se::DeviceAddressBase& addr) {\n+    return SE_DeviceAddressBase{const_cast<void*>(addr.opaque()), addr.size(),\n+                                addr.payload()};\n   };\n \n-  std::vector<SE_DeviceMemoryBase> arguments_bases;\n+  std::vector<SE_DeviceAddressBase> arguments_bases;\n   arguments_bases.resize(arguments.size());\n-  absl::c_transform(arguments, arguments_bases.begin(), DeviceMemoryBaseToC);\n+  absl::c_transform(arguments, arguments_bases.begin(), DeviceAddressBaseToC);\n \n-  SE_DeviceMemoryBase result_base = DeviceMemoryBaseToC(result);\n+  SE_DeviceAddressBase result_base = DeviceAddressBaseToC(result);\n \n-  std::vector<SE_DeviceMemoryBase> prefetch_bases;\n+  std::vector<SE_DeviceAddressBase> prefetch_bases;\n   prefetch_bases.resize(cross_program_prefetch_addrs.size());\n   absl::c_transform(cross_program_prefetch_addrs, prefetch_bases.begin(),\n-                    DeviceMemoryBaseToC);\n+                    DeviceAddressBaseToC);\n   int32_t rng_seed = run_options.run_options().rng_seed();\n \n   XLA_DeviceAssignment c_dev_assign{/*bytes=*/nullptr, /*size=*/0};"
        },
        {
            "sha": "5cc812b71deee37ae3e301556a0e7513cce02fd7",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_op_executable.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_op_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_op_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_op_executable.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/service_executable_run_options.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/tpu/c_api_decl.h\"\n #include \"xla/stream_executor/tpu/tpu_executable_interface.h\"\n #include \"xla/stream_executor/tpu/tpu_ops_c_api.h\"\n@@ -52,9 +52,9 @@ class TpuOpExecutable : public xla::legacy::TpuExecutableInterface {\n  private:\n   absl::Status LoadProgramAndEnqueueToStream(\n       const xla::ServiceExecutableRunOptions& run_options,\n-      absl::Span<const stream_executor::DeviceMemoryBase> arguments,\n-      stream_executor::DeviceMemoryBase result,\n-      const std::vector<stream_executor::DeviceMemoryBase>&\n+      absl::Span<const stream_executor::DeviceAddressBase> arguments,\n+      stream_executor::DeviceAddressBase result,\n+      const std::vector<stream_executor::DeviceAddressBase>&\n           cross_program_prefetch_addrs,\n       const std::vector<uint32_t>& cross_program_prefetch_offsets) override;\n "
        },
        {
            "sha": "02e6005026cc7c3c8dc338e7778c2dc5ab8e443d",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_ops_c_api.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_ops_c_api.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_ops_c_api.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_ops_c_api.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -177,11 +177,11 @@ typedef struct TpuExecutable_LoadProgramAndEnqueueToStream_Params {\n   void* priv;\n \n   const XLA_TpuProgram* program;\n-  SE_DeviceMemoryBase* arguments;\n+  SE_DeviceAddressBase* arguments;\n   size_t arguments_len;\n-  SE_DeviceMemoryBase* result;\n+  SE_DeviceAddressBase* result;\n   size_t cross_program_prefetch_addrs_len;\n-  SE_DeviceMemoryBase* cross_program_prefetch_addrs;\n+  SE_DeviceAddressBase* cross_program_prefetch_addrs;\n   size_t cross_program_prefetch_offsets_len;\n   const uint32_t* cross_program_prefetch_offsets;\n   int32_t rng_seed;\n@@ -225,11 +225,11 @@ TFTPU_CAPI_EXPORT void TpuExecute_RuntimeInputToPaddedData(\n     TpuExecute_RuntimeInputToPaddedData_Params* params);\n \n TFTPU_CAPI_EXPORT void TpuExecute_GetTpuEmbeddingMemoryAllocations(\n-    int device_ordinal, SE_DeviceMemoryBase** addrs, size_t* addrs_count,\n+    int device_ordinal, SE_DeviceAddressBase** addrs, size_t* addrs_count,\n     TF_Status* status);\n \n TFTPU_CAPI_EXPORT void TpuExecute_FreeTpuEmbeddingMemoryAllocations(\n-    int device_ordinal, SE_DeviceMemoryBase* addrs);\n+    int device_ordinal, SE_DeviceAddressBase* addrs);\n \n typedef struct ConfigureDistributedTpuOp_DoWork_Params {\n   int32_t struct_size;"
        },
        {
            "sha": "45c266666656c956b5ad1956fb2a5fae0a526795",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_stream.h",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_stream.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_stream.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_stream.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -21,7 +21,7 @@ limitations under the License.\n \n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -60,7 +60,7 @@ class TpuStream : public tensorflow::tpu::TpuStreamInterface {\n   }\n \n   absl::Status EnqueueTransferHostToDevice(\n-      stream_executor::DeviceMemoryBase device_dst, const void* host_src,\n+      stream_executor::DeviceAddressBase device_dst, const void* host_src,\n       uint64_t size) {\n     StatusHelper status;\n     stream_executor::tpu::ExecutorApiFn()\n@@ -78,7 +78,7 @@ class TpuStream : public tensorflow::tpu::TpuStreamInterface {\n   }\n \n   absl::Status EnqueueTransferDeviceToHost(\n-      stream_executor::DeviceMemoryBase device_src, void* host_dst,\n+      stream_executor::DeviceAddressBase device_src, void* host_dst,\n       uint64_t size) {\n     StatusHelper status;\n     stream_executor::tpu::ExecutorApiFn()\n@@ -89,8 +89,8 @@ class TpuStream : public tensorflow::tpu::TpuStreamInterface {\n   }\n \n   absl::Status EnqueueOnTpuDeviceSendRecvLocal(\n-      stream_executor::DeviceMemoryBase send_buffer,\n-      stream_executor::DeviceMemoryBase recv_buffer) override {\n+      stream_executor::DeviceAddressBase send_buffer,\n+      stream_executor::DeviceAddressBase recv_buffer) override {\n     StatusHelper status;\n     stream_executor::tpu::ExecutorApiFn()\n         ->TpuStream_TpuEnqueueOnDeviceSendRecvLocalFn(\n@@ -132,25 +132,25 @@ class TpuStream : public tensorflow::tpu::TpuStreamInterface {\n     return status.status();\n   }\n \n-  absl::Status Memcpy(stream_executor::DeviceMemoryBase* device_dst,\n+  absl::Status Memcpy(stream_executor::DeviceAddressBase* device_dst,\n                       const void* host_src, uint64_t size) override {\n     StatusHelper status;\n-    SE_DeviceMemoryBase se_base = ApiConverter::ToC(*device_dst);\n+    SE_DeviceAddressBase se_base = ApiConverter::ToC(*device_dst);\n     stream_executor::tpu::ExecutorApiFn()->TpuExecutor_MemcpyFromHostFn(\n         se_executor_, stream_, &se_base, host_src, size, status.c_status);\n     return status.status();\n   }\n-  absl::Status Memcpy(stream_executor::DeviceMemoryBase* device_dst,\n-                      const stream_executor::DeviceMemoryBase& device_src,\n+  absl::Status Memcpy(stream_executor::DeviceAddressBase* device_dst,\n+                      const stream_executor::DeviceAddressBase& device_src,\n                       uint64_t size) override {\n     return absl::UnimplementedError(\n         \"Memcpy from device to deviceis not implemented for TPU\");\n   }\n   absl::Status Memcpy(void* host_dst,\n-                      const stream_executor::DeviceMemoryBase& device_src,\n+                      const stream_executor::DeviceAddressBase& device_src,\n                       uint64_t size) override {\n     StatusHelper status;\n-    SE_DeviceMemoryBase se_base = ApiConverter::ToC(device_src);\n+    SE_DeviceAddressBase se_base = ApiConverter::ToC(device_src);\n     stream_executor::tpu::ExecutorApiFn()->TpuExecutor_MemcpyToHostFn(\n         se_executor_, stream_, host_dst, &se_base, size, status.c_status);\n     return status.status();"
        },
        {
            "sha": "e8cddcb112bfd06388513b3d05c6782d2719ec31",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_stream_interface.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_stream_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_stream_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_stream_interface.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -17,7 +17,7 @@ limitations under the License.\n #define XLA_STREAM_EXECUTOR_TPU_TPU_STREAM_INTERFACE_H_\n \n #include \"absl/status/status.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream_common.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n@@ -30,8 +30,8 @@ class TpuStreamInterface : public stream_executor::StreamCommon {\n       : StreamCommon(executor) {}\n   virtual bool IsSameSharedMemoryLocation(TpuStreamInterface* other) = 0;\n   virtual absl::Status EnqueueOnTpuDeviceSendRecvLocal(\n-      stream_executor::DeviceMemoryBase send_buffer,\n-      stream_executor::DeviceMemoryBase recv_buffer) = 0;\n+      stream_executor::DeviceAddressBase send_buffer,\n+      stream_executor::DeviceAddressBase recv_buffer) = 0;\n };\n \n }  // namespace tpu"
        },
        {
            "sha": "38f87e1c24e895a478c0f7a4d389c929eca9ed02",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_transfer_manager.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_transfer_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_transfer_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_transfer_manager.cc?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"xla/literal.h\"\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/tpu/c_api_conversions.h\"\n@@ -276,32 +276,32 @@ bool TpuTransferManager::CanShapedBufferBeAccessedNow(\n \n bool TpuTransferManager::CanBufferBeAccessedNow(\n     se::StreamExecutor* executor,\n-    const se::DeviceMemoryBase& device_buffer) const {\n+    const se::DeviceAddressBase& device_buffer) const {\n   auto* tpu_executor = down_cast<stream_executor::tpu::TpuExecutor*>(executor);\n-  SE_DeviceMemoryBase c_device_buffer{const_cast<void*>(device_buffer.opaque()),\n-                                      device_buffer.size(),\n-                                      device_buffer.payload()};\n+  SE_DeviceAddressBase c_device_buffer{\n+      const_cast<void*>(device_buffer.opaque()), device_buffer.size(),\n+      device_buffer.payload()};\n   return stream_executor::tpu::ExecutorApiFn()\n       ->TpuTransferManager_CanBufferBeAccessedNowFn(\n           manager_, tpu_executor->se_executor(), &c_device_buffer);\n }\n \n absl::Status TpuTransferManager::WriteSingleTupleIndexTable(\n     stream_executor::Stream* stream,\n-    absl::Span<const stream_executor::DeviceMemoryBase> elements,\n-    const xla::Shape& shape, stream_executor::DeviceMemoryBase* region) {\n+    absl::Span<const stream_executor::DeviceAddressBase> elements,\n+    const xla::Shape& shape, stream_executor::DeviceAddressBase* region) {\n   CHECK_GT(elements.size(), 0);\n-  SE_DeviceMemoryBase* elements_bases =\n-      new SE_DeviceMemoryBase[elements.size()];\n+  SE_DeviceAddressBase* elements_bases =\n+      new SE_DeviceAddressBase[elements.size()];\n   for (int i = 0; i < elements.size(); i++) {\n     elements_bases[i] =\n-        SE_DeviceMemoryBase{const_cast<void*>(elements[i].opaque()),\n-                            elements[i].size(), elements[i].payload()};\n+        SE_DeviceAddressBase{const_cast<void*>(elements[i].opaque()),\n+                             elements[i].size(), elements[i].payload()};\n   }\n   XLA_Shape c_shape;\n   ApiConverter::ToC(shape, &c_shape);\n-  SE_DeviceMemoryBase region_base{region->opaque(), region->size(),\n-                                  region->payload()};\n+  SE_DeviceAddressBase region_base{region->opaque(), region->size(),\n+                                   region->payload()};\n   StatusHelper status;\n \n   stream_executor::tpu::ExecutorApiFn()"
        },
        {
            "sha": "f886f075260f745599a630a7d4564d2363d2aab0",
            "filename": "third_party/xla/xla/stream_executor/tpu/tpu_transfer_manager.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_transfer_manager.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/064ad759c42134977b7b66e02d49298037353aea/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_transfer_manager.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Ftpu%2Ftpu_transfer_manager.h?ref=064ad759c42134977b7b66e02d49298037353aea",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n #include \"xla/service/shaped_buffer.h\"\n #include \"xla/service/transfer_manager.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -86,13 +86,13 @@ class TpuTransferManager : public xla::TpuTransferManagerInterface {\n \n   bool CanBufferBeAccessedNow(\n       se::StreamExecutor* executor,\n-      const se::DeviceMemoryBase& device_buffer) const override;\n+      const se::DeviceAddressBase& device_buffer) const override;\n \n   absl::Status WriteSingleTupleIndexTable(\n       stream_executor::Stream* stream,\n-      absl::Span<const stream_executor::DeviceMemoryBase> elements,\n+      absl::Span<const stream_executor::DeviceAddressBase> elements,\n       const xla::Shape& shape,\n-      stream_executor::DeviceMemoryBase* region) override;\n+      stream_executor::DeviceAddressBase* region) override;\n \n   absl::Status LinearizeToBuffers(\n       const xla::LiteralSlice& literal, const xla::Shape& device_shape,"
        }
    ],
    "stats": {
        "total": 5391,
        "additions": 2740,
        "deletions": 2651
    }
}