{
    "author": "ezhulenev",
    "message": "[xla:gpu] Extract collective clique requests and acquire into separate libraries\n\nThis is an NFC change to decouple collective execution from `xla::gpu::Thunk`, to be able to pass Collective communicators to FFI handlers. This change simply moves classes out of `Thunk` to separate targets.\n\nPiperOrigin-RevId: 836424405",
    "sha": "5d46b65af45d5694cb1676bc872d24a4a64a6b57",
    "files": [
        {
            "sha": "c801ae3d201c49ce992d2cb554ac0ca20d1fa260",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 87,
            "deletions": 4,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -294,7 +294,6 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu:resource_requests\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:command_buffer\",\n         \"//xla/stream_executor:device_memory\",\n@@ -748,7 +747,6 @@ xla_test(\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/service/gpu:resource_requests\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -1564,15 +1562,99 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"collective_clique_requests\",\n+    srcs = [\"collective_clique_requests.cc\"],\n+    hdrs = [\"collective_clique_requests.h\"],\n+    deps = [\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"collective_clique_requests_test\",\n+    srcs = [\"collective_clique_requests_test.cc\"],\n+    deps = [\n+        \":collective_clique_requests\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/service:global_device_id\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/platform:test_main\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"collective_params\",\n+    srcs = [\"collective_params.cc\"],\n+    hdrs = [\"collective_params.h\"],\n+    deps = [\n+        \":collective_clique_requests\",\n+        \"//xla:executable_run_options\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/backends/gpu/collectives:gpu_collectives\",\n+        \"//xla/runtime:device_id\",\n+        \"//xla/service:computation_placer\",\n+        \"//xla/service:executable\",\n+        \"//xla/service:global_device_id\",\n+        \"//xla/service/gpu:gpu_executable_run_options\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"collective_cliques\",\n+    srcs = [\"collective_cliques.cc\"],\n+    hdrs = [\"collective_cliques.h\"],\n+    deps = [\n+        \":collective_params\",\n+        \"//xla:util\",\n+        \"//xla/backends/gpu/collectives:gpu_clique\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/backends/gpu/collectives:gpu_cliques\",\n+        \"//xla/backends/gpu/collectives:gpu_collectives\",\n+        \"//xla/backends/gpu/runtime:collective_clique_requests\",\n+        \"//xla/core/collectives:communicator\",\n+        \"//xla/core/collectives:rank_id\",\n+        \"//xla/service/gpu:gpu_executable_run_options\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n+    ],\n+)\n+\n cc_library(\n     name = \"collective_thunk\",\n     srcs = [\"collective_thunk.cc\"],\n     hdrs = [\"collective_thunk.h\"],\n     deps = [\n+        \":collective_cliques\",\n+        \":collective_params\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n         \"//xla:debug_options_flags\",\n         \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n@@ -1950,6 +2032,8 @@ cc_library(\n     srcs = [\"thunk.cc\"],\n     hdrs = [\"thunk.h\"],\n     deps = [\n+        \":collective_cliques\",\n+        \":collective_params\",\n         \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla:executable_run_options\",\n@@ -1958,6 +2042,7 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_cliques\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n+        \"//xla/backends/gpu/runtime:collective_clique_requests\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/ffi:execution_context\",\n@@ -3233,7 +3318,6 @@ xla_test(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:executable\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/service/gpu:resource_requests\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:mock_stream\",\n@@ -3304,7 +3388,6 @@ xla_test(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:executable\",\n         \"//xla/service/gpu:buffer_allocations\",\n-        \"//xla/service/gpu:resource_requests\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\","
        },
        {
            "sha": "3b940cf7df778a70580a6812c428bf892b7080fa",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -142,10 +142,9 @@ CollectiveOpGroupMode AllReduceStartThunk::GetGroupMode(\n   return GetGroupModeInst(inst);\n }\n \n-absl::Status AllReduceStartThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n-  TF_RETURN_IF_ERROR(CollectiveThunk::Prepare(params, resource_requests));\n-  return collective_kernel_thunk_->Prepare(params, resource_requests);\n+absl::Status AllReduceStartThunk::Prepare(const PrepareParams& params) {\n+  TF_RETURN_IF_ERROR(CollectiveThunk::Prepare(params));\n+  return collective_kernel_thunk_->Prepare(params);\n }\n \n absl::Status AllReduceStartThunk::Initialize(const InitializeParams& params) {"
        },
        {
            "sha": "d3e06aa6489a601e9afda0a5abc1691e4ddaf273",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -79,8 +79,7 @@ class AllReduceStartThunk : public AllReduceReduceScatterThunkBase {\n   static CollectiveOpGroupMode GetGroupMode(\n       const HloAllReduceInstruction* inst);\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n \n  protected:"
        },
        {
            "sha": "7f579374c5a0a1af24332427ac48dee0ee624b74",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/service/gpu/resource_requests.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -152,7 +151,6 @@ TEST_F(BuffersDebugChecksumThunkTest, CalculatesChecksums) {\n   Thunk::InitializeParams init_params;\n   init_params.executor = executor_;\n   init_params.stream = stream_.get();\n-  ResourceRequests resource_requests;\n   auto execute_params = Thunk::ExecuteParams::Create(\n       ServiceExecutableRunOptions(), allocations, stream_.get(),\n       /*command_buffer_trace_stream=*/stream_.get(),\n@@ -165,7 +163,7 @@ TEST_F(BuffersDebugChecksumThunkTest, CalculatesChecksums) {\n       {{/*buffer_idx=*/0, inputs[0]}, {/*buffer_idx=*/1, inputs[1]}},\n       /*runs_before_checked_thunk=*/true, metadata_store);\n   TF_ASSERT_OK(thunk.Initialize(init_params));\n-  TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}, resource_requests));\n+  TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}));\n   TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));\n   TF_ASSERT_OK_AND_ASSIGN(std::vector<BufferDebugLogEntry> entries,\n                           device_log.ReadFromDevice(*stream_));"
        },
        {
            "sha": "cc2eb8eca41cd4e57f0629f32ef5a3d6ac831fc6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/service/gpu/resource_requests.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n@@ -151,7 +150,6 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n   Thunk::InitializeParams init_params;\n   init_params.executor = executor_;\n   init_params.stream = stream_.get();\n-  ResourceRequests resource_requests;\n   auto execute_params = Thunk::ExecuteParams::Create(\n       ServiceExecutableRunOptions(), allocations, stream_.get(),\n       /*command_buffer_trace_stream=*/stream_.get(),\n@@ -165,7 +163,7 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n       {{/*buffer_idx=*/0, inputs[0]}, {/*buffer_idx=*/1, inputs[1]}},\n       metadata_store);\n   TF_ASSERT_OK(thunk.Initialize(init_params));\n-  TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}, resource_requests));\n+  TF_ASSERT_OK(thunk.Prepare(Thunk::PrepareParams{}));\n   TF_ASSERT_OK(thunk.ExecuteOnStream(execute_params));\n   TF_ASSERT_OK_AND_ASSIGN(std::vector<BufferDebugFloatCheckEntry> entries,\n                           device_log.ReadFromDevice(*stream_));"
        },
        {
            "sha": "5a550acbe82a6a35b99fde1d062b8fe49318a552",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_clique_requests.cc",
            "status": "added",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -0,0 +1,74 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+\n+#include <cstdint>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+\n+namespace xla::gpu {\n+\n+absl::Status CollectiveCliqueRequests::RequestClique(\n+    const GpuCliqueKey& clique_key) {\n+  VLOG(5) << \"Add collective clique request: \" << clique_key.ToString();\n+\n+  // XLA compiler guarantees that all collective operations have the same\n+  // order on all replicas. We rely on this property to assign unique id to\n+  // clique requests simply based on the number of already recorded requests.\n+  int64_t id = cliques_.size();\n+  cliques_.try_emplace(clique_key, CliqueRequest{clique_key, id});\n+  return absl::OkStatus();\n+}\n+\n+std::vector<GpuCliqueKey> CollectiveCliqueRequests::RequestedCliques() const {\n+  std::vector<GpuCliqueKey> clique_keys;\n+  clique_keys.reserve(cliques_.size());\n+  for (const auto& [key, _] : cliques_) {\n+    clique_keys.push_back(key);\n+  }\n+\n+  return clique_keys;\n+}\n+\n+std::vector<CollectiveCliqueRequests::CliqueRequest>\n+CollectiveCliqueRequests::OrderedRequestedCliques() const {\n+  std::vector<CliqueRequest> cliques;\n+  cliques.reserve(cliques_.size());\n+  for (const auto& [_, request] : cliques_) {\n+    cliques.push_back(request);\n+  }\n+\n+  absl::c_sort(cliques, [](const CliqueRequest& a, const CliqueRequest& b) {\n+    // Acquire larger cliques first to be able to split them later.\n+    if (a.key.devices().size() > b.key.devices().size()) {\n+      return true;\n+    }\n+    if (b.key.devices().size() > a.key.devices().size()) {\n+      return false;\n+    }\n+\n+    // Prefer cliques with smaller id (comes earlier in execution order).\n+    return a.id < b.id;\n+  });\n+\n+  return cliques;\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "c75041d99b5118d894225a5921e75c2547054e60",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_clique_requests.h",
            "status": "added",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -0,0 +1,85 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_CLIQUE_REQUESTS_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_CLIQUE_REQUESTS_H_\n+\n+#include <cstdint>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+\n+namespace xla::gpu {\n+\n+// Collective thunks (including collective FFI calls) can request communicators\n+// for various collective clieques. XLA runtime is responsible for collecting\n+// such requests during the prepare stage and acquiring the cliques during the\n+// initialize stage.\n+class CollectiveCliqueRequests {\n+ public:\n+  // For each requested clique key, we also assign a monotonically increasing\n+  // id, that allows us to deterministically order clique requests.\n+  //\n+  // Example: 8 ranks splitted in different groups of communicators\n+  //\n+  // Group #0: [0,1], [2,3], [4,5], [6,7]\n+  // Group #1: [0,4], [1,5], [2,6], [3,7]\n+  //\n+  // Both groups #0 and #1 can be acqured by splitting [0...7] clique. To avoid\n+  // deadlocks all participants should acquire all cliques in a group #0 before\n+  // acquiring any cliques in a group #1.\n+  //\n+  // We rely on clique request id to guarantee that the order is identical\n+  // on all participating ranks (including ranks running on different hosts).\n+  //\n+  // Remember that clique requests are collected independently by running thunk\n+  // sequence prepare stage in parallel for all ranks. After all requests are\n+  // collected, XLA runtime initializes communicators for all requested cliques.\n+  //\n+  // This initialization must happen in identical order across all ranks, and\n+  // ranks might be running as separate processes or even on separate hosts, so\n+  // any communication between ranks is impossible.\n+  //\n+  // We rely on the fact, that XLA uses SPMD programming model, and all ranks\n+  // execute identical thunk sequence in exact same order, and assigned request\n+  // id is essentially a thunk index in the parent thunk sequence. However we\n+  // don't want to sort just by request index, because acquiring large cliques\n+  // first improves performance and memory utilization, as we have more chances\n+  // to reuse existing communicators when requesting smaller cliques via\n+  // communicator splitting.\n+  struct CliqueRequest {\n+    GpuCliqueKey key;\n+    int64_t id;\n+  };\n+\n+  // Adds a clique key to the list of requested cliques.\n+  absl::Status RequestClique(const GpuCliqueKey& clique_key);\n+\n+  // Returns all requested cliques in undefined order.\n+  std::vector<GpuCliqueKey> RequestedCliques() const;\n+\n+  // Returns all requested cliques in a deterministic order optimized for\n+  // efficient communicator acquisition.\n+  std::vector<CliqueRequest> OrderedRequestedCliques() const;\n+\n+ private:\n+  absl::flat_hash_map<GpuCliqueKey, CliqueRequest> cliques_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_CLIQUE_REQUESTS_H_"
        },
        {
            "sha": "d35cb0c8f0f4d985d3ec4eb6a821e8a56ac4489b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_clique_requests_test.cc",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_clique_requests_test.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -0,0 +1,53 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+\n+#include <vector>\n+\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/service/global_device_id.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla::gpu {\n+\n+TEST(CollectiveCliqueRequestsTest, OrderedRequests) {\n+  GlobalDeviceId d0 = GlobalDeviceId(0);\n+  GlobalDeviceId d1 = GlobalDeviceId(1);\n+  GlobalDeviceId d2 = GlobalDeviceId(2);\n+  GlobalDeviceId d3 = GlobalDeviceId(3);\n+\n+  GpuCliqueKey k0({d2, d3}, 2);\n+  GpuCliqueKey k1({d0, d1}, 2);\n+  GpuCliqueKey k2({d0, d1, d2, d3}, 4);\n+\n+  CollectiveCliqueRequests requests;\n+  TF_ASSERT_OK(requests.RequestClique(k0));\n+  TF_ASSERT_OK(requests.RequestClique(k1));\n+  TF_ASSERT_OK(requests.RequestClique(k2));\n+\n+  // Check that we acquire larger cliques first, and then cliques with smaller\n+  // id first, as acquiring cliques according to natural clique key order might\n+  // lead to deadlocks during communicator splitting.\n+  auto ordered_requests = requests.OrderedRequestedCliques();\n+  ASSERT_EQ(ordered_requests.size(), 3);\n+  EXPECT_EQ(ordered_requests[0].key, k2);\n+  EXPECT_EQ(ordered_requests[1].key, k0);\n+  EXPECT_EQ(ordered_requests[2].key, k1);\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "25349c3c18f332256c4553d5619c98244e40983a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_cliques.cc",
            "status": "renamed",
            "additions": 55,
            "deletions": 56,
            "changes": 111,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_cliques.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_cliques.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_cliques.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -1,4 +1,4 @@\n-/* Copyright 2024 The OpenXLA Authors.\n+/* Copyright 2025 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n@@ -13,7 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/service/gpu/resource_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n \n #include <cstddef>\n #include <cstdint>\n@@ -22,26 +22,25 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n+#include \"absl/base/no_destructor.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n-#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n+#include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/service/gpu/gpu_executable_run_options.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n-#include \"tsl/profiler/lib/traceme_encode.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n \n namespace {\n \n@@ -52,36 +51,57 @@ struct PersistentCliquesMap {\n };\n \n static PersistentCliquesMap& GetPersistentCliquesMap() {\n-  static auto* const persistent_cliques = new PersistentCliquesMap();\n+  static absl::NoDestructor<PersistentCliquesMap> persistent_cliques;\n   return *persistent_cliques;\n }\n }  // namespace\n \n-absl::Status ResourceRequests::AddClique(const GpuCliqueKey& clique_key) {\n-  VLOG(5) << \"Add collective clique request: \" << clique_key.ToString();\n+CollectiveCliques::CollectiveCliques(AcquiredCliquesMap cliques_map,\n+                                     int32_t num_transient_cliques)\n+    : cliques_map_(std::move(cliques_map)),\n+      num_transient_cliques_(num_transient_cliques) {}\n+\n+absl::StatusOr<Communicator*> CollectiveCliques::GetComm(\n+    const GpuCliqueKey& clique_key, RankId rank) const {\n+  // Check that we locked access to a clique for `clique_key`.\n+  auto clique = cliques_map_.find(clique_key);\n+  if (clique == cliques_map_.end()) {\n+    return NotFound(\"No clique found for clique key: %s\",\n+                    clique_key.ToString());\n+  }\n+\n+  // Check that clique has a communicator for our rank.\n+  auto communicator = (*clique->second)->comm(rank);\n+  if (!communicator.has_value()) {\n+    return Internal(\"Communicator for rank %d not found in a NCCL clique %s\",\n+                    rank.value(), clique_key.ToString());\n+  }\n \n-  // XLA compiler guarantees that all collective operations have the same\n-  // order on all replicas. We rely on this property to assign unique id to\n-  // clique requests simply based on the number of already recorded requests.\n-  int64_t id = cliques_.size();\n-  cliques_.try_emplace(clique_key, CliqueRequest{clique_key, id});\n-  return absl::OkStatus();\n+  return *communicator;\n }\n \n-std::vector<GpuCliqueKey> ResourceRequests::CliqueKeys() const {\n-  std::vector<GpuCliqueKey> clique_keys;\n-  for (const auto& [key, unused] : cliques_) {\n-    clique_keys.push_back(key);\n+absl::StatusOr<bool> CollectiveCliques::peer_access_enabled(\n+    const GpuCliqueKey& clique_key) const {\n+  // Check that we locked access to a clique for `clique_key`.\n+  auto clique = cliques_map_.find(clique_key);\n+  if (clique == cliques_map_.end()) {\n+    return NotFound(\"No clique found for clique key: %s\",\n+                    clique_key.ToString());\n   }\n-  return clique_keys;\n+\n+  return (*clique->second)->peer_access_enabled();\n }\n \n-absl::StatusOr<Thunk::CollectiveCliques>\n-ResourceRequests::AcquireCollectiveCliques(\n-    const Thunk::CollectiveExecuteParams& params, bool use_persistent_cliques) {\n-  if (cliques_.empty()) return Thunk::CollectiveCliques();\n+absl::StatusOr<CollectiveCliques> AcquireCollectiveCliques(\n+    const CollectiveParams& params, const CollectiveCliqueRequests& cliques,\n+    bool use_persistent_cliques) {\n+  std::vector<CollectiveCliqueRequests::CliqueRequest> ordered_cliques =\n+      cliques.OrderedRequestedCliques();\n+  if (ordered_cliques.empty()) {\n+    return CollectiveCliques();\n+  }\n \n-  VLOG(2) << \"Acquire \" << cliques_.size()\n+  VLOG(2) << \"Acquire \" << ordered_cliques.size()\n           << \" collective cliques for global device id \"\n           << params.global_device_id.value()\n           << \"; run_id=\" << params.run_id.ToInt()\n@@ -90,9 +110,8 @@ ResourceRequests::AcquireCollectiveCliques(\n           << \"; max number of channels for p2p \" << params.p2p_max_nchannels\n           << \"; use_persistent_cliques=\" << use_persistent_cliques;\n \n-  std::vector<CliqueRequest> ordered_cliques = GetOrderedCliqueRequests();\n   for (size_t i = 0; i < ordered_cliques.size(); ++i) {\n-    const CliqueRequest& r = ordered_cliques[i];\n+    const CollectiveCliqueRequests::CliqueRequest& r = ordered_cliques[i];\n     VLOG(2) << \"  clique #\" << i << \" (for global device id \"\n             << params.global_device_id.value() << \")\"\n             << \": num_local_participants=\" << r.key.num_local_participants()\n@@ -102,7 +121,7 @@ ResourceRequests::AcquireCollectiveCliques(\n   tsl::profiler::TraceMe trace([&] {\n     return tsl::profiler::TraceMeEncode(\n         \"AcquireCollectiveCliques\",\n-        {{\"num_cliques\", cliques_.size()},\n+        {{\"num_cliques\", ordered_cliques.size()},\n          {\"use_persistent_cliques\", use_persistent_cliques}});\n   });\n \n@@ -111,13 +130,12 @@ ResourceRequests::AcquireCollectiveCliques(\n   AcquiredCliquesMap cliques_map;\n   int32_t num_transient_cliques = 0;\n \n-  for (const CliqueRequest& r : ordered_cliques) {\n+  for (const CollectiveCliqueRequests::CliqueRequest& r : ordered_cliques) {\n     std::optional<RankId> rank = r.key.rank(params.global_device_id);\n \n     if (!rank.has_value()) {\n-      return absl::InternalError(absl::StrCat(\n-          \"Can't find global device id \", params.global_device_id.value(),\n-          \" in clique key \", r.key.ToString()));\n+      return Internal(\"Can't find global device id %d in clique key %s\",\n+                      params.global_device_id.value(), r.key.ToString());\n     }\n \n     TF_ASSIGN_OR_RETURN(const CliqueIdCallback* clique_id_callback,\n@@ -174,26 +192,7 @@ ResourceRequests::AcquireCollectiveCliques(\n           << \"; run_id=\" << params.run_id.ToInt()\n           << \"; num_transient_cliques=\" << num_transient_cliques;\n \n-  return Thunk::CollectiveCliques(std::move(cliques_map),\n-                                  num_transient_cliques);\n+  return CollectiveCliques(std::move(cliques_map), num_transient_cliques);\n }\n \n-std::vector<ResourceRequests::CliqueRequest>\n-ResourceRequests::GetOrderedCliqueRequests() {\n-  std::vector<CliqueRequest> cliques;\n-  cliques.reserve(cliques_.size());\n-  for (const auto& [_, request] : cliques_) cliques.push_back(request);\n-\n-  absl::c_sort(cliques, [](const CliqueRequest& a, const CliqueRequest& b) {\n-    // Acquire larger cliques first to be able to split them later.\n-    if (a.key.devices().size() > b.key.devices().size()) return true;\n-    if (b.key.devices().size() > a.key.devices().size()) return false;\n-\n-    // Prefer cliques with smaller id (comes earlier in execution order).\n-    return a.id < b.id;\n-  });\n-\n-  return cliques;\n-}\n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu",
            "previous_filename": "third_party/xla/xla/service/gpu/resource_requests.cc"
        },
        {
            "sha": "e2e31ba2664723784999836be8e39c35df85f24f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_cliques.h",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_cliques.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_cliques.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_cliques.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -0,0 +1,69 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_CLIQUES_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_CLIQUES_H_\n+\n+#include <cstdint>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n+#include \"xla/core/collectives/communicator.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+\n+namespace xla::gpu {\n+\n+// A collection of collective cliques acquired based on GPU clique requests\n+// collected from all thunks at prepare stage.\n+class CollectiveCliques {\n+ public:\n+  CollectiveCliques() = default;\n+  CollectiveCliques(AcquiredCliquesMap cliques_map,\n+                    int32_t num_transient_cliques);\n+\n+  absl::StatusOr<Communicator*> GetComm(const GpuCliqueKey& clique_key,\n+                                        RankId rank) const;\n+\n+  // Returns whether peer device memory access is possible between all devices\n+  // in the clique.\n+  absl::StatusOr<bool> peer_access_enabled(\n+      const GpuCliqueKey& clique_key) const;\n+\n+  bool empty() const { return cliques_map_.empty(); }\n+\n+  bool num_transient_cliques() const { return num_transient_cliques_; }\n+\n+ private:\n+  AcquiredCliquesMap cliques_map_;\n+\n+  // The number of acquired non-persistent clique. We need to keep track of\n+  // newly created communicators to insert rendezvous after first\n+  // initialization, because otherwise we observe deadlocks with NCCL\n+  // collectives backends.\n+  int32_t num_transient_cliques_ = 0;\n+};\n+\n+// Acquires collective cliques using the given collective parameters for all\n+// requested GPU cliques.\n+absl::StatusOr<CollectiveCliques> AcquireCollectiveCliques(\n+    const CollectiveParams& params, const CollectiveCliqueRequests& cliques,\n+    bool use_persistent_cliques);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_CLIQUES_H_"
        },
        {
            "sha": "2bd70e22d7a8b8da17892ab05045b319aaf45654",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_group_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -52,10 +52,9 @@ CollectiveGroupThunk::CollectiveGroupThunk(\n     thunks_.emplace_back(std::move(thunk));\n   }\n }\n-absl::Status CollectiveGroupThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status CollectiveGroupThunk::Prepare(const PrepareParams& params) {\n   for (const std::unique_ptr<Thunk>& thunk : thunks_) {\n-    TF_RETURN_IF_ERROR(thunk->Prepare(params, resource_requests));\n+    TF_RETURN_IF_ERROR(thunk->Prepare(params));\n   }\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "c7b392fb310ae7d21c4930c321e38b1e74bf7cd7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_group_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -39,8 +39,7 @@ class CollectiveGroupThunk : public Thunk {\n   CollectiveGroupThunk(const HloInstruction* instruction, Thunk::Kind kind,\n                        std::vector<std::unique_ptr<Thunk>> thunks,\n                        AsyncStreamKind stream_kind, ThunkId thunk_id);\n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status ExecuteOnStream(const Thunk::ExecuteParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   void ForAllThunks(absl::FunctionRef<void(const Thunk*)> fn) const override;"
        },
        {
            "sha": "e0071dd330d7a2c1af53646289b2496e8ad42f37",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -121,13 +121,13 @@ absl::StatusOr<bool> CollectiveKernelThunk::IsSupported(\n       collective_config_.operand_element_type[0], reduction_kind_, strategy);\n }\n \n-absl::Status CollectiveKernelThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status CollectiveKernelThunk::Prepare(const PrepareParams& params) {\n+  TF_RET_CHECK(params.collective_params != nullptr);\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n                                 /*use_nccl=*/false));\n-  return resource_requests.AddClique(clique_key);\n+  return params.clique_requests->RequestClique(clique_key);\n }\n \n int64_t CollectiveKernelThunk::GetInputSizeBytes() const {"
        },
        {
            "sha": "10803231b6ea70d166eb585adfef2d9c71bb1727",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -81,8 +81,7 @@ class CollectiveKernelThunk : public Thunk {\n       const CollectiveCliques* collective_cliques) const;\n \n   // The single host collective thunk actually requires a clique key.\n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) final;\n+  absl::Status Prepare(const PrepareParams& params) final;\n \n   // Allocate buffers and events as needed for cross device communication.\n   // If InitializeParams contains a PTX kernel, it will be used instead of the"
        },
        {
            "sha": "a386a9e9c8a928c97492b3313b9cde93f18889e2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -226,7 +226,7 @@ absl::StatusOr<se::DeviceMemoryBase> RunCollectiveKernelThunk(\n       &gpu_options);\n \n   TF_ASSIGN_OR_RETURN(auto collective_params,\n-                      Thunk::CollectiveExecuteParams::Create(\n+                      CollectiveParams::Create(\n                           run_options, /*async_streams=*/{},\n                           /*local_device_ordinal=*/executor->device_ordinal()));\n   std::vector<se::DeviceMemoryBase> allocated_buffers = {"
        },
        {
            "sha": "f5debb9d7850365d708d020dc11088d1f932321b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_params.cc",
            "status": "added",
            "additions": 114,
            "deletions": 0,
            "changes": 114,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_params.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_params.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_params.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -0,0 +1,114 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n+\n+#include <cstdint>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/executable_run_options.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/service/global_device_id.h\"\n+#include \"xla/service/gpu/gpu_executable_run_options.h\"\n+#include \"xla/service/service_executable_run_options.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+\n+using GlobalDeviceIdMap = CollectiveParams::GlobalDeviceIdMap;\n+\n+// Returns global device id for a local device ordinal or an error if global\n+// device id map is misconfigured and missing an entry for a local device.\n+static absl::StatusOr<GlobalDeviceId> GetGlobalDeviceId(\n+    const GlobalDeviceIdMap* device_id_map, int64_t local_device_ordinal) {\n+  // No local -> global mapping was provided; assume the identity mapping.\n+  if (!device_id_map) {\n+    return GlobalDeviceId(local_device_ordinal);\n+  }\n+\n+  // Find a global device id in a global device id map.\n+  auto it = device_id_map->find(local_device_ordinal);\n+  if (it == device_id_map->end()) {\n+    return absl::NotFoundError(\n+        absl::StrCat(\"No global device id found for local device ordinal: \",\n+                     local_device_ordinal));\n+  }\n+\n+  return it->second;\n+}\n+\n+absl::StatusOr<CollectiveParams> CollectiveParams::Create(\n+    const ServiceExecutableRunOptions& run_options,\n+    absl::Span<se::Stream* const> async_streams, int64_t local_device_ordinal,\n+    int64_t collective_max_nchannels, int64_t p2p_max_nchannels) {\n+  const GpuExecutableRunOptions* gpu_options =\n+      run_options.run_options().gpu_executable_run_options();\n+\n+  auto* collectives = gpu_options && gpu_options->collectives()\n+                          ? gpu_options->collectives()\n+                          : GpuCollectives::Default();\n+\n+  auto* device_id_map = gpu_options && gpu_options->gpu_global_device_ids()\n+                            ? &*gpu_options->gpu_global_device_ids()\n+                            : nullptr;\n+\n+  auto* clique_id_callback = gpu_options && gpu_options->clique_id_callback()\n+                                 ? &gpu_options->clique_id_callback()\n+                                 : nullptr;\n+\n+  auto* incarnations = gpu_options && gpu_options->incarnations().has_value()\n+                           ? &*gpu_options->incarnations()\n+                           : nullptr;\n+\n+  TF_ASSIGN_OR_RETURN(GlobalDeviceId global_device_id,\n+                      GetGlobalDeviceId(device_id_map, local_device_ordinal));\n+\n+  return CollectiveParams(collectives, run_options.stream()->parent(),\n+                          run_options.run_options().run_id(), async_streams,\n+                          local_device_ordinal, global_device_id,\n+                          run_options.run_options().device_assignment(),\n+                          device_id_map, clique_id_callback, incarnations,\n+                          collective_max_nchannels, p2p_max_nchannels);\n+}\n+\n+CollectiveParams::CollectiveParams(\n+    GpuCollectives* collectives, se::StreamExecutor* executor, RunId run_id,\n+    absl::Span<se::Stream* const> async_streams, int64_t local_device_ordinal,\n+    GlobalDeviceId global_device_id, const DeviceAssignment* device_assn,\n+    const GlobalDeviceIdMap* global_device_id_map,\n+    const CliqueIdCallback* nccl_clique_id_callback,\n+    const absl::flat_hash_map<GlobalDeviceId, IncarnationId>* incarnations,\n+    int64_t collective_max_nchannels, int64_t p2p_max_nchannels)\n+    : collectives(collectives),\n+      executor(executor),\n+      run_id(run_id),\n+      async_streams(async_streams.begin(), async_streams.end()),\n+      local_device_ordinal(local_device_ordinal),\n+      global_device_id(global_device_id),\n+      device_assn(device_assn),\n+      global_device_id_map(global_device_id_map),\n+      nccl_clique_id_callback(nccl_clique_id_callback),\n+      incarnations(incarnations),\n+      collective_max_nchannels(collective_max_nchannels),\n+      p2p_max_nchannels(p2p_max_nchannels) {}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "b694e06358fd1c6977dbca4095822be382fb600a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_params.h",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_params.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_params.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_params.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -0,0 +1,89 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_PARAMS_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_PARAMS_H_\n+\n+#include <cstdint>\n+#include <map>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/executable_run_options.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/service/global_device_id.h\"\n+#include \"xla/service/gpu/gpu_executable_run_options.h\"\n+#include \"xla/service/service_executable_run_options.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla::gpu {\n+\n+// Parameters capturing all the details required for collective execution of\n+// XLA executables (multiple partitions and replicas).\n+struct CollectiveParams {\n+  // A mapping from local device ordinals to global device IDs.\n+  //\n+  // TODO(ezhulenev): Use type safe RankId instead of int32_t.\n+  using GlobalDeviceIdMap = std::map<int32_t, GlobalDeviceId>;\n+\n+  // Creates NCCL execution parameters from the run options for the given\n+  // local device. Returns an error if run options are misconfigured (i.e.\n+  // missing a global device mapping for a local device ordinal).\n+  static absl::StatusOr<CollectiveParams> Create(\n+      const ServiceExecutableRunOptions& run_options,\n+      absl::Span<se::Stream* const> async_streams, int64_t local_device_ordinal,\n+      int64_t collective_max_nchannels = 0, int64_t p2p_max_nchannels = 0);\n+\n+  GpuCollectives* collectives;\n+  se::StreamExecutor* executor;\n+\n+  // XLA execution run id allows us to distinguish collective operations\n+  // from different concurrent executions and avoid deadlocks.\n+  RunId run_id;\n+\n+  // Streams for asynchronous collective communications.\n+  absl::InlinedVector<se::Stream*, 4> async_streams;\n+\n+  int64_t local_device_ordinal;\n+  GlobalDeviceId global_device_id;\n+\n+  const DeviceAssignment* device_assn;\n+  const GlobalDeviceIdMap* global_device_id_map;\n+  const CliqueIdCallback* nccl_clique_id_callback;\n+  const absl::flat_hash_map<GlobalDeviceId, IncarnationId>* incarnations;\n+\n+  int64_t collective_max_nchannels;\n+  int64_t p2p_max_nchannels;\n+\n+  bool need_barrier = false;\n+\n+ private:\n+  CollectiveParams(\n+      GpuCollectives* collectives, se::StreamExecutor* executor, RunId run_id,\n+      absl::Span<se::Stream* const> async_streams, int64_t local_device_ordinal,\n+      GlobalDeviceId global_device_id, const DeviceAssignment* device_assn,\n+      const GlobalDeviceIdMap* global_device_id_map,\n+      const CliqueIdCallback* nccl_clique_id_callback,\n+      const absl::flat_hash_map<GlobalDeviceId, IncarnationId>* incarnations,\n+      int64_t collective_max_nchannels, int64_t p2p_max_nchannels);\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_PARAMS_H_"
        },
        {
            "sha": "b15da2527fea59797d2109e85e51f821a17bb600",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -50,6 +50,7 @@ limitations under the License.\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape.h\"\n+#include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -99,7 +100,7 @@ bool IsTypeSupportedBy(PrimitiveType element_type, Thunk::Kind reduction_op) {\n }\n \n int64_t GetNumLocalParticipants(\n-    const Thunk::CollectiveExecuteParams& params,\n+    const CollectiveParams& params,\n     const std::vector<GlobalDeviceId>& participants) {\n   if (!params.global_device_id_map) {\n     return participants.size();\n@@ -242,7 +243,7 @@ CollectiveThunk::CollectiveThunk(Kind kind, ThunkInfo thunk_info, bool is_sync,\n       async_events_(is_sync ? nullptr : std::make_shared<AsyncEvents>()) {}\n \n absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n-    GpuCollectives* collectives, const Thunk::CollectiveExecuteParams& params,\n+    GpuCollectives* collectives, const CollectiveParams& params,\n     const std::vector<ReplicaGroup>& replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n     bool use_nccl) {\n@@ -315,8 +316,8 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n }\n \n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n-    const Thunk::CollectiveExecuteParams& params,\n-    const CollectiveConfig& collective_config, bool use_nccl) {\n+    const CollectiveParams& params, const CollectiveConfig& collective_config,\n+    bool use_nccl) {\n   TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n                       CollectiveThunk::GetGpuCollectives(params));\n   return GetGpuCliqueKey(collectives, params, collective_config.replica_groups,\n@@ -326,8 +327,8 @@ absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n }\n \n absl::StatusOr<CommunicatorHandle> GetComm(\n-    GpuCollectives* collectives, const Thunk::CollectiveExecuteParams& params,\n-    const Thunk::CollectiveCliques& collective_cliques,\n+    GpuCollectives* collectives, const CollectiveParams& params,\n+    const CollectiveCliques& collective_cliques,\n     const std::vector<ReplicaGroup>& replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind) {\n   TF_ASSIGN_OR_RETURN(GpuCliqueKey clique_key,\n@@ -426,15 +427,15 @@ absl::StatusOr<se::Event*> CollectiveThunk::AsyncEvents::GetEvent(\n   return event->second.get();\n }\n \n-absl::Status CollectiveThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status CollectiveThunk::Prepare(const PrepareParams& params) {\n+  TF_RET_CHECK(params.collective_params != nullptr);\n   TF_ASSIGN_OR_RETURN(GpuCollectives * collectives, GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetGpuCliqueKey(collectives, *params.collective_params,\n                       config().replica_groups, config().group_mode,\n                       GetAsyncStreamKind()));\n-  return resource_requests.AddClique(clique_key);\n+  return params.clique_requests->RequestClique(clique_key);\n }\n \n absl::Status CollectiveThunk::Initialize(const InitializeParams& params) {\n@@ -539,7 +540,7 @@ absl::StatusOr<std::vector<Communicator*>> CollectiveThunk::GetCommunicators(\n }\n \n std::string CollectiveThunk::GetDeviceString(\n-    const Thunk::CollectiveExecuteParams& collective_params) {\n+    const CollectiveParams& collective_params) {\n   GlobalDeviceId global_device_id = collective_params.global_device_id;\n   DeviceAssignment::LogicalID logical_id =\n       collective_params.device_assn->LogicalIdForDevice(global_device_id)"
        },
        {
            "sha": "bcccf2f735e4ac9315780b5353c447b3d999bc83",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -1,3 +1,4 @@\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n /* Copyright 2019 The OpenXLA Authors.\n \n@@ -32,6 +33,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n@@ -133,11 +135,9 @@ class CollectiveThunk : public Thunk {\n   };\n \n   // Logging support.\n-  static std::string GetDeviceString(\n-      const Thunk::CollectiveExecuteParams& params);\n+  static std::string GetDeviceString(const CollectiveParams& params);\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n \n   absl::Status Initialize(const InitializeParams& params) override;\n \n@@ -276,20 +276,20 @@ absl::Status AddOpDescription(absl::Status status, OpT op,\n //===----------------------------------------------------------------------===//\n \n absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n-    GpuCollectives* collectives, const Thunk::CollectiveExecuteParams& params,\n+    GpuCollectives* collectives, const CollectiveParams& params,\n     const std::vector<ReplicaGroup>& replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n     bool use_nccl = true);\n \n // Helper over GetGpuCliqueKey that builds key for AsyncStreamKind::kCollective.\n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n-    const CollectiveThunk::CollectiveExecuteParams& params,\n-    const CollectiveConfig& collective_config, bool use_nccl = true);\n+    const CollectiveParams& params, const CollectiveConfig& collective_config,\n+    bool use_nccl = true);\n \n // Returns a communicator and additional information about the clique.\n absl::StatusOr<CommunicatorHandle> GetComm(\n-    GpuCollectives* collectives, const Thunk::CollectiveExecuteParams& params,\n-    const Thunk::CollectiveCliques& collective_cliques,\n+    GpuCollectives* collectives, const CollectiveParams& params,\n+    const CollectiveCliques& collective_cliques,\n     const std::vector<ReplicaGroup>& replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind);\n "
        },
        {
            "sha": "54f391b3a621126a22114d94fe8cb7cdd9d582fd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -457,10 +457,9 @@ CommandBufferCmdExecutor::CommandBufferCmdExecutor(\n }\n \n absl::Status CommandBufferCmdExecutor::Prepare(\n-    const Thunk::PrepareParams& params,\n-    Thunk::ResourceRequestsInterface& resource_requests) {\n+    const Thunk::PrepareParams& params) {\n   for (auto& command : commands_) {\n-    TF_RETURN_IF_ERROR(command->Prepare(params, resource_requests));\n+    TF_RETURN_IF_ERROR(command->Prepare(params));\n   }\n   return absl::OkStatus();\n }\n@@ -1552,11 +1551,9 @@ absl::Status WhileCmd::Initialize(const Thunk::InitializeParams& params,\n   return absl::OkStatus();\n }\n \n-absl::Status WhileCmd::Prepare(\n-    const Thunk::PrepareParams& params,\n-    Thunk::ResourceRequestsInterface& resource_requests) {\n-  TF_RETURN_IF_ERROR(cond_commands_.Prepare(params, resource_requests));\n-  TF_RETURN_IF_ERROR(body_commands_.Prepare(params, resource_requests));\n+absl::Status WhileCmd::Prepare(const Thunk::PrepareParams& params) {\n+  TF_RETURN_IF_ERROR(cond_commands_.Prepare(params));\n+  TF_RETURN_IF_ERROR(body_commands_.Prepare(params));\n   return absl::OkStatus();\n }\n \n@@ -2062,17 +2059,16 @@ CollectiveCmd::CollectiveCmd(\n       config_(std::move(config)),\n       async_events_(std::move(async_events)) {}\n \n-absl::Status CollectiveCmd::Prepare(\n-    const Thunk::PrepareParams& params,\n-    Thunk::ResourceRequestsInterface& resource_requests) {\n+absl::Status CollectiveCmd::Prepare(const Thunk::PrepareParams& params) {\n+  TF_RET_CHECK(params.collective_params != nullptr);\n   TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n                       Thunk::GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetGpuCliqueKey(collectives, *params.collective_params,\n                       config().replica_groups, config().group_mode,\n                       AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));\n-  return resource_requests.AddClique(clique_key);\n+  return params.clique_requests->RequestClique(clique_key);\n }\n \n absl::StatusOr<const se::CommandBuffer::Command*>\n@@ -2592,8 +2588,7 @@ absl::Status DynamicSliceFusionCmd::Initialize(\n }\n \n absl::Status DynamicSliceFusionCmd::Prepare(\n-    const Thunk::PrepareParams& params,\n-    Thunk::ResourceRequestsInterface& resource_requests) {\n+    const Thunk::PrepareParams& params) {\n   for (DynamicSliceThunk::SliceDef& slice : slices_) {\n     VLOG(3) << \"DynamicSliceFusionCmd: slice: \" << slice.ToString();\n     if (slice.offsets.has_value()) {\n@@ -2609,7 +2604,7 @@ absl::Status DynamicSliceFusionCmd::Prepare(\n                    slice.orig_shape->dimensions().size());\n     }\n   }\n-  TF_RETURN_IF_ERROR(embedded_commands_.Prepare(params, resource_requests));\n+  TF_RETURN_IF_ERROR(embedded_commands_.Prepare(params));\n   if (offset_as_function_of_indvar_metadata_ != std::nullopt) {\n     Indvar(this) =\n         HloEvaluator()"
        },
        {
            "sha": "96a8031d4e1627158f143b28a96fd353409c6a4d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -264,9 +264,7 @@ class CommandBufferCmd {\n \n   // Prepare command for execution by allowing command to request shared state\n   // required for recording (i.e. collective commands request cliques).\n-  virtual absl::Status Prepare(\n-      const Thunk::PrepareParams& params,\n-      Thunk::ResourceRequestsInterface& resource_requests) {\n+  virtual absl::Status Prepare(const Thunk::PrepareParams& params) {\n     return absl::OkStatus();\n   }\n \n@@ -423,8 +421,7 @@ class CommandBufferCmdExecutor {\n       SynchronizationMode synchronization_mode);\n \n   // Prepares all commands added to a sequence.\n-  absl::Status Prepare(const Thunk::PrepareParams& params,\n-                       Thunk::ResourceRequestsInterface& resource_requests);\n+  absl::Status Prepare(const Thunk::PrepareParams& params);\n \n   // Initializes all commands added to a sequence.\n   absl::Status Initialize(const Thunk::InitializeParams& params,\n@@ -874,9 +871,7 @@ class WhileCmd : public CommandBufferCmd {\n   absl::Status Initialize(const Thunk::InitializeParams& params,\n                           StateManager& state) override;\n \n-  absl::Status Prepare(\n-      const Thunk::PrepareParams& params,\n-      Thunk::ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const Thunk::PrepareParams& params) override;\n \n   absl::StatusOr<const se::CommandBuffer::Command*> Record(\n       const Thunk::ExecuteParams& execute_params,\n@@ -1076,9 +1071,7 @@ class CollectiveCmd : public CommandBufferCmd {\n   CollectiveCmd(CommandBufferCmdType cmd_type, CollectiveConfig config,\n                 std::shared_ptr<CollectiveThunk::AsyncEvents> async_events);\n \n-  absl::Status Prepare(\n-      const Thunk::PrepareParams& params,\n-      Thunk::ResourceRequestsInterface& resource_requests) final;\n+  absl::Status Prepare(const Thunk::PrepareParams& params) final;\n \n   bool requires_initialization() override { return true; }\n \n@@ -1257,9 +1250,7 @@ class DynamicSliceFusionCmd : public CommandBufferCmd {\n   absl::Status Initialize(const Thunk::InitializeParams& params,\n                           StateManager& state) override;\n \n-  absl::Status Prepare(\n-      const Thunk::PrepareParams& params,\n-      Thunk::ResourceRequestsInterface& resource_requests) final;\n+  absl::Status Prepare(const Thunk::PrepareParams& params) final;\n \n   absl::StatusOr<const se::CommandBuffer::Command*> Record(\n       const Thunk::ExecuteParams& execute_params,"
        },
        {
            "sha": "3f37e098c9b16511abb83034c680a6d25a576259",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -118,20 +118,19 @@ CommandBufferThunk::ExecutorCommandBuffer::UpdateBufferAllocations(\n   return updated_allocs;\n }\n \n-absl::Status CommandBufferThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status CommandBufferThunk::Prepare(const PrepareParams& params) {\n   // We might end up with empty command sequence if all of the captured fusions\n   // are no-op (e.g. memcpy of size 0) and we have no emitted thunks for them.\n   if (commands_.empty()) {\n     return absl::OkStatus();\n   }\n \n-  TF_RETURN_IF_ERROR(commands_.Prepare(params, resource_requests));\n+  TF_RETURN_IF_ERROR(commands_.Prepare(params));\n \n   // Always prepare thunks if they are present so we are ready to fall back\n   // on them if we detect profiling activity.\n   if (thunks_) {\n-    TF_RETURN_IF_ERROR(thunks_->Prepare(params, resource_requests));\n+    TF_RETURN_IF_ERROR(thunks_->Prepare(params));\n   }\n \n   return absl::OkStatus();"
        },
        {
            "sha": "cfeeda00584e9ff72947659c16dc1f6905fef234",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -45,8 +45,7 @@ class CommandBufferThunk : public Thunk {\n \n   const std::unique_ptr<SequentialThunk>& thunks() const { return thunks_; }\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n "
        },
        {
            "sha": "81d3128db6a1815447afc624045891f78d067773",
            "filename": "third_party/xla/xla/backends/gpu/runtime/conditional_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -57,15 +57,14 @@ ConditionalThunk::ConditionalThunk(\n       branch_thunks_(std::move(branch_thunks)),\n       branch_index_is_bool_(branch_index_is_bool) {}\n \n-absl::Status ConditionalThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status ConditionalThunk::Prepare(const PrepareParams& params) {\n   if (branch_index_is_bool_) {\n     TF_RET_CHECK(branch_thunks_.size() == 2);\n   } else {\n     TF_RET_CHECK(!branch_thunks_.empty());\n   }\n   for (auto& branch_thunk : branch_thunks_) {\n-    TF_RETURN_IF_ERROR(branch_thunk->Prepare(params, resource_requests));\n+    TF_RETURN_IF_ERROR(branch_thunk->Prepare(params));\n   }\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "bf1b73bec3a2a0a26ba373be5804a03e0ca2fb70",
            "filename": "third_party/xla/xla/backends/gpu/runtime/conditional_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -58,8 +58,7 @@ class ConditionalThunk : public Thunk {\n   ConditionalThunk(const ConditionalThunk&) = delete;\n   ConditionalThunk& operator=(const ConditionalThunk&) = delete;\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n "
        },
        {
            "sha": "973fc5bcc0f0f9c7a8b168e17a97a3fceafc0510",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -458,8 +458,7 @@ absl::Status CustomCallThunk::ExecuteFfiHandler(\n   return Call(handler, *call_frame, options, stage);\n }\n \n-absl::Status CustomCallThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status CustomCallThunk::Prepare(const PrepareParams& params) {\n   if (bundle_.has_value()) {\n     const RunId run_id =\n         params.collective_params ? params.collective_params->run_id : RunId{-1};"
        },
        {
            "sha": "f037257200b26781c39cd33ce193c823d23f1b3d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -124,8 +124,7 @@ class CustomCallThunk : public Thunk {\n       xla::ffi::AttributesMap attributes,\n       const HloComputation* called_computation);\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n "
        },
        {
            "sha": "17760b9641897f4766f7a5c261a1e64fa74c5f3a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -44,7 +44,6 @@ limitations under the License.\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/service/gpu/resource_requests.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n@@ -243,7 +242,6 @@ TEST(CustomCallThunkTest, CustomCallWithOwnedHandlers) {\n   });\n   se::StreamExecutorMemoryAllocator allocator(executor);\n   Thunk::PrepareParams prepare_params = Thunk::PrepareParams{};\n-  ResourceRequests resource_requests;\n   BufferAllocations buffer_allocations({}, 0, &allocator);\n   Thunk::InitializeParams initialize_params;\n   initialize_params.stream = stream.get();\n@@ -264,7 +262,7 @@ TEST(CustomCallThunkTest, CustomCallWithOwnedHandlers) {\n   EXPECT_EQ(initialize_calls, 0);\n   EXPECT_EQ(execute_calls, 0);\n \n-  EXPECT_THAT(thunk->Prepare(prepare_params, resource_requests), IsOk());\n+  EXPECT_THAT(thunk->Prepare(prepare_params), IsOk());\n   EXPECT_EQ(instantiate_calls, 1);\n   EXPECT_EQ(prepare_calls, 1);\n   EXPECT_EQ(initialize_calls, 0);\n@@ -295,7 +293,6 @@ TEST(CustomCallThunkTest, CustomCallWithOwnedHandlersWithoutOptionalOnes) {\n   });\n   se::StreamExecutorMemoryAllocator allocator(executor);\n   Thunk::PrepareParams prepare_params = Thunk::PrepareParams{};\n-  ResourceRequests resource_requests;\n   Thunk::InitializeParams initialize_params = Thunk::InitializeParams{};\n   BufferAllocations buffer_allocations({}, 0, &allocator);\n   Thunk::ExecuteParams execute_params = Thunk::ExecuteParams::Create(\n@@ -310,7 +307,7 @@ TEST(CustomCallThunkTest, CustomCallWithOwnedHandlersWithoutOptionalOnes) {\n                               /*operands=*/{},\n                               /*results=*/{}, /*attributes=*/{},\n                               /*called_computation=*/nullptr));\n-  EXPECT_THAT(thunk->Prepare(prepare_params, resource_requests), IsOk());\n+  EXPECT_THAT(thunk->Prepare(prepare_params), IsOk());\n   EXPECT_THAT(thunk->Initialize(initialize_params), IsOk());\n   EXPECT_THAT(thunk->ExecuteOnStream(execute_params), IsOk());\n   EXPECT_EQ(execute_calls, 1);"
        },
        {
            "sha": "7a28e90821371cf687ba190f8531dc471d0e47b6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -216,8 +216,7 @@ DynamicSliceThunk::DynamicSliceThunk(\n   }\n }\n \n-absl::Status DynamicSliceThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status DynamicSliceThunk::Prepare(const PrepareParams& params) {\n   for (SliceDef& slice : slices_) {\n     VLOG(2) << \"DynamicSliceThunk: slice: \" << slice.ToString();\n     if (slice.offsets.has_value()) {\n@@ -236,7 +235,7 @@ absl::Status DynamicSliceThunk::Prepare(\n     }\n   }\n \n-  TF_RETURN_IF_ERROR(embedded_thunk_->Prepare(params, resource_requests));\n+  TF_RETURN_IF_ERROR(embedded_thunk_->Prepare(params));\n \n   if (offset_as_function_of_indvar_metadata_ != std::nullopt) {\n     Indvar(this) ="
        },
        {
            "sha": "c41bc02853cb2ba88310678352f4b395e3f3d3f1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -126,8 +126,7 @@ class DynamicSliceThunk : public Thunk {\n \n   const Thunk* embedded_thunk() const { return embedded_thunk_.get(); }\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n "
        },
        {
            "sha": "710b4fd0ceddb0ba76d2bc7de026efb0c9bf170f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -46,7 +46,6 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/service/gpu/resource_requests.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape.h\"\n@@ -1945,7 +1944,7 @@ TEST_F(DynamicSliceThunkTest,\n                                 /*device_ordinal=*/0,\n                                 /*memory_allocator=*/&allocator);\n \n-  Thunk::PrepareParams prepare_params{nullptr};\n+  Thunk::PrepareParams prepare_params{};\n \n   Thunk::ExecuteParams params = Thunk::ExecuteParams::Create(\n       run_options, /*buffer_allocations=*/allocations, stream.get(),\n@@ -1957,8 +1956,7 @@ TEST_F(DynamicSliceThunkTest,\n       {executor, source, &allocations, stream.get(), stream.get()}));\n \n   // Executing dynamic slice thunk.\n-  ResourceRequests resource_requests;\n-  TF_ASSERT_OK(thunk->Prepare(prepare_params, resource_requests));\n+  TF_ASSERT_OK(thunk->Prepare(prepare_params));\n   TF_ASSERT_OK(thunk->ExecuteOnStream(params));\n   TF_ASSERT_OK(stream->BlockHostUntilDone());\n "
        },
        {
            "sha": "ff8183d9d6b305f258f68dfcaec034d199839c43",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -89,15 +89,15 @@ absl::StatusOr<xla::gpu::GpuCollectives*> GetNvshmemCollectivesFromRegistry() {\n   return tsl::down_cast<xla::gpu::GpuCollectives*>(collectives);\n }\n \n-absl::Status NvshmemCollectiveThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status NvshmemCollectiveThunk::Prepare(const PrepareParams& params) {\n+  TF_RET_CHECK(params.collective_params != nullptr);\n   TF_ASSIGN_OR_RETURN(GpuCollectives * collectives, GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetGpuCliqueKey(collectives, *params.collective_params,\n                       config().replica_groups, config().group_mode,\n                       GetAsyncStreamKind(), /*use_nccl= */ false));\n-  return resource_requests.AddClique(clique_key);\n+  return params.clique_requests->RequestClique(clique_key);\n }\n \n absl::Status NvshmemCollectiveThunk::Initialize("
        },
        {
            "sha": "1bb43e3c47af4bff04e064d7cb3694fbffbb53a0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -42,8 +42,7 @@ class NvshmemCollectiveThunk : public Thunk {\n  public:\n   NvshmemCollectiveThunk(Kind kind, ThunkInfo thunk_info, bool is_sync);\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n \n   absl::Status Initialize(const InitializeParams& params) override;\n "
        },
        {
            "sha": "63506cb8151147c1b4a0fba92b1cabacf3c42939",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -195,8 +195,7 @@ AsyncStreamKind GetStreamKindForP2P(const HloInstruction* instr) {\n // Retrieves the current collective ID (replica or partition ID) for the\n // executing device.\n absl::StatusOr<const int64_t> GetCollectiveCurrentId(\n-    Thunk::CollectiveExecuteParams* collective_params,\n-    const P2PConfig& config) {\n+    CollectiveParams* collective_params, const P2PConfig& config) {\n   GlobalDeviceId global_device_id = collective_params->global_device_id;\n   TF_ASSIGN_OR_RETURN(\n       const DeviceAssignment::LogicalID current_logical_id,"
        },
        {
            "sha": "b45544cc609b4617ee9092c1aeba7015df5b2aa9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -104,7 +104,7 @@ P2PConfig GetP2PConfigForSendRecv(const HloSendRecvInstruction* instr,\n AsyncStreamKind GetStreamKindForP2P(const HloInstruction* instr);\n \n absl::StatusOr<const int64_t> GetCollectiveCurrentId(\n-    Thunk::CollectiveExecuteParams* collective_params, const P2PConfig& config);\n+    CollectiveParams* collective_params, const P2PConfig& config);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "9551f5e77ad634dd1aefd7f6c1663ac2dcc633b0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/sequential_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -73,10 +73,9 @@ std::string SequentialThunk::ToString(int indent) const {\n   return result;\n }\n \n-absl::Status SequentialThunk::Prepare(\n-    const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n+absl::Status SequentialThunk::Prepare(const PrepareParams& params) {\n   for (auto& thunk : thunks_) {\n-    TF_RETURN_IF_ERROR(thunk->Prepare(params, resource_requests));\n+    TF_RETURN_IF_ERROR(thunk->Prepare(params));\n   }\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "c4cd7ed2fd1e975c19a4b721fb2b0b39c78180f0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/sequential_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -41,8 +41,7 @@ class SequentialThunk : public Thunk {\n   const ThunkSequence& thunks() const { return thunks_; }\n   std::string ToString(int indent) const override;\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n "
        },
        {
            "sha": "afb17871c6cc3f10ff7c480a316da8ccb9d8e2cc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 142,
            "changes": 150,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n \n #include <algorithm>\n #include <cstdint>\n-#include <optional>\n #include <ostream>\n #include <string>\n #include <utility>\n@@ -27,155 +26,25 @@ limitations under the License.\n #include \"absl/functional/function_ref.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n-#include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n-#include \"xla/core/collectives/communicator.h\"\n-#include \"xla/core/collectives/rank_id.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/ffi/execution_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/service/global_device_id.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/gpu_executable_run_options.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n-namespace xla {\n-namespace gpu {\n-\n-//===----------------------------------------------------------------------===//\n-// Thunk::CollectiveCliques\n-//===----------------------------------------------------------------------===//\n-\n-Thunk::CollectiveCliques::CollectiveCliques(AcquiredCliquesMap cliques_map,\n-                                            int32_t num_transient_cliques)\n-    : cliques_map_(std::move(cliques_map)),\n-      num_transient_cliques_(num_transient_cliques) {}\n-\n-absl::StatusOr<Communicator*> Thunk::CollectiveCliques::GetComm(\n-    const GpuCliqueKey& clique_key, RankId rank) const {\n-  // Check that we locked access to a clique for `clique_key`.\n-  auto clique = cliques_map_.find(clique_key);\n-  if (clique == cliques_map_.end()) {\n-    return absl::NotFoundError(absl::StrCat(\"No clique found for clique key: \",\n-                                            clique_key.ToString()));\n-  }\n-\n-  // Check that clique has a communicator for our rank.\n-  auto communicator = (*clique->second)->comm(rank);\n-  if (!communicator.has_value()) {\n-    return absl::InternalError(\n-        absl::StrCat(\"Communicator for rank \", rank.value(),\n-                     \" not found in a NCCL clique \", clique_key.ToString()));\n-  }\n-\n-  return *communicator;\n-}\n-\n-absl::StatusOr<bool> Thunk::CollectiveCliques::peer_access_enabled(\n-    const GpuCliqueKey& clique_key) const {\n-  // Check that we locked access to a clique for `clique_key`.\n-  auto clique = cliques_map_.find(clique_key);\n-  if (clique == cliques_map_.end()) {\n-    return absl::NotFoundError(absl::StrCat(\"No clique found for clique key: \",\n-                                            clique_key.ToString()));\n-  }\n-\n-  return (*clique->second)->peer_access_enabled();\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// Thunk::CollectiveExecuteParams\n-//===----------------------------------------------------------------------===//\n-\n-using GlobalDeviceIdMap = Thunk::CollectiveExecuteParams::GlobalDeviceIdMap;\n-\n-// Returns global device id for a local device ordinal or an error if global\n-// device id map is misconfigured and missing an entry for a local device.\n-static absl::StatusOr<GlobalDeviceId> GetGlobalDeviceId(\n-    const GlobalDeviceIdMap* device_id_map, int64_t local_device_ordinal) {\n-  // No local -> global mapping was provided; assume the identity mapping.\n-  if (!device_id_map) {\n-    return GlobalDeviceId(local_device_ordinal);\n-  }\n-\n-  // Find a global device id in a global device id map.\n-  auto it = device_id_map->find(local_device_ordinal);\n-  if (it == device_id_map->end()) {\n-    return absl::NotFoundError(\n-        absl::StrCat(\"No global device id found for local device ordinal: \",\n-                     local_device_ordinal));\n-  }\n-\n-  return it->second;\n-}\n-\n-absl::StatusOr<Thunk::CollectiveExecuteParams>\n-Thunk::CollectiveExecuteParams::Create(\n-    const ServiceExecutableRunOptions& run_options,\n-    absl::Span<se::Stream* const> async_streams, int64_t local_device_ordinal,\n-    int64_t collective_max_nchannels, int64_t p2p_max_nchannels) {\n-  const GpuExecutableRunOptions* gpu_options =\n-      run_options.run_options().gpu_executable_run_options();\n-\n-  auto* collectives = gpu_options && gpu_options->collectives()\n-                          ? gpu_options->collectives()\n-                          : GpuCollectives::Default();\n-\n-  auto* device_id_map = gpu_options && gpu_options->gpu_global_device_ids()\n-                            ? &*gpu_options->gpu_global_device_ids()\n-                            : nullptr;\n-\n-  auto* clique_id_callback = gpu_options && gpu_options->clique_id_callback()\n-                                 ? &gpu_options->clique_id_callback()\n-                                 : nullptr;\n-\n-  auto* incarnations = gpu_options && gpu_options->incarnations().has_value()\n-                           ? &*gpu_options->incarnations()\n-                           : nullptr;\n-\n-  TF_ASSIGN_OR_RETURN(GlobalDeviceId global_device_id,\n-                      GetGlobalDeviceId(device_id_map, local_device_ordinal));\n-\n-  return CollectiveExecuteParams(\n-      collectives, run_options.stream()->parent(),\n-      run_options.run_options().run_id(), async_streams, local_device_ordinal,\n-      global_device_id, run_options.run_options().device_assignment(),\n-      device_id_map, clique_id_callback, incarnations, collective_max_nchannels,\n-      p2p_max_nchannels);\n-}\n-\n-Thunk::CollectiveExecuteParams::CollectiveExecuteParams(\n-    GpuCollectives* collectives, se::StreamExecutor* executor, RunId run_id,\n-    absl::Span<se::Stream* const> async_streams, int64_t local_device_ordinal,\n-    GlobalDeviceId global_device_id, const DeviceAssignment* device_assn,\n-    const GlobalDeviceIdMap* global_device_id_map,\n-    const CliqueIdCallback* nccl_clique_id_callback,\n-    const absl::flat_hash_map<GlobalDeviceId, IncarnationId>* incarnations,\n-    int64_t collective_max_nchannels, int64_t p2p_max_nchannels)\n-    : collectives(collectives),\n-      executor(executor),\n-      run_id(run_id),\n-      async_streams(async_streams.begin(), async_streams.end()),\n-      local_device_ordinal(local_device_ordinal),\n-      global_device_id(global_device_id),\n-      device_assn(device_assn),\n-      global_device_id_map(global_device_id_map),\n-      nccl_clique_id_callback(nccl_clique_id_callback),\n-      incarnations(incarnations),\n-      collective_max_nchannels(collective_max_nchannels),\n-      p2p_max_nchannels(p2p_max_nchannels) {}\n+namespace xla::gpu {\n \n //===----------------------------------------------------------------------===//\n // Thunk::ExecuteParams\n@@ -185,8 +54,7 @@ Thunk::ExecuteParams Thunk::ExecuteParams::Create(\n     const ServiceExecutableRunOptions& run_options,\n     const BufferAllocations& buffer_allocations, se::Stream* stream,\n     se::Stream* command_buffer_trace_stream,\n-    CollectiveExecuteParams* collective_params,\n-    CollectiveCliques* collective_cliques,\n+    CollectiveParams* collective_params, CollectiveCliques* collective_cliques,\n     ExecutionStreamIdMap additional_compute_streams) {\n   return ExecuteParams(&buffer_allocations, stream, command_buffer_trace_stream,\n                        collective_params, collective_cliques,\n@@ -218,9 +86,8 @@ Thunk::ExecuteParams Thunk::ExecuteParams::CloneWithNewAllocations(\n Thunk::ExecuteParams::ExecuteParams(\n     const BufferAllocations* buffer_allocations, se::Stream* stream,\n     se::Stream* command_buffer_trace_stream,\n-    CollectiveExecuteParams* collective_params,\n-    CollectiveCliques* collective_cliques, se::Stream* device_to_host_stream,\n-    se::Stream* host_to_device_stream,\n+    CollectiveParams* collective_params, CollectiveCliques* collective_cliques,\n+    se::Stream* device_to_host_stream, se::Stream* host_to_device_stream,\n     SendDeviceMemoryFunction* send_device_memory_function,\n     RecvDeviceMemoryFunction* recv_device_memory_function,\n     const ffi::ExecutionContext* ffi_execution_context,\n@@ -443,7 +310,7 @@ ThunkMetadataListProto GetMetadataListProtoFromThunkGraph(\n }\n \n absl::StatusOr<GpuCollectives* absl_nonnull> Thunk::GetGpuCollectives(\n-    CollectiveExecuteParams const& params) {\n+    const CollectiveParams& params) {\n   if (params.collectives == nullptr) {\n     return Internal(\"Collectives API is not provided\");\n   }\n@@ -458,5 +325,4 @@ ThunkInfoProto Thunk::ThunkInfo::ToProto() const {\n   return proto;\n }\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu"
        },
        {
            "sha": "d462d23512ac7a58f7c74d1211824d207666c04f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.h",
            "status": "modified",
            "additions": 13,
            "deletions": 115,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -34,21 +34,19 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n-#include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/core/collectives/communicator.h\"\n-#include \"xla/core/collectives/rank_id.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/ffi/execution_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/global_device_id.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/service/gpu/gpu_executable_run_options.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -237,107 +235,6 @@ class Thunk {\n     ThunkInfoProto ToProto() const;\n   };\n \n-  //===--------------------------------------------------------------------===//\n-  // ResourceRequests\n-  //===--------------------------------------------------------------------===//\n-\n-  // Each individual thunk can request various resources required for execution\n-  // at prepare stage. XLA executable is responsible for allocating them before\n-  // initializing and executing thunks.\n-  class ResourceRequestsInterface {\n-   public:\n-    virtual ~ResourceRequestsInterface() = default;\n-    virtual absl::Status AddClique(const GpuCliqueKey& clique_key) = 0;\n-  };\n-\n-  //===--------------------------------------------------------------------===//\n-  // CollectiveCliques\n-  //===--------------------------------------------------------------------===//\n-\n-  // A collection of collective cliques acquired based on resource requests\n-  // collected from all thunks at prepare stage.\n-  class CollectiveCliques {\n-   public:\n-    CollectiveCliques() = default;\n-    CollectiveCliques(AcquiredCliquesMap cliques_map,\n-                      int32_t num_transient_cliques);\n-\n-    absl::StatusOr<Communicator*> GetComm(const GpuCliqueKey& clique_key,\n-                                          RankId rank) const;\n-\n-    // Returns whether peer device memory access is possible between all devices\n-    // in the clique.\n-    absl::StatusOr<bool> peer_access_enabled(\n-        const GpuCliqueKey& clique_key) const;\n-\n-    bool empty() const { return cliques_map_.empty(); }\n-\n-    bool num_transient_cliques() const { return num_transient_cliques_; }\n-\n-   private:\n-    AcquiredCliquesMap cliques_map_;\n-\n-    // The number of acquired non-persistent clique. We need to keep track of\n-    // newly created communicators to insert rendezvous after first\n-    // initialization, because otherwise we observe deadlocks with NCCL\n-    // collectives backends.\n-    int32_t num_transient_cliques_ = 0;\n-  };\n-\n-  //===--------------------------------------------------------------------===//\n-  // CollectiveExecuteParams\n-  //===--------------------------------------------------------------------===//\n-\n-  // Parameters capturing all the details required for collective execution of\n-  // XLA executables (multiple partitions and replicas).\n-  struct CollectiveExecuteParams {\n-    // Creates NCCL execution parameters from the run options for the given\n-    // local device. Returns an error if run options are misconfigured (i.e.\n-    // missing a global device mapping for a local device ordinal).\n-    static absl::StatusOr<CollectiveExecuteParams> Create(\n-        const ServiceExecutableRunOptions& run_options,\n-        absl::Span<se::Stream* const> async_streams,\n-        int64_t local_device_ordinal, int64_t collective_max_nchannels = 0,\n-        int64_t p2p_max_nchannels = 0);\n-\n-    // A mapping from local device ordinals to global device IDs.\n-    using GlobalDeviceIdMap = std::map<int32_t, GlobalDeviceId>;\n-\n-    GpuCollectives* collectives;\n-    se::StreamExecutor* executor;\n-\n-    // XLA execution run id allows us to distinguish collective operations\n-    // from different concurrent executions and avoid deadlocks.\n-    RunId run_id;\n-\n-    // Streams for asynchronous collective communications.\n-    absl::InlinedVector<se::Stream*, 4> async_streams;\n-\n-    int64_t local_device_ordinal;\n-    GlobalDeviceId global_device_id;\n-\n-    const DeviceAssignment* device_assn;\n-    const GlobalDeviceIdMap* global_device_id_map;\n-    const CliqueIdCallback* nccl_clique_id_callback;\n-    const absl::flat_hash_map<GlobalDeviceId, IncarnationId>* incarnations;\n-\n-    int64_t collective_max_nchannels;\n-    int64_t p2p_max_nchannels;\n-\n-    bool need_barrier = false;\n-\n-   private:\n-    CollectiveExecuteParams(\n-        GpuCollectives* collectives, se::StreamExecutor* executor, RunId run_id,\n-        absl::Span<se::Stream* const> async_streams,\n-        int64_t local_device_ordinal, GlobalDeviceId global_device_id,\n-        const DeviceAssignment* device_assn,\n-        const GlobalDeviceIdMap* global_device_id_map,\n-        const CliqueIdCallback* nccl_clique_id_callback,\n-        const absl::flat_hash_map<GlobalDeviceId, IncarnationId>* incarnations,\n-        int64_t collective_max_nchannels, int64_t p2p_max_nchannels);\n-  };\n-\n   //===--------------------------------------------------------------------===//\n   // PrepareParams\n   //===--------------------------------------------------------------------===//\n@@ -347,7 +244,9 @@ class Thunk {\n   // back to executable, i.e. request collective cliques required at run time.\n   struct PrepareParams {\n     // Parameters for executing collective operations.\n-    const CollectiveExecuteParams* collective_params = nullptr;\n+    const CollectiveParams* collective_params = nullptr;\n+    // Clique requests for preparing collective communicators.\n+    CollectiveCliqueRequests* clique_requests = nullptr;\n   };\n \n   //===--------------------------------------------------------------------===//\n@@ -375,7 +274,7 @@ class Thunk {\n     se::Stream* command_buffer_trace_stream = nullptr;\n \n     // Parameters for executing collective operations.\n-    CollectiveExecuteParams* collective_params = nullptr;\n+    CollectiveParams* collective_params = nullptr;\n \n     // Collective cliques acquired based on resource requests.\n     CollectiveCliques* collective_cliques = nullptr;\n@@ -401,7 +300,7 @@ class Thunk {\n         const ServiceExecutableRunOptions& run_options,\n         const BufferAllocations& buffer_allocations, se::Stream* stream,\n         se::Stream* command_buffer_trace_stream,\n-        CollectiveExecuteParams* collective_params,\n+        CollectiveParams* collective_params,\n         CollectiveCliques* collective_cliques,\n         ExecutionStreamIdMap additional_compute_streams = {});\n \n@@ -421,7 +320,7 @@ class Thunk {\n     se::Stream* command_buffer_trace_stream;\n \n     // Parameters for executing collective operations.\n-    CollectiveExecuteParams* collective_params;\n+    CollectiveParams* collective_params;\n \n     // Collective cliques acquired based on resource requests.\n     CollectiveCliques* collective_cliques;\n@@ -449,7 +348,7 @@ class Thunk {\n \n     ExecuteParams(const BufferAllocations* buffer_allocations,\n                   se::Stream* stream, se::Stream* command_buffer_trace_stream,\n-                  CollectiveExecuteParams* collective_params,\n+                  CollectiveParams* collective_params,\n                   CollectiveCliques* collective_cliques,\n                   se::Stream* device_to_host_stream,\n                   se::Stream* host_to_device_stream,\n@@ -480,8 +379,7 @@ class Thunk {\n   // This may be called multiple times. Its main purpose is to pass resource\n   // requests up to the parent executable so it can acquire them before\n   // initialization and execution.\n-  virtual absl::Status Prepare(const PrepareParams& params,\n-                               ResourceRequestsInterface& resource_requests) {\n+  virtual absl::Status Prepare(const PrepareParams& params) {\n     return absl::OkStatus();\n   }\n \n@@ -550,9 +448,9 @@ class Thunk {\n   }\n \n   // A helper function to get the `GpuCollectives*` pointer from the\n-  // CollectiveExecuteParams.\n+  // CollectiveParams.\n   static absl::StatusOr<GpuCollectives* absl_nonnull> GetGpuCollectives(\n-      CollectiveExecuteParams const& params);\n+      CollectiveParams const& params);\n \n   // A helper function to get the `GpuCollectives*` pointer from the\n   // thunk parameters. Returns an error if collectives API is not provided."
        },
        {
            "sha": "307a4bb59378dd77433ee2bb2bccaac64bd4ed88",
            "filename": "third_party/xla/xla/backends/gpu/runtime/while_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -100,11 +100,9 @@ WhileThunk::WhileThunk(\n       body_thunk_sequence_(std::move(body_thunk_sequence)),\n       trip_count_(trip_count) {}\n \n-absl::Status WhileThunk::Prepare(const PrepareParams& params,\n-                                 ResourceRequestsInterface& resource_requests) {\n-  TF_RETURN_IF_ERROR(\n-      condition_thunk_sequence_->Prepare(params, resource_requests));\n-  TF_RETURN_IF_ERROR(body_thunk_sequence_->Prepare(params, resource_requests));\n+absl::Status WhileThunk::Prepare(const PrepareParams& params) {\n+  TF_RETURN_IF_ERROR(condition_thunk_sequence_->Prepare(params));\n+  TF_RETURN_IF_ERROR(body_thunk_sequence_->Prepare(params));\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "4ec690d85d591b0e15663483acf3a03f9b256337",
            "filename": "third_party/xla/xla/backends/gpu/runtime/while_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.h?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -64,8 +64,7 @@ class WhileThunk : public Thunk {\n   WhileThunk(const WhileThunk&) = delete;\n   WhileThunk& operator=(const WhileThunk&) = delete;\n \n-  absl::Status Prepare(const PrepareParams& params,\n-                       ResourceRequestsInterface& resource_requests) override;\n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n "
        },
        {
            "sha": "6903b485f4f1c5a192e8b6f471e3f0e4ab303f64",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 27,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -756,7 +756,6 @@ cc_library(\n         \":gpu_executable_proto_cc\",\n         \":gpu_executable_run_options\",\n         \":ir_emission_utils\",\n-        \":resource_requests\",\n         \":stream_executor_util\",\n         \"//xla:executable_run_options\",\n         \"//xla:shape_tree\",\n@@ -765,6 +764,9 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/runtime:annotation\",\n+        \"//xla/backends/gpu/runtime:collective_clique_requests\",\n+        \"//xla/backends/gpu/runtime:collective_cliques\",\n+        \"//xla/backends/gpu/runtime:collective_params\",\n         \"//xla/backends/gpu/runtime:command_buffer_conversion_pass\",\n         \"//xla/backends/gpu/runtime:nvshmem_collective_thunk\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n@@ -3430,32 +3432,6 @@ xla_cc_test(\n     ],\n )\n \n-cc_library(\n-    name = \"resource_requests\",\n-    srcs = [\"resource_requests.cc\"],\n-    hdrs = [\"resource_requests.h\"],\n-    deps = [\n-        \":gpu_executable_run_options\",\n-        \"//xla/backends/gpu/collectives:gpu_clique\",\n-        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n-        \"//xla/backends/gpu/collectives:gpu_cliques\",\n-        \"//xla/backends/gpu/runtime:thunk\",\n-        \"//xla/core/collectives:rank_id\",\n-        \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/base:core_headers\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/synchronization\",\n-        \"@local_tsl//tsl/profiler/lib:traceme\",\n-        \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n-    ],\n-)\n-\n cc_library(\n     name = \"intel_gpu_compiler\",\n     srcs = ["
        },
        {
            "sha": "c9414b5edb568e3cdc860d5b94f957ba64e15dda",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 15,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5d46b65af45d5694cb1676bc872d24a4a64a6b57/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=5d46b65af45d5694cb1676bc872d24a4a64a6b57",
            "patch": "@@ -41,6 +41,9 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/annotation.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_conversion_pass.h\"\n #include \"xla/backends/gpu/runtime/nvshmem_collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n@@ -62,7 +65,6 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/gpu_executable_run_options.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/service/gpu/resource_requests.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/service/hlo_value.h\"\n #include \"xla/service/maybe_owning_device_memory.h\"\n@@ -419,37 +421,36 @@ absl::Status ExecuteThunksImpl(\n   }\n \n   // Parameters for executing collective operations.\n-  TF_ASSIGN_OR_RETURN(Thunk::CollectiveExecuteParams collective_params,\n-                      Thunk::CollectiveExecuteParams::Create(\n-                          *run_options, async_comms_streams,\n-                          main_stream->parent()->device_ordinal(),\n-                          collective_max_nchannels, p2p_max_nchannels));\n+  TF_ASSIGN_OR_RETURN(\n+      CollectiveParams collective_params,\n+      CollectiveParams::Create(*run_options, async_comms_streams,\n+                               main_stream->parent()->device_ordinal(),\n+                               collective_max_nchannels, p2p_max_nchannels));\n \n-  ResourceRequests resource_requests;\n+  CollectiveCliqueRequests clique_requests;\n \n-  {  // Collect resource requirements from thunks.\n-    Thunk::PrepareParams prepare_params{&collective_params};\n+  {  // Prepare thunks for execution and collect requested GPU cliques.\n+    Thunk::PrepareParams prepare_params{&collective_params, &clique_requests};\n \n     tsl::profiler::TraceMe trace_prepare(\"Thunks::Prepare\");\n-    TF_RETURN_IF_ERROR(\n-        thunk_sequence.Prepare(prepare_params, resource_requests));\n+    TF_RETURN_IF_ERROR(thunk_sequence.Prepare(prepare_params));\n   }\n \n   std::vector<std::unique_ptr<CliqueKey>>* clique_keys =\n       run_options->run_options().clique_keys();\n   if (clique_keys != nullptr) {\n-    for (const GpuCliqueKey& clique_key : resource_requests.CliqueKeys()) {\n+    for (const GpuCliqueKey& clique_key : clique_requests.RequestedCliques()) {\n       clique_keys->push_back(std::make_unique<GpuCliqueKey>(clique_key));\n     }\n   }\n \n   // Acquire collective cliques requested by thunks.\n-  Thunk::CollectiveCliques collective_cliques;\n+  CollectiveCliques collective_cliques;\n   if (!mock_collectives) {\n     TF_ASSIGN_OR_RETURN(\n         collective_cliques,\n-        resource_requests.AcquireCollectiveCliques(\n-            collective_params,\n+        AcquireCollectiveCliques(\n+            collective_params, clique_requests,\n             debug_options\n                 ? debug_options->xla_gpu_collectives_use_persistent_cliques()\n                 : false));"
        },
        {
            "sha": "bd751bd089823f4d4f389e849a8be6f738b62458",
            "filename": "third_party/xla/xla/service/gpu/resource_requests.h",
            "status": "removed",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/345f4f76db5d73c81e43ddd2f7d3f82207930dd0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fresource_requests.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/345f4f76db5d73c81e43ddd2f7d3f82207930dd0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fresource_requests.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fresource_requests.h?ref=345f4f76db5d73c81e43ddd2f7d3f82207930dd0",
            "patch": "@@ -1,67 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_SERVICE_GPU_RESOURCE_REQUESTS_H_\n-#define XLA_SERVICE_GPU_RESOURCE_REQUESTS_H_\n-\n-#include <cstdint>\n-#include <vector>\n-\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n-#include \"xla/backends/gpu/runtime/thunk.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// Shared resources required for thunk initialization and execution.\n-class ResourceRequests : public Thunk::ResourceRequestsInterface {\n- public:\n-  absl::Status AddClique(const GpuCliqueKey& clique_key) final;\n-  std::vector<GpuCliqueKey> CliqueKeys() const;\n-\n-  absl::StatusOr<Thunk::CollectiveCliques> AcquireCollectiveCliques(\n-      const Thunk::CollectiveExecuteParams& params,\n-      bool use_persistent_cliques);\n-\n- private:\n-  struct CliqueRequest {\n-    GpuCliqueKey key;\n-    int64_t id;\n-  };\n-\n-  // Return clique requests deterministically ordered using a comparison\n-  // function that produces identical ordering for all participating ranks.\n-  //\n-  // Example: 8 ranks splitted in different groups of communicators\n-  //\n-  // Group #0: [0,1], [2,3], [4,5], [6,7]\n-  // Group #1: [0,4], [1,5], [2,6], [3,7]\n-  //\n-  // Both groups #0 and #1 can be acqured by splitting [0...7] clique. To avoid\n-  // deadlocks all participants should acquire all cliques in a group #0 before\n-  // acquiring any cliques in a group #1.\n-  //\n-  // We rely on clique request id to guarantee that the order is identical\n-  // on all participating ranks (including ranks running on different hosts).\n-  std::vector<CliqueRequest> GetOrderedCliqueRequests();\n-\n-  absl::flat_hash_map<GpuCliqueKey, CliqueRequest> cliques_;\n-};\n-}  // namespace gpu\n-}  // namespace xla\n-#endif  // XLA_SERVICE_GPU_RESOURCE_REQUESTS_H_"
        }
    ],
    "stats": {
        "total": 1290,
        "additions": 744,
        "deletions": 546
    }
}