{
    "author": "shawnwang18",
    "message": "PR #33073: [XLA:GPU] add a warm up iteration for command buffer thunk\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33073\n\nüìù Summary of Changes\n\nThis PR fix the issue that NCCL may call cuda-graph un-supported host side API during graph capturing (e.g. `cuCtxEnablePeerAccess`), this will break XLA cuda graph run.  To work around the issue, this PR introduces a warm up iteration for command buffer thunk, during warm up iteration, command buffer thunk are executed through normal thunks.  The warm up iteration will do the proper NCCL setup, so later iterations running through command buffer does not need to call NCCL setup APIs.\n\nSo for command buffer thunk, the execution phase are:\n\n1st iteration:   warm up (run through normal thunks)\n2nd iteration:   cuda graph creation iteration.\n3rd- iteration:  possible cuda graph update iteration\n\nüéØ Justification\nThe warm up iteration is required to setup proper NCCL connections, so the afterwards cuda-graph iterations does not need to call un-supported cuda APIs.\n\nüöÄ Kind of Contribution\nPlease remove what does not apply: üêõ Bug Fix\n\nüß™ Execution Tests:\nxla/service/gpu/tests/command_buffer_test.cc\nCopybara import of the project:\n\n--\n1db6d833a867968e16e45d3e3570699d22164ecf by Shawn Wang <shawnw@nvidia.com>:\n\nadd a warm up step for command buffer thunk\n\nMerging this change closes #33073\n\nPiperOrigin-RevId: 827922721",
    "sha": "3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3",
    "files": [
        {
            "sha": "81e46d535946cd00ce207f7484ecdb7db0ea56fb",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 3,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc?ref=3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3",
            "patch": "@@ -157,6 +157,13 @@ absl::Status CommandBufferThunk::Initialize(const InitializeParams& params) {\n     TF_RETURN_IF_ERROR(thunks_->Initialize(params));\n   }\n \n+  // If there are no thunks, or command buffer does not require initialization,\n+  // we can mark warm up as done immediately.\n+  if ((!thunks_ || !commands_.requires_initialization()) &&\n+      !cmd_buffer->warmup_done) {\n+    cmd_buffer->warmup_done = true;\n+  }\n+\n   // Construct ExecuteParams with empty fields for everything that is not needed\n   // for recording commands.\n   Thunk::ExecuteParams execute_params(\n@@ -177,9 +184,9 @@ absl::Status CommandBufferThunk::Initialize(const InitializeParams& params) {\n   // If commands require initialization, we also record them into the command\n   // buffer before execution. This is required to guarantee that collective\n   // commands recorded on all participating ranks to avoid deadlocks.\n-  if (cmd_buffer->command_buffer->state() ==\n-          se::CommandBuffer::State::kCreate ||\n-      commands_.requires_initialization()) {\n+  if (cmd_buffer->warmup_done && (cmd_buffer->command_buffer->state() ==\n+                                      se::CommandBuffer::State::kCreate ||\n+                                  commands_.requires_initialization())) {\n     VLOG(3) << \"Initialize command buffer on device #\"\n             << params.executor->device_ordinal()\n             << \" by recoding command buffer cmd sequence\"\n@@ -238,6 +245,14 @@ absl::Status CommandBufferThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   absl::MutexLock lock(cmd_buffer->mutex);\n \n+  // warm up iteration, run through thunks if they are present.\n+  if (!cmd_buffer->warmup_done && thunks_) {\n+    VLOG(2) << \"Executing warm up iteration of command buffer thunk\";\n+    TF_RETURN_IF_ERROR(thunks_->ExecuteOnStream(params));\n+    cmd_buffer->warmup_done = true;\n+    return absl::OkStatus();\n+  }\n+\n   // Update buffer allocations and collect all allocations that changed since\n   // the last command buffer execution.\n   auto updated_allocs = cmd_buffer->UpdateBufferAllocations(commands_, params);"
        },
        {
            "sha": "9f1b360a1db437cd3360b9f61ddbebbf5392ad13",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.h?ref=3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3",
            "patch": "@@ -101,6 +101,15 @@ class CommandBufferThunk : public Thunk {\n \n     // Number of command buffer executions since last update.\n     int64_t num_executions ABSL_GUARDED_BY(mutex) = 0;\n+\n+    // For GPU backend, NCCL may call cuda-graph un-supported host side API\n+    // during graph capturing (e.g. cuCtxEnablePeerAccess), this will break XLA\n+    // cuda graph run. To work around the issue, this PR introduces a warm up\n+    // iteration for command buffer thunk, during warm up iteration, command\n+    // buffer thunk are executed through normal thunks. The warm up iteration\n+    // will do the proper NCCL setup, so later iterations running through\n+    // command buffer does not need to call NCCL setup APIs.\n+    bool warmup_done ABSL_GUARDED_BY(mutex) = false;\n   };\n \n   // Command buffer thunk owns commands buffers instantiated on all executors."
        },
        {
            "sha": "98c53490804a7d06bfbb56bf6d23d5a5a3aeaba1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc?ref=3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3",
            "patch": "@@ -411,6 +411,11 @@ TEST(CommandBufferThunkTest, Memset32CmdCommandBuffersEnabledDuringProfiling) {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto profiler_lock,\n                           tsl::profiler::ProfilerLock::Acquire());\n+\n+  // skip warm up iteration\n+  TF_ASSERT_OK(thunk.ExecuteOnStream(params));\n+  TF_ASSERT_OK(stream->BlockHostUntilDone());\n+\n   // Execute command buffer thunk and verify that it set the memory.\n   TF_ASSERT_OK(thunk.ExecuteOnStream(params));\n   TF_ASSERT_OK(stream->BlockHostUntilDone());"
        },
        {
            "sha": "94341190445d00273a17784dc45b82023cca87f7",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3",
            "patch": "@@ -167,10 +167,12 @@ xla_test(\n         \"//xla/tests:hlo_pjrt_interpreter_reference_mixin\",\n         \"//xla/tests:hlo_pjrt_test_base\",\n         \"//xla/tests:literal_test_util\",\n+        \"//xla/tests:test_utils\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "14e71359821ce37df9add46118f9edd4dbbc9fb8",
            "filename": "third_party/xla/xla/service/gpu/tests/command_buffer_test.cc",
            "status": "modified",
            "additions": 130,
            "deletions": 31,
            "changes": 161,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc?ref=3cfb4f4aa83ae1406dd88ef21e489368a63c9cc3",
            "patch": "@@ -14,13 +14,16 @@ limitations under the License.\n ==============================================================================*/\n \n #include <cstdint>\n+#include <memory>\n+#include <optional>\n #include <utility>\n #include <variant>\n #include <vector>\n \n #include <gtest/gtest.h>\n #include \"absl/strings/ascii.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -31,6 +34,7 @@ limitations under the License.\n #include \"xla/tests/hlo_pjrt_interpreter_reference_mixin.h\"\n #include \"xla/tests/hlo_pjrt_test_base.h\"\n #include \"xla/tests/literal_test_util.h\"\n+#include \"xla/tests/test_utils.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/xla.pb.h\"\n@@ -70,6 +74,111 @@ class CommandBufferTest\n     debug_options.set_xla_gpu_command_buffer_scheduling_mode(GetParam());\n     return debug_options;\n   }\n+\n+  // Execute compiled module three times to exercise warm-up, create, and\n+  // update paths. Third run uses cloned arguments to encourage device buffer\n+  // address changes.\n+  void ExecuteThreePhasesAndExpect(std::unique_ptr<HloModule> module,\n+                                   absl::Span<const Literal* const> arguments,\n+                                   const Literal& expected,\n+                                   bool run_hlo_passes) {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::unique_ptr<OpaqueExecutable> executable,\n+        CreateExecutable(std::move(module), run_hlo_passes));\n+\n+    // 1) Warm-up (may run thunks)\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        Literal result1,\n+        test_runner().ExecuteWithExecutable(executable.get(), arguments));\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result1));\n+\n+    // 2) Create (record and execute command buffer)\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        Literal result2,\n+        test_runner().ExecuteWithExecutable(executable.get(), arguments));\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result2));\n+\n+    // 3) Update (execute with cloned arguments to attempt buffer changes)\n+    std::vector<Literal> cloned_args_storage;\n+    cloned_args_storage.reserve(arguments.size());\n+    std::vector<const Literal*> cloned_args;\n+    cloned_args.reserve(arguments.size());\n+    for (const Literal* arg : arguments) {\n+      cloned_args_storage.push_back(arg->Clone());\n+      cloned_args.push_back(&cloned_args_storage.back());\n+    }\n+\n+    TF_ASSERT_OK_AND_ASSIGN(Literal result3,\n+                            test_runner().ExecuteWithExecutable(\n+                                executable.get(), absl::MakeSpan(cloned_args)));\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result3));\n+  }\n+\n+  // Same as above, but generates fake inputs and compares results to a\n+  // reference execution. Useful for tests originally using RunAndCompare.\n+  ::testing::AssertionResult RunAndCompareThreeIterations(\n+      std::unique_ptr<HloModule> module, bool run_hlo_passes,\n+      const std::optional<ErrorSpec>& error) {\n+    // Verify module then clone for reference.\n+    TF_CHECK_OK(this->verifier().Run(module.get()).status());\n+    std::unique_ptr<HloModule> reference_module = module->Clone();\n+\n+    // Prepare fake args for both runners.\n+    absl::StatusOr<std::vector<Literal>> fake_args_or =\n+        MakeFakeArguments(module.get());\n+    if (!fake_args_or.ok()) {\n+      return ::testing::AssertionFailure() << fake_args_or.status().message();\n+    }\n+    std::vector<Literal> fake_args = std::move(*fake_args_or);\n+    std::vector<const Literal*> arg_ptrs = LiteralUtil::MakePointers(fake_args);\n+\n+    // Reference once.\n+    absl::StatusOr<Literal> reference = reference_runner().Execute(\n+        std::move(reference_module), absl::MakeSpan(arg_ptrs), run_hlo_passes);\n+    if (!reference.ok()) {\n+      return ::testing::AssertionFailure() << reference.status();\n+    }\n+\n+    // Compile once on test backend and run three iterations.\n+    absl::StatusOr<std::unique_ptr<OpaqueExecutable>> exec_or =\n+        CreateExecutable(std::move(module), run_hlo_passes);\n+    if (!exec_or.ok()) {\n+      return ::testing::AssertionFailure() << exec_or.status();\n+    }\n+    std::unique_ptr<OpaqueExecutable> exec = std::move(*exec_or);\n+\n+    // 1) Warm-up\n+    absl::StatusOr<Literal> r1 = test_runner().ExecuteWithExecutable(\n+        exec.get(), absl::MakeSpan(arg_ptrs));\n+    if (!r1.ok()) return ::testing::AssertionFailure() << r1.status();\n+    if (!LiteralTestUtil::NearOrEqual(*reference, *r1, error))\n+      return ::testing::AssertionFailure() << \"Mismatch on warm-up run\";\n+\n+    // 2) Create\n+    absl::StatusOr<Literal> r2 = test_runner().ExecuteWithExecutable(\n+        exec.get(), absl::MakeSpan(arg_ptrs));\n+    if (!r2.ok()) return ::testing::AssertionFailure() << r2.status();\n+    if (!LiteralTestUtil::NearOrEqual(*reference, *r2, error))\n+      return ::testing::AssertionFailure() << \"Mismatch on create run\";\n+\n+    // 3) Update with cloned args\n+    std::vector<Literal> cloned_args_storage;\n+    cloned_args_storage.reserve(arg_ptrs.size());\n+    std::vector<const Literal*> cloned_arg_ptrs;\n+    cloned_arg_ptrs.reserve(arg_ptrs.size());\n+    for (const Literal* a : arg_ptrs) {\n+      cloned_args_storage.push_back(a->Clone());\n+      cloned_arg_ptrs.push_back(&cloned_args_storage.back());\n+    }\n+\n+    absl::StatusOr<Literal> r3 = test_runner().ExecuteWithExecutable(\n+        exec.get(), absl::MakeSpan(cloned_arg_ptrs));\n+    if (!r3.ok()) return ::testing::AssertionFailure() << r3.status();\n+    if (!LiteralTestUtil::NearOrEqual(*reference, *r3, error))\n+      return ::testing::AssertionFailure() << \"Mismatch on update run\";\n+\n+    return ::testing::AssertionSuccess();\n+  }\n };\n \n // Test fixture that enables loop unrolling for command buffers.\n@@ -119,10 +228,8 @@ TEST_P(CommandBufferTest, Fusions) {\n   Literal argument = LiteralUtil::CreateR2<float>({{1.0, 2.0}, {3.0, 4.0}});\n   Literal expected = LiteralUtil::CreateR2<float>({{3.0, 8.0}, {15.0, 24.0}});\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      Literal result,\n-      Execute(std::move(module), {&argument}, /*run_hlo_passes=*/false));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+  ExecuteThreePhasesAndExpect(std::move(module), {&argument}, expected,\n+                              /*run_hlo_passes=*/false);\n }\n \n TEST_P(CommandBufferTest, TrueFalseConditional) {\n@@ -170,19 +277,17 @@ TEST_P(CommandBufferTest, TrueFalseConditional) {\n \n     Literal pred = LiteralUtil::CreateR0<bool>(true);\n     Literal expected = LiteralUtil::CreateR2<float>({{2.0, 4.0}, {6.0, 8.0}});\n-    TF_ASSERT_OK_AND_ASSIGN(Literal result, Execute(std::move(m), {&pred, &p1},\n-                                                    /*run_hlo_passes=*/false));\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+    ExecuteThreePhasesAndExpect(std::move(m), {&pred, &p1}, expected,\n+                                /*run_hlo_passes=*/false);\n   }\n \n   {  // Execute `false` branch.\n     TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(hlo_text));\n \n     Literal pred = LiteralUtil::CreateR0<bool>(false);\n     Literal expected = LiteralUtil::CreateR2<float>({{1.0, 4.0}, {9.0, 16.0}});\n-    TF_ASSERT_OK_AND_ASSIGN(Literal result, Execute(std::move(m), {&pred, &p1},\n-                                                    /*run_hlo_passes=*/false));\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+    ExecuteThreePhasesAndExpect(std::move(m), {&pred, &p1}, expected,\n+                                /*run_hlo_passes=*/false);\n   }\n }\n \n@@ -230,29 +335,26 @@ TEST_P(CommandBufferTest, IndexConditional) {\n \n     Literal index = LiteralUtil::CreateR0<int32_t>(0);\n     Literal expected = LiteralUtil::CreateR2<float>({{2.0, 4.0}, {6.0, 8.0}});\n-    TF_ASSERT_OK_AND_ASSIGN(Literal result, Execute(std::move(m), {&index, &p1},\n-                                                    /*run_hlo_passes=*/false));\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+    ExecuteThreePhasesAndExpect(std::move(m), {&index, &p1}, expected,\n+                                /*run_hlo_passes=*/false);\n   }\n \n   {  // Execute `1` branch.\n     TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(hlo_text));\n \n     Literal index = LiteralUtil::CreateR0<int32_t>(1);\n     Literal expected = LiteralUtil::CreateR2<float>({{1.0, 4.0}, {9.0, 16.0}});\n-    TF_ASSERT_OK_AND_ASSIGN(Literal result, Execute(std::move(m), {&index, &p1},\n-                                                    /*run_hlo_passes=*/false));\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+    ExecuteThreePhasesAndExpect(std::move(m), {&index, &p1}, expected,\n+                                /*run_hlo_passes=*/false);\n   }\n \n   {  // Execute `1024` branch (our of bound index executes N-1 branch).\n     TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(hlo_text));\n \n     Literal index = LiteralUtil::CreateR0<int32_t>(1024);\n     Literal expected = LiteralUtil::CreateR2<float>({{1.0, 4.0}, {9.0, 16.0}});\n-    TF_ASSERT_OK_AND_ASSIGN(Literal result, Execute(std::move(m), {&index, &p1},\n-                                                    /*run_hlo_passes=*/false));\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+    ExecuteThreePhasesAndExpect(std::move(m), {&index, &p1}, expected,\n+                                /*run_hlo_passes=*/false);\n   }\n }\n \n@@ -313,10 +415,8 @@ TEST_P(CommandBufferTest, WhileLoop) {\n   Literal expected_value = LiteralUtil::CreateR0<float>(20.0);\n   Literal expected = LiteralUtil::MakeTuple({&expected_cnt, &expected_value});\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      Literal result,\n-      Execute(std::move(module), {&argument}, /*run_hlo_passes=*/false));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+  ExecuteThreePhasesAndExpect(std::move(module), {&argument}, expected,\n+                              /*run_hlo_passes=*/false);\n }\n \n TEST_P(CommandBufferTest, ControlDependencyTest) {\n@@ -574,8 +674,8 @@ TEST_P(CommandBufferTest, DynamicSliceCopyFusionCmd) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(hlo_text, config));\n \n-  EXPECT_TRUE(\n-      RunAndCompareNoHloPasses(std::move(module), ErrorSpec{1e-3, 2e-3}));\n+  EXPECT_TRUE(RunAndCompareThreeIterations(\n+      std::move(module), /*run_hlo_passes=*/false, ErrorSpec{1e-3, 2e-3}));\n \n   if (!IsAtLeastCuda12900(GpuExecutor())) {\n     GTEST_SKIP() << \"While loop unrolling is not supported for CUDA < 12.9\";\n@@ -596,8 +696,9 @@ TEST_P(CommandBufferTest, DynamicSliceCopyFusionCmd) {\n   TF_ASSERT_OK_AND_ASSIGN(auto unrolled_module,\n                           ParseAndReturnVerifiedModule(hlo_text, config));\n \n-  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(unrolled_module),\n-                                       ErrorSpec{1e-3, 2e-3}));\n+  EXPECT_TRUE(RunAndCompareThreeIterations(std::move(unrolled_module),\n+                                           /*run_hlo_passes=*/false,\n+                                           ErrorSpec{1e-3, 2e-3}));\n }\n \n TEST_P(CommandBufferUnrollTest, WhileLoop) {\n@@ -663,10 +764,8 @@ TEST_P(CommandBufferUnrollTest, WhileLoop) {\n   Literal expected_value = LiteralUtil::CreateR0<float>(20.0);\n   Literal expected = LiteralUtil::MakeTuple({&expected_cnt, &expected_value});\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      Literal result,\n-      Execute(std::move(module), {&argument}, /*run_hlo_passes=*/false));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+  ExecuteThreePhasesAndExpect(std::move(module), {&argument}, expected,\n+                              /*run_hlo_passes=*/false);\n }\n \n TEST_P(CommandBufferUnrollTest, WhileLoopMultiDevice) {"
        }
    ],
    "stats": {
        "total": 198,
        "additions": 164,
        "deletions": 34
    }
}