{
    "author": "ezhulenev",
    "message": "[xla:gpu] Extract CollectiveMultimem library for allocating collective memory on top of multicast memory\n\nPiperOrigin-RevId: 840319224",
    "sha": "a8a509632cfd3d0913da76e48ddc225eff6efc72",
    "files": [
        {
            "sha": "9b7f2a7b2908ef25e8418e311df49c4a027d7cf8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=a8a509632cfd3d0913da76e48ddc225eff6efc72",
            "patch": "@@ -1248,7 +1248,9 @@ cc_library(\n     hdrs = [\"collective_kernel_thunk.h\"],\n     deps = [\n         \":all_reduce\",\n+        \":collective_cliques\",\n         \":collective_metadata_thunk\",\n+        \":collective_multimem\",\n         \":collective_thunk\",\n         \":thunk\",\n         \"//xla:shape_util\",\n@@ -1257,6 +1259,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/core/collectives:rank_id\",\n+        \"//xla/core/collectives:reduction_kind\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n@@ -1626,6 +1629,34 @@ xla_cc_test(\n     ],\n )\n \n+cc_library(\n+    name = \"collective_multimem\",\n+    srcs = [\"collective_multimem.cc\"],\n+    hdrs = [\"collective_multimem.h\"],\n+    compatible_with = get_compatible_with_portable(),\n+    deps = [\n+        \"//xla:util\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/core/collectives:rank_id\",\n+        \"//xla/runtime:device_id\",\n+        \"//xla/service:rendezvous\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/gpu:gpu_executor_header\",\n+        \"//xla/stream_executor/gpu:multicast_memory\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:btree\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n cc_library(\n     name = \"collective_params\",\n     srcs = [\"collective_params.cc\"],\n@@ -1979,6 +2010,7 @@ cc_library(\n     srcs = [\"collective_metadata_thunk.cc\"],\n     hdrs = [\"collective_metadata_thunk.h\"],\n     deps = [\n+        \":collective_multimem\",\n         \":collective_thunk\",\n         \":thunk\",\n         \"//xla:shape_util\",\n@@ -2000,11 +2032,13 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_protobuf//:protobuf_lite\",\n     ],"
        },
        {
            "sha": "f1b025177b624cf5ac50f4a67db43c055eed36d0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=a8a509632cfd3d0913da76e48ddc225eff6efc72",
            "patch": "@@ -33,7 +33,9 @@ limitations under the License.*/\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/all_reduce.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n #include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n@@ -155,8 +157,8 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n       sizeof(CollectiveKernelMetadata) + param_to_peers_ptrs_size_bytes, 0);\n   return CollectiveMetadataThunk::ConstructCollectiveMetadata(\n       std::move(parameters), params.stream, clique_key,\n-      state.multicast_device_ptr,\n-      /* device_ordinal= */ params.executor->device_ordinal(), state.metadata);\n+      state.multicast_device_ptr, params.executor->device_ordinal(),\n+      state.metadata);\n }\n \n absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n@@ -250,10 +252,12 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n \n   if (state != nullptr) {\n     if (strategy == AllReduceStrategy::kMultimem) {\n-      TF_ASSIGN_OR_RETURN(state->multicast_device_ptr,\n-                          address_space_provider_.SetupMultimemAddressSpace(\n-                              clique_key, params.executor,\n-                              state->local_buffers_handle.memory()));\n+      TF_ASSIGN_OR_RETURN(\n+          state->collective_multimem,\n+          CollectiveMultimem::Allocate(params.executor, clique_key, *rank,\n+                                       state->local_buffers_handle.memory()));\n+      state->multicast_device_ptr =\n+          state->collective_multimem->mapped_ptr(*rank);\n     }\n     TF_RETURN_IF_ERROR(ExchangeStateMetadata(clique_key, params, *state));\n   }"
        },
        {
            "sha": "ce2514635a7142a166f6e9b71a9ed07f1869e676",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=a8a509632cfd3d0913da76e48ddc225eff6efc72",
            "patch": "@@ -30,10 +30,13 @@ limitations under the License.*/\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n #include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n@@ -127,6 +130,7 @@ class CollectiveKernelThunk : public Thunk {\n     std::unique_ptr<se::Kernel> kernel;\n     uint32_t invocation_count = 0;\n \n+    std::shared_ptr<CollectiveMultimem> collective_multimem;\n     void* multicast_device_ptr = nullptr;\n \n     // Constructor to make OSS builds happy.\n@@ -168,7 +172,6 @@ class CollectiveKernelThunk : public Thunk {\n   // Reference to the buffer related information required for the collective.\n   std::vector<CollectiveThunk::Buffer> buffers_;\n \n-  CollectiveMetadataThunk::MultimemAddressSpaceProvider address_space_provider_;\n   // Guard access to the stream state across different threads (which control\n   // different streams).\n   absl::Mutex mutex_;"
        },
        {
            "sha": "70835b27bd0f65b3c06898e8b5bdcbc66b3baee8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 62,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=a8a509632cfd3d0913da76e48ddc225eff6efc72",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n+#include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n@@ -27,9 +28,11 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"google/protobuf/repeated_ptr_field.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -40,8 +43,6 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n-#include \"xla/stream_executor/gpu/gpu_executor.h\"\n-#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -241,7 +242,7 @@ absl::Status CollectiveMetadataThunk::ExecuteOnStream(\n absl::StatusOr<void*> CollectiveMetadataThunk::SetupMultimem(\n     const GpuCliqueKey& clique_key, const InitializeParams& params) {\n   se::DeviceMemoryBase memory_range;\n-  for (const CollectiveMetadataThunk::Buffer& parameter : parameters_) {\n+  for (const Buffer& parameter : parameters_) {\n     if (parameter.memory_space == xla::Layout::kGenericFastMemorySpace) {\n       TF_ASSIGN_OR_RETURN(\n           memory_range,\n@@ -252,72 +253,23 @@ absl::StatusOr<void*> CollectiveMetadataThunk::SetupMultimem(\n   }\n \n   // Since there is no parameter in the collective memory space, we don't need\n-  // to set up the multicast memory.\n+  // to set up the collective multimem.\n   if (memory_range.is_null()) {\n     return nullptr;\n   }\n \n-  if (!clique_key.is_local()) {\n-    return absl::UnimplementedError(absl::StrCat(\n-        XlaFormatDevice(params.executor->device_ordinal()),\n-        \"Multimem is not supported in multi-process mode in clique \",\n-        clique_key.ToString()));\n-  }\n+  GlobalDeviceId global_device_id = params.collective_params->global_device_id;\n \n-  return address_space_provider_.SetupMultimemAddressSpace(\n-      clique_key, params.executor, memory_range);\n-}\n+  std::optional<RankId> rank = clique_key.rank(global_device_id);\n+  TF_ASSIGN_OR_RETURN(std::shared_ptr<CollectiveMultimem> collective_multimem,\n+                      CollectiveMultimem::Allocate(params.executor, clique_key,\n+                                                   *rank, memory_range));\n \n-absl::Status Barrier(int device_number, const GpuCliqueKey& clique_key) {\n-  std::string start_rendezvous_key = absl::StrFormat(\n-      \"Barrier for device %d, \"\n-      \"clique %s\",\n-      device_number, clique_key.ToString());\n-  return Rendezvous(\n-      /*name=*/\n-      start_rendezvous_key, /*key=*/clique_key,\n-      /*num_threads=*/clique_key.num_local_participants());\n+  absl::MutexLock lock(mutex_);\n+  return (collective_multimem_[params.executor] =\n+              std::move(collective_multimem))\n+      ->mapped_ptr(*rank);\n }\n \n-absl::StatusOr<void*> CollectiveMetadataThunk::MultimemAddressSpaceProvider::\n-    SetupMultimemAddressSpace(const GpuCliqueKey& clique_key,\n-                              const se::StreamExecutor* stream_executor,\n-                              se::DeviceMemoryBase mapped_memory) {\n-  const auto* gpu_executor =\n-      dynamic_cast<const se::gpu::GpuExecutor*>(stream_executor);\n-  if (gpu_executor == nullptr) {\n-    return absl::UnimplementedError(\"Multicast is not supported on device.\");\n-  }\n-  int device_number = gpu_executor->device_ordinal();\n-  TF_RET_CHECK(clique_key.num_local_participants() > 0)\n-      << \"Number of local participants must be greater than 0.\";\n-  int64_t first_device = clique_key.devices()[0].value();\n-\n-  if (device_number == first_device) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<se::gpu::MulticastMemory> multicast_memory,\n-        gpu_executor->CreateMulticastMemory(\n-            mapped_memory.size(), clique_key.num_local_participants()));\n-    first_device_to_multicast_memory_.emplace(device_number,\n-                                              std::move(multicast_memory));\n-  }\n-\n-  // Wait for all devices to create the multicast object.\n-  TF_RETURN_IF_ERROR(Barrier(device_number, clique_key));\n-\n-  TF_RET_CHECK(first_device_to_multicast_memory_.contains(first_device))\n-      << \"Multicast memory is not created for device \" << first_device;\n-  // Add current devices to the multicast object.\n-  TF_RETURN_IF_ERROR(\n-      first_device_to_multicast_memory_[first_device]->SubscribeDevice(\n-          device_number));\n-\n-  // Wait for all devices to register the multicast object.\n-  TF_RETURN_IF_ERROR(Barrier(device_number, clique_key));\n-\n-  return first_device_to_multicast_memory_[first_device]->MapMemory(\n-      mapped_memory, gpu_executor);\n-};\n-\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "9fbd4a0f2660d481c6388e5cb2c0b06f8eebba65",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.h",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h?ref=a8a509632cfd3d0913da76e48ddc225eff6efc72",
            "patch": "@@ -21,44 +21,29 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n+\n class CollectiveMetadataThunk : public Thunk {\n  public:\n   struct Buffer {\n     BufferAllocation::Slice slice;\n     int64_t memory_space;\n   };\n \n-  class MultimemAddressSpaceProvider {\n-   public:\n-    // Initializes and multimem memory. Each thunk participant should call this\n-    // method once. Multimem should be setup before usage when multimem strategy\n-    // is selected.\n-    absl::StatusOr<void*> SetupMultimemAddressSpace(\n-        const GpuCliqueKey& clique_key,\n-        const se::StreamExecutor* stream_executor,\n-        se::DeviceMemoryBase mapped_memory);\n-\n-   private:\n-    absl::flat_hash_map<int,\n-                        std::unique_ptr<stream_executor::gpu::MulticastMemory>>\n-        first_device_to_multicast_memory_;\n-  };\n-\n   explicit CollectiveMetadataThunk(ThunkInfo thunk_info,\n                                    CollectiveConfig collective_config,\n                                    std::vector<Buffer> parameters,\n@@ -92,10 +77,13 @@ class CollectiveMetadataThunk : public Thunk {\n  private:\n   const CollectiveConfig collective_config_;\n   std::vector<Buffer> parameters_;\n-  MultimemAddressSpaceProvider address_space_provider_;\n   BufferAllocation::Slice result_;\n+\n+  absl::Mutex mutex_;\n+  absl::flat_hash_map<se::StreamExecutor*, std::shared_ptr<CollectiveMultimem>>\n+      collective_multimem_ ABSL_GUARDED_BY(mutex_);\n };\n-}  // namespace gpu\n-}  // namespace xla\n+\n+}  // namespace xla::gpu\n \n #endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_METADATA_THUNK_H_"
        },
        {
            "sha": "9749b021d8b4d2e8c3bd77d9fd1129dfde3d9a39",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.cc",
            "status": "added",
            "additions": 187,
            "deletions": 0,
            "changes": 187,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc?ref=a8a509632cfd3d0913da76e48ddc225eff6efc72",
            "patch": "@@ -0,0 +1,187 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/btree_map.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/service/rendezvous.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::gpu {\n+\n+CollectiveMultimem::CollectiveMultimem(\n+    GpuCliqueKey clique_key, absl::btree_map<RankId, void*> mapped_ptrs,\n+    std::unique_ptr<se::gpu::MulticastMemory> multicast_memory)\n+    : clique_key_(std::move(clique_key)),\n+      mapped_ptrs_(std::move(mapped_ptrs)),\n+      multicast_memory_(std::move(multicast_memory)) {}\n+\n+namespace {\n+\n+// Wrap GpuCliqueKey into a unique struct to guarantee we do not accidentally\n+// try to run multiple unrelated rendezvous for a same key.\n+struct AllocateRendezvousKey {\n+  GpuCliqueKey clique_key;\n+\n+  bool operator==(const AllocateRendezvousKey& other) const {\n+    return clique_key == other.clique_key;\n+  }\n+\n+  template <typename H>\n+  friend H AbslHashValue(H h, const AllocateRendezvousKey& key) {\n+    return H::combine(std::move(h), key.clique_key);\n+  }\n+};\n+\n+// Parameters passed to the rendezvous callback from all ranks.\n+struct AllocateParams {\n+  se::StreamExecutor* executor;\n+  RankId rank;\n+  se::DeviceMemoryBase map_to;\n+};\n+\n+struct RankCmp {\n+  bool operator()(const AllocateParams* a, const AllocateParams* b) const {\n+    return a->rank < b->rank;\n+  }\n+};\n+\n+struct RankFormatter {\n+  void operator()(std::string* out, const AllocateParams* param) const {\n+    absl::StrAppend(out, param->rank.value());\n+  }\n+};\n+\n+struct MappedPtrFormatter {\n+  void operator()(std::string* out,\n+                  const std::pair<RankId, void*>& mapped_ptr) const {\n+    auto& [rank, ptr] = mapped_ptr;\n+    absl::StrAppend(out, absl::StrFormat(\"%d:%p\", rank.value(), ptr));\n+  }\n+};\n+\n+}  // namespace\n+\n+absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n+CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n+                             const GpuCliqueKey& clique_key, RankId rank,\n+                             se::DeviceMemoryBase map_to) {\n+  VLOG(3) << absl::StrFormat(\n+      \"rank=[%d] Allocate collective multimem for clique: %s\", rank.value(),\n+      clique_key.ToString());\n+\n+  // We rely on in-process rendezvous to allocate the multicast memory and set\n+  // up memory mapping on all ranks, and don't support multi-process mode.\n+  if (!clique_key.is_local()) {\n+    return Unimplemented(\n+        \"%sMultimem is not supported in multi-process mode in clique %s\",\n+        XlaFormatDevice(executor->device_ordinal()), clique_key.ToString());\n+  }\n+\n+  std::string rendezvous_name = absl::StrFormat(\n+      \"CollectiveMultimem::Allocate for clique %s\", clique_key.ToString());\n+  AllocateRendezvousKey rendezvous_key = {clique_key};\n+  AllocateParams params = {executor, rank, map_to};\n+\n+  // A callback for rendezvous to allocate and map the multicast memory.\n+  auto allocate = [&](absl::Span<const AllocateParams*> params)\n+      -> absl::StatusOr<CollectiveMultimem> {\n+    // Sort all participants by rank to get deterministic execution.\n+    absl::c_sort(params, RankCmp{});\n+\n+    VLOG(3) << absl::StrFormat(\n+        \"ranks=[%s] Allocate collective multimem for clique: %s\",\n+        absl::StrJoin(params, \",\", RankFormatter{}), clique_key.ToString());\n+\n+    // We deterministically choose the first device to create the\n+    // multicast memory. We will map the rest of participants to it later.\n+    auto* gpu_executor =\n+        dynamic_cast<se::gpu::GpuExecutor*>(params[0]->executor);\n+    if (gpu_executor == nullptr) {\n+      return absl::UnimplementedError(\"Unsupported stream executor type\");\n+    }\n+\n+    TF_ASSIGN_OR_RETURN(\n+        std::unique_ptr<se::gpu::MulticastMemory> multicast_memory,\n+        gpu_executor->CreateMulticastMemory(params[0]->map_to.size(),\n+                                            params.size()));\n+\n+    // For all participating devices, subscribe to the multicast object.\n+    for (const auto* param : params) {\n+      TF_RETURN_IF_ERROR(\n+          multicast_memory->SubscribeDevice(param->executor->device_ordinal()));\n+    }\n+\n+    // For all participating devices, map to the multicast memory.\n+    absl::btree_map<RankId, void*> mapped_ptrs;\n+    for (const auto* param : params) {\n+      TF_ASSIGN_OR_RETURN(\n+          mapped_ptrs[param->rank],\n+          multicast_memory->MapMemory(\n+              param->map_to,\n+              dynamic_cast<se::gpu::GpuExecutor*>(param->executor)));\n+    }\n+\n+    VLOG(3) << absl::StrFormat(\n+        \"Allocated collective multimem for clique: %s; mapped_ptrs: [%s]\",\n+        clique_key.ToString(),\n+        absl::StrJoin(mapped_ptrs, \", \", MappedPtrFormatter{}));\n+\n+    return CollectiveMultimem(clique_key, std::move(mapped_ptrs),\n+                              std::move(multicast_memory));\n+  };\n+\n+  // We expect that all local participants will collectively allocate the\n+  // multicast memory.\n+  int64_t num_participants = clique_key.num_local_participants();\n+  return Rendezvous<CollectiveMultimem>(rendezvous_name, rendezvous_key, params,\n+                                        num_participants, allocate);\n+}\n+\n+absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n+CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n+                             const GpuCliqueKey& clique_key,\n+                             GlobalDeviceId global_device_id,\n+                             se::DeviceMemoryBase map_to) {\n+  if (std::optional<RankId> rank = clique_key.rank(global_device_id)) {\n+    return Allocate(executor, clique_key, *rank, map_to);\n+  }\n+  return InvalidArgument(\"Rank not found for device %v\", global_device_id);\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "b3cd226baddd01ff4ce9bd7124b5175e3026be01",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.h",
            "status": "added",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8a509632cfd3d0913da76e48ddc225eff6efc72/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h?ref=a8a509632cfd3d0913da76e48ddc225eff6efc72",
            "patch": "@@ -0,0 +1,81 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_\n+\n+#include <memory>\n+#include <optional>\n+\n+#include \"absl/container/btree_map.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::gpu {\n+\n+// CollectiveMultimem is a collection of per-device virtual memory ranges\n+// registered with the multicast memory.\n+class CollectiveMultimem {\n+ public:\n+  // Allocates a CollectiveMultimem for the given clique key and rank.\n+  //\n+  // This is a collective operation that must be called concurrently by all\n+  // participating devices in the clique. Implementation relies on the\n+  // rendezvous synchronization to ensure that all ranks arrive to the barrier.\n+  // The result is collectively owned by all participants.\n+  //\n+  // Rendezvous leader creates a multicast memory and maps all per-device\n+  // memories passed as `map_to` argument to the created multicast memory. Each\n+  // rank then gets a virtual memory address bound to the multicast memory, and\n+  // operations performed via this pointer gets broadcasted to all participating\n+  // devices.\n+  static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n+      se::StreamExecutor* executor, const GpuCliqueKey& clique_key, RankId rank,\n+      se::DeviceMemoryBase map_to);\n+\n+  // Allocates a CollectiveMultimem for the given global device id.\n+  static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n+      se::StreamExecutor* executor, const GpuCliqueKey& clique_key,\n+      GlobalDeviceId global_device_id, se::DeviceMemoryBase map_to);\n+\n+  const GpuCliqueKey& clique_key() const { return clique_key_; }\n+\n+  // Returns the device pointer to the multicast memory for the given rank.\n+  void* mapped_ptr(RankId rank) const { return mapped_ptrs_.at(rank); }\n+\n+ private:\n+  CollectiveMultimem(\n+      GpuCliqueKey clique_key, absl::btree_map<RankId, void*> mapped_ptrs,\n+      std::unique_ptr<se::gpu::MulticastMemory> multicast_memory);\n+\n+  // All devices in this clique will have access to the multicast memory.\n+  GpuCliqueKey clique_key_;\n+\n+  // A mapping from a participating rank to the mapped virtual memory pointer.\n+  absl::btree_map<RankId, void*> mapped_ptrs_;\n+\n+  // All virtual memory pointers are registered with this multicast memory.\n+  std::unique_ptr<se::gpu::MulticastMemory> multicast_memory_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_"
        }
    ],
    "stats": {
        "total": 433,
        "additions": 341,
        "deletions": 92
    }
}