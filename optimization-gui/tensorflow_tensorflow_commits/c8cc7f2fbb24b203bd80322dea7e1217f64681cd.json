{
    "author": "chsigg",
    "message": "[XLA:GPU] Enable generic triton emitter for all gemms, second attempt.\n\nAccording to benchmarks we have reached the neutrality with the legacy emitter. Switching to the new emitter by default. Legacy emitter will be kept for some time but is considered depricated and should not be used. It will be deleted in the near future.\n\nReverts 85c99b1ecb953424a031f172820efacfe73f9613\n\nPiperOrigin-RevId: 823475406",
    "sha": "c8cc7f2fbb24b203bd80322dea7e1217f64681cd",
    "files": [
        {
            "sha": "f3ae30cdba7cac6af6d38c93cfd783bdff9868f1",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=c8cc7f2fbb24b203bd80322dea7e1217f64681cd",
            "patch": "@@ -325,6 +325,10 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   // default value of the command line flag in `MakeDebugOptionsFlags`.\n   opts.add_xla_gpu_unsupported_generic_triton_emitter_features(\n       DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n+  opts.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+      DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n+  opts.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+      DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n   opts.set_xla_gpu_unsupported_enable_triton_multi_output_fusion(true);\n   opts.set_xla_gpu_enable_cudnn_int8x32_convolution_reordering(true);\n   opts.set_xla_gpu_triton_gemm_any(true);"
        },
        {
            "sha": "76582a0795013261d7749a739ff70789c22c3496",
            "filename": "third_party/xla/xla/debug_options_parsers_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_parsers_test.cc?ref=c8cc7f2fbb24b203bd80322dea7e1217f64681cd",
            "patch": "@@ -390,35 +390,37 @@ TEST(ParseRepeatedEnumFlagsTest, GenericTritonEmitterFeatures) {\n   const auto& enabled_features =\n       debug_options.xla_gpu_unsupported_generic_triton_emitter_features();\n \n-  // Check that the default setting is empty.\n+  // Check default setting.\n   ASSERT_THAT(\n       enabled_features,\n-      ElementsAre(DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM));\n+      testing::UnorderedElementsAre(\n+          DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM,\n+          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES,\n+          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION));\n \n   // Initialize the flag objects.\n   std::vector<tsl::Flag> flag_objects;\n   MakeDebugOptionsFlags(&flag_objects, &debug_options);\n \n   // Adding options.\n   SetXlaFlagsEnvVar(\n-      \"--xla_gpu_unsupported_generic_triton_emitter_features=+allow_all_gemm_\"\n-      \"shapes\");\n+      \"--xla_gpu_unsupported_generic_triton_emitter_features=\"\n+      \"-allow_all_gemm_shapes\");\n   ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n-  EXPECT_EQ(enabled_features.size(), 2);\n   EXPECT_THAT(\n       enabled_features,\n-      ElementsAre(DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM,\n-                  DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES));\n+      testing::UnorderedElementsAre(\n+          DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM,\n+          DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION));\n \n   // Overwriting options.\n   SetXlaFlagsEnvVar(\n       \"--xla_gpu_unsupported_generic_triton_emitter_features=disable_legacy_\"\n       \"gemm,allow_all_ops_in_gemm_fusion\");\n   ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n-  EXPECT_EQ(enabled_features.size(), 2);\n   EXPECT_THAT(\n       enabled_features,\n-      ElementsAre(\n+      testing::UnorderedElementsAre(\n           DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM,\n           DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION));\n \n@@ -427,10 +429,9 @@ TEST(ParseRepeatedEnumFlagsTest, GenericTritonEmitterFeatures) {\n       \"--xla_gpu_unsupported_generic_triton_emitter_features=-disable_legacy_\"\n       \"gemm,-unspecified,+enable_nested_gemm,+allow_all_ops_in_gemm_fusion\");\n   ParseFlagsFromEnvAndDieIfUnknown(\"XLA_FLAGS\", flag_objects);\n-  EXPECT_EQ(enabled_features.size(), 2);\n   EXPECT_THAT(\n       enabled_features,\n-      ElementsAre(\n+      testing::UnorderedElementsAre(\n           DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION,\n           DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM));\n }"
        },
        {
            "sha": "08ec663cd0fc7c000b9b32727009b2e4c0c495ba",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=c8cc7f2fbb24b203bd80322dea7e1217f64681cd",
            "patch": "@@ -1068,8 +1068,7 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n       if (code == absl::StatusCode::kInternal ||\n           code == absl::StatusCode::kFailedPrecondition ||\n           code == absl::StatusCode::kUnimplemented ||\n-          (debug_options_.xla_gpu_exhaustive_tiling_search() &&\n-           code == absl::StatusCode::kInvalidArgument)) {\n+          code == absl::StatusCode::kInvalidArgument) {\n         VLOG(5) << \"Compilation failed with status \" << executable_or.status()\n                 << \" that is ignored\";\n         return nullptr;"
        },
        {
            "sha": "aad712107a7e1fd8dcd8a2b7b79277a955fd8d95",
            "filename": "third_party/xla/xla/service/gpu/determinism_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc?ref=c8cc7f2fbb24b203bd80322dea7e1217f64681cd",
            "patch": "@@ -50,10 +50,6 @@ class DeterminismTest : public GpuCodegenTest {\n  public:\n   DeterminismTest() : debug_options_(HloTestBase::GetDebugOptionsForTest()) {\n     debug_options_.set_xla_gpu_exclude_nondeterministic_ops(true);\n-    // TODO(b/393299275): remove when the flag is enabled by default.\n-    debug_options_.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-    debug_options_.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n   }\n \n   se::CudaComputeCapability get_cuda_cc() const {"
        },
        {
            "sha": "4c0c9a74109c6479a23e4f6bba8fda1c3380ed69",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c8cc7f2fbb24b203bd80322dea7e1217f64681cd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc?ref=c8cc7f2fbb24b203bd80322dea7e1217f64681cd",
            "patch": "@@ -192,7 +192,8 @@ ENTRY e {\n               GmockMatch(match::Concatenate(match::Fusion(), match::Fusion())));\n }\n \n-TEST_F(NestGemmFusionTest, UnsupportedComputationsAreNotChanged) {\n+// TODO(b/393299275): update test to use a unsupported operation.\n+TEST_F(NestGemmFusionTest, DISABLED_UnsupportedComputationsAreNotChanged) {\n   // Fusions other than kTritonNestedGemmFusionKind are not supported.\n   // In this case pass should only change the supported fusions.\n   absl::string_view hlo = R\"("
        }
    ],
    "stats": {
        "total": 37,
        "additions": 19,
        "deletions": 18
    }
}