{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Move libdevice math calls to a rewrite pass.\n\nPiperOrigin-RevId: 827408949",
    "sha": "f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
    "files": [
        {
            "sha": "a8035f8517fdaafd702e56e38aa917191705797f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -249,11 +249,13 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"//xla/tsl/framework/mlir:status_scoped_diagnostic_handler\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\","
        },
        {
            "sha": "8a99afed13bd4a5c85b8c5b3145389629cdca1de",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 5,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -378,6 +378,8 @@ bool IsSupportedElementwiseLibdeviceFunction(const HloInstruction& hlo) {\n          output_type == PrimitiveType::F32 || output_type == PrimitiveType::F64;\n }\n \n+// TODO(willfroom): Remove this (and associated functions) once the legacy\n+// matmul is removed.\n absl::StatusOr<Value> EmitElementwiseLibdeviceFunction(\n     EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info, const HloInstruction& hlo,\n@@ -419,14 +421,9 @@ absl::StatusOr<Value> EmitElementwiseLibdeviceFunction(\n }\n \n absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n-                                      absl::string_view libdevice_path,\n                                       const se::DeviceDescription& device_info,\n                                       const HloInstruction& hlo,\n                                       ValueRange inputs) {\n-  if (IsSupportedElementwiseLibdeviceFunction(hlo)) {\n-    return EmitElementwiseLibdeviceFunction(b, libdevice_path, device_info, hlo,\n-                                            inputs);\n-  }\n   const bool is_integer =\n       mlir::isa<mlir::IntegerType>(getElementTypeOrSelf(inputs[0].getType()));\n \n@@ -504,6 +501,50 @@ absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n     case HloOpcode::kReducePrecision:\n       return mh::reducePrecision<mlir::tensor::BitcastOp>(\n           b.getLoc(), inputs[0], hlo.exponent_bits(), hlo.mantissa_bits(), &b);\n+    case HloOpcode::kAcos:\n+      return b.create<mm::AcosOp>(inputs[0]);\n+    case HloOpcode::kAcosh:\n+      return b.create<mm::AcoshOp>(inputs[0]);\n+    case HloOpcode::kAsin:\n+      return b.create<mm::AsinOp>(inputs[0]);\n+    case HloOpcode::kAsinh:\n+      return b.create<mm::AsinhOp>(inputs[0]);\n+    case HloOpcode::kAtan2:\n+      return b.create<mm::Atan2Op>(inputs[0], inputs[1]);\n+    case HloOpcode::kAtanh:\n+      return b.create<mm::AtanhOp>(inputs[0]);\n+    case HloOpcode::kCos:\n+      return b.create<mm::CosOp>(inputs[0]);\n+    case HloOpcode::kCosh:\n+      return b.create<mm::CoshOp>(inputs[0]);\n+    case HloOpcode::kExp:\n+      return b.create<mm::ExpOp>(inputs[0]);\n+    case HloOpcode::kErf:\n+      return b.create<mm::ErfOp>(inputs[0]);\n+    case HloOpcode::kExpm1:\n+      return b.create<mm::ExpM1Op>(inputs[0]);\n+    case HloOpcode::kLog:\n+      return b.create<mm::LogOp>(inputs[0]);\n+    case HloOpcode::kLog1p:\n+      return b.create<mm::Log1pOp>(inputs[0]);\n+    case HloOpcode::kPower:\n+      return b.create<mm::PowFOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kRemainder:\n+      return b.create<ma::RemFOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kRsqrt:\n+      return b.create<mm::RsqrtOp>(inputs[0]);\n+    case HloOpcode::kSin:\n+      return b.create<mm::SinOp>(inputs[0]);\n+    case HloOpcode::kSinh:\n+      return b.create<mm::SinhOp>(inputs[0]);\n+    case HloOpcode::kSqrt:\n+      return b.create<mm::SqrtOp>(inputs[0]);\n+    case HloOpcode::kTan:\n+      return b.create<mm::TanOp>(inputs[0]);\n+    case HloOpcode::kTanh:\n+      return b.create<mm::TanhOp>(inputs[0]);\n+    case HloOpcode::kCbrt:\n+      return b.create<mm::CbrtOp>(inputs[0]);\n     default:\n       return absl::InvalidArgumentError(\n           absl::StrCat(\"Unsupported elementwise operation \", hlo.ToString()));"
        },
        {
            "sha": "d1b641c507452514b7b612e419a6b3f7b66db57e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -212,9 +212,8 @@ absl::StatusOr<mlir::Value> EmitElementwiseLibdeviceFunction(\n     mlir::ValueRange inputs);\n \n absl::StatusOr<mlir::Value> EmitElementwise(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info, const HloInstruction& hlo,\n-    mlir::ValueRange inputs);\n+    EmitterLocOpBuilder& b, const se::DeviceDescription& device_info,\n+    const HloInstruction& hlo, mlir::ValueRange inputs);\n \n mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n "
        },
        {
            "sha": "c61afec73d39bd81bf2623c1bb796d31f849fd50",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 9,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"llvm/Support/FileSystem.h\"\n #include \"llvm/Support/LogicalResult.h\"\n #include \"llvm/Support/raw_ostream.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/Conversion/AffineToStandard/AffineToStandard.h\"\n #include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n #include \"mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h\"\n@@ -145,6 +146,7 @@ limitations under the License.\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tools/hlo_decomposer.h\"\n+#include \"xla/tsl/framework/mlir/status_scoped_diagnostic_handler.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -1535,9 +1537,8 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n     for (const TiledHloInstruction* operand : tiled_hlo.operands()) {\n       operands.push_back(values[operand].UnwrapUnsafe());\n     }\n-    TF_ASSIGN_OR_RETURN(\n-        Value result,\n-        EmitElementwise(b, libdevice_path, device_info, *hlo, operands));\n+    TF_ASSIGN_OR_RETURN(Value result,\n+                        EmitElementwise(b, device_info, *hlo, operands));\n     return ScalarOrTensor(result);\n   }\n \n@@ -1657,9 +1658,8 @@ absl::StatusOr<ScalarOrTensor> EmitScope(\n       for (const HloInstruction* operand : hlo->operands()) {\n         operands.push_back(values[operand].UnwrapUnsafe());\n       }\n-      TF_ASSIGN_OR_RETURN(\n-          Value elementwise_result,\n-          EmitElementwise(b, libdevice_path, device_info, *hlo, operands));\n+      TF_ASSIGN_OR_RETURN(Value elementwise_result,\n+                          EmitElementwise(b, device_info, *hlo, operands));\n       result = ScalarOrTensor(elementwise_result);\n     } else if (hlo->opcode() == HloOpcode::kTuple) {\n       TF_RET_CHECK(hlo->IsRoot()) << hlo->ToString();\n@@ -1993,7 +1993,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n   }\n \n   TF_RETURN_IF_ERROR(ir_emitter_triton_internal::LowerXTileToTriton(\n-      triton_module.get(), mlir_context, *fusion));\n+      triton_module.get(), mlir_context, *fusion, device_info));\n \n   VLOG(6) << DumpTritonIR(triton_module.get(),\n                           fusion->GetModule()\n@@ -2337,15 +2337,28 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n \n absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n                                 mlir::MLIRContext& mlir_context,\n-                                const HloFusionInstruction& fusion) {\n+                                const HloFusionInstruction& fusion,\n+                                const se::DeviceDescription& device_info) {\n   {  // Convert xTile ops to Triton ops.\n     mlir::PassManager pm(&mlir_context);\n     // Disable verifier because the Triton code may be invalid due to the\n     // unsupported types.\n     pm.enableVerifier(/*enabled=*/false);\n     pm.addPass(mlir::triton::xla::CreateTensorLowerToTritonPass());\n     pm.addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n-    if (mlir::failed(pm.run(xtile_dialect_module))) {\n+\n+    std::string libdevice_path =\n+        GetLibdevicePath(fusion.GetModule()->config(), device_info);\n+    absl::string_view triple = device_info.gpu_compute_capability().IsRocm()\n+                                   ? \"amdgcn-unknown-unknown\"\n+                                   : \"nvptx64-unknown-unknown\";\n+    pm.addPass(mlir::triton::xla::CreateTritonXLAMathToLibdevicePass(\n+        libdevice_path, triple));\n+\n+    tsl::StatusScopedDiagnosticHandler diagnostic_handler(&mlir_context);\n+    if (absl::Status status =\n+            diagnostic_handler.consumeStatus(pm.run(xtile_dialect_module));\n+        !status.ok()) {\n       return CreateInternalError(\n           \"Failed to lower from shared dialect to Triton.\", &fusion,\n           xtile_dialect_module);"
        },
        {
            "sha": "2c72d8d904db4fc7573a68d0cc6cd23b0ae0895b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -163,7 +163,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n // dialect module.\n absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n                                 mlir::MLIRContext& mlir_context,\n-                                const HloFusionInstruction& fusion);\n+                                const HloFusionInstruction& fusion,\n+                                const se::DeviceDescription& device_info);\n \n }  // namespace ir_emitter_triton_internal\n }  // namespace gpu"
        },
        {
            "sha": "b9df5db237f833728f14b90b84722c0e6503818f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -187,7 +187,7 @@ absl::Status LowerXTileIrToTritonAndFileCheck(\n     const HloFusionInstruction& fusion) {\n   TF_RETURN_IF_ERROR(ir_emitter_triton_internal::LowerXTileToTriton(\n       xtile_dialect_module, *test->symbolic_expr_context()->GetMLIRContext(),\n-      fusion));\n+      fusion, TestGpuDeviceInfo::RTXH100SXMDeviceInfo()));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);"
        },
        {
            "sha": "598d811d6e18ff49c2c2f122aedbb593e918dfb4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -45,6 +45,7 @@ cc_library(\n         \"triton_xla_lower_get_tid_pass.cc\",\n         \"triton_xla_lower_remote_access_pass.cc\",\n         \"triton_xla_lower_xtile_pass.cc\",\n+        \"triton_xla_math_to_libdevice.cc\",\n         \"triton_xla_squeeze_dims_pass.cc\",\n         \"triton_xla_unswitch_loops_pass.cc\",\n     ],\n@@ -58,7 +59,9 @@ cc_library(\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/xtile/ir:xtile\",\n+        \"//xla/service/gpu:target_util\",\n         \"//xla/service/llvm_ir:llvm_util\",\n+        \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -71,6 +74,7 @@ cc_library(\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//mlir:Analysis\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:FuncDialect\",\n@@ -79,6 +83,7 @@ cc_library(\n         \"@llvm-project//mlir:InliningUtils\",\n         \"@llvm-project//mlir:LLVMCommonConversion\",\n         \"@llvm-project//mlir:LLVMDialect\",\n+        \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MemRefDialect\",\n         \"@llvm-project//mlir:NVVMDialect\",\n         \"@llvm-project//mlir:Pass\","
        },
        {
            "sha": "b0b24556df24ad2552ce968544de080537a48dbb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include <memory>\n \n+#include \"absl/strings/string_view.h\"\n #include \"llvm/ADT/STLFunctionalExtras.h\"\n #include \"mlir/IR/Operation.h\"\n #include \"mlir/Pass/Pass.h\"\n@@ -49,6 +50,8 @@ std::unique_ptr<mlir::Pass> CreateTritonXLALowerRemoteAccessPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLALowerXTilePass();\n std::unique_ptr<mlir::Pass> CreateStableHLOLowerToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTensorLowerToTritonPass();\n+std::unique_ptr<mlir::Pass> CreateTritonXLAMathToLibdevicePass(\n+    absl::string_view libdevice_path, absl::string_view triple);\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "3bd60e67a5a88f5a66ba099f1971bd527efac0fe",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -227,4 +227,19 @@ def TensorLowerToTritonPass\n   let constructor = \"CreateTensorLowerToTritonPass()\";\n }\n \n+def TritonXLAMathToLibdevicePass\n+    : Pass<\"triton-xla-math-to-libdevice\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lowers math operations to tt.extern_elementwise calls to their libdevice equivalent.\";\n+  let dependentDialects = [\n+    \"tensor::TensorDialect\",\n+    \"triton::TritonDialect\"\n+  ];\n+  let options = [\n+    Option<\"libdevice_path_\", \"libdevice_path\", \"std::string\", \"\",\n+           \"Path to the libdevice library.\">,\n+    Option<\"triple_string_\", \"triple\", \"std::string\", \"\",\n+           \"Device triple string.\">,\n+  ];\n+}\n+\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "09d35c269367a07409abbc3bdaf805ae3e8612d9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_math_to_libdevice.mlir",
            "status": "added",
            "additions": 296,
            "deletions": 0,
            "changes": 296,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_math_to_libdevice.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_math_to_libdevice.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_math_to_libdevice.mlir?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -0,0 +1,296 @@\n+// RUN: xla-opt %s -split-input-file -triton-xla-math-to-libdevice=' \\\n+// RUN: libdevice_path=/path/to/libdevice triple=nvptx64-unknown-unknown' \\\n+// RUN: | FileCheck %s\n+\n+func.func @main(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.acos %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_acosf\"}\n+\n+// -----\n+\n+func.func @acosh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.acosh %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_acoshf\"}\n+\n+// -----\n+\n+func.func @asin(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.asin %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_asinf\"}\n+\n+// -----\n+\n+func.func @asinh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.asinh %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_asinhf\"}\n+\n+// -----\n+\n+func.func @atan2(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.atan2 %arg0, %arg1 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0, %arg1\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_atan2f\"}\n+\n+// -----\n+\n+func.func @atanh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.atanh %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_atanhf\"}\n+\n+// -----\n+\n+func.func @cos(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.cos %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_cosf\"}\n+\n+// -----\n+\n+func.func @cosh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.cosh %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_coshf\"}\n+\n+// -----\n+\n+func.func @exp(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.exp %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_expf\"}\n+\n+// -----\n+\n+func.func @exp_bf16(%arg0: tensor<1024xbf16>) -> tensor<1024xbf16> {\n+  %result = math.exp %arg0 : tensor<1024xbf16>\n+  return %result : tensor<1024xbf16>\n+}\n+\n+// CHECK:       %[[CAST:.*]] = arith.extf %arg0 : tensor<1024xbf16> to tensor<1024xf32>\n+// CHECK:       tt.extern_elementwise %[[CAST]]\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_fast_expf\"}\n+// CHECK:       arith.truncf {{.*}} : tensor<1024xf32> to tensor<1024xbf16>\n+\n+// -----\n+\n+func.func @exp_f16(%arg0: tensor<1024xf16>) -> tensor<1024xf16> {\n+  %result = math.exp %arg0 : tensor<1024xf16>\n+  return %result : tensor<1024xf16>\n+}\n+\n+// CHECK:       %[[CAST:.*]] = arith.extf %arg0 : tensor<1024xf16> to tensor<1024xf32>\n+// CHECK:       tt.extern_elementwise %[[CAST]]\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_fast_expf\"}\n+// CHECK:       arith.truncf {{.*}} : tensor<1024xf32> to tensor<1024xf16>\n+\n+// -----\n+\n+func.func @erf(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.erf %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_erff\"}\n+\n+// -----\n+\n+func.func @expm1(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.expm1 %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_expm1f\"}\n+\n+// -----\n+\n+func.func @log(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.log %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_logf\"}\n+\n+// -----\n+\n+\n+func.func @log_bf16(%arg0: tensor<1024xbf16>) -> tensor<1024xbf16> {\n+  %result = math.log %arg0 : tensor<1024xbf16>\n+  return %result : tensor<1024xbf16>\n+}\n+\n+// CHECK:       %[[CAST:.*]] = arith.extf %arg0 : tensor<1024xbf16> to tensor<1024xf32>\n+// CHECK:       tt.extern_elementwise %[[CAST]]\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_fast_logf\"}\n+// CHECK:       arith.truncf {{.*}} : tensor<1024xf32> to tensor<1024xbf16>\n+\n+// -----\n+\n+func.func @log_f16(%arg0: tensor<1024xf16>) -> tensor<1024xf16> {\n+  %result = math.log %arg0 : tensor<1024xf16>\n+  return %result : tensor<1024xf16>\n+}\n+\n+// CHECK:       %[[CAST:.*]] = arith.extf %arg0 : tensor<1024xf16> to tensor<1024xf32>\n+// CHECK:       tt.extern_elementwise %[[CAST]]\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_fast_logf\"}\n+// CHECK:       arith.truncf {{.*}} : tensor<1024xf32> to tensor<1024xf16>\n+\n+// -----\n+\n+func.func @log1p(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.log1p %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_log1pf\"}\n+\n+// -----\n+\n+func.func @powf(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.powf %arg0, %arg1 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0, %arg1\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_powf\"}\n+\n+// -----\n+\n+func.func @remf(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = arith.remf %arg0, %arg1 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0, %arg1\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_fmodf\"}\n+\n+// -----\n+\n+func.func @rsqrt(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.rsqrt %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_rsqrtf\"}\n+\n+// -----\n+\n+func.func @sin(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.sin %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_sinf\"}\n+\n+// -----\n+\n+func.func @sinh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.sinh %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_sinhf\"}\n+\n+// -----\n+\n+func.func @sqrt(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.sqrt %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_sqrtf\"}\n+\n+// -----\n+\n+func.func @tan(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.tan %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_tanf\"}\n+\n+// -----\n+\n+func.func @tanh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.tanh %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_tanhf\"}\n+\n+// -----\n+\n+func.func @cbrt(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n+  %result = math.cbrt %arg0 : tensor<1024xf32>\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK:       tt.extern_elementwise %arg0\n+// CHECK-SAME:    {libname = \"libdevice\", libpath = \"/path/to/libdevice\",\n+// CHECK-SAME:    pure = true, symbol = \"__nv_cbrtf\"}"
        },
        {
            "sha": "61a4edcb3ec6c140c3bf33886b5c3fb7789c9530",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_math_to_libdevice.cc",
            "status": "added",
            "additions": 280,
            "deletions": 0,
            "changes": 280,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc?ref=f9cbfa3b3c3a115405bafe1b645b45f8baa0d31d",
            "patch": "@@ -0,0 +1,280 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Math/IR/Math.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/service/gpu/target_util.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir::triton::xla {\n+\n+#define GEN_PASS_DEF_TRITONXLAMATHTOLIBDEVICEPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+using ::xla::gpu::TargetDeviceFunctionID;\n+\n+template <typename OpTy>\n+struct OpInfo;\n+\n+template <>\n+struct OpInfo<mlir::math::AcosOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kAcos;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::AcoshOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kAcosh;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::AsinOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kAsin;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::AsinhOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kAsinh;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::Atan2Op> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kAtan2;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::AtanhOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kAtanh;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::CosOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kCos;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::CoshOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kCosh;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::ExpOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kExp;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::ErfOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kErf;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::ExpM1Op> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kExpm1;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::LogOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kLog;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::Log1pOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kLog1p;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::PowFOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kPow;\n+};\n+\n+template <>\n+struct OpInfo<mlir::arith::RemFOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kFmod;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::RsqrtOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kRsqrt;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::SinOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kSin;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::SinhOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kSinh;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::SqrtOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kSqrt;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::TanOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kTan;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::TanhOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kTanh;\n+};\n+\n+template <>\n+struct OpInfo<mlir::math::CbrtOp> {\n+  static constexpr auto kFunctionID = TargetDeviceFunctionID::kCbrt;\n+};\n+\n+template <typename OpTy>\n+class ConvertToLibdevice : public mlir::OpRewritePattern<OpTy> {\n+ public:\n+  ConvertToLibdevice(mlir::MLIRContext* context,\n+                     absl::string_view libdevice_path,\n+                     const llvm::Triple& triple)\n+      : mlir::OpRewritePattern<OpTy>(context),\n+        libdevice_path_(libdevice_path),\n+        triple_(triple) {}\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      OpTy op, mlir::PatternRewriter& rewriter) const override {\n+    auto maybe_shaped_type = mlir::dyn_cast<mlir::ShapedType>(op.getType());\n+    mlir::Type output_type =\n+        maybe_shaped_type ? maybe_shaped_type.getElementType() : op.getType();\n+\n+    bool output_type_is_16bit_float =\n+        output_type.isBF16() || output_type.isF16();\n+    if (!(output_type_is_16bit_float || output_type.isF32() ||\n+          output_type.isF64())) {\n+      op.emitError() << \"unsupported output type\";\n+      return rewriter.notifyMatchFailure(op, \"unsupported output type\");\n+    }\n+\n+    absl::StatusOr<::xla::PrimitiveType> primitive_type_or =\n+        ::xla::gpu::triton::GetPrimitiveType(output_type);\n+    if (!primitive_type_or.ok()) {\n+      return rewriter.notifyMatchFailure(op, \"could not get primitive type\");\n+    }\n+\n+    ::xla::EmitterLocOpBuilder builder(op->getLoc(), rewriter);\n+\n+    llvm::SmallVector<Value, 2> casted_inputs;\n+    if (output_type_is_16bit_float) {\n+      // Upcast the inputs to F32.\n+      for (auto operand : op->getOperands()) {\n+        casted_inputs.push_back(\n+            ::xla::gpu::triton::Cast(builder, operand, rewriter.getF32Type()));\n+      }\n+    } else {\n+      casted_inputs = llvm::to_vector(op->getOperands());\n+    }\n+\n+    Value res = mlir::triton::ExternElementwiseOp::create(\n+        builder, casted_inputs[0].getType(), casted_inputs, \"libdevice\",\n+        libdevice_path_,\n+        ObtainDeviceFunctionName(OpInfo<OpTy>::kFunctionID, *primitive_type_or,\n+                                 triple_),\n+        /*pure=*/true);\n+\n+    if (res.getType() != output_type) {\n+      // Downcast back to the original output type.\n+      res = ::xla::gpu::triton::Cast(builder, res, output_type);\n+    }\n+\n+    rewriter.replaceOp(op, res);\n+\n+    return mlir::success();\n+  }\n+\n+ private:\n+  // These are both owned by the parent pass (TritonXLAMathToLibdevicePass), so\n+  // it is safe to store references here.\n+  absl::string_view libdevice_path_;\n+  const llvm::Triple& triple_;\n+};\n+\n+template <typename... OpTypes>\n+void AddPattens(mlir::RewritePatternSet& patterns,\n+                absl::string_view libdevice_path, const llvm::Triple& triple) {\n+  patterns.add<ConvertToLibdevice<OpTypes>...>(patterns.getContext(),\n+                                               libdevice_path, triple);\n+}\n+\n+class TritonXLAMathToLibdevicePass\n+    : public impl::TritonXLAMathToLibdevicePassBase<\n+          TritonXLAMathToLibdevicePass> {\n+ public:\n+  using TritonXLAMathToLibdevicePassBase::TritonXLAMathToLibdevicePassBase;\n+\n+ private:\n+  void runOnOperation() override {\n+    mlir::ModuleOp module = getOperation();\n+    mlir::MLIRContext* context = &getContext();\n+\n+    mlir::RewritePatternSet patterns(context);\n+\n+    llvm::Triple triple(triple_string_);\n+\n+    AddPattens<mlir::math::AcosOp, mlir::math::AcoshOp, mlir::math::AsinOp,\n+               mlir::math::AsinhOp, mlir::math::Atan2Op, mlir::math::AtanhOp,\n+               mlir::math::CosOp, mlir::math::CoshOp, mlir::math::ExpOp,\n+               mlir::math::ErfOp, mlir::math::ExpM1Op, mlir::math::LogOp,\n+               mlir::math::Log1pOp, mlir::math::PowFOp, mlir::arith::RemFOp,\n+               mlir::math::RsqrtOp, mlir::math::SinOp, mlir::math::SinhOp,\n+               mlir::math::SqrtOp, mlir::math::TanOp, mlir::math::TanhOp,\n+               mlir::math::CbrtOp>(patterns, libdevice_path_, triple);\n+\n+    if (mlir::failed(\n+            mlir::applyPatternsGreedily(module, std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<Pass> CreateTritonXLAMathToLibdevicePass(\n+    absl::string_view libdevice_path, absl::string_view triple) {\n+  TritonXLAMathToLibdevicePassOptions options;\n+  options.libdevice_path_ = libdevice_path;\n+  options.triple_string_ = triple;\n+\n+  return std::make_unique<TritonXLAMathToLibdevicePass>(options);\n+}\n+\n+}  // namespace mlir::triton::xla"
        }
    ],
    "stats": {
        "total": 693,
        "additions": 674,
        "deletions": 19
    }
}