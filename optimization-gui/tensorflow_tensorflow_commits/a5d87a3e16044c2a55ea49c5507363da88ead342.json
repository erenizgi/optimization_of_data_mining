{
    "author": "mrguenther",
    "message": "Add several unary op folders based on a new template.\n\nAdd a template for unary-op folder patterns, and implement several new folder patterns based on that template.\n\nPiperOrigin-RevId: 810204888",
    "sha": "a5d87a3e16044c2a55ea49c5507363da88ead342",
    "files": [
        {
            "sha": "32e88681d296a184031c169500d5435386f9d75b",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 941,
            "deletions": 4,
            "changes": 945,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a5d87a3e16044c2a55ea49c5507363da88ead342/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a5d87a3e16044c2a55ea49c5507363da88ead342/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=a5d87a3e16044c2a55ea49c5507363da88ead342",
            "patch": "@@ -1,3 +1,14 @@\n+diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel\n+--- stablehlo/BUILD.bazel\n++++ stablehlo/BUILD.bazel\n+@@ -1231,6 +1231,7 @@\n+     strip_include_prefix = \".\",\n+     deps = [\n+         \":base\",\n++        \":chlo_ops\",\n+         \":stablehlo_aggressive_simplification_inc_gen\",\n+         \":stablehlo_ops\",\n+         \":stablehlo_pass_inc_gen\",\n diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir b/stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir\n --- stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir\n +++ stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir\n@@ -247,7 +258,428 @@ diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -748,6 +748,19 @@\n+@@ -712,18 +712,412 @@\n+ // -----\n+ \n+ ////////\n++// AbsOp\n++\n++// CHECK-LABEL: func @fold_abs\n++func.func @fold_abs() -> (tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[INT_ZERO:%.*]] = stablehlo.constant dense<0> : tensor<i32>\n++  // CHECK-DAG: [[INT_TEN:%.*]] = stablehlo.constant dense<10> : tensor<i32>\n++  // CHECK-DAG: [[FLOAT_ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[FLOAT_HALF:%.*]] = stablehlo.constant dense<5.0{{.*}}e-01> : tensor<f32>\n++  // CHECK-DAG: [[FLOAT_INF:%.*]] = stablehlo.constant dense<0x7F800000> : tensor<f32>\n++  // CHECK:     return [[INT_ZERO]], [[INT_TEN]], [[INT_TEN]], [[FLOAT_ZERO]], [[FLOAT_HALF]], [[FLOAT_HALF]], [[FLOAT_INF]], [[FLOAT_INF]]\n++\n++  %int_zero = stablehlo.constant dense<0> : tensor<i32>\n++  %int_neg_ten = stablehlo.constant dense<-10> : tensor<i32>\n++  %int_pos_ten = stablehlo.constant dense<10> : tensor<i32>\n++\n++  %float_zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %float_neg_half = stablehlo.constant dense<-0.5> : tensor<f32>\n++  %float_pos_half = stablehlo.constant dense<0.5> : tensor<f32>\n++  %float_neg_inf = stablehlo.constant dense<0xFF800000> : tensor<f32> // -inf\n++  %float_pos_inf = stablehlo.constant dense<0x7F800000> : tensor<f32> // +inf\n++\n++  %0 = stablehlo.abs %int_zero : tensor<i32>\n++  %1 = stablehlo.abs %int_neg_ten : tensor<i32>\n++  %2 = stablehlo.abs %int_pos_ten : tensor<i32>\n++\n++  %3 = stablehlo.abs %float_zero : tensor<f32>\n++  %4 = stablehlo.abs %float_neg_half : tensor<f32>\n++  %5 = stablehlo.abs %float_pos_half : tensor<f32>\n++  %6 = stablehlo.abs %float_neg_inf : tensor<f32>\n++  %7 = stablehlo.abs %float_pos_inf : tensor<f32>\n++\n++  func.return %0, %1, %2, %3, %4, %5, %6, %7 : tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// CosineOp\n++\n++// CHECK-LABEL: func @fold_cosine\n++func.func @fold_cosine() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<{{1\\.0000.*}}> : tensor<f32>\n++  // CHECK-DAG: [[SQRT_THREE_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\\.8660.*|8\\.660.*[Ee]-01}}> : tensor<f32>\n++  // CHECK-DAG: [[SQRT_TWO_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\\.7071.*|7\\.071.*[Ee]-01}}> : tensor<f32>\n++  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<{{0\\.5000.*|5\\.000.*[Ee]-01|0.4999.*|4\\.999.*[Ee]-01}}> : tensor<f32>\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<{{-?(0\\.0000.*|[0-9]\\.[0-9]*[Ee]-(0?[5-9]|[1-9][0-9]))}}> : tensor<f32>\n++  // CHECK:     return [[ONE]], [[SQRT_THREE_OVER_TWO]], [[SQRT_TWO_OVER_TWO]], [[HALF]], [[ZERO]]\n++\n++  %0 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %1 = stablehlo.constant dense<0.5235987755982989> : tensor<f32> // pi/6\n++  %2 = stablehlo.constant dense<0.7853981633974483> : tensor<f32> // pi/4\n++  %3 = stablehlo.constant dense<1.0471975511965977> : tensor<f32> // pi/3\n++  %4 = stablehlo.constant dense<1.5707963267948966> : tensor<f32> // pi/2\n++\n++  %5 = stablehlo.cosine %0 : tensor<f32>\n++  %6 = stablehlo.cosine %1 : tensor<f32>\n++  %7 = stablehlo.cosine %2 : tensor<f32>\n++  %8 = stablehlo.cosine %3 : tensor<f32>\n++  %9 = stablehlo.cosine %4 : tensor<f32>\n++\n++  func.return %5, %6, %7, %8, %9 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// ErfOp\n++\n++// CHECK-LABEL: func @fold_erf\n++func.func @fold_erf() -> (tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[RESULT0:%.*]] = stablehlo.constant dense<-0.52049{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[RESULT1:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[RESULT2:%.*]] = stablehlo.constant dense<0.90000{{.*}}> : tensor<f32>\n++  // CHECK:     return [[RESULT0]], [[RESULT1]], [[RESULT2]]\n++\n++  %0 = stablehlo.constant dense<-0.5> : tensor<f32>\n++  %1 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %2 = stablehlo.constant dense<1.1631> : tensor<f32>\n++\n++  %3 = chlo.erf %0 : tensor<f32> -> tensor<f32>\n++  %4 = chlo.erf %1 : tensor<f32> -> tensor<f32>\n++  %5 = chlo.erf %2 : tensor<f32> -> tensor<f32>\n++\n++  func.return %3, %4, %5 : tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// ExpOp\n++\n++// CHECK-LABEL: func @fold_exponential\n++func.func @fold_exponential() -> (tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[E:%.*]] = stablehlo.constant dense<2.718{{.*}}> : tensor<f32>\n++  // CHECK:     return [[ONE]], [[E]]\n++\n++  %0 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %1 = stablehlo.constant dense<1.0> : tensor<f32>\n++\n++  %2 = stablehlo.exponential %0 : tensor<f32>\n++  %3 = stablehlo.exponential %1 : tensor<f32>\n++\n++  func.return %2, %3 : tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// LogOp\n++\n++// CHECK-LABEL: func @fold_log\n++func.func @fold_log() -> (tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<{{1\\.0.*|0\\.999.*}}> : tensor<f32>\n++  // CHECK-DAG: [[DO_NOT_FOLD_LOG_ZERO:%.*]] = stablehlo.log [[ZERO]] : tensor<f32>\n++  // CHECK:     return [[ZERO]], [[ONE]], [[DO_NOT_FOLD_LOG_ZERO]]\n++\n++  %0 = stablehlo.constant dense<1.0> : tensor<f32>\n++  %1 = stablehlo.constant dense<2.718281828459045> : tensor<f32>\n++  %2 = stablehlo.constant dense<0.0> : tensor<f32>\n++\n++  %3 = stablehlo.log %0 : tensor<f32>\n++  %4 = stablehlo.log %1 : tensor<f32>\n++  %5 = stablehlo.log %2 : tensor<f32>\n++\n++  func.return %3, %4, %5 : tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// LogisticOp\n++\n++// CHECK-LABEL: func @fold_logistic\n++func.func @fold_logistic() -> (tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<5.0{{.*}}e-01> : tensor<f32>\n++  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n++  // CHECK:     return [[ZERO]], [[HALF]], [[ONE]]\n++\n++  %neg_inf = stablehlo.constant dense<0xFF800000> : tensor<f32>\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %pos_inf = stablehlo.constant dense<0x7F800000> : tensor<f32>\n++\n++  %0 = stablehlo.logistic %neg_inf : tensor<f32>\n++  %1 = stablehlo.logistic %zero : tensor<f32>\n++  %2 = stablehlo.logistic %pos_inf : tensor<f32>\n++\n++  func.return %0, %1, %2 : tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// NegOp\n++\n++// CHECK-LABEL: func @fold_negate\n++func.func @fold_negate() -> (tensor<i32>, tensor<i32>, tensor<f32>) {\n++  // CHECK-DAG: [[RESULT0:%.*]] = stablehlo.constant dense<-4> : tensor<i32>\n++  // CHECK-DAG: [[RESULT1:%.*]] = stablehlo.constant dense<0> : tensor<i32>\n++  // CHECK-DAG: [[RESULT2:%.*]] = stablehlo.constant dense<9.999{{.*}}e+02> : tensor<f32>\n++  // CHECK:     return [[RESULT0]], [[RESULT1]], [[RESULT2]]\n++\n++  %0 = stablehlo.constant dense<4> : tensor<i32>\n++  %1 = stablehlo.constant dense<0> : tensor<i32>\n++  %2 = stablehlo.constant dense<-999.9> : tensor<f32>\n++\n++  %3 = stablehlo.negate %0 : tensor<i32>\n++  %4 = stablehlo.negate %1 : tensor<i32>\n++  %5 = stablehlo.negate %2 : tensor<f32>\n++\n++  func.return %3, %4, %5 : tensor<i32>, tensor<i32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// NotOp\n++\n++// CHECK-LABEL: func @fold_not\n++func.func @fold_not() -> (tensor<i32>, tensor<i32>) {\n++  // CHECK-DAG: [[RESULT0:%.*]] = stablehlo.constant dense<42> : tensor<i32>\n++  // CHECK-DAG: [[RESULT1:%.*]] = stablehlo.constant dense<-1> : tensor<i32>\n++  // CHECK:     return [[RESULT0]], [[RESULT1]]\n++\n++  %0 = stablehlo.constant dense<-43> : tensor<i32>\n++  %1 = stablehlo.constant dense<0> : tensor<i32>\n++\n++  %2 = stablehlo.not %0 : tensor<i32>\n++  %3 = stablehlo.not %1 : tensor<i32>\n++\n++  func.return %2, %3 : tensor<i32>, tensor<i32>\n++}\n++\n++// -----\n++\n++////////\n++// RoundOp\n++\n++// CHECK-LABEL: func @fold_round_nearest_afz\n++func.func @fold_round_nearest_afz() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[NEG_THREE:%.*]] = stablehlo.constant dense<-3.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[NEG_TWO:%.*]] = stablehlo.constant dense<-2.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[TWO:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[THREE:%.*]] = stablehlo.constant dense<3.0{{.*}}> : tensor<f32>\n++  // CHECK:     return [[NEG_THREE]], [[NEG_TWO]], [[ZERO]], [[ONE]], [[ONE]], [[TWO]], [[THREE]]\n++\n++  %0 = stablehlo.constant dense<-2.5> : tensor<f32>\n++  %1 = stablehlo.constant dense<-1.5> : tensor<f32>\n++  %2 = stablehlo.constant dense<0.4> : tensor<f32>\n++  %3 = stablehlo.constant dense<0.5> : tensor<f32>\n++  %4 = stablehlo.constant dense<0.6> : tensor<f32>\n++  %5 = stablehlo.constant dense<1.5> : tensor<f32>\n++  %6 = stablehlo.constant dense<2.5> : tensor<f32>\n++\n++  %7 = stablehlo.round_nearest_afz %0 : tensor<f32>\n++  %8 = stablehlo.round_nearest_afz %1 : tensor<f32>\n++  %9 = stablehlo.round_nearest_afz %2 : tensor<f32>\n++  %10 = stablehlo.round_nearest_afz %3 : tensor<f32>\n++  %11 = stablehlo.round_nearest_afz %4 : tensor<f32>\n++  %12 = stablehlo.round_nearest_afz %5 : tensor<f32>\n++  %13 = stablehlo.round_nearest_afz %6 : tensor<f32>\n++\n++  func.return %7, %8, %9, %10, %11, %12, %13 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// RoundNearestEvenOp\n++\n++// CHECK-LABEL: func @fold_round_nearest_even\n++func.func @fold_round_nearest_even() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[NEG_TWO:%.*]] = stablehlo.constant dense<-2.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[TWO:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>\n++  // CHECK:     return [[NEG_TWO]], [[NEG_TWO]], [[ZERO]], [[ZERO]], [[ONE]], [[TWO]], [[TWO]]\n++\n++  %0 = stablehlo.constant dense<-2.5> : tensor<f32>\n++  %1 = stablehlo.constant dense<-1.5> : tensor<f32>\n++  %2 = stablehlo.constant dense<0.4> : tensor<f32>\n++  %3 = stablehlo.constant dense<0.5> : tensor<f32>\n++  %4 = stablehlo.constant dense<0.6> : tensor<f32>\n++  %5 = stablehlo.constant dense<1.5> : tensor<f32>\n++  %6 = stablehlo.constant dense<2.5> : tensor<f32>\n++\n++  %7 = stablehlo.round_nearest_even %0 : tensor<f32>\n++  %8 = stablehlo.round_nearest_even %1 : tensor<f32>\n++  %9 = stablehlo.round_nearest_even %2 : tensor<f32>\n++  %10 = stablehlo.round_nearest_even %3 : tensor<f32>\n++  %11 = stablehlo.round_nearest_even %4 : tensor<f32>\n++  %12 = stablehlo.round_nearest_even %5 : tensor<f32>\n++  %13 = stablehlo.round_nearest_even %6 : tensor<f32>\n++\n++  func.return %7, %8, %9, %10, %11, %12, %13 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// RsqrtOp\n++\n++// CHECK-LABEL: func @fold_rsqrt\n++func.func @fold_rsqrt() -> (tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<5.0{{.*}}e-01> : tensor<f32>\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[DO_NOT_FOLD_RSQRT_ZERO:%.*]] = stablehlo.rsqrt [[ZERO]] : tensor<f32>\n++  // CHECK:     return [[HALF]], [[DO_NOT_FOLD_RSQRT_ZERO]]\n++\n++  %0 = stablehlo.constant dense<4.0> : tensor<f32>\n++  %1 = stablehlo.constant dense<0.0> : tensor<f32>\n++\n++  %2 = stablehlo.rsqrt %0 : tensor<f32>\n++  %3 = stablehlo.rsqrt %1 : tensor<f32>\n++\n++  func.return %2, %3 : tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// SignOp\n++\n++// CHECK-LABEL: func @fold_sign\n++func.func @fold_sign() -> (tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[INT_NEG_ONE:%.*]] = stablehlo.constant dense<-1> : tensor<i32>\n++  // CHECK-DAG: [[INT_ZERO:%.*]] = stablehlo.constant dense<0> : tensor<i32>\n++  // CHECK-DAG: [[INT_POS_ONE:%.*]] = stablehlo.constant dense<1> : tensor<i32>\n++  // CHECK-DAG: [[FLOAT_NEG_ONE:%.*]] = stablehlo.constant dense<-1.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[FLOAT_ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[FLOAT_POS_ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[FLOAT_NAN:%.*]] = stablehlo.constant dense<0x7FC00000> : tensor<f32>\n++  // CHECK-DAG: [[DO_NOT_FOLD_SIGN_NAN:%.*]] = stablehlo.sign [[FLOAT_NAN]] : tensor<f32>\n++  // CHECK:     return [[INT_NEG_ONE]], [[INT_ZERO]], [[INT_POS_ONE]], [[FLOAT_NEG_ONE]], [[FLOAT_ZERO]], [[FLOAT_POS_ONE]], [[DO_NOT_FOLD_SIGN_NAN]]\n++\n++  %0 = stablehlo.constant dense<-7> : tensor<i32>\n++  %1 = stablehlo.constant dense<0> : tensor<i32>\n++  %2 = stablehlo.constant dense<25> : tensor<i32>\n++  %3 = stablehlo.constant dense<-2.5> : tensor<f32>\n++  %4 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %5 = stablehlo.constant dense<0.1> : tensor<f32>\n++  %6 = stablehlo.constant dense<0x7FC00000> : tensor<f32> // NaN\n++\n++  %7 = stablehlo.sign %0 : tensor<i32>\n++  %8 = stablehlo.sign %1 : tensor<i32>\n++  %9 = stablehlo.sign %2 : tensor<i32>\n++  %10 = stablehlo.sign %3 : tensor<f32>\n++  %11 = stablehlo.sign %4 : tensor<f32>\n++  %12 = stablehlo.sign %5 : tensor<f32>\n++  %13 = stablehlo.sign %6 : tensor<f32>\n++\n++  func.return %7, %8, %9, %10, %11, %12, %13 : tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// SineOp\n++\n++// CHECK-LABEL: func @fold_sine\n++func.func @fold_sine() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<{{0\\.0000.*}}> : tensor<f32>\n++  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<{{0\\.5000.*|5\\.000.*[Ee]-01|0.4999.*|4\\.999.*[Ee]-01}}> : tensor<f32>\n++  // CHECK-DAG: [[SQRT_TWO_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\\.7071.*|7\\.071.*[Ee]-01}}> : tensor<f32>\n++  // CHECK-DAG: [[SQRT_THREE_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\\.8660.*|8\\.660.*[Ee]-01}}> : tensor<f32>\n++  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<{{1\\.0000.*|0\\.9999.*|9\\.999.*[Ee]-01}}> : tensor<f32>\n++  // CHECK:     return [[ZERO]], [[HALF]], [[SQRT_TWO_OVER_TWO]], [[SQRT_THREE_OVER_TWO]], [[ONE]]\n++\n++  %0 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %1 = stablehlo.constant dense<0.5235987755982989> : tensor<f32> // pi/6\n++  %2 = stablehlo.constant dense<0.7853981633974483> : tensor<f32> // pi/4\n++  %3 = stablehlo.constant dense<1.0471975511965977> : tensor<f32> // pi/3\n++  %4 = stablehlo.constant dense<1.5707963267948966> : tensor<f32> // pi/2\n++\n++  %5 = stablehlo.sine %0 : tensor<f32>\n++  %6 = stablehlo.sine %1 : tensor<f32>\n++  %7 = stablehlo.sine %2 : tensor<f32>\n++  %8 = stablehlo.sine %3 : tensor<f32>\n++  %9 = stablehlo.sine %4 : tensor<f32>\n++\n++  func.return %5, %6, %7, %8, %9 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n+ // SqrtOp\n+ \n+ // CHECK-LABEL: func @fold_sqrt\n+-func.func @fold_sqrt() -> (tensor<f32>) {\n+-  // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>\n+-  // CHECK: return [[RESULT0]]\n++func.func @fold_sqrt() -> (tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[TWO:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[NEG_ONE:%.*]] = stablehlo.constant dense<-1.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[DO_NOT_FOLD_FLOAT_SQRT_NEG_ONE:%.*]] = stablehlo.sqrt [[NEG_ONE]] : tensor<f32>\n++  // CHECK:     return [[TWO]], [[DO_NOT_FOLD_FLOAT_SQRT_NEG_ONE]]\n++\n+   %0 = stablehlo.constant dense<4.0> : tensor<f32>\n+-  %1 = stablehlo.sqrt %0 : tensor<f32>\n+-  func.return %1 : tensor<f32>\n+-}\n+-\n+-//\n++  %1 = stablehlo.constant dense<-1.0> : tensor<f32>\n++\n++  %2 = stablehlo.sqrt %0 : tensor<f32>\n++  %3 = stablehlo.sqrt %1 : tensor<f32>\n++\n++  func.return %2, %3 : tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// TanOp\n++\n++// CHECK-LABEL: func @fold_tan\n++func.func @fold_tan() -> (tensor<f32>) {\n++  // CHECK: [[ONE:%.*]] = stablehlo.constant dense<{{1\\.0.*|0\\.999.*}}> : tensor<f32>\n++  // CHECK: return [[ONE]]\n++  %pi_over_4 = stablehlo.constant dense<0.7853981633974483> : tensor<f32>\n++  %result = stablehlo.tan %pi_over_4 : tensor<f32>\n++  func.return %result : tensor<f32>\n++}\n++\n++// -----\n++\n++////////\n++// TanhOp\n++\n++// CHECK-LABEL: func @fold_tanh\n++func.func @fold_tanh() -> (tensor<f32>, tensor<f32>, tensor<f32>) {\n++  // CHECK-DAG: [[NEG_SQRT_ONE_FIFTH:%.*]] = stablehlo.constant dense<-0.44721{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>\n++  // CHECK-DAG: [[SQRT_ONE_FIFTH:%.*]] = stablehlo.constant dense<0.44721{{.*}}> : tensor<f32>\n++  // CHECK:     return [[NEG_SQRT_ONE_FIFTH]], [[ZERO]], [[SQRT_ONE_FIFTH]]\n++\n++  %neg_log_phi = stablehlo.constant dense<-0.4812118250596034> : tensor<f32>\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %log_phi = stablehlo.constant dense<0.4812118250596034> : tensor<f32>\n++\n++  %tanh_neg_log_phi = stablehlo.tanh %neg_log_phi : tensor<f32>\n++  %tanh_zero = stablehlo.tanh %zero : tensor<f32>\n++  %tanh_log_phi = stablehlo.tanh %log_phi : tensor<f32>\n++\n++  func.return %tanh_neg_log_phi, %tanh_zero, %tanh_log_phi : tensor<f32>, tensor<f32>, tensor<f32>\n++}\n++\n++// -----\n+ \n+ ////////\n+ // SetDimensionSizeOp\n+@@ -748,6 +1142,19 @@\n    // CHECK-NEXT: return [[RESULT0]]\n    %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<10xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<10>>\n    return %0 : tensor<?xf32, #stablehlo.bounds<10>>\n@@ -415,7 +847,240 @@ diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stable\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -1014,6 +1014,11 @@\n+@@ -21,6 +21,7 @@\n+ #include <numeric>\n+ #include <optional>\n+ #include <string>\n++#include <type_traits>\n+ #include <utility>\n+ \n+ #include \"llvm/ADT/APFloat.h\"\n+@@ -58,6 +59,7 @@\n+ #include \"mlir/Support/LogicalResult.h\"\n+ #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+ #include \"stablehlo/dialect/Base.h\"\n++#include \"stablehlo/dialect/ChloOps.h\"\n+ #include \"stablehlo/dialect/StablehloOps.h\"\n+ #include \"stablehlo/transforms/optimization/Passes.h\"\n+ \n+@@ -87,6 +89,14 @@\n+       /*isUnsigned=*/isUnsigned);\n+ }\n+ \n++template <typename T>\n++APSInt getAPSInt(unsigned bitWidth, T value, bool isSigned) {\n++  return APSInt({/*numBits=*/bitWidth, static_cast<uint64_t>(value),\n++                 /*isSigned=*/isSigned,\n++                 /*implicitTrunc=*/true},\n++                /*isUnsigned=*/!isSigned);\n++}\n++\n+ APFloat getAPFloat(\n+     Type type, double value,\n+     llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {\n+@@ -94,8 +104,8 @@\n+   if (!floatType) llvm::report_fatal_error(\"expected float type\");\n+ \n+   APFloat result(value);\n+-  bool losesInfo = false;\n+-  result.convert(floatType.getFloatSemantics(), roundingMode, &losesInfo);\n++  bool unusedLosesInfo = false;\n++  result.convert(floatType.getFloatSemantics(), roundingMode, &unusedLosesInfo);\n+   return result;\n+ }\n+ \n+@@ -351,6 +361,190 @@\n+           \"too many elements, fold \"\n+           \"limit is \" +\n+               std::to_string(options.foldOpElementLimit));\n++    return success();\n++  }\n++};\n++\n++namespace fold_unary {\n++\n++template <typename Impl, typename = void, typename... ArgTypes>\n++struct FolderExistsHelper : std::false_type {};\n++\n++template <typename Impl, typename... ArgTypes>\n++struct FolderExistsHelper<\n++    Impl,\n++    std::enable_if_t<sizeof(Impl::EvaluateOp(std::declval<ArgTypes>()...)) != 0,\n++                     void>,\n++    ArgTypes...> : std::true_type {};\n++\n++template <typename Impl, typename... ArgTypes>\n++struct FolderExists : FolderExistsHelper<Impl, void, ArgTypes...> {};\n++\n++template <typename Impl, typename CanonicalType, typename = void>\n++struct DirectFolderExists : std::false_type {};\n++\n++template <typename Impl, typename CanonicalType>\n++struct DirectFolderExists<\n++    Impl, CanonicalType,\n++    std::enable_if_t<std::is_convertible_v<decltype(Impl::EvaluateOp(\n++                                               std::declval<CanonicalType>())),\n++                                           std::optional<CanonicalType>>,\n++                     void>> : std::true_type {\n++  static_assert(FolderExists<Impl, CanonicalType>::value);\n++};\n++\n++template <typename Impl, typename CanonicalType, typename ComputeType,\n++          typename ConversionFn, typename = void>\n++struct ConvertingFolderExists : std::false_type {};\n++\n++template <typename Impl, typename CanonicalType, typename ComputeType,\n++          typename ConversionFn>\n++struct ConvertingFolderExists<\n++    Impl, CanonicalType, ComputeType, ConversionFn,\n++    std::enable_if_t<std::is_convertible_v<\n++                         decltype(std::declval<ConversionFn>()(\n++                             Impl::EvaluateOp(std::declval<ComputeType>()))),\n++                         std::optional<CanonicalType>> &&\n++                         !std::is_same_v<std::decay_t<CanonicalType>,\n++                                         std::decay_t<ComputeType>>,\n++                     void>> : std::true_type {\n++  static_assert(FolderExists<Impl, ComputeType>::value);\n++  static_assert(!DirectFolderExists<Impl, CanonicalType>::value);\n++};\n++\n++}  // namespace fold_unary\n++\n++template <typename Impl, typename OpType>\n++struct FoldUnaryOpPattern : public FoldOpRewritePattern<OpType> {\n++  using FoldOpRewritePattern<OpType>::FoldOpRewritePattern;\n++\n++  template <\n++      typename CanonicalType,\n++      std::enable_if_t<\n++          fold_unary::DirectFolderExists<Impl, CanonicalType>::value, int> = 0>\n++  static std::optional<CanonicalType> FoldIfImplemented(CanonicalType operand) {\n++    return Impl::EvaluateOp(operand);\n++  }\n++\n++  template <typename CanonicalType, typename ComputeType,\n++            typename ConversionFn =\n++                std::optional<CanonicalType> (*)(std::optional<ComputeType>),\n++            std::enable_if_t<\n++                fold_unary::ConvertingFolderExists<\n++                    Impl, CanonicalType, ComputeType, ConversionFn>::value,\n++                int> = 0>\n++  static std::optional<CanonicalType> FoldIfImplemented(\n++      ComputeType operand,\n++      ConversionFn&& convertResult = [](std::optional<ComputeType> result)\n++          -> std::optional<CanonicalType> {\n++        if (result == std::nullopt) return std::nullopt;\n++        return CanonicalType(*result);\n++      }) {\n++    return convertResult(Impl::EvaluateOp(operand));\n++  }\n++\n++  template <typename CanonicalType, typename ComputeType,\n++            typename ConversionFn = std::nullptr_t,\n++            std::enable_if_t<\n++                !fold_unary::FolderExists<Impl, ComputeType>::value, int> = 0>\n++  static std::nullopt_t FoldIfImplemented(\n++      ComputeType operand, ConversionFn&& convertResult = nullptr) {\n++    return std::nullopt;\n++  }\n++\n++  struct FoldDispatch {\n++    bool isSignedInt = false;\n++\n++    std::optional<APInt> operator()(APInt operand) const {\n++      if constexpr (fold_unary::DirectFolderExists<Impl, APInt>::value) {\n++        // Fold as a signedness-agnostic `APInt`.\n++        return FoldIfImplemented<APInt>(operand);\n++      } else if constexpr (fold_unary::DirectFolderExists<Impl,\n++                                                          APSInt>::value) {\n++        // Fold as a signedness-aware `APSInt`.\n++        return FoldIfImplemented<APSInt>(\n++            APSInt(std::move(operand), /*isUnsigned=*/!isSignedInt));\n++      } else {\n++        // Fold as a C++ primitive of type `int64_t` or `uint64_t`.\n++        return isSignedInt ? FoldAsIntType<int64_t>(operand)\n++                           : FoldAsIntType<uint64_t>(operand);\n++      }\n++    }\n++\n++    std::optional<APFloat> operator()(APFloat operand) const {\n++      if constexpr (fold_unary::DirectFolderExists<Impl, APFloat>::value) {\n++        // Fold as an `APFloat`.\n++        return FoldIfImplemented<APFloat>(operand);\n++      } else {\n++        // Fold as a `double`.\n++        return FoldAsDouble(operand);\n++      }\n++    }\n++\n++    template <typename ComputationDataType>\n++    std::optional<APInt> FoldAsIntType(APInt operand) const {\n++      const size_t bitWidth = operand.getBitWidth();\n++\n++      auto convertResult = [&](std::optional<ComputationDataType> result)\n++          -> std::optional<APInt> {\n++        if (result == std::nullopt) return std::nullopt;\n++        return APInt(bitWidth, *result, isSignedInt);\n++      };\n++\n++      ComputationDataType operandValue;\n++      if constexpr (std::is_signed_v<ComputationDataType>) {\n++        operandValue = static_cast<ComputationDataType>(operand.getSExtValue());\n++      } else {\n++        operandValue = static_cast<ComputationDataType>(operand.getZExtValue());\n++      }\n++\n++      return FoldIfImplemented<APInt, ComputationDataType>(operandValue,\n++                                                           convertResult);\n++    }\n++\n++    std::optional<APFloat> FoldAsDouble(APFloat operand) const {\n++      auto convertResult =\n++          [&](std::optional<double> result) -> std::optional<APFloat> {\n++        if (result == std::nullopt) return std::nullopt;\n++        APFloat resultAsAPFloat(*result);\n++        bool unusedLosesInfo;\n++        resultAsAPFloat.convert(operand.getSemantics(),\n++                                APFloat::rmNearestTiesToEven, &unusedLosesInfo);\n++        return resultAsAPFloat;\n++      };\n++\n++      APFloat operandCopy = operand;\n++      bool unusedLosesInfo;\n++      operandCopy.convert(APFloat::IEEEdouble(), APFloat::rmNearestTiesToEven,\n++                          &unusedLosesInfo);\n++      double operandValue = operandCopy.convertToDouble();\n++\n++      return FoldIfImplemented<APFloat, double>(operandValue, convertResult);\n++    }\n++  };\n++\n++  LogicalResult matchAndRewrite(OpType op,\n++                                PatternRewriter& rewriter) const override {\n++    auto elementType = op.getType().getElementType();\n++\n++    FailureOr<TypedAttr> result;\n++    if (elementType.isUnsignedInteger()) {\n++      result = foldUnaryOpIntOrFloat(rewriter, op,\n++                                     FoldDispatch{/*isSignedInt=*/false});\n++    } else if (elementType.isInteger()) {\n++      // Types with unspecified signedness are treated as signed per StableHLO\n++      // convention.\n++      result = foldUnaryOpIntOrFloat(rewriter, op,\n++                                     FoldDispatch{/*isSignedInt=*/true});\n++    } else if (elementType.isFloat()) {\n++      result = foldUnaryOpIntOrFloat(rewriter, op,\n++                                     FoldDispatch{/*isSignedInt=*/false});\n++    } else {\n++      return failure();\n++    }\n++    if (failed(result)) return failure();\n++\n++    rewriter.replaceOpWithNewOp<ConstantOp>(op, result.value());\n+     return success();\n+   }\n+ };\n+@@ -1014,6 +1208,11 @@\n      // No need to verify static shape or dtype here since we aren't evaluating\n      // dtype, just folding set_dim_size ops with no semantic meaning.\n  \n@@ -427,7 +1092,252 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n      SplatElementsAttr cstSplatAttr;\n      matchPattern(op.getSize(), m_Constant(&cstSplatAttr));\n      if (!cstSplatAttr)\n-@@ -1316,13 +1321,9 @@\n+@@ -1031,50 +1230,6 @@\n+     rewriter.replaceAllOpUsesWith(op, op.getOperand());\n+     return success();\n+   }\n+-};\n+-\n+-struct FoldSignOpPattern : public ShapeOpRewritePattern<SignOp> {\n+-  using ShapeOpRewritePattern::ShapeOpRewritePattern;\n+-\n+-  LogicalResult matchAndRewrite(SignOp op,\n+-                                PatternRewriter& rewriter) const override {\n+-    if (failed(validateShapeFoldDtype(rewriter, op, op.getType())))\n+-      return failure();\n+-\n+-    auto elementType = op.getType().getElementType();\n+-    auto res = foldUnaryOpIntOrFloat(rewriter, op, FoldSign(elementType));\n+-    if (failed(res)) return failure();\n+-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+-    return success();\n+-  }\n+-\n+-  struct FoldSign {\n+-    FoldSign(Type elementType) : elementType(elementType) {}\n+-    Type elementType;\n+-    double result;\n+-    APFloat operator()(APFloat operand) {\n+-      if (operand.isNegative())\n+-        result = -1.0;\n+-      else if (operand.isZero())\n+-        result = 0.0;\n+-      else\n+-        result = 1.0;\n+-      return getAPFloat(elementType, result);\n+-    }\n+-\n+-    APInt operator()(APInt operand) {\n+-      // SignOp only supports signed integers.\n+-      APSInt signedInt = getAPSInt(elementType, operand.getSExtValue());\n+-      int64_t result;\n+-      if (signedInt.isNegative())\n+-        result = -1;\n+-      else if (signedInt.isZero())\n+-        result = 0;\n+-      else\n+-        result = 1;\n+-      return getAPSInt(elementType, result);\n+-    }\n+-  };\n+ };\n+ \n+ template <typename RangeType>\n+@@ -1163,31 +1318,169 @@\n+   }\n+ };\n+ \n++struct FoldAbsOpPattern : public FoldUnaryOpPattern<FoldAbsOpPattern, AbsOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<APInt> EvaluateOp(APInt operand) {\n++    return operand.abs();\n++  }\n++  static std::optional<APFloat> EvaluateOp(APFloat operand) {\n++    return llvm::abs(operand);\n++  }\n++};\n++\n++struct FoldCosineOpPattern\n++    : public FoldUnaryOpPattern<FoldCosineOpPattern, CosineOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    return std::cos(operand);\n++  }\n++};\n++\n++struct FoldErfOpPattern\n++    : public FoldUnaryOpPattern<FoldErfOpPattern, chlo::ErfOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    return std::erf(operand);\n++  }\n++};\n++\n++struct FoldExpOpPattern : public FoldUnaryOpPattern<FoldExpOpPattern, ExpOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    return std::exp(operand);\n++  }\n++};\n++\n++struct FoldLogOpPattern : public FoldUnaryOpPattern<FoldLogOpPattern, LogOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    if (operand <= 0.0) return std::nullopt;\n++    return std::log(operand);\n++  }\n++};\n++\n++struct FoldLogisticOpPattern\n++    : public FoldUnaryOpPattern<FoldLogisticOpPattern, LogisticOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    return 1.0 / (1.0 + std::exp(-operand));\n++  }\n++};\n++\n++struct FoldNegOpPattern : public FoldUnaryOpPattern<FoldNegOpPattern, NegOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<APInt> EvaluateOp(APInt operand) { return -operand; }\n++  static std::optional<APFloat> EvaluateOp(APFloat operand) { return -operand; }\n++};\n++\n++struct FoldNotOpPattern : public FoldUnaryOpPattern<FoldNotOpPattern, NotOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<APInt> EvaluateOp(APInt operand) {\n++    operand.flipAllBits();\n++    return operand;\n++  }\n++};\n++\n++struct FoldRoundOpPattern\n++    : public FoldUnaryOpPattern<FoldRoundOpPattern, RoundOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<APFloat> EvaluateOp(APFloat operand) {\n++    operand.roundToIntegral(APFloat::rmNearestTiesToAway);\n++    return operand;\n++  }\n++};\n++\n++struct FoldRoundNearestEvenOpPattern\n++    : public FoldUnaryOpPattern<FoldRoundNearestEvenOpPattern,\n++                                RoundNearestEvenOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<APFloat> EvaluateOp(APFloat operand) {\n++    operand.roundToIntegral(APFloat::rmNearestTiesToEven);\n++    return operand;\n++  }\n++};\n++\n++struct FoldRsqrtOpPattern\n++    : public FoldUnaryOpPattern<FoldRsqrtOpPattern, RsqrtOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    if (operand <= 0.0) return std::nullopt;\n++    return 1.0 / std::sqrt(operand);\n++  }\n++};\n++\n++struct FoldSignOpPattern\n++    : public FoldUnaryOpPattern<FoldSignOpPattern, SignOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<APFloat> EvaluateOp(APFloat operand) {\n++    if (operand.isNaN()) return std::nullopt;\n++    if (operand.isZero()) return APFloat::getZero(operand.getSemantics());\n++    return APFloat::getOne(operand.getSemantics(),\n++                           /*Negative=*/operand.isNegative());\n++  }\n++\n++  static std::optional<APSInt> EvaluateOp(APSInt operand) {\n++    // SignOp only supports signed integers.\n++    if (operand.isUnsigned()) return std::nullopt;\n++\n++    int sign;\n++    if (operand.isNegative()) {\n++      sign = -1;\n++    } else if (operand.isZero()) {\n++      sign = 0;\n++    } else {\n++      sign = +1;\n++    }\n++    return getAPSInt(operand.getBitWidth(), sign, /*isSigned=*/true);\n++  }\n++};\n++\n++struct FoldSineOpPattern\n++    : public FoldUnaryOpPattern<FoldSineOpPattern, SineOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    return std::sin(operand);\n++  }\n++};\n++\n+ struct FoldSqrtOpPattern\n+-    : public FoldOpRewritePattern<mlir::stablehlo::SqrtOp> {\n+-  using FoldOpRewritePattern<mlir::stablehlo::SqrtOp>::FoldOpRewritePattern;\n+-\n+-  LogicalResult matchAndRewrite(mlir::stablehlo::SqrtOp op,\n+-                                PatternRewriter& rewriter) const final {\n+-    auto res = foldUnaryOpIntOrFloat(rewriter, op, FoldSqrt());\n+-    if (failed(res)) return failure();\n+-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+-    return success();\n+-  }\n+-\n+-  struct FoldSqrt {\n+-    std::optional<APFloat> operator()(APFloat operand) {\n+-      if (operand.getSizeInBits(operand.getSemantics()) == 64)\n+-        return APFloat(std::sqrt(operand.convertToDouble()));\n+-\n+-      if (operand.getSizeInBits(operand.getSemantics()) == 32)\n+-        return APFloat(sqrtf(operand.convertToFloat()));\n+-      return std::nullopt;\n+-    }\n+-\n+-    // TODO: Enable int folding.\n+-    std::optional<APInt> operator()(APInt operand) { return std::nullopt; }\n+-  };\n++    : public FoldUnaryOpPattern<FoldSqrtOpPattern, SqrtOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    if (operand < 0.0) return std::nullopt;\n++    return std::sqrt(operand);\n++  }\n++};\n++\n++struct FoldTanOpPattern : public FoldUnaryOpPattern<FoldTanOpPattern, TanOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    return std::tan(operand);\n++  }\n++};\n++\n++struct FoldTanhOpPattern\n++    : public FoldUnaryOpPattern<FoldTanhOpPattern, TanhOp> {\n++  using FoldUnaryOpPattern::FoldUnaryOpPattern;\n++\n++  static std::optional<double> EvaluateOp(double operand) {\n++    return std::tanh(operand);\n++  }\n+ };\n+ \n+ struct FoldIotaOpPattern : public FoldOpRewritePattern<IotaOp> {\n+@@ -1316,13 +1609,9 @@\n  \n      for (auto [inputValue, bodyArg] :\n           llvm::zip_equal(op.getOperands(), body.getArguments())) {\n@@ -444,7 +1354,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n          return rewriter.notifyMatchFailure(op,\n                                             \"Input must be a splat constant.\");\n  \n-@@ -1332,7 +1333,7 @@\n+@@ -1332,7 +1621,7 @@\n              op, \"Could not get the shape of the body argument.\");\n  \n        bodyArgConstantAttrs.push_back(DenseElementsAttr::get(\n@@ -453,4 +1363,31 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n      }\n  \n      for (BlockArgument bodyArg : body.getArguments()) {\n+@@ -1570,11 +1859,25 @@\n+     PatternBenefit benefit) {\n+   populateStablehloShapeFolderPatterns(context, patterns, options, benefit);\n+ \n+-  patterns->add<FoldIotaOpPattern,                    //\n++  patterns->add<FoldAbsOpPattern,                     //\n++                FoldCosineOpPattern,                  //\n++                FoldErfOpPattern,                     //\n++                FoldExpOpPattern,                     //\n++                FoldIotaOpPattern,                    //\n++                FoldLogOpPattern,                     //\n++                FoldLogisticOpPattern,                //\n++                FoldNegOpPattern,                     //\n++                FoldNotOpPattern,                     //\n+                 FoldReduceOpReducingZeroDims,         //\n+                 FoldReduceOpToConstantInitializer,    //\n+                 FoldReduceOpWithRedundantResults,     //\n++                FoldRoundOpPattern,                   //\n++                FoldRoundNearestEvenOpPattern,        //\n++                FoldRsqrtOpPattern,                   //\n++                FoldSineOpPattern,                    //\n+                 FoldSqrtOpPattern,                    //\n++                FoldTanOpPattern,                     //\n++                FoldTanhOpPattern,                    //\n+                 FoldTransposeOpPattern,               //\n+                 FoldWhileOpIfDeadAndPresumedPure,     //\n+                 FoldWhileOpPattern,                   //\n "
        }
    ],
    "stats": {
        "total": 945,
        "additions": 941,
        "deletions": 4
    }
}