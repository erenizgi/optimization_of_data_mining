{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Better cluster dimension support for custom kernel interfaces.\n\nPiperOrigin-RevId: 810899161",
    "sha": "f7af8ad4d311b874b6d541cc93e6326933d4c94a",
    "files": [
        {
            "sha": "f6a27c330eb8f94088d88bd3276edad18c6f3780",
            "filename": "third_party/xla/xla/service/gpu/kernels/custom_kernel.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcustom_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcustom_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcustom_kernel.cc?ref=f7af8ad4d311b874b6d541cc93e6326933d4c94a",
            "patch": "@@ -67,10 +67,16 @@ size_t CustomKernel::shared_memory_bytes() const {\n }\n \n std::string CustomKernel::ToString() const {\n+  std::string cluster_dims_str =\n+      cluster_dims_.has_value()\n+          ? absl::StrFormat(\"cluster: [%d, %d, %d]\", cluster_dims_->x,\n+                            cluster_dims_->y, cluster_dims_->z)\n+          : \"\";\n   return absl::StrFormat(\n-      \"%s grid: [%d, %d, %d] threads: [%d, %d, %d] shared_memory: %d bytes\",\n+      \"%s grid: [%d, %d, %d] threads: [%d, %d, %d] %s \"\n+      \"shared_memory: %d bytes\",\n       name_, block_dims_.x, block_dims_.y, block_dims_.z, thread_dims_.x,\n-      thread_dims_.y, thread_dims_.z, shared_memory_bytes_);\n+      thread_dims_.y, thread_dims_.z, cluster_dims_str, shared_memory_bytes_);\n }\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "4222a3ae496f0aefa58a18bfc91a880c8f588ff5",
            "filename": "third_party/xla/xla/service/gpu/kernels/ptx_custom_kernel.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.cc?ref=f7af8ad4d311b874b6d541cc93e6326933d4c94a",
            "patch": "@@ -57,4 +57,16 @@ absl::StatusOr<CustomKernel> GetPtxCustomKernel(std::string kernel_name,\n                       /*shared_memory_bytes=*/shared_memory_bytes);\n };\n \n+absl::StatusOr<CustomKernel> GetPtxCustomKernel(\n+    std::string kernel_name, absl::string_view ptx, int num_args,\n+    se::BlockDim block_dim, se::ThreadDim thread_dim,\n+    se::ClusterDim cluster_dim, size_t shared_memory_bytes) {\n+  se::KernelLoaderSpec kernel_spec =\n+      se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n+          ptx, kernel_name, /*arity=*/num_args, KernelArgsPacking);\n+  return CustomKernel(std::move(kernel_name), kernel_spec, block_dim,\n+                      thread_dim, cluster_dim,\n+                      /*shared_memory_bytes=*/shared_memory_bytes);\n+};\n+\n }  // namespace xla::gpu::kernel"
        },
        {
            "sha": "21e5eedee46f2b146c85911e2bc6ffe947b231d1",
            "filename": "third_party/xla/xla/service/gpu/kernels/ptx_custom_kernel.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel.h?ref=f7af8ad4d311b874b6d541cc93e6326933d4c94a",
            "patch": "@@ -32,6 +32,11 @@ absl::StatusOr<CustomKernel> GetPtxCustomKernel(std::string kernel_name,\n                                                 se::BlockDim block_dim,\n                                                 se::ThreadDim thread_dim,\n                                                 size_t shared_memory_bytes = 0);\n-}\n+\n+absl::StatusOr<CustomKernel> GetPtxCustomKernel(\n+    std::string kernel_name, absl::string_view ptx, int num_args,\n+    se::BlockDim block_dim, se::ThreadDim thread_dim,\n+    se::ClusterDim cluster_dim, size_t shared_memory_bytes = 0);\n+}  // namespace xla::gpu::kernel\n \n #endif  // XLA_SERVICE_GPU_KERNELS_PTX_CUSTOM_KERNEL_H_"
        },
        {
            "sha": "890ff04008fa3cdc05d7a13c60a34dc31d256e30",
            "filename": "third_party/xla/xla/service/gpu/kernels/ptx_custom_kernel_test.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7af8ad4d311b874b6d541cc93e6326933d4c94a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fptx_custom_kernel_test.cc?ref=f7af8ad4d311b874b6d541cc93e6326933d4c94a",
            "patch": "@@ -86,7 +86,6 @@ TEST(PtxCustomKernelTest, GetPtxCustomKernel) {\n       CustomKernel custom_kernel,\n       GetPtxCustomKernel(\"AddI32\", kAddI32KernelPtx, 3, se::BlockDim(4),\n                          se::ThreadDim(1), byte_length));\n-\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Kernel> kernel,\n                           executor->LoadKernel(custom_kernel.kernel_spec()));\n \n@@ -112,6 +111,48 @@ TEST(PtxCustomKernelTest, GetPtxCustomKernel) {\n \n   std::vector<int32_t> expected = {3, 3, 3, 3};\n   ASSERT_EQ(dst, expected);\n+  ASSERT_EQ(\n+      custom_kernel.ToString(),\n+      \"AddI32 grid: [4, 1, 1] threads: [1, 1, 1]  shared_memory: 16 bytes\");\n }\n \n+TEST(PtxCustomKernelTest, GetPtxCustomKernelWithClusterDim) {\n+  int64_t length = 4;\n+  int64_t byte_length = sizeof(int32_t) * length;\n+  se::gpu::CudaPlatform platform;\n+  TF_ASSERT_OK_AND_ASSIGN(se::StreamExecutor * executor,\n+                          platform.ExecutorForDevice(0));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      CustomKernel custom_kernel,\n+      GetPtxCustomKernel(\"AddI32\", kAddI32KernelPtx, 3, se::BlockDim(4),\n+                         se::ThreadDim(1), se::ClusterDim(2), byte_length));\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Kernel> kernel,\n+                          executor->LoadKernel(custom_kernel.kernel_spec()));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n+                          executor->CreateStream());\n+  se::DeviceMemory<int32_t> a = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceMemory<int32_t> b = executor->AllocateArray<int32_t>(length, 0);\n+  se::DeviceMemory<int32_t> c = executor->AllocateArray<int32_t>(length, 0);\n+  TF_CHECK_OK(stream->Memset32(&a, 1, byte_length));\n+  TF_CHECK_OK(stream->Memset32(&b, 2, byte_length));\n+  TF_CHECK_OK(stream->MemZero(&c, byte_length));\n+\n+  se::KernelArgsDeviceMemoryArray args(\n+      std::vector<se::DeviceMemoryBase>({a, b, c}),\n+      custom_kernel.shared_memory_bytes());\n+  TF_CHECK_OK(kernel->Launch(custom_kernel.thread_dims(),\n+                             custom_kernel.block_dims(), stream.get(), args));\n+\n+  TF_CHECK_OK(stream->BlockHostUntilDone());\n+\n+  std::vector<int32_t> dst(4, 42);\n+  TF_CHECK_OK(stream->Memcpy(dst.data(), c, byte_length));\n+\n+  std::vector<int32_t> expected = {3, 3, 3, 3};\n+  ASSERT_EQ(dst, expected);\n+  ASSERT_EQ(custom_kernel.ToString(),\n+            \"AddI32 grid: [4, 1, 1] threads: [1, 1, 1] cluster: [2, 1, 1] \"\n+            \"shared_memory: 16 bytes\");\n+}\n }  // namespace xla::gpu::kernel"
        }
    ],
    "stats": {
        "total": 72,
        "additions": 68,
        "deletions": 4
    }
}