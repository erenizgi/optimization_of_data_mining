{
    "author": "KanishAnand",
    "message": "This is a mechanical change to replace `tile_assignment().num_elements()` by `num_devices()`.\nThis is 2/N cl's to privatize `tile_assignment()` method.\n\nPiperOrigin-RevId: 840233415",
    "sha": "f402ed760b16d3ea43551e80c8797bc085db129e",
    "files": [
        {
            "sha": "84872df9bbf972c4864ed2e81652d2612e0d9912",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -349,7 +349,7 @@ inline void ForceOperandSharding(HloInstruction* inst, int operand_num,\n \n // Return whether the sharding is fully tiled.\n inline bool IsFullyTiled(const HloSharding& sharding) {\n-  return sharding.NumTiles() == sharding.tile_assignment().num_elements();\n+  return sharding.NumTiles() == sharding.num_devices();\n }\n \n // The sharding is replicated or the total number of tiles is over or equal to"
        },
        {
            "sha": "8635914284262012fe5e8dda2ba7dbfbc76313ea",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/cluster_environment.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fcluster_environment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fcluster_environment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fcluster_environment.cc?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -295,8 +295,8 @@ double ClusterEnvironment::ReshardingCost(const Shape& shape,\n     return 0.0;\n   }\n \n-  if (src_spec.tile_assignment().num_elements() > device_mesh_.num_elements() ||\n-      dst_spec.tile_assignment().num_elements() > device_mesh_.num_elements()) {\n+  if (src_spec.num_devices() > device_mesh_.num_elements() ||\n+      dst_spec.num_devices() > device_mesh_.num_elements()) {\n     LOG(WARNING)\n         << \"Full device sharding found when solving for the partial mesh \"\n         << spmd::ToString(device_mesh_.dimensions())"
        },
        {
            "sha": "fe44288aadf2da47822f12309cad232be1e470f3",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -526,6 +526,9 @@ class HloSharding {\n     return tile_assignment().dim(dim_index);\n   }\n \n+  // Returns the total number of devices used by sharding.\n+  int64_t num_devices() const { return tile_assignment().num_elements(); }\n+\n   // Gets the subgroup types array.\n   // REQUIRES: !IsTuple()\n   const std::vector<OpSharding::Type>& subgroup_types() const {"
        },
        {
            "sha": "3a4a004b2b12986ca512e7edbaa83c3082ef733f",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -293,13 +293,13 @@ bool IsSubTilingOrEqualSharding(const Shape& potential_sharded_shape,\n   }\n \n   // Use one contiguous storage to reduce allocation overhead.\n-  auto storage = std::make_unique<int32_t[]>(\n-      sharding.tile_assignment().num_elements() * tiled_data_rank);\n+  auto storage =\n+      std::make_unique<int32_t[]>(sharding.num_devices() * tiled_data_rank);\n   int32_t* storage_cursor = storage.get();\n   // Need a map here, because the MPMD partitioner sharding annotations can have\n   // non contiguous partition numbers.\n   absl::flat_hash_map<int32_t, int32_t*> sharding_offsets;\n-  sharding_offsets.reserve(sharding.tile_assignment().num_elements());\n+  sharding_offsets.reserve(sharding.num_devices());\n   auto get_sharding_offsets = [&](int64_t device) -> absl::Span<int32_t> {\n     auto it = sharding_offsets.find(device);\n     if (it == sharding_offsets.end()) {\n@@ -406,8 +406,7 @@ bool MergeSharding(const HloSharding& to_merge, HloSharding* dst,\n   }\n   if (!may_combine_partial_sharding || !to_merge.HasPartialReplication() ||\n       !dst->HasPartialReplication() ||\n-      to_merge.tile_assignment().num_elements() !=\n-          dst->tile_assignment().num_elements()) {\n+      to_merge.num_devices() != dst->num_devices()) {\n     goto check_if_more_specific;\n   }\n \n@@ -490,7 +489,7 @@ bool MergeShardingIfCompatible(const HloSharding& to_merge,\n     }\n   }\n \n-  const int64_t num_devices = to_merge.tile_assignment().num_elements();\n+  const int64_t num_devices = to_merge.num_devices();\n   const int64_t new_num_tiles = Product(merged_tile_dims);\n   if (num_devices % new_num_tiles != 0 || new_num_tiles < minimum_tiles) {\n     return false;\n@@ -1074,11 +1073,9 @@ HloSharding PropagateShardingThroughReshape(const Shape& source_shape,\n         DimensionVector reshape_dims(\n             reshaped->tile_assignment().dimensions().begin(),\n             reshaped->tile_assignment().dimensions().end());\n-        CHECK_EQ(\n-            sharding.tile_assignment().num_elements() % Product(reshape_dims),\n-            0);\n+        CHECK_EQ(sharding.num_devices() % Product(reshape_dims), 0);\n         int64_t num_replicated_dims =\n-            sharding.tile_assignment().num_elements() / Product(reshape_dims);\n+            sharding.num_devices() / Product(reshape_dims);\n         const int64_t diff =\n             reshape_dims.size() - target_shape.dimensions().size();\n         CHECK(diff == 0 || diff == 1);\n@@ -2182,7 +2179,7 @@ GroupedSharding GroupShardingOnDims(const HloSharding& sharding,\n       sharding.tile_assignment().Reshape(reshape_dimensions).Transpose(perm);\n \n   const int64_t num_device_groups = Product(group_dim_sizes);\n-  const int64_t num_devices = sharding.tile_assignment().num_elements();\n+  const int64_t num_devices = sharding.num_devices();\n   CHECK_EQ(num_devices % num_device_groups, 0);\n   const int64_t device_group_size = num_devices / num_device_groups;\n "
        },
        {
            "sha": "56b5baaef7ad79d43b4b1412f23e377f48c86b29",
            "filename": "third_party/xla/xla/python/ifrt/support/sharding_conversions_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2Fsharding_conversions_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2Fsharding_conversions_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2Fsharding_conversions_test.cc?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -108,8 +108,7 @@ class ShardingConversionsTest : public testing::TestWithParam<int> {\n \n     TF_ASSERT_OK_AND_ASSIGN(const std::vector<IndexDomain> index_domains,\n                             sharding->IndexDomains(shape));\n-    ASSERT_EQ(index_domains.size(),\n-              hlo_sharding.tile_assignment().num_elements());\n+    ASSERT_EQ(index_domains.size(), hlo_sharding.num_devices());\n     const xla::Shape xla_tile_shape = hlo_sharding.TileShape(xla_shape);\n     for (int i = 0; i < index_domains.size(); ++i) {\n       SCOPED_TRACE(absl::StrCat(\"on device \", i));"
        },
        {
            "sha": "2d319b0805a35dae732f7310b2bfd7be5e290503",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -314,12 +314,12 @@ absl::StatusOr<std::vector<IndexDomain>> HloSharding::IndexDomains(\n                                   single_device_shard_semantics);\n     }\n   }\n-  if (xla_hlo_sharding_.tile_assignment().num_elements() != num_devices) {\n-    return absl::InvalidArgumentError(absl::StrFormat(\n-        \"sharding's tile_assignment_devices and device count does not \"\n-        \"match: %d vs. %d; shape=%s, sharding=%s\",\n-        xla_hlo_sharding_.tile_assignment().num_elements(), num_devices,\n-        shape.DebugString(), DebugString()));\n+  if (xla_hlo_sharding_.num_devices() != num_devices) {\n+    return absl::InvalidArgumentError(\n+        absl::StrFormat(\"sharding's device count (%d) does not match provided \"\n+                        \"device count (%d); shape=%s, sharding=%s\",\n+                        xla_hlo_sharding_.num_devices(), num_devices,\n+                        shape.DebugString(), DebugString()));\n   }\n \n   const int64_t tiled_data_rank = xla_hlo_sharding_.TiledDataRank();"
        },
        {
            "sha": "c9cab01c97544088ba913aa4a795d27b0aff5640",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -299,8 +299,8 @@ std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n   for (int64_t i : replication_dims) {\n     group_size *= ShardCountAtDim(sharding, i);\n   }\n-  std::vector<std::vector<int64_t>> partition_groups(\n-      sharding.tile_assignment().num_elements() / group_size);\n+  std::vector<std::vector<int64_t>> partition_groups(sharding.num_devices() /\n+                                                     group_size);\n   sharding.tile_assignment().Each(\n       [&](absl::Span<const int64_t> indices, int64_t partition) {\n         int64_t group_id = 0;"
        },
        {
            "sha": "130f3d3fcaaadffd083b8736796c1d564205af5f",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -3275,8 +3275,7 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n         dim->set_padding_low(0);\n         if (i == input_sharded_dim) {\n           dim->set_padding_high(output_shard_size * split_factor *\n-                                    sharding.tile_assignment().num_elements() /\n-                                    replication_count -\n+                                    sharding.num_devices() / replication_count -\n                                 input_dim_size);\n         } else {\n           dim->set_padding_high(0);\n@@ -3313,8 +3312,7 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n       auto tmp_full_shape = tmp_shard_shape;\n       tmp_full_shape.set_dimensions(\n           output_sharded_dim, tmp_shard_shape.dimensions(output_sharded_dim) *\n-                                  sharding.tile_assignment().num_elements() /\n-                                  replication_count);\n+                                  sharding.num_devices() / replication_count);\n       auto tmp_output =\n           PartitionedHlo(tmp_reshape, tmp_full_shape, operand.state());\n \n@@ -3331,8 +3329,7 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n         if (i == output_sharded_dim) {\n           dim->set_padding_high(output_dim_size -\n                                 tmp_shard_shape.dimensions(output_sharded_dim) *\n-                                    sharding.tile_assignment().num_elements() /\n-                                    replication_count);\n+                                    sharding.num_devices() / replication_count);\n         } else {\n           dim->set_padding_high(0);\n         }\n@@ -3544,8 +3541,7 @@ absl::Status SpmdPartitioningVisitor::HandleAllReduce(HloInstruction* hlo) {\n     TF_RET_CHECK(ar->use_global_device_ids())\n         << \"Cross-partition allreduce in partial manual partitioning mode must \"\n            \"use global device IDs.\";\n-    std::vector<int64_t> partition_to_group_id(\n-        hlo->sharding().tile_assignment().num_elements());\n+    std::vector<int64_t> partition_to_group_id(hlo->sharding().num_devices());\n     hlo->sharding().tile_assignment().Each(\n         [&](absl::Span<const int64_t> indices, int64_t partition) {\n           int64_t group_id = 0;"
        },
        {
            "sha": "affc09fd30c592503656721397373b8102252789",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f402ed760b16d3ea43551e80c8797bc085db129e/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=f402ed760b16d3ea43551e80c8797bc085db129e",
            "patch": "@@ -395,8 +395,7 @@ std::vector<HloInstruction*> MakePartitionOffsets(\n       offsets.push_back(b->AddInstruction(\n           HloInstruction::CreateConstant(LiteralUtil::Zero(S32))));\n     } else {\n-      std::vector<int32_t> offset_array(\n-          sharding.tile_assignment().num_elements());\n+      std::vector<int32_t> offset_array(sharding.num_devices());\n       sharding.tile_assignment().Each(\n           [&](absl::Span<const int64_t> indices, int64_t device) {\n             offset_array[device] = indices[i] * shard_shape.dimensions(i);\n@@ -691,8 +690,7 @@ std::optional<HloSharding> PartialReplicateReshardCompatibleSharding(\n   if (!partial_sharding.ReplicateOnLastTileDim()) {\n     return std::nullopt;\n   }\n-  if (partial_sharding.tile_assignment().num_elements() !=\n-      target_sharding.tile_assignment().num_elements()) {\n+  if (partial_sharding.num_devices() != target_sharding.num_devices()) {\n     return std::nullopt;\n   }\n   const int64_t rank = partial_sharding.TiledDataRank();\n@@ -2458,7 +2456,7 @@ std::optional<std::vector<int64_t>> FindMatchingPartitionedDimsForGrouping(\n   if (sharding.IsTileMaximal() || device_groups.num_groups() < 2) {\n     return std::nullopt;\n   }\n-  const int64_t num_devices = sharding.tile_assignment().num_elements();\n+  const int64_t num_devices = sharding.num_devices();\n   if (num_devices != device_groups.num_elements()) {\n     return std::nullopt;\n   }\n@@ -2523,11 +2521,10 @@ HloSharding CreateMatchingShardingOnDims(\n   // If there is some partition across non-parallel dimensions in the\n   // other operand then partially replicate for the new\n   bool to_be_partially_replicated = false;\n-  if (num_tiles != source_sharding.tile_assignment().num_elements()) {\n-    CHECK_EQ(source_sharding.tile_assignment().num_elements() % num_tiles, 0);\n+  if (num_tiles != source_sharding.num_devices()) {\n+    CHECK_EQ(source_sharding.num_devices() % num_tiles, 0);\n     to_be_partially_replicated = true;\n-    tile_dims.push_back(source_sharding.tile_assignment().num_elements() /\n-                        num_tiles);\n+    tile_dims.push_back(source_sharding.num_devices() / num_tiles);\n   }\n   auto tgt_tile_assignment =\n       source_sharding.tile_assignment().Reshape(tile_dims);\n@@ -2877,8 +2874,8 @@ std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n     }\n   }\n \n-  std::vector<std::vector<int64_t>> partition_groups(\n-      sharding.tile_assignment().num_elements() / group_size);\n+  std::vector<std::vector<int64_t>> partition_groups(sharding.num_devices() /\n+                                                     group_size);\n   sharding.tile_assignment().Each(\n       [&](absl::Span<const int64_t> indices, int64_t partition) {\n         int64_t group_id = 0;\n@@ -2901,8 +2898,8 @@ std::vector<std::vector<int64_t>> GetPartitionGroupsAcrossTargetDims(\n   CHECK(target_dims.size() == group_sizes.size());\n   int64_t total_group_size = std::accumulate(\n       group_sizes.begin(), group_sizes.end(), 1, std::multiplies<int64_t>());\n-  std::vector<std::vector<int64_t>> groups(\n-      sharding.tile_assignment().num_elements() / total_group_size);\n+  std::vector<std::vector<int64_t>> groups(sharding.num_devices() /\n+                                           total_group_size);\n   sharding.tile_assignment().Each(\n       [&](absl::Span<const int64_t> indices, int64_t device) {\n         int64_t group_id = 0;\n@@ -2954,8 +2951,7 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsAcrossTargetDims(\n   //    [2,2,16,4,4]->[2x2x16, 4x4].\n   int64_t total_group_size = std::accumulate(\n       group_sizes.begin(), group_sizes.end(), 1, std::multiplies<int64_t>());\n-  int64_t num_replica_groups =\n-      sharding.tile_assignment().num_elements() / total_group_size;\n+  int64_t num_replica_groups = sharding.num_devices() / total_group_size;\n \n   std::vector<int64_t> reshape_dimensions;\n   reshape_dimensions.reserve(sharding.num_dimensions());\n@@ -3023,8 +3019,7 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsForReplication(\n     group_size *= sharding.dimension(i);\n   }\n \n-  int64_t num_replica_groups =\n-      sharding.tile_assignment().num_elements() / group_size;\n+  int64_t num_replica_groups = sharding.num_devices() / group_size;\n \n   // The compressed replica group list involves transposing and reshaping the\n   // initial tile assignment. We transpose the original tile assignment so that"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 39,
        "deletions": 49
    }
}