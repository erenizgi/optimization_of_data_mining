{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Add a way to allow register spilling in autotuner backends.\n\nPiperOrigin-RevId: 813751965",
    "sha": "3fc489cbdbac1d34cdf9647ea8ac720047df6e66",
    "files": [
        {
            "sha": "fb702ba428e1bc7bfaf009243bb4cb89b15c1a0a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3fc489cbdbac1d34cdf9647ea8ac720047df6e66/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3fc489cbdbac1d34cdf9647ea8ac720047df6e66/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=3fc489cbdbac1d34cdf9647ea8ac720047df6e66",
            "patch": "@@ -35,6 +35,16 @@ cc_library(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"gpu_codegen_backend_test\",\n+    srcs = [\"gpu_codegen_backend_test.cc\"],\n+    deps = [\n+        \":gpu_codegen_backend\",\n+        \"//xla:xla_proto_cc\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"block_level_emitter\",\n     srcs = [\"block_level_emitter.cc\"],"
        },
        {
            "sha": "68aef91ec1930854ac0189fbe4eebd7ba36c387b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 29,
            "deletions": 12,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3fc489cbdbac1d34cdf9647ea8ac720047df6e66/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3fc489cbdbac1d34cdf9647ea8ac720047df6e66/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=3fc489cbdbac1d34cdf9647ea8ac720047df6e66",
            "patch": "@@ -70,18 +70,9 @@ class GpuCodegenBackend : public CodegenBackend {\n     TF_RETURN_IF_ERROR(ApplyConfig(*root_instruction, config));\n \n     hlo_module->mutable_config().set_debug_options(debug_options_);\n-    DebugOptions& opts = hlo_module->mutable_config().mutable_debug_options();\n-    opts.set_xla_enable_dumping(false);\n-    // Avoid using another thread pool.\n-    opts.set_xla_gpu_force_compilation_parallelism(1);\n-    opts.set_xla_gpu_enable_llvm_module_compilation_parallelism(false);\n-    // Avoid using GPU graphs as we don't want to measure graph construction\n-    // time.\n-    opts.clear_xla_gpu_enable_command_buffer();\n-    // Avoid using async dot as we don't want to measure event overheads.\n-    opts.set_xla_gpu_async_dot(false);\n-    opts.set_xla_embed_ir_in_executable(false);\n-    opts.set_xla_gpu_kernel_cache_file(\"\");\n+    AdjustDebugOptionsForAutotuning(\n+        hlo_module->mutable_config().mutable_debug_options(),\n+        allow_register_spills_);\n \n     Compiler::CompileOptions options;\n     options.target_config = target_config_;\n@@ -93,6 +84,31 @@ class GpuCodegenBackend : public CodegenBackend {\n   }\n \n   bool CanProduceWrongResults() const override { return false; }\n+  // TODO b/443207721 - Remove this once we have a better way to handle register\n+  // spilling during autotuning.\n+  // Allows compilation to succeed even if kernels spill registers,\n+  // ignoring the `xla_gpu_filter_kernels_spilling_registers_on_autotuning`\n+  // flag. If not called, the flag's value is honored.\n+  void AllowRegisterSpills() { allow_register_spills_ = true; }\n+\n+  static void AdjustDebugOptionsForAutotuning(\n+      DebugOptions& debug_options, bool force_allow_register_spills) {\n+    debug_options.set_xla_enable_dumping(false);\n+    // Avoid using another thread pool.\n+    debug_options.set_xla_gpu_force_compilation_parallelism(1);\n+    debug_options.set_xla_gpu_enable_llvm_module_compilation_parallelism(false);\n+    // Avoid using GPU graphs as we don't want to measure graph construction\n+    // time.\n+    debug_options.clear_xla_gpu_enable_command_buffer();\n+    // Avoid using async dot as we don't want to measure event overheads.\n+    debug_options.set_xla_gpu_async_dot(false);\n+    debug_options.set_xla_embed_ir_in_executable(false);\n+    debug_options.set_xla_gpu_kernel_cache_file(\"\");\n+    if (force_allow_register_spills) {\n+      debug_options.set_xla_gpu_filter_kernels_spilling_registers_on_autotuning(\n+          false);\n+    }\n+  }\n \n  private:\n   // Optimize the HLO module.\n@@ -113,6 +129,7 @@ class GpuCodegenBackend : public CodegenBackend {\n   // and the codegen backend can directly produce an executable without a\n   // compiler instance.\n   Compiler* compiler_;\n+  bool allow_register_spills_ = false;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "e9f139ba1ce46ca8a27db8cdcf95ddbb3a83802f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend_test.cc",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3fc489cbdbac1d34cdf9647ea8ac720047df6e66/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3fc489cbdbac1d34cdf9647ea8ac720047df6e66/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend_test.cc?ref=3fc489cbdbac1d34cdf9647ea8ac720047df6e66",
            "patch": "@@ -0,0 +1,67 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n+\n+#include <gtest/gtest.h>\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class GpuCodegenBackendTest : public ::testing::Test {};\n+\n+TEST_F(GpuCodegenBackendTest, AdjustDebugOptionsForAutotuning) {\n+  DebugOptions debug_options;\n+  debug_options.set_xla_enable_dumping(true);\n+  debug_options.set_xla_gpu_force_compilation_parallelism(4);\n+  debug_options.set_xla_gpu_enable_llvm_module_compilation_parallelism(true);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n+  debug_options.set_xla_gpu_async_dot(true);\n+  debug_options.set_xla_embed_ir_in_executable(true);\n+  debug_options.set_xla_gpu_kernel_cache_file(\"foo.txt\");\n+  debug_options.set_xla_gpu_filter_kernels_spilling_registers_on_autotuning(\n+      true);\n+\n+  GpuCodegenBackend::AdjustDebugOptionsForAutotuning(\n+      debug_options, /*force_allow_register_spills=*/false);\n+\n+  EXPECT_FALSE(debug_options.xla_enable_dumping());\n+  EXPECT_EQ(debug_options.xla_gpu_force_compilation_parallelism(), 1);\n+  EXPECT_FALSE(\n+      debug_options.xla_gpu_enable_llvm_module_compilation_parallelism());\n+  EXPECT_TRUE(debug_options.xla_gpu_enable_command_buffer().empty());\n+  EXPECT_FALSE(debug_options.xla_gpu_async_dot());\n+  EXPECT_FALSE(debug_options.xla_embed_ir_in_executable());\n+  EXPECT_EQ(debug_options.xla_gpu_kernel_cache_file(), \"\");\n+  EXPECT_TRUE(\n+      debug_options.xla_gpu_filter_kernels_spilling_registers_on_autotuning());\n+}\n+\n+TEST_F(GpuCodegenBackendTest, AdjustDebugOptionsForAutotuningAllowSpilling) {\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_filter_kernels_spilling_registers_on_autotuning(\n+      true);\n+\n+  GpuCodegenBackend::AdjustDebugOptionsForAutotuning(\n+      debug_options, /*force_allow_register_spills=*/true);\n+\n+  EXPECT_FALSE(\n+      debug_options.xla_gpu_filter_kernels_spilling_registers_on_autotuning());\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 118,
        "additions": 106,
        "deletions": 12
    }
}