{
    "author": "vwbaker",
    "message": "Autotune reductions and transposes between the BlockLevelEmitter & the NativeEmitter\n\nAs there are still some bugs to work out, we are launching this behind a flag. In the mean time, nucleo and others can test this for themselves using the flag --xla_gpu_experimental_enable_fusion_autotuner\n\nPiperOrigin-RevId: 803139264",
    "sha": "98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0",
    "files": [
        {
            "sha": "b07afc7f547d3d098f3a9fd35d421ef364d0ce26",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0",
            "patch": "@@ -1893,6 +1893,7 @@ cc_library(\n         \"//xla/service:call_inliner\",\n         \"//xla/service:dump\",\n         \"//xla/service:float_support\",\n+        \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_verifier\",\n         \"//xla/service/gpu/autotuning:autotuner_pass\",\n@@ -1917,11 +1918,9 @@ cc_library(\n         \"//xla/service/gpu/transforms:triangular_solve_rewriter\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/cuda:assemble_compilation_provider\",\n         \"//xla/stream_executor/cuda:compilation_options\",\n         \"//xla/stream_executor/cuda:compilation_provider\","
        },
        {
            "sha": "ba3bff99166c770e0bf67c1a79f2838036a32356",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0",
            "patch": "@@ -1597,6 +1597,13 @@ absl::Status GpuCompiler::OptimizeHloModule(\n       RunCollectiveScheduleLinearizerPasses(hlo_module, stream_exec));\n \n   TF_RETURN_IF_ERROR(RunAsyncDotPasses(hlo_module));\n+  {\n+    HloPassPipeline pipeline(\"autotune-fusion-emitters\");\n+    TF_RETURN_IF_ERROR(AddFusionAutotuningPass(\n+        &pipeline, hlo_module, options, thread_pool.get_mutable(), stream_exec,\n+        ShapeSizeBytesFunction()));\n+    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+  }\n \n   return absl::OkStatus();\n }  // NOLINT(readability/fn_size)"
        },
        {
            "sha": "6ed4274dadc248bbd57d777a4f3353bde99530dd",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"tsl/platform/threadpool.h\"\n@@ -180,6 +181,14 @@ class GpuCompiler : public LLVMCompiler {\n     return absl::OkStatus();\n   }\n \n+  virtual absl::Status AddFusionAutotuningPass(\n+      HloPassPipeline* pipeline, HloModule* hlo_module,\n+      const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n+      stream_executor::StreamExecutor* stream_executor,\n+      HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n+    return absl::OkStatus();\n+  }\n+\n   // Runs cuDNN fusion and custom call compiler passes.\n   virtual absl::Status RunCudnnCompilerPasses(HloModule* module,\n                                               se::StreamExecutor* stream_exec,"
        },
        {
            "sha": "d456c45b5a51f74d1ed1e8b01fdf2fe4c032907b",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0",
            "patch": "@@ -41,9 +41,14 @@ limitations under the License.\n #include \"llvm/Support/SourceMgr.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/block_level_emitter.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n+#include \"xla/backends/gpu/autotuner/native_emitter.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/pass/hlo_pass_fix.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n@@ -87,6 +92,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/cudnn_vectorize_convolutions.h\"\n #include \"xla/service/gpu/transforms/gpusolver_rewriter.h\"\n #include \"xla/service/gpu/transforms/triangular_solve_rewriter.h\"\n+#include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_verifier.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n@@ -393,6 +399,59 @@ absl::Status NVPTXCompiler::AddGemmFusionAutotuningPasses(\n   return absl::OkStatus();\n }\n \n+namespace {\n+\n+// Returns true if the instruction is a fusion that would go through the native\n+// emitter, but may benefit from going through the block-level emitter.\n+// Currently, we only do this for reductions and transposes.\n+bool ShouldAutotuneBetweenFusionEmitters(const HloInstruction& instruction) {\n+  if (instruction.opcode() != HloOpcode::kFusion) {\n+    return false;\n+  }\n+  auto fusion = Cast<const HloFusionInstruction>(&instruction);\n+  // kCustom fusions have already been assigned to a backend and we don't want\n+  // to override it.\n+  if (fusion->fusion_kind() == HloInstruction::FusionKind::kCustom) {\n+    return false;\n+  }\n+  return absl::c_any_of(\n+      fusion->fused_instructions_computation()->instructions(),\n+      HloPredicateIsOp<HloOpcode::kReduce, HloOpcode::kTranspose>);\n+}\n+\n+}  // namespace\n+\n+absl::Status NVPTXCompiler::AddFusionAutotuningPass(\n+    HloPassPipeline* pipeline, HloModule* hlo_module,\n+    const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n+    stream_executor::StreamExecutor* stream_executor,\n+    HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n+  if (stream_executor == nullptr) {\n+    return absl::OkStatus();\n+  }\n+  const DebugOptions& debug_options = hlo_module->config().debug_options();\n+  if (debug_options.xla_gpu_autotune_level() == 0 ||\n+      debug_options.xla_gpu_exclude_nondeterministic_ops() ||\n+      !debug_options.xla_gpu_experimental_enable_fusion_autotuner()) {\n+    return absl::OkStatus();\n+  }\n+\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::make_unique<BlockLevelEmitterBackend>(\n+      stream_executor, &debug_options, this, shape_size_fn,\n+      /*use_default_config=*/true));\n+  backends.push_back(std::make_unique<NativeEmitterBackend>(\n+      stream_executor, &debug_options, this));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<AutotunerPass> autotuner_pass,\n+      AutotunerPass::Create(std::move(backends), debug_options, stream_executor,\n+                            thread_pool, ShouldAutotuneBetweenFusionEmitters,\n+                            options.device_allocator));\n+  pipeline->AddPass(std::move(autotuner_pass));\n+  return absl::OkStatus();\n+}\n+\n absl::Status NVPTXCompiler::RunCudnnCompilerPasses(\n     HloModule* module, se::StreamExecutor* stream_exec,\n     BinaryMap* dnn_compiled_graphs) {"
        },
        {
            "sha": "516c80929616bb4cac7953dd55747c44808b9856",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h?ref=98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0",
            "patch": "@@ -82,6 +82,12 @@ class NVPTXCompiler : public GpuCompiler {\n       const se::SemanticVersion& toolkit_version,\n       se::StreamExecutor* stream_executor) override;\n \n+  absl::Status AddFusionAutotuningPass(\n+      HloPassPipeline* pipeline, HloModule* hlo_module,\n+      const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n+      stream_executor::StreamExecutor* stream_executor,\n+      HloCostAnalysis::ShapeSizeFunction shape_size_fn) override;\n+\n   absl::Status RunCudnnCompilerPasses(HloModule* module,\n                                       se::StreamExecutor* stream_exec,\n                                       BinaryMap* dnn_compiled_graphs) override;"
        },
        {
            "sha": "b07400847d3f202c781280f96240107964bed51b",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=98865cd6d0cb263e365e5a46f9ba6f9e1ec997e0",
            "patch": "@@ -604,6 +604,9 @@ message DebugOptions {\n   // Enables an experimental feature for command buffer conversion on thunks.\n   optional bool xla_gpu_experimental_enable_command_buffer_on_thunks = 394;\n \n+  // If true, enable autotuning between the native & triton fusion emitters.\n+  optional bool xla_gpu_experimental_enable_fusion_autotuner = 409;\n+\n   // Enabling this flag will attempt to redirect every already-constructed\n   // fusion possible to the Triton emitter.\n   //\n@@ -1330,7 +1333,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 409\n+  // Next id: 410\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 89,
        "additions": 86,
        "deletions": 3
    }
}