{
    "author": "loislo",
    "message": "[XLA] Replace TF_ASSIGN_OR_RETURN and TF_RETURN_IF_ERROR with standard macros.\n\nThis change migrates from the TensorFlow-specific `TF_ASSIGN_OR_RETURN` and `TF_RETURN_IF_ERROR` macros to the more standard `ASSIGN_OR_RETURN` and `RETURN_IF_ERROR`. A new `tsl/platform/status_macros.h` is introduced to provide these macros in the open-source build, while internally they map to `util/task/status_macros.h`.\n\nPiperOrigin-RevId: 840316952",
    "sha": "20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2",
    "files": [
        {
            "sha": "6d4c270f5b856f5a0380581790858b0308a26aca",
            "filename": "third_party/xla/xla/hlo/ir/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD?ref=20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2",
            "patch": "@@ -102,6 +102,7 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:config\","
        },
        {
            "sha": "c54669e78da3db006f96fabf798fa268304bf225",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 45,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc?ref=20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2",
            "patch": "@@ -86,6 +86,7 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla {\n \n@@ -365,8 +366,8 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n                               }))\n       << proto.name() << \" instruction references invalid computation id(s)\";\n \n-  TF_ASSIGN_OR_RETURN(Shape shape, Shape::FromProto(proto.shape()));\n-  TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n+  ASSIGN_OR_RETURN(Shape shape, Shape::FromProto(proto.shape()));\n+  RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n \n   std::optional<int> arity = HloOpcodeArity(opcode);\n   if (arity) {\n@@ -480,15 +481,15 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n     case HloOpcode::kCompare: {\n       // Auto-upgraded from deprecated opcode skips the following.\n       if (!comparison_direction) {\n-        TF_ASSIGN_OR_RETURN(\n+        ASSIGN_OR_RETURN(\n             comparison_direction,\n             StringToComparisonDirection(proto.comparison_direction()));\n       }\n       auto comparison_type_str = proto.comparison_type();\n       if (!comparison_type_str.empty()) {\n         // If a comparison type is specified, it *must* be valid.\n-        TF_ASSIGN_OR_RETURN(auto comparison_type,\n-                            StringToComparisonType(comparison_type_str));\n+        ASSIGN_OR_RETURN(auto comparison_type,\n+                         StringToComparisonType(comparison_type_str));\n         instruction = CreateCompare(shape, operands(0), operands(1),\n                                     *comparison_direction, comparison_type);\n       } else {\n@@ -639,7 +640,7 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n     case HloOpcode::kConstant: {\n       // TODO(b/110214922): Revert this to CHECK(proto.has_literal()).\n       if (proto.has_literal()) {\n-        TF_ASSIGN_OR_RETURN(\n+        ASSIGN_OR_RETURN(\n             auto literal,\n             Literal::CreateFromProto(proto.literal(), prohibit_empty_literal));\n         instruction = CreateConstant(std::move(literal));\n@@ -659,8 +660,8 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n       // HloInstructionProto and do not appear as an HloComputationProto within\n       // the HloModuleProto.\n       TF_RET_CHECK(!proto.fusion_kind().empty());\n-      TF_ASSIGN_OR_RETURN(FusionKind fusion_kind,\n-                          StringToFusionKind(proto.fusion_kind()));\n+      ASSIGN_OR_RETURN(FusionKind fusion_kind,\n+                       StringToFusionKind(proto.fusion_kind()));\n \n       // Find the fused computation and set its fusion instruction.\n       TF_RET_CHECK(proto.called_computation_ids_size() == 1)\n@@ -714,9 +715,9 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n           CreateInfeed(data_shape, operands(0), proto.infeed_config());\n     } break;\n     case HloOpcode::kOutfeed: {\n-      TF_ASSIGN_OR_RETURN(Shape outfeed_shape,\n-                          Shape::FromProto(proto.outfeed_shape()));\n-      TF_RETURN_IF_ERROR(\n+      ASSIGN_OR_RETURN(Shape outfeed_shape,\n+                       Shape::FromProto(proto.outfeed_shape()));\n+      RETURN_IF_ERROR(\n           ShapeUtil::ValidateShapeWithOptionalLayout(outfeed_shape));\n       instruction = CreateOutfeed(outfeed_shape, operands(0), operands(1),\n                                   proto.outfeed_config());\n@@ -996,7 +997,7 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n             proto.operand_shapes_with_layout();\n         operand_shapes.reserve(operand_shapes_with_layout.size());\n         for (const ShapeProto& shape_proto : operand_shapes_with_layout) {\n-          TF_ASSIGN_OR_RETURN(Shape shape, Shape::FromProto(shape_proto));\n+          ASSIGN_OR_RETURN(Shape shape, Shape::FromProto(shape_proto));\n           operand_shapes.emplace_back(std::move(shape));\n         }\n         TF_RET_CHECK(proto.called_computation_ids_size() <= 1);\n@@ -1032,7 +1033,7 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n         custom_call_instr->set_window(proto.window());\n       }\n       if (proto.has_literal()) {\n-        TF_ASSIGN_OR_RETURN(\n+        ASSIGN_OR_RETURN(\n             auto literal,\n             Literal::CreateFromProto(proto.literal(), prohibit_empty_literal));\n         custom_call_instr->set_literal(std::move(literal));\n@@ -1205,15 +1206,13 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n       std::shared_ptr<const HloSharding> entry_hlo_sharding;\n       std::shared_ptr<const HloSharding> exit_hlo_sharding;\n       if (proto.has_domain_entry_sharding()) {\n-        TF_ASSIGN_OR_RETURN(\n-            HloSharding sharding,\n-            HloSharding::FromProto(proto.domain_entry_sharding()));\n+        ASSIGN_OR_RETURN(HloSharding sharding,\n+                         HloSharding::FromProto(proto.domain_entry_sharding()));\n         entry_hlo_sharding = std::make_shared<const HloSharding>(sharding);\n       }\n       if (proto.has_domain_exit_sharding()) {\n-        TF_ASSIGN_OR_RETURN(\n-            HloSharding sharding,\n-            HloSharding::FromProto(proto.domain_exit_sharding()));\n+        ASSIGN_OR_RETURN(HloSharding sharding,\n+                         HloSharding::FromProto(proto.domain_exit_sharding()));\n         exit_hlo_sharding = std::make_shared<const HloSharding>(sharding);\n       }\n       instruction = std::make_unique<HloDomainInstruction>(\n@@ -1365,8 +1364,8 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n         << \"No instruction with id \" << predecessor_id\n         << \" (local id: \" << local_predecessor_id << \") in computation \"\n         << proto.name();\n-    TF_RETURN_IF_ERROR(instruction_map.at(local_predecessor_id)\n-                           ->AddControlDependencyTo(instruction.get()));\n+    RETURN_IF_ERROR(instruction_map.at(local_predecessor_id)\n+                        ->AddControlDependencyTo(instruction.get()));\n   }\n \n   TF_RET_CHECK(!proto.name().empty());\n@@ -1383,8 +1382,8 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> HloInstruction::CreateFromProto(\n   instruction->local_id_ = CalculateLocalId(proto.id());\n \n   if (proto.has_sharding()) {\n-    TF_ASSIGN_OR_RETURN(HloSharding sharding,\n-                        HloSharding::FromProto(proto.sharding()));\n+    ASSIGN_OR_RETURN(HloSharding sharding,\n+                     HloSharding::FromProto(proto.sharding()));\n     // To allow for existing Hlo protos to not fail verification, apply tuple\n     // sharding normalization.\n     sharding = sharding.NormalizeTupleSharding(instruction->shape());\n@@ -3038,11 +3037,11 @@ absl::Status HloInstruction::SafelyDropAllControlDependencies() {\n   if (has_rare()) {\n     for (HloInstruction* predecessor : rare()->control_predecessors) {\n       for (HloInstruction* successor : rare()->control_successors) {\n-        TF_RETURN_IF_ERROR(predecessor->AddControlDependencyTo(successor));\n+        RETURN_IF_ERROR(predecessor->AddControlDependencyTo(successor));\n       }\n     }\n   }\n-  TF_RETURN_IF_ERROR(DropAllControlDeps());\n+  RETURN_IF_ERROR(DropAllControlDeps());\n   return absl::OkStatus();\n }\n \n@@ -3059,10 +3058,10 @@ bool HloInstruction::HasSuccessorControlDependencies() const {\n absl::Status HloInstruction::CopyAllControlDepsTo(HloInstruction* start,\n                                                   HloInstruction* end) const {\n   for (auto* ctrl_pred : control_predecessors()) {\n-    TF_RETURN_IF_ERROR(ctrl_pred->AddControlDependencyTo(start));\n+    RETURN_IF_ERROR(ctrl_pred->AddControlDependencyTo(start));\n   }\n   for (auto* ctrl_succ : control_successors()) {\n-    TF_RETURN_IF_ERROR(end->AddControlDependencyTo(ctrl_succ));\n+    RETURN_IF_ERROR(end->AddControlDependencyTo(ctrl_succ));\n   }\n   return absl::OkStatus();\n }\n@@ -3377,7 +3376,7 @@ absl::Status HloInstruction::ReplaceUseWithDifferentShape(\n   new_producer->AddUser(user);\n   // Custom fusions may not be able to handle deduplicated operands.\n   if (user->opcode() == HloOpcode::kFusion) {\n-    TF_RETURN_IF_ERROR(\n+    RETURN_IF_ERROR(\n         Cast<HloFusionInstruction>(user)->DeduplicateFusionOperands());\n   }\n   return absl::OkStatus();\n@@ -3482,11 +3481,11 @@ absl::Status HloInstruction::Defuse() {\n     defused_instructions[fused_instruction] = defused_instruction;\n   }\n \n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       ReplaceAllUsesWith(defused_instructions.at(fused_expression_root())));\n \n   HloModule* module = GetModule();\n-  TF_RETURN_IF_ERROR(parent()->RemoveInstruction(this));\n+  RETURN_IF_ERROR(parent()->RemoveInstruction(this));\n   return module->RemoveEmbeddedComputation(fused_computation);\n }\n \n@@ -3530,10 +3529,10 @@ absl::StatusOr<HloInstruction*> HloInstruction::UnfuseInstruction(\n \n   HloInstruction* new_parameter = AddFusionOperand(unfused_instruction);\n   // Replace the instruction in the fusion computation with the new parameter.\n-  TF_RETURN_IF_ERROR(instruction->ReplaceAllUsesWith(new_parameter));\n+  RETURN_IF_ERROR(instruction->ReplaceAllUsesWith(new_parameter));\n \n   // Remove the original instruction from the fusion computation.\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       fusion_computation->RemoveInstructionAndUnusedOperands(instruction));\n \n   return unfused_instruction;\n@@ -3552,7 +3551,7 @@ absl::Status HloInstruction::ReplaceAllUsesWithDifferentShape(\n   // Make a copy since users span might get mutated during the loop\n   std::vector<HloInstruction*> users_vector(users.begin(), users.end());\n   for (HloInstruction* user : users_vector) {\n-    TF_RETURN_IF_ERROR(ReplaceUseWithDifferentShape(user, new_producer));\n+    RETURN_IF_ERROR(ReplaceUseWithDifferentShape(user, new_producer));\n   }\n \n   if (parent_ && parent_->root_instruction() == this) {\n@@ -3593,7 +3592,7 @@ absl::Status HloInstruction::ReplaceAllUsesWithDifferentShape(\n                    new_producer);\n       new_producer->AddUser(user);\n       if (user->opcode() == HloOpcode::kFusion) {\n-        TF_RETURN_IF_ERROR(\n+        RETURN_IF_ERROR(\n             Cast<HloFusionInstruction>(user)->DeduplicateFusionOperands());\n       }\n     }\n@@ -4871,11 +4870,11 @@ static absl::Status PostOrderDFS(\n       dfs_stack.pop_back();\n \n       if (visitor->ShouldProcessNode(current_node)) {\n-        TF_RETURN_IF_ERROR(visitor->Preprocess(current_node));\n+        RETURN_IF_ERROR(visitor->Preprocess(current_node));\n         VLOG(2) << \"Visiting HLO %\" << current_node->name();\n-        TF_RETURN_IF_ERROR(current_node->Visit(visitor));\n+        RETURN_IF_ERROR(current_node->Visit(visitor));\n         visitor->SetVisitState(current_id, Visitor::kVisited);\n-        TF_RETURN_IF_ERROR(visitor->Postprocess(current_node));\n+        RETURN_IF_ERROR(visitor->Postprocess(current_node));\n       } else {\n         visitor->SetVisitState(current_id, Visitor::kVisited);\n       }\n@@ -4942,11 +4941,10 @@ absl::Status HloInstruction::Accept(\n     DfsHloVisitorBase<HloInstructionPtr>* visitor, bool call_finish_visit,\n     bool ignore_control_predecessors, bool cross_computation) {\n   VLOG(3) << \"HloInstruction::Accept(%\" << name() << \")\";\n-  TF_RETURN_IF_ERROR(PostOrderDFS(this, visitor, std::nullopt,\n-                                  ignore_control_predecessors,\n-                                  cross_computation));\n+  RETURN_IF_ERROR(PostOrderDFS(this, visitor, std::nullopt,\n+                               ignore_control_predecessors, cross_computation));\n   if (call_finish_visit) {\n-    TF_RETURN_IF_ERROR(visitor->FinishVisit(this));\n+    RETURN_IF_ERROR(visitor->FinishVisit(this));\n   }\n   return absl::OkStatus();\n }\n@@ -4966,12 +4964,12 @@ absl::Status HloInstruction::AcceptWithOperandOrder(\n     // objects (ignoring the internal ids we also have in our stack entries)\n     return operand_order(a.second, b.second);\n   };\n-  TF_RETURN_IF_ERROR(PostOrderDFS(this, visitor, func,\n-                                  /*ignore_control_predecessors=*/false,\n-                                  /*cross_computation=*/false));\n+  RETURN_IF_ERROR(PostOrderDFS(this, visitor, func,\n+                               /*ignore_control_predecessors=*/false,\n+                               /*cross_computation=*/false));\n   if (call_finish_visit) {\n     VLOG(3) << \"HloInstruction::AcceptWithOperandOrder BEFORE FINISH VISIT\";\n-    TF_RETURN_IF_ERROR(visitor->FinishVisit(this));\n+    RETURN_IF_ERROR(visitor->FinishVisit(this));\n     VLOG(3) << \"HloInstruction::AcceptWithOperandOrder AFTER FINISH VISIT\";\n   }\n   VLOG(2) << \"HloInstruction::AcceptWithOperandOrder EXIT\";"
        },
        {
            "sha": "9848d75ccff4b1a1549b061fbdb4436592a57081",
            "filename": "third_party/xla/xla/tsl/platform/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2FBUILD?ref=20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2",
            "patch": "@@ -768,6 +768,16 @@ cc_library(\n     ] + tf_platform_deps(\"types\"),\n )\n \n+# copybara:comment_begin(oss-only)\n+cc_library(\n+    name = \"status_macros\",\n+    hdrs = [\"status_macros.h\"],\n+    compatible_with = get_compatible_with_portable(),\n+    visibility = internal_visibility([]),  # The header should be used only in OSS.\n+    deps = [\":statusor\"],\n+)\n+# copybara:comment_end\n+\n cc_library(\n     name = \"byte_order\",\n     hdrs = [\"byte_order.h\"],"
        },
        {
            "sha": "6daad16f8f94ba5a98ca687653ce5b53e5c5124e",
            "filename": "third_party/xla/xla/tsl/platform/status_macros.h",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fstatus_macros.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fstatus_macros.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fstatus_macros.h?ref=20260e4d35b8dd5a5e4d258c10a9ac1fbcb6d0e2",
            "patch": "@@ -0,0 +1,46 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_TSL_PLATFORM_STATUS_MACROS_H_\n+#define XLA_TSL_PLATFORM_STATUS_MACROS_H_\n+\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+#ifndef ASSIGN_OR_RETURN\n+#define ASSIGN_OR_RETURN(lhs, rexpr) \\\n+  ASSIGN_OR_RETURN_IMPL(             \\\n+      TF_STATUS_MACROS_CONCAT_NAME(_status_or_value, __COUNTER__), lhs, rexpr)\n+\n+#define ASSIGN_OR_RETURN_IMPL(statusor, lhs, rexpr) \\\n+  auto statusor = (rexpr);                          \\\n+  if (ABSL_PREDICT_FALSE(!statusor.ok())) {         \\\n+    return statusor.status();                       \\\n+  }                                                 \\\n+  lhs = std::move(statusor).value()\n+#endif  // ASSIGN_OR_RETURN\n+\n+#ifndef RETURN_IF_ERROR\n+// For propagating errors when calling a function.\n+#define RETURN_IF_ERROR(...)                 \\\n+  do {                                       \\\n+    absl::Status _status = (__VA_ARGS__);    \\\n+    if (ABSL_PREDICT_FALSE(!_status.ok())) { \\\n+      MAYBE_ADD_SOURCE_LOCATION(_status)     \\\n+      return _status;                        \\\n+    }                                        \\\n+  } while (0)\n+#endif  // RETURN_IF_ERROR\n+\n+#endif  // XLA_TSL_PLATFORM_STATUS_MACROS_H_"
        }
    ],
    "stats": {
        "total": 145,
        "additions": 100,
        "deletions": 45
    }
}