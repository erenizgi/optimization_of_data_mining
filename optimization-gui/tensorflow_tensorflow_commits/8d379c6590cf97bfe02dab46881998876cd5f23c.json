{
    "author": "loislo",
    "message": "[XLA:GPU] Reorder operands of HLO ScaledDot.\n\nThe order of operands for `scaled-dot` is changed from `(lhs, lhs_scale, rhs, rhs_scale)` to `(lhs, rhs, lhs_scale, rhs_scale)`. This aligns the primary operands (`lhs`, `rhs`) at indices 0 and 1, similar to `dot-general` and cuDNN BlockScaledDot. All call sites and related analyses have been updated to reflect this change.\n\nPiperOrigin-RevId: 815614543",
    "sha": "8d379c6590cf97bfe02dab46881998876cd5f23c",
    "files": [
        {
            "sha": "2da3f9a064df1b7ec00809009731c4b8bc88918f",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -2460,8 +2460,8 @@ precision_config,preferred_element_type)`**\n | Arguments                | Type                      | Semantics             |\n | ------------------------ | ------------------------- | --------------------- |\n | `lhs`                    | `XlaOp`                   | array of type T       |\n-| `lhs_scale`              | `XlaOp`                   | array of type T       |\n | `rhs`                    | `XlaOp`                   | array of type T       |\n+| `lhs_scale`              | `XlaOp`                   | array of type T       |\n | `rhs_scale`              | `XlaOp`                   | array of type T       |\n | `dimension_number`       | `ScatterDimensionNumbers` | Dimension numbers for |\n :                          :                           : scatter operation     :\n@@ -2473,7 +2473,7 @@ precision_config,preferred_element_type)`**\n Similar to [DotGeneral](#dotgeneral).\n \n Creates a scaled dot op with operands 'lhs', 'lhs_scale', 'rhs', and\n-'rhs_scale', with contracting, batch, ragged, and group dimensions specified in\n+'rhs_scale', with contracting and batch dimensions specified in\n 'dimension_numbers'.\n \n > **Note:** `ScaledDot` is only found in HLO. It is not found in StableHLO."
        },
        {
            "sha": "779c5eac8f123ac7c3f6843eb34eb5426a462c1a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1131,19 +1131,19 @@ TEST_F(CuDnnFusionFileCheckTest, BlockScaledDotLowering) {\n   const std::string kHloText = R\"(\n block_scaled_dot {\n   %lhs = f8e4m3fn[256,128] parameter(0)\n-  %lhs_scale = f8e8m0fnu[256,4] parameter(1)\n-  %rhs = f8e4m3fn[384,128] parameter(2)\n+  %rhs = f8e4m3fn[384,128] parameter(1)\n+  %lhs_scale = f8e8m0fnu[256,4] parameter(2)\n   %rhs_scale = f8e8m0fnu[384,4] parameter(3)\n-  ROOT %result = f32[256,384] scaled-dot(%lhs, %lhs_scale, %rhs, %rhs_scale),\n+  ROOT %result = f32[256,384] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n       lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n ENTRY main {\n   %lhs = f8e4m3fn[256,128] parameter(0)\n-  %lhs_scale = f8e8m0fnu[256,4] parameter(1)\n-  %rhs = f8e4m3fn[384,128] parameter(2)\n+  %rhs = f8e4m3fn[384,128] parameter(1)\n+  %lhs_scale = f8e8m0fnu[256,4] parameter(2)\n   %rhs_scale = f8e8m0fnu[384,4] parameter(3)\n-  ROOT %result = f32[256,384] fusion(%lhs,%lhs_scale, %rhs, %rhs_scale),\n+  ROOT %result = f32[256,384] fusion(%lhs, %rhs, %lhs_scale, %rhs_scale),\n       kind=kCustom, calls=block_scaled_dot,\n       backend_config={\"fusion_backend_config\":{kind:\"__cudnn$fusion\"}}\n })\";"
        },
        {
            "sha": "24268fca654848032381aa274d893f498cdc243a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -39,8 +39,8 @@ struct DotOperands {\n // an accumulator and their respective scaling factors.\n struct ScaledDotOperands {\n   ::mlir::Value lhs;\n-  ::mlir::Value lhs_scale;\n   ::mlir::Value rhs;\n+  ::mlir::Value lhs_scale;\n   ::mlir::Value rhs_scale;\n   ::mlir::Value accumulator;\n };"
        },
        {
            "sha": "7a2174a65cef1314063b7c55819f69bfc6882058",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1187,13 +1187,13 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n         Value lhs, MaskDotOperand(b, *tiled_hlo_dot.operand(0), dot_args[0],\n                                   ki_i32, lhs_contracting_dim_idx));\n     TF_ASSIGN_OR_RETURN(\n-        Value lhs_scale,\n-        MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1], ki_i32,\n-                       lhs_contracting_dim_idx));\n+        Value rhs, MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1],\n+                                  ki_i32, rhs_contracting_dim_idx));\n \n     TF_ASSIGN_OR_RETURN(\n-        Value rhs, MaskDotOperand(b, *tiled_hlo_dot.operand(2), dot_args[2],\n-                                  ki_i32, rhs_contracting_dim_idx));\n+        Value lhs_scale,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(2), dot_args[2], ki_i32,\n+                       lhs_contracting_dim_idx));\n \n     TF_ASSIGN_OR_RETURN(\n         Value rhs_scale,\n@@ -1205,12 +1205,12 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n     TF_ASSIGN_OR_RETURN(lhs,\n                         CanonicalizeDotOperand(b, lhs, lhs_contracting_dim_idx,\n                                                DotOperandSide::kLhs));\n-    TF_ASSIGN_OR_RETURN(\n-        lhs_scale, CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n-                                          DotOperandSide::kLhs));\n     TF_ASSIGN_OR_RETURN(rhs,\n                         CanonicalizeDotOperand(b, rhs, rhs_contracting_dim_idx,\n                                                DotOperandSide::kRhs));\n+    TF_ASSIGN_OR_RETURN(\n+        lhs_scale, CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n+                                          DotOperandSide::kLhs));\n     TF_ASSIGN_OR_RETURN(\n         rhs_scale, CanonicalizeDotOperand(b, rhs_scale, rhs_contracting_dim_idx,\n                                           DotOperandSide::kRhs));\n@@ -1219,7 +1219,7 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n         Value acc_next,\n         triton::EmitSingleTileScaledDot(\n             b, scaled_dot,\n-            triton::ScaledDotOperands{lhs, lhs_scale, rhs, rhs_scale, acc}));\n+            triton::ScaledDotOperands{lhs, rhs, lhs_scale, rhs_scale, acc}));\n     b.create<mlir::scf::YieldOp>(acc_next);\n   }\n "
        },
        {
            "sha": "5a7973ead3a2d0150150f4c65d29eacd3a8f8029",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 36,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -3802,10 +3802,10 @@ TEST_F(TritonEmitterTest, ScaledDotIsSupportedByReferencePlatform) {\n \n     ENTRY entry {\n      lhs = bf16[4,4] parameter(0)\n-     lhs_scale = bf16[1,1] parameter(1)\n-     rhs = bf16[4,4] parameter(2)\n+     rhs = bf16[4,4] parameter(1)\n+     lhs_scale = bf16[1,1] parameter(2)\n      rhs_scale = bf16[1,1] parameter(3)\n-     ROOT dot = bf16[4,4] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+     ROOT dot = bf16[4,4] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n          lhs_contracting_dims={1},\n          rhs_contracting_dims={1}\n     }\n@@ -3943,35 +3943,35 @@ ENTRY entry {\n \n struct ScaleDotTestParams {\n   std::string lhs_type;\n-  std::string lhs_scale_type;\n   std::string rhs_type;\n+  std::string lhs_scale_type;\n   std::string rhs_scale_type;\n   std::string output_type;\n   std::string expected_triton_type;\n \n   std::string PrepareHloText(absl::string_view hlo_template) const {\n     return absl::StrReplaceAll(hlo_template,\n                                {{\"$lhs_type\", lhs_type},\n-                                {\"$lhs_scale_type\", lhs_scale_type},\n                                 {\"$rhs_type\", rhs_type},\n+                                {\"$lhs_scale_type\", lhs_scale_type},\n                                 {\"$rhs_scale_type\", rhs_scale_type},\n                                 {\"$output_type\", output_type}});\n   }\n   static std::string ToString(\n       const ::testing::TestParamInfo<ScaleDotTestParams>& info) {\n     const ScaleDotTestParams& params = info.param;\n-    auto name = absl::StrCat(params.lhs_type, \"_\", params.lhs_scale_type, \"_\",\n-                             params.rhs_type, \"_\", params.rhs_scale_type, \"_\",\n-                             params.output_type);\n+    auto name = absl::StrCat(params.lhs_type, \"_\", params.rhs_type, \"_\",\n+                             params.lhs_scale_type, \"_\", params.rhs_scale_type,\n+                             \"_\", params.output_type);\n     absl::StrReplaceAll({{\"[\", \"_\"}, {\"]\", \"_\"}, {\",\", \"x\"}}, &name);\n     return name;\n   }\n };\n \n std::ostream& operator<<(std::ostream& stream, const ScaleDotTestParams& tc) {\n   return stream << \"{\\n\\tlhs_type:\" << tc.lhs_type\n-                << \",\\n\\tlhs_scale_type:\" << tc.lhs_scale_type\n                 << \",\\n\\trhs_type:\" << tc.rhs_type\n+                << \",\\n\\tlhs_scale_type:\" << tc.lhs_scale_type\n                 << \",\\n\\trhs_scale_type:\" << tc.rhs_scale_type\n                 << \",\\n\\toutput_type:\" << tc.output_type << \"\\n}\";\n }\n@@ -3999,12 +3999,12 @@ HloModule m\n flhs (p0: $lhs_type) -> $lhs_type {\n   ROOT p0 = $lhs_type{1,0} parameter(0)\n }\n-flhs_scale (p0: $lhs_scale_type) -> $lhs_scale_type {\n-  ROOT p0 = $lhs_scale_type{1,0} parameter(0)\n-}\n frhs (p0: $rhs_type) -> $rhs_type {\n   ROOT p0 = $rhs_type{1,0} parameter(0)\n }\n+flhs_scale (p0: $lhs_scale_type) -> $lhs_scale_type {\n+  ROOT p0 = $lhs_scale_type{1,0} parameter(0)\n+}\n frhs_scale (p0: $rhs_scale_type) -> $rhs_scale_type {\n   ROOT p0 = $rhs_scale_type{1,0} parameter(0)\n }\n@@ -4025,30 +4025,30 @@ triton_dot {\n         }\n       }\n     }\n-  lhs_scale = $lhs_scale_type parameter(1)\n-  lhs_scale1 = $lhs_scale_type{1,0} fusion(lhs_scale),\n+  rhs = $rhs_type parameter(1)\n+  rhs1 = $rhs_type{1,0} fusion(rhs),\n     kind=kCustom,\n-    calls=flhs_scale,\n+    calls=frhs,\n     backend_config={\n       \"fusion_backend_config\":{\n         \"kind\":\"__triton_nested_gemm_fusion\",\n         \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"256\"]}],\n           \"num_warps\":\"4\",\n           \"num_stages\":\"1\",\n           \"num_ctas\":\"1\",\n         }\n       }\n     }\n-  rhs = $rhs_type parameter(2)\n-  rhs1 = $rhs_type{1,0} fusion(rhs),\n+  lhs_scale = $lhs_scale_type parameter(2)\n+  lhs_scale1 = $lhs_scale_type{1,0} fusion(lhs_scale),\n     kind=kCustom,\n-    calls=frhs,\n+    calls=flhs_scale,\n     backend_config={\n       \"fusion_backend_config\":{\n         \"kind\":\"__triton_nested_gemm_fusion\",\n         \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"128\",\"256\"]}],\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n           \"num_warps\":\"4\",\n           \"num_stages\":\"1\",\n           \"num_ctas\":\"1\",\n@@ -4070,17 +4070,17 @@ triton_dot {\n         }\n       }\n     }\n-  ROOT _ = $output_type{1,0} scaled-dot(lhs1, lhs_scale1, rhs1, rhs_scale1),\n+  ROOT _ = $output_type{1,0} scaled-dot(lhs1, rhs1, lhs_scale1, rhs_scale1),\n     lhs_contracting_dims={1},\n     rhs_contracting_dims={0}\n }\n \n ENTRY e {\n-  p0 = $lhs_type{1,0} parameter(0)\n-  p1 = $lhs_scale_type{1,0} parameter(1)\n-  p2 = $rhs_type{1,0} parameter(2)\n-  p3 = $rhs_scale_type{1,0} parameter(3)\n-  ROOT _ = $output_type{1,0} fusion(p0, p1, p2, p3),\n+  lhs = $lhs_type{1,0} parameter(0)\n+  rhs = $rhs_type{1,0} parameter(1)\n+  lhs_scale = $lhs_scale_type{1,0} parameter(2)\n+  rhs_scale = $rhs_scale_type{1,0} parameter(3)\n+  ROOT _ = $output_type{1,0} fusion(lhs, rhs, lhs_scale, rhs_scale),\n     kind=kCustom,\n     calls=triton_dot,\n     backend_config={\n@@ -4138,10 +4138,10 @@ HloModule FP8ScaledDotGetsFused\n \n ENTRY e {\n   lhs = $lhs_type parameter(0)\n-  lhs_scale = $lhs_scale_type parameter(1)\n   rhs = $rhs_type parameter(2)\n+  lhs_scale = $lhs_scale_type parameter(1)\n   rhs_scale = $rhs_scale_type parameter(3)\n-  ROOT _ = $output_type{1,0} scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT _ = $output_type{1,0} scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1},\n     rhs_contracting_dims={0}\n }\n@@ -4167,12 +4167,11 @@ ENTRY e {\n INSTANTIATE_TEST_SUITE_P(\n     TritonScaledDotGemmTest, TritonScaledDotGemmTest,\n     ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[128,128]\",\n-                                         \"f8e8m0fnu[128,4]\",\n                                          \"f8e4m3fn[128,256]\",\n-                                         \"f8e8m0fnu[4,256]\", \"bf16[128,256]\",\n-                                         \"f8E4M3FN\"},\n-                      ScaleDotTestParams{\"f8e5m2[128,128]\", \"f8e8m0fnu[128,4]\",\n-                                         \"f8e5m2[128,256]\", \"f8e8m0fnu[4,256]\",\n+                                         \"f8e8m0fnu[128,4]\", \"f8e8m0fnu[4,256]\",\n+                                         \"bf16[128,256]\", \"f8E4M3FN\"},\n+                      ScaleDotTestParams{\"f8e5m2[128,128]\", \"f8e5m2[128,256]\",\n+                                         \"f8e8m0fnu[128,4]\", \"f8e8m0fnu[4,256]\",\n                                          \"bf16[128,256]\", \"f8E5M2\"}),\n     ScaleDotTestParams::ToString);\n \n@@ -4210,10 +4209,10 @@ HloModule ScaledDotWithBatchGetFusedAndExecutedCorrectly\n \n ENTRY e {\n   lhs = f8e4m3fn[3,128,128] parameter(0)\n-  lhs_scale = f8e8m0fnu[3,128,4] parameter(1)\n-  rhs = f8e4m3fn[3,128,128] parameter(2)\n+  rhs = f8e4m3fn[3,128,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[3,128,4] parameter(2)\n   rhs_scale = f8e8m0fnu[3,128,4 ] parameter(3)\n-  ROOT _ = bf16[3,128,128] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT _ = bf16[3,128,128] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_batch_dims={0},\n     rhs_batch_dims={0},\n     lhs_contracting_dims={2},"
        },
        {
            "sha": "f64bdf9c2bcf2131e567d675d40e79c2f712c092",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -2343,7 +2343,7 @@ ENTRY triton_computation {\n   lhs_scale = f8e8m0fnu[16, 1] parameter(1)\n   rhs = $0[32, 16] parameter(2)\n   rhs_scale = f8e8m0fnu[1, 16] parameter(3)\n-  ROOT dot = f32[16, 16] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT dot = f32[16, 16] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n       lhs_contracting_dims={1},\n       rhs_contracting_dims={0}\n }"
        },
        {
            "sha": "16ef2fbd837e5f33a7e94cabe5230f974b1bd9e8",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -441,8 +441,8 @@ HloInstructionIndexing ComputeOutputToInputDotOpIndexing(\n HloInstructionIndexing ComputeOutputToInputScaledDotOpIndexing(\n     const HloScaledDotInstruction* scaled_dot, MLIRContext* mlir_context) {\n   const Shape& lhs_shape = scaled_dot->operand(0)->shape();\n-  const Shape& lhs_scale_shape = scaled_dot->operand(1)->shape();\n-  const Shape& rhs_shape = scaled_dot->operand(2)->shape();\n+  const Shape& rhs_shape = scaled_dot->operand(1)->shape();\n+  const Shape& lhs_scale_shape = scaled_dot->operand(2)->shape();\n   const Shape& rhs_scale_shape = scaled_dot->operand(3)->shape();\n \n   auto [lhs_map, rhs_map] = ComputeDotOperandsIndexingImpl(\n@@ -455,7 +455,7 @@ HloInstructionIndexing ComputeOutputToInputScaledDotOpIndexing(\n       RescaleIndexingMap(rhs_map, rhs_shape, rhs_scale_shape);\n \n   return HloInstructionIndexing::FromIndexingMaps(\n-      {lhs_map, lhs_scale_map, rhs_map, rhs_scale_map});\n+      {lhs_map, rhs_map, lhs_scale_map, rhs_scale_map});\n }\n \n HloInstructionIndexing ComputeOutputToInputDynamicSliceOpIndexing("
        },
        {
            "sha": "e4a614c199644fc4d870a8a634f59d3c8b0509c9",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -2463,7 +2463,7 @@ TEST_F(IndexingAnalysisTest, ScaledDotOp) {\n       b = f32[10,2] parameter(1)\n       a_scale = f32[2,2] parameter(2)\n       b_scale = f32[2,2] parameter(3)\n-      ROOT dot = f32[2,2] scaled-dot(a, a_scale, b, b_scale),\n+      ROOT dot = f32[2,2] scaled-dot(a, b, a_scale, b_scale),\n         lhs_contracting_dims={1},\n         rhs_contracting_dims={0}\n     }\n@@ -2476,13 +2476,13 @@ TEST_F(IndexingAnalysisTest, ScaledDotOp) {\n       d1 in [0, 1],\n       s0 in [0, 9]\n     operand id = 1\n-      (d0, d1)[s0] -> (d0, s0 floordiv 5),\n+      (d0, d1)[s0] -> (s0, d1),\n       domain:\n       d0 in [0, 1],\n       d1 in [0, 1],\n       s0 in [0, 9]\n     operand id = 2\n-      (d0, d1)[s0] -> (s0, d1),\n+      (d0, d1)[s0] -> (d0, s0 floordiv 5),\n       domain:\n       d0 in [0, 1],\n       d1 in [0, 1],"
        },
        {
            "sha": "821faade208690c38ed2dfe81ed3602b0dd8e605",
            "filename": "third_party/xla/xla/hlo/builder/xla_builder.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -2116,7 +2116,7 @@ XlaOp XlaBuilder::DotGeneral(\n }\n \n XlaOp XlaBuilder::ScaledDot(\n-    XlaOp lhs, XlaOp lhs_scale, XlaOp rhs, XlaOp rhs_scale,\n+    XlaOp lhs, XlaOp rhs, XlaOp lhs_scale, XlaOp rhs_scale,\n     const DotDimensionNumbers& dimension_numbers,\n     const PrecisionConfig* precision_config,\n     std::optional<PrimitiveType> preferred_element_type) {\n@@ -2135,7 +2135,7 @@ XlaOp XlaBuilder::ScaledDot(\n       *instr.mutable_precision_config() = *precision_config;\n     }\n     return AddInstruction(std::move(instr), HloOpcode::kScaledDot,\n-                          {lhs, lhs_scale, rhs, rhs_scale});\n+                          {lhs, rhs, lhs_scale, rhs_scale});\n   });\n }\n \n@@ -2152,12 +2152,12 @@ absl::StatusOr<XlaOp> XlaBuilder::DotGeneralInternal(\n   return AddInstruction(std::move(instr), HloOpcode::kDot, {lhs, rhs});\n }\n \n-XlaOp ScaledDot(const XlaOp lhs, const XlaOp lhs_scale, const XlaOp rhs,\n+XlaOp ScaledDot(const XlaOp lhs, const XlaOp rhs, const XlaOp lhs_scale,\n                 const XlaOp rhs_scale,\n                 const DotDimensionNumbers& dimension_numbers,\n                 const PrecisionConfig* precision_config,\n                 std::optional<PrimitiveType> preferred_element_type) {\n-  return lhs.builder()->ScaledDot(lhs, lhs_scale, rhs, rhs_scale,\n+  return lhs.builder()->ScaledDot(lhs, rhs, lhs_scale, rhs_scale,\n                                   dimension_numbers, precision_config,\n                                   preferred_element_type);\n }"
        },
        {
            "sha": "66db2dd84bbc5417b6364c41caea2de6c416494b",
            "filename": "third_party/xla/xla/hlo/builder/xla_builder.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -709,7 +709,7 @@ class XlaBuilder {\n       std::optional<PrimitiveType> preferred_element_type = std::nullopt);\n \n   XlaOp ScaledDot(\n-      XlaOp lhs, XlaOp lhs_scale, XlaOp rhs, XlaOp rhs_scale,\n+      XlaOp lhs, XlaOp rhs, XlaOp lhs_scale, XlaOp rhs_scale,\n       const DotDimensionNumbers& dimension_number,\n       const PrecisionConfig* precision_config = nullptr,\n       std::optional<PrimitiveType> preferred_element_type = std::nullopt);\n@@ -1445,7 +1445,7 @@ class XlaBuilder {\n                          const RaggedDotDimensionNumbers& dimension_numbers,\n                          const PrecisionConfig* precision_config,\n                          std::optional<PrimitiveType> preferred_element_type);\n-  friend XlaOp ScaledDot(XlaOp lhs, XlaOp lhs_scale, XlaOp rhs, XlaOp rhs_scale,\n+  friend XlaOp ScaledDot(XlaOp lhs, XlaOp rhs, XlaOp lhs_scale, XlaOp rhs_scale,\n                          const DotDimensionNumbers& dimension_number,\n                          const PrecisionConfig* precision_config,\n                          std::optional<PrimitiveType> preferred_element_type);"
        },
        {
            "sha": "c8fc2f709fad60c9ede245a11e591a1fc6bcce7a",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1131,13 +1131,13 @@ absl::StatusOr<Literal> HloEvaluator::EvaluateDotOp(\n absl::StatusOr<Literal> HloEvaluator::EvaluateScaledDotOp(\n     const DotDimensionNumbers& dim_numbers,\n     const PrecisionConfig& precision_config, const Literal& lhs,\n-    const Literal& lhs_scale, const Literal& rhs, const Literal& rhs_scale) {\n+    const Literal& rhs, const Literal& lhs_scale, const Literal& rhs_scale) {\n   std::unique_ptr<HloInstruction> lhs_instr =\n       HloInstruction::CreateConstant(lhs.Clone());\n-  std::unique_ptr<HloInstruction> lhs_scale_instr =\n-      HloInstruction::CreateConstant(lhs_scale.Clone());\n   std::unique_ptr<HloInstruction> rhs_instr =\n       HloInstruction::CreateConstant(rhs.Clone());\n+  std::unique_ptr<HloInstruction> lhs_scale_instr =\n+      HloInstruction::CreateConstant(lhs_scale.Clone());\n   std::unique_ptr<HloInstruction> rhs_scale_instr =\n       HloInstruction::CreateConstant(rhs_scale.Clone());\n \n@@ -1148,7 +1148,7 @@ absl::StatusOr<Literal> HloEvaluator::EvaluateScaledDotOp(\n \n   std::unique_ptr<HloInstruction> cloned_instruction =\n       HloInstruction::CreateScaledDot(\n-          dot_shape, lhs_instr.get(), lhs_scale_instr.get(), rhs_instr.get(),\n+          dot_shape, lhs_instr.get(), rhs_instr.get(), lhs_scale_instr.get(),\n           rhs_scale_instr.get(), dim_numbers, precision_config);\n   return Evaluate(cloned_instruction.get());\n }"
        },
        {
            "sha": "dcc439ad7d18b3d2de2dff6b5fdff3452eb5b15f",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.h?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -196,7 +196,7 @@ class HloEvaluator : public ConstDfsHloVisitorWithDefault,\n   absl::StatusOr<Literal> EvaluateScaledDotOp(\n       const DotDimensionNumbers& dim_numbers,\n       const PrecisionConfig& precision_config, const Literal& lhs,\n-      const Literal& lhs_scale, const Literal& rhs, const Literal& rhs_scale);\n+      const Literal& rhs, const Literal& lhs_scale, const Literal& rhs_scale);\n \n   void set_dynamic_dimension_inference(\n       DynamicDimensionInference* dynamic_dimension_inference) override {"
        },
        {
            "sha": "ec0945eb6b937c4d2a9ed0ce8e808537fef1680e",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1986,7 +1986,7 @@ TEST_F(HloEvaluatorTest, ScaledDot) {\n   dot_dnums.add_lhs_contracting_dimensions(1);\n   dot_dnums.add_rhs_contracting_dimensions(0);\n   b.AddInstruction(HloInstruction::CreateScaledDot(\n-      shape, lhs_instr, lhs_scale_instr, rhs_instr, rhs_scale_instr, dot_dnums,\n+      shape, lhs_instr, rhs_instr, lhs_scale_instr, rhs_scale_instr, dot_dnums,\n       DefaultPrecisionConfig(4)));\n   m_->AddEntryComputation(b.Build());\n \n@@ -2031,7 +2031,7 @@ TEST_F(HloEvaluatorTest, ScaledDotWithOneMissingScale) {\n   dot_dnums.add_lhs_contracting_dimensions(1);\n   dot_dnums.add_rhs_contracting_dimensions(0);\n   b.AddInstruction(HloInstruction::CreateScaledDot(\n-      shape, lhs_instr, lhs_scale_instr, rhs_instr, rhs_scale_instr, dot_dnums,\n+      shape, lhs_instr, rhs_instr, lhs_scale_instr, rhs_scale_instr, dot_dnums,\n       DefaultPrecisionConfig(4)));\n   m_->AddEntryComputation(b.Build());\n \n@@ -2079,7 +2079,7 @@ TEST_F(HloEvaluatorTest, ScaledDotWithBatchDim) {\n   dot_dnums.add_lhs_contracting_dimensions(2);\n   dot_dnums.add_rhs_contracting_dimensions(1);\n   b.AddInstruction(HloInstruction::CreateScaledDot(\n-      shape, lhs_instr, lhs_scale_instr, rhs_instr, rhs_scale_instr, dot_dnums,\n+      shape, lhs_instr, rhs_instr, lhs_scale_instr, rhs_scale_instr, dot_dnums,\n       DefaultPrecisionConfig(4)));\n   m_->AddEntryComputation(b.Build());\n "
        },
        {
            "sha": "19f72d61ee3be7946747a09764048597d2f59dd7",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator_typed_visitor.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1737,8 +1737,8 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n \n   absl::Status HandleScaledDot(const HloInstruction* dot) override {\n     auto lhs = dot->operand(0);\n-    auto lhs_scale = dot->operand(1);\n-    auto rhs = dot->operand(2);\n+    auto rhs = dot->operand(1);\n+    auto lhs_scale = dot->operand(2);\n     auto rhs_scale = dot->operand(3);\n     CHECK(dot->shape().IsArray());\n     CHECK(lhs->shape().IsArray());\n@@ -1781,7 +1781,7 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n     TF_ASSIGN_OR_RETURN(Literal rhs_scale_literal,\n                         evaluate_scale(rhs, rhs_scale));\n     return HandleScaledDotSlowPathWithLiterals(\n-        dot, lhs_literal, lhs_scale_literal, rhs_literal, rhs_scale_literal);\n+        dot, lhs_literal, rhs_literal, lhs_scale_literal, rhs_scale_literal);\n   }\n \n  private:\n@@ -1837,7 +1837,7 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n \n   absl::Status HandleScaledDotSlowPathWithLiterals(\n       const HloInstruction* dot, const Literal& lhs_literal,\n-      const Literal& lhs_scale_literal, const Literal& rhs_literal,\n+      const Literal& rhs_literal, const Literal& lhs_scale_literal,\n       const Literal& rhs_scale_literal) {\n     const auto& dnums = dot->dot_dimension_numbers();\n     CHECK(ShapeUtil::SameElementType(lhs_literal.shape(), rhs_literal.shape()));"
        },
        {
            "sha": "1c59e599c569bcbcb805173f9259bf141ab5287e",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1673,11 +1673,11 @@ HloInstruction::CreateTriangularSolve(const Shape& shape, HloInstruction* a,\n }\n \n /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateScaledDot(\n-    const Shape& shape, HloInstruction* lhs, HloInstruction* lhs_scale,\n-    HloInstruction* rhs, HloInstruction* rhs_scale,\n+    const Shape& shape, HloInstruction* lhs, HloInstruction* rhs,\n+    HloInstruction* lhs_scale, HloInstruction* rhs_scale,\n     const DotDimensionNumbers& dimension_numbers,\n     const PrecisionConfig& precision_config) {\n-  return std::make_unique<HloScaledDotInstruction>(shape, lhs, lhs_scale, rhs,\n+  return std::make_unique<HloScaledDotInstruction>(shape, lhs, rhs, lhs_scale,\n                                                    rhs_scale, dimension_numbers,\n                                                    precision_config);\n }"
        },
        {
            "sha": "039eebda3acc319bcf07c076b6ed2fa8483eebdd",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -476,8 +476,8 @@ class alignas(kInstructionTypeMask + 1) HloInstruction {\n   // 'rhs_scale', with contracting, batch, ragged, and group dimensions\n   // specified in 'dimension_numbers'.\n   static std::unique_ptr<HloInstruction> CreateScaledDot(\n-      const Shape& shape, HloInstruction* lhs, HloInstruction* lhs_scale,\n-      HloInstruction* rhs, HloInstruction* rhs_scale,\n+      const Shape& shape, HloInstruction* lhs, HloInstruction* rhs,\n+      HloInstruction* lhs_scale, HloInstruction* rhs_scale,\n       const DotDimensionNumbers& dimension_numbers,\n       const PrecisionConfig& precision_config);\n "
        },
        {
            "sha": "f62516fc76e5556bdec75766bfda45ce8538ad03",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -3945,16 +3945,16 @@ HloRaggedDotInstruction::CloneWithNewOperandsImpl(\n }\n \n HloScaledDotInstruction::HloScaledDotInstruction(\n-    const Shape& shape, HloInstruction* lhs, HloInstruction* lhs_scale,\n-    HloInstruction* rhs, HloInstruction* rhs_scale,\n+    const Shape& shape, HloInstruction* lhs, HloInstruction* rhs,\n+    HloInstruction* lhs_scale, HloInstruction* rhs_scale,\n     const DotDimensionNumbers& dimension_numbers,\n     const PrecisionConfig& precision_config)\n     : HloInstruction(HloOpcode::kScaledDot, shape),\n       dot_dimension_numbers_(dimension_numbers),\n       precision_config_(precision_config) {\n   AppendOperand(lhs);\n-  AppendOperand(lhs_scale);\n   AppendOperand(rhs);\n+  AppendOperand(lhs_scale);\n   AppendOperand(rhs_scale);\n }\n "
        },
        {
            "sha": "ab7ec582626278de6fa1e9590ae6d313ed40d481",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -2716,8 +2716,8 @@ class HloScaledDotInstruction : public HloInstruction {\n   // 'rhs_scale' as the scale factors. Dimensions of the scale factors should\n   // have the same order as the dimensions of the dot operation.\n   explicit HloScaledDotInstruction(const Shape& shape, HloInstruction* lhs,\n-                                   HloInstruction* lhs_scale,\n                                    HloInstruction* rhs,\n+                                   HloInstruction* lhs_scale,\n                                    HloInstruction* rhs_scale,\n                                    const DotDimensionNumbers& dimension_numbers,\n                                    const PrecisionConfig& precision_config);"
        },
        {
            "sha": "e7a6a35b095e5f4dc5389029e0c8a35200288dcf",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1747,10 +1747,10 @@ TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsAreGenerated) {\n   std::unique_ptr<VerifiedHloModule> module = ParseAndReturnVerifiedModule(R\"(\n     ENTRY e {\n       p0 = f32[1024,1024] parameter(0)\n-      p0_scale = f32[1024,8] parameter(1)\n-      p1 = f32[1024,1024] parameter(2)\n+      p1 = f32[1024,1024] parameter(1)\n+      p0_scale = f32[1024,8] parameter(2)\n       p1_scale = f32[8,1024] parameter(3)\n-      ROOT r = f32[1024,1024] scaled-dot(p0, p0_scale, p1, p1_scale),\n+      ROOT r = f32[1024,1024] scaled-dot(p0, p1, p0_scale, p1_scale),\n         lhs_contracting_dims={1}, rhs_contracting_dims={0}\n     })\")\n                                                   .value();\n@@ -1777,19 +1777,19 @@ TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsHaveCuBlasFallback) {\n \n     fusion_computation {\n       p0 = f32[1024,1024] parameter(0)\n-      p0_scale = f32[1024,8] parameter(1)\n-      p1 = f32[1024,1024] parameter(2)\n+      p1 = f32[1024,1024] parameter(1)\n+      p0_scale = f32[1024,8] parameter(2)\n       p1_scale = f32[8,1024] parameter(3)\n-      ROOT r = f32[1024,1024] scaled-dot(p0, p0_scale, p1, p1_scale),\n+      ROOT r = f32[1024,1024] scaled-dot(p0, p1, p0_scale, p1_scale),\n         lhs_contracting_dims={1}, rhs_contracting_dims={0}\n     }\n \n     ENTRY e {\n       p0 = f32[1024,1024] parameter(0)\n-      p0_scale = f32[1024,8] parameter(1)\n-      p1 = f32[1024,1024] parameter(2)\n+      p1 = f32[1024,1024] parameter(1)\n+      p0_scale = f32[1024,8] parameter(2)\n       p1_scale = f32[8,1024] parameter(3)\n-      ROOT r = f32[1024,1024] fusion(p0, p0_scale, p1, p1_scale),\n+      ROOT r = f32[1024,1024] fusion(p0, p1, p0_scale, p1_scale),\n         kind=kCustom, calls=fusion_computation\n     })\")\n                                                   .value();"
        },
        {
            "sha": "f402003bb36946d7eec9febfab856a2425d54917",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -437,10 +437,10 @@ TEST_F(FloatSupportTest, ScaledDotIsIgnored) {\n \n     ENTRY main {\n       lhs = bf16[1024, 1024] parameter(0)\n-      lhs_scale = bf16[1, 1] parameter(1)\n-      rhs = bf16[1024, 1024] parameter(2)\n+      rhs = bf16[1024, 1024] parameter(1)\n+      lhs_scale = bf16[1, 1] parameter(2)\n       rhs_scale = bf16[1, 1] parameter(3)\n-      ROOT r = bf16[1024, 1024] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+      ROOT r = bf16[1024, 1024] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n          lhs_contracting_dims={1},\n          rhs_contracting_dims={1}\n     }"
        },
        {
            "sha": "bf3f7dc0fd0ad5a9ff9aa406aa5a89fff32bd76f",
            "filename": "third_party/xla/xla/service/gpu/model/symbolic_tile_analysis.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsymbolic_tile_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsymbolic_tile_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsymbolic_tile_analysis.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1054,7 +1054,7 @@ IndexingMap InsertTilingParameterForContractingDimensions(\n     if (consumer->opcode() == HloOpcode::kScaledDot) {\n       CHECK(operand_index >= 0 && operand_index <= 3);\n       contracting_dimensions =\n-          operand_index <= 1\n+          (operand_index == 0 || operand_index == 2)\n               ? consumer->dot_dimension_numbers().lhs_contracting_dimensions()\n               : consumer->dot_dimension_numbers().rhs_contracting_dimensions();\n     }"
        },
        {
            "sha": "6e0dd2abe30884a83ecaccc8b836fda755b28e75",
            "filename": "third_party/xla/xla/service/gpu/model/symbolic_tile_analysis_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsymbolic_tile_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsymbolic_tile_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsymbolic_tile_analysis_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -858,19 +858,20 @@ TEST_F(SymbolicTileAnalysisTest, ScaledDotOffsetIndexingIsCorrect) {\n                           ParseAndReturnVerifiedModule(R\"(\n fusion {\n   lhs = f8e4m3fn[128,64] parameter(0)\n-  lhs_scale = f8e8m0fnu[128,2] parameter(1)\n-  rhs = f8e4m3fn[64,128] parameter(2)\n+  rhs = f8e4m3fn[64,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[128,2] parameter(2)\n   rhs_scale = f8e8m0fnu[2,128] parameter(3)\n-  ROOT dot = f32[128,128] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT dot = f32[128,128] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n ENTRY main {\n-  p0 = f8e4m3fn[128,64] parameter(0)\n-  p1 = f8e8m0fnu[128,2] parameter(1)\n-  p2 = f8e4m3fn[64,128] parameter(2)\n-  p3 = f8e8m0fnu[2,128] parameter(3)\n-  ROOT fusion = f32[128,128] fusion(p0, p1, p2, p3), kind=kLoop, calls=fusion\n+  lhs = f8e4m3fn[128,64] parameter(0)\n+  rhs = f8e4m3fn[64,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[128,2] parameter(2)\n+  rhs_scale = f8e8m0fnu[2,128] parameter(3)\n+  ROOT fusion = f32[128,128] fusion(lhs, rhs, lhs_scale, rhs_scale),\n+    kind=kLoop, calls=fusion\n })\"));\n   std::optional<SymbolicTileAnalysis> analysis = TryAnalyzeModule(module.get());\n   ASSERT_TRUE(analysis.has_value());\n@@ -905,7 +906,7 @@ ENTRY main {\n     pid_0 in [0, 63]\n   )\"));\n \n-  const TiledHloInstruction* rhs = dot->operand(2);\n+  const TiledHloInstruction* rhs = dot->operand(1);\n   EXPECT_THAT(*rhs, MatchTiledHloInstruction(\n                         /*tile_sizes=*/{32, 16}, /*tile_strides=*/{1, 1},\n                         /*tile_offsets_indexing=*/R\"("
        },
        {
            "sha": "db066543a6613340e431d040e6ca79b239b2cff7",
            "filename": "third_party/xla/xla/service/gpu/split_k_gemm_rewriter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -327,12 +327,12 @@ absl::Status MakeDotComputationSplitKBatch(HloComputation* computation,\n                           block_size * config.split_k;\n           return scale;\n         };\n-        HloInstruction* lhs_scale = dot->mutable_operand(1);\n+        HloInstruction* lhs_scale = dot->mutable_operand(2);\n         if (analysis.lhs_scale_block_size().has_value()) {\n           TF_ASSIGN_OR_RETURN(\n               lhs_scale,\n               assign_scale_operand(TritonFusionAnalysis::Scope::LHS_SCALE,\n-                                   lhs_contracting_idx, 1,\n+                                   lhs_contracting_idx, 2,\n                                    *analysis.lhs_scale_block_size()));\n         }\n         HloInstruction* rhs_scale = dot->mutable_operand(3);\n@@ -365,11 +365,11 @@ absl::Status MakeDotComputationSplitKBatch(HloComputation* computation,\n         TF_ASSIGN_OR_RETURN(\n             HloInstruction * rhs,\n             MakeSplitKOperand(*dot, analysis, config, rhs_contracting_idx,\n-                              TritonFusionAnalysis::Scope::RHS, 2,\n+                              TritonFusionAnalysis::Scope::RHS, 1,\n                               padded_k_size));\n         TF_ASSIGN_OR_RETURN(\n             expanded,\n-            MakeScaledDotHlo(lhs, lhs_scale, rhs, rhs_scale, new_dim_numbers,\n+            MakeScaledDotHlo(lhs, rhs, lhs_scale, rhs_scale, new_dim_numbers,\n                              dot->precision_config(), accumulator_dtype));\n       }\n       // Make the added batch dimension the major-most, keep the order of the"
        },
        {
            "sha": "c98b92322e17016bf6ab0168595964dcb8e98dd6",
            "filename": "third_party/xla/xla/service/gpu/split_k_gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 36,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -824,19 +824,19 @@ TEST_F(SplitKTest, ScaledDot_SameBlockSize) {\n   const std::string hlo_text = R\"(\n triton_gemm_dot {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-  rhs = f8e5m2[32,128] parameter(2)\n+  rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(2)\n   rhs_scale = f8e8m0fnu[32,4] parameter(3)\n-  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n ENTRY %entry_computation {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-  rhs = f8e5m2[32,128] parameter(2)\n+  rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(2)\n   rhs_scale = f8e8m0fnu[32,4] parameter(3)\n-  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale, rhs_scale),\n       kind=kCustom, calls=triton_gemm_dot\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n@@ -852,28 +852,28 @@ ENTRY %entry_computation {\n   EXPECT_THAT(\n       dot_fusion->called_computations()[0]->root_instruction(),\n       GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n                               m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n                               m::Op().WithShape(F8E8M0FNU, {32, 3, 2}))));\n }\n \n TEST_F(SplitKTest, ScaledDot_DifferentBlockSize) {\n   const std::string hlo_text = R\"(\n triton_gemm_dot {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-  rhs = f8e5m2[32,128] parameter(2)\n+  rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(2)\n   rhs_scale = f8e8m0fnu[32,8] parameter(3)\n-  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n ENTRY %entry_computation {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-  rhs = f8e5m2[32,128] parameter(2)\n+  rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(2)\n   rhs_scale = f8e8m0fnu[32,8] parameter(3)\n-  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale, rhs_scale),\n       kind=kCustom, calls=triton_gemm_dot\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n@@ -889,27 +889,27 @@ ENTRY %entry_computation {\n   EXPECT_THAT(\n       dot_fusion->called_computations()[0]->root_instruction(),\n       GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n                               m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n                               m::Op().WithShape(F8E8M0FNU, {32, 3, 4}))));\n }\n \n TEST_F(SplitKTest, ScaledDot_LhsOnly) {\n   const std::string hlo_text = R\"(\n triton_gemm_dot {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-  rhs = f8e5m2[32,128] parameter(2)\n+  rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(2)\n   rhs_scale = f8e5m2[] constant(1.0)\n-  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n ENTRY %entry_computation {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-  rhs = f8e5m2[32,128] parameter(2)\n-  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs),\n+  rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(2)\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale),\n       kind=kCustom, calls=triton_gemm_dot\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n@@ -924,19 +924,19 @@ ENTRY %entry_computation {\n       GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n   EXPECT_THAT(dot_fusion->called_computations()[0]->root_instruction(),\n               GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                                      m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n                                       m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                                      m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n                                       m::Op().WithShape(F8E5M2, {}))));\n }\n \n TEST_F(SplitKTest, ScaledDot_RhsOnly) {\n   const std::string hlo_text = R\"(\n triton_gemm_dot {\n   lhs = f8e4m3fn[16,128] parameter(0)\n-  lhs_scale = f8e4m3fn[] constant(1.0)\n   rhs = f8e5m2[32,128] parameter(1)\n+  lhs_scale = f8e4m3fn[] constant(1.0)\n   rhs_scale = f8e8m0fnu[32,4] parameter(2)\n-  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n@@ -960,28 +960,28 @@ ENTRY %entry_computation {\n   EXPECT_THAT(\n       dot_fusion->called_computations()[0]->root_instruction(),\n       GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                              m::Op().WithShape(F8E4M3FN, {}),\n                               m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                              m::Op().WithShape(F8E4M3FN, {}),\n                               m::Op().WithShape(F8E8M0FNU, {32, 3, 2}))));\n }\n \n TEST_F(SplitKTest, ScaledDot_IncompatibleBlockSize) {\n   const std::string hlo_text = R\"(\n triton_gemm_dot {\n   lhs = f8e4m3fn[16,35] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,7] parameter(1)\n-  rhs = f8e5m2[32,35] parameter(2)\n+  rhs = f8e5m2[32,35] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,7] parameter(2)\n   rhs_scale = f8e8m0fnu[32,5] parameter(3)\n-  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n     lhs_contracting_dims={1}, rhs_contracting_dims={1}\n }\n \n ENTRY %entry_computation {\n   lhs = f8e4m3fn[16,35] parameter(0)\n-  lhs_scale = f8e8m0fnu[16,7] parameter(1)\n-  rhs = f8e5m2[32,35] parameter(2)\n+  rhs = f8e5m2[32,35] parameter(1)\n+  lhs_scale = f8e8m0fnu[16,7] parameter(2)\n   rhs_scale = f8e8m0fnu[32,5] parameter(3)\n-  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale, rhs_scale),\n       kind=kCustom, calls=triton_gemm_dot\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n@@ -997,19 +997,19 @@ TEST_F(SplitKTest, ScaledDot_SmallDimension) {\n   const std::string hlo_text = R\"(\n     triton_gemm_dot {\n       lhs = f8e4m3fn[16,128] parameter(0)\n-      lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-      rhs = f8e5m2[32,128] parameter(2)\n+      rhs = f8e5m2[32,128] parameter(1)\n+      lhs_scale = f8e8m0fnu[16,4] parameter(2)\n       rhs_scale = f8e8m0fnu[32,4] parameter(3)\n-      ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+      ROOT dot = f32[16,32] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n         lhs_contracting_dims={1}, rhs_contracting_dims={1}\n     }\n \n     ENTRY %entry_computation {\n       lhs = f8e4m3fn[16,128] parameter(0)\n-      lhs_scale = f8e8m0fnu[16,4] parameter(1)\n-      rhs = f8e5m2[32,128] parameter(2)\n+      rhs = f8e5m2[32,128] parameter(1)\n+      lhs_scale = f8e8m0fnu[16,4] parameter(2)\n       rhs_scale = f8e8m0fnu[32,4] parameter(3)\n-      ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+      ROOT fusion = f32[16,32] fusion(lhs, rhs, lhs_scale, rhs_scale),\n           kind=kCustom, calls=triton_gemm_dot\n     })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,"
        },
        {
            "sha": "05c618bda33341f0ac0ea83aaa3e4aa6aba7dd7d",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fusion_compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -598,20 +598,20 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n       const auto& dimension_numbers = hlo->dot_dimension_numbers();\n       std::array<std::shared_ptr<graph::Tensor_attributes>, 2> dot_operands;\n       for (int i = 0; i < 2; ++i) {\n-        const Shape& input_shape = hlo->operand(i * 2)->shape();\n-        const Shape& scale_shape = hlo->operand(i * 2 + 1)->shape();\n+        const Shape& input_shape = hlo->operand(i)->shape();\n+        const Shape& scale_shape = hlo->operand(i + 2)->shape();\n         int dim = i == 0 ? dimension_numbers.lhs_contracting_dimensions(0)\n                          : dimension_numbers.rhs_contracting_dimensions(0);\n         int block_size =\n             input_shape.dimensions(dim) / scale_shape.dimensions(dim);\n \n-        auto scale = operand(i * 2 + 1);\n+        auto scale = operand(i + 2);\n         scale->set_reordering_type(fe::TensorReordering_t::F8_128x4);\n         auto dq_attrs = graph::Block_scale_dequantize_attributes()\n                             .set_block_size(block_size)\n                             .set_compute_data_type(fe::DataType_t::FLOAT);\n         dot_operands[i] =\n-            graph.block_scale_dequantize(operand(i * 2), scale, dq_attrs);\n+            graph.block_scale_dequantize(operand(i), scale, dq_attrs);\n         dot_operands[i]->set_name(\n             absl::StrCat(hlo->name(), i == 0 ? \"_lhs\" : \"_rhs\", \"_dq\"));\n       }"
        },
        {
            "sha": "47b688a30b62c5675e2a9f15ec6ba53df5dbf33c",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -868,8 +868,8 @@ class GemmFusionVisitor : public DfsHloRewriteVisitor {\n     };\n     std::vector<HloInstruction*> new_operands{\n         create_parameter(0, \"lhs\"),\n-        create_parameter(1, \"lhs_scale\"),\n-        create_parameter(2, \"rhs\"),\n+        create_parameter(1, \"rhs\"),\n+        create_parameter(2, \"lhs_scale\"),\n         create_parameter(3, \"rhs_scale\"),\n     };\n     builder.AddInstruction("
        },
        {
            "sha": "6bd7dbded1fa84963914f7d1040b8f927882d571",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -1485,10 +1485,10 @@ TEST_F(GemmFusionTest, ScaledDotIsFused) {\n \n     ENTRY entry {\n      lhs = bf16[4,4] parameter(0)\n-     lhs_scale = bf16[1,1] parameter(1)\n-     rhs = bf16[4,4] parameter(2)\n+     rhs = bf16[4,4] parameter(1)\n+     lhs_scale = bf16[1,1] parameter(2)\n      rhs_scale = bf16[1,1] parameter(3)\n-     ROOT dot = bf16[4,4] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+     ROOT dot = bf16[4,4] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n          lhs_contracting_dims={1},\n          rhs_contracting_dims={1},\n          metadata={op_name=\"foo\"}\n@@ -1503,11 +1503,11 @@ TEST_F(GemmFusionTest, ScaledDotIsFused) {\n   constexpr absl::string_view kExpectedHloText = R\"(\n     CHECK: %[[FUSION_DOT:.*]] (\n     CHECK:   %[[LHS:.*]] = bf16[4,4]{1,0} parameter(0)\n-    CHECK:   %[[LHS_SCALE:.*]] = bf16[1,1]{1,0} parameter(1)\n-    CHECK:   %[[RHS:.*]] = bf16[4,4]{1,0} parameter(2)\n+    CHECK:   %[[RHS:.*]] = bf16[4,4]{1,0} parameter(1)\n+    CHECK:   %[[LHS_SCALE:.*]] = bf16[1,1]{1,0} parameter(2)\n     CHECK:   %[[RHS_SCALE:.*]] = bf16[1,1]{1,0} parameter(3)\n     CHECK:   ROOT %dot.1 = bf16[4,4]{1,0} scaled-dot(\n-    CHECK:       %[[LHS]], %[[LHS_SCALE]], %[[RHS]], %[[RHS_SCALE]]),\n+    CHECK:       %[[LHS]], %[[RHS]], %[[LHS_SCALE]], %[[RHS_SCALE]]),\n     CHECK:     lhs_contracting_dims={1},\n     CHECK:     rhs_contracting_dims={1},\n     CHECK:     metadata={op_name=\"foo\"}"
        },
        {
            "sha": "993ae4043b977a9294bebbebe1a169415a41b6c2",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -270,8 +270,8 @@ absl::Status MakeNestedFusionFromGemmFusion(\n     const se::DeviceDescription& device_description) {\n   TF_RETURN_IF_ERROR(IsDot(*dot));\n   const bool is_scaled_dot = dot->opcode() == HloOpcode::kScaledDot;\n-  const int lhs = 0;\n-  const int rhs = is_scaled_dot ? 2 : 1;\n+  constexpr int lhs = 0;\n+  constexpr int rhs = 1;\n   TF_ASSIGN_OR_RETURN(TritonGemmConfig config, GetTritonGemmConfig(*fusion));\n   HloComputation* computation = fusion->called_computation();\n \n@@ -294,7 +294,7 @@ absl::Status MakeNestedFusionFromGemmFusion(\n       config));\n \n   if (is_scaled_dot) {\n-    constexpr int kLhsScale = 1;\n+    constexpr int kLhsScale = 2;\n     constexpr int kRhsScale = 3;\n     constexpr int kContractingScaleFactor = 32;\n     auto scale_config = config;"
        },
        {
            "sha": "324cf215361b91e1a98671b99feb4b7674fc0491",
            "filename": "third_party/xla/xla/service/gpu/transforms/scaled_dot_rewriter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fscaled_dot_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fscaled_dot_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fscaled_dot_rewriter.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -171,8 +171,8 @@ absl::StatusOr<bool> ScaledDotRewriter::RewriteComputation(\n     }\n     changed = true;\n     HloScaledDotInstruction* dot = Cast<HloScaledDotInstruction>(instruction);\n-    TF_ASSIGN_OR_RETURN(HloInstruction * lhs, Dequantize(dot, 0, 1, \"LHS\"));\n-    TF_ASSIGN_OR_RETURN(HloInstruction * rhs, Dequantize(dot, 2, 3, \"RHS\"));\n+    TF_ASSIGN_OR_RETURN(HloInstruction * lhs, Dequantize(dot, 0, 2, \"LHS\"));\n+    TF_ASSIGN_OR_RETURN(HloInstruction * rhs, Dequantize(dot, 1, 3, \"RHS\"));\n \n     TF_RETURN_IF_ERROR(dot->ReplaceAllUsesWith(\n         computation->AddInstruction(HloInstruction::CreateDot("
        },
        {
            "sha": "1bc4f6c0589c8b33aa93ea957ed620f76e1023e2",
            "filename": "third_party/xla/xla/service/gpu/transforms/scaled_dot_rewriter_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fscaled_dot_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fscaled_dot_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fscaled_dot_rewriter_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -58,10 +58,10 @@ TEST_P(ScaledDotRewriterTestFixture, ScaledDot) {\n \n         ENTRY main {\n           lhs = $0[1024,512] parameter(0)\n-          lhs_scale = $1[32,2] parameter(1)\n-          rhs = $0[64,512] parameter(2)\n+          rhs = $0[64,512] parameter(1)\n+          lhs_scale = $1[32,2] parameter(2)\n           rhs_scale = $1[64,2] parameter(3)\n-          ROOT dot = f32[1024,64] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+          ROOT dot = f32[1024,64] scaled-dot(lhs, rhs, lhs_scale, rhs_scale),\n             lhs_contracting_dims={1},\n             rhs_contracting_dims={1}\n         }"
        },
        {
            "sha": "cfc47a333955d02ca8e9125d25604794927e6876",
            "filename": "third_party/xla/xla/service/gpu/triton_fusion_analysis.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -191,9 +191,9 @@ absl::StatusOr<std::optional<int64_t>> GetBlockSize(\n   CHECK(dot.opcode() == HloOpcode::kScaledDot);\n   CHECK(scope == TritonFusionAnalysis::Scope::LHS ||\n         scope == TritonFusionAnalysis::Scope::RHS);\n-  int operand_number = scope == TritonFusionAnalysis::Scope::LHS ? 0 : 2;\n+  int operand_number = scope == TritonFusionAnalysis::Scope::LHS ? 0 : 1;\n   const Shape& input = dot.operand(operand_number)->shape();\n-  const Shape& scale = dot.operand(operand_number + 1)->shape();\n+  const Shape& scale = dot.operand(operand_number + 2)->shape();\n \n   if (!ShapeUtil::IsScalar(scale)) {\n     TF_ASSIGN_OR_RETURN(int dim_idx,\n@@ -211,9 +211,9 @@ int ScopeToScaledDotOperandIdx(TritonFusionAnalysis::Scope scope) {\n   switch (scope) {\n     case TritonFusionAnalysis::Scope::LHS:\n       return 0;\n-    case TritonFusionAnalysis::Scope::LHS_SCALE:\n-      return 1;\n     case TritonFusionAnalysis::Scope::RHS:\n+      return 1;\n+    case TritonFusionAnalysis::Scope::LHS_SCALE:\n       return 2;\n     case TritonFusionAnalysis::Scope::RHS_SCALE:\n       return 3;\n@@ -289,7 +289,7 @@ absl::Status TritonFusionAnalysis::ExecuteForDotFusion(\n       continue;  // Scale operands are optional.\n     }\n     if (is_scaled_dot_) {\n-      // Operands for scaled dot: (lhs, lhs_scale, rhs, rhs_scale)\n+      // Operands for scaled dot: (lhs, rhs, lhs_scale, rhs_scale)\n       operand_number = ScopeToScaledDotOperandIdx(scope);\n       // Scalar scales are skipped.\n       if ((scope == Scope::LHS_SCALE || scope == Scope::RHS_SCALE) &&"
        },
        {
            "sha": "a049fb92417a5cc226c9d611814772b00feb78e2",
            "filename": "third_party/xla/xla/service/gpu/triton_fusion_analysis_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -926,25 +926,25 @@ HloModule test\n \n scaled_dot {\n   %lhs = f32[4,128,1024] parameter(0)\n-  %lhs_scale = f32[4,128,32] parameter(1)\n-  %rhs = f32[4,1024,256] parameter(2)\n+  %rhs = f32[4,1024,256] parameter(1)\n+  %lhs_scale = f32[4,128,32] parameter(2)\n   %rhs_scale = f32[4,32,256] parameter(3)\n-  ROOT %dot = f32[4,128,256] scaled-dot(%lhs, %lhs_scale, %rhs, %rhs_scale),\n+  ROOT %dot = f32[4,128,256] scaled-dot(%lhs, %rhs, %lhs_scale, %rhs_scale),\n       lhs_batch_dims={0}, lhs_contracting_dims={2},\n       rhs_batch_dims={0}, rhs_contracting_dims={1}\n })\"));\n   const HloComputation* dot_computation = *module->computations().begin();\n   TF_ASSERT_OK_AND_ASSIGN(const auto analysis,\n                           TritonFusionAnalysis::Execute(*dot_computation));\n   const HloInstruction* lhs = dot_computation->parameter_instruction(0);\n-  const HloInstruction* lhs_scale = dot_computation->parameter_instruction(1);\n-  const HloInstruction* rhs = dot_computation->parameter_instruction(2);\n+  const HloInstruction* rhs = dot_computation->parameter_instruction(1);\n+  const HloInstruction* lhs_scale = dot_computation->parameter_instruction(2);\n   const HloInstruction* rhs_scale = dot_computation->parameter_instruction(3);\n \n   using Scope = TritonFusionAnalysis::Scope;\n   EXPECT_EQ(*analysis.ScopeParameters(Scope::LHS).begin(), lhs);\n-  EXPECT_EQ(*analysis.ScopeParameters(Scope::LHS_SCALE).begin(), lhs_scale);\n   EXPECT_EQ(*analysis.ScopeParameters(Scope::RHS).begin(), rhs);\n+  EXPECT_EQ(*analysis.ScopeParameters(Scope::LHS_SCALE).begin(), lhs_scale);\n   EXPECT_EQ(*analysis.ScopeParameters(Scope::RHS_SCALE).begin(), rhs_scale);\n   for (const auto& hlo : dot_computation->instructions()) {\n     EXPECT_TRUE(analysis.QueryInstructionScope(*hlo).has_value());"
        },
        {
            "sha": "2d427ef4569cc09d576f34efcef3650c7a65aa1f",
            "filename": "third_party/xla/xla/service/hlo_creation_utils.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -407,7 +407,7 @@ absl::StatusOr<HloInstruction*> MakeRaggedDotHlo(\n }\n \n absl::StatusOr<HloInstruction*> MakeScaledDotHlo(\n-    HloInstruction* lhs, HloInstruction* lhs_scale, HloInstruction* rhs,\n+    HloInstruction* lhs, HloInstruction* rhs, HloInstruction* lhs_scale,\n     HloInstruction* rhs_scale, const DotDimensionNumbers& dim_numbers,\n     const PrecisionConfig& precision_config,\n     std::optional<PrimitiveType> preferred_element_type) {\n@@ -420,7 +420,7 @@ absl::StatusOr<HloInstruction*> MakeScaledDotHlo(\n       ShapeInference::InferDotOpShape(lhs->shape(), rhs->shape(), dim_numbers,\n                                       preferred_element_type));\n   return computation->AddInstruction(\n-      HloInstruction::CreateScaledDot(dot_shape, lhs, lhs_scale, rhs, rhs_scale,\n+      HloInstruction::CreateScaledDot(dot_shape, lhs, rhs, lhs_scale, rhs_scale,\n                                       dim_numbers, precision_config));\n }\n "
        },
        {
            "sha": "d22d0affbf181ab327a6bbb09a77901be1876cbb",
            "filename": "third_party/xla/xla/service/hlo_creation_utils.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.h?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -196,7 +196,7 @@ absl::StatusOr<HloInstruction*> MakeRaggedDotHlo(\n // computation). An optional preferred_element_type can be specified to override\n // the element type.\n absl::StatusOr<HloInstruction*> MakeScaledDotHlo(\n-    HloInstruction* lhs, HloInstruction* lhs_scale, HloInstruction* rhs,\n+    HloInstruction* lhs, HloInstruction* rhs, HloInstruction* lhs_scale,\n     HloInstruction* rhs_scale, const DotDimensionNumbers& dim_numbers,\n     const PrecisionConfig& precision_config,\n     std::optional<PrimitiveType> preferred_element_type);"
        },
        {
            "sha": "49b8ca175888202867311ff57cf749f00a5ee0ae",
            "filename": "third_party/xla/xla/service/hlo_verifier.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -330,9 +330,9 @@ absl::Status ShapeVerifier::HandleScaledDot(HloInstruction* scaled_dot) {\n \n   TF_ASSIGN_OR_RETURN(auto dim_numbers,\n                       DotOperandDims::FromScaledDot(scaled_dot));\n-  TF_RETURN_IF_ERROR(ScalesShapeVerifier(scaled_dot, dim_numbers, 0, 1));\n-  TF_RETURN_IF_ERROR(ScalesShapeVerifier(scaled_dot, dim_numbers, 2, 3));\n-  if (ShapeUtil::IsScalar(scaled_dot->operand(1)->shape()) &&\n+  TF_RETURN_IF_ERROR(ScalesShapeVerifier(scaled_dot, dim_numbers, 0, 2));\n+  TF_RETURN_IF_ERROR(ScalesShapeVerifier(scaled_dot, dim_numbers, 1, 3));\n+  if (ShapeUtil::IsScalar(scaled_dot->operand(2)->shape()) &&\n       ShapeUtil::IsScalar(scaled_dot->operand(3)->shape())) {\n     return absl::FailedPreconditionError(absl::StrFormat(\n         \"At least one of the scales should be not a scalar in %s\",\n@@ -341,7 +341,7 @@ absl::Status ShapeVerifier::HandleScaledDot(HloInstruction* scaled_dot) {\n   TF_ASSIGN_OR_RETURN(\n       const Shape expected,\n       ShapeInference::InferDotOpShape(\n-          scaled_dot->operand(0)->shape(), scaled_dot->operand(2)->shape(),\n+          scaled_dot->operand(0)->shape(), scaled_dot->operand(1)->shape(),\n           scaled_dot->dot_dimension_numbers(),\n           /*preferred_element_type=*/scaled_dot->shape().element_type()));\n   return CheckShape(scaled_dot, expected);"
        },
        {
            "sha": "2500073d57143a47ef4821eab47a350c6e87099b",
            "filename": "third_party/xla/xla/service/hlo_verifier_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -4927,7 +4927,7 @@ TEST_F(HloVerifierTest, ScaledDotWithNoScalesFails) {\n       b = f32[10,2] parameter(1)\n       a_scale = f32[] constant(1)\n       b_scale = f32[] constant(1)\n-      ROOT dot = f32[2,2] scaled-dot(a, a_scale, b, b_scale),\n+      ROOT dot = f32[2,2] scaled-dot(a, b, a_scale, b_scale),\n         lhs_contracting_dims={1},\n         rhs_contracting_dims={0}\n     }\n@@ -4951,7 +4951,7 @@ TEST_F(HloVerifierTest, ScaledDotWithBothScalesSucceeds) {\n       b = f8e8m0fnu[10,2] parameter(1)\n       a_scale = f8e5m2[2,2] parameter(2)\n       b_scale = f8e8m0fnu[2,2] parameter(3)\n-      ROOT dot = f32[2,2] scaled-dot(a, a_scale, b, b_scale),\n+      ROOT dot = f32[2,2] scaled-dot(a, b, a_scale, b_scale),\n         lhs_contracting_dims={1},\n         rhs_contracting_dims={0}\n     }\n@@ -4969,7 +4969,7 @@ TEST_F(HloVerifierTest, ScaledDotInvalidScaleShapeFails) {\n       b = f32[10,2] parameter(1)\n       a_scale = f32[2,2,2] parameter(2)\n       b_scale = f32[2,2,2] parameter(3)\n-      ROOT dot = f32[2,2] scaled-dot(a, a_scale, b, b_scale),\n+      ROOT dot = f32[2,2] scaled-dot(a, b, a_scale, b_scale),\n         lhs_contracting_dims={1},\n         rhs_contracting_dims={0}\n     }\n@@ -4993,7 +4993,7 @@ TEST_F(HloVerifierTest, ScaledDotWithInvalidScaleContractingDimSizeFails) {\n       b = f32[10,2] parameter(1)\n       a_scale = f32[2,6] parameter(2)\n       b_scale = f32[6,2] parameter(3)\n-      ROOT dot = f32[2,2] scaled-dot(a, a_scale, b, b_scale),\n+      ROOT dot = f32[2,2] scaled-dot(a, b, a_scale, b_scale),\n         lhs_contracting_dims={1},\n         rhs_contracting_dims={0}\n     }\n@@ -5016,7 +5016,7 @@ TEST_F(HloVerifierTest, ScaledDotWithScaleNonContractingDimSucceeds) {\n       b = f32[10,2] parameter(1)\n       a_scale = f32[1,5] parameter(2)\n       b_scale = f32[5,1] parameter(3)\n-      ROOT dot = f32[2,2] scaled-dot(a, a_scale, b, b_scale),\n+      ROOT dot = f32[2,2] scaled-dot(a, b, a_scale, b_scale),\n         lhs_contracting_dims={1},\n         rhs_contracting_dims={0}\n     }"
        },
        {
            "sha": "09e2592d0ec4618962e1f28badc1c7d79dd11398",
            "filename": "third_party/xla/xla/service/matmul_indexing_utils.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fmatmul_indexing_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fmatmul_indexing_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmatmul_indexing_utils.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -45,7 +45,8 @@ enum class Side { kLhs, kRhs };\n \n Side GetSide(const HloInstruction& dot, int operand_number) {\n   if (dot.opcode() == HloOpcode::kScaledDot) {\n-    return operand_number < 2 ? Side::kLhs : Side::kRhs;\n+    return (operand_number == 0 || operand_number == 2) ? Side::kLhs\n+                                                        : Side::kRhs;\n   }\n   return operand_number == 0 ? Side::kLhs : Side::kRhs;\n }\n@@ -129,19 +130,19 @@ absl::StatusOr<std::array<DotOperandDims, 4>> DotOperandDims::FromScaledDot(\n     const HloInstruction* scaled_dot) {\n   TF_ASSIGN_OR_RETURN(auto lhs_dims, FromDotOperand(scaled_dot, 0));\n   DotOperandDims lhs_scale_dims;\n-  if (scaled_dot->operand(1)->opcode() != HloOpcode::kConstant ||\n-      !scaled_dot->operand(1)->shape().dimensions().empty()) {\n-    TF_ASSIGN_OR_RETURN(lhs_scale_dims, FromDotOperand(scaled_dot, 1));\n+  if (scaled_dot->operand(2)->opcode() != HloOpcode::kConstant ||\n+      !scaled_dot->operand(2)->shape().dimensions().empty()) {\n+    TF_ASSIGN_OR_RETURN(lhs_scale_dims, FromDotOperand(scaled_dot, 2));\n   }\n \n-  TF_ASSIGN_OR_RETURN(auto rhs_dims, FromDotOperand(scaled_dot, 2));\n+  TF_ASSIGN_OR_RETURN(auto rhs_dims, FromDotOperand(scaled_dot, 1));\n   DotOperandDims rhs_scale_dims;\n   if (scaled_dot->operand(3)->opcode() != HloOpcode::kConstant ||\n       !scaled_dot->operand(3)->shape().dimensions().empty()) {\n     TF_ASSIGN_OR_RETURN(rhs_scale_dims, FromDotOperand(scaled_dot, 3));\n   }\n \n-  return std::array<DotOperandDims, 4>{lhs_dims, lhs_scale_dims, rhs_dims,\n+  return std::array<DotOperandDims, 4>{lhs_dims, rhs_dims, lhs_scale_dims,\n                                        rhs_scale_dims};\n }\n "
        },
        {
            "sha": "542a8b48c93870aaddd48198bde3784f427af46d",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d379c6590cf97bfe02dab46881998876cd5f23c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=8d379c6590cf97bfe02dab46881998876cd5f23c",
            "patch": "@@ -4316,9 +4316,9 @@ absl::Status SpmdPartitioningVisitor::HandleDotHelper(\n         Cast<HloCustomCallInstruction>(hlo);\n     PartitionedHlo& lhs_operand =\n         GetPartitionedHlo(block_scaled_dot->operand(0));\n-    PartitionedHlo& lhs_scale = GetPartitionedHlo(block_scaled_dot->operand(2));\n     PartitionedHlo& raw_rhs_operand =\n         GetPartitionedHlo(block_scaled_dot->operand(1));\n+    PartitionedHlo& lhs_scale = GetPartitionedHlo(block_scaled_dot->operand(2));\n     PartitionedHlo& raw_rhs_scale =\n         GetPartitionedHlo(block_scaled_dot->operand(3));\n     // If lhs and rhs are the same instruction, make a copy for rhs."
        }
    ],
    "stats": {
        "total": 401,
        "additions": 201,
        "deletions": 200
    }
}