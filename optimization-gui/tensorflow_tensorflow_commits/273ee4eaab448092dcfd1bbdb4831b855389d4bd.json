{
    "author": "seantalts",
    "message": "[XLA:CPU] Add PyTorch/ATen tanh benchmark to XLA CPU benchmarks.\n\nPiperOrigin-RevId: 802734189",
    "sha": "273ee4eaab448092dcfd1bbdb4831b855389d4bd",
    "files": [
        {
            "sha": "8bebf90d6e63384f3fb0e72a41c99dbf7ac5e2ba",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD?ref=273ee4eaab448092dcfd1bbdb4831b855389d4bd",
            "patch": "@@ -514,7 +514,6 @@ xla_cc_test(\n         \"//xla/tsl/platform:test_main\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n     ],\n )\n "
        },
        {
            "sha": "9f04c66b2d474bcd984592910c1f065f323b0fe2",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/tanh_benchmark_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Ftanh_benchmark_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Ftanh_benchmark_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Ftanh_benchmark_test.cc?ref=273ee4eaab448092dcfd1bbdb4831b855389d4bd",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n \n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/backends/cpu/benchmarks/hlo_benchmark_runner.h\"\n #include \"xla/backends/cpu/benchmarks/multi_benchmark_config.h\"\n #include \"xla/literal.h\"\n@@ -75,7 +74,7 @@ static void BM_TanhF16(benchmark::State& state) {\n }\n \n static void BM_TanhF64(benchmark::State& state, HloBenchmarkOptions options) {\n-  int64_t d0 = state.range(0);\n+  const int64_t d0 = state.range(0);\n \n   absl::string_view hlo = R\"(\n     HloModule tanh_f64_$d0\n@@ -94,6 +93,9 @@ static void BM_TanhF64(benchmark::State& state, HloBenchmarkOptions options) {\n   std::vector<const Literal*> args = {&p0};\n   CHECK_OK(\n       RunHloBenchmark(state, hlo, args, {{\"$d0\", absl::StrCat(d0)}}, options));\n+\n+  state.SetItemsProcessed(state.iterations() * d0);\n+  state.SetBytesProcessed(state.iterations() * d0 * sizeof(double));\n }\n \n #define REGISTER_TANH_BENCHMARK(NAME) \\"
        },
        {
            "sha": "af1179e932bd7acaca66619345504fc5adc2e631",
            "filename": "third_party/xla/xla/codegen/intrinsic/simple_jit_runner.cc",
            "status": "modified",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fsimple_jit_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fsimple_jit_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fsimple_jit_runner.cc?ref=273ee4eaab448092dcfd1bbdb4831b855389d4bd",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/call_once.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"llvm/ADT/StringMap.h\"\n #include \"llvm/ExecutionEngine/JITEventListener.h\"\n@@ -34,6 +35,7 @@ limitations under the License.\n #include \"llvm/ExecutionEngine/Orc/ThreadSafeModule.h\"\n #include \"llvm/ExecutionEngine/SectionMemoryManager.h\"\n #include \"llvm/IR/BasicBlock.h\"\n+#include \"llvm/IR/Constants.h\"\n #include \"llvm/IR/DerivedTypes.h\"\n #include \"llvm/IR/Function.h\"\n #include \"llvm/IR/IRBuilder.h\"\n@@ -45,6 +47,7 @@ limitations under the License.\n #include \"llvm/IR/Verifier.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n #include \"llvm/Support/Alignment.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/Errc.h\"\n #include \"llvm/Support/Error.h\"\n #include \"llvm/Support/ErrorHandling.h\"\n@@ -255,4 +258,74 @@ std::unique_ptr<llvm::TargetMachine> CreateHostTargetMachine() {\n   LOG_IF(FATAL, !target_machine) << \"Failed to create target machine\";\n   return target_machine;\n }\n+// Creates a new LLVM function that wraps an existing function by unrolling\n+// calls in a sequence.\n+//\n+// This function takes an `original_func` and generates a new function with an\n+// identical signature. Instead of a loop, the new function's body consists of\n+// an explicitly unrolled sequence of `unroll_factor` calls to the original\n+// function. This avoids loop overhead and is suitable for small K.\n+//\n+// `vector_size`: The size of the vectors being processed.\n+// Returns a pointer to the newly created unrolled wrapper function.\n+llvm::Function* CreateKTimesWrapper(llvm::Module* module,\n+                                    llvm::Function* original_func,\n+                                    int unroll_factor, size_t vector_size) {\n+  CHECK_GE(unroll_factor, 1);\n+\n+  llvm::LLVMContext& ctx = module->getContext();\n+  llvm::IRBuilder<> builder(ctx);\n+\n+  llvm::FunctionType* func_type = original_func->getFunctionType();\n+  std::string wrapper_name =\n+      std::string(original_func->getName()) + \"_unrolled_k_times\";\n+  llvm::Function* wrapper_func = llvm::Function::Create(\n+      func_type, llvm::Function::InternalLinkage, wrapper_name, module);\n+\n+  llvm::BasicBlock* entry =\n+      llvm::BasicBlock::Create(ctx, \"entry\", wrapper_func);\n+  builder.SetInsertPoint(entry);\n+\n+  // Collect the wrapper's arguments to be used as the base for each call.\n+  std::vector<llvm::Value*> wrapper_args;\n+  for (auto& arg : wrapper_func->args()) {\n+    wrapper_args.push_back(&arg);\n+  }\n+\n+  llvm::Value* last_result = nullptr;\n+  CHECK(!wrapper_args.empty()) << \"Function has no arguments.\";\n+  llvm::Type* scalar_type =\n+      llvm::cast<llvm::VectorType>(wrapper_args[0]->getType())\n+          ->getElementType();\n+  CHECK(scalar_type->isFloatingPointTy())\n+      << \"Only floating point types are supported.\";\n+  // Perturb the first argument by adding a small constant to\n+  // prevent the compiler from optimizing. The delta value is not important.\n+  llvm::Value* delta = llvm::ConstantFP::get(scalar_type, 0.000001);\n+\n+  // Use a C++ loop to generate an unrolled sequence of LLVM instructions.\n+  for (int k = 0; k < unroll_factor; ++k) {\n+    std::vector<llvm::Value*> call_args = wrapper_args;  // Reset to base args\n+\n+    // Perturb the first argument: arg0 + (k * 0.000001f)\n+    llvm::Value* k_fp =\n+        llvm::ConstantFP::get(scalar_type, static_cast<double>(k));\n+\n+    llvm::Value* offset_scalar = builder.CreateFMul(k_fp, delta);\n+    llvm::Value* offset_vec = builder.CreateVectorSplat(\n+        llvm::ElementCount::getFixed(vector_size), offset_scalar);\n+\n+    // Create the new argument for this specific call\n+    call_args[0] = builder.CreateFAdd(wrapper_args[0], offset_vec,\n+                                      \"perturbed.arg.\" + std::to_string(k));\n+\n+    last_result = builder.CreateCall(original_func, call_args,\n+                                     \"call.\" + std::to_string(k));\n+  }\n+\n+  // After the loop, `last_result` holds the result of the final call.\n+  builder.CreateRet(last_result);\n+\n+  return wrapper_func;\n+}\n }  // namespace xla::codegen::intrinsic"
        },
        {
            "sha": "f84fb55aacdc1916962d8c7d05ae04d0c76e431d",
            "filename": "third_party/xla/xla/codegen/intrinsic/simple_jit_runner.h",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fsimple_jit_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/273ee4eaab448092dcfd1bbdb4831b855389d4bd/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fsimple_jit_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fsimple_jit_runner.h?ref=273ee4eaab448092dcfd1bbdb4831b855389d4bd",
            "patch": "@@ -152,6 +152,22 @@ class JitRunner {\n \n std::unique_ptr<llvm::TargetMachine> CreateHostTargetMachine();\n \n+// Creates a new LLVM function that wraps an existing function by\n+// unrolling calls in a sequence.\n+//\n+// This function takes an `original_func` and generates a new function with an\n+// identical signature. Instead of a loop, the new function's body consists of\n+// an explicitly unrolled sequence of `unroll_factor` calls to the original\n+// function. This avoids loop overhead and is suitable for small K.\n+// This primarily serves as a knob to attempt to reduce the dependence of very\n+// small kernels on memory bandwidth.\n+//\n+// `vector_size`: The size of the vectors being processed.\n+// Returns a pointer to the newly created unrolled wrapper function.\n+llvm::Function* CreateKTimesWrapper(llvm::Module* module,\n+                                    llvm::Function* original_func,\n+                                    int unroll_factor, size_t vector_size);\n+\n }  // namespace xla::codegen::intrinsic\n \n #endif  // XLA_CODEGEN_INTRINSIC_SIMPLE_JIT_RUNNER_H_"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 93,
        "deletions": 3
    }
}