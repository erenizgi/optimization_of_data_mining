{
    "author": "majiddadashi",
    "message": "Generalize `PackInt8IntoDenseInt4` to support 2-bit and 4-bit packing.\n\nRename `PackInt8IntoDenseInt4` to `PackInt8IntoDenseInt` and add a `bit_width` parameter. Implement packing logic for both 2-bit and 4-bit integers. Update existing call sites in `quantize.cc` and `transpose.cc` to use the new function signature. Add new unit tests for both 2-bit and 4-bit packing.\n\nPiperOrigin-RevId: 818850218",
    "sha": "6d36365d7c640621020cd7c10429929b9c71342b",
    "files": [
        {
            "sha": "efc6ba5a9c04d361010ca2fdeaabdf432f8386c9",
            "filename": "tensorflow/lite/kernels/internal/portable_tensor_utils.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 17,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.cc?ref=6d36365d7c640621020cd7c10429929b9c71342b",
            "patch": "@@ -143,23 +143,40 @@ void UnpackPackedIntToInt8(const int8_t* src_buffer, int num_elements,\n   }\n }\n \n-void PackInt8IntoDenseInt4(const int8_t* src_buffer, int num_elements,\n-                           int8_t* dst_buffer) {\n-  // num_elements means the number of elements regardless of packed or unpacked.\n-  // For example, 3 elements means both\n-  //   1) Packed: 3 int4's = 12 bit -> 16 bits (padded) = 2 bytes.\n-  //      stored in src_buffer[0] and src_buffer[1] (i = 0..1)\n-  //   2) Unpacked: 3 int8's = 3 bytes.\n-  //      stored in dst_buffer[0], dst_buffer[1] and dst_buffer[2] (j = 0..2)\n-  for (int i = 0; i < num_elements - 1; i += 2) {\n-    dst_buffer[i / 2] = src_buffer[i] & 0x0F;\n-    dst_buffer[i / 2] |= src_buffer[i + 1] << 4;\n-  }\n-  auto packed_size = (num_elements + 1) / 2;\n-\n-  // Copy the final nibble if the buffer is odd-lengthed\n-  if (num_elements % 2 != 0) {\n-    dst_buffer[packed_size - 1] = src_buffer[num_elements - 1] & 0x0F;\n+void PackInt8IntoDenseInt(const int8_t* src_buffer, int num_elements,\n+                          int bit_width, int8_t* dst_buffer) {\n+  assert(bit_width == 2 || bit_width == 4);\n+  if (bit_width == 4) {\n+    // num_elements means the number of elements regardless of packed or\n+    // unpacked. For example, 3 elements means both\n+    //   1) Unpacked: 3 int8's = 3 bytes.\n+    //      stored in src_buffer[0], src_buffer[1] and src_buffer[2] (j = 0..2)\n+    //   2) Packed: 3 int4's = 12 bit -> 16 bits (padded) = 2 bytes.\n+    //      stored in dst_buffer[0] and dst_buffer[1] (i = 0..1)\n+    for (int i = 0; i < num_elements / 2; ++i) {\n+      dst_buffer[i] = (src_buffer[2 * i] & 0x0F) | (src_buffer[2 * i + 1] << 4);\n+    }\n+    // If the buffer size is odd, pack the final nibble.\n+    if (num_elements % 2 != 0) {\n+      dst_buffer[num_elements / 2] = src_buffer[num_elements - 1] & 0x0F;\n+    }\n+  } else if (bit_width == 2) {\n+    for (int i = 0; i < num_elements / 4; ++i) {\n+      dst_buffer[i] = (src_buffer[4 * i] & 0x03) |\n+                      ((src_buffer[4 * i + 1] & 0x03) << 2) |\n+                      ((src_buffer[4 * i + 2] & 0x03) << 4) |\n+                      ((src_buffer[4 * i + 3] & 0x03) << 6);\n+    }\n+    // Handle the remaining elements.\n+    int remaining_elements = num_elements % 4;\n+    if (remaining_elements > 0) {\n+      int8_t packed_val = 0;\n+      for (int i = 0; i < remaining_elements; ++i) {\n+        packed_val |= (src_buffer[num_elements - remaining_elements + i] & 0x03)\n+                      << (i * 2);\n+      }\n+      dst_buffer[num_elements / 4] = packed_val;\n+    }\n   }\n }\n "
        },
        {
            "sha": "c70ac94db5fcc784de6b7a288f067d48661d0123",
            "filename": "tensorflow/lite/kernels/internal/portable_tensor_utils.h",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Finternal%2Fportable_tensor_utils.h?ref=6d36365d7c640621020cd7c10429929b9c71342b",
            "patch": "@@ -635,20 +635,24 @@ void UnpackDenseInt4IntoInt8(const int8_t* src_buffer, int num_elements,\n void UnpackPackedIntToInt8(const int8_t* src_buffer, int num_elements,\n                            int bit_width, int8_t* dst_buffer);\n \n-// Pack `src_buffer` into a densely packed buffer of int4 values.\n+// Pack `src_buffer` into a densely packed buffer of int2 or int4 values.\n // Parameters:\n-//   src_buffer   : Buffer containing int4 values stored in int8 memory.\n+//   src_buffer   : Buffer containing int2 or int4 values stored in int8\n+//                  memory.\n //   num_elements : Number of elements stored in the buffer. Note that this can\n //                  be smaller than the size of `src_buffer` by 1 if it's odd,\n //                  in which case the last nibble in `src_buffer` is ignored.\n //                  This should be equal to the size of `dst_buffer`.\n+//   bit_width    : The bit width of the packed elements (either 2 or 4).\n //   dst_buffer   : Buffer to pack into. Should be allocated by the caller.\n //                  Size should be at least `num_elements`.\n // Notes:\n-//   For example, given `src_buffer = {0x02, 0x01, 0x04, 0x03}`, calling this\n-//   function will return `dst_buffer = {0x12, 0x34}`.\n-void PackInt8IntoDenseInt4(const int8_t* src_buffer, int num_elements,\n-                           int8_t* dst_buffer);\n+//   For 4-bit packing: e.g., given `src_buffer = {0x02, 0x01, 0x04, 0x03}`,\n+//   calling this function will return `dst_buffer = {0x12, 0x34}`.\n+//   For 2-bit packing: e.g., given `src_buffer = {0x00, 0x01, 0x00, 0x02}`,\n+//   calling this function will return `dst_buffer = {0x84}`.\n+void PackInt8IntoDenseInt(const int8_t* src_buffer, int num_elements,\n+                          int bit_width, int8_t* dst_buffer);\n }  // namespace tensor_utils\n \n }  // namespace tflite"
        },
        {
            "sha": "c1ae8831a0d26c8070f6c73e980c93e7d8de491b",
            "filename": "tensorflow/lite/kernels/internal/tensor_utils_test.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Finternal%2Ftensor_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Finternal%2Ftensor_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Finternal%2Ftensor_utils_test.cc?ref=6d36365d7c640621020cd7c10429929b9c71342b",
            "patch": "@@ -2149,6 +2149,44 @@ TEST(uKernels, UnpackInt2OddLength) {\n               testing::Pointwise(testing::Eq(), expected_output));\n }\n \n+TEST(uKernels, PackInt4Basic) {\n+  const int8_t input[4] = {-8, 3, -2, -5};\n+  const int8_t expected_output[2] = {0x38, static_cast<int8_t>(0xBE)};\n+  int8_t actual_output[2];\n+  PackInt8IntoDenseInt(input, 4, 4, actual_output);\n+  EXPECT_THAT(actual_output,\n+              testing::Pointwise(testing::Eq(), expected_output));\n+}\n+\n+TEST(uKernels, PackInt4OddLength) {\n+  // `num_elements` is odd, so the last element 0x4 should be ignored\n+  const int8_t input[3] = {1, 2, 3};\n+  const int8_t expected_output[2] = {0x21, 0x03};\n+  int8_t actual_output[2];\n+  PackInt8IntoDenseInt(input, 3, 4, actual_output);\n+  EXPECT_THAT(actual_output,\n+              testing::Pointwise(testing::Eq(), expected_output));\n+}\n+\n+TEST(uKernels, PackInt2Basic) {\n+  const int8_t input[4] = {0, -1, -2, 1};\n+  const int8_t expected_output[1] = {0x6C};\n+  int8_t actual_output[1];\n+  PackInt8IntoDenseInt(input, 4, 2, actual_output);\n+  EXPECT_THAT(actual_output,\n+              testing::Pointwise(testing::Eq(), expected_output));\n+}\n+\n+TEST(uKernels, PackInt2OddLength) {\n+  // `num_elements` is odd\n+  const int8_t input[3] = {0, -2, 1};\n+  const int8_t expected_output[1] = {0x18};\n+  int8_t actual_output[1];\n+  PackInt8IntoDenseInt(input, 3, 2, actual_output);\n+  EXPECT_THAT(actual_output,\n+              testing::Pointwise(testing::Eq(), expected_output));\n+}\n+\n }  // namespace tensor_utils\n }  // namespace tflite\n "
        },
        {
            "sha": "ed6a444d3a47c31836011be7143a8de0b7303fbc",
            "filename": "tensorflow/lite/kernels/quantize.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Fquantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Fquantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fquantize.cc?ref=6d36365d7c640621020cd7c10429929b9c71342b",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"tensorflow/lite/core/c/common.h\"\n #include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n+#include \"tensorflow/lite/kernels/internal/portable_tensor_utils.h\"\n #include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n #include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n #include \"tensorflow/lite/kernels/internal/reference/requantize.h\"\n@@ -109,8 +110,8 @@ void AffineQuantizeToInt4(const tflite::QuantizationParams& op_params,\n     int32_t clamped = std::min(std::max(unclamped, min_val), max_val);\n     quantized_buffer[i] = clamped;\n   }\n-  tensor_utils::PackInt8IntoDenseInt4(quantized_buffer.data(), flat_size,\n-                                      output_data);\n+  tensor_utils::PackInt8IntoDenseInt(quantized_buffer.data(), flat_size,\n+                                     /*bit_width=*/4, output_data);\n }\n \n void ReportError(TfLiteContext* context, TfLiteType input_type,"
        },
        {
            "sha": "66c84e652015d7025b7789661d555569aa607e4f",
            "filename": "tensorflow/lite/kernels/transpose.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d36365d7c640621020cd7c10429929b9c71342b/tensorflow%2Flite%2Fkernels%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Ftranspose.cc?ref=6d36365d7c640621020cd7c10429929b9c71342b",
            "patch": "@@ -142,10 +142,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n           params, GetTensorShape(op_context.input), unpacked_input_data.get(),\n           GetTensorShape(op_context.output), unpacked_output_data.get());\n       // Pack the output back to int4.\n-      tflite::tensor_utils::PackInt8IntoDenseInt4(\n+      tflite::tensor_utils::PackInt8IntoDenseInt(\n           unpacked_output_data.get(),\n           GetTensorShape(op_context.input).FlatSize(),\n-          GetTensorData<int8_t>(op_context.output));\n+          /*bit_width=*/4, GetTensorData<int8_t>(op_context.output));\n       break;\n     }\n     case kTfLiteInt16:"
        }
    ],
    "stats": {
        "total": 114,
        "additions": 87,
        "deletions": 27
    }
}