{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Use xtile::MaskOp in reduce.\n\nThis is a step towards allowing multi-dimension reduce in the tiled emitter.\n\nPiperOrigin-RevId: 835164756",
    "sha": "10fe76f2824f4a9965438cc5fed90ca532199d11",
    "files": [
        {
            "sha": "71b3ceaead1cb0197cd10c975808c0fe2e043f64",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=10fe76f2824f4a9965438cc5fed90ca532199d11",
            "patch": "@@ -37,6 +37,7 @@ void CreateTritonXlaPipeline(\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerAtomicsPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerGetTidPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerXTilePass());\n+  pm->addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n \n   auto* cuda_cc = gpu_cc.cuda_compute_capability();\n   bool is_at_least_hopper = cuda_cc != nullptr && cuda_cc->IsAtLeastHopper();"
        },
        {
            "sha": "b52834f1b0d494e073aff374d2312cf31115c52a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 23,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=10fe76f2824f4a9965438cc5fed90ca532199d11",
            "patch": "@@ -226,11 +226,6 @@ absl::StatusOr<TensorValue> EmitReduce(\n   const HloReduceInstruction& hlo_reduce =\n       *::xla::Cast<HloReduceInstruction>(tiled_hlo_reduce.hlo());\n   TensorValue input = values[tiled_hlo_reduce.operand(0)];\n-  llvm::ArrayRef<int64_t> input_shape = input.getType().getShape();\n-  absl::Span<const int64_t> source_tensor_shape =\n-      hlo_reduce.operand(0)->shape().dimensions();\n-\n-  int reduction_dimension = hlo_reduce.dimensions().front();\n \n   // Since every shape is padded to a power of 2 in Triton, the input tile may\n   // be padded with arbitrary values. These values could affect the result of\n@@ -240,29 +235,30 @@ absl::StatusOr<TensorValue> EmitReduce(\n   // hlo_reduce.operand(1) is thus always the right choice to ensure that the\n   // reduction is computed correctly, since it is the neutral value with\n   // regards to the reducer.\n-  int64_t source_tensor_reduction_dimension_size =\n-      source_tensor_shape[reduction_dimension];\n-  int64_t input_reduction_dimension_size = input_shape[reduction_dimension];\n-  if (input_reduction_dimension_size !=\n-      source_tensor_reduction_dimension_size) {\n-    TensorValue range = Iota(b, input_reduction_dimension_size);\n-    TensorValue bcast =\n-        BroadcastInDims(b, range, input_shape, {reduction_dimension});\n-    TensorValue constant = CreateConst(\n-        b, b.getI32Type(), source_tensor_reduction_dimension_size, input_shape);\n-    Value mask =\n-        b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast, constant);\n-\n-    TensorValue neutral = BroadcastInDims(\n-        b, values[tiled_hlo_reduce.operand(1)], input_shape, /*dims=*/{});\n-    input = mlir::cast<TensorValue>(\n-        b.create<arith::SelectOp>(mask, input, neutral).getResult());\n+\n+  absl::Span<const int64_t> unpadded_tile_sizes =\n+      tiled_hlo_reduce.operand(0)->tile_sizes();\n+  llvm::SmallVector<int64_t> mask_dim_bounds;\n+  mask_dim_bounds.reserve(unpadded_tile_sizes.size());\n+  for (auto [idx, dim_size] : llvm::enumerate(unpadded_tile_sizes)) {\n+    if (absl::c_contains(hlo_reduce.dimensions(), idx)) {\n+      // We only need to mask the reduction dimensions.\n+      mask_dim_bounds.push_back(dim_size);\n+    } else {\n+      mask_dim_bounds.push_back(input.getType().getDimSize(idx));\n+    }\n   }\n+  mlir::Value neutral_value =\n+      mlir::tensor::ExtractOp::create(b, values[tiled_hlo_reduce.operand(1)]);\n+  // Use createOrFold as the mask may be be reduntant, in which case it will be\n+  // folded away.\n+  input = mlir::cast<TensorValue>(\n+      b.createOrFold<xtile::MaskOp>(input, mask_dim_bounds, neutral_value));\n \n   Value init_value = values[tiled_hlo_reduce.operand(1)];\n \n   stablehlo::ReduceOp reduction =\n-      b.create<stablehlo::ReduceOp>(input, init_value, reduction_dimension);\n+      b.create<stablehlo::ReduceOp>(input, init_value, hlo_reduce.dimensions());\n   {\n     TF_ASSIGN_OR_RETURN(Type result_ty,\n                         TritonType(b, hlo_reduce.shape().element_type()));\n@@ -1627,7 +1623,14 @@ absl::Status IsTritonSupportedFusion(const HloFusionInstruction& fusion,\n             absl::StrCat(\"Pad is not supported: \", hlo->ToString()));\n       }\n     }\n+\n+    if (hlo->opcode() == HloOpcode::kReduce && hlo->dimensions().size() != 1) {\n+      return absl::FailedPreconditionError(\n+          absl::StrCat(\"Reduction with only a single dimension is supported: \",\n+                       hlo->ToString()));\n+    }\n   }\n+\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "9537bcfc01d6aff4906dc66d714936062181b178",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=10fe76f2824f4a9965438cc5fed90ca532199d11",
            "patch": "@@ -972,15 +972,13 @@ ENTRY main {\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_reduction_computation\",\n                                 R\"(\n \n-        CHECK:  stablehlo.iota\n-        CHECK:  stablehlo.broadcast_in_dim\n+        CHECK:  xtile.mask\n         CHECK:  stablehlo.reduce(%[[SELECT:.*]] init: %{{.*}}) across dimensions = [2] : (tensor<4x2x8x8x1xf32>, tensor<f32>) -> tensor<4x2x8x1xf32>\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n       this, xtile_module_and_hlo_module.first.get(), R\"(\n-CHECK:  tt.make_range\n-CHECK-COUNT-4:  tt.expand_dims\n+CHECK:  xtile.mask\n CHECK:  \"tt.reduce\"(%[[SELECT:.*]]) <{axis = 2 : i32}>\n   )\",\n       GetFusionInstruction(*xtile_module_and_hlo_module.second,\n@@ -1024,11 +1022,8 @@ ENTRY main {\n                                 R\"(\n ; Make sure input reduction tile is padded with a neutral value.\n CHECK:  %[[LOAD:.*]] = xtile.extract\n-CHECK:  %[[RANGE:.*]] = stablehlo.iota\n-CHECK:  %[[BROADCAST:.*]] = stablehlo.broadcast_in_dim %[[RANGE]]\n-CHECK:  %[[CMPI:.*]] = arith.cmpi slt, %[[BROADCAST]]\n-CHECK:  %[[SELECT:.*]] = arith.select %[[CMPI]], %[[LOAD]], %{{.*}}\n-CHECK: %[[REDUCE:.*]] = stablehlo.reduce(%[[SELECT]] init: %{{.*}}) across dimensions = [0] : (tensor<8x4xf32>, tensor<f32>) -> tensor<4xf32>\n+CHECK:  %[[MASKED:.*]] = xtile.mask %[[LOAD]]\n+CHECK:  %[[REDUCE:.*]] = stablehlo.reduce(%[[MASKED]] init: %{{.*}}) across dimensions = [0] : (tensor<8x4xf32>, tensor<f32>) -> tensor<4xf32>\n CHECK:   reducer(%[[ARG0:.*]]: tensor<f32>, %[[ARG1:.*]]: tensor<f32>)  {\n CHECK:   %[[MAX:.*]] = arith.maximumf %[[ARG0]], %[[ARG1]] : tensor<f32>\n CHECK:   stablehlo.return %[[MAX]] : tensor<f32>\n@@ -1039,12 +1034,8 @@ CHECK: }\n       this, xtile_module_and_hlo_module.first.get(), R\"(\n ; Make sure input reduction tile is padded with a neutral value.\n CHECK:  %[[LOAD:.*]] = xtile.extract\n-CHECK:  %[[RANGE:.*]] = tt.make_range\n-CHECK:  %[[EXPAND:.*]] = tt.expand_dims %[[RANGE]]\n-CHECK:  %[[BROADCAST:.*]] = tt.broadcast %[[EXPAND]]\n-CHECK:  %[[CMPI:.*]] = arith.cmpi slt, %[[BROADCAST]]\n-CHECK:  %[[SELECT:.*]] = arith.select %[[CMPI]], %[[LOAD]]\n-CHECK:  \"tt.reduce\"(%[[SELECT]]) <{axis = 0 : i32}>\n+CHECK:  %[[MASKED:.*]] = xtile.mask %[[LOAD]]\n+CHECK:  \"tt.reduce\"(%[[MASKED]]) <{axis = 0 : i32}>\n CHECK:  ^bb0(%[[ARG2:.*]]: f32, %[[ARG3:.*]]: f32):\n CHECK:    %[[MAXIMUM:.*]] = arith.maximumf %[[ARG2]], %[[ARG3]] : f32\n CHECK:    tt.reduce.return %[[MAXIMUM]] : f32"
        },
        {
            "sha": "a218774db86235cc6a8889e1902095d06bd7abf3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=10fe76f2824f4a9965438cc5fed90ca532199d11",
            "patch": "@@ -216,8 +216,8 @@ ENTRY e {\n       block_level_parameters,\n       R\"(\n CHECK: %[[INIT:.*]] = arith.constant dense<0.000000e+00> : tensor<f32>\n-CHECK: %[[REDUCE_INPUT:.*]] = arith.select {{.*}}\n-CHECK: %[[RES:.*]] = stablehlo.reduce(%[[REDUCE_INPUT]] init: %[[INIT]]) across dimensions = [0] : (tensor<256x16xf32>, tensor<f32>) -> tensor<16xf32>\n+CHECK: %[[MASKED_INPUT:.*]] = xtile.mask {{.*}}\n+CHECK: %[[RES:.*]] = stablehlo.reduce(%[[MASKED_INPUT]] init: %[[INIT]]) across dimensions = [0] : (tensor<256x16xf32>, tensor<f32>) -> tensor<16xf32>\n CHECK: reducer(%[[ARG_0:.*]]: tensor<f32>, %[[ARG_1:.*]]: tensor<f32>)  {\n CHECK:   %[[SUM:.*]] = arith.addf %[[ARG_0]], %[[ARG_1]] : tensor<f32>\n CHECK:   stablehlo.return %[[SUM]] : tensor<f32>"
        },
        {
            "sha": "89b8ea2c452d4ce7e1ce712165bd6912b722c547",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_lower_xtile.mlir",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir?ref=10fe76f2824f4a9965438cc5fed90ca532199d11",
            "patch": "@@ -81,3 +81,24 @@ func.func @fold_transpose_into_ptr(\n   // CHECK: return %[[PTR]] : !tt.ptr<f64>\n   return %ptr : !tt.ptr<f64>\n }\n+\n+// -----\n+\n+// CHECK-LABEL: @mask_lowers_to_stable_hlo(%arg0: tensor<32xf64>, %arg1: f64) -> tensor<32xf64>\n+func.func @mask_lowers_to_stable_hlo(%arg0: tensor<32xf64>, %arg1: f64) -> tensor<32xf64> {\n+  // CHECK: %[[BOUND:.*]] = arith.constant dense<10> : tensor<32xi32>\n+  // CHECK: %[[IDX:.*]] = stablehlo.iota dim = 0 : tensor<32xi32>\n+  // CHECK: %[[IDX_BROADCASTED:.*]] = stablehlo.broadcast_in_dim %[[IDX]],\n+  // CHECK-SAME: dims = [0] : (tensor<32xi32>) -> tensor<32xi32>\n+  // CHECK: %[[MASK:.*]] = arith.cmpi slt, %[[IDX_BROADCASTED]], %[[BOUND]]\n+  // CHECK-SAME: : tensor<32xi32>\n+  // CHECK: %[[INIT:.*]] = tensor.from_elements %arg1 : tensor<f64>\n+  // CHECK: %[[INIT_TENSOR:.*]] = stablehlo.broadcast_in_dim %[[INIT]],\n+  // CHECK-SAME: dims = [] : (tensor<f64>) -> tensor<32xf64>\n+  // CHECK: %[[RESULT:.*]] = arith.select %[[MASK]], %arg0, %[[INIT_TENSOR]]\n+  // CHECK-SAME: : tensor<32xi1>, tensor<32xf64>\n+  %paded = xtile.mask %arg0 bounds [10], %arg1 : tensor<32xf64>\n+  // CHECK: return %[[RESULT]] : tensor<32xf64>\n+  return %paded : tensor<32xf64>\n+}\n+"
        },
        {
            "sha": "e4266d6b93abceb11fd54701e97e453805798d87",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_xtile_pass.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 1,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/10fe76f2824f4a9965438cc5fed90ca532199d11/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc?ref=10fe76f2824f4a9965438cc5fed90ca532199d11",
            "patch": "@@ -45,6 +45,7 @@ limitations under the License.\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Inliner.h\"\n #include \"mlir/Transforms/InliningUtils.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n@@ -290,6 +291,50 @@ class XTileInsertToTriton\n   }\n };\n \n+class XTileMaskToTriton : public mlir::OpRewritePattern<::xla::xtile::MaskOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      ::xla::xtile::MaskOp op, mlir::PatternRewriter& rewriter) const override {\n+    llvm::SmallVector<int64_t> masked_dimensions = op.getMaskedDimensions();\n+    if (masked_dimensions.size() != 1) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"triton masking only supports masking over a single dimension\");\n+    }\n+\n+    int64_t mask_dimension = masked_dimensions.front();\n+    int64_t mask_bound = op.getBounds()[mask_dimension];\n+    int64_t masked_dim_size = op.getType().getDimSize(mask_dimension);\n+    auto iota_type =\n+        mlir::RankedTensorType::get(masked_dim_size, rewriter.getI32Type());\n+    auto range = stablehlo::IotaOp::create(rewriter, op.getLoc(), iota_type, 0);\n+    auto bcast_type = mlir::RankedTensorType::get(op.getType().getShape(),\n+                                                  iota_type.getElementType());\n+    auto bcast = stablehlo::BroadcastInDimOp::create(\n+        rewriter, op.getLoc(), bcast_type, range, {mask_dimension});\n+    auto constant = mlir::arith::ConstantOp::create(\n+        rewriter, op.getLoc(),\n+        mlir::DenseElementsAttr::get(bcast_type,\n+                                     rewriter.getI32IntegerAttr(mask_bound)));\n+    Value mask = arith::CmpIOp::create(\n+        rewriter, op.getLoc(), arith::CmpIPredicate::slt, bcast, constant);\n+\n+    auto mask_value_tensor = mlir::tensor::FromElementsOp::create(\n+        rewriter, op.getLoc(),\n+        mlir::RankedTensorType::get({}, op.getValue().getType()),\n+        op.getValue());\n+    auto neutral = stablehlo::BroadcastInDimOp::create(\n+        rewriter, op.getLoc(), op.getType(), mask_value_tensor,\n+        ArrayRef<int64_t>{});\n+\n+    rewriter.replaceOpWithNewOp<arith::SelectOp>(op, mask, op.getSource(),\n+                                                 neutral);\n+\n+    return mlir::success();\n+  }\n+};\n+\n class FoldIntoMemrefToPtr : public mlir::OpRewritePattern<MemrefToPtrOp> {\n  public:\n   using OpRewritePattern::OpRewritePattern;\n@@ -321,7 +366,7 @@ class TritonXLALowerXTilePass\n     mlir::RewritePatternSet patterns(context);\n \n     patterns.add<XTileEntryToTriton, XTileExtractToTriton, XTileInsertToTriton,\n-                 FoldIntoMemrefToPtr>(context);\n+                 XTileMaskToTriton, FoldIntoMemrefToPtr>(context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(module, std::move(patterns)))) {\n       signalPassFailure();"
        }
    ],
    "stats": {
        "total": 143,
        "additions": 102,
        "deletions": 41
    }
}