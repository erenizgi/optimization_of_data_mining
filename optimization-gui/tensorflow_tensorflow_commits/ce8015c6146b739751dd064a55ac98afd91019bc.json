{
    "author": "akuegel",
    "message": "[XLA:CPU] Remove obsolete IndexedArrayAnalysisPrinterPass.\n\nIt is not being used anymore.\n\nPiperOrigin-RevId: 825927601",
    "sha": "ce8015c6146b739751dd064a55ac98afd91019bc",
    "files": [
        {
            "sha": "0b67a0bb77d60d124d65750d325819ff6dfb2193",
            "filename": "third_party/xla/xla/hlo/analysis/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2FBUILD?ref=ce8015c6146b739751dd064a55ac98afd91019bc",
            "patch": "@@ -582,47 +582,6 @@ xla_cc_test(\n     ],\n )\n \n-cc_library(\n-    name = \"indexed_array_analysis\",\n-    srcs = [\"indexed_array_analysis.cc\"],\n-    hdrs = [\"indexed_array_analysis.h\"],\n-    deps = [\n-        \"//xla:literal\",\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/hlo/evaluator:hlo_evaluator\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/pass:hlo_pass\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/container:inlined_vector\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"indexed_array_analysis_test\",\n-    srcs = [\"indexed_array_analysis_test.cc\"],\n-    deps = [\n-        \":indexed_array_analysis\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-    ],\n-)\n-\n cc_library(\n     name = \"interval\",\n     srcs = [\"interval.cc\"],"
        },
        {
            "sha": "2cd08d3129bcda4b543ac8c544c6a035ad818796",
            "filename": "third_party/xla/xla/hlo/analysis/indexed_array_analysis.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 1189,
            "changes": 1189,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f1d4574bd4e79987d6ffedebdde9694e42cbbe1/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f1d4574bd4e79987d6ffedebdde9694e42cbbe1/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis.cc?ref=6f1d4574bd4e79987d6ffedebdde9694e42cbbe1",
            "patch": "@@ -1,1189 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/hlo/analysis/indexed_array_analysis.h\"\n-\n-#include <algorithm>\n-#include <cstdint>\n-#include <functional>\n-#include <iterator>\n-#include <numeric>\n-#include <optional>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/container/inlined_vector.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/str_join.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/hlo/evaluator/hlo_evaluator.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/map_util.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/statusor.h\"\n-\n-namespace xla {\n-\n-namespace {\n-using Analysis = IndexedArrayAnalysis;\n-using UnknownArray = Analysis::UnknownArray;\n-using ConstantArray = Analysis::ConstantArray;\n-using ReshapedArray = Analysis::ReshapedArray;\n-using ScalarIndexedArray = Analysis::ScalarIndexedArray;\n-using absl::StrJoin;\n-}  // namespace\n-\n-std::string IndexedArrayAnalysis::ToString(Array* root, bool print_constants) {\n-  switch (root->kind()) {\n-    case Array::kUnknown: {\n-      auto* unknown_tensor = root->as<UnknownArray>();\n-      return absl::StrCat(\"%\", unknown_tensor->instruction().name());\n-    }\n-\n-    case Array::kConstant: {\n-      if (print_constants) {\n-        std::string contents = root->as<ConstantArray>()->literal()->ToString();\n-        return absl::StrCat(\"(constant \", ShapeUtil::HumanString(root->shape()),\n-                            \" \", contents, \")\");\n-      }\n-      return absl::StrCat(\"(constant \", ShapeUtil::HumanString(root->shape()),\n-                          \")\");\n-    }\n-\n-    case Array::kReshaped: {\n-      ReshapedArray* reshaped_array = root->as<ReshapedArray>();\n-      return absl::StrCat(\n-          \"(reshape \", ToString(reshaped_array->operand(), print_constants),\n-          \" to \", ShapeUtil::HumanString(reshaped_array->shape()), \")\");\n-    }\n-\n-    case Array::kScalarIndexedConstant:\n-    case Array::kScalarIndexed: {\n-      auto* indexed_array = root->as<ScalarIndexedArray>();\n-      std::string name = root->kind() == Array::kScalarIndexedConstant\n-                             ? \"scalar-indexed-const\"\n-                             : \"scalar-indexed\";\n-      return absl::StrCat(\n-          \"(\", name, \" \", ToString(indexed_array->source(), print_constants),\n-          \" \", ToString(indexed_array->indices(), print_constants), \" \",\n-          indexed_array->source_dim(), \"->[\",\n-          StrJoin(indexed_array->output_dims(), \",\"), \"])\");\n-    }\n-  }\n-}\n-\n-absl::StatusOr<Analysis::Array*> IndexedArrayAnalysis::GetArrayFor(\n-    const HloInstruction* instr) {\n-  auto it = cache_.find(instr);\n-  if (it != cache_.end()) {\n-    return it->second;\n-  }\n-\n-  TF_RETURN_IF_ERROR(TraverseAndPopulateCache(instr));\n-  return FindOrDie(cache_, instr);\n-}\n-\n-absl::Status IndexedArrayAnalysis::TraverseAndPopulateCache(\n-    const HloInstruction* root) {\n-  // Depth first search over the DAG, invoking ComputeArrayFor in post order.\n-  // The HLO instructions already in the cache are considered leaves.\n-\n-  absl::InlinedVector<const HloInstruction*, 4> stack;\n-\n-  enum DfsState { kDiscovered, kVisited };\n-  absl::flat_hash_map<const HloInstruction*, DfsState> dfs_state_map;\n-\n-  stack.push_back(root);\n-  InsertOrDie(&dfs_state_map, root, kDiscovered);\n-\n-  do {\n-    const HloInstruction* instr = stack.back();\n-    if (cache_.contains(instr)) {\n-      stack.pop_back();\n-      continue;\n-    }\n-\n-    switch (FindOrDie(dfs_state_map, instr)) {\n-      case kDiscovered: {\n-        for (const HloInstruction* operand : instr->operands()) {\n-          if (!cache_.contains(operand)) {\n-            stack.push_back(operand);\n-            CHECK(!dfs_state_map.contains(operand) ||\n-                  dfs_state_map[operand] == kDiscovered);\n-            dfs_state_map[operand] = kDiscovered;\n-          }\n-        }\n-        dfs_state_map[instr] = kVisited;\n-        break;\n-      }\n-\n-      case kVisited:\n-        stack.pop_back();\n-        TF_ASSIGN_OR_RETURN(Array * array, ComputeArrayFor(instr));\n-        InsertOrDie(&cache_, instr, array);\n-        break;\n-    }\n-  } while (!stack.empty());\n-\n-  return absl::OkStatus();\n-}\n-\n-absl::StatusOr<Analysis::Array*> IndexedArrayAnalysis::ComputeArrayFor(\n-    const HloInstruction* instr) {\n-  Array* computed_array;\n-  if (instr->IsElementwise() && instr->operand_count() == 1) {\n-    TF_ASSIGN_OR_RETURN(\n-        computed_array,\n-        ComputeArrayForElementwiseUnaryOp(\n-            instr->opcode(), FindOrDie(cache_, instr->operand(0))));\n-  } else if (instr->IsElementwise() && instr->operand_count() == 2) {\n-    TF_ASSIGN_OR_RETURN(\n-        computed_array,\n-        ComputeArrayForElementwiseBinaryOp(\n-            instr->opcode(), FindOrDie(cache_, instr->operand(0)),\n-            FindOrDie(cache_, instr->operand(1))));\n-  } else if (instr->opcode() == HloOpcode::kConstant) {\n-    TF_ASSIGN_OR_RETURN(computed_array,\n-                        ComputeArrayForConstant(instr->literal()));\n-  } else if (instr->opcode() == HloOpcode::kGather) {\n-    TF_ASSIGN_OR_RETURN(\n-        computed_array,\n-        ComputeArrayForGather(instr->shape(), instr->gather_dimension_numbers(),\n-                              instr->gather_slice_sizes(),\n-                              FindOrDie(cache_, instr->operand(0)),\n-                              FindOrDie(cache_, instr->operand(1))));\n-  } else if (instr->opcode() == HloOpcode::kReshape) {\n-    TF_ASSIGN_OR_RETURN(\n-        computed_array,\n-        ComputeArrayForReshape(instr->shape(),\n-                               FindOrDie(cache_, instr->operand(0))));\n-  } else if (instr->opcode() == HloOpcode::kDot) {\n-    TF_ASSIGN_OR_RETURN(\n-        computed_array,\n-        ComputeArrayForDot(instr->shape(), instr->dot_dimension_numbers(),\n-                           instr->precision_config(),\n-                           FindOrDie(cache_, instr->operand(0)),\n-                           FindOrDie(cache_, instr->operand(1))));\n-  } else {\n-    computed_array = nullptr;\n-  }\n-\n-  if (!computed_array) {\n-    computed_array = Construct<UnknownArray>(instr);\n-  }\n-\n-  return computed_array;\n-}\n-\n-absl::StatusOr<Analysis::Array*> IndexedArrayAnalysis::ComputeArrayForConstant(\n-    const Literal& literal) {\n-  return Construct<ConstantArray>(&literal);\n-}\n-\n-absl::StatusOr<ScalarIndexedArray*> IndexedArrayAnalysis::FoldGatherOfGather(\n-    ScalarIndexedArray* source, Array* indices, int64_t source_dim,\n-    absl::Span<const int64_t> output_dims, Shape shape) {\n-  // We want to transform Gather(Gather(A, X), Y) => Gather(A, Gather(X, Y)).\n-  // `source` is the inner Gather(A, X).\n-\n-  Array* a = source->source();\n-  Array* x = source->indices();\n-  Array* y = indices;\n-\n-  // This bit is slightly tricky, so we do a naive \"simulation\" of the two\n-  // consecutive gather operations to infer what the composed gather should look\n-  // like.\n-\n-  enum class IndexComponent { Ungathered, GatheredFirst, GatheredSecond };\n-\n-  std::vector<IndexComponent> simulated_index(a->shape().dimensions().size(),\n-                                              IndexComponent::Ungathered);\n-\n-  // Simulate the first gather.\n-  EraseAt(&simulated_index, source->source_dim());\n-  for (int64_t gather_dim : source->output_dims()) {\n-    simulated_index.insert(simulated_index.begin() + gather_dim,\n-                           IndexComponent::GatheredFirst);\n-  }\n-\n-  // Simulate the second gather.\n-  EraseAt(&simulated_index, source_dim);\n-  for (int64_t output_dim : output_dims) {\n-    simulated_index.insert(simulated_index.begin() + output_dim,\n-                           IndexComponent::GatheredSecond);\n-  }\n-\n-  int64_t source_dim_for_index_array =\n-      FindIndex(source->output_dims(), source_dim);\n-  CHECK_NE(source_dim_for_index_array, source->output_dims().size());\n-\n-  std::vector<int64_t> output_dims_for_index_array;\n-  int64_t gathered_index_components_seen = 0;\n-  for (IndexComponent simulation_dim : simulated_index) {\n-    if (simulation_dim == IndexComponent::GatheredSecond) {\n-      output_dims_for_index_array.push_back(gathered_index_components_seen);\n-    }\n-    if (simulation_dim != IndexComponent::Ungathered) {\n-      gathered_index_components_seen++;\n-    }\n-  }\n-\n-  std::vector<int64_t> dim_sizes_for_composed_index;\n-  std::vector<int64_t> output_dims_for_new_gather;\n-  for (int64_t i = 0, e = simulated_index.size(); i < e; i++) {\n-    if (simulated_index[i] != IndexComponent::Ungathered) {\n-      dim_sizes_for_composed_index.push_back(shape.dimensions(i));\n-      output_dims_for_new_gather.push_back(i);\n-    }\n-  }\n-\n-  Array* inner_indices = ConstructScalarIndexedArray(\n-      x, y, source_dim_for_index_array, output_dims_for_index_array,\n-      ShapeUtil::MakeShape(x->shape().element_type(),\n-                           dim_sizes_for_composed_index));\n-  return ConstructScalarIndexedArray(a, inner_indices, source->source_dim(),\n-                                     output_dims_for_new_gather,\n-                                     std::move(shape));\n-}\n-\n-absl::StatusOr<Analysis::Array*> IndexedArrayAnalysis::ComputeArrayForGather(\n-    const Shape& shape, const GatherDimensionNumbers& dim_numbers,\n-    absl::Span<const int64_t> slice_sizes, Array* source, Array* indices) {\n-  if (dim_numbers.index_vector_dim() != indices->shape().dimensions().size()) {\n-    VLOG(3) << \"ComputeArrayForGather: indices are not scalar\";\n-    return nullptr;\n-  }\n-\n-  CHECK_EQ(dim_numbers.start_index_map_size(), 1);\n-\n-  // We can also handle dim_numbers.collapsed_slice_dims_size() == 0 here,\n-  // should it become relevant.\n-\n-  if (dim_numbers.collapsed_slice_dims_size() != 1 ||\n-      dim_numbers.collapsed_slice_dims(0) != dim_numbers.start_index_map(0)) {\n-    VLOG(3) << \"ComputeArrayForGather: gather operations must elide \"\n-               \"start_index_map[0] and \"\n-               \"start_index_map[0] only\";\n-    return nullptr;\n-  }\n-\n-  // ScalarIndexedArray cannot represent gathers that \"slice\" along some\n-  // dimensions -- for instance it cannot represent a gather that picks 5 [2,3]\n-  // arrays from an array of size [7,4,6].  We check that condition down below:\n-\n-  for (int64_t i = 0, e = source->shape().dimensions().size(); i < e; i++) {\n-    if (i != dim_numbers.collapsed_slice_dims(0) &&\n-        source->shape().dimensions(i) != slice_sizes[i]) {\n-      VLOG(3) << \"ComputeArrayForGather: slice_sizes[\" << i\n-              << \"] != source->shape().dimensions(\" << i << \") -- \"\n-              << source->shape().dimensions(i) << \" vs. \" << slice_sizes[i]\n-              << \" with dim_numbers.collapsed_slice_dims(0) = \"\n-              << dim_numbers.collapsed_slice_dims(0);\n-      return nullptr;\n-    }\n-  }\n-\n-  int64_t source_dim = dim_numbers.start_index_map(0);\n-  std::vector<int64_t> output_dims;\n-  for (int64_t i = 0, e = shape.dimensions().size(); i < e; i++) {\n-    if (!absl::c_binary_search(dim_numbers.offset_dims(), i)) {\n-      output_dims.push_back(i);\n-    }\n-  }\n-\n-  if (auto* indexed = dynamic_cast<ScalarIndexedArray*>(source)) {\n-    if (absl::c_linear_search(indexed->output_dims(), source_dim)) {\n-      return FoldGatherOfGather(indexed, indices, source_dim, output_dims,\n-                                shape);\n-    }\n-  } else if (auto* constant = dynamic_cast<ConstantArray*>(source)) {\n-    return Construct<ScalarIndexedConstantArray>(constant, indices, source_dim,\n-                                                 output_dims, shape);\n-  }\n-\n-  return Construct<ScalarIndexedArray>(source, indices, source_dim, output_dims,\n-                                       shape);\n-}\n-\n-namespace {\n-// Returns an index into `values` such that the product of the range\n-// [values.begin()+index, values.end()) is equal to `product`.  If there is no\n-// such index, return -1.  All integers in `values` must be positive.\n-int64_t FindSuffixWithProduct(absl::Span<const int64_t> values,\n-                              int64_t product) {\n-  DCHECK(absl::c_all_of(values, [](int64_t value) { return value > 0; }));\n-\n-  int64_t current_product = 1;\n-  int64_t i;\n-  for (i = values.size() - 1; i >= 0 && product > current_product; --i) {\n-    current_product *= values[i];\n-  }\n-\n-  if (product == current_product) {\n-    return i + 1;\n-  }\n-\n-  return -1;\n-}\n-\n-struct ReshapePassthroughDimPair {\n-  int64_t result_dim;\n-  int64_t operand_dim;\n-};\n-\n-// Returns a set of dimension pairs such for all (result_dim, operand_dim) in\n-// the set:\n-//\n-// output_index[result_dim] = SourceIndexOfReshape(output_index)[operand_dim]\n-//\n-// The returned vector of pairs is sorted in both the result_dim and the\n-// operand_dim components.\n-std::vector<ReshapePassthroughDimPair> ComputeReshapePassthroughDimPairs(\n-    absl::Span<const int64_t> operand_shape,\n-    absl::Span<const int64_t> result_shape) {\n-  // A reshape can be seen as an index mapping from output index to input index:\n-  //\n-  // (i_0, ..., i_n) = f(o_0, ..., o_m)\n-  //\n-  // This function returns the pairs (j, k) for which the following invariant\n-  // holds for all indices in the shape:\n-  //\n-  //   o_j == i_k\n-  //\n-  // And this occurs when:\n-  //\n-  //    O_{j+1} * ... * O_n == I_{k+1} * ...  * I_m\n-  //\n-  // (where O_x are the sizes of the output shape and I_x are the sizes of the\n-  // input shape) and the size of the dimension j of the result is the same as\n-  // the size of dimension k in the operand.\n-  //\n-  // These conditions are sufficient because the Reshape HLO is spec'ed such\n-  // that the rightmost dimensions are always minor in the flattening and refine\n-  // operation.\n-\n-  std::vector<ReshapePassthroughDimPair> result;\n-  int64_t result_subarray_size = 1;\n-  for (int64_t result_dim = result_shape.size() - 1; result_dim >= 0;\n-       --result_dim) {\n-    int64_t candidate_operand_dim =\n-        FindSuffixWithProduct(operand_shape, result_subarray_size);\n-\n-    // result_subarray_size does not include the elements in the current\n-    // `result_dim` dimension (we multiply in result_shape[result_dim] at the\n-    // end of loop body) so candidate_operand_dim can never be zero.\n-    CHECK_NE(candidate_operand_dim, 0)\n-        << \"result_dim = \" << result_dim\n-        << \", result_subarray_size = \" << result_subarray_size\n-        << \", result_shape = [\" << StrJoin(result_shape, \",\") << \"]\"\n-        << \", operand_shape = [\" << StrJoin(operand_shape, \",\") << \"]\";\n-\n-    if (candidate_operand_dim != -1 &&\n-        result_shape[result_dim] == operand_shape[candidate_operand_dim - 1]) {\n-      result.push_back({/*result_dim=*/result_dim,\n-                        /*operand_dim=*/candidate_operand_dim - 1});\n-    }\n-    result_subarray_size *= result_shape[result_dim];\n-  }\n-\n-  absl::c_reverse(result);\n-\n-  if (VLOG_IS_ON(3)) {\n-    std::vector<std::string> result_strings;\n-    absl::c_transform(result, std::back_inserter(result_strings),\n-                      [](ReshapePassthroughDimPair value) {\n-                        return absl::StrCat(value.result_dim, \"->\",\n-                                            value.operand_dim);\n-                      });\n-    VLOG(3) << \"For a reshape from [\" << StrJoin(operand_shape, \",\") << \"] to [\"\n-            << StrJoin(result_shape, \",\") << \"] passthrough indices are [\"\n-            << StrJoin(result_strings, \",\")\n-            << \"] (legend: `result`->`operand`)\";\n-  }\n-\n-  DCHECK(absl::c_is_sorted(\n-      result, [](ReshapePassthroughDimPair lhs, ReshapePassthroughDimPair rhs) {\n-        return lhs.result_dim < rhs.result_dim;\n-      }));\n-\n-  DCHECK(absl::c_is_sorted(\n-      result, [](ReshapePassthroughDimPair lhs, ReshapePassthroughDimPair rhs) {\n-        return lhs.operand_dim < rhs.operand_dim;\n-      }));\n-\n-  return result;\n-}\n-\n-// Return true if `dim` is stated as an passthrough operand dim in\n-// `passthrough_dims`.\n-bool IsReshapePassthroughOperandDim(\n-    absl::Span<const ReshapePassthroughDimPair> passthrough_dims, int64_t dim) {\n-  return absl::c_any_of(passthrough_dims,\n-                        [&](ReshapePassthroughDimPair passthrough_dim_pair) {\n-                          return passthrough_dim_pair.operand_dim == dim;\n-                        });\n-}\n-\n-// Maps `operand_dim` which must be an passthrough operand dimension to its\n-// corresponding passthrough result dimension based on `passthrough_dims`.\n-int64_t MapPassthroughOperandDimToResultDim(\n-    absl::Span<const ReshapePassthroughDimPair> passthrough_dims,\n-    int64_t operand_dim) {\n-  auto it = absl::c_find_if(\n-      passthrough_dims, [&](ReshapePassthroughDimPair passthrough_dim_pair) {\n-        return passthrough_dim_pair.operand_dim == operand_dim;\n-      });\n-  CHECK(it != passthrough_dims.end());\n-  return it->result_dim;\n-}\n-\n-int64_t FindSourcePositionForPassthroughResultDim(\n-    absl::Span<const int64_t> operand_shape,\n-    absl::Span<const int64_t> result_shape, int64_t source_passthrough_dim) {\n-  VLOG(3) << \"FindSourcePositionForPassthroughResultDim([\"\n-          << StrJoin(operand_shape, \",\") << \"], [\" << StrJoin(result_shape, \",\")\n-          << \"], \" << source_passthrough_dim << \")\";\n-\n-  int64_t indexed_source_subarray_size =\n-      std::accumulate(operand_shape.begin() + source_passthrough_dim + 1,\n-                      operand_shape.end(), 1LL, std::multiplies<int64_t>());\n-\n-  return FindSuffixWithProduct(result_shape, indexed_source_subarray_size);\n-}\n-\n-Shape StripDegenerateDimensions(const Shape& shape) {\n-  DimensionVector new_dims;\n-  absl::c_copy_if(shape.dimensions(), std::back_inserter(new_dims),\n-                  [](int64_t dim) { return dim != 1; });\n-  return ShapeUtil::MakeShape(shape.element_type(), new_dims);\n-}\n-};  // namespace\n-\n-absl::StatusOr<ScalarIndexedArray*>\n-IndexedArrayAnalysis::ReshapeToRemoveDegenerateDims(\n-    ScalarIndexedArray* operand) {\n-  const Shape& shape = operand->shape();\n-  if (!ShapeUtil::HasDegenerateDimensions(shape)) {\n-    return operand;\n-  }\n-\n-  // We only need to reshape out the degenerate dims from the indices and the\n-  // source (except the source dim).\n-\n-  const Shape& source_shape = operand->source()->shape();\n-  DimensionVector new_source_shape_dims;\n-  for (int64_t i = 0, e = source_shape.dimensions().size(); i < e; i++) {\n-    if (i == operand->source_dim() || source_shape.dimensions(i) != 1) {\n-      new_source_shape_dims.push_back(source_shape.dimensions(i));\n-    }\n-  }\n-\n-  Shape new_source_shape =\n-      ShapeUtil::MakeShape(shape.element_type(), new_source_shape_dims);\n-  Shape new_indices_shape =\n-      StripDegenerateDimensions(operand->indices()->shape());\n-\n-  TF_ASSIGN_OR_RETURN(\n-      Array* const new_source,\n-      ComputeArrayForReshape(new_source_shape, operand->source()));\n-  TF_ASSIGN_OR_RETURN(\n-      Array* const new_indices,\n-      ComputeArrayForReshape(new_indices_shape, operand->indices()));\n-\n-  // Build the new output dims while keeping track of the degenerate dims that\n-  // will no longer be present.\n-  DimensionVector new_output_dims;\n-  int64_t degenerate_dims_seen = 0;\n-  for (int64_t i = 0, e = shape.dimensions().size(); i < e; i++) {\n-    if (shape.dimensions(i) == 1) {\n-      degenerate_dims_seen++;\n-    } else if (absl::c_linear_search(operand->output_dims(), i)) {\n-      new_output_dims.push_back(i - degenerate_dims_seen);\n-    }\n-  }\n-\n-  // Similarly, build the new source dim while keeping track of the degenerate\n-  // dims that will no longer be present.\n-  int64_t degenerate_dims_before_source_dim =\n-      std::count(source_shape.dimensions().begin(),\n-                 source_shape.dimensions().begin() + operand->source_dim(), 1);\n-  int64_t new_source_dim =\n-      operand->source_dim() - degenerate_dims_before_source_dim;\n-\n-  return ConstructScalarIndexedArray(\n-      new_source, new_indices, new_source_dim,\n-      InlinedVectorToVector(new_output_dims),\n-      StripDegenerateDimensions(operand->shape()));\n-}\n-\n-absl::StatusOr<ScalarIndexedArray*>\n-IndexedArrayAnalysis::ReshapeToAddDegenerateDims(\n-    ScalarIndexedArray* operand, absl::Span<const int64_t> degenerate_dims) {\n-  if (degenerate_dims.empty()) {\n-    return operand;\n-  }\n-\n-  CHECK(!ShapeUtil::HasDegenerateDimensions(operand->shape()));\n-\n-  DimensionVector new_output_dims = [&]() {\n-    // To make things easy we use a \"scratch\" buffer of bools where the i'th\n-    // element is true iff the i'th component of the result index is an output\n-    // index.\n-\n-    absl::InlinedVector<bool, 6> output_dims_bitvector(\n-        operand->shape().dimensions().size());\n-    for (int64_t output_dim : operand->output_dims()) {\n-      output_dims_bitvector[output_dim] = true;\n-    }\n-\n-    for (int64_t degenerate_dim : degenerate_dims) {\n-      InsertAt(&output_dims_bitvector, degenerate_dim, false);\n-    }\n-\n-    DimensionVector result;\n-    result.reserve(operand->output_dims().size());\n-    for (int64_t i = 0, e = output_dims_bitvector.size(); i < e; i++) {\n-      if (output_dims_bitvector[i]) {\n-        result.push_back(i);\n-      }\n-    }\n-\n-    return result;\n-  }();\n-\n-  DimensionVector new_result_shape_dims;\n-  absl::c_copy(operand->shape().dimensions(),\n-               std::back_inserter(new_result_shape_dims));\n-  for (int64_t degenerate_dim : degenerate_dims) {\n-    InsertAt(&new_result_shape_dims, degenerate_dim, 1);\n-  }\n-\n-  DimensionVector new_source_shape_dims = new_result_shape_dims;\n-  for (int64_t output_dim : new_output_dims) {\n-    EraseAt(&new_source_shape_dims, output_dim);\n-  }\n-\n-  int64_t new_source_dim = [&]() {\n-    for (int i = 0, e = new_source_shape_dims.size(); i < e; i++) {\n-      int64_t non_degenerate_dims_seen = 0;\n-      if (non_degenerate_dims_seen == operand->source_dim()) {\n-        return i;\n-      }\n-      if (new_source_shape_dims[new_source_dim] != 1) {\n-        non_degenerate_dims_seen++;\n-      }\n-    }\n-    LOG(FATAL) << \"Did not find source dim in \" << ToString(operand);\n-  }();\n-\n-  int64_t source_dim_size =\n-      operand->source()->shape().dimensions(operand->source_dim());\n-  InsertAt(&new_source_shape_dims, /*index=*/new_source_dim,\n-           /*value=*/source_dim_size);\n-\n-  Shape new_source_shape = ShapeUtil::MakeShape(operand->shape().element_type(),\n-                                                new_source_shape_dims);\n-  Shape new_result_shape = ShapeUtil::MakeShape(operand->shape().element_type(),\n-                                                new_result_shape_dims);\n-\n-  TF_ASSIGN_OR_RETURN(\n-      Array* const new_source,\n-      ComputeArrayForReshape(new_source_shape, operand->source()));\n-  return ConstructScalarIndexedArray(\n-      new_source, operand->indices(), new_source_dim,\n-      InlinedVectorToVector(new_output_dims), new_result_shape);\n-}\n-\n-absl::StatusOr<ScalarIndexedArray*> IndexedArrayAnalysis::FoldReshapeOfGather(\n-    const Shape& shape, ScalarIndexedConstantArray* operand) {\n-  VLOG(3) << \"FoldReshapeOfGather(\" << ToString(operand) << \")\";\n-\n-  // To make things easier on ourselves, instead of directly trying to fold the\n-  // reshape of `operand` to `shape`, we call\n-  // `FoldReshapeOfGatherNoDegenerateDims` on shapes without degenerate dims and\n-  // handle the degenerate dimensions here by inserting reshapes.\n-\n-  TF_ASSIGN_OR_RETURN(ScalarIndexedArray* const operand_without_degenerate_dims,\n-                      ReshapeToRemoveDegenerateDims(operand));\n-\n-  Shape output_shape_without_degenerate_dims = StripDegenerateDimensions(shape);\n-  TF_ASSIGN_OR_RETURN(\n-      ScalarIndexedArray* const folded_reshape_without_degenerate_dims,\n-      FoldReshapeOfGatherNoDegenerateDims(\n-          output_shape_without_degenerate_dims,\n-          operand_without_degenerate_dims->as<ScalarIndexedConstantArray>()));\n-\n-  if (folded_reshape_without_degenerate_dims == nullptr) {\n-    return nullptr;\n-  }\n-\n-  DimensionVector degenerate_result_dims;\n-  for (int64_t i = 0, e = shape.dimensions().size(); i < e; i++) {\n-    if (shape.dimensions(i) == 1) {\n-      degenerate_result_dims.push_back(i);\n-    }\n-  }\n-\n-  return ReshapeToAddDegenerateDims(folded_reshape_without_degenerate_dims,\n-                                    degenerate_result_dims);\n-}\n-\n-absl::StatusOr<ScalarIndexedArray*>\n-IndexedArrayAnalysis::FoldReshapeOfGatherNoDegenerateDims(\n-    const Shape& shape, ScalarIndexedConstantArray* scalar_indexed) {\n-  VLOG(3) << \"FoldReshapeOfGatherNoDegenerateDims(\" << ToString(scalar_indexed)\n-          << \")\";\n-  CHECK(!ShapeUtil::HasDegenerateDimensions(shape));\n-  CHECK(!ShapeUtil::HasDegenerateDimensions(scalar_indexed->shape()));\n-\n-  // Try to fold Reshape(ScalarIndexed(Const, Indices))\n-  //          => ScalarIndexed(Const', Indices)\n-  //\n-  // We can view the reshape and the scalar-indexed operations as functions that\n-  // map an output index (i.e. an index into the result) to an input index\n-  // (i.e. an index into the operand).  The key idea used here is that the\n-  // output-to-input mapping for some reshape operations may \"pass through\" some\n-  // output dimensions into the input space unchanged -- i.e. there may exist\n-  // output dimension \"O\" and input dimension \"I\" such that OutputIndex[O] is\n-  // always == InputIndexForReshape(OutputIndex)[I].  If these pass-through\n-  // dimensions in the input space of the reshape happen to be include all the\n-  // output dimensions for the scalar-indexed node then, roughly, the following\n-  // holds:\n-  //\n-  //    SourceIndexOfScalarIndexed(SourceIndexOfReshape(Idx))\n-  // == SourceIndexOfScalarIndexed(SourceIndexOfReshape(Ps ++ Qs))\n-  //\n-  //      Where Ps are the set of the pass-through components of Idx that are\n-  //      also the output dims of the scalar-indexed node, and Qs are the rest.\n-  //      For brevity, we're playing fast and loose with the notation here -- we\n-  //      don't literally require Idx to be a concatenation of Ps and Qs, as\n-  //      suggested by the \"++\".\n-  //\n-  // == SourceIndexOfScalarIndexed(Ps ++ SourceIndexOfReshape(Qs))\n-  //\n-  //      Again, we're playing fast and loose with the notation around \"++\".\n-  //      Generally this ++ will be a different function that the ++ in the\n-  //      previous step.\n-  //\n-  // If the scalar-indexed node has a constant as the source then the\n-  // SourceIndexOfReshape function can be \"folded into\" the constant itself by\n-  // reshaping it, leaving us with:\n-  //\n-  // == SourceIndexOfScalarIndexed(Ps ++ Qs)\n-  // == SourceIndexOfScalarIndexed(Idx)\n-  //\n-  // which is just a scalar-indexed node (with parameters different from the\n-  // scalar-indexed node we started with) with a reshaped constant as the\n-  // source.\n-  //\n-  // We can't fold SourceIndexOfReshape into the constant without introducing\n-  // another precondition: since the new scalar-indexed node will have a\n-  // reshaped (constant) array as its source it will, in general, have a\n-  // different source dimension than the original scalar-indexed node.  This\n-  // source dimension will have to be a passthrough dimension of the\n-  // SourceIndexOfReshape indexing function that is folded into the source. And\n-  // such a dimension need not exist so this is a non-trivial precondition.\n-\n-  std::vector<ReshapePassthroughDimPair> reshape_passthrough_dims =\n-      ComputeReshapePassthroughDimPairs(\n-          /*operand_shape=*/scalar_indexed->shape().dimensions(),\n-          /*result_shape=*/shape.dimensions());\n-\n-  auto is_reshape_passthrough_operand_dim = [&](int64_t operand_dim) {\n-    return IsReshapePassthroughOperandDim(reshape_passthrough_dims,\n-                                          operand_dim);\n-  };\n-\n-  if (!absl::c_all_of(scalar_indexed->output_dims(),\n-                      is_reshape_passthrough_operand_dim)) {\n-    VLOG(3) << \"Not all output dims are passthrough dims \"\n-            << ToString(scalar_indexed);\n-    return nullptr;\n-  }\n-\n-  // To compute the shape of the source for the new scalar-indexed node we're\n-  // going to create, we first \"undo\" the scalar-indexed operation.\n-  std::vector<int64_t> new_scalar_indexed_source_shape(\n-      shape.dimensions().begin(), shape.dimensions().end());\n-  for (int64_t i = scalar_indexed->output_dims().size() - 1; i >= 0; i--) {\n-    int64_t output_dim = scalar_indexed->output_dims()[i];\n-    int64_t output_dim_after_reshape = MapPassthroughOperandDimToResultDim(\n-        reshape_passthrough_dims, output_dim);\n-    EraseAt(&new_scalar_indexed_source_shape, output_dim_after_reshape);\n-  }\n-\n-  // After this, we need to add in the dimension that will be the source\n-  // dimension for the new scalar-indexed node.  A scalar-indexed node \"removes\"\n-  // the source dimensions and \"adds\" the output dimensions, so to get back to\n-  // the shape for the *source* of the scalar-indexed node we need to remove the\n-  // output dims (which we did above) and then add back the source dim (which we\n-  // are about to do below):\n-\n-  const Shape& scalar_indexed_source_shape = scalar_indexed->source()->shape();\n-\n-  int64_t source_dim_for_new_scalar_indexed_node =\n-      FindSourcePositionForPassthroughResultDim(\n-          /*operand_shape=*/scalar_indexed_source_shape.dimensions(),\n-          /*result_shape=*/new_scalar_indexed_source_shape,\n-          scalar_indexed->source_dim());\n-\n-  // We may not be able to find a source dim for the new scalar-indexed node.\n-  // For instance consider:\n-  //\n-  //   operand = s32[3,5,2] constant({...})\n-  //   indices = s32[7] parameter(0)\n-  //   gather = s32[3,2,7] gather(operand, indices),\n-  //       offset_dims={0,1},\n-  //       collapsed_slice_dims={1},\n-  //       start_index_map={1},\n-  //       index_vector_dim=1,\n-  //       slice_sizes={3,1,2}\n-  //   reshape = s32[6,7] reshape(gather)\n-  //\n-  // In this case the gather maps to:\n-  //    (scalar-indexed-const (constant s32[3,5,2]) %indices 1->[2])\n-  //\n-  // and the reshape passes through dimension 2 from its input into dimension 1\n-  // in its output.  However, we can't rewrite the reshape as a scalar-indexed\n-  // node because then we'd have to reshape the [3,5,2] `operand` array to\n-  // [6,5], but then dimension 1 of the reshaped [6,5] array indexes differently\n-  // (a.k.a. isn't pass-through) than the [3,5,2] array.\n-\n-  if (source_dim_for_new_scalar_indexed_node == -1) {\n-    VLOG(3) << \"Could not compute the source dim for the new scalar indexed \"\n-               \"node: scalar_indexed_source_shape = [\"\n-            << StrJoin(scalar_indexed_source_shape.dimensions(), \",\")\n-            << \"] and new_scalar_indexed_source_shape = [\"\n-            << StrJoin(new_scalar_indexed_source_shape, \",\") << \"]\";\n-    return nullptr;\n-  }\n-\n-  InsertAt(\n-      &new_scalar_indexed_source_shape, source_dim_for_new_scalar_indexed_node,\n-      scalar_indexed_source_shape.dimensions(scalar_indexed->source_dim()));\n-\n-  CHECK_EQ(absl::c_accumulate(new_scalar_indexed_source_shape, 1LL,\n-                              std::multiplies<int64_t>()),\n-           ShapeUtil::ElementsIn(scalar_indexed_source_shape));\n-\n-  CHECK(IsReshapePassthroughOperandDim(\n-      ComputeReshapePassthroughDimPairs(\n-          /*operand_shape=*/scalar_indexed_source_shape.dimensions(),\n-          /*result_shape=*/new_scalar_indexed_source_shape),\n-      scalar_indexed->source_dim()));\n-\n-  auto map_passthrough_operand_dim_to_result_dim = [&](int64_t result_dim) {\n-    return MapPassthroughOperandDimToResultDim(reshape_passthrough_dims,\n-                                               result_dim);\n-  };\n-\n-  std::vector<int64_t> output_dims_for_new_scalar_indexed_node;\n-  absl::c_transform(scalar_indexed->output_dims(),\n-                    std::back_inserter(output_dims_for_new_scalar_indexed_node),\n-                    map_passthrough_operand_dim_to_result_dim);\n-\n-  TF_ASSIGN_OR_RETURN(const Literal* new_scalar_indexed_source_literal,\n-                      TakeOwnership(scalar_indexed->literal().Reshape(\n-                          new_scalar_indexed_source_shape)));\n-  TF_ASSIGN_OR_RETURN(\n-      Array * new_scalar_indexed_source,\n-      ComputeArrayForConstant(*new_scalar_indexed_source_literal));\n-\n-  return ConstructScalarIndexedArray(\n-      new_scalar_indexed_source, scalar_indexed->indices(),\n-      source_dim_for_new_scalar_indexed_node,\n-      output_dims_for_new_scalar_indexed_node, shape);\n-}\n-\n-absl::StatusOr<Analysis::Array*> IndexedArrayAnalysis::ComputeArrayForReshape(\n-    const Shape& shape, Array* operand) {\n-  if (ShapeUtil::Compatible(operand->shape(), shape)) {\n-    return operand;\n-  }\n-\n-  if (auto* scalar_indexed =\n-          dynamic_cast<ScalarIndexedConstantArray*>(operand)) {\n-    TF_ASSIGN_OR_RETURN(Analysis::Array * reshape_folded_into_gather,\n-                        FoldReshapeOfGather(shape, scalar_indexed));\n-    if (reshape_folded_into_gather) {\n-      return reshape_folded_into_gather;\n-    }\n-  }\n-\n-  if (auto* constant_array = dynamic_cast<ConstantArray*>(operand)) {\n-    TF_ASSIGN_OR_RETURN(\n-        Literal* const new_literal,\n-        TakeOwnership(constant_array->literal()->Reshape(shape.dimensions())));\n-    return Construct<ConstantArray>(new_literal);\n-  }\n-\n-  return Construct<ReshapedArray>(operand, shape);\n-}\n-\n-absl::StatusOr<Analysis::Array*>\n-IndexedArrayAnalysis::ComputeArrayForElementwiseBinaryOp(HloOpcode opcode,\n-                                                         Array* lhs,\n-                                                         Array* rhs) {\n-  // Try to fold BinaryOp(Broadcast(Const0), ScalarIndexed(Const1, Indices))\n-  //          => ScalarIndexed(BinaryOp(Broadcast'(Const0), Const1), Indices)\n-  //\n-  // We can do this if every output dimension from the scalar-indexed node is a\n-  // broadcasted dimension for the broadcast node.  Informally, the precondition\n-  // means Broadcast(Const0)[IDX] is solely a function of the components of IDX\n-  // that are not output-dims for the scalar-indexed node. In other words, for\n-  // every assignment to the non-output dims in IDX we have a \"constant\" LHS to\n-  // the BinaryOp.  This transform propagates this \"constant\" to the source for\n-  // the scalar-indexed node.\n-\n-  ScalarIndexedConstantArray* lhs_scalar_indexed_const =\n-      dynamic_cast<ScalarIndexedConstantArray*>(lhs);\n-  ScalarIndexedConstantArray* rhs_scalar_indexed_const =\n-      dynamic_cast<ScalarIndexedConstantArray*>(rhs);\n-\n-  bool lhs_is_indexed;\n-\n-  // One of the operands must be scalar-indexed and the other must be a\n-  // broadcast of a constant.\n-  if (lhs_scalar_indexed_const && !rhs_scalar_indexed_const) {\n-    lhs_is_indexed = true;\n-  } else if (rhs_scalar_indexed_const && !lhs_scalar_indexed_const) {\n-    lhs_is_indexed = false;\n-  } else {\n-    return nullptr;\n-  }\n-\n-  ScalarIndexedConstantArray* scalar_indexed_const =\n-      lhs_is_indexed ? lhs_scalar_indexed_const : rhs_scalar_indexed_const;\n-  UnknownArray* candidate_broadcast_array =\n-      dynamic_cast<UnknownArray*>(lhs_is_indexed ? rhs : lhs);\n-  if (!candidate_broadcast_array ||\n-      candidate_broadcast_array->instruction().opcode() !=\n-          HloOpcode::kBroadcast) {\n-    return nullptr;\n-  }\n-\n-  const HloInstruction* broadcast_instr =\n-      &candidate_broadcast_array->instruction();\n-  const HloInstruction* broadcast_const_operand = broadcast_instr->operand(0);\n-  if (broadcast_const_operand->opcode() != HloOpcode::kConstant) {\n-    return nullptr;\n-  }\n-\n-  absl::Span<const int64_t> broadcast_dims = broadcast_instr->dimensions();\n-  auto is_broadcasted_dim = [&](int64_t output_dim) {\n-    return absl::c_find(broadcast_dims, output_dim) == broadcast_dims.end();\n-  };\n-\n-  // All of the output dims must be \"broadcasted\" dims for the other operand.\n-  if (!absl::c_all_of(scalar_indexed_const->output_dims(),\n-                      is_broadcasted_dim)) {\n-    return nullptr;\n-  }\n-\n-  // To figure out the broadcast dimensions for the (constant) source for the\n-  // scalar-indexed node, we \"simulate\" the index transformation done by the\n-  // existing broadcast:\n-  enum class IndexComponent { Broadcasted, NotBroadcasted };\n-  std::vector<IndexComponent> simulated_index(\n-      broadcast_instr->shape().dimensions().size(),\n-      IndexComponent::Broadcasted);\n-  for (int64_t broadcast_dim : broadcast_dims) {\n-    simulated_index[broadcast_dim] = IndexComponent::NotBroadcasted;\n-  }\n-\n-  // The scalar-indexed node \"removes\" the source dim and \"inserts\" the output\n-  // dims.  We do the opposite here to undo the scalar-indexed operation.\n-  absl::Span<const int64_t> output_dims = scalar_indexed_const->output_dims();\n-  for (int64_t i = output_dims.size() - 1; i >= 0; --i) {\n-    CHECK(simulated_index[output_dims[i]] == IndexComponent::Broadcasted);\n-    EraseAt(&simulated_index, output_dims[i]);\n-  }\n-\n-  InsertAt(&simulated_index, scalar_indexed_const->source_dim(),\n-           IndexComponent::Broadcasted);\n-\n-  // new_inner_broadcast_dims holds the broadcast dimensions for the inner\n-  // BinaryOp(Broadcast'(Const0), Const1).  We now translate simulated_index to\n-  // new_inner_broadcast_dims.\n-  std::vector<int64_t> new_inner_broadcast_dims;\n-  for (int64_t i = 0; i < simulated_index.size(); i++) {\n-    if (simulated_index[i] == IndexComponent::NotBroadcasted) {\n-      new_inner_broadcast_dims.push_back(i);\n-    }\n-  }\n-\n-  // inner_broadcast_result is the Broadcast'(Const0) bit in\n-  // BinaryOp(Broadcast'(Const0), Const1)\n-  TF_ASSIGN_OR_RETURN(\n-      Literal inner_broadcast_result,\n-      broadcast_const_operand->literal().Broadcast(\n-          scalar_indexed_const->source()->shape(), new_inner_broadcast_dims));\n-\n-  // literal_for_new_source is BinaryOp(Broadcast'(Const0), Const1)\n-  const Literal* literal_for_new_source;\n-  if (lhs_is_indexed) {\n-    TF_ASSIGN_OR_RETURN(\n-        literal_for_new_source,\n-        TakeOwnership(HloEvaluator{}.EvaluateElementwiseBinaryOp(\n-            opcode, scalar_indexed_const->literal(), inner_broadcast_result)));\n-  } else {\n-    TF_ASSIGN_OR_RETURN(\n-        literal_for_new_source,\n-        TakeOwnership(HloEvaluator{}.EvaluateElementwiseBinaryOp(\n-            opcode, inner_broadcast_result, scalar_indexed_const->literal())));\n-  }\n-\n-  ConstantArray* new_source = Construct<ConstantArray>(literal_for_new_source);\n-  return Construct<ScalarIndexedConstantArray>(\n-      new_source, scalar_indexed_const->indices(),\n-      scalar_indexed_const->source_dim(),\n-      std::vector<int64_t>(scalar_indexed_const->output_dims().begin(),\n-                           scalar_indexed_const->output_dims().end()),\n-      scalar_indexed_const->shape());\n-}\n-\n-absl::StatusOr<Analysis::Array*>\n-IndexedArrayAnalysis::ComputeArrayForElementwiseUnaryOp(HloOpcode opcode,\n-                                                        Array* operand) {\n-  auto* scalar_indexed_const =\n-      dynamic_cast<ScalarIndexedConstantArray*>(operand);\n-  if (scalar_indexed_const == nullptr) {\n-    return nullptr;\n-  }\n-\n-  // Fold UnaryOp(ScalarIndexed(Const, Indices))\n-  //   => ScalarIndexed(UnaryOp(Const), Indices)\n-\n-  TF_ASSIGN_OR_RETURN(Literal * literal_for_new_source,\n-                      TakeOwnership(HloEvaluator{}.EvaluateElementwiseUnaryOp(\n-                          opcode, scalar_indexed_const->literal())));\n-  ConstantArray* new_source = Construct<ConstantArray>(literal_for_new_source);\n-  return Construct<ScalarIndexedConstantArray>(\n-      new_source, scalar_indexed_const->indices(),\n-      scalar_indexed_const->source_dim(),\n-      SpanToVector(scalar_indexed_const->output_dims()),\n-      scalar_indexed_const->shape());\n-}\n-\n-namespace {\n-\n-// Returns the non-contracting non-batch dimension (as per `contracting_dims`\n-// and `batch_dims`) if there is exactly one, otherwise returns nullopt.\n-std::optional<int64_t> GetOnlyNonContractingNonBatchDim(\n-    int64_t rank, absl::Span<const int64_t> contracting_dims,\n-    absl::Span<const int64_t> batch_dims) {\n-  std::optional<int64_t> result;\n-  for (int64_t dim = 0; dim < rank; dim++) {\n-    if (!absl::c_linear_search(contracting_dims, dim) &&\n-        !absl::c_linear_search(batch_dims, dim)) {\n-      if (result.has_value()) {\n-        return std::nullopt;\n-      }\n-      result = dim;\n-    }\n-  }\n-  return result;\n-}\n-\n-// Returns true if `indexed_array`, which is either the LHS or the RHS of a Dot\n-// HLO, can be folded into the dot operation.  For now these conditions are both\n-// necessary and sufficient.\n-//\n-// `tag` describes the caller.  Used only for logging.\n-//\n-// `contracting_dims` and `batch_dims` are the contracting and batch dimensions\n-// of whatever operand `indexed_array` is to the dot (LHS or RHS).\n-bool CanFoldDotIntoIndexedArray(\n-    absl::string_view tag, Analysis::ScalarIndexedConstantArray* indexed_array,\n-    absl::Span<const int64_t> contracting_dims,\n-    absl::Span<const int64_t> batch_dims) {\n-  std::optional<int64_t> non_contracting_non_batch_dim =\n-      GetOnlyNonContractingNonBatchDim(\n-          indexed_array->shape().dimensions().size(), contracting_dims,\n-          batch_dims);\n-  if (!non_contracting_non_batch_dim.has_value()) {\n-    VLOG(3) << tag << \": multiple or no non-contracting non-batch dimensions\";\n-    return false;\n-  }\n-\n-  if (indexed_array->output_dims().size() != 1 ||\n-      indexed_array->output_dims()[0] != *non_contracting_non_batch_dim) {\n-    VLOG(3) << tag << \": output dims != the lhs non-contracting non-batch dim\";\n-    return false;\n-  }\n-\n-  int64_t indexed_array_rank = indexed_array->shape().dimensions().size();\n-  if (indexed_array->source_dim() < (indexed_array_rank - 2)) {\n-    // This restriction can be lifted by inserting reshape nodes.\n-    VLOG(3) << tag\n-            << \": source dim is not in the low two dims, won't be able to form \"\n-               \"a matmul\";\n-    return false;\n-  }\n-\n-  return true;\n-}\n-\n-}  // namespace\n-\n-absl::StatusOr<Analysis::Array*>\n-IndexedArrayAnalysis::ComputeArrayForDotWithIndexedLhs(\n-    const Shape& shape, const DotDimensionNumbers& dim_numbers,\n-    const PrecisionConfig& precision_config, ScalarIndexedConstantArray* lhs,\n-    ConstantArray* rhs) {\n-  VLOG(3) << \"ComputeArrayForDotWithIndexedLhs(\" << ToString(lhs) << \" \"\n-          << ToString(rhs);\n-  if (!CanFoldDotIntoIndexedArray(\n-          \"ComputeArrayForDotWithIndexedLhs\", lhs, /*contracting_dims=*/\n-          dim_numbers.lhs_contracting_dimensions(),\n-          /*batch_dims=*/dim_numbers.lhs_batch_dimensions())) {\n-    return nullptr;\n-  }\n-\n-  int64_t lhs_rank = lhs->shape().dimensions().size();\n-  DotDimensionNumbers new_dim_numbers = dim_numbers;\n-  new_dim_numbers.set_lhs_contracting_dimensions(\n-      0, lhs->source_dim() == (lhs_rank - 1) ? (lhs_rank - 2) : (lhs_rank - 1));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      Literal * literal_for_new_source,\n-      TakeOwnership(HloEvaluator{}.EvaluateDotOp(\n-          new_dim_numbers, precision_config, lhs->literal(), *rhs->literal())));\n-\n-  // The new source dimension is wherever the non-batch non-contracting LHS\n-  // dimension \"went\".\n-  int64_t new_source_dim = dim_numbers.lhs_batch_dimensions_size() +\n-                           dim_numbers.rhs_batch_dimensions_size();\n-\n-  ConstantArray* new_source = Construct<ConstantArray>(literal_for_new_source);\n-  return Construct<ScalarIndexedConstantArray>(\n-      new_source, lhs->indices(), new_source_dim,\n-      SpanToVector(lhs->output_dims()), shape);\n-}\n-\n-absl::StatusOr<Analysis::Array*>\n-IndexedArrayAnalysis::ComputeArrayForDotWithIndexedRhs(\n-    const Shape& shape, const DotDimensionNumbers& dim_numbers,\n-    const PrecisionConfig& precision_config, ConstantArray* lhs,\n-    ScalarIndexedConstantArray* rhs) {\n-  VLOG(3) << \"ComputeArrayForDotWithIndexedRhs(\" << ToString(lhs) << \" \"\n-          << ToString(rhs);\n-  if (!CanFoldDotIntoIndexedArray(\n-          \"ComputeArrayForDotWithIndexedRhs\", rhs, /*contracting_dims=*/\n-          dim_numbers.rhs_contracting_dimensions(),\n-          /*batch_dims=*/dim_numbers.rhs_batch_dimensions())) {\n-    return nullptr;\n-  }\n-\n-  int64_t rhs_rank = rhs->shape().dimensions().size();\n-\n-  DotDimensionNumbers new_dim_numbers = dim_numbers;\n-  new_dim_numbers.set_rhs_contracting_dimensions(\n-      0, rhs->source_dim() == (rhs_rank - 1) ? (rhs_rank - 2) : (rhs_rank - 1));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      Literal * literal_for_new_source,\n-      TakeOwnership(HloEvaluator{}.EvaluateDotOp(\n-          new_dim_numbers, precision_config, *lhs->literal(), rhs->literal())));\n-\n-  // The new source dimension is wherever the non-batch non-contracting RHS\n-  // dimension \"went\".\n-  int64_t new_source_dim = dim_numbers.lhs_batch_dimensions_size() +\n-                           dim_numbers.rhs_batch_dimensions_size() + 1;\n-\n-  ConstantArray* new_source = Construct<ConstantArray>(literal_for_new_source);\n-  return Construct<ScalarIndexedConstantArray>(\n-      new_source, rhs->indices(), new_source_dim,\n-      SpanToVector(rhs->output_dims()), shape);\n-}\n-\n-absl::StatusOr<Analysis::Array*> IndexedArrayAnalysis::ComputeArrayForDot(\n-    const Shape& shape, const DotDimensionNumbers& dim_numbers,\n-    const PrecisionConfig& precision_config, Array* lhs, Array* rhs) {\n-  // Intuitively, if\n-  //\n-  //  - The LHS of a dot product is a gathered sequence of rows from a constant\n-  //    array (i.e. LHS[I,J] = Const[Indices[I],J]) and the RHS is a constant\n-  //\n-  //  OR\n-  //\n-  //  - If the RHS of a dot product is a gathered sequence of columns from a\n-  //    constant array (i.e. RHS[I,J] = Const[I, Indices[J]]) and the LHS is a\n-  //    constant\n-  //\n-  // then the result of the dot product itself is a gather from a constant\n-  // array.  E.g. Dot(LHS, ConstRhs) where LHS[I,J] = Const[Indices[I],J] can be\n-  // rewritten as Result where Result[I,J] = Dot(Const, ConstRhs)[Indices[I],\n-  // J].\n-  //\n-  // We do a general version of this rewrite here.\n-  VLOG(3) << \"ComputeArrayForDot(\" << ToString(lhs) << \" \" << ToString(rhs);\n-  if (auto* lhs_indexed_array =\n-          dynamic_cast<ScalarIndexedConstantArray*>(lhs)) {\n-    if (auto* rhs_constant = dynamic_cast<ConstantArray*>(rhs)) {\n-      return ComputeArrayForDotWithIndexedLhs(shape, dim_numbers,\n-                                              precision_config,\n-                                              lhs_indexed_array, rhs_constant);\n-    }\n-  }\n-\n-  if (auto* rhs_indexed_array =\n-          dynamic_cast<ScalarIndexedConstantArray*>(rhs)) {\n-    if (auto* lhs_constant = dynamic_cast<ConstantArray*>(lhs)) {\n-      return ComputeArrayForDotWithIndexedRhs(shape, dim_numbers,\n-                                              precision_config, lhs_constant,\n-                                              rhs_indexed_array);\n-    }\n-  }\n-\n-  return nullptr;\n-}\n-\n-absl::StatusOr<bool> IndexedArrayAnalysisPrinterPass::Run(\n-    HloModule* module,\n-    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  if (!VLOG_IS_ON(2)) {\n-    return false;\n-  }\n-\n-  IndexedArrayAnalysis analysis;\n-  for (auto* computation :\n-       module->MakeNonfusionComputations(execution_threads)) {\n-    for (auto* instr : computation->instructions()) {\n-      TF_ASSIGN_OR_RETURN(Analysis::Array * t, analysis.GetArrayFor(instr));\n-      if (!dynamic_cast<UnknownArray*>(t) && !dynamic_cast<ConstantArray*>(t)) {\n-        VLOG(2) << instr->ToString() << \"   ->   \" << analysis.ToString(t);\n-      }\n-    }\n-  }\n-\n-  return false;\n-}\n-\n-}  // namespace xla"
        },
        {
            "sha": "83bf625a3371202acc3eaa12cc223ca6fbf4e5a1",
            "filename": "third_party/xla/xla/hlo/analysis/indexed_array_analysis.h",
            "status": "removed",
            "additions": 0,
            "deletions": 395,
            "changes": 395,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f1d4574bd4e79987d6ffedebdde9694e42cbbe1/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f1d4574bd4e79987d6ffedebdde9694e42cbbe1/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis.h?ref=6f1d4574bd4e79987d6ffedebdde9694e42cbbe1",
            "patch": "@@ -1,395 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_HLO_ANALYSIS_INDEXED_ARRAY_ANALYSIS_H_\n-#define XLA_HLO_ANALYSIS_INDEXED_ARRAY_ANALYSIS_H_\n-\n-#include <type_traits>\n-\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/pass/hlo_pass_interface.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/statusor.h\"\n-\n-namespace xla {\n-\n-// IndexedArrayAnalysis decides if an HLO instruction can be rewritten as a\n-// gather from another array.  It does this by mapping HLO instructions to\n-// instances of IndexedArrayAnalysis::Array, which can be inspected to discover\n-// whether said HLO is equivalent to a gather.\n-class IndexedArrayAnalysis {\n- public:\n-  // IndexedArrayAnalysis maps each HLO instruction to an instance of a Array.\n-  // Array really just a sum type of the classes that inherit from it.  The\n-  // meaning of each of the subtypes is documented on the subtype declaration.\n-  //\n-  // Array instances are immutable once created.\n-  class Array {\n-   public:\n-    enum Kind {\n-      kUnknown,\n-      kConstant,\n-      kReshaped,\n-      kScalarIndexedConstant,\n-      kScalarIndexed\n-    };\n-\n-    virtual Kind kind() const = 0;\n-    virtual const Shape& shape() const = 0;\n-\n-    // Does a checked downcast from `Array` to `T` which must be one of its\n-    // subtypes.\n-    template <typename T>\n-    T* as() {\n-      static_assert((std::is_base_of<Array, T>::value),\n-                    \"target type not derived from source type\");\n-      // We skip the CHECK and hence the dynamic_cast if RTTI is disabled.\n-#if !defined(__GNUC__) || defined(__GXX_RTTI)\n-      CHECK_NE(dynamic_cast<T*>(this), nullptr);\n-#endif  // !defined(__GNUC__) || defined(__GXX_RTTI)\n-\n-      return static_cast<T*>(this);\n-    }\n-\n-    virtual ~Array() = default;\n-\n-    Array& operator=(const Array& other) = delete;\n-  };\n-\n-  // Represents an HLO instruction that was not analyzable by this\n-  // IndexedArrayAnalysis.  Instances of UnknownArray just wrap an existing\n-  // HloInstruction.\n-  class UnknownArray : public Array {\n-   public:\n-    Kind kind() const override { return kUnknown; }\n-    const Shape& shape() const override { return instruction().shape(); }\n-    const HloInstruction& instruction() const { return instruction_; }\n-\n-   private:\n-    explicit UnknownArray(const HloInstruction* instr) : instruction_(*instr) {}\n-\n-    const HloInstruction& instruction_;\n-\n-    friend class IndexedArrayAnalysis;\n-  };\n-\n-  // Represents a constant value.  This constant value may be present in the HLO\n-  // module being analyzed, or it could have been created on the fly by the\n-  // analysis.\n-  class ConstantArray : public Array {\n-   public:\n-    Kind kind() const override { return kConstant; }\n-    const Shape& shape() const override { return literal()->shape(); }\n-    const Literal* literal() const { return literal_; }\n-\n-   private:\n-    explicit ConstantArray(const Literal* literal) : literal_(literal) {}\n-    const Literal* literal_;\n-\n-    friend class IndexedArrayAnalysis;\n-  };\n-\n-  // Represents an Array that is a reshape of another Array.\n-  class ReshapedArray : public Array {\n-   public:\n-    Kind kind() const override { return kReshaped; }\n-\n-    // The array to reshape.\n-    Array* operand() const { return operand_; }\n-\n-    // The output shape.\n-    const Shape& shape() const override { return shape_; }\n-\n-   private:\n-    explicit ReshapedArray(Array* operand, Shape shape)\n-        : operand_(operand), shape_(shape) {}\n-\n-    Array* operand_;\n-    const Shape shape_;\n-\n-    friend class IndexedArrayAnalysis;\n-  };\n-\n-  // ---------------------------------------------------------------------------\n-  // Indexed Array Overview\n-  // ---------------------------------------------------------------------------\n-  //\n-  // ScalarIndexedArray and ScalarIndexedConstantArray form the core of this\n-  // analysis.  ScalarIndexedConstantArray is just a specialization of\n-  // ScalarIndexedArray so we will only discuss ScalarIndexedArray in this\n-  // overview.\n-  //\n-  // A ScalarIndexedArray represents an array that can be computed by indexing\n-  // into a \"source\" array using an \"indices\" tensor.  A simple example is a\n-  // gather operation gathering 12 rows out of a [100,100] matrix -- such an\n-  // operation will be represented by an instance of a ScalarIndexedArray with\n-  // the [100,100] matrix as the \"source\" array and the [12]-shaped indices\n-  // array as the \"indices\" tensor.  The ScalarIndexedArray operation itself\n-  // will be of shape [12,100] (assuming we were gathering with axis=0).\n-  //\n-  // Gather operations are not the only operation that maps to\n-  // ScalarIndexedArray instances (if that were true there would be little point\n-  // in having a separate analysis).  We can often infer ScalarIndexedArrays for\n-  // other operations too.  For instance, consider:\n-  //\n-  //   %source = f32[100,100] constant\n-  //   %indices = s32[12] ...\n-  //   %gather = f32[12,100] ... gather from %source using %indices at axis 0\n-  //   %dot = dot(%gather, other_constant) [canonical contracting dims]\n-  //\n-  // The dot operation itself is also a ScalarIndexedArray with source =\n-  // dot(constant, other_constant) and indices = %indices.  A reshape of %gather\n-  // to [12,5,20] too is a ScalarIndexedArray with source = an appropriately\n-  // reshaped constant and indices = %indices.\n-\n-  // Represents the result of a gather operation.  This gather operation may\n-  // explicitly be present in the HLO module being analyzed, or it could have\n-  // been created on the fly by the analysis.\n-  //\n-  // An instance of ScalarIndexedArray represents a array whose I'th element can\n-  // be mapped to the J'th element of the `source` array (where I and J are\n-  // multidimensional indices) in this way:\n-  //\n-  //   I' = remove components at positions `output_dims` from I\n-  //   G' = remove components not at positions `output_dims` from I\n-  //   T  = indices[G']\n-  //   J  = I' with T inserted at position `source_dim`\n-  //\n-  // For example, if source is of shape [11,13,17,19], indices is of shape\n-  // [23,29], output_dims is [0,2] and source_dim is 2 then the output is of\n-  // shape [23,11,29,13,19] and the output index [A,B,C,D,E] is mapped to the\n-  // input index [B,D,indices[A,C],E].\n-  class ScalarIndexedArray : public Array {\n-   public:\n-    Kind kind() const override { return kScalarIndexed; }\n-    const Shape& shape() const override { return shape_; }\n-\n-    Array* source() const { return source_; }\n-    Array* indices() const { return indices_; }\n-\n-    // `source_dim` is the dimension in the source array that is being indexed\n-    // over using indices from the `indices` array.  See the class documentation\n-    // and the overview for more details.\n-    int64_t source_dim() const { return source_dim_; }\n-\n-    // `output_dims` are the dimensions in the output array that are being used\n-    // to compute an index into the `indices` array.  See the class\n-    // documentation and the overview for more details.\n-    absl::Span<const int64_t> output_dims() const { return output_dims_; }\n-\n-   private:\n-    explicit ScalarIndexedArray(Array* source, Array* indices,\n-                                int64_t source_dim,\n-                                std::vector<int64_t> output_dims, Shape shape)\n-        : source_(source),\n-          indices_(indices),\n-          source_dim_(source_dim),\n-          output_dims_(std::move(output_dims)),\n-          shape_(std::move(shape)) {}\n-\n-    Array* source_;\n-    Array* indices_;\n-    int64_t source_dim_;\n-    std::vector<int64_t> output_dims_;\n-    Shape shape_;\n-\n-    friend class IndexedArrayAnalysis;\n-  };\n-\n-  // A ScalarIndexedConstantArray is just a ScalarIndexedArray constrained to\n-  // have a ConstantArray instance as the source.  This is an ergonomic\n-  // concession -- in theory it is possible to just keep ScalarIndexedArray and\n-  // check source()->kind().\n-  class ScalarIndexedConstantArray : public ScalarIndexedArray {\n-   public:\n-    Kind kind() const override { return kScalarIndexedConstant; }\n-\n-    const Literal& literal() const {\n-      return *source()->as<ConstantArray>()->literal();\n-    }\n-\n-   private:\n-    explicit ScalarIndexedConstantArray(Array* source, Array* indices,\n-                                        int64_t source_dim,\n-                                        std::vector<int64_t> output_dims,\n-                                        Shape shape)\n-        : ScalarIndexedArray(source, indices, source_dim,\n-                             std::move(output_dims), std::move(shape)) {\n-      CHECK(dynamic_cast<ConstantArray*>(source));\n-    }\n-\n-    friend class IndexedArrayAnalysis;\n-  };\n-\n-  // Returns an Array instance for `instr`.  The IndexedArrayAnalysis instance\n-  // keeps ownership of the returned Array instance.\n-  //\n-  // Caching Behavior: IndexedArrayAnalysis has a cache mapping HLO\n-  // instructions to IndexedArrayAnalysis::Array instances.  This entire cache\n-  // becomes stale and may cause the analysis to return incorrect results if any\n-  // transitive operand (stopping at the containing computation) is modified for\n-  // any HLO instruction on which GetArrayFor has been invoked.\n-  //\n-  // NB!  By inspecting the implementation, you may be able to infer a stronger\n-  // caching guarantee than what is mentioned above.  Nevertheless, what is\n-  // stated above is the contract.\n-  absl::StatusOr<Array*> GetArrayFor(const HloInstruction* instr);\n-\n-  // Pretty-prints the expression rooted at `root`.\n-  std::string ToString(Array* root, bool print_constants = false);\n-\n- private:\n-  // Helper function that ensures that every HLO instruction that is\n-  // transitively used by `root` has an entry in `cache_`.\n-  absl::Status TraverseAndPopulateCache(const HloInstruction* root);\n-\n-  // Creates an Array instance for `instr` under the assumption that all\n-  // operations of `instr` are present in `cache_`.\n-  absl::StatusOr<Array*> ComputeArrayFor(const HloInstruction* instr);\n-\n-  absl::StatusOr<Array*> ComputeArrayForConstant(const Literal& literal);\n-\n-  absl::StatusOr<Array*> ComputeArrayForGather(\n-      const Shape& shape, const GatherDimensionNumbers& dim_numbers,\n-      absl::Span<const int64_t> slice_sizes, Array* source, Array* indices);\n-\n-  absl::StatusOr<Array*> ComputeArrayForDotWithIndexedLhs(\n-      const Shape& shape, const DotDimensionNumbers& dim_numbers,\n-      const PrecisionConfig& precision_config, ScalarIndexedConstantArray* lhs,\n-      ConstantArray* rhs);\n-\n-  absl::StatusOr<Array*> ComputeArrayForDotWithIndexedRhs(\n-      const Shape& shape, const DotDimensionNumbers& dim_numbers,\n-      const PrecisionConfig& precision_config, ConstantArray* lhs,\n-      ScalarIndexedConstantArray* rhs);\n-\n-  absl::StatusOr<Array*> ComputeArrayForDot(\n-      const Shape& shape, const DotDimensionNumbers& dim_numbers,\n-      const PrecisionConfig& precision_config, Array* lhs, Array* rhs);\n-\n-  // This tries to fold a ScalarIndexedArray which has another\n-  // ScalarIndexedArray as a source into a ScalarIndexedArray that instead has a\n-  // ScalarIndexedArray as indices.  If `source` happened to be a\n-  // ScalarIndexedConstantArray this can result in an expression that is more\n-  // canonical.\n-  //\n-  // As an example, consider a gather operation, G0, gathering 7 elements from\n-  // an array \"Arr\" of shape [100] resulting in an array of shape [7], and a\n-  // second gather operation, G1, which gathers 3 elements out of the result of\n-  // G0 resulting in an array of shape [3].  Let the indices uses by G0 be I0\n-  // (of shape [7]) and the indices used by G1 be I1 (of shape [3]).  We can\n-  // instead rewrite G1 to gather directly from \"Arr\" with the three indices\n-  // from I0 as per I1.  In other words, we can rewrite:\n-  //\n-  //    G0 = [Arr[i] for i in I0]\n-  //    G1 = [G0[i]  for i in I1]\n-  //\n-  // into\n-  //\n-  //    I2 = [I0[i]  for i in I1]\n-  //    G1 = [Arr[i] for i in I2]\n-  absl::StatusOr<ScalarIndexedArray*> FoldGatherOfGather(\n-      ScalarIndexedArray* source, Array* indices, int64_t source_dim,\n-      absl::Span<const int64_t> output_dims, Shape shape);\n-\n-  // Reshapes a scalar-indexed node to remove the degenerate dimensions in its\n-  // output.  The result is always a scalar-indexed node.\n-  absl::StatusOr<ScalarIndexedArray*> ReshapeToRemoveDegenerateDims(\n-      ScalarIndexedArray* operand);\n-\n-  // Reshapes a scalar-indexed node such that the result has the degenerate\n-  // dimensions `degenerate_dims`.  The result is always a scalar-indexed node.\n-  absl::StatusOr<ScalarIndexedArray*> ReshapeToAddDegenerateDims(\n-      ScalarIndexedArray* operand, absl::Span<const int64_t> degenerate_dims);\n-\n-  absl::StatusOr<ScalarIndexedArray*> FoldReshapeOfGather(\n-      const Shape& shape, ScalarIndexedConstantArray* operand);\n-  absl::StatusOr<ScalarIndexedArray*> FoldReshapeOfGatherNoDegenerateDims(\n-      const Shape& shape, ScalarIndexedConstantArray* scalar_indexed);\n-  absl::StatusOr<Array*> ComputeArrayForReshape(const Shape& shape,\n-                                                Array* operand);\n-\n-  absl::StatusOr<Array*> ComputeArrayForElementwiseBinaryOp(HloOpcode opcode,\n-                                                            Array* lhs,\n-                                                            Array* rhs);\n-  absl::StatusOr<Array*> ComputeArrayForElementwiseUnaryOp(HloOpcode opcode,\n-                                                           Array* operand);\n-\n-  template <typename T, typename... Args>\n-  T* Construct(Args&&... args) {\n-    T* new_tensor = new T(std::forward<Args>(args)...);\n-    owned_tensors_.push_back(std::unique_ptr<T>(new_tensor));\n-    return new_tensor;\n-  }\n-\n-  ScalarIndexedArray* ConstructScalarIndexedArray(\n-      Array* source, Array* indices, int64_t source_dim,\n-      std::vector<int64_t> output_dims, Shape shape) {\n-    if (source->kind() == Array::kConstant) {\n-      return Construct<ScalarIndexedConstantArray>(source, indices, source_dim,\n-                                                   std::move(output_dims),\n-                                                   std::move(shape));\n-    } else {\n-      return Construct<ScalarIndexedArray>(source, indices, source_dim,\n-                                           std::move(output_dims),\n-                                           std::move(shape));\n-    }\n-  }\n-\n-  Literal* TakeOwnership(Literal literal) {\n-    owned_literals_.push_back(std::move(literal));\n-    return &owned_literals_.back();\n-  }\n-\n-  absl::StatusOr<Literal*> TakeOwnership(\n-      absl::StatusOr<Literal> literal_or_error) {\n-    TF_ASSIGN_OR_RETURN(Literal literal, std::move(literal_or_error));\n-    owned_literals_.push_back(std::move(literal));\n-    return &owned_literals_.back();\n-  }\n-\n-  std::vector<std::unique_ptr<Array>> owned_tensors_;\n-  std::vector<Literal> owned_literals_;\n-  absl::flat_hash_map<const HloInstruction*, Array*> cache_;\n-};\n-\n-// A pass that prints all non-trivial results returned by IndexedArrayAnalysis.\n-// This pass is a no-op if !VLOG_IS_ON(2) so it should be fine to\n-// unconditionally add to the regular HLO pass pipeline.\n-class IndexedArrayAnalysisPrinterPass : public HloModulePass {\n- public:\n-  absl::string_view name() const override {\n-    return \"indexed-array-analysis-printer-pass\";\n-  }\n-  using HloPassInterface::Run;\n-  absl::StatusOr<bool> Run(\n-      HloModule* module,\n-      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n-};\n-\n-}  // namespace xla\n-\n-#endif  // XLA_HLO_ANALYSIS_INDEXED_ARRAY_ANALYSIS_H_"
        },
        {
            "sha": "574a487c330e1ef0e40ccd37f462b190fef7ee5e",
            "filename": "third_party/xla/xla/hlo/analysis/indexed_array_analysis_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 974,
            "changes": 974,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f1d4574bd4e79987d6ffedebdde9694e42cbbe1/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f1d4574bd4e79987d6ffedebdde9694e42cbbe1/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexed_array_analysis_test.cc?ref=6f1d4574bd4e79987d6ffedebdde9694e42cbbe1",
            "patch": "@@ -1,974 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/hlo/analysis/indexed_array_analysis.h\"\n-\n-#include <memory>\n-#include <string>\n-\n-#include <gtest/gtest.h>\n-#include \"absl/log/log.h\"\n-#include \"absl/strings/ascii.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"tsl/platform/statusor.h\"\n-\n-namespace xla {\n-namespace {\n-class IndexedArrayAnalysisTest : public HloHardwareIndependentTestBase {\n- protected:\n-  void AssertArrayForRootExpressionIs(const std::string& hlo_text,\n-                                      const std::string& root_expression) {\n-    AssertArrayForRootExpressionIsImpl(hlo_text, root_expression,\n-                                       /*print_constants=*/false);\n-  }\n-\n-  void AssertArrayWithConstantsForRootExpressionIs(\n-      const std::string& hlo_text, const std::string& root_expression) {\n-    AssertArrayForRootExpressionIsImpl(hlo_text, root_expression,\n-                                       /*print_constants=*/true);\n-  }\n-\n- private:\n-  // Replaces sequences of whitespace with a single space.  This makes the\n-  // strings being matched against \"whitespace insensitive\" which lets us indent\n-  // them for readability.\n-  std::string CanonicalizeWhitespace(const std::string& text) {\n-    std::string result;\n-\n-    for (char c : text) {\n-      if (!absl::ascii_isspace(c)) {\n-        result.push_back(c);\n-      } else if (!result.empty() && result.back() != ' ') {\n-        result.push_back(' ');\n-      }\n-    }\n-\n-    while (!result.empty() && result.back() == ' ') {\n-      result.pop_back();\n-    }\n-\n-    return result;\n-  }\n-\n-  void AssertArrayForRootExpressionIsImpl(const std::string& hlo_text,\n-                                          const std::string& root_expression,\n-                                          bool print_constants) {\n-    IndexedArrayAnalysis indexed_tensor_analysis;\n-    TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> m,\n-                            ParseAndReturnVerifiedModule(hlo_text));\n-\n-    TF_ASSERT_OK_AND_ASSIGN(IndexedArrayAnalysis::Array* const array_result,\n-                            indexed_tensor_analysis.GetArrayFor(\n-                                m->entry_computation()->root_instruction()));\n-    std::string string_result = CanonicalizeWhitespace(\n-        indexed_tensor_analysis.ToString(array_result, print_constants));\n-    LOG(INFO) << string_result;\n-    ASSERT_EQ(string_result, CanonicalizeWhitespace(root_expression));\n-  }\n-};\n-\n-TEST_F(IndexedArrayAnalysisTest, SimpleOneToOneGather) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,3] parameter(0)\n-  indices = s32[5] parameter(1)\n-  ROOT gather = s32[5,3] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,3}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text,\n-                                 \"(scalar-indexed %operand %indices 0->[0])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, SimpleOneToOneConstantGather) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,3] constant({{1,2,3},{1,2,3},{1,2,3}})\n-  indices = s32[5] parameter(0)\n-  ROOT gather = s32[5,3] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,3}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(\n-      hlo_text, \"(scalar-indexed-const (constant s32[3,3]) %indices 0->[0])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherIsNotScalarIndexed0) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,3] constant({{1,2,3},{1,2,3},{1,2,3}})\n-  indices = s32[5,2] parameter(0)\n-  ROOT gather = s32[5] gather(operand, indices),\n-      offset_dims={},\n-      collapsed_slice_dims={0,1},\n-      start_index_map={0,1},\n-      index_vector_dim=1,\n-      slice_sizes={1,1}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, \"%gather\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherIsNotScalarIndexed1) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,3,1] parameter(0)\n-  indices = s32[5] parameter(1)\n-  ROOT gather = s32[5,3] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0,2},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,3,1}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, \"%gather\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherIsNotScalarIndexed2) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,3,1] parameter(0)\n-  indices = s32[5] parameter(1)\n-  ROOT gather = s32[5,2,3] gather(operand, indices),\n-      offset_dims={1,2},\n-      collapsed_slice_dims={2},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={2,3,1}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, \"%gather\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherIsNotScalarIndexed3) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,3] parameter(0)\n-  indices = s32[5] parameter(1)\n-  ROOT gather = s32[5,2] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,2}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, \"%gather\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherOfGather_OneToOne) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,3] constant({{1,2,3},{1,2,3},{1,2,3}})\n-  indices_a = s32[5] parameter(0)\n-  indices_b = s32[2] parameter(1)\n-  gather_a = s32[5,3] gather(operand, indices_a),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,3}\n-  ROOT gather_b = s32[2,3] gather(gather_a, indices_b),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,3}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(\n-      hlo_text,\n-      \"(scalar-indexed-const (constant s32[3,3]) (scalar-indexed %indices_a \"\n-      \"%indices_b 0->[0]) 0->[0])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherOfGather_ManyToOneWithOneToOne) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,2] parameter(0)\n-  indices_a = s32[5,7] parameter(1)\n-  indices_b = s32[2] parameter(2)\n-  gather_a = s32[5,3,7] gather(operand, indices_a),\n-      offset_dims={1},\n-      collapsed_slice_dims={1},\n-      start_index_map={1},\n-      index_vector_dim=2,\n-      slice_sizes={3,1}\n-  ROOT gather_b = s32[5,3,2] gather(gather_a, indices_b),\n-      offset_dims={0,1},\n-      collapsed_slice_dims={2},\n-      start_index_map={2},\n-      index_vector_dim=1,\n-      slice_sizes={5,3,1}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text,\n-                                 \"(scalar-indexed %operand (scalar-indexed \"\n-                                 \"%indices_a %indices_b 1->[1]) 1->[0,2])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherOfGather_OneToOneWithManyToOne) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,6] parameter(0)\n-  indices_a = s32[2] parameter(1)\n-  indices_b = s32[5,7] parameter(2)\n-  gather_a = s32[2,6] gather(operand, indices_a),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,6}\n-  ROOT gather_b = s32[5,6,7] gather(gather_a, indices_b),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=2,\n-      slice_sizes={1,6}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text,\n-                                 \"(scalar-indexed %operand (scalar-indexed \"\n-                                 \"%indices_a %indices_b 0->[0,1]) 0->[0,2])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, GatherOfGather_ManyToOneWithManyToOne) {\n-  std::string hlo_text = R\"(\n-HloModule SimpleGather\n-\n-ENTRY main {\n-  operand = s32[3,2] parameter(0)\n-  indices_a = s32[5,7] parameter(1)\n-  indices_b = s32[4,8] parameter(2)\n-  gather_a = s32[5,3,7] gather(operand, indices_a),\n-      offset_dims={1},\n-      collapsed_slice_dims={1},\n-      start_index_map={1},\n-      index_vector_dim=2,\n-      slice_sizes={3,1}\n-  ROOT gather_b = s32[4,5,3,8] gather(gather_a, indices_b),\n-      offset_dims={1,2},\n-      collapsed_slice_dims={2},\n-      start_index_map={2},\n-      index_vector_dim=2,\n-      slice_sizes={5,3,1}\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(\n-      hlo_text,\n-      \"(scalar-indexed %operand (scalar-indexed %indices_a %indices_b \"\n-      \"1->[0,2]) 1->[0,1,3])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather0) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[3,4] constant({{1,2,3,4},{1,2,3,4},{1,2,3,4}})\n-  indices = s32[5] parameter(0)\n-  gather = s32[5,4] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT reshape = s32[5,2,2] reshape(gather)\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(\n-      hlo_text, \"(scalar-indexed-const (constant s32[3,2,2]) %indices 0->[0])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather1) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[3,4] constant({{1,2,3,4},{1,2,3,4},{1,2,3,4}})\n-  indices = s32[5,7] parameter(0)\n-  gather = s32[5,4,7] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=2,\n-      slice_sizes={1,4}\n-  ROOT reshape = s32[5,2,2,7] reshape(gather)\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(\n-      hlo_text,\n-      \"(scalar-indexed-const (constant s32[3,2,2]) %indices 0->[0,3])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather2) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[3,2,6] constant({\n-      {{1,2,3,4,5,6},{1,2,3,4,5,6}},\n-      {{1,2,3,4,5,6},{1,2,3,4,5,6}},\n-      {{1,2,3,4,5,6},{1,2,3,4,5,6}}})\n-  indices = s32[5,7] parameter(0)\n-  gather = s32[5,2,6,7] gather(operand, indices),\n-      offset_dims={1,2},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=2,\n-      slice_sizes={1,2,6}\n-  ROOT reshape = s32[5,3,4,7] reshape(gather)\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(\n-      hlo_text,\n-      \"(scalar-indexed-const (constant s32[3,3,4]) %indices 0->[0,3])\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather3) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[2,6] constant({\n-      {1,2,3,4,5,6},{1,2,3,4,5,6}})\n-  indices = s32[1] parameter(0)\n-  gather = s32[1,6] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,6}\n-  ROOT reshape = s32[1,1,6] reshape(gather)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(scalar-indexed-const\n-  (constant s32[2,1,1,6])\n-  (reshape %indices to s32[])\n-  0->[])\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather4) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[2,3]{1,0} constant({ { 1, 2, 3 }, { 1, 2, 3 } })\n-\n-  i.0 = s64[1,3]{1,0} parameter(0)\n-  g.0 = s32[1,3,3]{2,1,0} gather(operand, i.0), offset_dims={2},\n-    collapsed_slice_dims={0}, start_index_map={0},\n-    index_vector_dim=2, slice_sizes={1,3}\n-\n-  i.1 = s64[1] parameter(1)\n-  g.1 = s32[1,1,3]{2,1,0} gather(g.0, i.1), offset_dims={0,2},\n-    collapsed_slice_dims={1}, start_index_map={1},\n-    index_vector_dim=1, slice_sizes={1,1,3}\n-\n-  ROOT reshape = s32[1,3]{1,0} reshape(g.1)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(scalar-indexed-const\n-  (constant s32[2,1,3])\n-   (reshape\n-     (scalar-indexed %i.0 %i.1 1->[1])\n-     to s64[])\n-  0->[])\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather5) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[1,6] constant({{1,2,3,4,5,6}})\n-  indices = s32[1] parameter(0)\n-  gather = s32[1,6] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,6}\n-  ROOT reshape = s32[1,1,6] reshape(gather)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(scalar-indexed-const\n-  (constant s32[1,1,1,6])\n-  (reshape %indices to s32[])\n-  0->[])\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather6) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[1,2,6] constant({{\n-      {1,2,3,4,5,6},{1,2,3,4,5,6}}})\n-  indices = s32[1] parameter(0)\n-  gather = s32[1,1,6] gather(operand, indices),\n-      offset_dims={1,2},\n-      collapsed_slice_dims={1},\n-      start_index_map={1},\n-      index_vector_dim=1,\n-      slice_sizes={1,1,6}\n-  ROOT reshape = s32[1,1,1,6] reshape(gather)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(scalar-indexed-const\n-  (constant s32[2,1,1,1,6] s32[2,1,1,1,6] {\n-    { /*i0=0*/ { /*i1=0*/ { /*i2=0*/ { 1, 2, 3, 4, 5, 6 } } } },\n-    { /*i0=1*/ { /*i1=0*/ { /*i2=0*/ { 1, 2, 3, 4, 5, 6 } } } } })\n-  (reshape %indices to s32[])\n-  0->[])\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text,\n-                                              expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGather7) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[2,6] constant({\n-      {1,2,3,4,5,6},{1,2,3,4,5,6}})\n-  indices = s32[1,5] parameter(0)\n-  gather = s32[1,5,6] gather(operand, indices),\n-      offset_dims={2},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=2,\n-      slice_sizes={1,6}\n-  ROOT reshape = s32[1,1,5,6] reshape(gather)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(scalar-indexed-const\n-  (constant s32[2,1,1,6] s32[2,1,1,6] {\n-    { /*i0=0*/ { /*i1=0*/ { 1, 2, 3, 4, 5, 6 } } },\n-    { /*i0=1*/ { /*i1=0*/ { 1, 2, 3, 4, 5, 6 } } } })\n-  (reshape %indices to s32[5])\n-  0->[2])\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text,\n-                                              expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGatherNoFold0) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[3,4] constant({{1,2,3,4},{1,2,3,4},{1,2,3,4}})\n-  indices = s32[5,6] parameter(0)\n-  gather = s32[5,4,6] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=2,\n-      slice_sizes={1,4}\n-  ROOT reshape = s32[5,2,2,2,3] reshape(gather)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(reshape\n-  (scalar-indexed-const\n-    (constant s32[3,4])\n-    %indices\n-    0->[0,2])\n-  to s32[5,2,2,2,3])\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGatherNoFold1) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[3,5,2] constant({\n-      {{1,2},{3,4},{5,6},{7,8},{9,10}},\n-      {{1,2},{3,4},{5,6},{7,8},{9,10}},\n-      {{1,2},{3,4},{5,6},{7,8},{9,10}}})\n-  indices = s32[7] parameter(0)\n-  gather = s32[3,2,7] gather(operand, indices),\n-      offset_dims={0,1},\n-      collapsed_slice_dims={1},\n-      start_index_map={1},\n-      index_vector_dim=1,\n-      slice_sizes={3,1,2}\n-  ROOT reshape = s32[6,7] reshape(gather)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(reshape\n-  (scalar-indexed-const\n-    (constant s32[3,5,2])\n-    %indices\n-    1->[2])\n-  to s32[6,7])\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, ReshapeOfGatherNoFold2) {\n-  std::string hlo_text = R\"(\n-HloModule ReshapeOfGather\n-\n-ENTRY main {\n-  operand = s32[3,4,1] constant({\n-    {{1},{2},{3},{4}},\n-    {{1},{2},{3},{4}},\n-    {{1},{2},{3},{4}}})\n-  indices = s32[5,6] parameter(0)\n-  gather = s32[5,4,6,1] gather(operand, indices),\n-      offset_dims={1,3},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=2,\n-      slice_sizes={1,4,1}\n-  ROOT reshape = s32[5,2,2,2,3,1] reshape(gather)\n-}\n-)\";\n-\n-  const char* expected_root_expression = R\"(\n-(reshape\n-  (scalar-indexed-const\n-    (constant s32[3,4,1])\n-    %indices\n-    0->[0,2])\n-  to s32[5,2,2,2,3,1])\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, expected_root_expression);\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, UnaryOpOfGather) {\n-  std::string hlo_text = R\"(\n-HloModule UnaryOpOfGather\n-\n-ENTRY main {\n-  operand = f32[3,4] constant({{1,2,3,4},{1,3,2,4},{4,3,2,1}})\n-  indices = s32[5] parameter(0)\n-  gather = f32[5,4] gather(operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT tanh = f32[5,4] tanh(gather)\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const (constant f32[3,4] f32[3,4] {\n-  { 0.761594176, 0.964027584, 0.995054781, 0.999329329 },\n-  { 0.761594176, 0.995054781, 0.964027584, 0.999329329 },\n-  { 0.999329329, 0.995054781, 0.964027584, 0.761594176 }\n-}) %indices 0->[0]))\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, AddBroadcastedScalarWithGather) {\n-  std::string hlo_text = R\"(\n-HloModule AddBroadcastedScalarWithGather\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{1,3,2,4},{4,3,2,1}})\n-  constant = s32[] constant(5)\n-  constant_broadcasted = s32[5,4] broadcast(constant), dimensions={}\n-  indices = s32[5] parameter(0)\n-  gather = s32[5,4] gather(gather_operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT add = s32[5,4] add(gather, constant_broadcasted)\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const (constant s32[3,4] s32[3,4] {\n-  { 6, 7, 8, 9 },\n-  { 6, 8, 7, 9 },\n-  { 9, 8, 7, 6 }\n-}) %indices 0->[0]))\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest,\n-       SubtractBroadcastedScalarWithGather_GatherIsLhs) {\n-  std::string hlo_text = R\"(\n-HloModule SubtractBroadcastedScalarWithGather\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{1,3,2,4},{4,3,2,1}})\n-  constant = s32[] constant(5)\n-  constant_broadcasted = s32[5,4] broadcast(constant), dimensions={}\n-  indices = s32[5] parameter(0)\n-  gather = s32[5,4] gather(gather_operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT sub = s32[5,4] subtract(gather, constant_broadcasted)\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const (constant s32[3,4] s32[3,4] {\n-  { -4, -3, -2, -1 },\n-  { -4, -2, -3, -1 },\n-  { -1, -2, -3, -4 }\n-}) %indices 0->[0]))\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest,\n-       SubtractBroadcastedScalarWithGather_GatherIsRhs) {\n-  std::string hlo_text = R\"(\n-HloModule SubtractBroadcastedScalarWithGather\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{1,3,2,4},{4,3,2,1}})\n-  constant = s32[] constant(5)\n-  constant_broadcasted = s32[5,4] broadcast(constant), dimensions={}\n-  indices = s32[5] parameter(0)\n-  gather = s32[5,4] gather(gather_operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT sub = s32[5,4] subtract(constant_broadcasted, gather)\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const (constant s32[3,4] s32[3,4] {\n-  { 4, 3, 2, 1 },\n-  { 4, 2, 3, 1 },\n-  { 1, 2, 3, 4 }\n-}) %indices 0->[0]))\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, AddBroadcastedVectorWithGather) {\n-  std::string hlo_text = R\"(\n-HloModule AddBroadcastedVectorWithGather\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{1,3,2,4},{4,3,2,1}})\n-  constant_vect = s32[4] constant({10,11,12,13})\n-  constant_broadcasted = s32[5,4] broadcast(constant_vect), dimensions={1}\n-  indices = s32[5] parameter(0)\n-  gather = s32[5,4] gather(gather_operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT add = s32[5,4] add(gather, constant_broadcasted)\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const (constant s32[3,4] s32[3,4] {\n-  { 11, 13, 15, 17 },\n-  { 11, 14, 14, 17 },\n-  { 14, 14, 14, 14 }\n-}) %indices 0->[0]))\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, AddBroadcastedVectorWithGather_Negative) {\n-  std::string hlo_text = R\"(\n-HloModule AddBroadcastedVectorWithGather\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{1,3,2,4},{4,3,2,1}})\n-  constant_vect = s32[5] constant({10,11,12,13,14})\n-  constant_broadcasted = s32[5,4] broadcast(constant_vect), dimensions={0}\n-  indices = s32[5] parameter(0)\n-  gather = s32[5,4] gather(gather_operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT add = s32[5,4] add(gather, constant_broadcasted)\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, \"%add\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, RegularUnaryOp) {\n-  std::string hlo_text = R\"(\n-HloModule RegularUnaryOp\n-\n-ENTRY main {\n-  input = f32[100] parameter(0)\n-  ROOT tanh = f32[100] tanh(input)\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, \"%tanh\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, RegularBinaryOp) {\n-  std::string hlo_text = R\"(\n-HloModule RegularUnaryOp\n-\n-ENTRY main {\n-  input0 = f32[100] parameter(0)\n-  input1 = f32[100] parameter(1)\n-  ROOT add = f32[100] add(input0, input1)\n-}\n-)\";\n-\n-  AssertArrayForRootExpressionIs(hlo_text, \"%add\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, DotOpBasic_0) {\n-  std::string hlo_text = R\"(\n-HloModule DotOp\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{5,6,7,8},{9,10,11,12}})\n-  dot_rhs_constant = s32[4,3] constant({{1,2,3},{4,5,6},{7,8,9},{10,11,12}})\n-  indices = s32[5] parameter(0)\n-  dot_lhs = s32[5,4] gather(gather_operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,4}\n-  ROOT dot = s32[5,3] dot(dot_lhs, dot_rhs_constant), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const\n-  (constant s32[3,3] s32[3,3] {\n-    { 70, 80, 90 },\n-    { 158, 184, 210 },\n-    { 246, 288, 330 } })\n-  %indices 0->[0]))\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, DotOpBasic_1) {\n-  std::string hlo_text = R\"(\n-HloModule DotOp\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{5,6,7,8},{9,10,11,12}})\n-  dot_rhs_constant = s32[3,3] constant({{1,2,3},{4,5,6},{7,8,9}})\n-  indices = s32[5] parameter(0)\n-  dot_lhs = s32[3,5] gather(gather_operand, indices),\n-      offset_dims={0},\n-      collapsed_slice_dims={1},\n-      start_index_map={1},\n-      index_vector_dim=1,\n-      slice_sizes={3,1}\n-  ROOT dot = s32[5,3] dot(dot_lhs, dot_rhs_constant), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const\n-  (constant s32[4,3] s32[4,3] {\n-    { 84, 99, 114 },\n-    { 96, 114, 132 },\n-    { 108, 129, 150 },\n-    { 120, 144, 168 } })\n-   %indices 0->[1]))\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, DotOpBasic_2) {\n-  std::string hlo_text = R\"(\n-HloModule DotOp\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{5,6,7,8},{9,10,11,12}})\n-  dot_lhs_constant = s32[4,3] constant({{1,2,3},{4,5,6},{7,8,9},{10,11,12}})\n-  indices = s32[5] parameter(0)\n-  dot_rhs = s32[3,5] gather(gather_operand, indices),\n-      offset_dims={0},\n-      collapsed_slice_dims={1},\n-      start_index_map={1},\n-      index_vector_dim=1,\n-      slice_sizes={3,1}\n-  ROOT dot = s32[4,5] dot(dot_lhs_constant, dot_rhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const\n-  (constant s32[4,4] s32[4,4] {\n-    { 38, 44, 50, 56 },\n-    { 83, 98, 113, 128 },\n-    { 128, 152, 176, 200 },\n-    { 173, 206, 239, 272 } })\n-  %indices 1->[1])\n-)\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, DotOpBasic_3) {\n-  std::string hlo_text = R\"(\n-HloModule DotOp\n-\n-ENTRY main {\n-  gather_operand = s32[4,3] constant({{1,2,3},{4,5,6},{7,8,9},{10,11,12}})\n-  dot_lhs_constant = s32[4,3] constant({{1,2,3},{4,5,6},{7,8,9},{10,11,12}})\n-  indices = s32[5] parameter(0)\n-  dot_rhs = s32[5,3] gather(gather_operand, indices),\n-      offset_dims={1},\n-      collapsed_slice_dims={0},\n-      start_index_map={0},\n-      index_vector_dim=1,\n-      slice_sizes={1,3}\n-  ROOT dot = s32[4,5] dot(dot_lhs_constant, dot_rhs), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const\n-  (constant s32[4,4] s32[4,4] {\n-    { 14, 32, 50, 68 },\n-    { 32, 77, 122, 167 },\n-    { 50, 122, 194, 266 },\n-    { 68, 167, 266, 365 } })\n-  %indices 1->[0])\n-)\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, DotOpWithBatch) {\n-  std::string hlo_text = R\"(\n-HloModule DotOp\n-\n-ENTRY main {\n-  gather_operand = s32[2,3,2] constant({{{1,2},{3,4},{5,6}},{{7,8},{9,10},{11,12}}})\n-  dot_lhs_constant = s32[2,2,3] constant({{{1,2,3},{4,5,6}},{{7,8,9},{10,11,12}}})\n-  indices = s32[4] parameter(0)\n-  dot_rhs = s32[2,3,4] gather(gather_operand, indices),\n-      offset_dims={0,1},\n-      collapsed_slice_dims={2},\n-      start_index_map={2},\n-      index_vector_dim=1,\n-      slice_sizes={2,3,1}\n-  ROOT dot = s32[2,2,4] dot(dot_lhs_constant, dot_rhs),\n-      lhs_contracting_dims={2}, rhs_contracting_dims={1},\n-      lhs_batch_dims={0}, rhs_batch_dims={0}\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, R\"(\n-(scalar-indexed-const\n-  (constant s32[2,2,2] s32[2,2,2] {\n-    { { 22, 28 },\n-      { 49, 64 } },\n-    { { 220, 244 },\n-      { 301, 334 } } })\n-  %indices 3->[2])\n-)\");\n-}\n-\n-TEST_F(IndexedArrayAnalysisTest, DotOpNegative) {\n-  std::string hlo_text = R\"(\n-HloModule DotOp\n-\n-ENTRY main {\n-  gather_operand = s32[3,4] constant({{1,2,3,4},{5,6,7,8},{9,10,11,12}})\n-  dot_rhs_constant = s32[2,3] constant({{1,2,3},{4,5,6}})\n-  indices = s32[2] parameter(0)\n-  dot_lhs = s32[3,2] gather(gather_operand, indices),\n-      offset_dims={0},\n-      collapsed_slice_dims={1},\n-      start_index_map={1},\n-      index_vector_dim=1,\n-      slice_sizes={3,1}\n-  ROOT dot = s32[3,3] dot(dot_lhs, dot_rhs_constant), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-}\n-)\";\n-\n-  AssertArrayWithConstantsForRootExpressionIs(hlo_text, \"%dot\");\n-}\n-\n-}  // namespace\n-}  // namespace xla"
        },
        {
            "sha": "cd5f53d7544fe6bff37e254b91b5226c8f926355",
            "filename": "third_party/xla/xla/hlo/tools/hlo_opt/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2FBUILD?ref=ce8015c6146b739751dd064a55ac98afd91019bc",
            "patch": "@@ -41,7 +41,6 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/analysis:alias_info\",\n-        \"//xla/hlo/analysis:indexed_array_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/hlo/tools/tests:hlo_opt_test_only_passes\","
        },
        {
            "sha": "430582c02191bbe8672cd097f1faba2172b194d2",
            "filename": "third_party/xla/xla/hlo/tools/hlo_opt/opt_lib.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc?ref=ce8015c6146b739751dd064a55ac98afd91019bc",
            "patch": "@@ -36,7 +36,6 @@ limitations under the License.\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"xla/hlo/analysis/indexed_array_analysis.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n@@ -295,7 +294,6 @@ void OptProvider::RegisterAllHardwareIndependentPasses() {\n   RegisterPass<HostOffloadLegalize>();\n   RegisterPass<HostOffloadingPrepare>(\n       /*rewrite=*/HostOffloadingPrepare::Rewrite::kElideMoveToHost);\n-  RegisterPass<IndexedArrayAnalysisPrinterPass>();\n   RegisterPass<InfeedTokenPropagation>();\n   RegisterPass<InstructionHoister>();\n   RegisterPass<LiteralCanonicalizer>("
        },
        {
            "sha": "5139c0305c2277af901c013605704c478cd37082",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=ce8015c6146b739751dd064a55ac98afd91019bc",
            "patch": "@@ -229,7 +229,6 @@ cc_library(\n         \"//xla/backends/cpu/transforms/collectives:all_reduce_combiner\",\n         \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_ordering\",\n-        \"//xla/hlo/analysis:indexed_array_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/ir:hlo_module_group\",\n         \"//xla/hlo/pass:hlo_pass\","
        },
        {
            "sha": "7005327351d19ec1a58cac992873a232f339970b",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce8015c6146b739751dd064a55ac98afd91019bc/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=ce8015c6146b739751dd064a55ac98afd91019bc",
            "patch": "@@ -105,7 +105,6 @@ limitations under the License.\n #include \"xla/backends/cpu/xnn_support.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n-#include \"xla/hlo/analysis/indexed_array_analysis.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -866,7 +865,6 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   pipeline.AddPass<TopkRewriter>([](const HloSortInstruction* sort, int64_t) {\n     return sort->operand(0)->shape().element_type() == F32;\n   });\n-  pipeline.AddPass<IndexedArrayAnalysisPrinterPass>();\n   pipeline.AddPass<TransposeFolding>(\n       [&](const HloInstruction& dot, int64_t operand) -> absl::StatusOr<bool> {\n         if (DotImplementationCanHandleTranspose(dot, *target_machine_features,"
        }
    ],
    "stats": {
        "total": 2605,
        "additions": 0,
        "deletions": 2605
    }
}