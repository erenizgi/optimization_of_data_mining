{
    "author": "ezhulenev",
    "message": "[xla:gpu] Remove unused op_id from CollectiveConfig\n\nPiperOrigin-RevId: 836529329",
    "sha": "ba4dbe731b9e6218a6204d27b5901e9144ed0472",
    "files": [
        {
            "sha": "41884d9b3a9ca256e600719b762dbaae4dd65f9b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc?ref=ba4dbe731b9e6218a6204d27b5901e9144ed0472",
            "patch": "@@ -163,7 +163,6 @@ CollectiveKernelThunkMetadata CreateCollectiveKernelThunk(\n       /*operand_count=*/1,\n       /*operand_element_type=*/{PrimitiveType::F32},\n       /*replica_groups=*/{replica_group},\n-      /*op_id=*/0,\n       /*group_mode=*/\n       CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_CROSS_REPLICA,\n       /*use_symmetric_buffer=*/false};"
        },
        {
            "sha": "9d6407f2de6fb4fedca27767677ec2a88bbf5ee3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=ba4dbe731b9e6218a6204d27b5901e9144ed0472",
            "patch": "@@ -59,7 +59,6 @@ CollectiveConfig CollectiveMetadataThunk::GetCollectiveConfig(\n         hlo.operand(i)->shape().element_type());\n   }\n \n-  config.op_id = static_cast<int64_t>(hlo.GetModule()->unique_id());\n   if (hlo.has_backend_config()) {\n     xla::gpu::GpuBackendConfig backend_config =\n         hlo.backend_config<GpuBackendConfig>().value_or(GpuBackendConfig());"
        },
        {
            "sha": "61768cc1556371deea8b2ba49fe55892805f6ef2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=ba4dbe731b9e6218a6204d27b5901e9144ed0472",
            "patch": "@@ -105,7 +105,6 @@ CollectivePermuteStartThunk::CollectivePermuteStartThunk(\n     config.operand_element_type.push_back(\n         instr->operand(i)->shape().element_type());\n   }\n-  config.SetCollectiveOpKindAndID(instr);\n   config.group_mode = GetGroupMode(instr);\n \n   // With a collective permute, all execution instances together form one\n@@ -268,10 +267,9 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n                         comm_handle.comm->NumRanks());\n \n     auto rendezvous_name = absl::StrFormat(\n-        \"rendezvous before calling collective-permute; run_id=%ld; op id:%d; \"\n-        \"num_local_participants:%d\",\n-        params.collective_params->run_id.ToInt(), config_.config.op_id,\n-        num_local_participants);\n+        \"rendezvous before calling collective-permute: run_id=%ld; \"\n+        \"num_local_participants=%d\",\n+        params.collective_params->run_id.ToInt(), num_local_participants);\n     auto rendezvous_key = CallRendezvousKey{params.collective_params->run_id};\n \n     // Perform a rendezvous to make sure all receivers have their events\n@@ -310,10 +308,9 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n                         comm_handle.comm->NumRanks());\n \n     auto rendezvous_name = absl::StrFormat(\n-        \"rendezvous after calling collective-permute; run_id=%ld; op id:%d; \"\n-        \"num_local_participants:%d\",\n-        params.collective_params->run_id.ToInt(), config_.config.op_id,\n-        num_local_participants);\n+        \"rendezvous after calling collective-permute: run_id=%ld; \"\n+        \"num_local_participants=%d\",\n+        params.collective_params->run_id.ToInt(), num_local_participants);\n     auto rendezvous_key = CallRendezvousKey{params.collective_params->run_id};\n \n     // Perform a rendezvous to make sure all senders have their events"
        },
        {
            "sha": "e500ca25ed84c164f9622f5c191dace61dbf1e26",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 28,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=ba4dbe731b9e6218a6204d27b5901e9144ed0472",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n@@ -181,24 +182,6 @@ bool CollectiveConfig::IsDegenerate(int64_t replica_count,\n   }\n }\n \n-void CollectiveConfig::SetCollectiveOpKindAndID(\n-    const HloCollectivePermuteInstruction* instr) {\n-  if (instr->channel_id().has_value()) {\n-    op_id = *instr->channel_id();\n-  } else {\n-    op_id = static_cast<int64_t>(instr->GetModule()->unique_id());\n-  }\n-}\n-\n-void CollectiveConfig::SetCollectiveOpKindAndID(\n-    const HloSendRecvInstruction* instr) {\n-  if (instr->channel_id().has_value()) {\n-    op_id = *instr->channel_id();\n-  } else {\n-    op_id = static_cast<int64_t>(instr->GetModule()->unique_id());\n-  }\n-}\n-\n CollectiveConfig GetCollectiveConfig(\n     const HloInstruction* hlo, std::optional<bool> use_global_device_ids) {\n   CollectiveConfig config;\n@@ -210,12 +193,6 @@ CollectiveConfig GetCollectiveConfig(\n   }\n   config.replica_groups = hlo->replica_groups();\n \n-  if (hlo->channel_id().has_value()) {\n-    config.op_id = *hlo->channel_id();\n-  } else {\n-    config.op_id = static_cast<int64_t>(hlo->GetModule()->unique_id());\n-  }\n-\n   config.group_mode = GetCollectiveOpGroupMode(hlo->channel_id().has_value(),\n                                                use_global_device_ids)\n                           .value();\n@@ -489,16 +466,14 @@ absl::Status CollectiveThunk::ExecuteOnStream(const ExecuteParams& params) {\n             << \"] Do a rendezvous after a first call to \"\n             << Thunk::KindToString(kind())\n             << \"; run_id=\" << params.collective_params->run_id.ToInt()\n-            << \"; op_id=\" << config().op_id\n             << \"; num_local_participants=\" << num_local_participants\n             << \"; rank=\" << rank.value()\n             << \"; clique_key=\" << clique_key.ToString();\n \n     auto rendezvous_key = FirstCallRendezvousKey{std::move(clique_key)};\n     auto rendezvous_name = absl::StrFormat(\n-        \"first call to collective operation %d; kind=%s; run_id=%ld\",\n-        config().op_id, Thunk::KindToString(kind()),\n-        params.collective_params->run_id.ToInt());\n+        \"first call to collective operation: kind=%s; run_id=%ld\",\n+        Thunk::KindToString(kind()), params.collective_params->run_id.ToInt());\n \n     const xla::DebugOptions debug_options = xla::GetDebugOptionsFromFlags();\n "
        },
        {
            "sha": "0a83da101eb4571e64de4aa2ab12ba7379b3fc33",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=ba4dbe731b9e6218a6204d27b5901e9144ed0472",
            "patch": "@@ -38,7 +38,6 @@ limitations under the License.\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n@@ -52,14 +51,13 @@ limitations under the License.\n namespace xla::gpu {\n \n struct CollectiveConfig {\n-  void SetCollectiveOpKindAndID(const HloCollectivePermuteInstruction* instr);\n-  void SetCollectiveOpKindAndID(const HloSendRecvInstruction* instr);\n+  // Returns if the collective communication operation is degenerate because all\n+  // the groups formed by the operation are singleton.\n   bool IsDegenerate(int64_t replica_count, int64_t partition_count) const;\n \n   int64_t operand_count;\n   std::vector<PrimitiveType> operand_element_type;\n   std::vector<ReplicaGroup> replica_groups;\n-  int64_t op_id;\n   CollectiveOpGroupMode group_mode;\n   bool use_symmetric_buffer;\n };"
        },
        {
            "sha": "5fb99cc89136efc448f38a9d1bf276b2e722bc3e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_permute_thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_permute_thunk.cc?ref=ba4dbe731b9e6218a6204d27b5901e9144ed0472",
            "patch": "@@ -75,7 +75,6 @@ NvshmemCollectivePermuteStartThunk::NvshmemCollectivePermuteStartThunk(\n   for (const HloInstruction* operand : instr->operands()) {\n     config.operand_element_type.push_back(operand->shape().element_type());\n   }\n-  config.SetCollectiveOpKindAndID(instr);\n   config.group_mode = GetGroupMode(instr);\n \n   // With a collective permute, all execution instances together form one"
        },
        {
            "sha": "4128c0ac5cfda5a42ab6cd2b7faaf49038c4e69e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba4dbe731b9e6218a6204d27b5901e9144ed0472/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc?ref=ba4dbe731b9e6218a6204d27b5901e9144ed0472",
            "patch": "@@ -95,7 +95,6 @@ P2PConfig GetP2PConfigForSendRecv(const HloSendRecvInstruction* instr,\n \n   config.operand_count = 1;\n   config.operand_element_type.push_back(shape.element_type());\n-  config.SetCollectiveOpKindAndID(instr);\n   config.group_mode = GetCollectiveOpGroupMode(\n                           instr->channel_id().value_or(0) > 0, std::nullopt)\n                           .value();"
        }
    ],
    "stats": {
        "total": 56,
        "additions": 11,
        "deletions": 45
    }
}