{
    "author": "KanishAnand",
    "message": "(2/N) Add support for `NamedSharding` in existing `HloShardingUtil` methods. Remaining methods will be updated in follow up cl's.\n\nPiperOrigin-RevId: 842252683",
    "sha": "37b13b82702bc818c1f0c83e9b45515890eed9d1",
    "files": [
        {
            "sha": "488dfdb2793421a5f5c5005e2eb1aa4e7cdc3914",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=37b13b82702bc818c1f0c83e9b45515890eed9d1",
            "patch": "@@ -130,16 +130,6 @@ class HloSharding {\n                        metadata);\n   }\n \n-  explicit HloSharding(NamedSharding named_sharding)\n-      : replicated_(false),\n-        maximal_(false),\n-        tuple_(false),\n-        manual_(false),\n-        unknown_(false),\n-        unreduced_(false),\n-        replicate_on_last_tile_dim_(false),\n-        named_sharding_(std::move(named_sharding)) {}\n-\n   // Creates a subgroup sharding with device-level tile assignment, the\n   // sharding type of each subgroup is defined by subgroup_types. When creating\n   // the HloSharding, subgroup dims of the same type will be merged.\n@@ -493,6 +483,11 @@ class HloSharding {\n   // REQUIRES: !IsReplicated() && !IsTuple()\n   const TileAssignment& tile_assignment() const { return tile_assignment_; }\n \n+  const NamedSharding& named_sharding() const {\n+    CHECK(UseNamedShardingLeaf());\n+    return named_sharding_.value();\n+  }\n+\n   // Returns the number of dimensions.\n   int64_t num_dimensions() const { return tile_assignment().num_dimensions(); }\n \n@@ -668,9 +663,15 @@ class HloSharding {\n \n   const ShardGroup& GetShardGroup() const { return shard_group_; }\n \n-  std::optional<NamedSharding> named_sharding() const {\n-    return named_sharding_;\n-  }\n+  explicit HloSharding(NamedSharding named_sharding)\n+      : replicated_(false),\n+        maximal_(false),\n+        tuple_(false),\n+        manual_(false),\n+        unknown_(false),\n+        unreduced_(false),\n+        replicate_on_last_tile_dim_(false),\n+        named_sharding_(std::move(named_sharding)) {}\n \n  private:\n   explicit HloSharding(bool manual, bool replicated, bool unknown,"
        },
        {
            "sha": "01ab052d24a22be0fef71efb44a9541da989b2fb",
            "filename": "third_party/xla/xla/hlo/ir/named_sharding.h",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fnamed_sharding.h?ref=37b13b82702bc818c1f0c83e9b45515890eed9d1",
            "patch": "@@ -64,8 +64,6 @@ class NamedSharding {\n     return !(*this == other);\n   }\n \n-  const Mesh& mesh() const { return mesh_; }\n-\n   // TODO(b/456212087): Add validation checks\n   explicit NamedSharding(Mesh mesh,\n                          absl::Span<const DimensionSharding> dim_shardings = {},\n@@ -78,6 +76,14 @@ class NamedSharding {\n         unreduced_axes_(unreduced_axes.begin(), unreduced_axes.end()),\n         metadata_(metadata.begin(), metadata.end()) {}\n \n+  const Mesh& mesh() const { return mesh_; }\n+  absl::Span<const DimensionSharding> dim_shardings() const {\n+    return dim_shardings_;\n+  }\n+  absl::Span<const AxisRef> replicated_axes() const { return replicated_axes_; }\n+  absl::Span<const AxisRef> unreduced_axes() const { return unreduced_axes_; }\n+  absl::Span<const OpMetadata> metadata() const { return metadata_; }\n+\n  private:\n   friend class HloSharding;\n "
        },
        {
            "sha": "66f7bf731b8dad1e48e2526d0c0a6987a65ef589",
            "filename": "third_party/xla/xla/hlo/utils/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2FBUILD?ref=37b13b82702bc818c1f0c83e9b45515890eed9d1",
            "patch": "@@ -151,6 +151,8 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:mesh_and_axis\",\n+        \"//xla/hlo/ir:named_sharding\",\n         \"//xla/hlo/ir:tile_assignment\",\n         \"//xla/service:call_graph\",\n         \"//xla/service:dot_as_convolution_util\",\n@@ -184,6 +186,8 @@ xla_cc_test(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:mesh_and_axis\",\n+        \"//xla/hlo/ir:named_sharding\",\n         \"//xla/hlo/ir:tile_assignment\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:test\","
        },
        {
            "sha": "c8ac759cf20973cb0f21847e065039e47f7aec78",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=37b13b82702bc818c1f0c83e9b45515890eed9d1",
            "patch": "@@ -48,6 +48,8 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/hlo/ir/mesh_and_axis.h\"\n+#include \"xla/hlo/ir/named_sharding.h\"\n #include \"xla/hlo/ir/tile_assignment.h\"\n #include \"xla/hlo/utils/hlo_container_util.h\"\n #include \"xla/layout.h\"\n@@ -1080,6 +1082,21 @@ HloSharding PropagateShardingAlongDimsAndReplicateOthers(\n     return source_sharding;\n   }\n \n+  if (source_sharding.UseNamedShardingLeaf()) {\n+    std::vector<NamedSharding::DimensionSharding> target_dim_shardings(\n+        target_shape_rank);\n+    for (int i = 0; i < source_dims.size(); ++i) {\n+      target_dim_shardings[target_dims[i]] =\n+          source_sharding.named_sharding().dim_shardings()[source_dims[i]];\n+    }\n+\n+    return HloSharding(NamedSharding(\n+        source_sharding.named_sharding().mesh(), target_dim_shardings,\n+        source_sharding.named_sharding().replicated_axes(),\n+        source_sharding.named_sharding().unreduced_axes(),\n+        source_sharding.named_sharding().metadata()));\n+  }\n+\n   HloSharding replicate_other_dims =\n       PartiallyReplicateTiledShardingOnAllDimsExcept(source_sharding,\n                                                      source_dims);\n@@ -1493,6 +1510,22 @@ HloSharding PartiallyReplicateTiledShardingOnDims(\n   if (sharding.IsTileMaximal() || sharding.IsManual()) {\n     return sharding;\n   }\n+\n+  if (sharding.UseNamedShardingLeaf()) {\n+    std::vector<NamedSharding::DimensionSharding> dim_shardings(\n+        sharding.named_sharding().dim_shardings().begin(),\n+        sharding.named_sharding().dim_shardings().end());\n+    for (int64_t dim : dims_to_replicate) {\n+      if (dim < dim_shardings.size()) {\n+        dim_shardings[dim] = NamedSharding::DimensionSharding();\n+      }\n+    }\n+    return HloSharding(NamedSharding(\n+        sharding.named_sharding().mesh(), dim_shardings,\n+        sharding.named_sharding().replicated_axes(),\n+        sharding.named_sharding().unreduced_axes(), sharding.metadata()));\n+  }\n+\n   int64_t group_count = 1;\n   DimensionVector valid_dims_to_replicate;\n   for (int64_t dim : dims_to_replicate) {\n@@ -1555,6 +1588,15 @@ HloSharding PartiallyReplicateTiledShardingOnAllDimsExcept(\n \n HloSharding ReplicateAllDataDims(const HloSharding& sharding,\n                                  int64_t data_rank) {\n+  if (sharding.UseNamedShardingLeaf()) {\n+    std::vector<NamedSharding::DimensionSharding> dim_shardings(\n+        data_rank >= 0 ? data_rank : sharding.num_dimensions());\n+    return HloSharding(NamedSharding(\n+        sharding.named_sharding().mesh(), dim_shardings,\n+        sharding.named_sharding().replicated_axes(),\n+        sharding.named_sharding().unreduced_axes(), sharding.metadata()));\n+  }\n+\n   if (sharding.IsManual()) {\n     return sharding;\n   }\n@@ -1580,6 +1622,34 @@ HloSharding RemoveShapeDimensions(const HloSharding& sharding,\n   if (sharding.IsTileMaximal() || dims_to_remove.empty()) {\n     return sharding;\n   }\n+\n+  if (sharding.UseNamedShardingLeaf()) {\n+    // Check to ensure subgroup dimensions are not passed in dims_to_remove as\n+    // named sharding doesn't handle them as part of dim_shardings but separate\n+    // replicated, unreduced axes as opposed to tile hlo sharding format which\n+    // uses tile dimensions to represent subgroup dimensions as well.\n+    DCHECK(\n+        std::all_of(dims_to_remove.begin(), dims_to_remove.end(),\n+                    [&](int64_t i) { return i < sharding.num_dimensions(); }));\n+\n+    std::vector<NamedSharding::DimensionSharding> new_dim_shardings;\n+    new_dim_shardings.reserve(sharding.num_dimensions() -\n+                              dims_to_remove.size());\n+    for (int64_t i = 0; i < sharding.num_dimensions(); ++i) {\n+      if (absl::c_linear_search(dims_to_remove, i)) {\n+        CHECK_EQ(sharding.dimension(i), 1);\n+      } else {\n+        new_dim_shardings.push_back(\n+            sharding.named_sharding().dim_shardings()[i]);\n+      }\n+    }\n+\n+    return HloSharding(NamedSharding(\n+        sharding.named_sharding().mesh(), new_dim_shardings,\n+        sharding.named_sharding().replicated_axes(),\n+        sharding.named_sharding().unreduced_axes(), sharding.metadata()));\n+  }\n+\n   DimensionVector new_tile_shape;\n   new_tile_shape.reserve(sharding.num_dimensions() - dims_to_remove.size());\n   for (int64_t i = 0; i < sharding.num_dimensions(); ++i) {"
        },
        {
            "sha": "1f521eedaa8006894b2043e06e3c2d752ef6d60e",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.h",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h?ref=37b13b82702bc818c1f0c83e9b45515890eed9d1",
            "patch": "@@ -251,9 +251,10 @@ HloSharding PartiallyReplicateTiledShardingOnAllDimsExcept(\n HloSharding ReplicateAllDataDims(const HloSharding& sharding,\n                                  int64_t data_rank = -1);\n \n-// Returns a sharding the removes given tile dimensions.\n+// Returns a sharding that removes given sharding dimensions.\n //\n-// Precondition: if not tile maximal, the size of each tile dimension must be 1.\n+// Precondition: if not tile maximal, the size of each sharding dimension must\n+// be 1.\n HloSharding RemoveShapeDimensions(const HloSharding& sharding,\n                                   absl::Span<const int64_t> dims_to_remove);\n \n@@ -264,19 +265,30 @@ std::optional<HloSharding> TransposeShardingWithCollapsedDims(\n     const HloSharding& source, absl::Span<int64_t const> src_to_tgt,\n     absl::Span<int64_t const> tgt_to_src);\n \n-// Given a `source_sharding`, preserve the tiles along the `source_dims` and\n-// replicate the rest. The `target_dims` are used to determine the order of the\n-// dimensions in the resulting sharding. If `source_dims` and `target_dims` are\n-// in the different order (i.e., different ArgSort results), we need to\n-// transpose the tile assignment.\n+// Given a `source_sharding`, preserve the dimensions along the `source_dims`\n+// and replicate the rest. The `target_dims` are used to determine the order of\n+// the dimensions in the resulting sharding.\n //\n+// [For tiled sharding format] If `source_dims` and `target_dims` are in the\n+// different order (i.e., different ArgSort results), we need to transpose the\n+// tile assignment.\n // Given the following input,\n //   * source_sharding = {devices=[2,3,5,7,11]<=[2310]}\n //   * source_dims = [2, 4, 1]\n //   * target_dims = [2, 1, 3]\n //   * target_shape_rank = 5\n // The result shoule be {devices=[1,11,5,3,1,14]<=[2,3,5,7,11]T(4,2,1,0,3)\n // last_tile_dim_replicate}.\n+//\n+// [For named sharding format]\n+// Given the following input,\n+//   * mesh = Mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+//   * source_sharding = NamedSharding(mesh, {{\"a\"}, {\"b\"}, {\"c\"}, {\"d\"},\n+//   {\"e\"}})\n+//   * source_dims = [2, 4, 1]\n+//   * target_dims = [2, 1, 3]\n+//   * target_shape_rank = 5\n+// The result shoule be NamedSharding(mesh, {{}, {\"e\"}, {\"c\"}, {\"b\"}, {}})\n HloSharding PropagateShardingAlongDimsAndReplicateOthers(\n     const HloSharding& source_sharding, absl::Span<const int64_t> source_dims,\n     absl::Span<const int64_t> target_dims, int64_t target_shape_rank);"
        },
        {
            "sha": "ab7a203e0d2ae1032d97b4176f157785d65c903e",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util_test.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc?ref=37b13b82702bc818c1f0c83e9b45515890eed9d1",
            "patch": "@@ -27,6 +27,8 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/array.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/hlo/ir/mesh_and_axis.h\"\n+#include \"xla/hlo/ir/named_sharding.h\"\n #include \"xla/hlo/ir/tile_assignment.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/test.h\"\n@@ -566,6 +568,18 @@ TEST(HloShardingUtilTest, PropagateShardingAlongDimsAndReplicateOthers1) {\n   HloSharding expected = HloSharding::PartialTile(\n       TileAssignment({1, 11, 5, 3, 1, 14}, {2, 3, 5, 7, 11}, {4, 2, 1, 0, 3}));\n   EXPECT_EQ(target_sharding, expected);\n+\n+  {\n+    Mesh mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+    NamedSharding source_sharding =\n+        test_utils::FromAxisNames(mesh, {{\"a\"}, {\"b\"}, {\"c\"}, {\"d\"}, {\"e\"}});\n+    HloSharding target_sharding = PropagateShardingAlongDimsAndReplicateOthers(\n+        HloSharding(source_sharding), source_dims, target_dims,\n+        target_shape_rank);\n+    NamedSharding expected =\n+        test_utils::FromAxisNames(mesh, {{}, {\"e\"}, {\"c\"}, {\"b\"}, {}});\n+    EXPECT_EQ(target_sharding.named_sharding(), expected);\n+  }\n }\n \n TEST(HloShardingUtilTest, PropagateShardingAlongDimsAndReplicateOthers2) {\n@@ -578,6 +592,18 @@ TEST(HloShardingUtilTest, PropagateShardingAlongDimsAndReplicateOthers2) {\n   HloSharding expected = HloSharding::PartialTile(\n       TileAssignment({2, 5, 11, 21}, {2, 3, 5, 7, 11}, {0, 2, 4, 1, 3}));\n   EXPECT_EQ(target_sharding, expected);\n+\n+  {\n+    Mesh mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+    NamedSharding source_sharding =\n+        test_utils::FromAxisNames(mesh, {{\"a\"}, {\"b\"}, {\"c\"}, {\"d\"}, {\"e\"}});\n+    HloSharding target_sharding = PropagateShardingAlongDimsAndReplicateOthers(\n+        HloSharding(source_sharding), source_dims, target_dims,\n+        target_shape_rank);\n+    NamedSharding expected =\n+        test_utils::FromAxisNames(mesh, {{\"a\"}, {\"c\"}, {\"e\"}});\n+    EXPECT_EQ(target_sharding.named_sharding(), expected);\n+  }\n }\n \n TEST(HloShardingUtilTest, PropagateShardingAlongDimsAndReplicateOthers3) {\n@@ -590,6 +616,35 @@ TEST(HloShardingUtilTest, PropagateShardingAlongDimsAndReplicateOthers3) {\n   HloSharding expected = HloSharding::PartialTile(\n       TileAssignment({11, 7, 1, 3, 10}, {2, 3, 5, 7, 11}, {4, 3, 1, 0, 2}));\n   EXPECT_EQ(target_sharding, expected);\n+\n+  {\n+    Mesh mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+    NamedSharding source_sharding =\n+        test_utils::FromAxisNames(mesh, {{\"a\"}, {\"b\"}, {\"c\"}, {\"d\"}, {\"e\"}});\n+    HloSharding target_sharding = PropagateShardingAlongDimsAndReplicateOthers(\n+        HloSharding(source_sharding), source_dims, target_dims,\n+        target_shape_rank);\n+    NamedSharding expected =\n+        test_utils::FromAxisNames(mesh, {{\"e\"}, {\"d\"}, {}, {\"b\"}});\n+    EXPECT_EQ(target_sharding.named_sharding(), expected);\n+  }\n+}\n+\n+TEST(HloShardingUtilTest, PropagateShardingAlongDimsAndReplicateOthers4) {\n+  Mesh mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+  NamedSharding source_sharding =\n+      test_utils::FromAxisNames(mesh, {{\"a\"}, {\"c\", \"b\"}, {}, {\"d\"}, {}}, {},\n+                                /*unreduced_axes=*/{\"e\"});\n+  std::vector<int64_t> source_dims = {2, 1, 3};\n+  std::vector<int64_t> target_dims = {0, 3, 1};\n+  int64_t target_shape_rank = 4;\n+  HloSharding target_sharding = PropagateShardingAlongDimsAndReplicateOthers(\n+      HloSharding(source_sharding), source_dims, target_dims,\n+      target_shape_rank);\n+  NamedSharding expected =\n+      test_utils::FromAxisNames(mesh, {{}, {\"d\"}, {}, {\"c\", \"b\"}}, {},\n+                                /*unreduced_axes=*/{\"e\"});\n+  EXPECT_EQ(target_sharding.named_sharding(), expected);\n }\n \n TEST(HloShardingUtilTest, MergeManualSubgroupSharding) {"
        },
        {
            "sha": "8e79e7c16d2e84358924e6cdfd5e4bc37b95ce8d",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37b13b82702bc818c1f0c83e9b45515890eed9d1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=37b13b82702bc818c1f0c83e9b45515890eed9d1",
            "patch": "@@ -3053,7 +3053,7 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsForReplication(\n std::optional<Mesh> GetMeshFromSharding(const HloSharding& sharding) {\n   // For V3 shardings, use the mesh associated with the named sharding.\n   if (sharding.UseNamedShardingLeaf()) {\n-    return sharding.named_sharding()->mesh();\n+    return sharding.named_sharding().mesh();\n   }\n \n   // For V2 shardings, create the mesh from the tile assignment."
        }
    ],
    "stats": {
        "total": 194,
        "additions": 171,
        "deletions": 23
    }
}