{
    "author": "apivovarov",
    "message": "Reverts 7ee34510221ae39d5be027dfa5f0f02dc0a5914a\n\nPiperOrigin-RevId: 808033554",
    "sha": "9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
    "files": [
        {
            "sha": "37ddbf74aa64f664241ac08e58ab2782436194e5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 20,
            "deletions": 2,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -930,7 +930,7 @@ cuda_library(\n     ],\n )\n \n-cuda_library(\n+cc_library(\n     name = \"select_k_exec_stub\",\n     srcs = [\"select_k_exec_stub.cc\"],\n     hdrs = [\"select_k_exec.h\"],\n@@ -979,7 +979,6 @@ cc_library(\n     name = \"select_k_thunk\",\n     srcs = [\"select_k_thunk.cc\"],\n     hdrs = [\"select_k_thunk.h\"],\n-    tags = [\"gpu\"],\n     deps = [\n         \":thunk\",\n         \":thunk_proto_cc\",\n@@ -995,13 +994,32 @@ cc_library(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n     ] + if_cuda_is_configured(\n         [\":select_k_exec_raft\"],\n         no_cuda = [\":select_k_exec_stub\"],\n     ),\n )\n \n+xla_cc_test(\n+    name = \"select_k_thunk_test\",\n+    srcs = [\"select_k_thunk_test.cc\"],\n+    deps = [\n+        \":select_k_thunk\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n+        \"//xla/codegen/emitters:kernel_arguments\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"memset_thunk\",\n     srcs = [\"memset_thunk.cc\"],"
        },
        {
            "sha": "58d84feeaa755e58a25090cb30eb2d1c86cb42d4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_exec_raft.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 3,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_exec_raft.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -82,6 +82,12 @@ class OwningScratchAllocator {\n     return absl::NotFoundError(\"Pointer not found\");\n   }\n \n+  se::DeviceMemoryAllocator* get_allocator() const { return allocator_; }\n+\n+  void set_allocator(se::DeviceMemoryAllocator* allocator) {\n+    allocator_ = allocator;\n+  }\n+\n  private:\n   int device_ordinal_;\n   se::DeviceMemoryAllocator* allocator_;\n@@ -96,6 +102,14 @@ class XlaDeviceMemoryResource : public rmm::mr::device_memory_resource {\n                           se::DeviceMemoryAllocator* allocator)\n       : scratch_allocator_(device_ordinal, allocator) {}\n \n+  se::DeviceMemoryAllocator* get_allocator() const {\n+    return scratch_allocator_.get_allocator();\n+  }\n+\n+  void set_allocator(se::DeviceMemoryAllocator* allocator) {\n+    scratch_allocator_.set_allocator(allocator);\n+  }\n+\n  protected:\n   void* do_allocate(std::size_t bytes, rmm::cuda_stream_view stream) override {\n     auto mem = scratch_allocator_.AllocateBytes(bytes);\n@@ -122,6 +136,8 @@ class XlaDeviceMemoryResource : public rmm::mr::device_memory_resource {\n // RAII wrapper for RAFT resources bound to a CUDA stream\n struct RaftStreamResource : public se::Stream::Resource {\n   raft::resources res;\n+  std::shared_ptr<XlaDeviceMemoryResource> xla_dev_mem_res;\n+  ~RaftStreamResource() override = default;\n \n   // Factory to create a RaftStreamResource tied to a CUDA stream.\n   // Sets up `raft::resources` with a custom XlaDeviceMemoryResource\n@@ -138,9 +154,10 @@ struct RaftStreamResource : public se::Stream::Resource {\n       cudaStream_t cuda_stream) {\n     // Assign our custom AllocatorForRaft for this device\n     auto handle = std::make_unique<RaftStreamResource>();\n-    raft::resource::set_workspace_resource(\n-        handle->res,\n-        std::make_shared<XlaDeviceMemoryResource>(device_ordinal, allocator));\n+    handle->xla_dev_mem_res =\n+        std::make_shared<XlaDeviceMemoryResource>(device_ordinal, allocator);\n+    raft::resource::set_workspace_resource(handle->res,\n+                                           handle->xla_dev_mem_res);\n     // Set Cuda Stream\n     raft::resource::set_cuda_stream(handle->res,\n                                     rmm::cuda_stream_view{cuda_stream});\n@@ -246,6 +263,8 @@ absl::Status select_k_exec(int device_ordinal,\n   SelectAlgo algo = choose_select_k_algorithm<T>(batch, n, k);\n   VLOG(3) << \"select_k_exec_raft: \"\n           << \"device_ordinal: \" << device_ordinal << \", \"\n+          << \"allocator: \" << allocator << \", \"\n+          << \"stream: \" << stream << \", \"\n           << \"data_in: \" << data_in.opaque() << \" (\" << data_in.size() << \"B)\"\n           << \", data_out: \" << data_out.opaque() << \" (\" << data_out.size()\n           << \"B)\"\n@@ -268,6 +287,13 @@ absl::Status select_k_exec(int device_ordinal,\n   TF_RET_CHECK(resContainer != nullptr)\n       << \"Failed to create or retrieve RaftStreamResource\";\n \n+  // resContainer is scoped to a single stream.\n+  // Because a stream does not execute select_k_exec concurrently from multiple\n+  // threads, it is safe to update the allocator without additional locking.\n+  if (allocator != resContainer->xla_dev_mem_res->get_allocator()) {\n+    resContainer->xla_dev_mem_res->set_allocator(allocator);\n+  }\n+\n   try {\n     // Wrap raw device pointers in RAFT matrix views\n     auto input_view ="
        },
        {
            "sha": "401690bd62ed9d59f61e7fbd232b7afb080f8134",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_thunk.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"xla/backends/gpu/runtime/select_k_exec.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -99,4 +100,14 @@ absl::Status SelectKThunk::ExecuteOnStream(const ExecuteParams& params) {\n                        primitive_util::LowercasePrimitiveTypeName(dtype_)));\n   }\n }\n+\n+absl::StatusOr<ThunkProto> SelectKThunk::ToProto() const {\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+\n+  SelectKThunkProto* select_k_thunk_proto = proto.mutable_select_k_thunk();\n+  (void)select_k_thunk_proto;\n+  // TODO(upwind): Add fields for SelectKThunkProto.\n+  return proto;\n+}\n }  // namespace xla::gpu"
        },
        {
            "sha": "b12fb443bf84cab91a1f6816b5c5ba10999be23d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.h?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -16,12 +16,12 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_RUNTIME_SELECT_K_THUNK_H_\n #define XLA_BACKENDS_GPU_RUNTIME_SELECT_K_THUNK_H_\n \n-#include <cstddef>\n #include <cstdint>\n #include <string>\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -61,6 +61,8 @@ class SelectKThunk : public Thunk {\n     return args_;\n   }\n \n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n  private:\n   std::uint32_t batch_size_;\n   std::uint32_t num_elements_;"
        },
        {
            "sha": "117cd7e279b3e8ae6a2e3b535aba9f3554a68623",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_thunk_test.cc",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk_test.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -0,0 +1,75 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/select_k_thunk.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::tsl::proto_testing::EqualsProto;\n+\n+TEST(SelectKThunkTest, ToProto) {\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.profile_annotation = \"profile_annotation\";\n+  thunk_info.execution_stream_id = 123;\n+\n+  BufferAllocation alloc0(/*index=*/0, /*size=*/20, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc0, /*offset=*/0, /*size=*/20);\n+\n+  BufferAllocation alloc1(/*index=*/1, /*size=*/12, /*color=*/0);\n+  BufferAllocation::Slice slice1(&alloc1, /*offset=*/0, /*size=*/12);\n+\n+  BufferAllocation alloc2(/*index=*/2, /*size=*/12, /*color=*/0);\n+  BufferAllocation::Slice slice2(&alloc2, /*offset=*/0, /*size=*/12);\n+\n+  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {1, 5}), slice0);\n+  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {1, 3}), slice1);\n+  emitters::KernelArgument arg2(ShapeUtil::MakeShape(U32, {1, 3}), slice2);\n+  arg0.set_written(false);\n+  arg1.set_written(true);\n+  arg2.set_written(true);\n+\n+  emitters::KernelArguments kernel_arguments({arg0, arg1, arg2});\n+\n+  auto c1 = HloInstruction::CreateConstant(\n+      LiteralUtil::CreateR2<float>({{.125f, 0.875f, .5f, .25f, 0.75f}}));\n+  auto topKInst = HloInstruction::CreateCustomCall(\n+      ShapeUtil::MakeShape(F32, {1, 5}), {c1.get()}, \"__gpu$TopK\");\n+\n+  SelectKThunk thunk(topKInst.get(), 1, 5, 3, F32, kernel_arguments);\n+  TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, thunk.ToProto());\n+  EXPECT_THAT(proto, EqualsProto(R\"pb(\n+                thunk_info { profile_annotation: \"custom-call\" }\n+                select_k_thunk {}\n+              )pb\"));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "80c1c1edb2155305c54abff7a1bc65555459f409",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -142,6 +142,10 @@ message InfeedThunkProto {\n   repeated ShapedSliceProto dest_slices = 1;\n }\n \n+message SelectKThunkProto {\n+  // TODO(upwind): Add fields for SelectKThunkProto.\n+}\n+\n message ThunkProto {\n   ThunkInfoProto thunk_info = 1;\n \n@@ -164,6 +168,7 @@ message ThunkProto {\n     HostExecuteDoneThunkProto host_execute_done_thunk = 17;\n     DynamicSliceThunkProto dynamic_slice_thunk = 18;\n     MemzeroThunkProto memzero_thunk = 19;\n+    SelectKThunkProto select_k_thunk = 20;\n     InfeedThunkProto infeed_thunk = 21;\n   }\n }"
        },
        {
            "sha": "5257ac9b8d1a4150f65e7a480c53492298110081",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -451,6 +451,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_detect_unstable_reductions(\n       DebugOptions::UNSTABLE_REDUCTION_DETECTION_MODE_NONE);\n   opts.set_xla_gpu_experimental_scaled_dot_with_triton(false);\n+  opts.set_xla_gpu_experimental_use_raft_select_k(false);\n   return opts;\n }\n \n@@ -2542,6 +2543,12 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n                 \"that checks for unstable reductions in HLO computations. \"\n                 \"Acceptable values are: 'none', 'log', and 'crash'. 'none' is \"\n                 \"the default.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_experimental_use_raft_select_k\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_gpu_experimental_use_raft_select_k),\n+      debug_options->xla_gpu_experimental_use_raft_select_k(),\n+      \"If true, use the raft::matrix::select_k implementation of TopK.\"));\n }  // NOLINT(readability/fn_size)\n \n // Allocates flag_values and flag_objects; this function must not be called more"
        },
        {
            "sha": "c7f887ae42337bd283b6bacef97f9aa81001542a",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -407,6 +407,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:ragged_all_to_all_thunk\",\n         \"//xla/backends/gpu/runtime:recv_thunk\",\n         \"//xla/backends/gpu/runtime:replica_id_thunk\",\n+        \"//xla/backends/gpu/runtime:select_k_thunk\",\n         \"//xla/backends/gpu/runtime:send_thunk\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\","
        },
        {
            "sha": "e53b5f36ac605e9b171ef0373170d490bf0b1afc",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -724,6 +724,8 @@ absl::Status RunOptimizationPasses(\n     const AlgebraicSimplifierOptions& layout_insensitive_algsimp_opts,\n     absl::string_view platform_name) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n+  se::GpuComputeCapability gpu_version =\n+      gpu_target_config.device_description.gpu_compute_capability();\n \n   HloPassPipeline pipeline(\"optimization\");\n   AddHloVerifier(&pipeline, !debug_options.xla_ignore_channel_id());\n@@ -738,7 +740,7 @@ absl::Status RunOptimizationPasses(\n     pipeline.AddPass<WindowedEinsumHandler>();\n   }\n   pipeline.AddPass<TopKSplitter>();\n-  pipeline.AddPass<TopkSpecializer>();\n+  pipeline.AddPass<TopkSpecializer>(gpu_version);\n   pipeline.AddPass<TopkDecomposer>();\n \n   pipeline.AddPass<DotDimensionSorter>();\n@@ -876,9 +878,6 @@ absl::Status RunOptimizationPasses(\n   // Expand the sort op to support stable sorting if required.\n   pipeline.AddPass<StableSortExpander>();\n \n-  se::GpuComputeCapability gpu_version =\n-      gpu_target_config.device_description.gpu_compute_capability();\n-\n   // Build simplification pipeline.  The passes in here are run to a fixed\n   // point.\n   [&, &pipeline ="
        },
        {
            "sha": "f466b349466150eb6d4f7f84f70967a9e8d31150",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 12,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -110,6 +110,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/ragged_all_to_all_thunk.h\"\n #include \"xla/backends/gpu/runtime/recv_thunk.h\"\n #include \"xla/backends/gpu/runtime/replica_id_thunk.h\"\n+#include \"xla/backends/gpu/runtime/select_k_thunk.h\"\n #include \"xla/backends/gpu/runtime/send_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -1432,25 +1433,53 @@ absl::Status IrEmitterUnnested::EmitTopKCustomCall(\n           : std::tuple<size_t, size_t, size_t>{\n                 1, data_shape.dimensions(0), top_elements_shape.dimensions(0)};\n \n-  auto wavefront_size =\n-      ir_emitter_context_->gpu_device_info().threads_per_warp();\n-\n-  // Load TopK custom kernel.\n-  TF_ASSIGN_OR_RETURN(\n-      CustomKernel kernel,\n-      kernel::topk::GetTopKKernel(\"topk\", data_shape.element_type(), n, k,\n-                                  batch_size, platform_name(), wavefront_size));\n-\n   // Prepare kernel arguments.\n   TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n                       emitters::KernelArguments::Create(\n                           ir_emitter_context_->buffer_assignment(),\n                           GetDefaultBufferAlignment(), instr));\n \n-  auto thunk = std::make_unique<CustomKernelThunk>(instr, std::move(kernel),\n-                                                   kernel_arguments);\n-  AddThunkToThunkSequence(std::move(thunk));\n+  auto dtype = data_shape.element_type();\n+  bool is_cuda = std::holds_alternative<stream_executor::CudaComputeCapability>(\n+      ir_emitter_context_->gpu_compute_capability());\n+  if (is_cuda && instr->GetModule()\n+                     ->config()\n+                     .debug_options()\n+                     .xla_gpu_experimental_use_raft_select_k()) {\n+    // The heuristic for deciding when to use TopK Custom Kernel versus\n+    // Raft::matrix::select_k was developed as part of the initial research\n+    // in b/409009349.\n+    // CustomCall TopK requires k <= 16 and n >= 1024\n+    bool use_raft_select_k = false;\n+    if (dtype == PrimitiveType::F32) {\n+      use_raft_select_k =\n+          (n < 1024) || (n == 1024 && k > 12) || (n > 1024 && k >= 8);\n+    } else if (dtype == PrimitiveType::BF16) {\n+      use_raft_select_k = n < 1024 || k >= 8;\n+    }\n+\n+    VLOG(3) << \"EmitTopKCustomCall: dtype=\" << dtype << \", n=\" << n\n+            << \", k=\" << k << \", use_raft_select_k=\" << use_raft_select_k;\n+\n+    if (use_raft_select_k) {\n+      AddThunkToThunkSequence(std::make_unique<SelectKThunk>(\n+          instr, batch_size, n, k, dtype, kernel_arguments));\n+      return absl::OkStatus();\n+    }\n+  }\n+\n+  auto wavefront_size =\n+      ir_emitter_context_->gpu_device_info().threads_per_warp();\n+\n+  TF_RET_CHECK(k <= 16) << \"CustomCall TopK requires k <= 16\";\n+  // Load TopK custom kernel.\n+  TF_ASSIGN_OR_RETURN(\n+      CustomKernel kernel,\n+      kernel::topk::GetTopKKernel(\"topk\", dtype, n, k, batch_size,\n+                                  platform_name(), wavefront_size));\n \n+  AddThunkToThunkSequence(std::make_unique<CustomKernelThunk>(\n+      instr, std::move(kernel), kernel_arguments));\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "9652eca6352f05c1bbe4a8d3fe1e900b80ef5126",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -2766,6 +2766,8 @@ cc_library(\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:tuple_util\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n@@ -2787,6 +2789,7 @@ xla_test(\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:platform_util\",\n         \"//xla/service:topk_rewriter\",\n+        \"//xla/stream_executor:device_description\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "6b9e46d7d4c807ce3303ad0bcfb56d46ed025f16",
            "filename": "third_party/xla/xla/service/gpu/transforms/topk_specializer.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 14,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -19,6 +19,8 @@ limitations under the License.\n \n #include <initializer_list>\n #include <string>\n+#include <utility>\n+#include <variant>\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n@@ -37,33 +39,66 @@ limitations under the License.\n #include \"xla/service/tuple_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n \n namespace {\n \n absl::StatusOr<HloInstruction*> SmallBufferOptimization(\n-    HloCustomCallInstruction* topk) {\n+    HloCustomCallInstruction* topk, bool is_cuda,\n+    bool xla_gpu_experimental_use_raft_select_k) {\n   Shape data_shape = topk->operand(0)->shape();\n+  auto dtype = data_shape.element_type();\n   auto supported_dtypes = {F32, BF16};\n-  if (!absl::c_linear_search(supported_dtypes, data_shape.element_type())) {\n-    return InvalidArgument(\n-        \"Invalid Dtype: %s\",\n-        primitive_util::LowercasePrimitiveTypeName(data_shape.element_type()));\n+  if (!absl::c_linear_search(supported_dtypes, dtype)) {\n+    return InvalidArgument(\"Invalid Dtype: %s\",\n+                           primitive_util::LowercasePrimitiveTypeName(dtype));\n   }\n   // We only support topk of the shape [x] or [batch, x].\n   if (data_shape.dimensions().size() > 2) {\n     return InvalidArgument(\"Invalid input dimensions: %s\",\n                            data_shape.ToString());\n   }\n   bool has_batch = data_shape.dimensions().size() == 2;\n-  constexpr size_t max_k = 16;\n-  constexpr size_t min_n = 1024;\n+  size_t max_k = 16;  // CustomCall TopK requires k <= 16 and n >= 1024\n+  size_t min_n = 1024;\n+  size_t batch = 0;\n+  if (has_batch) {\n+    batch = data_shape.dimensions(0);\n+  }\n   size_t n = data_shape.dimensions(has_batch ? 1 : 0);\n   size_t k = topk->shape().tuple_shapes(0).dimensions(has_batch ? 1 : 0);\n+  double ratio = static_cast<double>(k) / n;\n+  if (ratio >= 0.85) {\n+    return InvalidArgument(\n+        \"k/n ratio (%f) is too high for TopK. Falling back to sort + slice.\",\n+        ratio);\n+  }\n+  if (is_cuda && xla_gpu_experimental_use_raft_select_k) {\n+    // The heuristic for deciding when to use Raft select_k versus Sort + Slice\n+    // was developed as part of the initial research in b/409009349\n+    if (dtype == F32) {\n+      min_n = 1;\n+      max_k = 128;\n+      if (batch >= 64 && n >= 16384) {\n+        max_k = 256;\n+      }\n+    } else if (dtype == BF16) {\n+      min_n = 1;\n+      max_k = 128;\n+      if (batch >= 16 && n >= 65536) {\n+        max_k = 256;\n+      }\n+      if (batch >= 64 && batch <= 128 && n >= 8192 && n <= 32768) {\n+        max_k = 64;\n+      }\n+    }\n+  }\n+\n   if (k > max_k) {\n     return InvalidArgument(\"k too large (%d), must be <= %d\", k, max_k);\n   }\n@@ -83,14 +118,26 @@ absl::StatusOr<HloInstruction*> SmallBufferOptimization(\n \n class SpecializeTopkVisitor : public DfsHloRewriteVisitor {\n  public:\n+  explicit SpecializeTopkVisitor(se::GpuComputeCapability compute_capability)\n+      : compute_capability_(std::move(compute_capability)) {}\n+\n   absl::Status HandleCustomCall(HloInstruction* inst) override {\n     HloCustomCallInstruction* topk = DynCast<HloCustomCallInstruction>(inst);\n     if (topk == nullptr || topk->custom_call_target() != \"TopK\") {\n       return absl::OkStatus();\n     }\n     TF_RET_CHECK(topk->operand_count() == 1);\n-\n-    if (auto small_topk = SmallBufferOptimization(topk); small_topk.ok()) {\n+    bool is_cuda =\n+        std::holds_alternative<stream_executor::CudaComputeCapability>(\n+            compute_capability_);\n+\n+    if (auto small_topk = SmallBufferOptimization(\n+            topk, is_cuda,\n+            inst->GetModule()\n+                ->config()\n+                .debug_options()\n+                .xla_gpu_experimental_use_raft_select_k());\n+        small_topk.ok()) {\n       return ReplaceInstruction(topk, *small_topk);\n     } else {  // NOLINT(readability-else-after-return)\n       VLOG(2) << \"Small TopK optimization doesn't match: \"\n@@ -99,15 +146,18 @@ class SpecializeTopkVisitor : public DfsHloRewriteVisitor {\n \n     return absl::OkStatus();\n   }\n+\n+ private:\n+  se::GpuComputeCapability compute_capability_;\n };\n \n }  // namespace\n \n absl::StatusOr<bool> TopkSpecializer::Run(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  return SpecializeTopkVisitor().RunOnModule(module, execution_threads);\n+  return SpecializeTopkVisitor(compute_capability_)\n+      .RunOnModule(module, execution_threads);\n }\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu"
        },
        {
            "sha": "f5e593c175f2d744424a197d002f7acf0e130eef",
            "filename": "third_party/xla/xla/service/gpu/transforms/topk_specializer.h",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.h?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -16,24 +16,32 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_TRANSFORMS_TOPK_SPECIALIZER_H_\n #define XLA_SERVICE_GPU_TRANSFORMS_TOPK_SPECIALIZER_H_\n \n+#include <utility>\n+\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/stream_executor/device_description.h\"\n \n namespace xla::gpu {\n \n // This pass transforms eligible TopK CustomCall into a call to be executed by\n // runtime/topk.cc.\n class TopkSpecializer : public HloModulePass {\n  public:\n+  explicit TopkSpecializer(se::GpuComputeCapability compute_capability)\n+      : compute_capability_(std::move(compute_capability)) {}\n   absl::string_view name() const override { return \"topk-specializer\"; }\n \n   using HloPassInterface::Run;\n   absl::StatusOr<bool> Run(\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+\n+ private:\n+  se::GpuComputeCapability compute_capability_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "83054fc24aa131ca5e0cd5eba93a522ec9167d7b",
            "filename": "third_party/xla/xla/service/gpu/transforms/topk_specializer_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer_test.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/topk_rewriter.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -135,8 +136,12 @@ TEST_P(TopkTest, ProducesCorrectResult) {\n   const auto [n_kb, k, batch_size, dtype] = GetParam();\n   const size_t n = n_kb * 1024;\n   TF_ASSERT_OK_AND_ASSIGN(auto topk_module, TopkHlo(n, k, batch_size, dtype));\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n-                          gpu::TopkSpecializer().Run(topk_module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<xla::se::DeviceDescription> device_desc,\n+      GetTestPlatform()->DescriptionForDevice(0));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      bool changed, gpu::TopkSpecializer(device_desc->gpu_compute_capability())\n+                        .Run(topk_module.get()));\n   ASSERT_TRUE(changed);\n   EXPECT_TRUE(\n       RunAndCompare(std::move(topk_module), std::nullopt, ToSortAndSlice));"
        },
        {
            "sha": "fe65a875118d82157966594df756d7f9454efa6a",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_opt.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -174,7 +174,7 @@ class GpuOptProvider : public CompiledOptProvider {\n     RegisterPass<gpu::ReductionLayoutNormalizer>();\n     RegisterPass<gpu::SanitizeConstantNames>();\n     RegisterPass<gpu::TopKSplitter>();\n-    RegisterPass<gpu::TopkSpecializer>();\n+    RegisterPass<gpu::TopkSpecializer>(gpu_compute_capability);\n     RegisterPass<gpu::TransposeDimensionGrouper>();\n     RegisterPass<gpu::WindowedEinsumHandler>();\n     // go/keep-sorted end"
        },
        {
            "sha": "36eb2c0533ac8b4b6cf40f811b5400552a2e484c",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c11aec4bbf4aacee5fb6efc848273ecbbe9c182/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=9c11aec4bbf4aacee5fb6efc848273ecbbe9c182",
            "patch": "@@ -1335,10 +1335,13 @@ message DebugOptions {\n   // We need this to enable triton scaled dot emitter for testing.\n   optional bool xla_gpu_experimental_scaled_dot_with_triton = 410;\n \n+  // If true, use third-party library raft::matrix::select_k to implement TopK.\n+  optional bool xla_gpu_experimental_use_raft_select_k = 413;\n+\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 413\n+  // Next id: 414\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 322,
        "additions": 282,
        "deletions": 40
    }
}