{
    "author": "apivovarov",
    "message": "Add parameterized test for TopK thunk selection on GPU\n\nThis change introduces a new parameterized test, SelectKOrCustomKernelThunk, in GpuCompilerSelectKTest. The test verifies that XLA correctly selects the GPU TopK implementation based on the input size (N) and the requested top-K value (k).\n\nPiperOrigin-RevId: 809122526",
    "sha": "d912ca4cd5e4a2a4f825e3ee3bc6d999b71be02b",
    "files": [
        {
            "sha": "d7e26fbf8cd62e750f7b313cd6bc1b3cb7446a37",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d912ca4cd5e4a2a4f825e3ee3bc6d999b71be02b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d912ca4cd5e4a2a4f825e3ee3bc6d999b71be02b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=d912ca4cd5e4a2a4f825e3ee3bc6d999b71be02b",
            "patch": "@@ -2160,6 +2160,118 @@ m {\n                                       std::move(ref_module), std::nullopt,\n                                       /*run_hlo_passes=*/false));\n }\n+\n+// Define a test-specific enum for expected TopK implementations.\n+enum class TopKImpl {\n+  kCustomKernel,  // Custom GPU kernel\n+  kSelectK,       // raft::select_k\n+  kSort           // Fallback Sort+Slice\n+};\n+\n+// Test fixture for verifying GPU TopK lowering to SelectK or custom kernel.\n+class GpuCompilerSelectKTest\n+    : public GpuCompilerTest,\n+      public ::testing::WithParamInterface<std::tuple<int, int, TopKImpl>> {};\n+\n+// Test lowering of TopK to different GPU implementations\n+// (CustomKernel, raft::select_k, or Sort+Slice (LLVM/CUBSort)).\n+TEST_P(GpuCompilerSelectKTest, SelectKOrCustomKernelThunk) {\n+  auto [n, k, expected_impl] = GetParam();\n+\n+  // Generate HLO text with parameters substituted.\n+  std::string hlo_text = absl::Substitute(R\"(\n+HloModule m\n+\n+ENTRY main {\n+  p = f32[8,$0]{1,0} parameter(0)\n+  ROOT t = (f32[8,$1]{1,0}, s32[8,$1]{1,0}) topk(p), k=$1, largest=true\n+}\n+)\",\n+                                          n, k);\n+\n+  // Configure module with debug options for experimental raft select_k.\n+  HloModuleConfig config;\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  debug_options.set_xla_gpu_experimental_use_raft_select_k(true);\n+  config.set_debug_options(debug_options);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(hlo_text, config));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> compiled_module,\n+      backend().compiler()->RunHloPasses(module->Clone(),\n+                                         backend().default_stream_executor(),\n+                                         /*device_allocator=*/nullptr));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Executable> executable,\n+      backend().compiler()->RunBackend(std::move(compiled_module),\n+                                       backend().default_stream_executor(),\n+                                       {/*device_allocator=*/nullptr,\n+                                        /*thread_pool=*/nullptr,\n+                                        /*layout_canonicalization_callback=*/{},\n+                                        /*is_autotuning_compilation=*/false}));\n+\n+  // Downcast to GPU executable\n+  xla::gpu::GpuExecutable* gpu_executable =\n+      tensorflow::down_cast<xla::gpu::GpuExecutable*>(executable.get());\n+  ASSERT_NE(gpu_executable, nullptr);\n+\n+  // Get the thunk sequence and check its size and type\n+  const SequentialThunk& seq_thunk = gpu_executable->GetThunk();\n+  std::vector<Thunk::Kind> kinds;\n+  kinds.reserve(seq_thunk.thunks().size());\n+  for (const auto& thunk : seq_thunk.thunks()) {\n+    kinds.push_back(thunk->kind());\n+  }\n+\n+  using ::testing::ElementsAre;\n+\n+  switch (expected_impl) {\n+    case TopKImpl::kCustomKernel:\n+      EXPECT_THAT(kinds, ElementsAre(Thunk::Kind::kCustomKernel));\n+      break;\n+\n+    case TopKImpl::kSelectK:\n+      EXPECT_THAT(kinds, ElementsAre(Thunk::Kind::kSelectK));\n+      break;\n+\n+    case TopKImpl::kSort: {\n+      if (kinds.size() == 1) {\n+        // LLVM\n+        EXPECT_THAT(kinds, ElementsAre(Thunk::Kind::kCommandBuffer));\n+      } else if (kinds.size() == 4) {\n+        // CUBSort\n+        EXPECT_THAT(kinds,\n+                    ElementsAre(Thunk::Kind::kKernel, Thunk::Kind::kCubSort,\n+                                Thunk::Kind::kKernel, Thunk::Kind::kKernel));\n+      } else {\n+        FAIL() << \"Unexpected thunk sequence size: \" << kinds.size();\n+      }\n+      break;\n+    }\n+\n+    default:\n+      FAIL() << \"Unexpected TopKImpl: \" << static_cast<int>(expected_impl);\n+  }\n+}\n+\n+auto SelectKTestParams() {\n+  // Depending on N and K, XLA chooses different TopK implementations:\n+  // CustomKernel, raft::select_k, or Sort+Slice.\n+  // The heuristic for selecting between TopK CustomKernel and\n+  // raft::matrix::select_k was developed as part of the initial research\n+  // described in b/409009349.\n+  return ::testing::Values(std::make_tuple(1023, 4, TopKImpl::kSelectK),\n+                           std::make_tuple(1024, 4, TopKImpl::kCustomKernel),\n+                           std::make_tuple(1024, 16, TopKImpl::kSelectK),\n+                           std::make_tuple(8192, 24, TopKImpl::kSelectK),\n+                           std::make_tuple(8192, 512, TopKImpl::kSort));\n+}\n+// Instantiate the test suite with (n, k, expected_kind) pairs.\n+INSTANTIATE_TEST_SUITE_P(SelectKOrCustomKernel, GpuCompilerSelectKTest,\n+                         SelectKTestParams());\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 112,
        "deletions": 0
    }
}