{
    "author": "sergey-kozub",
    "message": "PR #30735: [NFC] Remove META scope from triton fusion analysis\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30735\n\nüìù Summary of Changes\nRemove `TritonFusionAnalysis::Scope::META` enum value, which is not used.\nNext PR will add scopes for LHS/RHS scales.\n\nüöÄ Kind of Contribution\n‚ôªÔ∏è Cleanup\n\nCopybara import of the project:\n\n--\n1c3c4aadc2178e2b9a210d068198fcf637021b08 by Sergey Kozub <skozub@nvidia.com>:\n\n[NFC] Remove META scope from triton fusion analysis\n\nMerging this change closes #30735\n\nPiperOrigin-RevId: 800452159",
    "sha": "cccd091be6fe28ba83d94d41d297c790f18a7bde",
    "files": [
        {
            "sha": "fc9b5ec3cfd446daf89440e28235635b1892fedc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 39,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc?ref=cccd091be6fe28ba83d94d41d297c790f18a7bde",
            "patch": "@@ -111,25 +111,9 @@ using ::mlir::ValueRange;\n \n namespace {\n \n+// Internal indices for the corresponding input scopes.\n const int kLhsIndex = 0;\n const int kRhsIndex = 1;\n-const int kMetaIndex = 2;\n-const int kOutputIndex = 3;\n-\n-// Returns the index of the operand in the fusion's parameter list. This\n-// might work better as a map from Scope->X, but it needs more refactoring.\n-size_t GetOperandIndex(TritonFusionAnalysis::Scope scope) {\n-  switch (scope) {\n-    case TritonFusionAnalysis::Scope::LHS:\n-      return kLhsIndex;\n-    case TritonFusionAnalysis::Scope::RHS:\n-      return kRhsIndex;\n-    case TritonFusionAnalysis::Scope::META:\n-      return kMetaIndex;\n-    case TritonFusionAnalysis::Scope::OUTPUT:\n-      return kOutputIndex;\n-  }\n-}\n \n absl::StatusOr<Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t) {\n   switch (t) {\n@@ -1620,16 +1604,10 @@ class Scopes {\n     out_.batch_dim_idx = dims.out_batch_dim_idx;\n   }\n \n-  std::vector<const Side*> input_scopes() const {\n-    if (meta_.has_value()) {\n-      return {&lhs_, &rhs_, &meta_.value()};\n-    }\n-    return {&lhs_, &rhs_};\n-  }\n+  std::vector<const Side*> input_scopes() const { return {&lhs_, &rhs_}; }\n   const Side& lhs() const { return lhs_; }\n   const Side& rhs() const { return rhs_; }\n   const Side& out() const { return out_; }\n-  const std::optional<Side>& meta() const { return meta_; }\n   const Value& pid_m() const { return pid_m_; }\n   const Value& pid_k() const { return pid_k_; }\n   const Value& pid_n() const { return pid_n_; }\n@@ -1638,7 +1616,6 @@ class Scopes {\n   Side lhs_;\n   Side rhs_;\n   Side out_;\n-  std::optional<Side> meta_;\n \n   Value pid_m_;\n   Value pid_k_;\n@@ -1663,24 +1640,19 @@ class IterableInput {\n         block_k_(block_k),\n         hlo_instr_(hlo_instr),\n         side_(side),\n-        boundary_checks_(boundary_checks) {};\n+        boundary_checks_(boundary_checks) {}\n \n   static absl::StatusOr<IterableInput> CreateIterableInput(\n       size_t iter_arg_index, EmitterLocOpBuilder& b, const MatMulDims& dims,\n       const Side* side, const HloInstruction* hlo_instr, int64_t block_k) {\n-    Type input_ty;\n-    if (side->scope == TritonFusionAnalysis::Scope::META) {\n-      input_ty = b.getI16Type();\n-    } else {\n-      TF_ASSIGN_OR_RETURN(input_ty,\n-                          TritonType(b, hlo_instr->shape().element_type()));\n-    }\n+    TF_ASSIGN_OR_RETURN(Type input_ty,\n+                        TritonType(b, hlo_instr->shape().element_type()));\n     int contracting_dimension =\n         (side->scope == TritonFusionAnalysis::Scope::RHS)\n             ? dims.rhs_contracting_dim_idx\n             : dims.lhs_contracting_dim_idx;\n \n-    return IterableInput(iter_arg_index, GetOperandIndex(side->scope),\n+    return IterableInput(iter_arg_index, static_cast<int>(side->scope),\n                          contracting_dimension, input_ty,\n                          StorageType(b, input_ty), hlo_instr, side,\n                          /*boundary_checks=*/{}, block_k);\n@@ -1710,7 +1682,7 @@ class IterableInput {\n \n   // Index of the iter_arg of the ForOp associated with this input.\n   size_t iter_arg_index_;\n-  // Index used to differentiate it between LHS, RHS, and META inputs.\n+  // Index used to differentiate it between LHS and RHS inputs.\n   size_t operand_index_;\n   // Index of the contracting dimension in the input.\n   int contracting_dimension_;\n@@ -1851,7 +1823,7 @@ absl::Status EmitForLoopBody(EmitterLocOpBuilder& b,\n                              const llvm::SmallVector<IterableInput>& inputs,\n                              Value ki, ValueRange iter_args) {\n   SmallVector<Value> args_for_yield;\n-  std::array<absl::flat_hash_map<const HloInstruction*, Value>, 3> values;\n+  std::array<absl::flat_hash_map<const HloInstruction*, Value>, 2> values;\n \n   // Load tiles of all parameters of LHS and RHS scopes and advance pointers.\n   for (const IterableInput& input : inputs) {\n@@ -1882,8 +1854,6 @@ absl::Status EmitForLoopBody(EmitterLocOpBuilder& b,\n     dot_rhs = EmitMaskOnInput(b, MaskExpandDimension::kMinor, dot_rhs, 1, ki,\n                               dims.k, dims.config.block_k, scopes.pid_k(),\n                               dims.config.block_n);\n-    // Masking the metadata is not necessary, as the inputs are masked\n-    // (i.e. zeroed out), so the padded metadata can hold any values.\n   }\n \n   TF_ASSIGN_OR_RETURN(\n@@ -1950,7 +1920,7 @@ absl::Status EmitMatMul(EmitterLocOpBuilder& b,\n   ma::ConstantOp accumulator_init =\n       CreateConst(b, acc_ty, 0, {block_m, block_n});\n \n-  // Calculate the sizes of the lhs, rhs, meta, and output sides.\n+  // Calculate the sizes of the lhs, rhs, and output sides.\n   Scopes scopes(b, dot_instr, analysis, dims, config, launch_config);\n \n   llvm::SmallVector<IterableInput> inputs;"
        },
        {
            "sha": "6936694a8de2a848cce04eb34b329eb63461caee",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fusion_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc?ref=cccd091be6fe28ba83d94d41d297c790f18a7bde",
            "patch": "@@ -248,8 +248,6 @@ class GemmDimensionAdapter {\n                        lhs_noncontracting_index,\n                        dot_.shape().dimensions_size() - 1};\n         break;\n-      case TritonFusionAnalysis::Scope::META:\n-        LOG(FATAL) << \"Unsupported scope.\";\n     }\n \n     Result result;"
        },
        {
            "sha": "e3339958ef8c4e8adad0d30e717cbbb55a2ad6ca",
            "filename": "third_party/xla/xla/service/gpu/triton_fusion_analysis.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc?ref=cccd091be6fe28ba83d94d41d297c790f18a7bde",
            "patch": "@@ -234,11 +234,8 @@ bool TritonFusionAnalysis::IsBatchDimMinorForInt4Parameter(\n absl::Status TritonFusionAnalysis::ExecuteForDotFusion(\n     const HloInstruction& dot, const int split_k) {\n   DotRequirements lhs_requirements(kNoSplitRequirement);\n-  for (const Scope scope : {Scope::LHS, Scope::RHS, Scope::META}) {\n+  for (const Scope scope : {Scope::LHS, Scope::RHS}) {\n     const int operand_number = static_cast<int>(scope);\n-    if (dot.operand_count() < operand_number + 1) {\n-      continue;  // Meta scope is optional.\n-    }\n     TF_ASSIGN_OR_RETURN(auto context, FusionContext::FromDotOperand(\n                                           dot, operand_number, split_k));\n     TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n@@ -339,8 +336,6 @@ std::string ScopeToString(TritonFusionAnalysis::Scope s) {\n       return \"LHS\";\n     case TritonFusionAnalysis::Scope::RHS:\n       return \"RHS\";\n-    case TritonFusionAnalysis::Scope::META:\n-      return \"META\";\n     case TritonFusionAnalysis::Scope::OUTPUT:\n       return \"OUTPUT\";\n   }"
        },
        {
            "sha": "490b59ad487dde4d9850013c4906cf5fc19c6219",
            "filename": "third_party/xla/xla/service/gpu/triton_fusion_analysis.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cccd091be6fe28ba83d94d41d297c790f18a7bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.h?ref=cccd091be6fe28ba83d94d41d297c790f18a7bde",
            "patch": "@@ -50,10 +50,9 @@ class TritonFusionAnalysis {\n       const HloDotInstruction& dot, int split_k = 1);\n \n   // A scope is an HLO graph that can be tiled efficiently using same or\n-  // compatible tile shapes on all operations. GEMM fusion has 3 or 4 scopes\n-  // defined by left operand, right operand, optional meta (third operand) and\n-  // output.\n-  enum class Scope { LHS = 0, RHS = 1, META = 2, OUTPUT = 3 };\n+  // compatible tile shapes on all operations. GEMM dot fusion has 3 scopes\n+  // defined by left operand, right operand and output.\n+  enum class Scope { LHS = 0, RHS = 1, OUTPUT = 2 };\n \n   using IterationSpecByInstructionMap =\n       ConstHloInstructionMap<TensorIterationSpec>;"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 13,
        "deletions": 51
    }
}