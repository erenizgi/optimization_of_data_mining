{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 850610151",
    "sha": "6d72e276f3c2d3da847a39a253f7c52673fe2f4d",
    "files": [
        {
            "sha": "b5d1447c24b649502895fb0665d7a15bddfbae40",
            "filename": "tensorflow/core/data/root_dataset.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d72e276f3c2d3da847a39a253f7c52673fe2f4d/tensorflow%2Fcore%2Fdata%2Froot_dataset.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d72e276f3c2d3da847a39a253f7c52673fe2f4d/tensorflow%2Fcore%2Fdata%2Froot_dataset.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdata%2Froot_dataset.cc?ref=6d72e276f3c2d3da847a39a253f7c52673fe2f4d",
            "patch": "@@ -114,24 +114,24 @@ void AddTraceMetadata(const RootDataset::Params& params, const Options& options,\n         kAlgorithm, model::AutotuneAlgorithm_Name(params.autotune_algorithm)));\n     trace_metadata->push_back(std::make_pair(\n         kCpuBudget,\n-        strings::Printf(\"%lld\", static_cast<long long>(\n+        absl::StrFormat(\"%lld\", static_cast<long long>(\n                                     params.autotune_cpu_budget_func()))));\n     int64_t ram_budget = params.ComputeInitialAutotuneRamBudget();\n     trace_metadata->push_back(std::make_pair(\n         kRamBudget,\n-        strings::Printf(\"%lld\", static_cast<long long>(ram_budget / 1.0e6))));\n+        absl::StrFormat(\"%lld\", static_cast<long long>(ram_budget / 1.0e6))));\n   }\n   if (params.max_intra_op_parallelism >= 0) {\n     trace_metadata->push_back(std::make_pair(\n         kIntraOpParallelism,\n-        strings::Printf(\"%lld\", static_cast<long long>(value_or_default(\n+        absl::StrFormat(\"%lld\", static_cast<long long>(value_or_default(\n                                     params.max_intra_op_parallelism, 0,\n                                     port::MaxParallelism())))));\n   }\n   if (params.private_threadpool_size >= 0) {\n     trace_metadata->push_back(std::make_pair(\n         kPrivateThreadpoolSize,\n-        strings::Printf(\"%lld\", static_cast<long long>(value_or_default(\n+        absl::StrFormat(\"%lld\", static_cast<long long>(value_or_default(\n                                     params.private_threadpool_size, 0,\n                                     port::MaxParallelism())))));\n   }\n@@ -210,7 +210,7 @@ class RootDataset::Iterator : public DatasetIterator<RootDataset> {\n         ctx->SetModel(model_);\n       }\n \n-      absl::flat_hash_set<string> experiments = GetExperiments();\n+      absl::flat_hash_set<std::string> experiments = GetExperiments();\n       if (experiments.contains(\"stage_based_autotune_v2\")) {\n         model_->AddExperiment(\"stage_based_autotune_v2\");\n       }\n@@ -281,13 +281,13 @@ class RootDataset::Iterator : public DatasetIterator<RootDataset> {\n     if (mem_bw != INT64_MAX) {\n       traceme_metadata.push_back(std::make_pair(\n           kMemBandwidth,\n-          strings::Printf(\"%lld\", static_cast<long long>(mem_bw))));\n+          absl::StrFormat(\"%lld\", static_cast<long long>(mem_bw))));\n     }\n     const auto memory_info = port::GetMemoryInfo();\n     const auto memory_usage = memory_info.total - memory_info.free;\n     traceme_metadata.push_back(std::make_pair(\n         kRamUsage,\n-        strings::Printf(\"%lld out of %lld (%.2f%%)\",\n+        absl::StrFormat(\"%lld out of %lld (%.2f%%)\",\n                         static_cast<long long>(memory_usage / 1.0e6),\n                         static_cast<long long>(memory_info.total / 1.0e6),\n                         static_cast<double>(100 * memory_usage) /\n@@ -296,7 +296,7 @@ class RootDataset::Iterator : public DatasetIterator<RootDataset> {\n     if (io_statistics.roundtrip_latency_usec.count > 0) {\n       traceme_metadata.push_back(std::make_pair(\n           kReadRoundtripLatency,\n-          strings::Printf(\n+          absl::StrFormat(\n               \"(count: %lld, mean: %lld, std dev: %lld)\",\n               static_cast<long long>(\n                   io_statistics.roundtrip_latency_usec.count),\n@@ -307,7 +307,7 @@ class RootDataset::Iterator : public DatasetIterator<RootDataset> {\n     if (io_statistics.response_bytes.count > 0) {\n       traceme_metadata.push_back(std::make_pair(\n           kReadResponseBytes,\n-          strings::Printf(\n+          absl::StrFormat(\n               \"(count: %lld, mean: %lld, std dev: %lld)\",\n               static_cast<long long>(io_statistics.response_bytes.count),\n               static_cast<long long>(io_statistics.response_bytes.mean),\n@@ -413,7 +413,7 @@ RootDataset::RootDataset(core::RefCountPtr<DatasetBase> input,\n RootDataset::~RootDataset() = default;\n \n std::unique_ptr<IteratorBase> RootDataset::MakeIteratorInternal(\n-    const string& prefix) const {\n+    const std::string& prefix) const {\n   return std::make_unique<Iterator>(\n       Iterator::Params{this, name_utils::IteratorPrefix(kDatasetType, prefix)});\n }\n@@ -426,15 +426,15 @@ const std::vector<PartialTensorShape>& RootDataset::output_shapes() const {\n   return input_->output_shapes();\n }\n \n-string RootDataset::DebugString() const {\n+std::string RootDataset::DebugString() const {\n   return name_utils::DatasetDebugString(kDatasetType);\n }\n \n int64_t RootDataset::CardinalityInternal(CardinalityOptions options) const {\n   return input_->Cardinality(options);\n }\n \n-absl::Status RootDataset::Get(OpKernelContext* ctx, int64 index,\n+absl::Status RootDataset::Get(OpKernelContext* ctx, int64_t index,\n                               std::vector<Tensor>* out_tensors) const {\n   std::vector<const DatasetBase*> inputs;\n   TF_RETURN_IF_ERROR(this->InputDatasets(&inputs));"
        },
        {
            "sha": "cb1b7d235d7473ba2e9eb0f57af363540fe05c82",
            "filename": "tensorflow/core/data/serialization_utils.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 35,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d72e276f3c2d3da847a39a253f7c52673fe2f4d/tensorflow%2Fcore%2Fdata%2Fserialization_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d72e276f3c2d3da847a39a253f7c52673fe2f4d/tensorflow%2Fcore%2Fdata%2Fserialization_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdata%2Fserialization_utils.cc?ref=6d72e276f3c2d3da847a39a253f7c52673fe2f4d",
            "patch": "@@ -60,8 +60,8 @@ constexpr char kOutputNode[] = \".output_node\";\n \n absl::Status FromGraphDef(\n     FunctionLibraryRuntime* flr, const GraphDef& graph_def,\n-    const std::vector<std::pair<string, Tensor>>& input_list,\n-    const string& output_node, Tensor* result) {\n+    const std::vector<std::pair<std::string, Tensor>>& input_list,\n+    const std::string& output_node, Tensor* result) {\n   FunctionLibraryRuntime* cloned_flr = nullptr;\n   std::unique_ptr<ProcessFunctionLibraryRuntime> pflr = nullptr;\n   std::unique_ptr<FunctionLibraryDefinition> lib_def = nullptr;\n@@ -80,7 +80,7 @@ absl::Status FromGraphDef(\n // FindStatefulOps searches `graph_def` for all of its stateful ops storing\n // their names in `stateful_op_names`.\n absl::Status FindStatefulOps(const GraphDef& graph_def,\n-                             std::vector<string>* stateful_op_names) {\n+                             std::vector<std::string>* stateful_op_names) {\n   FunctionLibraryDefinition lib_def(OpRegistry::Global(), graph_def.library());\n \n   // Iterate over all nodes in the graph.\n@@ -174,12 +174,12 @@ absl::Status UpdateCheckpointElements(\n VariantTensorDataReader::VariantTensorDataReader(\n     const std::vector<const tensorflow::VariantTensorData*>& data) {\n   for (const auto& d : data) {\n-    string metadata;\n+    std::string metadata;\n     d->get_metadata(&metadata);\n     auto keys = str_util::Split(metadata, kDelimiter, str_util::SkipEmpty());\n-    const string name = keys[0];\n+    const std::string name = keys[0];\n     data_[name] = d;\n-    map_[name] = std::map<string, size_t>();\n+    map_[name] = std::map<std::string, size_t>();\n     for (size_t i = 1; i < keys.size(); ++i) {\n       map_[name][keys[i]] = i - 1;\n     }\n@@ -188,7 +188,7 @@ VariantTensorDataReader::VariantTensorDataReader(\n \n absl::Status VariantTensorDataReader::ReadScalar(absl::string_view key,\n                                                  int64_t* val) const {\n-  string prefix;\n+  std::string prefix;\n   TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n   return ReadScalar(prefix, key, val);\n }\n@@ -201,7 +201,7 @@ absl::Status VariantTensorDataReader::ReadScalar(absl::string_view name,\n \n absl::Status VariantTensorDataReader::ReadScalar(absl::string_view key,\n                                                  tstring* val) const {\n-  string prefix;\n+  std::string prefix;\n   TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n   return ReadScalar(prefix, key, val);\n }\n@@ -214,15 +214,15 @@ absl::Status VariantTensorDataReader::ReadScalar(absl::string_view name,\n \n absl::Status VariantTensorDataReader::ReadTensor(absl::string_view key,\n                                                  Tensor* val) const {\n-  string prefix;\n+  std::string prefix;\n   TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n   return ReadTensor(prefix, key, val);\n }\n \n absl::Status VariantTensorDataReader::ReadTensor(FunctionLibraryRuntime* flr,\n                                                  absl::string_view key,\n                                                  Tensor* val) const {\n-  string prefix;\n+  std::string prefix;\n   TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n   return ReadTensorInternal(flr, prefix, key, val);\n }\n@@ -241,7 +241,7 @@ absl::Status VariantTensorDataReader::ReadTensor(FunctionLibraryRuntime* flr,\n }\n \n bool VariantTensorDataReader::Contains(absl::string_view key) const {\n-  string prefix;\n+  std::string prefix;\n   if (!ExtractIteratorPrefix(key, &prefix).ok()) {\n     return false;\n   }\n@@ -250,26 +250,26 @@ bool VariantTensorDataReader::Contains(absl::string_view key) const {\n \n bool VariantTensorDataReader::Contains(absl::string_view n,\n                                        absl::string_view key) const {\n-  string name(n);\n+  std::string name(n);\n   auto it = map_.find(name);\n   if (it == map_.end()) {\n     return false;\n   }\n   const auto& bucket = it->second;\n-  return bucket.find(string(key)) != bucket.end();\n+  return bucket.find(std::string(key)) != bucket.end();\n }\n \n template <typename T>\n absl::Status VariantTensorDataReader::ReadScalarInternal(absl::string_view n,\n                                                          absl::string_view key,\n                                                          T* val) const {\n-  string name(n);\n+  std::string name(n);\n   auto it = map_.find(name);\n   if (it == map_.end()) {\n     return errors::NotFound(name);\n   }\n   const auto& bucket = it->second;\n-  auto key_it = bucket.find(string(key));\n+  auto key_it = bucket.find(std::string(key));\n   if (key_it == bucket.end()) {\n     return errors::NotFound(key);\n   }\n@@ -283,13 +283,13 @@ absl::Status VariantTensorDataReader::ReadTensorInternal(\n   if (Contains(n, absl::StrCat(key, kIsDataset))) {\n     return ReadDatasetInternal(flr, n, key, val);\n   }\n-  string name(n);\n+  std::string name(n);\n   auto it = map_.find(name);\n   if (it == map_.end()) {\n     return errors::NotFound(name);\n   }\n   const auto& bucket = it->second;\n-  auto key_it = bucket.find(string(key));\n+  auto key_it = bucket.find(std::string(key));\n   if (key_it == bucket.end()) {\n     return errors::NotFound(key);\n   }\n@@ -314,12 +314,12 @@ absl::Status VariantTensorDataReader::ReadDatasetInternal(\n   return absl::OkStatus();\n }\n \n-std::map<string, Tensor> VariantTensorDataReader::ReadAllTensors() {\n-  std::map<string, Tensor> result;\n+std::map<std::string, Tensor> VariantTensorDataReader::ReadAllTensors() {\n+  std::map<std::string, Tensor> result;\n   for (const auto& entry : map_) {\n-    string key1 = entry.first;\n+    std::string key1 = entry.first;\n     for (const auto& inner : entry.second) {\n-      string key2 = inner.first;\n+      std::string key2 = inner.first;\n       size_t index = inner.second;\n       result[absl::StrCat(key1, kDelimiter, key2)] =\n           data_[key1]->tensors(index);\n@@ -330,7 +330,7 @@ std::map<string, Tensor> VariantTensorDataReader::ReadAllTensors() {\n \n absl::Status VariantTensorDataWriter::WriteScalar(absl::string_view key,\n                                                   const int64_t val) {\n-  string prefix;\n+  std::string prefix;\n   TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n   return WriteScalar(prefix, key, val);\n }\n@@ -343,7 +343,7 @@ absl::Status VariantTensorDataWriter::WriteScalar(absl::string_view name,\n \n absl::Status VariantTensorDataWriter::WriteScalar(absl::string_view key,\n                                                   const tstring& val) {\n-  string prefix;\n+  std::string prefix;\n   TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n   return WriteScalar(prefix, key, val);\n }\n@@ -356,7 +356,7 @@ absl::Status VariantTensorDataWriter::WriteScalar(absl::string_view name,\n \n absl::Status VariantTensorDataWriter::WriteTensor(absl::string_view key,\n                                                   const Tensor& val) {\n-  string prefix;\n+  std::string prefix;\n   TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n   return WriteTensor(prefix, key, val);\n }\n@@ -370,8 +370,8 @@ absl::Status VariantTensorDataWriter::WriteTensor(absl::string_view name,\n void VariantTensorDataWriter::MaybeFlush() {\n   if (is_flushed_) return;\n   for (auto& keys : keys_) {\n-    const string name = keys.first;\n-    string metadata = name;\n+    const std::string name = keys.first;\n+    std::string metadata = name;\n     for (size_t i = 0; i < keys_[name].size(); ++i) {\n       absl::StrAppend(&metadata, kDelimiter, keys_[name][i]);\n     }\n@@ -426,12 +426,12 @@ absl::Status VariantTensorDataWriter::WriteTensorInternal(absl::string_view n,\n     return errors::FailedPrecondition(\n         \"Cannot call WriteTensor after GetData or ReleaseData is called\");\n   }\n-  DCHECK_EQ(key.find(kDelimiter), string::npos);\n-  string name(n);\n+  DCHECK_EQ(key.find(kDelimiter), std::string::npos);\n+  std::string name(n);\n   if (keys_.count(name) == 0) {\n-    keys_[name] = std::vector<string>();\n+    keys_[name] = std::vector<std::string>();\n   }\n-  keys_[name].push_back(string(key));\n+  keys_[name].push_back(std::string(key));\n   if (data_.count(name) == 0) {\n     data_[name] = std::make_unique<VariantTensorData>();\n     data_[name]->set_type_name(\"tensorflow::Iterator\");\n@@ -445,14 +445,14 @@ absl::Status VariantTensorDataWriter::WriteDatasetInternal(\n   GraphDef graph_def;\n   SerializationContext ctx((SerializationContext::Params()));\n   TF_RETURN_IF_ERROR(AsGraphDef(dataset, std::move(ctx), &graph_def));\n-  string output_node;\n+  std::string output_node;\n   for (const auto& node : graph_def.node()) {\n     if (node.op() == kRetvalOp) {\n       output_node = node.input(0);\n       break;\n     }\n   }\n-  string result;\n+  std::string result;\n   graph_def.SerializeToString(&result);\n   TF_RETURN_IF_ERROR(WriteScalar(n, absl::StrCat(key, kIsDataset), \"\"));\n   TF_RETURN_IF_ERROR(\n@@ -553,8 +553,8 @@ REGISTER_UNARY_VARIANT_DECODE_FUNCTION(IteratorStateVariant,\n \n absl::Status AsGraphDefForRewrite(\n     OpKernelContext* ctx, const DatasetBase* input,\n-    std::vector<std::pair<string, Tensor>>* input_list, GraphDef* result,\n-    string* dataset_node) {\n+    std::vector<std::pair<std::string, Tensor>>* input_list, GraphDef* result,\n+    std::string* dataset_node) {\n   SerializationContext::Params params(ctx);\n   params.input_list = input_list;\n   params.external_state_policy = ExternalStatePolicy::POLICY_IGNORE;\n@@ -580,7 +580,7 @@ absl::Status AsGraphDef(const DatasetBase* dataset,\n   }\n   if (serialization_ctx.external_state_policy() ==\n       ExternalStatePolicy::POLICY_WARN) {\n-    std::vector<string> stateful_op_names;\n+    std::vector<std::string> stateful_op_names;\n     TF_RETURN_IF_ERROR(FindStatefulOps(*graph_def, &stateful_op_names));\n     if (!stateful_op_names.empty()) {\n       LOG(WARNING) << \"We found the following stateful ops in the dataset \""
        }
    ],
    "stats": {
        "total": 94,
        "additions": 47,
        "deletions": 47
    }
}