{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 830279814",
    "sha": "5ff7541cb811d385fb069b94f2ca7eed4a87fbb1",
    "files": [
        {
            "sha": "f3c3fd045a3d6fb7b7fd547273a020aacc87d878",
            "filename": "tensorflow/cc/gradients/array_grad.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Farray_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Farray_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcc%2Fgradients%2Farray_grad.cc?ref=5ff7541cb811d385fb069b94f2ca7eed4a87fbb1",
            "patch": "@@ -218,9 +218,9 @@ REGISTER_GRADIENT_OP(\"GatherNd\", GatherNdGrad);\n absl::Status CheckNumericsGrad(const Scope& scope, const Operation& op,\n                                const std::vector<Output>& grad_inputs,\n                                std::vector<Output>* grad_outputs) {\n-  string message;\n+  std::string message;\n   TF_RETURN_IF_ERROR(GetNodeAttr(op.node()->attrs(), \"message\", &message));\n-  string err_msg = absl::StrCat(\n+  std::string err_msg = absl::StrCat(\n       \"Not a number (NaN) or infinity (Inf) values detected in gradient. \",\n       message);\n   grad_outputs->push_back(CheckNumerics(scope, grad_inputs[0], err_msg));\n@@ -411,7 +411,7 @@ REGISTER_GRADIENT_OP(\"DepthToSpace\", DepthToSpaceGrad);\n absl::Status MirrorPadGrad(const Scope& scope, const Operation& op,\n                            const std::vector<Output>& grad_inputs,\n                            std::vector<Output>* grad_outputs) {\n-  string mode;\n+  std::string mode;\n   TF_RETURN_IF_ERROR(GetNodeAttr(op.node()->attrs(), \"mode\", &mode));\n   grad_outputs->push_back(tensorflow::ops::internal::MirrorPadGrad(\n       scope, grad_inputs[0], op.input(1), mode));\n@@ -424,7 +424,7 @@ REGISTER_GRADIENT_OP(\"MirrorPad\", MirrorPadGrad);\n absl::Status MirrorPadGradGrad(const Scope& scope, const Operation& op,\n                                const std::vector<Output>& grad_inputs,\n                                std::vector<Output>* grad_outputs) {\n-  string mode;\n+  std::string mode;\n   TF_RETURN_IF_ERROR(GetNodeAttr(op.node()->attrs(), \"mode\", &mode));\n   grad_outputs->push_back(MirrorPad(scope, grad_inputs[0], op.input(1), mode));\n   grad_outputs->push_back(NoGradient());"
        },
        {
            "sha": "deb90eec264ee7cd4dac9c7ec5731af49af30f8d",
            "filename": "tensorflow/cc/gradients/image_grad.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fimage_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fimage_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcc%2Fgradients%2Fimage_grad.cc?ref=5ff7541cb811d385fb069b94f2ca7eed4a87fbb1",
            "patch": "@@ -95,7 +95,7 @@ absl::Status ScaleAndTranslateGradHelper(const Scope& scope,\n                                          const Operation& op,\n                                          const std::vector<Output>& grad_inputs,\n                                          std::vector<Output>* grad_outputs) {\n-  string kernel_type;\n+  std::string kernel_type;\n   TF_RETURN_IF_ERROR(\n       GetNodeAttr(op.node()->attrs(), \"kernel_type\", &kernel_type));\n   bool antialias;\n@@ -117,7 +117,7 @@ absl::Status CropAndResizeGradHelper(const Scope& scope, const Operation& op,\n                                      const std::vector<Output>& grad_inputs,\n                                      std::vector<Output>* grad_outputs) {\n   DataType input_type;\n-  string method;\n+  std::string method;\n   TF_RETURN_IF_ERROR(GetNodeAttr(op.node()->attrs(), \"method\", &method));\n   TF_RETURN_IF_ERROR(GetNodeAttr(op.node()->attrs(), \"T\", &input_type));\n   auto image_shape = Shape(scope, op.input(0));"
        },
        {
            "sha": "b77f551223702458ee221fa8bbe604ddf5be9454",
            "filename": "tensorflow/cc/gradients/image_grad_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fimage_grad_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fimage_grad_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcc%2Fgradients%2Fimage_grad_test.cc?ref=5ff7541cb811d385fb069b94f2ca7eed4a87fbb1",
            "patch": "@@ -203,7 +203,7 @@ class ScaleAndTranslateGradTest : public ::testing::Test {\n \n   template <typename T>\n   void MakeOp(const Tensor& x_data, const Input& y_shape, Input scale,\n-              Input translation, const string& kernel_type, bool antialias,\n+              Input translation, const std::string& kernel_type, bool antialias,\n               Output* x, Output* y) {\n     *x = Const<T>(scope_, x_data);\n     *y = ScaleAndTranslate(scope_, *x, y_shape, scale, translation,\n@@ -216,7 +216,7 @@ class ScaleAndTranslateGradTest : public ::testing::Test {\n   template <typename X_T, typename Y_T, typename JAC_T>\n   void TestScaleAndTranslate(const TensorShape x_shape, const int out_height,\n                              const int out_width, Input scale,\n-                             Input translation, const string& kernel_type,\n+                             Input translation, const std::string& kernel_type,\n                              bool antialias) {\n     Tensor x_data = MakeData<X_T>(x_shape);\n     Output x, y;"
        },
        {
            "sha": "c785af15f9544777e26eec2bb32ed84af4f817d8",
            "filename": "tensorflow/cc/gradients/math_grad.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fmath_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fmath_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcc%2Fgradients%2Fmath_grad.cc?ref=5ff7541cb811d385fb069b94f2ca7eed4a87fbb1",
            "patch": "@@ -1070,8 +1070,8 @@ absl::Status MatMulGradHelper(const Scope& scope, const bool is_batch,\n absl::Status MatMulGradCommon(const Scope& scope, const Operation& op,\n                               const bool is_batch,\n                               const std::vector<Output>& grad_inputs,\n-                              const string& attr_adj_x,\n-                              const string& attr_adj_y,\n+                              const std::string& attr_adj_x,\n+                              const std::string& attr_adj_y,\n                               std::vector<Output>* grad_outputs) {\n   auto a = op.input(0);\n   auto b = op.input(1);"
        },
        {
            "sha": "6309080492c1dad0595be392c3b0d0c76e258d38",
            "filename": "tensorflow/cc/gradients/nn_grad.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fnn_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ff7541cb811d385fb069b94f2ca7eed4a87fbb1/tensorflow%2Fcc%2Fgradients%2Fnn_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcc%2Fgradients%2Fnn_grad.cc?ref=5ff7541cb811d385fb069b94f2ca7eed4a87fbb1",
            "patch": "@@ -54,7 +54,7 @@ absl::Status SoftmaxGrad(const Scope& scope, const Operation& op,\n REGISTER_GRADIENT_OP(\"Softmax\", SoftmaxGrad);\n \n bool IsZero(const Scope& scope, const Output& grad) {\n-  string op_type_name = grad.op().node()->type_string();\n+  std::string op_type_name = grad.op().node()->type_string();\n   if (op_type_name == \"ZerosLike\" || op_type_name == \"Zeros\") {\n     return true;\n   }\n@@ -204,7 +204,7 @@ REGISTER_GRADIENT_OP(\"L2Loss\", L2LossGrad);\n absl::Status BiasAddGradHelper(const Scope& scope, const Operation& op,\n                                const std::vector<Output>& grad_inputs,\n                                std::vector<Output>* grad_outputs) {\n-  string data_format;\n+  std::string data_format;\n   TF_RETURN_IF_ERROR(\n       GetNodeAttr(op.output(0).node()->attrs(), \"data_format\", &data_format));\n   auto dx_1 =\n@@ -218,9 +218,9 @@ REGISTER_GRADIENT_OP(\"BiasAdd\", BiasAddGradHelper);\n absl::Status Conv2DGrad(const Scope& scope, const Operation& op,\n                         const std::vector<Output>& grad_inputs,\n                         std::vector<Output>* grad_outputs) {\n-  string data_format;\n-  string padding;\n-  std::vector<int32> strides;\n+  std::string data_format;\n+  std::string padding;\n+  std::vector<int32_t> strides;\n   bool use_cudnn_on_gpu;\n   auto attrs = op.output(0).node()->attrs();\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"data_format\", &data_format));\n@@ -245,10 +245,10 @@ REGISTER_GRADIENT_OP(\"Conv2D\", Conv2DGrad);\n absl::Status MaxPoolGradHelper(const Scope& scope, const Operation& op,\n                                const std::vector<Output>& grad_inputs,\n                                std::vector<Output>* grad_outputs) {\n-  string data_format;\n-  string padding;\n-  std::vector<int32> strides;\n-  std::vector<int32> ksize;\n+  std::string data_format;\n+  std::string padding;\n+  std::vector<int32_t> strides;\n+  std::vector<int32_t> ksize;\n   auto attrs = op.output(0).node()->attrs();\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"data_format\", &data_format));\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"ksize\", &ksize));\n@@ -265,8 +265,8 @@ REGISTER_GRADIENT_OP(\"MaxPool\", MaxPoolGradHelper);\n absl::Status MaxPoolGradV2Helper(const Scope& scope, const Operation& op,\n                                  const std::vector<Output>& grad_inputs,\n                                  std::vector<Output>* grad_outputs) {\n-  string data_format;\n-  string padding;\n+  std::string data_format;\n+  std::string padding;\n   auto attrs = op.output(0).node()->attrs();\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"data_format\", &data_format));\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"padding\", &padding));\n@@ -283,10 +283,10 @@ REGISTER_GRADIENT_OP(\"MaxPoolV2\", MaxPoolGradV2Helper);\n absl::Status MaxPool3DGradHelper(const Scope& scope, const Operation& op,\n                                  const std::vector<Output>& grad_inputs,\n                                  std::vector<Output>* grad_outputs) {\n-  std::vector<int32> ksize;\n-  std::vector<int32> strides;\n-  string padding;\n-  string data_format;\n+  std::vector<int32_t> ksize;\n+  std::vector<int32_t> strides;\n+  std::string padding;\n+  std::string data_format;\n   auto attrs = op.output(0).node()->attrs();\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"ksize\", &ksize));\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"strides\", &strides));\n@@ -304,10 +304,10 @@ REGISTER_GRADIENT_OP(\"MaxPool3D\", MaxPool3DGradHelper);\n absl::Status AvgPoolGradHelper(const Scope& scope, const Operation& op,\n                                const std::vector<Output>& grad_inputs,\n                                std::vector<Output>* grad_outputs) {\n-  std::vector<int32> ksize;\n-  std::vector<int32> strides;\n-  string padding;\n-  string data_format;\n+  std::vector<int32_t> ksize;\n+  std::vector<int32_t> strides;\n+  std::string padding;\n+  std::string data_format;\n   auto attrs = op.output(0).node()->attrs();\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"ksize\", &ksize));\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"strides\", &strides));\n@@ -325,10 +325,10 @@ REGISTER_GRADIENT_OP(\"AvgPool\", AvgPoolGradHelper);\n absl::Status AvgPool3DGradHelper(const Scope& scope, const Operation& op,\n                                  const std::vector<Output>& grad_inputs,\n                                  std::vector<Output>* grad_outputs) {\n-  std::vector<int32> ksize;\n-  std::vector<int32> strides;\n-  string padding;\n-  string data_format;\n+  std::vector<int32_t> ksize;\n+  std::vector<int32_t> strides;\n+  std::string padding;\n+  std::string data_format;\n   auto attrs = op.output(0).node()->attrs();\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"ksize\", &ksize));\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"strides\", &strides));"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 33,
        "deletions": 33
    }
}