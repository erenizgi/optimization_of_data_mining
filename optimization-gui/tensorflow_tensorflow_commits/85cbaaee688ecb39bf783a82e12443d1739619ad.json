{
    "author": "philipphack",
    "message": "PR #29073: SPMD Partitioning for MX Block Scaled Dots\n\nImported from GitHub PR https://github.com/openxla/xla/pull/29073\n\nEnables the SPMD partitioning of custom calls representing [block-scaled dots based on microscaling (MX) formats](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf) of the form\n\n(A, B, X_A, X_B) -> D,\n\nwhere X_A and X_B are the tensor block scales of dot operands A and B.\n\nThe implementation extends the present partitioning rules for regular dots to the block-scaled case by templating the functions of the SPMD dot handler. This is a non-functional change for convolutions and dots not operating on MX types.\nCopybara import of the project:\n\n--\nd33b9d92a9ab67f99fa64c112f5a8351650706a5 by Philipp Hack <phack@nvidia.com>:\n\nSPMD partitioning for block-scaled dots implemented as custom calls.\n\nMerging this change closes #29073\n\nPiperOrigin-RevId: 803415369",
    "sha": "85cbaaee688ecb39bf783a82e12443d1739619ad",
    "files": [
        {
            "sha": "07bb582f76f1be1c5991aa37fd3a3d70f51ece64",
            "filename": "third_party/xla/xla/service/spmd/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -30,6 +30,7 @@ cc_library(\n     hdrs = [\n         \"convolution_handler.h\",\n         \"custom_call_handler.h\",\n+        \"dot_handler.h\",\n         \"spmd_partitioner.h\",\n         \"spmd_partitioner_util.h\",\n     ],\n@@ -435,10 +436,12 @@ xla_cc_test(\n     srcs = [\"dot_handler_test.cc\"],\n     deps = [\n         \":stateful_rng_spmd_partitioner\",\n+        \"//xla:literal_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_verifier\",\n         \"//xla/service:sharding_propagation\","
        },
        {
            "sha": "885fd1dbdba6c5a651d09f0004b3fa05d657fac6",
            "filename": "third_party/xla/xla/service/spmd/convolution_handler.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 61,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -52,10 +52,7 @@ namespace {\n absl::StatusOr<HloInstruction*> PartitionConvolutionWithBatchGroupCount(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -135,9 +132,8 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionWithBatchGroupCount(\n       lhs.sharding(), lhs_to_output_indices);\n \n   // Create partitioned convolution.\n-  TF_ASSIGN_OR_RETURN(\n-      auto sharded_conv,\n-      create_sharded_conv(lhs.hlo(), rhs.hlo(), b, conv_window));\n+  TF_ASSIGN_OR_RETURN(auto sharded_conv,\n+                      create_sharded_conv(lhs, rhs, b, conv_window));\n   sharded_conv->set_sharding(aligned_output_sharding);\n   return PartitionedHlo(sharded_conv, output_base_shape, lhs.state())\n       .Reshard(output_sharding)\n@@ -148,10 +144,7 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionWithBatchGroupCount(\n absl::StatusOr<HloInstruction*> PartitionConvolutionWithFeatureGroupCount(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -231,9 +224,8 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionWithFeatureGroupCount(\n   auto aligned_output_sharding = hlo_sharding_util::TransposeSharding(\n       lhs.sharding(), lhs_to_output_indices);\n \n-  TF_ASSIGN_OR_RETURN(\n-      auto sharded_conv,\n-      create_sharded_conv(lhs.hlo(), rhs.hlo(), b, conv_window));\n+  TF_ASSIGN_OR_RETURN(auto sharded_conv,\n+                      create_sharded_conv(lhs, rhs, b, conv_window));\n   sharded_conv->set_sharding(aligned_output_sharding);\n   return PartitionedHlo(sharded_conv, output_base_shape, lhs.state())\n       .Reshard(output_sharding)\n@@ -246,10 +238,7 @@ absl::StatusOr<HloInstruction*>\n PartitionConvolutionWithSpatialDimensionHaloExchangeOnRHS(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -518,7 +507,11 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnRHS(\n   }\n \n   TF_ASSIGN_OR_RETURN(\n-      auto conv, create_sharded_conv(conv_lhs, rhs_with_halo, b, new_window));\n+      auto conv,\n+      create_sharded_conv(\n+          PartitionedHlo(conv_lhs, lhs.base_shape(), lhs.state()),\n+          PartitionedHlo(rhs_with_halo, rhs.base_shape(), rhs.state()), b,\n+          new_window));\n \n   auto ar = collective_ops_creator.create_cross_partition_all_reduce(\n       b, conv, MakeBinaryAdd(original_hlo->shape().element_type(), module), {},\n@@ -535,10 +528,7 @@ absl::StatusOr<HloInstruction*>\n PartitionConvolutionWithSpatialDimensionHaloExchangeOnLHS(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -745,7 +735,10 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnLHS(\n   }\n \n   TF_ASSIGN_OR_RETURN(\n-      auto conv, create_sharded_conv(lhs_with_halo, rhs.hlo(), b, new_window));\n+      auto conv,\n+      create_sharded_conv(\n+          PartitionedHlo(lhs_with_halo, lhs.base_shape(), lhs.state()), rhs, b,\n+          new_window));\n   auto ar =\n       lhs.state().collective_ops_creator.create_cross_partition_all_reduce(\n           b, conv, MakeBinaryAdd(output_base_shape.element_type(), module), {},\n@@ -761,10 +754,7 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnLHS(\n absl::StatusOr<HloInstruction*> PartitionConvolutionTiledOutput(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n   const auto& dnums = original_hlo->convolution_dimension_numbers();\n@@ -833,8 +823,10 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionTiledOutput(\n \n   TF_ASSIGN_OR_RETURN(\n       auto sharded_conv,\n-      create_sharded_conv(resharded_operand_and_window->sharded_input,\n-                          rhs.hlo(), b, new_window));\n+      create_sharded_conv(\n+          PartitionedHlo(resharded_operand_and_window->sharded_input,\n+                         lhs.base_shape(), lhs.state()),\n+          rhs, b, new_window));\n \n   auto shard_shape = MakePartitionedShape(output_base_shape, output_sharding);\n   if (!resharded_operand_and_window->dynamic_slice_index_on_output\n@@ -852,10 +844,7 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionTiledOutput(\n absl::StatusOr<HloInstruction*> PartitionConvolutionBaseCase(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, const SpmdPartitionerOptions& options,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n@@ -925,6 +914,8 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionBaseCase(\n   return nullptr;\n }\n \n+}  // namespace\n+\n absl::StatusOr<std::unique_ptr<HloInstruction>> CreateShardedConvolution(\n     const HloInstruction& conv,\n     const dot_as_convolution_util::DotConvolutionDimsInfo& dot_dnums,\n@@ -1007,17 +998,12 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> CreateShardedConvolution(\n       batch_group_count, window, conv_dnums, conv.precision_config());\n }\n \n-}  // namespace\n-\n // Partition convolution.\n absl::StatusOr<HloInstruction*> PartitionConvolution(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const dot_as_convolution_util::DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, const SpmdPartitionerOptions& options,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n@@ -1039,28 +1025,13 @@ absl::Status SpmdPartitioningVisitor::HandleConvolution(HloInstruction* hlo) {\n   if (hlo->sharding().HasUniqueDevice()) {\n     return DefaultAction(hlo);\n   }\n-  const auto dims_info = dot_as_convolution_util::ParseConvolutionDimsInfo(hlo);\n-\n-  auto create_sharded_conv =\n-      [&](HloInstruction* lhs_hlo, HloInstruction* rhs_hlo,\n-          spmd::SpmdBuilder* b,\n-          const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n-    if (dims_info.conv_spatial_dims.empty() &&\n-        hlo->feature_group_count() == 1 && hlo->batch_group_count() == 1) {\n-      TF_ASSIGN_OR_RETURN(\n-          auto sharded_conv,\n-          dot_as_convolution_util::CreateShardedConvForDotGeneralConvolution(\n-              *hlo, dims_info, lhs_hlo, rhs_hlo));\n-      return b->AddInstruction(std::move(sharded_conv));\n-    } else {\n-      TF_ASSIGN_OR_RETURN(auto sharded_conv,\n-                          CreateShardedConvolution(*hlo, dims_info, lhs_hlo,\n-                                                   rhs_hlo, conv_window));\n-      return b->AddInstruction(std::move(sharded_conv));\n-    }\n-  };\n+  const dot_as_convolution_util::DotConvolutionDimsInfo dims_info =\n+      dot_as_convolution_util::ParseConvolutionDimsInfo(hlo);\n+\n+  CreateShardedConvolutionFunctor create_sharded_conv_functor(hlo, dims_info);\n \n-  return HandleDotHelper(hlo, dims_info, create_sharded_conv);\n+  return HandleDotHelper<CreateShardedConvolutionFunctor>(\n+      hlo, dims_info, create_sharded_conv_functor);\n }\n \n }  // namespace spmd"
        },
        {
            "sha": "c52c42885d1cf0396059b36709f2ed3613fc9cbf",
            "filename": "third_party/xla/xla/service/spmd/convolution_handler.h",
            "status": "modified",
            "additions": 44,
            "deletions": 4,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.h?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/service/dot_as_convolution_util.h\"\n+#include \"xla/service/spmd/dot_handler.h\"\n #include \"xla/service/spmd/spmd_partitioner.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -36,14 +37,53 @@ absl::StatusOr<HloInstruction*> PartitionConvolution(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const dot_as_convolution_util::DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, const SpmdPartitionerOptions& options,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b);\n \n+absl::StatusOr<std::unique_ptr<HloInstruction>> CreateShardedConvolution(\n+    const HloInstruction& conv,\n+    const dot_as_convolution_util::DotConvolutionDimsInfo& dot_dnums,\n+    HloInstruction* sharded_lhs_hlo, HloInstruction* sharded_rhs_hlo,\n+    const Window& conv_window);\n+\n+// Functor class for creating sharded convolutions with operands of type\n+// PartitionedHlo.\n+class CreateShardedConvolutionFunctor final\n+    : public CreateShardedFunctorBase<PartitionedHlo> {\n+ public:\n+  CreateShardedConvolutionFunctor(\n+      HloInstruction* conv,\n+      const dot_as_convolution_util::DotConvolutionDimsInfo& dims_info)\n+      : conv_(conv), dims_info_(dims_info) {}\n+\n+  // Implements the creation of sharded convolutions.\n+  absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHlo& ll, const PartitionedHlo& rr, spmd::SpmdBuilder* b,\n+      const Window& conv_window) const override {\n+    HloInstruction* l = ll.hlo();\n+    HloInstruction* r = rr.hlo();\n+    if (dims_info_.conv_spatial_dims.empty() &&\n+        conv_->feature_group_count() == 1 && conv_->batch_group_count() == 1) {\n+      TF_ASSIGN_OR_RETURN(\n+          auto sharded_conv,\n+          dot_as_convolution_util::CreateShardedConvForDotGeneralConvolution(\n+              *conv_, dims_info_, l, r));\n+      return b->AddInstruction(std::move(sharded_conv));\n+    } else {\n+      TF_ASSIGN_OR_RETURN(\n+          auto sharded_conv,\n+          CreateShardedConvolution(*conv_, dims_info_, l, r, conv_window));\n+      return b->AddInstruction(std::move(sharded_conv));\n+    }\n+  }\n+\n+ private:\n+  HloInstruction* conv_;\n+  const dot_as_convolution_util::DotConvolutionDimsInfo& dims_info_;\n+};\n+\n }  // namespace spmd\n }  // namespace xla\n "
        },
        {
            "sha": "3fb82561ec710cf95e61e343c6fd5d4e6c85f77e",
            "filename": "third_party/xla/xla/service/spmd/custom_call_handler.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -481,6 +481,39 @@ absl::Status SpmdPartitioningVisitor::HandleCustomCall(HloInstruction* hlo) {\n     return absl::OkStatus();\n   }\n \n+  // Block-scaled dot with MX operands.\n+  if (hlo->custom_call_target() == \"__op$block_scaled_dot\") {\n+    // Evaluate the dimension numbers of the block-scaled dot.\n+    int dimensions_size = hlo->operand(0)->shape().dimensions_size();\n+    TF_RET_CHECK(dimensions_size == 2 || dimensions_size == 3);\n+    DotDimensionNumbers dimension_numbers;\n+    dimension_numbers.add_lhs_contracting_dimensions(dimensions_size - 1);\n+    dimension_numbers.add_rhs_contracting_dimensions(dimensions_size - 1);\n+    if (dimensions_size == 3) {\n+      dimension_numbers.add_lhs_batch_dimensions(0);\n+      dimension_numbers.add_rhs_batch_dimensions(0);\n+    }\n+\n+    HloCustomCallInstruction* block_scaled_dot =\n+        Cast<HloCustomCallInstruction>(hlo);\n+    CreateShardedScaledDotFunctor create_sharded_scaled_dot_functor(\n+        block_scaled_dot, dimension_numbers);\n+\n+    // Create a regular dot with equivalent operand and output shape to compute\n+    // the mapping for HandleDotHelper.\n+    PrecisionConfig precision_config;\n+    precision_config.mutable_operand_precision()->Resize(\n+        2, PrecisionConfig::DEFAULT);\n+    std::unique_ptr<HloInstruction> dot = HloInstruction::CreateDot(\n+        hlo->shape(), hlo->mutable_operand(0), hlo->mutable_operand(1),\n+        dimension_numbers, precision_config);\n+    dot_as_convolution_util::DotConvolutionDimsInfo mapping =\n+        dot_as_convolution_util::ParseDotGeneralFromDot(dot.get());\n+\n+    return HandleDotHelper<CreateShardedScaledDotFunctor>(\n+        hlo, mapping, create_sharded_scaled_dot_functor);\n+  }\n+\n   return DefaultAction(hlo);\n }\n "
        },
        {
            "sha": "bfb61daf4ff4678363b12634e5c30b55dc61a30f",
            "filename": "third_party/xla/xla/service/spmd/custom_call_handler.h",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.h?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -20,6 +20,10 @@ limitations under the License.\n #include <memory>\n \n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/shape_inference.h\"\n+#include \"xla/service/spmd/dot_handler.h\"\n+#include \"xla/service/spmd/spmd_partitioner.h\"\n \n namespace xla {\n namespace spmd {\n@@ -30,6 +34,40 @@ namespace spmd {\n std::unique_ptr<HloInstruction> CreateCustomCallSPMDInternal_RotateRight(\n     HloInstruction* input, int64_t dim, int64_t amount);\n \n+// Functor class for creating sharded block-scaled dots with operands of type\n+// PartitionedHloMX.\n+class CreateShardedScaledDotFunctor final\n+    : public CreateShardedFunctorBase<PartitionedHloMX> {\n+ public:\n+  CreateShardedScaledDotFunctor(HloCustomCallInstruction* block_scaled_dot,\n+                                const DotDimensionNumbers& dimension_numbers)\n+      : block_scaled_dot_(block_scaled_dot),\n+        dimension_numbers_(dimension_numbers) {}\n+\n+  // Implements the creation of sharded block-scaled dots.\n+  absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHloMX& ll, const PartitionedHloMX& rr, SpmdBuilder* b,\n+      const Window& conv_window) const override {\n+    HloInstruction* l = ll.operand().hlo();\n+    HloInstruction* r = rr.operand().hlo();\n+    HloInstruction* l_scale = ll.scale().hlo();\n+    HloInstruction* r_scale = rr.scale().hlo();\n+    TF_ASSIGN_OR_RETURN(Shape sharded_scaled_dot_shape,\n+                        ShapeInference::InferDotOpShape(\n+                            l->shape(), r->shape(), dimension_numbers_,\n+                            /*preferred_element_type=*/\n+                            block_scaled_dot_->shape().element_type()));\n+\n+    return b->AddInstruction(HloInstruction::CreateCustomCall(\n+        sharded_scaled_dot_shape, {l, r, l_scale, r_scale},\n+        \"__op$block_scaled_dot\", \"\"));\n+  }\n+\n+ private:\n+  HloCustomCallInstruction* block_scaled_dot_;\n+  const DotDimensionNumbers& dimension_numbers_;\n+};\n+\n }  // namespace spmd\n }  // namespace xla\n "
        },
        {
            "sha": "6ed7a62bad82785c7d4572e2ac12eb7adf6163a1",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 544,
            "deletions": 433,
            "changes": 977,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"xla/service/spmd/dot_handler.h\"\n+\n #include <algorithm>\n #include <cstdint>\n #include <deque>\n@@ -40,7 +42,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/utils/hlo_sharding_util.h\"\n@@ -50,6 +51,7 @@ limitations under the License.\n #include \"xla/service/shape_inference.h\"\n #include \"xla/service/sharding_propagation.h\"\n #include \"xla/service/spmd/convolution_handler.h\"\n+#include \"xla/service/spmd/custom_call_handler.h\"\n #include \"xla/service/spmd/spmd_partitioner.h\"\n #include \"xla/service/spmd/spmd_partitioner_util.h\"\n #include \"xla/shape.h\"\n@@ -66,27 +68,103 @@ namespace xla {\n namespace spmd {\n \n namespace {\n+\n using dot_as_convolution_util::DotConvolutionDimsInfo;\n using hlo_sharding_util::GroupedSharding;\n+\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n+absl::StatusOr<HloInstruction*> PartitionDot(\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n+    const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n+    const SpmdPartitionerOptions& options, SpmdBuilder* b,\n+    std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n+        windowed_dot_general_loops,\n+    SpmdPartitioningVisitor* visitor);\n+\n+PartitionedHlo MakePartitionedHloMaybeMX(\n+    const PartitionedHlo& p, const Shape& base_shape,\n+    const PartitionedHlo::PartitioningState& state) {\n+  return PartitionedHlo(p.hlo(), base_shape, state);\n+}\n+\n+PartitionedHloMX MakePartitionedHloMaybeMX(\n+    const PartitionedHloMX& p, const std::pair<Shape, Shape>& base_shapes,\n+    const PartitionedHlo::PartitioningState& state) {\n+  return PartitionedHloMX(\n+      PartitionedHlo(p.operand().hlo(), base_shapes.first, state),\n+      PartitionedHlo(p.scale().hlo(), base_shapes.second, state));\n+}\n+\n+PartitionedHlo ReplicatePartiallySharded(\n+    const PartitionedHlo& partitioned, absl::Span<const int64_t> batch_dims,\n+    const GroupedSharding& grouped, SpmdBuilder* b,\n+    const PartitionedHlo::PartitioningState& state) {\n+  HloInstruction* partially_sharded = PerGroupSliceFromReplicated(\n+      partitioned.Replicate().hlo(), partitioned.state().partition_id,\n+      grouped.device_groups, batch_dims, grouped.group_dim_sizes, b);\n+  partially_sharded->set_sharding(HloSharding::Replicate());\n+  return PartitionedHlo(partially_sharded, partially_sharded->shape(), state);\n+}\n+\n+PartitionedHloMX ReplicatePartiallySharded(\n+    const PartitionedHloMX& partitioned, absl::Span<const int64_t> batch_dims,\n+    const GroupedSharding& grouped, SpmdBuilder* b,\n+    const PartitionedHlo::PartitioningState& state) {\n+  return PartitionedHloMX(\n+      ReplicatePartiallySharded(partitioned.operand(), batch_dims, grouped, b,\n+                                state),\n+      ReplicatePartiallySharded(partitioned.scale(), batch_dims, grouped, b,\n+                                state));\n+}\n+\n }  // namespace\n \n-absl::Status SpmdPartitioningVisitor::HandleDot(HloInstruction* hlo) {\n-  DotConvolutionDimsInfo mapping =\n-      dot_as_convolution_util::ParseDotGeneralFromDot(hlo);\n+std::pair<Shape, Shape> GetPerGroupBaseShape(\n+    const hlo_sharding_util::GroupedSharding& grouped_sharding,\n+    const PartitionedHloMX::ShapesMX& original_base_shapes) {\n+  std::pair<Shape, Shape> pair(std::move(original_base_shapes));\n+  return std::make_pair(GetPerGroupBaseShape(grouped_sharding, pair.first),\n+                        GetPerGroupBaseShape(grouped_sharding, pair.second));\n+}\n \n-  auto create_sharded_dot =\n-      [&](HloInstruction* l, HloInstruction* r, SpmdBuilder* b,\n-          const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n+// Functor class for creating sharded dots with operands of type PartitionedHlo.\n+class CreateShardedDotFunctor final\n+    : public CreateShardedFunctorBase<PartitionedHlo> {\n+ public:\n+  CreateShardedDotFunctor(HloDotInstruction* dot) : dot_(dot) {}\n+\n+  // Implements the creation of sharded dots.\n+  absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHlo& ll, const PartitionedHlo& rr, SpmdBuilder* b,\n+      const Window& conv_window) const override {\n+    HloInstruction* l = ll.hlo();\n+    HloInstruction* r = rr.hlo();\n     TF_ASSIGN_OR_RETURN(\n         auto sharded_dot_shape,\n         ShapeInference::InferDotOpShape(\n-            l->shape(), r->shape(), hlo->dot_dimension_numbers(),\n-            /*preferred_element_type=*/hlo->shape().element_type()));\n+            l->shape(), r->shape(), dot_->dot_dimension_numbers(),\n+            /*preferred_element_type=*/dot_->shape().element_type()));\n     return b->AddInstruction(HloInstruction::CreateDot(\n-        sharded_dot_shape, l, r, hlo->dot_dimension_numbers(),\n-        hlo->precision_config()));\n-  };\n-  return HandleDotHelper(hlo, mapping, create_sharded_dot);\n+        sharded_dot_shape, l, r, dot_->dot_dimension_numbers(),\n+        dot_->precision_config()));\n+  }\n+\n+ private:\n+  HloDotInstruction* dot_;\n+};\n+\n+absl::Status SpmdPartitioningVisitor::HandleDot(HloInstruction* hlo) {\n+  DotConvolutionDimsInfo mapping =\n+      dot_as_convolution_util::ParseDotGeneralFromDot(hlo);\n+\n+  HloDotInstruction* dot = Cast<HloDotInstruction>(hlo);\n+\n+  CreateShardedDotFunctor create_sharded_dot_functor(dot);\n+  return HandleDotHelper<CreateShardedDotFunctor>(hlo, mapping,\n+                                                  create_sharded_dot_functor);\n }\n \n namespace {\n@@ -278,6 +356,7 @@ bool should_enable_windowed_einsum_with_threshold(\n   }\n }\n \n+template <typename CreateShardedFunctor>\n std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n     int64_t num_partitions, int64_t output_lhs_non_contracting_partitions,\n     int64_t output_rhs_non_contracting_partitions,\n@@ -298,10 +377,7 @@ std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n     const HloInstruction* original_hlo = nullptr,\n     const PartitionedHlo* const partitioned_lhs = nullptr,\n     const PartitionedHlo* const partitioned_rhs = nullptr,\n-    std::optional<absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>>\n-        create_sharded_dot = std::nullopt,\n+    std::optional<CreateShardedFunctor> create_sharded_dot = std::nullopt,\n     SpmdBuilder* b = nullptr, HloModule* module = nullptr,\n     SpmdPartitioningVisitor* visitor = nullptr) {\n   if (num_partitions > max_iterations) {\n@@ -394,8 +470,7 @@ std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n                                           partitioned_rhs->state())\n                                .Replicate()\n                          : *partitioned_rhs;\n-      dot = (*create_sharded_dot)(new_lhs.hlo(), new_rhs.hlo(), b, conv_window)\n-                .value();\n+      dot = (*create_sharded_dot)(new_lhs, new_rhs, b, conv_window).value();\n       computation_time_in_ms = visitor->GetComputationTimeInMilliSec(dot);\n \n       collective = lhs_needs_ag ? new_lhs.hlo() : new_rhs.hlo();\n@@ -434,8 +509,7 @@ std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n       new_lhs = new_lhs.PadWithZero();\n       new_rhs = new_rhs.PadWithZero();\n \n-      dot = (*create_sharded_dot)(new_lhs.hlo(), new_rhs.hlo(), b, conv_window)\n-                .value();\n+      dot = (*create_sharded_dot)(new_lhs, new_rhs, b, conv_window).value();\n       computation_time_in_ms = visitor->GetComputationTimeInMilliSec(dot);\n \n       std::vector<int64_t> lhs_contracting_dims;\n@@ -750,15 +824,12 @@ std::vector<ReplicaGroup> GetLoopReplicaGroups(HloInstruction* while_loop) {\n // is tiled in other dimensions. Or both operands are partitioned in the same\n // way along contracting dimensions, but the output is partitioned along\n // non-contracting dimensions.\n+template <typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    int64_t loop_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    int64_t loop_partitions, const CreateShardedFunctor& create_sharded_dot,\n     const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -1163,9 +1234,12 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n     }\n \n     // The generated original dot will not be used.\n-    TF_ASSIGN_OR_RETURN(auto original_dot,\n-                        create_sharded_dot(original_dot_lhs, original_dot_rhs,\n-                                           &body_b, conv_window));\n+    TF_ASSIGN_OR_RETURN(\n+        auto original_dot,\n+        create_sharded_dot(\n+            PartitionedHlo(original_dot_lhs, lhs.base_shape(), lhs.state()),\n+            PartitionedHlo(original_dot_rhs, rhs.base_shape(), rhs.state()),\n+            &body_b, conv_window));\n     VLOG(2) << original_dot->ToString();\n \n     // Generate the correct shape of the new dot/conv.\n@@ -1334,7 +1408,10 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n       }\n     }\n     TF_ASSIGN_OR_RETURN(\n-        auto dot, create_sharded_dot(dot_lhs, dot_rhs, &body_b, conv_window));\n+        auto dot, create_sharded_dot(\n+                      PartitionedHlo(dot_lhs, lhs.base_shape(), lhs.state()),\n+                      PartitionedHlo(dot_rhs, rhs.base_shape(), rhs.state()),\n+                      &body_b, conv_window));\n     if (windowed_at_contracting_dims || operands_sharded_at_contracting_dims) {\n       // Accumulate the partial output to the result buffer.\n       o = body_b.AddInstruction(\n@@ -1709,15 +1786,13 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n // one at a time. The base shapes and shardings can be changed during the\n // recursion as we group devices together. So refer to the passed in shapes and\n // shardings for inputs and output, and do not use shape inference.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionBaseCase(\n-    PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n-    const HloSharding& output_sharding,\n+    PartitionedHloMaybeMX lhs, PartitionedHloMaybeMX rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    const CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -1782,8 +1857,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   if (lhs_batch_partitions == rhs_batch_partitions &&\n       rhs_batch_partitions == num_partitions &&\n       lhs_sharding_transposed_to_match_rhs == rhs_sharding) {\n-    TF_ASSIGN_OR_RETURN(\n-        auto dot, create_sharded_dot(lhs.hlo(), rhs.hlo(), b, conv_window));\n+    TF_ASSIGN_OR_RETURN(auto dot, create_sharded_dot(lhs, rhs, b, conv_window));\n     dot->set_sharding(*lhs_sharding_transposed_to_match_output);\n     return PartitionedHlo(dot, output_base_shape, lhs.state())\n         .Reshard(output_sharding)\n@@ -1808,7 +1882,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n         return nullptr;\n       }\n       auto resharded_rhs = rhs.Reshard(*lhs_sharding_transposed_to_match_rhs);\n-      return create_sharded_dot(lhs.hlo(), resharded_rhs.hlo(), b, conv_window);\n+      return create_sharded_dot(lhs, resharded_rhs, b, conv_window);\n     }\n     // RHS and output are batch partitioned in the same way.\n     if (rhs_batch_partitions == num_partitions &&\n@@ -1822,7 +1896,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n         return nullptr;\n       }\n       auto resharded_lhs = lhs.Reshard(*rhs_sharding_transposed_to_match_lhs);\n-      return create_sharded_dot(resharded_lhs.hlo(), rhs.hlo(), b, conv_window);\n+      return create_sharded_dot(resharded_lhs, rhs, b, conv_window);\n     }\n     return nullptr;\n   };\n@@ -1842,10 +1916,10 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   // the current dot. We also skip any reshape operand as long as it only has\n   // the lhs or rhs of the dot as the only user since reshape ops won't change\n   // the functional meaning of the pattern.\n-  auto has_reshape_operand = [](PartitionedHlo& hlo) -> bool {\n-    return hlo.hlo()->opcode() == HloOpcode::kReshape ||\n-           hlo.hlo()->opcode() == HloOpcode::kBitcast ||\n-           hlo.hlo()->opcode() == HloOpcode::kTranspose;\n+  auto has_reshape_operand = [](auto& partitioned_hlo_maybe_mx) -> bool {\n+    return partitioned_hlo_maybe_mx.hlo()->opcode() == HloOpcode::kReshape ||\n+           partitioned_hlo_maybe_mx.hlo()->opcode() == HloOpcode::kBitcast ||\n+           partitioned_hlo_maybe_mx.hlo()->opcode() == HloOpcode::kTranspose;\n   };\n   const auto& attrs = original_hlo->frontend_attributes().map();\n   bool should_skip_windowed_einsum =\n@@ -1874,42 +1948,55 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   }\n \n   std::optional<WindowedEinsumConfig> e_config = std::nullopt;\n-  if (!should_skip_windowed_einsum) {\n-    e_config = GetWindowedEinsumConfiguration(\n-        num_partitions, output_lhs_non_contracting_partitions,\n-        output_rhs_non_contracting_partitions, rhs_contracting_partitions,\n-        rhs_non_contracting_partitions, rhs_batch_partitions,\n-        lhs_contracting_partitions, lhs_non_contracting_partitions,\n-        lhs_batch_partitions, ShapeSizeInBytes(rhs.base_shape()),\n-        ShapeSizeInBytes(lhs.base_shape()), ShapeSizeInBytes(output_base_shape),\n-        options, output_sharding_transposed_to_match_lhs,\n-        output_sharding_transposed_to_match_rhs,\n-        lhs_sharding_transposed_to_match_rhs,\n-        rhs_sharding_transposed_to_match_lhs, lhs_sharding, rhs_sharding,\n-        output_sharding, conv_window, dims_mapping, indices_map,\n-        visitor->call_graph(), options.max_windowed_einsum_iteration,\n-        original_hlo, &lhs, &rhs, create_sharded_dot, b, module, visitor);\n-  }\n-  if (e_config) {\n-    int64_t loop_partitions = 1;\n-    for (int64_t dim : e_config->windowing_dims) {\n-      loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n-    }\n-    if (e_config->windowing_dims.empty()) {\n-      loop_partitions = num_partitions;\n-    }\n-\n-    VLOG(2) << \"Emit windowed dot.\";\n-    return EmitWindowedDotGeneral(\n-        lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n-        num_partitions, loop_partitions, create_sharded_dot, conv_window,\n-        module, original_hlo, options, b, windowed_dot_general_loops, *e_config,\n-        indices_map, lhs_sharding_transposed_to_match_output,\n-        rhs_sharding_transposed_to_match_output,\n-        rhs_sharding_transposed_to_match_lhs,\n-        lhs_sharding_transposed_to_match_rhs,\n-        output_sharding_transposed_to_match_rhs,\n-        output_sharding_transposed_to_match_lhs);\n+  // Disable windowed einsums for block-scaled dot.\n+  if constexpr (std::is_same_v<PartitionedHloMaybeMX, PartitionedHlo>) {\n+    if (!should_skip_windowed_einsum) {\n+      e_config = GetWindowedEinsumConfiguration<CreateShardedFunctor>(\n+          num_partitions, output_lhs_non_contracting_partitions,\n+          output_rhs_non_contracting_partitions, rhs_contracting_partitions,\n+          rhs_non_contracting_partitions, rhs_batch_partitions,\n+          lhs_contracting_partitions, lhs_non_contracting_partitions,\n+          lhs_batch_partitions, ShapeSizeInBytes(rhs.base_shape()),\n+          ShapeSizeInBytes(lhs.base_shape()),\n+          ShapeSizeInBytes(output_base_shape), options,\n+          output_sharding_transposed_to_match_lhs,\n+          output_sharding_transposed_to_match_rhs,\n+          lhs_sharding_transposed_to_match_rhs,\n+          rhs_sharding_transposed_to_match_lhs, lhs_sharding, rhs_sharding,\n+          output_sharding, conv_window, dims_mapping, indices_map,\n+          visitor->call_graph(), options.max_windowed_einsum_iteration,\n+          original_hlo, &lhs, &rhs, create_sharded_dot, b, module, visitor);\n+    }\n+    if (e_config) {\n+      int64_t loop_partitions = 1;\n+      for (int64_t dim : e_config->windowing_dims) {\n+        loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n+      }\n+      if (e_config->windowing_dims.empty()) {\n+        loop_partitions = num_partitions;\n+      }\n+      if (e_config) {\n+        int64_t loop_partitions = 1;\n+        for (int64_t dim : e_config->windowing_dims) {\n+          loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n+        }\n+        if (e_config->windowing_dims.empty()) {\n+          loop_partitions = num_partitions;\n+        }\n+\n+        VLOG(2) << \"Emit windowed dot.\";\n+        return EmitWindowedDotGeneral(\n+            lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n+            num_partitions, loop_partitions, create_sharded_dot, conv_window,\n+            module, original_hlo, options, b, windowed_dot_general_loops,\n+            *e_config, indices_map, lhs_sharding_transposed_to_match_output,\n+            rhs_sharding_transposed_to_match_output,\n+            rhs_sharding_transposed_to_match_lhs,\n+            lhs_sharding_transposed_to_match_rhs,\n+            output_sharding_transposed_to_match_rhs,\n+            output_sharding_transposed_to_match_lhs);\n+      }\n+    }\n   }\n \n   {\n@@ -1942,8 +2029,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n \n     lhs = lhs.PadWithZero();\n     rhs = rhs.PadWithZero();\n-    TF_ASSIGN_OR_RETURN(\n-        auto dot, create_sharded_dot(lhs.hlo(), rhs.hlo(), b, conv_window));\n+    TF_ASSIGN_OR_RETURN(auto dot, create_sharded_dot(lhs, rhs, b, conv_window));\n     std::vector<int64_t> lhs_contracting_dims;\n     lhs_contracting_dims.reserve(dims_mapping.contracting_dims.size());\n     for (const auto& cd : dims_mapping.contracting_dims) {\n@@ -1963,14 +2049,14 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   if (lhs_non_contracting_partitions == num_partitions &&\n       output_lhs_non_contracting_partitions == num_partitions &&\n       lhs_sharding_transposed_to_match_output == output_sharding) {\n-    return create_sharded_dot(lhs.hlo(), rhs.Replicate().hlo(), b, conv_window);\n+    return create_sharded_dot(lhs, rhs.Replicate(), b, conv_window);\n   }\n \n   // RHS and output have the same partitioned non-contracting dimensions.\n   if (rhs_non_contracting_partitions == num_partitions &&\n       output_rhs_non_contracting_partitions == num_partitions &&\n       rhs_sharding_transposed_to_match_output == output_sharding) {\n-    return create_sharded_dot(lhs.Replicate().hlo(), rhs.hlo(), b, conv_window);\n+    return create_sharded_dot(lhs.Replicate(), rhs, b, conv_window);\n   }\n \n   if (may_reshard_if_mismatch) {\n@@ -1980,24 +2066,21 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n           lhs.Reshard(*output_sharding_transposed_to_match_lhs);\n       auto resharded_rhs =\n           rhs.Reshard(*output_sharding_transposed_to_match_rhs);\n-      return create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                                conv_window);\n+      return create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window);\n     }\n     // Output is partitioned along LHS non-contracting dimensions.\n     if (output_lhs_non_contracting_partitions == num_partitions) {\n       auto resharded_lhs =\n           lhs.Reshard(*output_sharding_transposed_to_match_lhs);\n       auto replicated_rhs = rhs.Replicate();\n-      return create_sharded_dot(resharded_lhs.hlo(), replicated_rhs.hlo(), b,\n-                                conv_window);\n+      return create_sharded_dot(resharded_lhs, replicated_rhs, b, conv_window);\n     }\n     // Output is partitioned along RHS non-contracting dimensions.\n     if (output_rhs_non_contracting_partitions == num_partitions) {\n       auto replicated_lhs = lhs.Replicate();\n       auto resharded_rhs =\n           rhs.Reshard(*output_sharding_transposed_to_match_rhs);\n-      return create_sharded_dot(replicated_lhs.hlo(), resharded_rhs.hlo(), b,\n-                                conv_window);\n+      return create_sharded_dot(replicated_lhs, resharded_rhs, b, conv_window);\n     }\n   }\n \n@@ -2033,8 +2116,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   lhs = lhs.PadWithZero();\n   rhs = rhs.PadWithZero();\n \n-  TF_ASSIGN_OR_RETURN(auto dot,\n-                      create_sharded_dot(lhs.hlo(), rhs.hlo(), b, conv_window));\n+  TF_ASSIGN_OR_RETURN(auto dot, create_sharded_dot(lhs, rhs, b, conv_window));\n \n   std::vector<int64_t> lhs_contracting_dims;\n   lhs_contracting_dims.reserve(dims_mapping.contracting_dims.size());\n@@ -2047,42 +2129,29 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n       MakeBinaryAdd(output_base_shape.element_type(), module));\n }\n \n-absl::StatusOr<HloInstruction*> PartitionDot(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n-    const Shape& output_base_shape, const HloSharding& output_sharding,\n-    const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n-    const SpmdPartitionerOptions& options, SpmdBuilder* b,\n-    std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n-        windowed_dot_general_loops,\n-    SpmdPartitioningVisitor* visitor);\n+}  // namespace\n \n+namespace {\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n-    PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n-    const HloSharding& output_sharding,\n+    PartitionedHloMaybeMX lhs, PartitionedHloMaybeMX rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n     int64_t lhs_contracting_partitions, int64_t rhs_contracting_partitions,\n     int64_t lhs_non_contracting_partitions,\n     int64_t rhs_non_contracting_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     SpmdPartitioningVisitor* visitor) {\n-  std::vector<std::pair<HloInstruction*, HloSharding>>\n+  std::vector<std::pair<PartitionedHloMaybeMX, HloSharding>>\n       top_level_sharding_to_reset;\n-  absl::Cleanup cleaner = [&] {\n+  absl::Cleanup cleaner = [&top_level_sharding_to_reset] {\n     for (auto& to_reset : top_level_sharding_to_reset) {\n-      to_reset.first->set_sharding(to_reset.second);\n+      to_reset.first.set_sharding(to_reset.second);\n     }\n   };\n   std::vector<int64_t> lhs_dims;\n@@ -2128,8 +2197,8 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n   }\n   auto output_grouped =\n       hlo_sharding_util::GroupShardingOnDims(output_sharding, output_dims);\n-  PartitionedHlo per_group_lhs = lhs;\n-  PartitionedHlo per_group_rhs = rhs;\n+  PartitionedHloMaybeMX per_group_lhs = lhs;\n+  PartitionedHloMaybeMX per_group_rhs = rhs;\n   if (lhs_rhs_dims_matching) {\n     auto lhs_grouped =\n         hlo_sharding_util::GroupShardingOnDims(lhs.sharding(), lhs_dims);\n@@ -2154,38 +2223,34 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n         lhs_grouped);\n     auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n         lhs.state(), lhs_grouped.device_groups, b);\n-    top_level_sharding_to_reset.emplace_back(lhs.hlo(), lhs.sharding());\n-    lhs.hlo()->set_sharding(lhs_grouped.sharding);\n-    top_level_sharding_to_reset.emplace_back(rhs.hlo(), rhs.sharding());\n-    rhs.hlo()->set_sharding(rhs_grouped.sharding);\n+    top_level_sharding_to_reset.emplace_back(lhs, lhs.sharding());\n+    lhs.set_sharding(lhs_grouped.sharding);\n+    top_level_sharding_to_reset.emplace_back(rhs, rhs.sharding());\n+    rhs.set_sharding(rhs_grouped.sharding);\n     CHECK(lhs.hlo() != rhs.hlo() ||\n           lhs_grouped.sharding == rhs_grouped.sharding);\n-    per_group_lhs = PartitionedHlo(\n-        lhs.hlo(), GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n+    per_group_lhs = MakePartitionedHloMaybeMX(\n+        lhs, GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n         per_group_partitioner_state);\n-    per_group_rhs = PartitionedHlo(\n-        rhs.hlo(), GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n+    per_group_rhs = MakePartitionedHloMaybeMX(\n+        rhs, GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n         per_group_partitioner_state);\n   } else {\n     auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n         lhs.state(), output_grouped.device_groups, b);\n     auto reshard_to_output_batch =\n-        [&](const PartitionedHlo& operand, absl::Span<const int64_t> batch_dims,\n+        [&](const PartitionedHloMaybeMX& operand,\n+            absl::Span<const int64_t> batch_dims,\n             absl::Span<const int64_t> contracting_dims,\n             absl::Span<const int64_t> non_contracting_dims,\n             int64_t contracting_dim_partitions,\n             int64_t non_contracting_dim_partitions,\n             int64_t other_contracting_dim_partitions,\n             std::vector<int64_t>* sharding_dims_adjusted_to_output)\n-        -> std::optional<PartitionedHlo> {\n+        -> std::optional<PartitionedHloMaybeMX> {\n       if (operand.sharding().IsTileMaximal()) {\n-        auto partially_sharded = PerGroupSliceFromReplicated(\n-            operand.Replicate().hlo(), operand.state().partition_id,\n-            output_grouped.device_groups, batch_dims,\n-            output_grouped.group_dim_sizes, b);\n-        partially_sharded->set_sharding(HloSharding::Replicate());\n-        return PartitionedHlo(partially_sharded, partially_sharded->shape(),\n-                              per_group_partitioner_state);\n+        return ReplicatePartiallySharded(operand, batch_dims, output_grouped, b,\n+                                         per_group_partitioner_state);\n       }\n       auto& original_tiling = operand.sharding().tile_assignment();\n       // It's possible that the operand is not initially sharded on batch\n@@ -2245,12 +2310,11 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n         return std::nullopt;\n       }\n       auto resharded = operand.Reshard(UngroupSharding(grouped));\n-      top_level_sharding_to_reset.emplace_back(resharded.hlo(),\n-                                               resharded.sharding());\n-      resharded.hlo()->set_sharding(grouped.sharding);\n-      return PartitionedHlo(resharded.hlo(),\n-                            GetPerGroupBaseShape(grouped, operand.base_shape()),\n-                            per_group_partitioner_state);\n+      top_level_sharding_to_reset.emplace_back(resharded, resharded.sharding());\n+      resharded.set_sharding(grouped.sharding);\n+      return MakePartitionedHloMaybeMX(\n+          resharded, GetPerGroupBaseShape(grouped, operand.base_shape()),\n+          per_group_partitioner_state);\n     };\n     std::vector<int64_t> lhs_contracting_dims;\n     std::vector<int64_t> rhs_contracting_dims;\n@@ -2446,31 +2510,29 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n   return std::nullopt;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n-    bool lhs_matching, PartitionedHlo matching, PartitionedHlo other,\n-    int64_t matching_contracting_partitions,\n+    bool lhs_matching, PartitionedHloMaybeMX matching,\n+    PartitionedHloMaybeMX other, int64_t matching_contracting_partitions,\n     int64_t other_contracting_partitions,\n     absl::Span<const DotConvolutionDimsInfo::DimNums>\n         partitioned_non_contracting_dims,\n     int64_t other_non_contracting_partitions,\n     int64_t output_other_non_contracting_partitions,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     SpmdPartitioningVisitor* visitor) {\n-  std::vector<std::pair<HloInstruction*, HloSharding>>\n+  std::vector<std::pair<PartitionedHloMaybeMX, HloSharding>>\n       top_level_sharding_to_reset;\n-  absl::Cleanup cleaner = [&] {\n+  absl::Cleanup cleaner = [&top_level_sharding_to_reset] {\n     for (auto& to_reset : top_level_sharding_to_reset) {\n-      to_reset.first->set_sharding(to_reset.second);\n+      to_reset.first.set_sharding(to_reset.second);\n     }\n   };\n \n@@ -2491,7 +2553,7 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n   }\n \n   auto try_sharding_for_other_operand = [&](const HloSharding& sharding) {\n-    PartitionedHlo other_reshard = other.Reshard(sharding);\n+    PartitionedHloMaybeMX other_reshard = other.Reshard(sharding);\n     std::optional<GroupedSharding> grouped_sharding =\n         GetNonContractingPartitionGroupedShardingForOtherOperand(\n             lhs_matching, output_base_shape, other_reshard.hlo()->shape(),\n@@ -2526,21 +2588,20 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n   matching = matching.Reshard(UngroupSharding(matching_grouped));\n   auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n       matching.state(), matching_grouped.device_groups, b);\n-  top_level_sharding_to_reset.emplace_back(matching.hlo(), matching.sharding());\n-  matching.hlo()->set_sharding(matching_grouped.sharding);\n-  auto matching_p = PartitionedHlo(\n-      matching.hlo(),\n-      GetPerGroupBaseShape(matching_grouped, matching.base_shape()),\n+  top_level_sharding_to_reset.emplace_back(matching, matching.sharding());\n+  matching.set_sharding(matching_grouped.sharding);\n+  PartitionedHloMaybeMX matching_p = MakePartitionedHloMaybeMX(\n+      matching, GetPerGroupBaseShape(matching_grouped, matching.base_shape()),\n       per_group_partitioner_state);\n \n-  auto partially_replicated_other = other.hlo();\n+  PartitionedHloMaybeMX partially_replicated_other = other;\n   if (other_grouped && other_grouped->group_dims.size() == 1 &&\n       other_grouped->group_dims[0] == other.base_shape().dimensions().size()) {\n     // Group on replication dim.\n     other = other.Reshard(UngroupSharding(*other_grouped));\n-    partially_replicated_other = other.hlo();\n-    top_level_sharding_to_reset.emplace_back(other.hlo(), other.sharding());\n-    partially_replicated_other->set_sharding(other_grouped->sharding);\n+    partially_replicated_other = other;\n+    top_level_sharding_to_reset.emplace_back(other, other.sharding());\n+    partially_replicated_other.set_sharding(other_grouped->sharding);\n   } else if (other_grouped && !other.sharding().IsReplicated()) {\n     HloSharding target_sharding = UngroupSharding(*other_grouped);\n     GroupedSharding target_group_sharding =\n@@ -2557,16 +2618,15 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n       other = other.Reshard(target_sharding);\n     }\n     partially_replicated_other =\n-        other\n-            .Reshard(hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n-                other.sharding(), other_grouped->group_dims))\n-            .hlo();\n+        other.Reshard(hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n+            other.sharding(), other_grouped->group_dims));\n     top_level_sharding_to_reset.emplace_back(\n-        partially_replicated_other, partially_replicated_other->sharding());\n-    partially_replicated_other->set_sharding(other_grouped->sharding);\n+        partially_replicated_other, partially_replicated_other.sharding());\n+    partially_replicated_other.set_sharding(other_grouped->sharding);\n   }\n \n-  auto other_p = PartitionedHlo(partially_replicated_other, other.base_shape(),\n+  PartitionedHloMaybeMX other_p =\n+      MakePartitionedHloMaybeMX(partially_replicated_other, other.base_shape(),\n                                 per_group_partitioner_state);\n   return PartitionDot(\n       lhs_matching ? matching_p : other_p, lhs_matching ? other_p : matching_p,\n@@ -2709,9 +2769,10 @@ GetDotGroupPartitionContractingOutputShardings(\n   return std::make_pair(inner_output_sharding, outer_output_tmp_sharding);\n }\n \n+template <typename PartitionedHloMaybeMX>\n std::pair<HloSharding, HloSharding>\n GetDotGroupPartitionContractingLhsRhsShardings(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     absl::Span<const DotConvolutionDimsInfo::DimNums>\n         partitioned_contracting_dims) {\n   HloSharding lhs_sharding = lhs.sharding();\n@@ -2743,30 +2804,28 @@ GetDotGroupPartitionContractingLhsRhsShardings(\n   return std::make_pair(lhs_sharding, rhs_sharding);\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n-    PartitionedHlo lhs, PartitionedHlo rhs,\n+    PartitionedHloMaybeMX lhs, PartitionedHloMaybeMX rhs,\n     absl::Span<const DotConvolutionDimsInfo::DimNums>\n         partitioned_contracting_dims,\n     int64_t output_batch_partitions,\n     int64_t output_lhs_non_contracting_partitions,\n     int64_t output_rhs_non_contracting_partitions,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     SpmdPartitioningVisitor* visitor) {\n-  std::vector<std::pair<HloInstruction*, HloSharding>>\n+  std::vector<std::pair<PartitionedHloMaybeMX, HloSharding>>\n       top_level_sharding_to_reset;\n-  absl::Cleanup cleaner = [&] {\n+  absl::Cleanup cleaner = [&top_level_sharding_to_reset] {\n     for (auto& to_reset : top_level_sharding_to_reset) {\n-      to_reset.first->set_sharding(to_reset.second);\n+      to_reset.first.set_sharding(to_reset.second);\n     }\n   };\n   std::vector<int64_t> lhs_dims;\n@@ -2806,10 +2865,10 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n   lhs = lhs.PadWithZeroOnSpecifiedDims(lhs_dims);\n   rhs = rhs.PadWithZeroOnSpecifiedDims(rhs_dims);\n \n-  top_level_sharding_to_reset.emplace_back(lhs.hlo(), lhs_sharding);\n-  lhs.hlo()->set_sharding(lhs_grouped.sharding);\n-  top_level_sharding_to_reset.emplace_back(rhs.hlo(), rhs_sharding);\n-  rhs.hlo()->set_sharding(rhs_grouped.sharding);\n+  top_level_sharding_to_reset.emplace_back(lhs, lhs_sharding);\n+  lhs.set_sharding(lhs_grouped.sharding);\n+  top_level_sharding_to_reset.emplace_back(rhs, rhs_sharding);\n+  rhs.set_sharding(rhs_grouped.sharding);\n \n   HloSharding inner_output_sharding = HloSharding::Replicate();\n   HloSharding outer_output_tmp_sharding = HloSharding::Replicate();\n@@ -2837,11 +2896,21 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n         hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n             output_sharding, get_non_slice_dims()));\n   }\n-  auto inner_creator =\n-      [&](HloInstruction* l, HloInstruction* r, SpmdBuilder* b,\n-          const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n-    TF_ASSIGN_OR_RETURN(auto inner_dot,\n-                        create_sharded_dot(l, r, b, conv_window));\n+\n+  std::function<absl::StatusOr<HloInstruction*>(const PartitionedHloMaybeMX&,\n+                                                const PartitionedHloMaybeMX&,\n+                                                SpmdBuilder*, const Window&)>\n+      inner_creator =\n+          [&](const PartitionedHloMaybeMX& l, const PartitionedHloMaybeMX& r,\n+              SpmdBuilder* b,\n+              const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n+    // inner_creator will become create_sharded_dot's operator() target. Call\n+    // create_sharded_dot's original CreateSharded function here by setting\n+    // call_custom_create_sharded to false.\n+    TF_ASSIGN_OR_RETURN(\n+        auto inner_dot,\n+        create_sharded_dot(l, r, b, conv_window,\n+                           /*call_custom_create_sharded=*/false));\n     HloInstruction* result = inner_dot;\n     if (!output_slice_dims.empty()) {\n       // Create an AllReduce along slice dims first to allow a reduce-scatter.\n@@ -2868,7 +2937,7 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n       if (output_replicate_dim_grouped) {\n         result = lhs.state().partitioner->AllReduceAlongShardingDims(\n             b, result, outer_output_tmp_sharding, lhs.state().next_channel_id,\n-            {output_base_shape.dimensions_size()},\n+            {static_cast<int64_t>(output_base_shape.dimensions().size())},\n             lhs.state().collective_ops_creator,\n             MakeBinaryAdd(output_base_shape.element_type(), module));\n       }\n@@ -2897,31 +2966,35 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n     TF_ASSIGN_OR_RETURN(\n         maybe_windowed_dot,\n         PartitionDot(\n-            PartitionedHlo(lhs.hlo(),\n-                           GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n-                           inner_state),\n-            PartitionedHlo(rhs.hlo(),\n-                           GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n-                           inner_state),\n+            MakePartitionedHloMaybeMX(\n+                lhs, GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n+                inner_state),\n+            MakePartitionedHloMaybeMX(\n+                rhs, GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n+                inner_state),\n             predicted_inner_output_base_shape, inner_output_sharding,\n             dims_mapping, num_partitions / group_count, predicted_inner_creator,\n             conv_window, module, original_hlo, options, b,\n             windowed_dot_general_loops, visitor));\n   }\n   int new_num_windowed_loops = windowed_dot_general_loops->size();\n \n+  // create_sharded_dot's operator() will call inner_creator instead of\n+  // its CreateSharded function.\n+  create_sharded_dot.SetCustomCreateSharded(std::move(inner_creator));\n+\n   TF_ASSIGN_OR_RETURN(\n       auto inner_dot,\n-      PartitionDot(\n-          PartitionedHlo(lhs.hlo(),\n-                         GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n-                         inner_state),\n-          PartitionedHlo(rhs.hlo(),\n-                         GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n-                         inner_state),\n-          inner_output_base_shape, inner_output_sharding, dims_mapping,\n-          num_partitions / group_count, inner_creator, conv_window, module,\n-          original_hlo, options, b, windowed_dot_general_loops, visitor));\n+      PartitionDot(MakePartitionedHloMaybeMX(\n+                       lhs, GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n+                       inner_state),\n+                   MakePartitionedHloMaybeMX(\n+                       rhs, GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n+                       inner_state),\n+                   inner_output_base_shape, inner_output_sharding, dims_mapping,\n+                   num_partitions / group_count, create_sharded_dot,\n+                   conv_window, module, original_hlo, options, b,\n+                   windowed_dot_general_loops, visitor));\n \n   // Reenables the inner reshard if there is an inner dot and no actual\n   // windowed_dot_general_loops generated.\n@@ -2943,10 +3016,9 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n   }\n \n   maybe_windowed_dot->set_sharding(outer_output_tmp_sharding);\n-  auto d = PartitionedHlo(maybe_windowed_dot, output_base_shape, lhs.state())\n-               .Reshard(output_sharding)\n-               .hlo();\n-  return d;\n+  return PartitionedHlo(maybe_windowed_dot, output_base_shape, lhs.state())\n+      .Reshard(output_sharding)\n+      .hlo();\n }\n \n DotConvolutionDimsInfo ConvertDimNumsWithFeatureGroupCount(\n@@ -3117,7 +3189,7 @@ EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n     const int64_t new_num_partitions =\n         num_partitions / matching_non_contracting_partitions;\n     std::optional<WindowedEinsumConfig> e_config =\n-        GetWindowedEinsumConfiguration(\n+        GetWindowedEinsumConfiguration<CreateShardedDotFunctor>(\n             new_num_partitions, output_matching_non_contracting_partitions,\n             output_other_non_contracting_partitions,\n             other_contracting_partitions, other_non_contracting_partitions,\n@@ -3154,6 +3226,7 @@ EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n // The general idea is similar as the one in\n // LhsIsBestMatchForNonContractingPartitioning with one all-gather replaced by\n // reduce-scatter.\n+template <typename CreateShardedFunctor>\n bool PrioritizeContractingDimensionsPartitioning(\n     const DotConvolutionDimsInfo& dims_mapping, const PartitionedHlo& lhs,\n     const PartitionedHlo& rhs, const Shape& output_base_shape,\n@@ -3165,11 +3238,7 @@ bool PrioritizeContractingDimensionsPartitioning(\n     int64_t output_rhs_non_contracting_partitions, int64_t lhs_batch_partitions,\n     int64_t rhs_batch_partitions, int64_t output_batch_partitions,\n     bool require_matching_devices_to_group, SpmdBuilder* b,\n-    const Window& conv_window,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    const Window& conv_window, const CreateShardedFunctor& create_sharded_dot,\n     SpmdPartitioningVisitor* visitor) {\n   const bool may_group_on_lhs_non_contracting =\n       lhs_non_contracting_partitions == output_lhs_non_contracting_partitions &&\n@@ -3301,20 +3370,21 @@ bool PrioritizeContractingDimensionsPartitioning(\n       hlo_sharding_util::TransposeShardingWithCollapsedDims(\n           rhs_sharding, indices_map.rhs_to_lhs_indices,\n           indices_map.lhs_to_rhs_indices);\n-  std::optional<WindowedEinsumConfig> e_config = GetWindowedEinsumConfiguration(\n-      new_num_partitions, new_output_lhs_non_contracting_partitions,\n-      new_output_rhs_non_contracting_partitions, 1,\n-      rhs_non_contracting_partitions, rhs_batch_partitions, 1,\n-      lhs_non_contracting_partitions, lhs_batch_partitions,\n-      ShapeSizeInBytes(GetPerGroupBaseShape(rhs_grouped, rhs.base_shape())),\n-      ShapeSizeInBytes(GetPerGroupBaseShape(lhs_grouped, lhs.base_shape())),\n-      ShapeSizeInBytes(inner_output_base_shape), options,\n-      output_sharding_transposed_to_match_lhs,\n-      output_sharding_transposed_to_match_rhs,\n-      lhs_sharding_transposed_to_match_rhs,\n-      rhs_sharding_transposed_to_match_lhs, lhs_grouped.sharding,\n-      output_sharding, rhs_grouped.sharding, conv_window, dims_mapping,\n-      indices_map, visitor->call_graph());\n+  std::optional<WindowedEinsumConfig> e_config =\n+      GetWindowedEinsumConfiguration<CreateShardedFunctor>(\n+          new_num_partitions, new_output_lhs_non_contracting_partitions,\n+          new_output_rhs_non_contracting_partitions, 1,\n+          rhs_non_contracting_partitions, rhs_batch_partitions, 1,\n+          lhs_non_contracting_partitions, lhs_batch_partitions,\n+          ShapeSizeInBytes(GetPerGroupBaseShape(rhs_grouped, rhs.base_shape())),\n+          ShapeSizeInBytes(GetPerGroupBaseShape(lhs_grouped, lhs.base_shape())),\n+          ShapeSizeInBytes(inner_output_base_shape), options,\n+          output_sharding_transposed_to_match_lhs,\n+          output_sharding_transposed_to_match_rhs,\n+          lhs_sharding_transposed_to_match_rhs,\n+          rhs_sharding_transposed_to_match_lhs, lhs_grouped.sharding,\n+          output_sharding, rhs_grouped.sharding, conv_window, dims_mapping,\n+          indices_map, visitor->call_graph());\n   if (!e_config) {\n     return false;\n   }\n@@ -3359,9 +3429,14 @@ bool PrioritizeContractingDimensionsPartitioning(\n   *other_hlo->mutable_shape() =\n       GetPerGroupBaseShape(other_grouped, other_base_shape);\n   HloInstruction* dot =\n-      create_sharded_dot(lhs_matching_iterations ? lhs.hlo() : other_hlo,\n-                         lhs_matching_iterations ? other_hlo : rhs.hlo(), b,\n-                         conv_window)\n+      create_sharded_dot(\n+          lhs_matching_iterations\n+              ? lhs\n+              : PartitionedHlo(other_hlo, other_base_shape, rhs.state()),\n+          lhs_matching_iterations\n+              ? PartitionedHlo(other_hlo, other_base_shape, lhs.state())\n+              : rhs,\n+          b, conv_window)\n           .value();\n   const double computation_time_in_ms =\n       visitor->GetComputationTimeInMilliSec(dot);\n@@ -3394,21 +3469,20 @@ bool PrioritizeContractingDimensionsPartitioning(\n \n // Return if it would be better to match the LHS operand or RHS operand\n // of a dot for non-contracting partitioning.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n bool LhsIsBestMatchForNonContractingPartitioning(\n-    const DotConvolutionDimsInfo& dims_mapping, const PartitionedHlo& lhs,\n-    const PartitionedHlo& rhs, const Shape& output_base_shape,\n-    const HloSharding& output_sharding, const SpmdPartitionerOptions& options,\n-    int64_t num_partitions, int64_t lhs_non_contracting_partitions,\n+    const DotConvolutionDimsInfo& dims_mapping,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n+    const SpmdPartitionerOptions& options, int64_t num_partitions,\n+    int64_t lhs_non_contracting_partitions,\n     int64_t rhs_non_contracting_partitions, int64_t lhs_matching_partitions,\n     int64_t rhs_matching_partitions, int64_t lhs_contracting_partitions,\n     int64_t rhs_contracting_partitions,\n     int64_t output_lhs_non_contracting_partitions,\n     int64_t output_rhs_non_contracting_partitions, int64_t lhs_batch_partitions,\n     int64_t rhs_batch_partitions, SpmdBuilder* b, const Window& conv_window,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    const CreateShardedFunctor& create_sharded_dot,\n     SpmdPartitioningVisitor* visitor) {\n   const bool may_group_on_lhs_non_contracting =\n       lhs_non_contracting_partitions == output_lhs_non_contracting_partitions &&\n@@ -3432,94 +3506,102 @@ bool LhsIsBestMatchForNonContractingPartitioning(\n   // with the smaller all_gather as it has potentially smaller extra\n   // collective-permute overhead outside of the while loop; 2) Otherwise, we\n   // choose the all_gather with longer runtime to overlap with.\n-  if (may_group_on_lhs_non_contracting && may_group_on_rhs_non_contracting &&\n-      options.choose_faster_windowed_einsum_over_mem) {\n-    const DotDimensionIndexMapping indices_map = ComputeDimensionIndexMapping(\n-        dims_mapping, lhs.base_shape().dimensions().size(),\n-        rhs.base_shape().dimensions().size(),\n-        output_base_shape.dimensions().size());\n-    std::optional<int64_t> lhs_matching_iterations;\n-    std::optional<int64_t> rhs_matching_iterations;\n-    std::tie(lhs_matching_iterations, rhs_matching_iterations) =\n-        EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n-            dims_mapping, lhs, rhs, output_base_shape, output_sharding, options,\n-            num_partitions, lhs_non_contracting_partitions,\n-            rhs_non_contracting_partitions, lhs_matching_partitions,\n-            rhs_matching_partitions, lhs_contracting_partitions,\n-            rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n-            output_rhs_non_contracting_partitions, lhs_batch_partitions,\n-            rhs_batch_partitions, conv_window, visitor);\n-    if (lhs_matching_iterations && rhs_matching_iterations) {\n-      const int64_t lhs_all_gather_bytes =\n-          ShapeUtil::ByteSizeOf(lhs.hlo()->shape()) *\n-          rhs_non_contracting_partitions;\n-      const int64_t rhs_all_gather_bytes =\n-          ShapeUtil::ByteSizeOf(rhs.hlo()->shape()) *\n-          lhs_non_contracting_partitions;\n-      auto lhs_grouped =\n-          GetNonContractingPartitionGroupedShardingForMatchedOperand(\n-              /*lhs_matching=*/true, lhs.sharding(), output_sharding,\n-              dims_mapping.lhs_non_contracting_dims);\n-      auto lhs_all_gather_subgroups = lhs_grouped.device_groups;\n-      auto rhs_grouped =\n-          GetNonContractingPartitionGroupedShardingForMatchedOperand(\n-              /*lhs_matching=*/false, rhs.sharding(), output_sharding,\n-              dims_mapping.rhs_non_contracting_dims);\n-      auto rhs_all_gather_subgroups = rhs_grouped.device_groups;\n-      const double lhs_all_gather_time_in_ms =\n-          visitor->GetCommunicationTimeInMilliSec(\n-              lhs_all_gather_bytes,\n-              CollectiveDeviceList(\n-                  visitor->CreateReplicaGroups(lhs_all_gather_subgroups)));\n-      const double rhs_all_gather_time_in_ms =\n-          visitor->GetCommunicationTimeInMilliSec(\n-              rhs_all_gather_bytes,\n-              CollectiveDeviceList(\n-                  visitor->CreateReplicaGroups(rhs_all_gather_subgroups)));\n-\n-      HloInstruction* compute_lhs = lhs.hlo();\n-      Shape lhs_original_shape = compute_lhs->shape();\n-      *compute_lhs->mutable_shape() =\n-          GetPerGroupBaseShape(lhs_grouped, lhs.base_shape());\n-      HloInstruction* compute_rhs = rhs.hlo();\n-      Shape rhs_original_shape = compute_rhs->shape();\n-      *compute_rhs->mutable_shape() =\n-          GetPerGroupBaseShape(rhs_grouped, rhs.base_shape());\n-      HloInstruction* dot =\n-          create_sharded_dot(compute_lhs, compute_rhs, b, conv_window).value();\n-      const double computation_time_in_ms =\n-          visitor->GetComputationTimeInMilliSec(dot);\n-      *compute_lhs->mutable_shape() = lhs_original_shape;\n-      *compute_rhs->mutable_shape() = rhs_original_shape;\n-\n-      VLOG(2) << \"lhs: \" << lhs.hlo()->ToString() << \"\\n\"\n-              << \"rhs: \" << rhs.hlo()->ToString() << \"\\n\"\n-              << \"lhs_non_contracting_partitions: \"\n-              << lhs_non_contracting_partitions\n-              << \" rhs_non_contracting_partitions: \"\n-              << rhs_non_contracting_partitions << \"\\n\"\n-              << \"lhs_matching_iterations: \" << *lhs_matching_iterations\n-              << \" rhs_matching_iterations: \" << *rhs_matching_iterations\n-              << \"\\n\"\n-              << \"lhs_all_gather_bytes: \" << lhs_all_gather_bytes\n-              << \" rhs_all_gather_bytes: \" << rhs_all_gather_bytes << \"\\n\"\n-              << \"lhs_all_gather_time_in_ms: \" << lhs_all_gather_time_in_ms\n-              << \" rhs_all_gather_time_in_ms: \" << rhs_all_gather_time_in_ms\n-              << \"\\n\"\n-              << \"dot: \" << dot->ToString() << \"\\n\"\n-              << \"computation_time_in_ms: \" << computation_time_in_ms;\n-      if (computation_time_in_ms == 0.0 || lhs_all_gather_time_in_ms == 0.0 ||\n-          rhs_all_gather_time_in_ms == 0.0) {\n-        lhs_matching = *lhs_matching_iterations < *rhs_matching_iterations;\n-      } else if ((computation_time_in_ms <= lhs_all_gather_time_in_ms) &&\n-                 (computation_time_in_ms <= rhs_all_gather_time_in_ms)) {\n-        lhs_matching = lhs_all_gather_bytes / rhs_non_contracting_partitions >\n-                       rhs_all_gather_bytes / lhs_non_contracting_partitions;\n+  //\n+  // Disable windowed einsums for block-scaled dot.\n+  if constexpr (std::is_same_v<PartitionedHloMaybeMX, PartitionedHlo>) {\n+    if (may_group_on_lhs_non_contracting && may_group_on_rhs_non_contracting &&\n+        options.choose_faster_windowed_einsum_over_mem) {\n+      const DotDimensionIndexMapping indices_map = ComputeDimensionIndexMapping(\n+          dims_mapping, lhs.base_shape().dimensions().size(),\n+          rhs.base_shape().dimensions().size(),\n+          output_base_shape.dimensions().size());\n+      std::optional<int64_t> lhs_matching_iterations;\n+      std::optional<int64_t> rhs_matching_iterations;\n+      std::tie(lhs_matching_iterations, rhs_matching_iterations) =\n+          EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n+              dims_mapping, lhs, rhs, output_base_shape, output_sharding,\n+              options, num_partitions, lhs_non_contracting_partitions,\n+              rhs_non_contracting_partitions, lhs_matching_partitions,\n+              rhs_matching_partitions, lhs_contracting_partitions,\n+              rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n+              output_rhs_non_contracting_partitions, lhs_batch_partitions,\n+              rhs_batch_partitions, conv_window, visitor);\n+      if (lhs_matching_iterations && rhs_matching_iterations) {\n+        const int64_t lhs_all_gather_bytes =\n+            ShapeUtil::ByteSizeOf(lhs.hlo()->shape()) *\n+            rhs_non_contracting_partitions;\n+        const int64_t rhs_all_gather_bytes =\n+            ShapeUtil::ByteSizeOf(rhs.hlo()->shape()) *\n+            lhs_non_contracting_partitions;\n+        auto lhs_grouped =\n+            GetNonContractingPartitionGroupedShardingForMatchedOperand(\n+                /*lhs_matching=*/true, lhs.sharding(), output_sharding,\n+                dims_mapping.lhs_non_contracting_dims);\n+        auto lhs_all_gather_subgroups = lhs_grouped.device_groups;\n+        auto rhs_grouped =\n+            GetNonContractingPartitionGroupedShardingForMatchedOperand(\n+                /*lhs_matching=*/false, rhs.sharding(), output_sharding,\n+                dims_mapping.rhs_non_contracting_dims);\n+        auto rhs_all_gather_subgroups = rhs_grouped.device_groups;\n+        const double lhs_all_gather_time_in_ms =\n+            visitor->GetCommunicationTimeInMilliSec(\n+                lhs_all_gather_bytes,\n+                CollectiveDeviceList(\n+                    visitor->CreateReplicaGroups(lhs_all_gather_subgroups)));\n+        const double rhs_all_gather_time_in_ms =\n+            visitor->GetCommunicationTimeInMilliSec(\n+                rhs_all_gather_bytes,\n+                CollectiveDeviceList(\n+                    visitor->CreateReplicaGroups(rhs_all_gather_subgroups)));\n+\n+        HloInstruction* compute_lhs = lhs.hlo();\n+        Shape lhs_original_shape = compute_lhs->shape();\n+        *compute_lhs->mutable_shape() =\n+            GetPerGroupBaseShape(lhs_grouped, lhs.base_shape());\n+        HloInstruction* compute_rhs = rhs.hlo();\n+        Shape rhs_original_shape = compute_rhs->shape();\n+        *compute_rhs->mutable_shape() =\n+            GetPerGroupBaseShape(rhs_grouped, rhs.base_shape());\n+        HloInstruction* dot =\n+            create_sharded_dot(\n+                PartitionedHlo(compute_lhs, lhs.base_shape(), lhs.state()),\n+                PartitionedHlo(compute_rhs, rhs.base_shape(), rhs.state()), b,\n+                conv_window)\n+                .value();\n+        const double computation_time_in_ms =\n+            visitor->GetComputationTimeInMilliSec(dot);\n+        *compute_lhs->mutable_shape() = lhs_original_shape;\n+        *compute_rhs->mutable_shape() = rhs_original_shape;\n+\n+        VLOG(2) << \"lhs: \" << lhs.hlo()->ToString() << \"\\n\"\n+                << \"rhs: \" << rhs.hlo()->ToString() << \"\\n\"\n+                << \"lhs_non_contracting_partitions: \"\n+                << lhs_non_contracting_partitions\n+                << \" rhs_non_contracting_partitions: \"\n+                << rhs_non_contracting_partitions << \"\\n\"\n+                << \"lhs_matching_iterations: \" << *lhs_matching_iterations\n+                << \" rhs_matching_iterations: \" << *rhs_matching_iterations\n+                << \"\\n\"\n+                << \"lhs_all_gather_bytes: \" << lhs_all_gather_bytes\n+                << \" rhs_all_gather_bytes: \" << rhs_all_gather_bytes << \"\\n\"\n+                << \"lhs_all_gather_time_in_ms: \" << lhs_all_gather_time_in_ms\n+                << \" rhs_all_gather_time_in_ms: \" << rhs_all_gather_time_in_ms\n+                << \"\\n\"\n+                << \"dot: \" << dot->ToString() << \"\\n\"\n+                << \"computation_time_in_ms: \" << computation_time_in_ms;\n+        if (computation_time_in_ms == 0.0 || lhs_all_gather_time_in_ms == 0.0 ||\n+            rhs_all_gather_time_in_ms == 0.0) {\n+          lhs_matching = *lhs_matching_iterations < *rhs_matching_iterations;\n+        } else if ((computation_time_in_ms <= lhs_all_gather_time_in_ms) &&\n+                   (computation_time_in_ms <= rhs_all_gather_time_in_ms)) {\n+          lhs_matching = lhs_all_gather_bytes / rhs_non_contracting_partitions >\n+                         rhs_all_gather_bytes / lhs_non_contracting_partitions;\n+        } else {\n+          lhs_matching = lhs_all_gather_time_in_ms > rhs_all_gather_time_in_ms;\n+        }\n       } else {\n-        lhs_matching = lhs_all_gather_time_in_ms > rhs_all_gather_time_in_ms;\n+        lhs_matching = lhs_matching_iterations.has_value();\n       }\n-    } else {\n-      lhs_matching = lhs_matching_iterations.has_value();\n     }\n   }\n   return lhs_matching;\n@@ -3530,10 +3612,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    CreateShardedConvolutionFunctor& create_sharded_dot,\n     const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -3653,8 +3732,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n             resharded_rhs.Reshard(*lhs_sharding_transposed_to_match_rhs);\n         TF_ASSIGN_OR_RETURN(\n             sharded_conv,\n-            create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                               conv_window));\n+            create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window));\n         auto lhs_sharding_transposed_to_match_output =\n             hlo_sharding_util::TransposeShardingWithCollapsedDims(\n                 resharded_lhs.sharding(), indices_map.lhs_to_output_indices,\n@@ -3672,8 +3750,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n             resharded_lhs.Reshard(*rhs_sharding_transposed_to_match_lhs);\n         TF_ASSIGN_OR_RETURN(\n             sharded_conv,\n-            create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                               conv_window));\n+            create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window));\n         auto rhs_sharding_transposed_to_match_output =\n             hlo_sharding_util::TransposeShardingWithCollapsedDims(\n                 resharded_rhs.sharding(), indices_map.rhs_to_output_indices,\n@@ -3698,8 +3775,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n             resharded_rhs.Reshard(*output_sharding_transposed_to_match_rhs);\n         TF_ASSIGN_OR_RETURN(\n             sharded_conv,\n-            create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                               conv_window));\n+            create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window));\n         sharded_conv->set_sharding(target_output_sharding);\n       }\n \n@@ -3716,10 +3792,7 @@ absl::StatusOr<std::optional<HloInstruction*>> PartitionConv(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    CreateShardedConvolutionFunctor& create_sharded_dot,\n     const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -3791,15 +3864,13 @@ absl::StatusOr<std::optional<HloInstruction*>> PartitionConv(\n   return std::nullopt;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchDims(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -3837,15 +3908,13 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchDims(\n   return nullptr;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingDims(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -3910,16 +3979,21 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingDims(\n       }\n     }\n   }\n-  const bool prioritize_contracting_for_faster_windowed_einsum =\n-      PrioritizeContractingDimensionsPartitioning(\n-          dims_mapping, lhs, rhs, output_base_shape, output_sharding, options,\n-          num_partitions, lhs_non_contracting_partitions,\n-          rhs_non_contracting_partitions, lhs_contracting_partitions,\n-          rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n-          output_rhs_non_contracting_partitions, lhs_batch_partitions,\n-          rhs_batch_partitions, output_batch_partitions,\n-          require_matching_devices_to_group, b, conv_window, create_sharded_dot,\n-          visitor);\n+\n+  bool prioritize_contracting_for_faster_windowed_einsum = false;\n+  // Disable windowed einsum path for block-scaled dot.\n+  if constexpr (std::is_same_v<PartitionedHloMaybeMX, PartitionedHlo>) {\n+    prioritize_contracting_for_faster_windowed_einsum =\n+        PrioritizeContractingDimensionsPartitioning(\n+            dims_mapping, lhs, rhs, output_base_shape, output_sharding, options,\n+            num_partitions, lhs_non_contracting_partitions,\n+            rhs_non_contracting_partitions, lhs_contracting_partitions,\n+            rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n+            output_rhs_non_contracting_partitions, lhs_batch_partitions,\n+            rhs_batch_partitions, output_batch_partitions,\n+            require_matching_devices_to_group, b, conv_window,\n+            create_sharded_dot, visitor);\n+  }\n   if (!(matching_dims.empty() ||\n         prioritize_contracting_for_faster_windowed_einsum)) {\n     TF_ASSIGN_OR_RETURN(\n@@ -3946,15 +4020,13 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingDims(\n   return nullptr;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingDims(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -3987,6 +4059,7 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingDims(\n       return dot;\n     }\n   }\n+\n   if (lhs_contracting_partitions > 1 && rhs_contracting_partitions > 1) {\n     // If part of contracting dims match, try them.\n     std::vector<DotConvolutionDimsInfo::DimNums> matching_dims;\n@@ -4015,33 +4088,32 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingDims(\n   return nullptr;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotRemovingOutputPartialReplication(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     bool require_matching_devices_to_group, SpmdPartitioningVisitor* visitor) {\n   if (lhs.sharding().IsReplicated() && rhs.sharding().IsReplicated() &&\n       output_sharding.ReplicateOnLastTileDim()) {\n     auto grouped_output = hlo_sharding_util::GroupShardingOnDims(\n-        output_sharding, {output_base_shape.dimensions_size()});\n+        output_sharding,\n+        {static_cast<int64_t>(output_base_shape.dimensions().size())});\n     auto inner_state = CreatePerGroupPartitioningState(\n         lhs.state(), grouped_output.device_groups, b);\n     TF_ASSIGN_OR_RETURN(\n         auto dot,\n-        PartitionDot(PartitionedHlo(lhs.hlo(), lhs.base_shape(), inner_state),\n-                     PartitionedHlo(rhs.hlo(), rhs.base_shape(), inner_state),\n-                     output_base_shape, grouped_output.sharding, dims_mapping,\n-                     output_sharding.NumTiles(), create_sharded_dot,\n-                     conv_window, module, original_hlo, options, b,\n-                     windowed_dot_general_loops, visitor));\n+        PartitionDot(\n+            MakePartitionedHloMaybeMX(lhs, lhs.base_shape(), inner_state),\n+            MakePartitionedHloMaybeMX(rhs, rhs.base_shape(), inner_state),\n+            output_base_shape, grouped_output.sharding, dims_mapping,\n+            output_sharding.NumTiles(), create_sharded_dot, conv_window, module,\n+            original_hlo, options, b, windowed_dot_general_loops, visitor));\n     if (dot) {\n       return dot;\n     }\n@@ -4052,15 +4124,13 @@ absl::StatusOr<HloInstruction*> PartitionDotRemovingOutputPartialReplication(\n // Recursive partitioning function. If there are partial dimensions matching\n // in the operands and output, group the devices and recursively partition\n // the in-group dot.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDot(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -4071,14 +4141,18 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n   // Case 0: Try partition the purely spatially-partitioned convolution with\n   // convolution spatial dimension partitioned or depthwise parallel\n   // dimension partitioned.\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<HloInstruction*> partitioned_conv,\n-      PartitionConv(lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n-                    num_partitions, create_sharded_dot, conv_window, module,\n-                    original_hlo, options, b, windowed_dot_general_loops,\n-                    require_matching_devices_to_group, visitor));\n-  if (partitioned_conv.has_value()) {\n-    return partitioned_conv.value();\n+  if constexpr (std::is_same_v<CreateShardedFunctor,\n+                               CreateShardedConvolutionFunctor>) {\n+    TF_ASSIGN_OR_RETURN(\n+        std::optional<HloInstruction*> partitioned_conv,\n+        PartitionConv(lhs, rhs, output_base_shape, output_sharding,\n+                      dims_mapping, num_partitions, create_sharded_dot,\n+                      conv_window, module, original_hlo, options, b,\n+                      windowed_dot_general_loops,\n+                      require_matching_devices_to_group, visitor));\n+    if (partitioned_conv.has_value()) {\n+      return partitioned_conv.value();\n+    }\n   }\n \n   HloInstruction* partitioned_dot;\n@@ -4160,15 +4234,13 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n }\n \n // Reshard the LHS and RHS to match the output sharding.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> ReshardLHSRHSToMatchOutputSharding(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, SpmdBuilder* b) {\n+    const CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    SpmdBuilder* b) {\n   const bool consider_other_operand = false;\n   const bool may_combine_partial_sharding = false;\n   const HloSharding infered_lhs_sharding =\n@@ -4180,20 +4252,17 @@ absl::StatusOr<HloInstruction*> ReshardLHSRHSToMatchOutputSharding(\n           &output_sharding, &rhs.sharding(), 1, dims_mapping,\n           consider_other_operand, may_combine_partial_sharding);\n \n-  return create_sharded_dot(lhs.Reshard(infered_lhs_sharding).hlo(),\n-                            rhs.Reshard(infered_rhs_sharding).hlo(), b,\n-                            conv_window);\n+  return create_sharded_dot(lhs.Reshard(infered_lhs_sharding),\n+                            rhs.Reshard(infered_rhs_sharding), b, conv_window);\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDot(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -4202,7 +4271,7 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n   // resharding the groups.\n   for (bool require_matching_devices_to_group : {true, false}) {\n     TF_ASSIGN_OR_RETURN(\n-        auto try_partition,\n+        HloInstruction * try_partition,\n         PartitionDot(lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n                      num_partitions, create_sharded_dot, conv_window, module,\n                      original_hlo, require_matching_devices_to_group, options,\n@@ -4220,8 +4289,8 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n \n   // Default action.\n   TF_ASSIGN_OR_RETURN(\n-      auto dot, create_sharded_dot(lhs.Replicate().hlo(), rhs.Replicate().hlo(),\n-                                   b, conv_window));\n+      HloInstruction * dot,\n+      create_sharded_dot(lhs.Replicate(), rhs.Replicate(), b, conv_window));\n   dot->set_sharding(HloSharding::Replicate());\n   return PartitionedHlo(dot, output_base_shape, lhs.state())\n       .Reshard(output_sharding)\n@@ -4230,37 +4299,79 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n \n }  // namespace\n \n+template <typename CreateShardedFunctor>\n absl::Status SpmdPartitioningVisitor::HandleDotHelper(\n     HloInstruction* hlo, const DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot) {\n+    CreateShardedFunctor& create_sharded_dot) {\n   if (hlo->sharding().HasUniqueDevice()) {\n     return DefaultAction(hlo);\n   }\n-  PartitionedHlo& lhs = GetPartitionedHlo(hlo->operand(0));\n-  PartitionedHlo& raw_rhs = GetPartitionedHlo(hlo->operand(1));\n-  // If lhs and rhs are the same instruction, make a copy for rhs.\n-  const PartitionedHlo& rhs =\n-      (lhs.hlo() == raw_rhs.hlo())\n-          ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs, builder())\n-          : raw_rhs;\n-\n+  HloInstruction* partitioned_dot;\n   Window conv_window;\n-  if (hlo->opcode() == HloOpcode::kConvolution) {\n-    conv_window = hlo->window();\n-  }\n+  if constexpr (std::is_same_v<CreateShardedFunctor,\n+                               CreateShardedScaledDotFunctor>) {\n+    HloCustomCallInstruction* block_scaled_dot =\n+        Cast<HloCustomCallInstruction>(hlo);\n+    PartitionedHlo& lhs_operand =\n+        GetPartitionedHlo(block_scaled_dot->operand(0));\n+    PartitionedHlo& lhs_scale = GetPartitionedHlo(block_scaled_dot->operand(2));\n+    PartitionedHlo& raw_rhs_operand =\n+        GetPartitionedHlo(block_scaled_dot->operand(1));\n+    PartitionedHlo& raw_rhs_scale =\n+        GetPartitionedHlo(block_scaled_dot->operand(3));\n+    // If lhs and rhs are the same instruction, make a copy for rhs.\n+    const PartitionedHlo& rhs_operand =\n+        (lhs_operand.hlo() == raw_rhs_operand.hlo())\n+            ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs_operand, builder())\n+            : raw_rhs_operand;\n+    const PartitionedHlo& rhs_scale =\n+        (lhs_operand.hlo() == raw_rhs_operand.hlo())\n+            ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs_scale, builder())\n+            : raw_rhs_scale;\n+\n+    PartitionedHloMX lhs(lhs_operand, lhs_scale);\n+    PartitionedHloMX rhs(rhs_operand, rhs_scale);\n \n-  TF_ASSIGN_OR_RETURN(\n-      auto partitioned_dot,\n-      PartitionDot(lhs, rhs, hlo->shape(), hlo->sharding(), dims_mapping,\n-                   num_partitions_, create_sharded_dot, conv_window, module_,\n-                   hlo, options_, &b_, &windowed_dot_general_loops_, this));\n-  SetPartitionedHlo(hlo, [&] { return partitioned_dot; });\n+    TF_ASSIGN_OR_RETURN(\n+        partitioned_dot,\n+        PartitionDot(lhs, rhs, hlo->shape(), hlo->sharding(), dims_mapping,\n+                     num_partitions_, create_sharded_dot, conv_window, module_,\n+                     hlo, options_, &b_, &windowed_dot_general_loops_, this));\n+  } else {\n+    PartitionedHlo lhs = GetPartitionedHlo(hlo->operand(0));\n+    PartitionedHlo raw_rhs = GetPartitionedHlo(hlo->operand(1));\n+    // If lhs and rhs are the same instruction, make a copy for rhs.\n+    const PartitionedHlo rhs =\n+        (lhs.hlo() == raw_rhs.hlo())\n+            ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs, builder())\n+            : raw_rhs;\n+\n+    if (hlo->opcode() == HloOpcode::kConvolution) {\n+      conv_window = hlo->window();\n+    }\n+\n+    TF_ASSIGN_OR_RETURN(\n+        partitioned_dot,\n+        PartitionDot(lhs, rhs, hlo->shape(), hlo->sharding(), dims_mapping,\n+                     num_partitions_, create_sharded_dot, conv_window, module_,\n+                     hlo, options_, &b_, &windowed_dot_general_loops_, this));\n+  }\n+  SetPartitionedHlo(hlo, [partitioned_dot] { return partitioned_dot; });\n   return absl::OkStatus();\n }\n \n+template absl::Status\n+SpmdPartitioningVisitor::HandleDotHelper<CreateShardedDotFunctor>(\n+    HloInstruction*, const DotConvolutionDimsInfo&, CreateShardedDotFunctor&);\n+template absl::Status\n+SpmdPartitioningVisitor::HandleDotHelper<CreateShardedScaledDotFunctor>(\n+    HloInstruction*, const DotConvolutionDimsInfo&,\n+    CreateShardedScaledDotFunctor&);\n+template absl::Status\n+SpmdPartitioningVisitor::HandleDotHelper<CreateShardedConvolutionFunctor>(\n+    HloInstruction*, const DotConvolutionDimsInfo&,\n+    CreateShardedConvolutionFunctor&);\n+\n namespace {\n \n // Finds a cluster of nodes that produce the inputs for `hlo` which only"
        },
        {
            "sha": "0cc25656a83f30db0bcaa796da90e5d3d5c060da",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.h",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.h?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -0,0 +1,73 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_SPMD_DOT_HANDLER_H_\n+#define XLA_SERVICE_SPMD_DOT_HANDLER_H_\n+\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/spmd/spmd_partitioner.h\"\n+#include \"xla/service/spmd/spmd_partitioner_util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace spmd {\n+\n+class CreateShardedConvolutionFunctor;\n+class CreateShardedDotFunctor;\n+class CreateShardedScaledDotFunctor;\n+\n+// Abstract base class for functors creating sharded dots, block-scaled dots and\n+// convolutions.\n+template <typename PartitionedHloMaybeMX>\n+class CreateShardedFunctorBase {\n+ public:\n+  virtual ~CreateShardedFunctorBase() = default;\n+\n+  // Implemented in derived classes to create sharded dots, block-scaled dots\n+  // and convolutions.\n+  virtual absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHloMaybeMX& ll, const PartitionedHloMaybeMX& rr,\n+      SpmdBuilder* b, const Window& conv_window) const = 0;\n+\n+  void SetCustomCreateSharded(\n+      std::function<absl::StatusOr<HloInstruction*>(\n+          const PartitionedHloMaybeMX&, const PartitionedHloMaybeMX&,\n+          SpmdBuilder*, const Window&)>&& custom_create_sharded) {\n+    custom_create_sharded_ = std::move(custom_create_sharded);\n+  }\n+\n+  absl::StatusOr<HloInstruction*> operator()(\n+      const PartitionedHloMaybeMX& ll, const PartitionedHloMaybeMX& rr,\n+      SpmdBuilder* builder, const Window& conv_window,\n+      bool call_custom_create_sharded = true) const {\n+    if (call_custom_create_sharded && custom_create_sharded_) {\n+      return custom_create_sharded_(ll, rr, builder, conv_window);\n+    }\n+    return CreateSharded(ll, rr, builder, conv_window);\n+  }\n+\n+ private:\n+  // May hold a function which can be optionally called instead of\n+  // CreateSharded.\n+  std::function<absl::StatusOr<HloInstruction*>(const PartitionedHloMaybeMX&,\n+                                                const PartitionedHloMaybeMX&,\n+                                                SpmdBuilder*, const Window&)>\n+      custom_create_sharded_;\n+};\n+\n+}  // namespace spmd\n+}  // namespace xla\n+\n+#endif  // XLA_SERVICE_SPMD_DOT_HANDLER_H_"
        },
        {
            "sha": "49c8b0e744e91e097717161e9509438fdd45259d",
            "filename": "third_party/xla/xla/service/spmd/dot_handler_test.cc",
            "status": "modified",
            "additions": 214,
            "deletions": 0,
            "changes": 214,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -26,6 +26,8 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/utils/hlo_matchers.h\"\n+#include \"xla/literal_util.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_verifier.h\"\n #include \"xla/service/sharding_propagation.h\"\n@@ -38,6 +40,8 @@ namespace xla {\n namespace spmd {\n namespace {\n \n+namespace op = xla::testing::opcode_matchers;\n+\n class DotHandlerTest : public HloHardwareIndependentTestBase {\n  public:\n   absl::StatusOr<std::unique_ptr<HloModule>> PartitionComputation(\n@@ -338,6 +342,216 @@ ENTRY main {\n   }\n }\n \n+TEST_F(DotHandlerTest, MXCustomCall_BatchAndBatch) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,128,512]{2,1,0} parameter(0), sharding={devices=[8,1,1]<=[8]}\n+  lhs_scale = f8e8m0fnu[8,128,16] parameter(1), sharding={devices=[8,1,1]<=[8]}\n+  rhs = f8e4m3fn[8,1024,512]{2,1,0} parameter(2), sharding={devices=[8,1,1]<=[8]}\n+  rhs_scale = f8e8m0fnu[8,1024,16] parameter(3), sharding={devices=[8,1,1]<=[8]}\n+  ROOT block_scaled_dot = f32[8,128,1024]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[1,8,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::Reshape(op::Transpose(op::AllToAll(\n+                  op::Reshape(op::CustomCall({\"__op$block_scaled_dot\"}))))));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_BatchAndNonContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,128,512]{2,1,0} parameter(0), sharding={devices=[8,1,1]<=[8]}\n+  lhs_scale = f8e8m0fnu[8,128,16]{2,1,0} parameter(1), sharding={devices=[8,1,1]<=[8]}\n+  rhs = f8e4m3fn[8,1024,512]{2,1,0} parameter(2), sharding={devices=[1,8,1]<=[8]}\n+  rhs_scale = f8e8m0fnu[8,32,512]{2,1,0} parameter(3), sharding={devices=[1,8,1]<=[8]}\n+  ROOT block_scaled_dot = f32[8,128,1024]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::CustomCall({\"__op$block_scaled_dot\"}, op::Parameter(0),\n+                             op::Reshape(op::Transpose(op::AllToAll())),\n+                             op::Parameter(1),\n+                             op::Reshape(op::Transpose(op::AllToAll()))));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ContractingAndContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[128,512]{1,0} parameter(0), sharding={devices=[1,8]<=[8]}\n+  lhs_scale = f8e8m0fnu[128,16]{1,0} parameter(1), sharding={devices=[1,8]<=[8]}\n+  rhs = f8e4m3fn[1024,512]{1,0} parameter(2), sharding={devices=[1,8]<=[8]}\n+  rhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(3), sharding={devices=[1,8]<=[8]}\n+  ROOT block_scaled_dot = f32[128,1024]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::DynamicSlice(\n+          op::AllReduce(op::CustomCall({\"__op$block_scaled_dot\"})),\n+          op::Reshape(op::DynamicSlice(op::Constant(LiteralUtil::CreateR1<int>(\n+                                           {0, 16, 32, 48, 64, 80, 96, 112})),\n+                                       op::PartitionId())),\n+          op::Constant(LiteralUtil::CreateR0<int>(0))));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_NonContractingAndContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[128,512]{1,0} parameter(0), sharding={devices=[8,1]<=[8]}\n+  lhs_scale = f8e8m0fnu[128,16]{1,0} parameter(1), sharding={devices=[8,1]<=[8]}\n+  rhs = f8e4m3fn[1024,512]{1,0} parameter(2), sharding={devices=[1,8]<=[8]}\n+  rhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(3), sharding={devices=[1,8]<=[8]}\n+  ROOT block_scaled_dot = f32[128,1024]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::Parameter(0),\n+                     op::AllGather(), op::Parameter(1), op::AllGather()));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ContractingAndReplicated) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={devices=[1,8]<=[8]}\n+  lhs_scale = f8e4m3fn[1024,16]{1,0} parameter(1), sharding={devices=[1,8]<=[8]}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={replicated}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::AllReduce(op::CustomCall({\"__op$block_scaled_dot\"})));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_BatchNonContractingAndBatchNonContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,1024,512]{2,1,0} parameter(0), sharding={devices=[4,2,1]7,6,5,4,3,2,1,0}\n+  lhs_scale = f8e8m0fnu[8,1024,16]{2,1,0} parameter(1), sharding={devices=[4,2,1]7,6,5,4,3,2,1,0}\n+  rhs = f8e4m3fn[8,128,512]{2,1,0} parameter(2), sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n+  rhs_scale = f8e8m0fnu[8,128,16]{2,1,0} parameter(3), sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n+  ROOT block_scaled_dot = f32[8,1024,128]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::CollectivePermute(op::CustomCall({\"__op$block_scaled_dot\"})));\n+}\n+\n+TEST_F(DotHandlerTest,\n+       MXCustomCall_ContractingNonContractingAndContractingNonContracting0) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n+  rhs_scale = f8e8m0fnu[128,16] parameter(3), sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::AllGather(),\n+                     op::AllGather(), op::AllGather(), op::AllGather()));\n+}\n+\n+TEST_F(DotHandlerTest,\n+       MXCustomCall_ContractingNonContractingAndContractingNonContracting1) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={replicated}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::AllReduce(op::CustomCall({\"__op$block_scaled_dot\"})));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ReplicatedAndReplicated0) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={replicated}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={replicated}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1,4]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::DynamicSlice(),\n+                     op::Parameter(2), op::DynamicSlice(), op::Parameter(3)));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ReplicatedAndReplicated1) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={replicated}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={replicated}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1]0,1,2,3,4,5,6,7}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::DynamicSlice(),\n+                     op::Parameter(2), op::DynamicSlice(), op::Parameter(3)));\n+}\n+\n }  // namespace\n }  // namespace spmd\n }  // namespace xla"
        },
        {
            "sha": "4bec21743c0155c006db3059a821d224a287baff",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 91,
            "deletions": 8,
            "changes": 99,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -489,8 +489,6 @@ class PartitionedHlo {\n   };\n   PartitionedHlo(HloInstruction* hlo, Shape base_shape, PartitioningState state)\n       : hlo_(hlo), base_shape_(base_shape), state_(std::move(state)) {\n-    CHECK(hlo->has_sharding())\n-        << \"PartitionedHlo is missing sharding:\" << hlo->ToString();\n   }\n \n   PartitionedHlo(PartitionedHlo&& other) = default;\n@@ -545,8 +543,14 @@ class PartitionedHlo {\n   // Returns the sharding of the SPMD instruction.\n   const HloSharding& sharding() const { return hlo_->sharding(); }\n \n-  // Returns the SPMD instruction's number of dimensions.\n-  int64_t num_dimensions() const { return base_shape_.dimensions().size(); }\n+  void set_sharding(const HloSharding& sharding) {\n+    hlo_->set_sharding(sharding);\n+  }\n+\n+  // Returns the rank of the SPMD instruction.\n+  const int64_t num_dimensions() const {\n+    return base_shape_.dimensions().size();\n+  }\n \n   int64_t NewChannel() const { return (*state_.next_channel_id)++; }\n \n@@ -624,6 +628,87 @@ class PartitionedHlo {\n   PartitioningState state_;\n };\n \n+// Combines two identically sharded PartitionedHlo instructions describing the\n+// operand and scale tensors in OCP microscaling (MX) formats used in\n+// block-scaled dots. See\n+// https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf.\n+class PartitionedHloMX {\n+ public:\n+  class ShapesMX {\n+   public:\n+    ShapesMX(const Shape& operand_shape, const Shape& scale_shape)\n+        : shapes_(operand_shape, scale_shape) {};\n+\n+    absl::Span<const int64_t> dimensions() const {\n+      return shapes_.first.dimensions();\n+    }\n+\n+    operator Shape() const { return shapes_.first; }\n+\n+    operator std::pair<Shape, Shape>() const { return shapes_; }\n+\n+   private:\n+    std::pair<Shape, Shape> shapes_;\n+  };\n+\n+  PartitionedHloMX(const PartitionedHlo& operand, const PartitionedHlo& scale)\n+      : operand_(operand.hlo(), operand.base_shape(), operand.state()),\n+        scale_(scale.hlo(), scale.base_shape(), scale.state()) {\n+    CHECK_EQ(operand.sharding(), scale.sharding())\n+        << \"Operand and scale must be identically sharded.\";\n+  };\n+\n+  PartitionedHlo operand() const { return operand_; }\n+\n+  PartitionedHlo scale() const { return scale_; }\n+\n+  const HloSharding& sharding() const { return operand_.sharding(); }\n+\n+  void set_sharding(const HloSharding& sharding) {\n+    operand_.set_sharding(sharding);\n+    scale_.set_sharding(sharding);\n+  }\n+\n+  const ShapesMX base_shape() const {\n+    return ShapesMX(operand_.base_shape(), scale_.base_shape());\n+  }\n+\n+  const PartitionedHlo::PartitioningState& state() const {\n+    return operand_.state();\n+  }\n+\n+  // Returns the operand instruction.\n+  HloInstruction* hlo() const { return operand_.hlo(); }\n+\n+  PartitionedHloMX Replicate() const {\n+    return PartitionedHloMX(operand_.Replicate(), scale_.Replicate());\n+  }\n+\n+  PartitionedHloMX Reshard(const HloSharding& target) const {\n+    return PartitionedHloMX(operand_.Reshard(target), scale_.Reshard(target));\n+  }\n+\n+  PartitionedHloMX PadWithZero(\n+      absl::Span<const int64_t> left_padded_dims = {},\n+      absl::Span<const int64_t> skipped_dims = {}) const {\n+    return PartitionedHloMX(\n+        operand_.PadWithZero(left_padded_dims, skipped_dims),\n+        scale_.PadWithZero(left_padded_dims, skipped_dims));\n+  }\n+\n+  PartitionedHloMX PadWithZeroOnSpecifiedDims(\n+      absl::Span<const int64_t> dims,\n+      absl::Span<const int64_t> left_padded_dims = {}) const {\n+    return PartitionedHloMX(\n+        operand_.PadWithZeroOnSpecifiedDims(dims, left_padded_dims),\n+        scale_.PadWithZeroOnSpecifiedDims(dims, left_padded_dims));\n+  }\n+\n+ private:\n+  PartitionedHlo operand_;\n+  PartitionedHlo scale_;\n+};\n+\n class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n  public:\n   SpmdPartitioningVisitor(\n@@ -677,13 +762,11 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n   absl::Status HandleWhile(HloInstruction* hlo) override;\n \n   // Implementation of dot partitioning given DotGeneralDimsMapping.\n+  template <typename CreateShardedFunctor>\n   absl::Status HandleDotHelper(\n       HloInstruction* hlo,\n       const dot_as_convolution_util::DotConvolutionDimsInfo& dims_mapping,\n-      absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-          HloInstruction*, HloInstruction*, SpmdBuilder*,\n-          const Window& conv_window)>\n-          create_sharded_dot);\n+      CreateShardedFunctor& create_sharded_dot);\n \n   // Common handle for elementwise HLOs.\n   absl::Status HandleElementwise(HloInstruction* hlo);"
        },
        {
            "sha": "c883db7ca98a2a40867ce4c092f3014cf97a9cad",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85cbaaee688ecb39bf783a82e12443d1739619ad/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=85cbaaee688ecb39bf783a82e12443d1739619ad",
            "patch": "@@ -1672,6 +1672,110 @@ ENTRY entry {\n   CollectiveOpsCompareShardedUnsharded(hlo_text, /*num_partitions=*/4);\n }\n \n+TEST_F(CollectiveOpsTestE2EShardedUnsharded, BlockScaledDotBatchAndBatch) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[4,16,64]{2,1,0}, f8e8m0fnu[4,16,2]{2,1,0}, f8e4m3fn[4,4,64]{2,1,0}, f8e8m0fnu[4,4,2]{2,1,0})->f32[4,16,4]{2,1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[4,16,64]{2,1,0} parameter(0), sharding={devices=[2,1,1]<=[2]}\n+  lhs_scale = f8e8m0fnu[4,16,2]{2,1,0} parameter(1), sharding={devices=[2,1,1]<=[2]}\n+  rhs = f8e4m3fn[4,4,64]{2,1,0} parameter(2), sharding={devices=[2,1,1]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,4,2]{2,1,0} parameter(3), sharding={devices=[2,1,1]<=[2]}\n+  ROOT block_scaled_dot = f32[4,16,4]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[1,2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotBatchAndNonContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[4,16,64]{2,1,0}, f8e8m0fnu[4,16,2]{2,1,0}, f8e4m3fn[4,4,64]{2,1,0}, f8e8m0fnu[4,4,2]{2,1,0})->f32[4,16,4]{2,1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[4,16,64]{2,1,0} parameter(0), sharding={devices=[2,1,1]<=[2]}\n+  lhs_scale = f8e8m0fnu[4,16,2]{2,1,0} parameter(1), sharding={devices=[2,1,1]<=[2]}\n+  rhs = f8e4m3fn[4,4,64]{2,1,0} parameter(2), sharding={devices=[1,2,1]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,4,2]{2,1,0} parameter(3), sharding={devices=[1,2,1]<=[2]}\n+  ROOT block_scaled_dot = f32[4,16,4]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotContractingAndContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[16,64]{1,0}, f8e8m0fnu[16,2]{1,0}, f8e4m3fn[4,64]{1,0}, f8e8m0fnu[4,2]{1,0})->f32[16,4]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[16,64]{1,0} parameter(0), sharding={devices=[1,2]<=[2]}\n+  lhs_scale = f8e8m0fnu[16,2]{1,0} parameter(1), sharding={devices=[1,2]<=[2]}\n+  rhs = f8e4m3fn[4,64]{1,0} parameter(2), sharding={devices=[1,2]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,2]{1,0} parameter(3), sharding={devices=[1,2]<=[2]}\n+  ROOT block_scaled_dot = f32[16,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotNonContractingAndContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[16,128]{1,0}, f8e8m0fnu[16,4]{1,0}, f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4]{1,0})->f32[16,4]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[16,128]{1,0} parameter(0), sharding={devices=[2,1]<=[2]}\n+  lhs_scale = f8e8m0fnu[16,4]{1,0} parameter(1), sharding={devices=[2,1]<=[2]}\n+  rhs = f8e4m3fn[4,128]{1,0} parameter(2), sharding={devices=[1,2]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,4]{1,0} parameter(3), sharding={devices=[1,2]<=[2]}\n+  ROOT block_scaled_dot = f32[16,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotContractingAndReplicated) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[16,128]{1,0}, f8e8m0fnu[16,4]{1,0}, f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4]{1,0})->f32[16,4]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[16,128]{1,0} parameter(0), sharding={devices=[1,2]<=[2]}\n+  lhs_scale = f8e8m0fnu[16,4]{1,0} parameter(1), sharding={devices=[1,2]<=[2]}\n+  rhs = f8e4m3fn[4,128]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[4,4]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[16,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotReplicatedAndReplicated) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4], f8e4m3fn[1,128]{1,0}, f8e8m0fnu[1,4]{1,0})->f32[4,1]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[4,128]{1,0} parameter(0), sharding={replicated}\n+  lhs_scale = f8e8m0fnu[4,4]{1,0} parameter(1), sharding={replicated}\n+  rhs = f8e4m3fn[1,128]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[1,4]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[4,1]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotContractingNonContractingAndContractingNonContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[8,128]{1,0}, f8e8m0fnu[8,4]{1,0}, f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4]{1,0})->f32[8,4]{1,0}}, num_partitions=4\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,128]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  lhs_scale = f8e8m0fnu[8,4]{1,0} parameter(1), sharding={devices=[2,2]<=[4]}\n+  rhs = f8e4m3fn[4,128]{1,0} parameter(2), sharding={devices=[2,2]<=[4]}\n+  rhs_scale = f8e8m0fnu[4,4]{1,0} parameter(3), sharding={devices=[2,2]<=[4]}\n+  ROOT dot = f32[8,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,2]<=[4]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text, /*num_partitions=*/4);\n+}\n+\n // E2E tests comparing the results of windowed einsum and non-windowed cases.\n class CollectiveOpsTestE2EWindowedNonWindowed : public CollectiveOpsTestE2E {\n  public:"
        }
    ],
    "stats": {
        "total": 1682,
        "additions": 1176,
        "deletions": 506
    }
}