{
    "author": "khasanovaa",
    "message": "Add proto [de]serialization for HostExecuteStartThunk\n\nPiperOrigin-RevId: 820645056",
    "sha": "30d25d6d18f9891fbfa5bce3b69256657ad911ed",
    "files": [
        {
            "sha": "6dbf2dfcd2c23edc7a9de1727fbcd126b93761c5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=30d25d6d18f9891fbfa5bce3b69256657ad911ed",
            "patch": "@@ -2449,6 +2449,7 @@ tf_proto_library(\n         \":convolution_filter_thunk_proto\",\n         \":dynamic_slice_thunk_proto\",\n         \"//xla:xla_data_proto\",\n+        \"//xla/core/host_offloading:host_offloading_executable_proto\",\n         \"//xla/service:buffer_assignment_proto\",\n         \"//xla/service/gpu:backend_configs\",\n         \"//xla/service/gpu:gpu_conv_runner_proto\",\n@@ -2744,6 +2745,7 @@ xla_test(\n     deps = [\n         \":host_execute_thunk\",\n         \":thunk\",\n+        \":thunk_proto_cc\",\n         \"//xla:executable_run_options\",\n         \"//xla:literal\",\n         \"//xla:literal_util\",\n@@ -2770,12 +2772,14 @@ xla_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:casts\",\n+        \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n "
        },
        {
            "sha": "b65308411c0aa28c5409970a58fe9c28f5b68fab",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_execute_thunk.cc",
            "status": "modified",
            "additions": 76,
            "deletions": 6,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc?ref=30d25d6d18f9891fbfa5bce3b69256657ad911ed",
            "patch": "@@ -19,11 +19,13 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n+#include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n \n #include \"absl/base/call_once.h\"\n+#include \"absl/base/casts.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n@@ -425,24 +427,84 @@ HostExecuteStartThunk::HostExecuteStartThunk(\n     Thunk::ThunkInfo thunk_info,\n     const HostOffloadingExecutableProto& host_offloading_executable_proto,\n     absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4> args,\n-    absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4> results)\n+    absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4> results,\n+    std::shared_ptr<HostExecuteAsyncEvents> async_events)\n     : Thunk(Thunk::Kind::kHostExecuteStart, std::move(thunk_info)),\n       args_(std::move(args)),\n       results_(std::move(results)),\n-      executable_proto_(host_offloading_executable_proto),\n-      async_events_(std::make_shared<HostExecuteAsyncEvents>()) {}\n+      executable_proto_(host_offloading_executable_proto) {\n+  async_events_ =\n+      async_events ? async_events : std::make_shared<HostExecuteAsyncEvents>();\n+}\n \n std::string HostExecuteStartThunk::ToString(int indent) const { return \"\"; }\n \n absl::StatusOr<ThunkProto> HostExecuteStartThunk::ToProto() const {\n-  return Unimplemented(\"Not implemented yet.\");\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+  HostExecuteStartThunkProto* host_execute_start_thunk_proto =\n+      proto.mutable_host_execute_start_thunk();\n+\n+  *host_execute_start_thunk_proto->mutable_executable_proto() =\n+      executable_proto_;\n+\n+  for (const auto& [slice, shape] : args_) {\n+    ShapedSliceProto* arg_proto = host_execute_start_thunk_proto->add_args();\n+    TF_ASSIGN_OR_RETURN(*arg_proto->mutable_slice(), slice.ToProto());\n+    *arg_proto->mutable_shape() = shape.ToProto();\n+  }\n+\n+  for (const auto& [slice, shape] : results_) {\n+    ShapedSliceProto* result_proto =\n+        host_execute_start_thunk_proto->add_results();\n+    TF_ASSIGN_OR_RETURN(*result_proto->mutable_slice(), slice.ToProto());\n+    *result_proto->mutable_shape() = shape.ToProto();\n+  }\n+\n+  auto async_events_unique_id = GetAsyncEventsUniqueId();\n+  // By design, async_events_unique_id should always be present for\n+  // HostExecuteStartThunk.\n+  CHECK_NE(async_events_unique_id, std::nullopt);\n+\n+  host_execute_start_thunk_proto->set_async_events_unique_id(\n+      async_events_unique_id.value().value());\n+\n+  return proto;\n }\n \n absl::StatusOr<std::unique_ptr<HostExecuteStartThunk>>\n HostExecuteStartThunk::FromProto(\n     ThunkInfo thunk_info, const HostExecuteStartThunkProto& proto,\n-    absl::Span<const BufferAllocation> buffer_allocations) {\n-  return Unimplemented(\"Not implemented yet.\");\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    HostExecuteAsyncEventsMap& async_events_map) {\n+  absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4> args, results;\n+  auto shaped_slice_from_proto =\n+      [&](const auto& shaped_slice_protos,\n+          absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4>&\n+              slices_and_shapes) -> absl::Status {\n+    for (const auto& shaped_slice_proto : shaped_slice_protos) {\n+      TF_ASSIGN_OR_RETURN(auto slice,\n+                          BufferAllocation::Slice::FromProto(\n+                              shaped_slice_proto.slice(), buffer_allocations));\n+      TF_ASSIGN_OR_RETURN(auto shape,\n+                          Shape::FromProto(shaped_slice_proto.shape()));\n+      slices_and_shapes.push_back({slice, shape});\n+    }\n+    return absl::OkStatus();\n+  };\n+\n+  TF_RETURN_IF_ERROR(shaped_slice_from_proto(proto.args(), args));\n+  TF_RETURN_IF_ERROR(shaped_slice_from_proto(proto.results(), results));\n+\n+  // If async_events_map already contains an entry for the given unique id,\n+  // that means that the pairing done thunk is already serialized and we reuse\n+  // the id to connect them. Otherwise, create a new entry.\n+  auto [async_event_it, _] = async_events_map.try_emplace(\n+      AsyncEventsUniqueId(proto.async_events_unique_id()),\n+      std::make_shared<HostExecuteAsyncEvents>());\n+  return std::make_unique<HostExecuteStartThunk>(\n+      thunk_info, proto.executable_proto(), std::move(args), std::move(results),\n+      async_event_it->second);\n }\n \n static HostOffloadingAllocator* GetHostOffloadingAllocator(\n@@ -556,6 +618,14 @@ absl::Status HostExecuteStartThunk::ExecuteOnStream(\n   return absl::OkStatus();\n }\n \n+std::optional<AsyncEventsUniqueId>\n+HostExecuteStartThunk::GetAsyncEventsUniqueId() const {\n+  CHECK(async_events_)\n+      << \"async_events_ must not be null in HostExecuteStartThunk\";\n+  // We rely on the fact that the pointer to async_events_ is unique.\n+  return absl::bit_cast<AsyncEventsUniqueId>(async_events_.get());\n+}\n+\n // HostExecuteDoneThunk\n \n HostExecuteDoneThunk::HostExecuteDoneThunk("
        },
        {
            "sha": "a0df8525dfce08f98c7cfe92dad70ea15dee57a9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_execute_thunk.h",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.h?ref=30d25d6d18f9891fbfa5bce3b69256657ad911ed",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_RUNTIME_HOST_EXECUTE_THUNK_H_\n \n #include <memory>\n+#include <optional>\n #include <string>\n #include <utility>\n \n@@ -70,6 +71,10 @@ class HostExecuteAsyncEvents {\n       events_ ABSL_GUARDED_BY(events_mu_);\n };\n \n+using HostExecuteAsyncEventsMap =\n+    absl::flat_hash_map<AsyncEventsUniqueId,\n+                        std::shared_ptr<HostExecuteAsyncEvents>>;\n+\n class HostExecuteStartThunk : public Thunk {\n  public:\n   struct SliceAndShape {\n@@ -95,9 +100,14 @@ class HostExecuteStartThunk : public Thunk {\n   std::string ToString(int indent) const override;\n \n   absl::StatusOr<ThunkProto> ToProto() const override;\n+\n+  // If async_events_map already contains an entry for the given unique id, we\n+  // reuse the id to connect the start and done thunks. Otherwise, insert a new\n+  // entry into the map.\n   static absl::StatusOr<std::unique_ptr<HostExecuteStartThunk>> FromProto(\n       ThunkInfo thunk_info, const HostExecuteStartThunkProto& proto,\n-      absl::Span<const BufferAllocation> buffer_allocations);\n+      absl::Span<const BufferAllocation> buffer_allocations,\n+      HostExecuteAsyncEventsMap& async_events_map);\n \n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n@@ -118,12 +128,14 @@ class HostExecuteStartThunk : public Thunk {\n     return &executable_proto_;\n   }\n \n- protected:\n+  std::optional<AsyncEventsUniqueId> GetAsyncEventsUniqueId() const override;\n+\n   HostExecuteStartThunk(\n       Thunk::ThunkInfo thunk_info,\n       const HostOffloadingExecutableProto& host_offloading_executable_proto,\n       absl::InlinedVector<SliceAndShape, 4> args,\n-      absl::InlinedVector<SliceAndShape, 4> results);\n+      absl::InlinedVector<SliceAndShape, 4> results,\n+      std::shared_ptr<HostExecuteAsyncEvents> async_events = nullptr);\n \n  private:\n   absl::once_flag executable_init_flag_;"
        },
        {
            "sha": "4d59720875a23dae70641f49ff43952785f1e3e8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_execute_thunk_test.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk_test.cc?ref=30d25d6d18f9891fbfa5bce3b69256657ad911ed",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <cstring>\n #include <memory>\n #include <utility>\n+#include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n@@ -52,6 +53,7 @@ limitations under the License.\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/casts.h\"\n \n@@ -584,6 +586,60 @@ TEST(HostExecuteDoneThunkTest, WaitingOnErrorEvent) {\n               absl_testing::StatusIs(absl::StatusCode::kInternal));\n }\n \n+TEST(HostExecuteStartThunkTest, ProtoRoundTrip) {\n+  static constexpr char const* kHloModule = R\"(\n+    HloModule module\n+    ENTRY add_inplace {\n+      p0 = s32[] parameter(0)\n+      ROOT add = s32[] add(p0, p0)\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto hlo_module,\n+                          ParseAndReturnUnverifiedModule(kHloModule, {}));\n+\n+  BufferAllocation alloc_arg(/*index=*/0, 4, /*color=*/0);\n+  BufferAllocation alloc_result(/*index=*/1, 4, /*color=*/0);\n+\n+  BufferAllocation::Slice slice_arg(&alloc_arg, 0, 4);\n+  BufferAllocation::Slice slice_result(&alloc_result, 0, 4);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto thunk,\n+                          CreateHostExecuteStartThunk(\n+                              Thunk::ThunkInfo(), *hlo_module,\n+                              {{slice_arg, ShapeUtil::MakeShape(S32, {})}},\n+                              {{slice_result, ShapeUtil::MakeShape(S32, {})}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, thunk->ToProto());\n+\n+  std::vector<BufferAllocation> buffer_allocations = {\n+      BufferAllocation(/*index=*/0, /*size=*/4, /*color=*/0),\n+      BufferAllocation(/*index=*/1, /*size=*/4, /*color=*/0)};\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Thunk::ThunkInfo thunk_info,\n+                          Thunk::ThunkInfo::FromProto(proto.thunk_info()));\n+  HostExecuteAsyncEventsMap async_events_map;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HostExecuteStartThunk> round_trip_thunk,\n+      HostExecuteStartThunk::FromProto(thunk_info,\n+                                       proto.host_execute_start_thunk(),\n+                                       buffer_allocations, async_events_map));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto,\n+                          round_trip_thunk->ToProto());\n+  EXPECT_EQ(async_events_map.size(), 1);\n+  EXPECT_EQ(async_events_map.begin()->first,\n+            thunk->GetAsyncEventsUniqueId().value());\n+\n+  // ids are expected to be different, so drop them for the comparison.\n+  round_trip_proto.mutable_host_execute_start_thunk()\n+      ->clear_async_events_unique_id();\n+  proto.mutable_host_execute_start_thunk()->clear_async_events_unique_id();\n+\n+  EXPECT_THAT(round_trip_proto, tsl::proto_testing::EqualsProto(proto));\n+}\n+\n }  // namespace\n \n }  // namespace gpu"
        },
        {
            "sha": "7543054d6c08113b85785681e3635bf9739a6c70",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=30d25d6d18f9891fbfa5bce3b69256657ad911ed",
            "patch": "@@ -19,6 +19,7 @@ package xla.gpu;\n \n import \"xla/backends/gpu/runtime/convolution_filter_thunk.proto\";\n import \"xla/backends/gpu/runtime/dynamic_slice_thunk.proto\";\n+import \"xla/core/host_offloading/host_offloading_executable.proto\";\n import \"xla/service/buffer_assignment.proto\";\n import \"xla/service/gpu/gpu_conv_runner.proto\";\n import \"xla/service/gpu/gpu_norm_runner.proto\";\n@@ -141,8 +142,21 @@ message CudnnThunkProto {\n   optional int64 sdpa_dropout_seed = 3;\n }\n \n-message HostExecuteStartThunkProto {}\n-message HostExecuteDoneThunkProto {}\n+message ShapedSliceProto {\n+  xla.buffer_assignment.BufferAllocationSliceProto slice = 1;\n+  xla.ShapeProto shape = 2;\n+}\n+\n+message HostExecuteStartThunkProto {\n+  HostOffloadingExecutableProto executable_proto = 1;\n+  repeated ShapedSliceProto args = 2;\n+  repeated ShapedSliceProto results = 3;\n+  uint64 async_events_unique_id = 4;\n+}\n+\n+message HostExecuteDoneThunkProto {\n+  uint64 async_events_unique_id = 1;\n+}\n \n message DynamicSliceThunkProto {\n   SequentialThunkProto embedded_thunk = 1;\n@@ -164,11 +178,6 @@ message Memset32BitValueThunkProto {\n   uint32 value = 2;\n }\n \n-message ShapedSliceProto {\n-  xla.buffer_assignment.BufferAllocationSliceProto slice = 1;\n-  xla.ShapeProto shape = 2;\n-}\n-\n message InfeedThunkProto {\n   repeated ShapedSliceProto dest_slices = 1;\n }"
        },
        {
            "sha": "a4d5c911348c061cceb4735f39239acb9faa87e9",
            "filename": "third_party/xla/xla/core/host_offloading/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30d25d6d18f9891fbfa5bce3b69256657ad911ed/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD?ref=30d25d6d18f9891fbfa5bce3b69256657ad911ed",
            "patch": "@@ -118,7 +118,9 @@ strict_cc_test(\n tf_proto_library(\n     name = \"host_offloading_executable_proto\",\n     srcs = [\"host_offloading_executable.proto\"],\n-    compatible_with = get_compatible_with_libtpu_portable(),\n+    compatible_with = get_compatible_with_libtpu_portable() + [\n+        # copybara:uncomment \"//buildenv/target:non_prod\",\n+    ],\n     deps = [\n         \"//xla/service:hlo_proto\",\n         \"//xla/service/cpu:executable_proto\","
        }
    ],
    "stats": {
        "total": 187,
        "additions": 170,
        "deletions": 17
    }
}