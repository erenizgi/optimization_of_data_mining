{
    "author": "mwhittaker",
    "message": "Rolling back https://github.com/openxla/xla/pull/29395\n\nReverts 95a60058f6ae17b7d42bcb5fb607b39e65eaab6b\n\nPiperOrigin-RevId: 811439747",
    "sha": "556a313929861a2ace23d1c34fcf6d2c7f215ca7",
    "files": [
        {
            "sha": "b8befa7ac29031b097e3a4fa6ccf65fd5b77b935",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=556a313929861a2ace23d1c34fcf6d2c7f215ca7",
            "patch": "@@ -165,7 +165,6 @@ cc_library(\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:btree\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/container:node_hash_map\",\n         \"@com_google_absl//absl/functional:function_ref\","
        },
        {
            "sha": "985084b1d10db5e36c7fb13b5b4692a93744aa94",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_cliques.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 128,
            "changes": 140,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc?ref=556a313929861a2ace23d1c34fcf6d2c7f215ca7",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/btree_map.h\"\n-#include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/node_hash_map.h\"\n #include \"absl/functional/function_ref.h\"\n@@ -101,8 +100,6 @@ namespace {\n struct ProcessGpuCliques {\n   absl::Mutex mu;\n   absl::node_hash_map<GpuCliqueKey, LockableGpuClique> map ABSL_GUARDED_BY(mu);\n-  std::vector<tensorflow::CoordinatedTaskStateInfo> task_state_infos\n-      ABSL_GUARDED_BY(mu);\n };\n }  // namespace\n \n@@ -228,43 +225,6 @@ static absl::StatusOr<bool> EnablePeerAccess(\n   return true;\n }\n \n-// Returns a non-ok status if the provided clique key is \"stale\". A clique key\n-// is stale if its incarnations don't match the latest incarnations or if any of\n-// the tasks specified in the clique key have failed.\n-//\n-// REQUIRES: GetProcessGpuCliques().mu held\n-static absl::Status CheckCliqueKeyIsntStale(\n-    absl::Span<const tensorflow::CoordinatedTaskStateInfo> task_state_infos,\n-    const GpuCliqueKey& clique_key) {\n-  if (task_state_infos.empty()) {\n-    // If we don't have any task state info, assume the clique key isn't stale.\n-    return absl::OkStatus();\n-  }\n-\n-  // Create an index from incarnation id to task state info.\n-  using Info = tensorflow::CoordinatedTaskStateInfo;\n-  absl::flat_hash_map<IncarnationId, const Info*> incarnation_to_info;\n-  for (const Info& info : task_state_infos) {\n-    incarnation_to_info[IncarnationId(info.incarnation())] = &info;\n-  }\n-\n-  // Check that every incarnation is fresh.\n-  for (IncarnationId id : clique_key.incarnations()) {\n-    auto it = incarnation_to_info.find(id);\n-    if (it == incarnation_to_info.end()) {\n-      return FailedPrecondition(\"Incarnation id %d is stale\", id.value());\n-    }\n-    const auto& [unused, info] = *it;\n-    if (info->state() !=\n-        tensorflow::CoordinatedTaskState::TASKSTATE_CONNECTED) {\n-      return FailedPrecondition(\"Task with incarnation id %d is not connected\",\n-                                id.value());\n-    }\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n // Joins a GpuClique initialization rendezvous for a `clique_key` and returns\n // a lock that gives an access to initialized clique (access is shared between\n // all participating ranks that own a shared pointer).\n@@ -340,11 +300,6 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         clique_key.ToString(), DeviceRanksToString(ranks), nroots,\n         clique_ids.fingerprint(), peer_access_enabled);\n \n-    ProcessGpuCliques& cliques = GetProcessGpuCliques();\n-    absl::MutexLock lock(cliques.mu);\n-    TF_RETURN_IF_ERROR(\n-        CheckCliqueKeyIsntStale(cliques.task_state_infos, clique_key));\n-\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<Communicator>> created_comms,\n         collectives->CreateCommunicators(clique_key, clique_ids, ranks,\n@@ -361,6 +316,9 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         clique_key.ToString(), DeviceRanksToString(ranks), nroots,\n         clique_ids.fingerprint(), peer_access_enabled);\n \n+    ProcessGpuCliques& cliques = GetProcessGpuCliques();\n+    absl::MutexLock lock(cliques.mu);\n+\n     // Create a new clique with given clique key and communicators.\n     auto emplaced =\n         cliques.map.try_emplace(clique_key, clique_key, clique_ids,\n@@ -515,11 +473,6 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         peer_access_enabled,\n         absl::StrJoin(rank_mapping, \",\", rank_mapping_formatter));\n \n-    ProcessGpuCliques& cliques = GetProcessGpuCliques();\n-    absl::MutexLock lock(cliques.mu);\n-    TF_RETURN_IF_ERROR(\n-        CheckCliqueKeyIsntStale(cliques.task_state_infos, clique_key));\n-\n     TF_ASSIGN_OR_RETURN(\n         auto splitted_comms,\n         collectives->SplitCommunicators(parent_comms, color, keys, config));\n@@ -537,6 +490,9 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         peer_access_enabled,\n         absl::StrJoin(rank_mapping, \",\", rank_mapping_formatter));\n \n+    ProcessGpuCliques& cliques = GetProcessGpuCliques();\n+    absl::MutexLock lock(cliques.mu);\n+\n     // Create a new clique with given clique key and communicators.\n     auto emplaced =\n         cliques.map.try_emplace(clique_key, clique_key, std::nullopt,\n@@ -608,10 +564,7 @@ absl::StatusOr<std::shared_ptr<LockableGpuClique::Lock>> AcquireGpuClique(\n             auto lockable_clique = [&]() -> LockableGpuClique* {\n               absl::MutexLock lock(cliques.mu);\n               auto it = cliques.map.find(clique_key);\n-              absl::Status stale =\n-                  CheckCliqueKeyIsntStale(cliques.task_state_infos, clique_key);\n-              return it == cliques.map.end() || !stale.ok() ? nullptr\n-                                                            : &it->second;\n+              return it == cliques.map.end() ? nullptr : &it->second;\n             }();\n \n             return lockable_clique ? lockable_clique->Acquire()\n@@ -664,14 +617,7 @@ bool CliqueKeyContainsIncarnation(\n                         });\n }\n \n-// Aborts and invalidates all cliques that have been created via\n-// AcquireGpuClique with any of the provided incarnations. For example, if\n-// incarnations is [1, 2], then all cliques with a clique key that includes\n-// incarnations 1 or 2 will be aborted.\n-//\n-// REQUIRES: GetProcessGpuCliques().mu held\n-static absl::Status AbortCliquesWithIncarnations(\n-    absl::node_hash_map<GpuCliqueKey, LockableGpuClique>& map,\n+absl::Status AbortCliquesWithIncarnations(\n     absl::Span<const IncarnationId> incarnations) {\n   VLOG(1) << \"Aborting GPU cliques for incarnations \"\n           << absl::StrJoin(incarnations, \", \",\n@@ -680,8 +626,10 @@ static absl::Status AbortCliquesWithIncarnations(\n                            });\n   const absl::flat_hash_set<IncarnationId> incarnation_set(incarnations.begin(),\n                                                            incarnations.end());\n+  ProcessGpuCliques& cliques = GetProcessGpuCliques();\n+  absl::MutexLock lock(cliques.mu);\n   absl::Status result;\n-  for (auto it = map.begin(); it != map.end();) {\n+  for (auto it = cliques.map.begin(); it != cliques.map.end();) {\n     auto copy = it++;\n     auto& [key, lockable_clique] = *copy;\n     if (!CliqueKeyContainsIncarnation(key, incarnation_set)) {\n@@ -693,73 +641,9 @@ static absl::Status AbortCliquesWithIncarnations(\n       LOG(ERROR) << \"Error aborting GPU clique \" << key.ToString() << \": \" << s;\n       result = std::move(s);\n     }\n-    map.erase(copy);\n+    cliques.map.erase(copy);\n   }\n   return result;\n }\n \n-// Aborts all NCCL collectives when a task fails, as reported by the\n-// UpdateGlobalProcessInfo.\n-//\n-// REQUIRES: GetProcessGpuCliques().mu held\n-static absl::Status AbortOnFailure(\n-    absl::node_hash_map<GpuCliqueKey, LockableGpuClique>& map,\n-    absl::Span<const tensorflow::CoordinatedTaskStateInfo> previous_state,\n-    absl::Span<const tensorflow::CoordinatedTaskStateInfo> current_state) {\n-  if (previous_state.empty()) {\n-    // When a job first starts, there is no previous job state.\n-    return absl::OkStatus();\n-  }\n-\n-  // We expect previous_state and current_state to have the same size, and we\n-  // expect for every i, previous_state[i] and current_state[i] correspond to\n-  // the same task.\n-  if (previous_state.size() != current_state.size()) {\n-    return FailedPrecondition(\n-        \"Previous and current job states have different sizes: %d vs %d\",\n-        previous_state.size(), current_state.size());\n-  }\n-\n-  std::vector<IncarnationId> failed_incarnations;\n-  for (int i = 0; i < previous_state.size(); ++i) {\n-    const tensorflow::CoordinatedTaskStateInfo& previous = previous_state[i];\n-    const tensorflow::CoordinatedTaskStateInfo& current = current_state[i];\n-    if (previous.task().task_id() != current.task().task_id()) {\n-      return FailedPrecondition(\n-          \"Previous and current job states have mismatched task ids: %d vs %d\",\n-          previous.task().task_id(), current.task().task_id());\n-    }\n-    if (previous.state() !=\n-        tensorflow::CoordinatedTaskState::TASKSTATE_CONNECTED) {\n-      // A task that was not previously connected cannot fail.\n-      continue;\n-    }\n-    if (current.state() !=\n-            tensorflow::CoordinatedTaskState::TASKSTATE_CONNECTED ||\n-        previous.incarnation() != current.incarnation()) {\n-      // The task is either failed, or restarted with a different incarnation.\n-      VLOG(1) << \"Task \" << previous.task().task_id() << \" (incarnation \"\n-              << previous.incarnation() << \") failed\";\n-      failed_incarnations.push_back(IncarnationId(previous.incarnation()));\n-    }\n-  }\n-\n-  if (!failed_incarnations.empty()) {\n-    return AbortCliquesWithIncarnations(map, failed_incarnations);\n-  }\n-  return absl::OkStatus();\n-}\n-\n-absl::Status UpdateGlobalProcessInfo(\n-    absl::Span<tensorflow::CoordinatedTaskStateInfo> infos) {\n-  ProcessGpuCliques& cliques = GetProcessGpuCliques();\n-  absl::MutexLock lock(&cliques.mu);\n-  absl::Status s = AbortOnFailure(cliques.map, cliques.task_state_infos, infos);\n-  if (!s.ok()) {\n-    LOG(WARNING) << s;\n-  }\n-  cliques.task_state_infos = {infos.begin(), infos.end()};\n-  return s;\n-}\n-\n }  // namespace xla::gpu"
        },
        {
            "sha": "beb94d1c7958db51426ce6c00c23b3641d613c16",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_cliques.h",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.h?ref=556a313929861a2ace23d1c34fcf6d2c7f215ca7",
            "patch": "@@ -69,11 +69,12 @@ absl::StatusOr<std::shared_ptr<LockableGpuClique::Lock>> AcquireGpuClique(\n     const GpuCollectives::CliqueIdCallback& clique_id_callback, RankId rank,\n     const AcquiredCliquesMap& acquired_cliques, int64_t max_nchannels = 0);\n \n-// Updates the global set of task state information. This function aborts and\n-// invalidates all cliques that were created via AcquireGpuClique with\n-// incarnations that have become stale.\n-absl::Status UpdateGlobalProcessInfo(\n-    absl::Span<tensorflow::CoordinatedTaskStateInfo> infos);\n+// Aborts and invalidates all cliques that have been created via\n+// AcquireGpuClique with any of the provided incarnations. For example, if\n+// incarnations is [1, 2], then all cliques with a clique key that includes\n+// incarnations 1 or 2 will be aborted.\n+absl::Status AbortCliquesWithIncarnations(\n+    absl::Span<const IncarnationId> incarnations);\n \n }  // namespace xla::gpu\n "
        },
        {
            "sha": "b23c5bdf1c1d1e84c68c6138afc194048c8e37ed",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 3,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=556a313929861a2ace23d1c34fcf6d2c7f215ca7",
            "patch": "@@ -567,6 +567,55 @@ static absl::flat_hash_map<std::string, PjRtDeviceAttribute> GetAttrsForDevices(\n   return attrs;\n }\n \n+// Aborts all NCCL collectives when a task fails, as reported by the\n+// JobStateUpdate.\n+absl::Status AbortOnFailure(\n+    absl::Span<const tensorflow::CoordinatedTaskStateInfo> previous_state,\n+    absl::Span<const tensorflow::CoordinatedTaskStateInfo> current_state) {\n+  if (previous_state.empty()) {\n+    // When a job first starts, there is no previous job state.\n+    return absl::OkStatus();\n+  }\n+\n+  // We expect previous_state and current_state to have the same size, and we\n+  // expect for every i, previous_state[i] and current_state[i] correspond to\n+  // the same task.\n+  if (previous_state.size() != current_state.size()) {\n+    return FailedPrecondition(\n+        \"Previous and current job states have different sizes: %d vs %d\",\n+        previous_state.size(), current_state.size());\n+  }\n+\n+  std::vector<IncarnationId> failed_incarnations;\n+  for (int i = 0; i < previous_state.size(); ++i) {\n+    const tensorflow::CoordinatedTaskStateInfo& previous = previous_state[i];\n+    const tensorflow::CoordinatedTaskStateInfo& current = current_state[i];\n+    if (previous.task().task_id() != current.task().task_id()) {\n+      return FailedPrecondition(\n+          \"Previous and current job states have mismatched task ids: %d vs %d\",\n+          previous.task().task_id(), current.task().task_id());\n+    }\n+    if (previous.state() !=\n+        tensorflow::CoordinatedTaskState::TASKSTATE_CONNECTED) {\n+      // A task that was not previously connected cannot fail.\n+      continue;\n+    }\n+    if (current.state() !=\n+            tensorflow::CoordinatedTaskState::TASKSTATE_CONNECTED ||\n+        previous.incarnation() != current.incarnation()) {\n+      // The task is either failed, or restarted with a different incarnation.\n+      VLOG(1) << \"Task \" << previous.task().task_id() << \" (incarnation \"\n+              << previous.incarnation() << \") failed\";\n+      failed_incarnations.push_back(IncarnationId(previous.incarnation()));\n+    }\n+  }\n+\n+  if (!failed_incarnations.empty()) {\n+    return xla::gpu::AbortCliquesWithIncarnations(failed_incarnations);\n+  }\n+  return absl::OkStatus();\n+}\n+\n StreamExecutorGpuClient::StreamExecutorGpuClient(\n     std::string platform_name, LocalClient* client,\n     std::vector<std::unique_ptr<PjRtStreamExecutorDevice>> devices,\n@@ -649,10 +698,12 @@ void StreamExecutorGpuClient::UpdateGlobalProcessInfo(\n   if (!abort_collectives_on_failure_) {\n     return;\n   }\n-  absl::Status s = ::xla::gpu::UpdateGlobalProcessInfo(infos);\n-  if (!s.ok()) {\n-    LOG(WARNING) << s;\n+\n+  absl::MutexLock lock(task_state_infos_mu_);\n+  if (absl::Status s = AbortOnFailure(task_state_infos_, infos); !s.ok()) {\n+    LOG(ERROR) << s;\n   }\n+  task_state_infos_ = {infos.begin(), infos.end()};\n }\n \n absl::StatusOr<std::unique_ptr<PjRtClient::AsyncHostToDeviceTransferManager>>"
        },
        {
            "sha": "39aadf60fa8cc39208c91d327ffdaec8d34e2e2a",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/556a313929861a2ace23d1c34fcf6d2c7f215ca7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h?ref=556a313929861a2ace23d1c34fcf6d2c7f215ca7",
            "patch": "@@ -186,6 +186,10 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n   const bool abort_collectives_on_failure_ = false;\n   std::optional<xla::StreamExecutorGpuTopologyDescription> topology_;\n   std::shared_ptr<KeyValueStoreInterface> kv_store_;\n+\n+  absl::Mutex task_state_infos_mu_;\n+  std::vector<tensorflow::CoordinatedTaskStateInfo> task_state_infos_\n+      ABSL_GUARDED_BY(task_state_infos_mu_);\n };\n \n std::vector<std::unique_ptr<PjRtStreamExecutorDevice>> BuildLocalDevices("
        }
    ],
    "stats": {
        "total": 213,
        "additions": 76,
        "deletions": 137
    }
}