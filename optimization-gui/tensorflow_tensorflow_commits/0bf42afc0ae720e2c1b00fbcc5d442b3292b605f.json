{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Add simple bufferization for xtile extract/insert.\n\nFor now this just unconditionally does a copy from the memref buffer to the bufferized tensor, in the future we can do some analysis to avoid this in the un-strided case when we know all instances are in-bounds.\n\nPiperOrigin-RevId: 833313677",
    "sha": "0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
    "files": [
        {
            "sha": "f4a26521dbd09c5ae98440cb7e4c6892c9cfa753",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -310,7 +310,6 @@ static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n   emitters::RegisterOptimizationPasses(pm);\n \n   pm.addPass(CreateShloToVectorPass());\n-  pm.addPass(CreateXTileToVectorPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(CreateLowerXTileEntryPass());\n   pm.addNestedPass<mlir::func::FuncOp>("
        },
        {
            "sha": "7a2175e1ca962567dff5fa65228f7b78f966c606",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -53,7 +53,6 @@ cc_library(\n         \"memref_copy_to_loops.cc\",\n         \"shlo_to_vector.cc\",\n         \"tensor_ops_to_vector.cc\",\n-        \"xtile_to_vector.cc\",\n     ],\n     hdrs = [\"passes.h\"],\n     deps = ["
        },
        {
            "sha": "f31f83b1af725425b0821dec5353002595e05bfe",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -37,7 +37,6 @@ namespace xla::cpu {\n std::unique_ptr<mlir::Pass> CreateLinalgElementwiseToVectorPass();\n std::unique_ptr<mlir::Pass> CreateLowerXTileEntryPass();\n std::unique_ptr<mlir::Pass> CreateShloToVectorPass();\n-std::unique_ptr<mlir::Pass> CreateXTileToVectorPass();\n std::unique_ptr<mlir::Pass> CreateTensorOpsToVectorPass();\n std::unique_ptr<mlir::Pass> CreateMemrefCopyToLoopsPass();\n "
        },
        {
            "sha": "3741957a81ceac3ced7179bf031c22c0084681eb",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -15,17 +15,6 @@ limitations under the License.\n \n include \"mlir/Pass/PassBase.td\"\n \n-def XTileToVectorPass : Pass<\"xtile-cpu-xtile-to-vector\", \"mlir::ModuleOp\"> {\n-  let summary = \"Lowering xtile ops to vector ops\";\n-\n-  let constructor = \"CreateXTileToVectorPass()\";\n-\n-  let dependentDialects = [\n-    \"mlir::vector::VectorDialect\",\n-    \"xla::xtile::XTileDialect\",\n-  ];\n-}\n-\n def LowerXTileEntryPass : Pass<\"xtile-cpu-lower-xtile-entry\", \"mlir::ModuleOp\"> {\n   let summary = \"Lowers the entry function into the form required by the CPU runtime\";\n "
        },
        {
            "sha": "92513bb55b6d4b3beb93c968a0cc752569c9de59",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/xtile_to_vector.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e14921bb0817ec755c6047e35e2377da2de1c1c0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e14921bb0817ec755c6047e35e2377da2de1c1c0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir?ref=e14921bb0817ec755c6047e35e2377da2de1c1c0",
            "patch": "@@ -1,58 +0,0 @@\n-// RUN: fusion_compiler_opt %s --xtile-cpu-xtile-to-vector -cse -split-input-file | FileCheck %s\n-\n-// CHECK-LABEL: @simple_insert_extract\n-// CHECK-SAME: (%[[INPUT:.*]]: memref<1024xf32>, %[[OUTPUT:.*]]: memref<1024xf32>, %[[TILE_ID:.*]]: index)\n-xtile.entry_func @simple_insert_extract(%input: memref<1024xf32>, %output: memref<1024xf32>, %tile_id: index) {\n-  // CHECK-DAG: %[[POISON:.*]] = ub.poison : f32\n-  // CHECK-DAG: %[[C_0:.*]] = arith.constant 0 : index\n-  // CHECK: %[[IN_SUBVIEW:.*]] = memref.subview %[[INPUT]][%[[TILE_ID]]] [1] [1]\n-  // CHECK-SAME: memref<1024xf32> to memref<1xf32, strided<[1], offset: ?>>\n-  // CHECK: %[[MASK:.*]] = vector.create_mask\n-  // CHECK: %[[EXTRACT:.*]] = vector.transfer_read %[[IN_SUBVIEW]][%[[C_0]]], %[[POISON]], %[[MASK]]\n-  %tile = xtile.extract %input[%tile_id][1][1] : memref<1024xf32> -> tensor<1xf32>\n-  // CHECK: %[[OUT_SUBVIEW:.*]] = memref.subview %[[OUTPUT]][%[[TILE_ID]]] [1] [1]\n-  // CHECK-SAME: memref<1024xf32> to memref<1xf32, strided<[1], offset: ?>>\n-  // CHECK: vector.transfer_write %[[EXTRACT]], %[[OUT_SUBVIEW]][%[[C_0]]], %[[MASK]]\n-  xtile.insert %tile into %output[%tile_id][1][1] : tensor<1xf32> -> memref<1024xf32>\n-  xtile.return\n-}\n-\n-// -----\n-\n-// CHECK: @reduce_dimension(%[[INPUT:.*]]: memref<16x1024xf32>, %[[OUTPUT:.*]]: memref<16x1024xf32>, %[[TILE_ID:.*]]: index)\n-xtile.entry_func @reduce_dimension(%input: memref<16x1024xf32>, %output: memref<16x1024xf32>, %tile_id: index) {\n-  // CHECK: %[[C_0:.*]] = arith.constant 0 : index\n-  %offset = arith.constant 0 : index\n-  // CHECK: memref.subview %[[INPUT]][%[[C_0]], %[[TILE_ID]]] [10, 1] [1, 1]\n-  // CHECK-SAME: memref<16x1024xf32> to memref<10xf32, strided<[1024], offset: ?>>\n-  %tile = xtile.extract %input[%offset, %tile_id][10, 1][1, 1] : memref<16x1024xf32> -> tensor<10xf32>\n-  // CHECK: memref.subview %[[OUTPUT]][%[[C_0]], %[[TILE_ID]]] [10, 1] [1, 1]\n-  // CHECK-SAME: memref<16x1024xf32> to memref<10xf32, strided<[1024], offset: ?>>\n-  xtile.insert %tile into %output[%offset, %tile_id][10, 1][1, 1] : tensor<10xf32> -> memref<16x1024xf32>\n-  xtile.return\n-}\n-\n-// -----\n-\n-// CHECK: @extract_strided(%[[SOURCE:.*]]: memref<16xf32>, %[[TILE_ID:.*]]: index)\n-func.func @extract_strided(%source: memref<16xf32>, %tile_id: index) -> tensor<8xf32> {\n-  // CHECK: memref.subview %[[SOURCE]][%[[TILE_ID]]] [8] [2] :\n-  // CHECK-SAME: memref<16xf32> to memref<8xf32, strided<[2], offset: ?>>\n-  %tile = xtile.extract %source[%tile_id][8][2] : memref<16xf32> -> tensor<8xf32>\n-  return %tile : tensor<8xf32>\n-}\n-\n-// -----\n-\n-// CHECK: @insert_strided(\n-// CHECK-SAME: %[[SOURCE:.*]]: tensor<8xf32>,\n-// CHECK-SAME: %[[DESTINATION:.*]]: memref<16xf32>,\n-// CHECK-SAME: %[[TILE_ID:.*]]: index)\n-func.func @insert_strided(%source: tensor<8xf32>, %destination: memref<16xf32>, %tile_id: index) {\n-  // CHECK: memref.subview %[[DESTINATION]][%[[TILE_ID]]] [8] [2] :\n-  // CHECK-SAME: memref<16xf32> to memref<8xf32, strided<[2], offset: ?>>\n-  xtile.insert %source into %destination[%tile_id][8][2] : tensor<8xf32> -> memref<16xf32>\n-  return\n-}\n-\n-"
        },
        {
            "sha": "4c3c89e0471292a62c9f73409155a7b020ba0ea2",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/xtile_to_vector.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 224,
            "changes": 224,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e14921bb0817ec755c6047e35e2377da2de1c1c0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e14921bb0817ec755c6047e35e2377da2de1c1c0/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc?ref=e14921bb0817ec755c6047e35e2377da2de1c1c0",
            "patch": "@@ -1,224 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include <cassert>\n-#include <cstdint>\n-#include <memory>\n-#include <optional>\n-#include <utility>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"llvm/ADT/ArrayRef.h\"\n-#include \"llvm/ADT/DenseSet.h\"\n-#include \"llvm/ADT/STLExtras.h\"\n-#include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/ADT/SmallVectorExtras.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n-#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n-#include \"mlir/Dialect/UB/IR/UBOps.h\"\n-#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n-#include \"mlir/IR/AffineExpr.h\"\n-#include \"mlir/IR/Attributes.h\"\n-#include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinTypes.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"mlir/IR/OpDefinition.h\"\n-#include \"mlir/IR/PatternMatch.h\"\n-#include \"mlir/IR/Value.h\"\n-#include \"mlir/IR/ValueRange.h\"\n-#include \"mlir/IR/Visitors.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Support/LLVM.h\"\n-#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"xla/backends/cpu/codegen/tiled/transforms/lowering_utils.h\"\n-#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n-#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n-\n-namespace xla::cpu {\n-\n-#define GEN_PASS_DECL_XTILETOVECTORPASS\n-#define GEN_PASS_DEF_XTILETOVECTORPASS\n-#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n-\n-namespace {\n-\n-// Dims are dropped in the subview so we use the identity map.\n-mlir::AffineMapAttr GetIdentityMap(xtile::TiledBufferInterface op) {\n-  int64_t rank = op.getTile().getType().getRank();\n-  return mlir::AffineMapAttr::get(\n-      mlir::AffineMap::getMultiDimIdentityMap(rank, op.getContext()));\n-}\n-\n-mlir::TypedValue<mlir::MemRefType> GetSubView(\n-    mlir::ImplicitLocOpBuilder& builder, xtile::TiledBufferInterface op) {\n-  auto get_static_fold_result = [&](llvm::ArrayRef<int64_t> input) {\n-    return llvm::map_to_vector(input, [&builder](int64_t value) {\n-      return mlir::OpFoldResult(builder.getIndexAttr(value));\n-    });\n-  };\n-\n-  auto offsets = llvm::SmallVector<mlir::OpFoldResult>(op.getOffsets());\n-  auto full_tile_shape = get_static_fold_result(op.getFullTileShape());\n-  auto strides = get_static_fold_result(op.getStrides());\n-\n-  mlir::MemRefType subview_type =\n-      mlir::memref::SubViewOp::inferRankReducedResultType(\n-          op.getTile().getType().getShape(), op.getBuffer().getType(), offsets,\n-          full_tile_shape, get_static_fold_result(op.getStrides()));\n-\n-  return builder.create<mlir::memref::SubViewOp>(\n-      subview_type, op.getBuffer(), offsets, full_tile_shape, strides);\n-}\n-\n-llvm::SmallVector<mlir::Value> GetZeroIndexVector(\n-    mlir::ImplicitLocOpBuilder& builder, int64_t rank) {\n-  return llvm::SmallVector<mlir::Value>(\n-      rank, builder.create<mlir::arith::ConstantIndexOp>(0));\n-}\n-\n-mlir::ArrayAttr GetInBoundsAttr(mlir::ImplicitLocOpBuilder& builder,\n-                                int64_t rank) {\n-  // TODO(willfroom): Add proper support for inBounds attr.\n-  llvm::SmallVector<mlir::Attribute> in_bounds(rank,\n-                                               builder.getBoolAttr(false));\n-  return builder.getArrayAttr(in_bounds);\n-}\n-\n-// Get the mask for the given transfer_<read/write> op on a subview of the\n-// original memeref.\n-// The inequality we need to satisfy in 1D is:\n-//  1. offset + subview_idx * stride <= size - 1\n-//  2. subview_idx * stride <= size - 1 - offset\n-//  3. subview_idx <= (size - 1 - offset) / stride\n-//  4. subview_idx < ((size - 1 - offset) / stride) + 1\n-//  5. subview_idx < (size + stride - 1 - offset) / stride\n-mlir::Value GetMask(mlir::ImplicitLocOpBuilder& builder,\n-                    xtile::TiledBufferInterface op) {\n-  mlir::RankedTensorType tile_tensor_type = op.getTile().getType();\n-\n-  auto get_const_index_op = [&](int64_t value) {\n-    return builder.create<mlir::arith::ConstantIndexOp>(value);\n-  };\n-\n-  if (tile_tensor_type.getRank() == 0) {\n-    // Vector transfer read/write currently don't support 0D masks.\n-    auto mask_0D_type = mlir::VectorType::get({1}, builder.getI1Type());\n-    return builder.create<mlir::vector::CreateMaskOp>(\n-        mask_0D_type, mlir::OpFoldResult(builder.getIndexAttr(1)));\n-  }\n-\n-  llvm::SmallDenseSet<unsigned> reduced_dims = op.getReducedDimensions();\n-  llvm::SmallVector<mlir::Value> upper_bounds;\n-  int64_t idx = 0;\n-  for (auto [offset, size, stride] :\n-       llvm::zip(op.getOffsets(), op.getBuffer().getType().getShape(),\n-                 op.getStrides())) {\n-    if (reduced_dims.contains(idx++)) {\n-      continue;\n-    }\n-    upper_bounds.push_back(builder.create<mlir::arith::DivSIOp>(\n-        builder.create<mlir::arith::SubIOp>(\n-            get_const_index_op(size + stride - 1), offset),\n-        get_const_index_op(stride)));\n-  }\n-\n-  auto mask_type = mlir::VectorType::get(op.getTile().getType().getShape(),\n-                                         builder.getI1Type());\n-  return builder.create<mlir::vector::CreateMaskOp>(mask_type, upper_bounds);\n-}\n-\n-struct LowerExtractTile : mlir::OpRewritePattern<xtile::ExtractTileOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      xtile::ExtractTileOp op, mlir::PatternRewriter& rewriter) const override {\n-    mlir::ImplicitLocOpBuilder builder(op->getLoc(), rewriter);\n-    auto vector_type = GetVectorType(op.getResult().getType());\n-\n-    mlir::TypedValue<mlir::MemRefType> buffer_subview = GetSubView(builder, op);\n-\n-    int64_t reduced_rank = vector_type.getRank();\n-\n-    // The subview is already offset so the read has zero offsets.\n-    auto zero_index = GetZeroIndexVector(builder, reduced_rank);\n-    mlir::Value padding =\n-        builder.create<mlir::ub::PoisonOp>(vector_type.getElementType());\n-    mlir::Value mask = GetMask(builder, op);\n-    auto in_bounds = GetInBoundsAttr(builder, reduced_rank);\n-\n-    mlir::Value vector_value = rewriter.create<mlir::vector::TransferReadOp>(\n-        op->getLoc(), vector_type, buffer_subview, zero_index,\n-        GetIdentityMap(op), padding, mask, in_bounds);\n-\n-    rewriter.replaceOp(op, WriteVectorToTensor(builder, vector_value));\n-    return mlir::success();\n-  }\n-};\n-\n-struct LowerInsertTile : mlir::OpRewritePattern<xtile::InsertTileOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      xtile::InsertTileOp op, mlir::PatternRewriter& rewriter) const override {\n-    mlir::ImplicitLocOpBuilder builder(op->getLoc(), rewriter);\n-    mlir::TypedValue<mlir::VectorType> vector_tile =\n-        ReadTensorToVector(builder, op.getSource());\n-\n-    mlir::TypedValue<mlir::MemRefType> buffer_subview = GetSubView(builder, op);\n-\n-    int64_t reduced_rank = vector_tile.getType().getRank();\n-\n-    // The subview is already offset so the write has zero offsets.\n-    auto zero_index = GetZeroIndexVector(builder, reduced_rank);\n-    mlir::Value mask = GetMask(builder, op);\n-    auto in_bounds = GetInBoundsAttr(builder, reduced_rank);\n-\n-    mlir::vector::TransferWriteOp transfer_write =\n-        builder.create<mlir::vector::TransferWriteOp>(\n-            vector_tile, buffer_subview, zero_index, GetIdentityMap(op), mask,\n-            in_bounds);\n-\n-    rewriter.replaceOp(op, transfer_write);\n-    return mlir::success();\n-  }\n-};\n-\n-class XTileToVectorPass\n-    : public impl::XTileToVectorPassBase<XTileToVectorPass> {\n- public:\n-  using XTileToVectorPassBase::XTileToVectorPassBase;\n-\n-  void runOnOperation() override {\n-    mlir::MLIRContext* context = &getContext();\n-    mlir::RewritePatternSet patterns(context);\n-    patterns.add<LowerExtractTile, LowerInsertTile>(context);\n-    if (mlir::failed(\n-            mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n-      signalPassFailure();\n-      return;\n-    }\n-  }\n-};\n-\n-}  // namespace\n-\n-std::unique_ptr<mlir::Pass> CreateXTileToVectorPass() {\n-  return std::make_unique<XTileToVectorPass>();\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "ce17ae13a9e0f81959d595ac55fd837610470029",
            "filename": "third_party/xla/xla/codegen/tools/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -39,6 +39,8 @@ xla_cc_binary(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:AffineDialect\",\n         \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:ArithTransforms\",\n+        \"@llvm-project//mlir:BufferizationTransforms\",\n         \"@llvm-project//mlir:ComplexDialect\",\n         \"@llvm-project//mlir:DLTIDialect\",\n         \"@llvm-project//mlir:FuncDialect\",\n@@ -53,6 +55,7 @@ xla_cc_binary(\n         \"@llvm-project//mlir:SCFDialect\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TensorDialect\",\n+        \"@llvm-project//mlir:TensorTransforms\",\n         \"@llvm-project//mlir:Transforms\",\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@stablehlo//:stablehlo_ops\","
        },
        {
            "sha": "02f7496c0fc4b972490c1efe4832f9276b9e12b8",
            "filename": "third_party/xla/xla/codegen/tools/emitters_opt.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -18,6 +18,9 @@ limitations under the License.\n #include \"llvm/ADT/Twine.h\"\n #include \"mlir/Dialect/Affine/IR/AffineOps.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Arith/Transforms/BufferizableOpInterfaceImpl.h\"\n+#include \"mlir/Dialect/Bufferization/Transforms/FuncBufferizableOpInterfaceImpl.h\"\n+#include \"mlir/Dialect/Bufferization/Transforms/Passes.h\"\n #include \"mlir/Dialect/Complex/IR/Complex.h\"\n #include \"mlir/Dialect/DLTI/DLTI.h\"\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"\n@@ -29,6 +32,7 @@ limitations under the License.\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/Dialect/Tensor/Transforms/BufferizableOpInterfaceImpl.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Pass/PassOptions.h\"\n@@ -72,6 +76,13 @@ int main(int argc, char** argv) {\n   xla::cpu::registerXlaCpuTransformsPasses();\n   xla::cpu::registerXTileCpuTransformsPasses();\n   xla::xtile::registerXTileTransformsPasses();\n+  mlir::bufferization::registerBufferizationPasses();\n+\n+  mlir::arith::registerBufferizableOpInterfaceExternalModels(registry);\n+  mlir::bufferization::func_ext::registerBufferizableOpInterfaceExternalModels(\n+      registry);\n+  mlir::tensor::registerBufferizableOpInterfaceExternalModels(registry);\n+\n   mlir::registerPassPipeline(\n       \"xla-test-optimize\",\n       \"Test pipeline of passes up to inlining. No vectorization, also does not \""
        },
        {
            "sha": "27bbf1064e723bfee6f73e20ef9deabc9254fbcd",
            "filename": "third_party/xla/xla/codegen/xtile/ir/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -21,6 +21,7 @@ td_library(\n     compatible_with = get_compatible_with_portable(),\n     includes = [\".\"],\n     deps = [\n+        \"@llvm-project//mlir:BufferizableOpInterfaceTdFiles\",\n         \"@llvm-project//mlir:BuiltinDialectTdFiles\",\n         \"@llvm-project//mlir:CallInterfacesTdFiles\",\n         \"@llvm-project//mlir:ControlFlowInterfacesTdFiles\",\n@@ -76,6 +77,7 @@ cc_library(\n     name = \"xtile\",\n     srcs = [\n         \"xtile_attrs.cc\",\n+        \"xtile_bufferization.cc\",\n         \"xtile_dialect.cc\",\n         \"xtile_ops.cc\",\n     ],\n@@ -88,11 +90,14 @@ cc_library(\n         \":xtile_attrs_inc_gen\",\n         \":xtile_dialect_inc_gen\",\n         \":xtile_ops_inc_gen\",\n+        \"//xla/codegen/emitters:implicit_arith_op_builder\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:BufferizationDialect\",\n+        \"@llvm-project//mlir:BufferizationInterfaces\",\n         \"@llvm-project//mlir:BytecodeOpInterface\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FunctionInterfaces\","
        },
        {
            "sha": "2042a3c0507f19e3ffed03f09a96de1b3d606b2a",
            "filename": "third_party/xla/xla/codegen/xtile/ir/transforms/tests/bufferize.mlir",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Ftests%2Fbufferize.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Ftests%2Fbufferize.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftransforms%2Ftests%2Fbufferize.mlir?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -0,0 +1,67 @@\n+// RUN: emitters_opt %s -one-shot-bufferize -canonicalize -cse \\\n+// RUN: -split-input-file | FileCheck %s\n+\n+// CHECK: @extract_strided(%[[SOURCE:.*]]: memref<16xf32>, %[[OFFSET:.*]]: index)\n+func.func @extract_strided(%source: memref<16xf32>, %tile_id: index) -> tensor<8xf32> {\n+  // CHECK-DAG: %[[C1:.*]] = arith.constant 1 : index\n+  // CHECK-DAG: %[[C2:.*]] = arith.constant 2 : index\n+  // CHECK-DAG: %[[C8:.*]] = arith.constant 8 : index\n+  // CHECK-DAG: %[[C15:.*]] = arith.constant 15 : index\n+\n+  // CHECK: %[[SHIFT:.*]] = arith.subi %[[C15]], %[[OFFSET]] : index\n+  // CHECK: %[[STRIDED_SHIFT:.*]] = arith.divsi %[[SHIFT]], %[[C2]] : index\n+  // CHECK: %[[ELEMENTS_TO_END:.*]] = arith.addi %[[STRIDED_SHIFT]], %[[C1]] : index\n+  // CHECK: %[[SIZE:.*]] = arith.minsi %[[ELEMENTS_TO_END]], %[[C8]] : index\n+\n+  // CHECK: %[[INPUT_SUBVIEW:.*]] = memref.subview %[[SOURCE]]\n+  // CHECK-SAME: [%[[OFFSET]]] [%[[SIZE]]] [2]\n+  // CHECK-SAME: : memref<16xf32> to memref<?xf32, strided<[2], offset: ?>>\n+\n+  // CHECK: %[[BUFFER:.*]] = memref.alloc() : memref<8xf32>\n+\n+  // CHECK: %[[BUFFER_SUBVIEW:.*]] = memref.subview %[[BUFFER]]\n+  // CHECK-SAME: [0] [%[[SIZE]]] [1] : memref<8xf32> to memref<?xf32, strided<[1]>>\n+\n+  // CHECK: memref.copy %[[INPUT_SUBVIEW]], %[[BUFFER_SUBVIEW]]\n+  // CHECK-SAME: : memref<?xf32, strided<[2], offset: ?>> to memref<?xf32, strided<[1]>>\n+\n+  // CHECK: %[[TILE:.*]] = bufferization.to_tensor %[[BUFFER]] restrict writable\n+  // CHECK-SAME: : memref<8xf32> to tensor<8xf32>\n+  %tile = xtile.extract %source[%tile_id][8][2] : memref<16xf32> -> tensor<8xf32>\n+  // CHECK: return %[[TILE]] : tensor<8xf32>\n+  return %tile : tensor<8xf32>\n+}\n+\n+// -----\n+\n+// CHECK: @insert_strided(\n+// CHECK-SAME: %[[SOURCE:.*]]: tensor<8xf32>,\n+// CHECK-SAME: %[[DESTINATION:.*]]: memref<16xf32>,\n+// CHECK-SAME: %[[OFFSET:.*]]: index)\n+func.func @insert_strided(%source: tensor<8xf32>, %destination: memref<16xf32>, %tile_id: index) {\n+  // CHECK-DAG: %[[C1:.*]] = arith.constant 1 : index\n+  // CHECK-DAG: %[[C2:.*]] = arith.constant 2 : index\n+  // CHECK-DAG: %[[C8:.*]] = arith.constant 8 : index\n+  // CHECK-DAG: %[[C15:.*]] = arith.constant 15 : index\n+\n+  // CHECK: %[[SOURCE_BUFFER:.*]] = bufferization.to_buffer %[[SOURCE]]\n+  // CHECK-SAME: : tensor<8xf32> to memref<8xf32, strided<[?], offset: ?>>\n+\n+  // CHECK: %[[SHIFT:.*]] = arith.subi %[[C15]], %[[OFFSET]] : index\n+  // CHECK: %[[STRIDED_SHIFT:.*]] = arith.divsi %[[SHIFT]], %[[C2]] : index\n+  // CHECK: %[[ELEMENTS_TO_END:.*]] = arith.addi %[[STRIDED_SHIFT]], %[[C1]] : index\n+  // CHECK: %[[SIZE:.*]] = arith.minsi %[[ELEMENTS_TO_END]], %[[C8]] : index\n+\n+  // CHECK: %[[SOURCE_SUBVIEW:.*]] = memref.subview %[[SOURCE_BUFFER]][0] [%[[SIZE]]] [1]\n+  // CHECK-SAME: : memref<8xf32, strided<[?], offset: ?>> to memref<?xf32, strided<[?], offset: ?>>\n+\n+  // CHECK: %[[DESTINATION_SUBVIEW:.*]] = memref.subview %[[DESTINATION]]\n+  // CHECK-SAME: [%[[OFFSET]]] [%[[SIZE]]] [2]\n+  // CHECK-SAME: : memref<16xf32> to memref<?xf32, strided<[2], offset: ?>>\n+\n+  // CHECK: memref.copy %[[SOURCE_SUBVIEW]], %[[DESTINATION_SUBVIEW]]\n+  // CHECK-SAME: : memref<?xf32, strided<[?], offset: ?>>\n+  // CHECK-SAME: to memref<?xf32, strided<[2], offset: ?>>\n+  xtile.insert %source into %destination[%tile_id][8][2] : tensor<8xf32> -> memref<16xf32>\n+  return\n+}"
        },
        {
            "sha": "b2b6ef8ec00ee7c9123e47b0b16ccce170f01040",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_bufferization.cc",
            "status": "added",
            "additions": 254,
            "deletions": 0,
            "changes": 254,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_bufferization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_bufferization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_bufferization.cc?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -0,0 +1,254 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <algorithm>\n+#include <cassert>\n+#include <cstdint>\n+\n+#include \"absl/log/check.h\"\n+#include \"llvm/ADT/DenseSet.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/SmallVectorExtras.h\"\n+#include \"llvm/Support/LogicalResult.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h\"\n+#include \"mlir/Dialect/Bufferization/IR/Bufferization.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+#include \"mlir/IR/OperationSupport.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"xla/codegen/emitters/implicit_arith_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+\n+namespace xla::xtile {\n+\n+static llvm::SmallVector<mlir::OpFoldResult> GetStaticFoldResult(\n+    mlir::OpBuilder& builder, llvm::ArrayRef<int64_t> input) {\n+  return llvm::map_to_vector(input, [&builder](int64_t value) {\n+    return mlir::OpFoldResult(builder.getIndexAttr(value));\n+  });\n+}\n+\n+static llvm::SmallVector<mlir::OpFoldResult> GetDynamicFoldResult(\n+    mlir::ValueRange input) {\n+  return llvm::SmallVector<mlir::OpFoldResult>(input);\n+}\n+\n+// Get the size of the memref subview with the output size clamped to inbound\n+// elements, if full_size is true then unit values are inserted for reduced\n+// dimensions.\n+// The derivation of these bounds is as follows:\n+//   index + tile_size * stride <= size - 1\n+//   tile_size * stride <= size - 1 - index\n+//   tile_size <= size - 1 - index / stride\n+//   tile_size < ((size - 1 - index) / stride) + 1\n+static llvm::SmallVector<mlir::OpFoldResult> GetClampedTileSize(\n+    mlir::ImplicitLocOpBuilder& builder, TiledBufferInterface op,\n+    bool full_size) {\n+  llvm::SmallVector<mlir::OpFoldResult> tile_size;\n+  llvm::SmallDenseSet<unsigned> reduced_dims = op.getReducedDimensions();\n+  int64_t idx = 0;\n+  for (auto [buffer_size, offset, stride, full_tile_size] :\n+       llvm::zip(op.getBuffer().getType().getShape(), op.getOffsets(),\n+                 op.getStrides(), op.getFullTileShape())) {\n+    if (reduced_dims.contains(idx++)) {\n+      if (full_size) {\n+        tile_size.emplace_back(builder.getIndexAttr(1));\n+      }\n+      continue;\n+    }\n+    emitters::ImplicitArithOpBuilder arith_builder(\n+        mlir::arith::ConstantIndexOp::create(builder, (buffer_size - 1)),\n+        &builder);\n+    auto numerator = arith_builder - offset;\n+    // The stride can be 0 for single element tiles.\n+    // TODO(willfroom): Fix tile analysis so this never happens.\n+    auto clamped_stride = std::max<int64_t>(stride, 1);\n+    auto bound = numerator / clamped_stride + 1;\n+    tile_size.emplace_back(bound.min(full_tile_size));\n+  }\n+\n+  return tile_size;\n+}\n+\n+// Get the subview of the op buffer with its size clamped such that all elements\n+// are in bounds.\n+static mlir::TypedValue<mlir::MemRefType> GetClampedSubView(\n+    mlir::ImplicitLocOpBuilder& builder, TiledBufferInterface op) {\n+  auto tile_size = GetClampedTileSize(builder, op, true);\n+\n+  auto offsets = GetDynamicFoldResult(op.getOffsets());\n+  auto strides = GetStaticFoldResult(builder, op.getStrides());\n+\n+  mlir::RankedTensorType tile_type = op.getTile().getType();\n+  llvm::SmallVector<int64_t> output_shape(tile_type.getRank(),\n+                                          mlir::ShapedType::kDynamic);\n+  mlir::MemRefType subview_type =\n+      mlir::memref::SubViewOp::inferRankReducedResultType(\n+          output_shape, op.getBuffer().getType(), offsets, tile_size, strides);\n+\n+  return mlir::memref::SubViewOp::create(builder, subview_type, op.getBuffer(),\n+                                         offsets, tile_size, strides);\n+}\n+\n+// Get the subview of the local buffer - i.e it has 0 offsets & unit strides.\n+static mlir::TypedValue<mlir::MemRefType> GetLocalBufferSubview(\n+    mlir::ImplicitLocOpBuilder& builder,\n+    mlir::TypedValue<mlir::MemRefType> buffer,\n+    llvm::ArrayRef<mlir::OpFoldResult> tile_size,\n+    llvm::ArrayRef<int64_t> full_tile_shape) {\n+  mlir::SmallVector<mlir::OpFoldResult> buffer_offsets(\n+      buffer.getType().getRank(), builder.getIndexAttr(0));\n+  mlir::SmallVector<mlir::OpFoldResult> buffer_strides(\n+      buffer.getType().getRank(), builder.getIndexAttr(1));\n+\n+  mlir::MemRefType buffer_subview_type =\n+      mlir::memref::SubViewOp::inferRankReducedResultType(\n+          full_tile_shape, buffer.getType(), buffer_offsets, tile_size,\n+          buffer_strides);\n+  return mlir::memref::SubViewOp::create(builder, buffer_subview_type, buffer,\n+                                         buffer_offsets, tile_size,\n+                                         buffer_strides);\n+}\n+\n+// Extract the slice of the tensor that is clamped to be within bounds of the\n+// target buffer.\n+static mlir::TypedValue<mlir::RankedTensorType> GetTensorSlice(\n+    mlir::ImplicitLocOpBuilder& builder, InsertTileOp op) {\n+  auto tile_size = GetClampedTileSize(builder, op, false);\n+\n+  mlir::SmallVector<mlir::OpFoldResult> offsets(tile_size.size(),\n+                                                builder.getIndexAttr(0));\n+  mlir::SmallVector<mlir::OpFoldResult> strides(tile_size.size(),\n+                                                builder.getIndexAttr(1));\n+\n+  return mlir::tensor::ExtractSliceOp::create(builder, op.getSource(), offsets,\n+                                              tile_size, strides);\n+}\n+\n+// Get a buffer copied from the original buffer that is padded to the full tile\n+// size.\n+static mlir::TypedValue<mlir::MemRefType> GetPaddedTileBuffer(\n+    mlir::ImplicitLocOpBuilder& builder, ExtractTileOp op) {\n+  auto buffer_tile_subview = GetClampedSubView(builder, op);\n+  mlir::RankedTensorType tile_type = op.getResult().getType();\n+  auto buffer = mlir::memref::AllocOp::create(\n+      builder, GetStaticFoldResult(builder, tile_type.getShape()),\n+      tile_type.getElementType());\n+\n+  auto local_tile_size = GetClampedTileSize(builder, op, false);\n+  auto local_buffer_subview =\n+      GetLocalBufferSubview(builder, buffer, local_tile_size,\n+                            buffer_tile_subview.getType().getShape());\n+\n+  mlir::memref::CopyOp::create(builder, buffer_tile_subview,\n+                               local_buffer_subview);\n+\n+  return buffer;\n+}\n+\n+bool ExtractTileOp::bufferizesToMemoryRead(\n+    mlir::OpOperand& operand, const mlir::bufferization::AnalysisState& state) {\n+  return true;\n+}\n+\n+bool ExtractTileOp::bufferizesToMemoryWrite(\n+    mlir::OpOperand& operand, const mlir::bufferization::AnalysisState& state) {\n+  return true;\n+}\n+\n+mlir::bufferization::AliasingValueList ExtractTileOp::getAliasingValues(\n+    mlir::OpOperand& operand, const mlir::bufferization::AnalysisState& state) {\n+  return {};\n+}\n+\n+bool ExtractTileOp::isWritable(\n+    mlir::Value value, const mlir::bufferization::AnalysisState& state) {\n+  // We currently unconditionally create a new buffer to store the extracted\n+  // tile so it is always writable.\n+  if (value == getResult()) {\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+llvm::LogicalResult ExtractTileOp::bufferize(\n+    mlir::RewriterBase& rewriter,\n+    const mlir::bufferization::BufferizationOptions& options,\n+    mlir::bufferization::BufferizationState& state) {\n+  mlir::ImplicitLocOpBuilder builder(getLoc(), rewriter);\n+  auto buffer = GetPaddedTileBuffer(builder, *this);\n+  auto to_tensor_op =\n+      rewriter.replaceOpWithNewOp<mlir::bufferization::ToTensorOp>(\n+          this->getOperation(), getType(), buffer);\n+  to_tensor_op.setWritable(true);\n+  to_tensor_op.setRestrict(true);\n+  return mlir::success();\n+}\n+\n+bool InsertTileOp::bufferizesToMemoryRead(\n+    mlir::OpOperand& operand, const mlir::bufferization::AnalysisState& state) {\n+  return true;\n+}\n+\n+bool InsertTileOp::bufferizesToMemoryWrite(\n+    mlir::OpOperand& operand, const mlir::bufferization::AnalysisState& state) {\n+  DCHECK_EQ(operand.getOperandNumber(), 0)\n+      << \"This should only be called on the tensor operand.\";\n+  return false;\n+}\n+\n+mlir::bufferization::AliasingValueList InsertTileOp::getAliasingValues(\n+    mlir::OpOperand& operand, const mlir::bufferization::AnalysisState& state) {\n+  return {};\n+}\n+\n+bool InsertTileOp::isWritable(mlir::Value value,\n+                              const mlir::bufferization::AnalysisState& state) {\n+  if (value == getDestination()) {\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+llvm::LogicalResult InsertTileOp::bufferize(\n+    mlir::RewriterBase& rewriter,\n+    const mlir::bufferization::BufferizationOptions& options,\n+    mlir::bufferization::BufferizationState& state) {\n+  mlir::ImplicitLocOpBuilder builder(getLoc(), rewriter);\n+\n+  auto tile_slice = GetTensorSlice(builder, *this);\n+  auto target_buffer_subview = GetClampedSubView(builder, *this);\n+  auto materialize_op = mlir::bufferization::MaterializeInDestinationOp::create(\n+      builder, tile_slice, target_buffer_subview);\n+  materialize_op.setWritable(true);\n+\n+  rewriter.eraseOp(this->getOperation());\n+  return mlir::success();\n+}\n+\n+}  // namespace xla::xtile"
        },
        {
            "sha": "ffa99e02dcb902935b9fd2b51694a9183cd552d0",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_ops.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.h?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"llvm/ADT/DenseSet.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Bytecode/BytecodeOpInterface.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h\"  // IWYU pragma: keep\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // IWYU pragma: keep\n #include \"mlir/IR/Attributes.h\"  // IWYU pragma: keep\n #include \"mlir/IR/BuiltinTypes.h\"  // IWYU pragma: keep"
        },
        {
            "sha": "fa240c77cc938cadcc4ccf24856af96c2eec6ac7",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_ops.td",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0bf42afc0ae720e2c1b00fbcc5d442b3292b605f/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td?ref=0bf42afc0ae720e2c1b00fbcc5d442b3292b605f",
            "patch": "@@ -15,7 +15,7 @@ limitations under the License.\n \n #ifndef XLA_CODEGEN_XTILE_IR_XTILE_OPS\n #define XLA_CODEGEN_XTILE_IR_XTILE_OPS\n-\n+include \"mlir/Dialect/Bufferization/IR/BufferizableOpInterface.td\"\n include \"mlir/Interfaces/CallInterfaces.td\"\n include \"mlir/Interfaces/ControlFlowInterfaces.td\"\n include \"mlir/Interfaces/FunctionInterfaces.td\"\n@@ -33,7 +33,14 @@ class XTile_Op<string mnemonic, list<Trait> traits = []> :\n       Op<XTileDialect, mnemonic, traits> {\n }\n \n-def TiledBufferInterface : OpInterface<\"TiledBufferInterface\"> {\n+def TiledBufferInterface : OpInterface<\"TiledBufferInterface\",\n+    [DeclareOpInterfaceMethods<BufferizableOpInterface,\n+      [\"bufferizesToMemoryRead\",\n+       \"bufferizesToMemoryWrite\",\n+       \"getAliasingValues\",\n+       \"isWritable\",\n+       \"bufferize\"]>\n+    ]> {\n   let description = [{\n \n   }];"
        }
    ],
    "stats": {
        "total": 648,
        "additions": 350,
        "deletions": 298
    }
}