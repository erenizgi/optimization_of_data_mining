{
    "author": "beckerhe",
    "message": "Store ThunkProto in GpuExecutable.\n\nThis change modifies GpuExecutable to generate and store the ThunkProto during creation, before running thunk passes. The stored proto is then used when serializing the GpuExecutable to a proto, instead of generating it on demand after thunk passes ran.\n\nThis is a temporary measure to make debug dumping of GPU executables possible. Long term we want to split GpuExecutable into 2 entities - one that is being produced by the compiler and doesn't depend on runtime facilities, and a second one which gets generated from the first one and has all the execution code. But this is unfortunately a bigger refactoring, therefore we need a quicker way.\n\nPiperOrigin-RevId: 841662771",
    "sha": "325a5b2649bcdf4522a1a58ea354268399f14488",
    "files": [
        {
            "sha": "df3767982ce5f96081ceae123bfc123b75b37a55",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/325a5b2649bcdf4522a1a58ea354268399f14488/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/325a5b2649bcdf4522a1a58ea354268399f14488/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=325a5b2649bcdf4522a1a58ea354268399f14488",
            "patch": "@@ -88,6 +88,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_address_allocator.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/event_based_timer.h\"\n+#include \"xla/stream_executor/kernel_stats.h\"\n #include \"xla/stream_executor/module_spec.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/rocm/rocm_platform_id.h\"\n@@ -238,6 +239,10 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(\n \n   GpuExecutableThunkPassBufferAllocator allocator(next_idx);\n \n+  // TODO(b/461380690): Remove this once we have a better way to distinguish\n+  // between compiler-generated and runtime-loaded GPU executables.\n+  absl::StatusOr<ThunkProto> thunk_proto = params.executable->ToProto();\n+\n   TF_RETURN_IF_ERROR(RunThunkPasses(\n       params.debug_options, params.device_description, params.executable.get(),\n       params.debug_module.get(), allocator));\n@@ -251,7 +256,7 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(\n       std::move(allocator.MutableAllocations()), std::move(params.alias_info),\n       std::move(params.debug_options), std::move(params.constants),\n       std::move(params.output_info), params.enable_debug_info_manager,\n-      std::move(params.module_stats)));\n+      std::move(params.module_stats), std::move(thunk_proto)));\n }\n \n // Implementation note: HLO profiling is always enabled for GPU executables,\n@@ -268,7 +273,8 @@ GpuExecutable::GpuExecutable(\n     std::unique_ptr<GpuAliasInfo> alias_info, DebugOptions debug_options,\n     std::vector<ConstantInfo> constants,\n     absl::flat_hash_map<ShapeIndex, OutputInfo> output_info,\n-    bool enable_debug_info_manager, ModuleStats module_stats)\n+    bool enable_debug_info_manager, ModuleStats module_stats,\n+    absl::StatusOr<ThunkProto> thunk_proto)\n     : Executable(std::move(debug_module)),\n       text_(std::move(asm_text)),\n       binary_(std::move(binary)),\n@@ -288,7 +294,8 @@ GpuExecutable::GpuExecutable(\n           debug_options.xla_debug_buffer_assignment_show_max()),\n       constants_(std::move(constants)),\n       output_info_(std::move(output_info)),\n-      enable_debug_info_manager_(enable_debug_info_manager) {\n+      enable_debug_info_manager_(enable_debug_info_manager),\n+      thunk_proto_(std::move(thunk_proto)) {\n   if (gpu_version_.IsRocm()) {\n     // ROCm uses hsaco hashes to distinguish between modules.\n     // Bad things happen if multiple modules with identical code are loaded.\n@@ -1230,7 +1237,10 @@ absl::StatusOr<GpuExecutableProto> GpuExecutable::ToProto() const {\n \n   *proto.mutable_gpu_compute_capability() = gpu_version_.ToProto();\n \n-  TF_ASSIGN_OR_RETURN(*proto.mutable_thunk(), thunks_->ToProto());\n+  // TODO(b/461380690): Generate the proto on-the-fly once we have a better way\n+  // to distinguish between compiler-generated and runtime-loaded GPU\n+  // executables.\n+  TF_ASSIGN_OR_RETURN(*proto.mutable_thunk(), thunk_proto_);\n \n   proto.set_module_name(module_name_);\n   *proto.mutable_program_shape() = program_shape_.ToProto();"
        },
        {
            "sha": "867dbf2275fb4c41d9a7f58ed7493ec7caca2084",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/325a5b2649bcdf4522a1a58ea354268399f14488/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/325a5b2649bcdf4522a1a58ea354268399f14488/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=325a5b2649bcdf4522a1a58ea354268399f14488",
            "patch": "@@ -247,7 +247,8 @@ class GpuExecutable : public Executable {\n       std::unique_ptr<GpuAliasInfo> alias_info, DebugOptions debug_options,\n       std::vector<ConstantInfo> constants,\n       absl::flat_hash_map<ShapeIndex, OutputInfo> output_info,\n-      bool enable_debug_info_manager, ModuleStats module_stats);\n+      bool enable_debug_info_manager, ModuleStats module_stats,\n+      absl::StatusOr<ThunkProto> thunk_proto);\n \n   // GpuExecutable check with either AMD's ISA version, or Nvidia's major minor\n   // version for compute capability, depending on the hardware.\n@@ -369,6 +370,10 @@ class GpuExecutable : public Executable {\n \n   GpuExecutable(const GpuExecutable&) = delete;\n   GpuExecutable& operator=(const GpuExecutable&) = delete;\n+\n+  // Stores the thunk graph as a proto from before running the thunk pass.\n+  // Might contain an error if the given thunk graph is not serializable.\n+  absl::StatusOr<ThunkProto> thunk_proto_;\n };\n \n absl::StatusOr<absl::flat_hash_map<ShapeIndex, GpuExecutable::OutputInfo>>"
        },
        {
            "sha": "33483843b616cc8390f4286c7e82d55e803feab6",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/325a5b2649bcdf4522a1a58ea354268399f14488/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/325a5b2649bcdf4522a1a58ea354268399f14488/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=325a5b2649bcdf4522a1a58ea354268399f14488",
            "patch": "@@ -649,5 +649,77 @@ TEST(GpuExecutableTest, FromProtoWithSymbolResolver) {\n   EXPECT_EQ(symbol_resolver_invocations, 1);\n }\n \n+TEST(GpuExecutableTest, ToProtoReturnsUnchangedThunkGraph) {\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_graph_min_graph_size(1);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n+\n+  auto create_executable = [&]() {\n+    ThunkSequence thunk_sequence;\n+    thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+        ThunkInfoWithId(1),\n+        /*kernel_name=*/\"test_kernel_0\",\n+        /*kernel_arguments=*/emitters::KernelArguments({}),\n+        /*launch_dimensions=*/LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0,\n+        /*tma_metadata=*/se::gpu::TmaMetadata()));\n+    thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+        ThunkInfoWithId(2),\n+        /*kernel_name=*/\"test_kernel_1\",\n+        /*kernel_arguments=*/emitters::KernelArguments({}),\n+        /*launch_dimensions=*/LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0,\n+        /*tma_metadata=*/se::gpu::TmaMetadata()));\n+    thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+        ThunkInfoWithId(3),\n+        /*kernel_name=*/\"test_kernel_2\",\n+        /*kernel_arguments=*/emitters::KernelArguments({}),\n+        /*launch_dimensions=*/LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0,\n+        /*tma_metadata=*/se::gpu::TmaMetadata()));\n+    thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+        ThunkInfoWithId(4),\n+        /*kernel_name=*/\"test_kernel_3\",\n+        /*kernel_arguments=*/emitters::KernelArguments({}),\n+        /*launch_dimensions=*/LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0,\n+        /*tma_metadata=*/se::gpu::TmaMetadata()));\n+    thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+        ThunkInfoWithId(5),\n+        /*kernel_name=*/\"test_kernel_4\",\n+        /*kernel_arguments=*/emitters::KernelArguments({}),\n+        /*launch_dimensions=*/LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0,\n+        /*tma_metadata=*/se::gpu::TmaMetadata()));\n+\n+    GpuExecutable::Params params;\n+    params.executable = std::make_unique<SequentialThunk>(\n+        ThunkInfoWithId(20), std::move(thunk_sequence));\n+    params.debug_options = debug_options;\n+\n+    params.module_name = \"test_module\";\n+    return GpuExecutable::Create(std::move(params));\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable,\n+                          create_executable());\n+\n+  // We expect our 5 kernel launches got wrapped in a command buffer thunk.\n+  // If this assertion fails, you might need to either adjust the thunk graph or\n+  // the debug options such that we do some kind of thunk graph transformation\n+  // that we can test for.\n+  ASSERT_THAT(executable->GetThunk().thunks(), SizeIs(1));\n+\n+  // The proto should be a straight dump of the thunk graph, without any\n+  // transformation.\n+  TF_ASSERT_OK_AND_ASSIGN(GpuExecutableProto proto, executable->ToProto());\n+  ASSERT_TRUE(proto.thunk().has_sequential_thunk());\n+  EXPECT_THAT(proto.thunk().sequential_thunk().thunks(), SizeIs(5));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 97,
        "additions": 92,
        "deletions": 5
    }
}