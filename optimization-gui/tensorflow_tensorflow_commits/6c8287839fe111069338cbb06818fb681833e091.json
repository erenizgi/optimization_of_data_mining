{
    "author": "ZixuanJiang",
    "message": "Extend more cases in `PatternMatchMergeOrSplitSharding`.\n\nBefore this change, we only support the pattern `[..., X*Y, ..., Z, ...] -> [..., X, ..., Y, ...]`. This changes supports `[..., X*Y, ...] -> [..., X, ...]`, where Y can be in any other dimensions in the target sharding.\n\nThis cl can help match the complicated all-to-all pattern.\n\nPiperOrigin-RevId: 800091685",
    "sha": "6c8287839fe111069338cbb06818fb681833e091",
    "files": [
        {
            "sha": "1ed1634fb44531b2cb84663571a1b35a6585d4e9",
            "filename": "third_party/xla/xla/service/spmd/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c8287839fe111069338cbb06818fb681833e091/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c8287839fe111069338cbb06818fb681833e091/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD?ref=6c8287839fe111069338cbb06818fb681833e091",
            "patch": "@@ -108,7 +108,6 @@ xla_cc_test(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n-        \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/hlo/utils:hlo_sharding_util\","
        },
        {
            "sha": "246275573abff5736b0c969f10a85c7ca46ee722",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 17,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c8287839fe111069338cbb06818fb681833e091/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c8287839fe111069338cbb06818fb681833e091/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=6c8287839fe111069338cbb06818fb681833e091",
            "patch": "@@ -2031,11 +2031,10 @@ std::tuple<HloSharding, HloSharding, int64_t> CreateSplitShardingTuple(\n   return std::make_tuple(std::move(split_source), std::move(split_target), dim);\n }\n \n-// Matching the following patterns, where X and Y cannot be 1, Z can be 1.\n-// 1. [..,X,..,Y,..] -> [..,X*Y,..,1,..]\n-// 2. [..,Y,..,X,..] -> [..,1,..,X*Y,..]\n-// 3. [..,X*Y,..,Z,..] -> [..,X,..,Y,..]\n-// 4. [..,Z,..,X*Y,..] -> [..,Y,..,X,..]\n+// Matching the following patterns, where X and Y cannot be 1.\n+// 1. [..,X,..,Y,..] <-> [..,X*Y,..,1,..]\n+// 2. [..,Y,..,X,..] <-> [..,1,..,X*Y,..]\n+// 3. [..,X*Y,..] -> [..,X,..], Y can be in any other dimension in the result.\n // Output tuple:\n // - HloSharding: Split source sharding with the new dimension added.\n // - HloSharding: Split target sharding with the new dimension added.\n@@ -2095,18 +2094,9 @@ PatternMatchMergeOrSplitSharding(const Shape& base_shape,\n     }\n   }\n \n-  // Iterate combination of diff_index_2 and diff_index_2.\n-  for (auto it_i = diff_index_2.begin(); it_i != diff_index_2.end(); ++it_i) {\n-    for (auto it_j = std::next(it_i); it_j != diff_index_2.end(); ++it_j) {\n-      int64_t i = *it_i;\n-      int64_t j = *it_j;\n-      if (source.tile_assignment().dim(i) < target.tile_assignment().dim(i)) {\n-        std::swap(i, j);\n-      }\n-      if (source.tile_assignment().dim(i) !=\n-          target.tile_assignment().dim(i) * target.tile_assignment().dim(j)) {\n-        continue;\n-      }\n+  // Iterate each index in diff_index_2.\n+  for (int64_t i : diff_index_2) {\n+    if (source.tile_assignment().dim(i) > target.tile_assignment().dim(i)) {\n       return CreateSplitShardingTuple(source, target, i,\n                                       target.tile_assignment().dim(i));\n     }"
        },
        {
            "sha": "bd3177e3efa95bc7ff800657b3e798f222a13d8a",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 6,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c8287839fe111069338cbb06818fb681833e091/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c8287839fe111069338cbb06818fb681833e091/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=6c8287839fe111069338cbb06818fb681833e091",
            "patch": "@@ -15266,7 +15266,7 @@ ENTRY %main.21 {\n   XLA_VLOG_LINES(1, module->ToString());\n   auto* gather = FindInstruction(module.get(), HloOpcode::kGather);\n   EXPECT_NE(gather, nullptr);\n-  EXPECT_THAT(gather, op::Shape(\"bf16[2048,64]\"));\n+  EXPECT_THAT(gather, op::Shape(\"bf16[4096,32]\"));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterCostModelForUnmatchedSharding) {\n@@ -15419,6 +15419,29 @@ ENTRY main {\n   EXPECT_EQ(NumOfInstructions(entry, HloOpcode::kCollectivePermute), 0);\n }\n \n+TEST_P(SpmdPartitioningTest, ReshardToASingleAllToAll) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  %param0 = f32[8,32] parameter(0), sharding={devices=[8,4]<=[2,4,4]T(0,2,1)}\n+  ROOT %copy = f32[8,32] copy(%param0), sharding={devices=[2,16]<=[32]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/32));\n+  auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[1,8]\"));\n+  auto reshape0 = AllOf(op::Reshape(op::Reshape(op::Reshape(param0))),\n+                        op::Shape(\"f32[1,1,1,4,2]\"));\n+  auto all_to_all = AllOf(op::AllToAll(reshape0), op::Shape(\"f32[1,1,1,4,2]\"));\n+  auto transpose =\n+      AllOf(op::Transpose(all_to_all), op::Shape(\"f32[1,4,1,1,2]\"));\n+  auto reshape1 = AllOf(op::Reshape(op::Reshape(op::Reshape(transpose))),\n+                        op::Shape(\"f32[4,2]\"));\n+  auto copy = AllOf(op::Copy(reshape1), op::Shape(\"f32[4,2]\"));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(), copy);\n+}\n+\n TEST_P(SpmdPartitioningTest, ReshardCrash) {\n   const char* const hlo_string = R\"(\n HloModule Test\n@@ -16205,16 +16228,21 @@ ENTRY entry {\n   // TODO(b/353990256). Involuntary full rematerialization between shardings\n   // {devices=[2,2,2]<=[8]} to {devices=[8,1,1]<=[8]}.\n   auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[16,16,16]\"));\n-  auto param0_replicated = op::AllReduce(op::AllReduce(\n-      op::AllReduce(op::DynamicUpdateSlice(op::Broadcast(), param0, _, _, _))));\n+  auto param0_replicated =\n+      AllOf(op::AllReduce(op::AllReduce(op::AllReduce(\n+                op::DynamicUpdateSlice(op::Broadcast(), param0, _, _, _)))),\n+            op::Shape(\"f32[32,32,32]\"));\n   auto param0_reshard = AllOf(op::Shape(\"f32[4,32,32]\"),\n                               op::DynamicSlice(param0_replicated, _, _, _));\n   auto cholesky =\n       AllOf(op::Cholesky(param0_reshard), op::Shape(\"f32[4,32,32]\"));\n-  auto cholesky_replicated =\n-      op::AllReduce(op::DynamicUpdateSlice(op::Broadcast(), cholesky, _, _, _));\n+  auto cholesky_partially_replicated =\n+      AllOf(op::AllReduce(op::DynamicUpdateSlice(\n+                op::Broadcast(), op::Copy(op::Reshape(cholesky)), _, _, _, _)),\n+            op::Shape(\"f32[1,16,32,32]\"));\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              AllOf(op::DynamicSlice(cholesky_replicated, _, _, _),\n+              AllOf(op::Reshape(op::DynamicSlice(cholesky_partially_replicated,\n+                                                 _, _, _, _)),\n                     op::Shape(\"f32[16,16,16]\")));\n }\n "
        }
    ],
    "stats": {
        "total": 65,
        "additions": 41,
        "deletions": 24
    }
}