{
    "author": "vwbaker",
    "message": "Set optimize_scratch_bytes to false in Fusion Autotuner Pass\n\nNeither the BlockLevelEmitter or NativeEmitter uses scratch bytes, so this should be set to false. Setting it to true can cause close configs (within 4us) to be overridden by slower configs.\n\nPiperOrigin-RevId: 815741688",
    "sha": "96e13c7e57be379dc8905d94ce255bceb874aa14",
    "files": [
        {
            "sha": "640b7ab7cbb811f3b40005482ac89ab2c4598802",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96e13c7e57be379dc8905d94ce255bceb874aa14/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96e13c7e57be379dc8905d94ce255bceb874aa14/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=96e13c7e57be379dc8905d94ce255bceb874aa14",
            "patch": "@@ -48,7 +48,8 @@ namespace gpu {\n namespace {\n \n AutotuneConfig GetAutotuneConfig(const DebugOptions& debug_options,\n-                                 bool is_deviceless) {\n+                                 bool is_deviceless,\n+                                 bool optimize_scratch_bytes) {\n   AutotuneConfig autotune_config;\n   autotune_config.check_buffers = debug_options.xla_gpu_autotune_level() >= 4;\n   autotune_config.relative_tolerance =\n@@ -66,6 +67,7 @@ AutotuneConfig GetAutotuneConfig(const DebugOptions& debug_options,\n     // If we are running on a deviceless target, we want to use default configs.\n     autotune_config.use_default_config = true;\n   }\n+  autotune_config.optimize_scratch_bytes = optimize_scratch_bytes;\n \n   autotune_config.expect_all_instructions_in_cache =\n       debug_options.xla_gpu_require_complete_aot_autotune_results();\n@@ -88,11 +90,11 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     stream_executor::StreamExecutor* stream_executor,\n     tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n     const Compiler::TargetConfig* target_config,\n-    se::DeviceMemoryAllocator* allocator) {\n+    se::DeviceMemoryAllocator* allocator, bool optimize_scratch_bytes) {\n   std::unique_ptr<Profiler> profiler = nullptr;\n   bool is_deviceless = stream_executor == nullptr;\n   AutotuneConfig autotune_config =\n-      GetAutotuneConfig(debug_options, is_deviceless);\n+      GetAutotuneConfig(debug_options, is_deviceless, optimize_scratch_bytes);\n \n   if (!is_deviceless) {\n     profiler = GpuProfiler::Create(stream_executor,"
        },
        {
            "sha": "372a5a9b334fbf624551461b9cf9757d7e4adcda",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96e13c7e57be379dc8905d94ce255bceb874aa14/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96e13c7e57be379dc8905d94ce255bceb874aa14/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=96e13c7e57be379dc8905d94ce255bceb874aa14",
            "patch": "@@ -44,7 +44,8 @@ class AutotunerPass : public HloModulePass {\n       const DebugOptions& debug_options, se::StreamExecutor* stream_executor,\n       tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n       const Compiler::TargetConfig* target_config,\n-      se::DeviceMemoryAllocator* allocator = nullptr);\n+      se::DeviceMemoryAllocator* allocator = nullptr,\n+      bool optimize_scratch_bytes = true);\n \n   absl::string_view name() const override { return \"autotuner\"; }\n "
        },
        {
            "sha": "d1810723ee5940360ed4d355d4b7d0abf8450950",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96e13c7e57be379dc8905d94ce255bceb874aa14/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96e13c7e57be379dc8905d94ce255bceb874aa14/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=96e13c7e57be379dc8905d94ce255bceb874aa14",
            "patch": "@@ -432,7 +432,8 @@ absl::Status NVPTXCompiler::AddFusionAutotuningPass(\n       std::unique_ptr<AutotunerPass> autotuner_pass,\n       AutotunerPass::Create(std::move(backends), debug_options, stream_executor,\n                             thread_pool, ShouldAutotuneBetweenFusionEmitters,\n-                            target_config, options.device_allocator));\n+                            target_config, options.device_allocator,\n+                            /*optimize_scratch_bytes=*/false));\n   pipeline->AddPass(std::move(autotuner_pass));\n   return absl::OkStatus();\n }"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 9,
        "deletions": 5
    }
}