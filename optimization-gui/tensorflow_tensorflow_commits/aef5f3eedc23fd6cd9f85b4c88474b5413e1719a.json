{
    "author": "ZixuanJiang",
    "message": "Refactor `HloSharding::ValidateNonTuple`.\n\nProcess maximal shardings explicitly and then handle non-maximal shardings.\n\nPiperOrigin-RevId: 830627295",
    "sha": "aef5f3eedc23fd6cd9f85b4c88474b5413e1719a",
    "files": [
        {
            "sha": "48e432c644a53ec8b8eb01019847909f9eb15c45",
            "filename": "third_party/xla/xla/hlo/ir/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD?ref=aef5f3eedc23fd6cd9f85b4c88474b5413e1719a",
            "patch": "@@ -236,7 +236,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n "
        },
        {
            "sha": "dcf751cc411130691e191e9ce41c21103950145b",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 49,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc?ref=aef5f3eedc23fd6cd9f85b4c88474b5413e1719a",
            "patch": "@@ -51,7 +51,6 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/protobuf.h\"\n \n namespace xla {\n namespace {\n@@ -305,7 +304,9 @@ HloSharding HloSharding::Subgroup(\n                   kOrderedTypes[6] == 5);\n     for (OpSharding::Type type : kOrderedTypes) {\n       auto& dims = type_to_dims[type];\n-      if (dims.empty()) continue;\n+      if (dims.empty()) {\n+        continue;\n+      }\n       int64_t dim_size = 1;\n       for (int64_t dim : dims) {\n         perm.push_back(dim);\n@@ -725,9 +726,8 @@ absl::StatusOr<ShapeTree<HloSharding>> HloSharding::AsShapeTree(\n       index_to_sharding.second = *it++;\n     }\n     return result;\n-  } else {\n-    return ShapeTree<HloSharding>(shape, *this);\n   }\n+  return ShapeTree<HloSharding>(shape, *this);\n }\n \n absl::StatusOr<HloSharding> HloSharding::GetTupleSharding(\n@@ -776,7 +776,7 @@ int64_t HloSharding::GetUniqueDevice() const {\n absl::Status HloSharding::ValidateTuple(\n     const Shape& shape, std::optional<int64_t> num_devices) const {\n   if (!shape.IsTuple()) {\n-    return tsl::errors::InvalidArgument(\n+    return absl::InvalidArgumentError(\n         \"Sharding is tuple-shaped but validation shape is not.\");\n   }\n   TF_RETURN_IF_ERROR(CheckLeafCount(shape));\n@@ -817,69 +817,84 @@ absl::Status HloSharding::Validate(const Shape& shape,\n   return status;\n }\n \n+namespace {\n+absl::Status DeviceInRange(int64_t device, std::optional<int64_t> num_devices) {\n+  if (device < 0) {\n+    return absl::InvalidArgumentError(\n+        absl::StrFormat(\"device %d is negative in tile assignment\", device));\n+  }\n+  if (num_devices.has_value() && device >= *num_devices) {\n+    return absl::InvalidArgumentError(\n+        absl::StrFormat(\"device %d >= num_devices (%d) in tile assignment\",\n+                        device, *num_devices));\n+  }\n+  return absl::OkStatus();\n+}\n+}  // namespace\n+\n absl::Status HloSharding::ValidateNonTuple(\n     const Shape& shape, std::optional<int64_t> num_devices) const {\n   if (shape.IsTuple()) {\n     return absl::InvalidArgumentError(\n         \"Validation shape is a tuple but sharding is not.\");\n   }\n-  if (replicated_) {\n+  if (replicated_ || manual_ || unreduced_ || unknown_) {\n     return absl::OkStatus();\n   }\n \n-  // All tile assignments must be less than the number of available devices and\n-  // unique.\n-  bool all_devices_seen;\n-  if (!tile_assignment_.iota_) {\n-    absl::flat_hash_set<int64_t> seen_devices;\n-    absl::Status status = tile_assignment_.array().EachStatus(\n-        [&num_devices, &seen_devices](absl::Span<const int64_t> indices,\n-                                      int32_t device) {\n-          if (num_devices.has_value() && device >= *num_devices) {\n-            return absl::InvalidArgumentError(\n-                absl::StrCat(\"device \", device, \" > num_devices (\",\n-                             *num_devices, \") in tile assignment\"));\n-          } else if (seen_devices.contains(device)) {\n-            return absl::InvalidArgumentError(absl::StrCat(\n-                \"device \", device, \" is not unique in tile assignment\"));\n-          }\n-          seen_devices.insert(device);\n-          return absl::OkStatus();\n-        });\n-    TF_RETURN_IF_ERROR(status);\n-    all_devices_seen =\n-        !num_devices.has_value() || seen_devices.size() == *num_devices;\n-  } else {\n-    all_devices_seen = !num_devices.has_value() ||\n-                       tile_assignment_.iota_->num_elements() == *num_devices;\n+  if (maximal_) {\n+    CHECK(!tile_assignment_.iota_);\n+    if (tile_assignment_.array().num_elements() != 1) {\n+      return absl::InvalidArgumentError(\n+          \"Tile maximal sharding must have a single device assignment.\");\n+    }\n+    return DeviceInRange(tile_assignment_.first(), num_devices);\n   }\n \n-  if (IsTileMaximal() || IsManual() || IsUnreduced() || IsUnknown()) {\n-    return absl::OkStatus();\n+  // The correct constructor has to be used to create tile maximal shardings.\n+  if (tile_assignment_.num_elements() == 1) {\n+    return absl::InvalidArgumentError(\n+        \"Tile assignment only contains a single device. If a replicated \"\n+        \"sharding was intended, use HloSharding::Replicated(). If a device \"\n+        \"placement was intended, use HloSharding::AssignDevice()\");\n   }\n \n   // The tile assignment tensor must have the same rank as the tiled data rank.\n   if (shape.dimensions().size() != TiledDataRank()) {\n-    return tsl::errors::InvalidArgument(\n+    return absl::InvalidArgumentError(absl::StrCat(\n         \"Number of tile assignment dimensions (excluding subgroups) is \"\n-        \"different than the input rank. \"\n-        \"sharding=\",\n-        ToString(), \", input_shape=\", ShapeUtil::HumanString(shape));\n+        \"different than the input rank. sharding=\",\n+        ToString(), \", input_shape=\", ShapeUtil::HumanString(shape)));\n   }\n \n-  // All devices should be seen in the tile assignment.\n-  if (!all_devices_seen) {\n-    return tsl::errors::InvalidArgument(\"tile_assignment should have \",\n-                                        *num_devices, \" devices\");\n+  if (tile_assignment_.iota_) {\n+    if (num_devices.has_value() &&\n+        tile_assignment_.iota_->num_elements() != *num_devices) {\n+      return absl::InvalidArgumentError(absl::StrFormat(\n+          \"tile_assignment should have %d devices but has %d\", *num_devices,\n+          tile_assignment_.iota_->num_elements()));\n+    }\n+    return absl::OkStatus();\n   }\n \n-  // The correct constructor has to be used to create tile maximal shardings.\n-  if (tile_assignment_.num_elements() == 1) {\n-    return tsl::errors::InvalidArgument(\n-        \"Tile assignment only contains a single device. If a replicated \"\n-        \"sharding was intended, use HloSharding::Replicated(). If a device \"\n-        \"placement was intended, use HloSharding::AssignDevice()\");\n+  absl::flat_hash_set<int64_t> seen_devices;\n+  absl::Status status = tile_assignment_.array().EachStatus(\n+      [&num_devices, &seen_devices](absl::Span<const int64_t> indices,\n+                                    int64_t device) {\n+        TF_RETURN_IF_ERROR(DeviceInRange(device, num_devices));\n+        if (!seen_devices.insert(device).second) {\n+          return absl::InvalidArgumentError(absl::StrCat(\n+              \"device \", device, \" is not unique in tile assignment\"));\n+        }\n+        return absl::OkStatus();\n+      });\n+  TF_RETURN_IF_ERROR(status);\n+  if (num_devices.has_value() && seen_devices.size() != *num_devices) {\n+    return absl::InvalidArgumentError(\n+        absl::StrFormat(\"tile_assignment should have %d devices but has %d\",\n+                        *num_devices, seen_devices.size()));\n   }\n+\n   return absl::OkStatus();\n }\n \n@@ -1165,9 +1180,8 @@ HloSharding HloSharding::GetSubSharding(const Shape& shape,\n         absl::MakeConstSpan(\n             &*begin_it,\n             &*(begin_it + ShapeUtil::GetLeafCountTuple(*sub_shape))));\n-  } else {\n-    return tuple_elements_[sharding_index];\n   }\n+  return tuple_elements_[sharding_index];\n }\n \n std::optional<HloSharding> HloSharding::ExtractSingleSharding() const {"
        },
        {
            "sha": "2fd15f0fd28aa4811cb24bb78de2ea6417ee99fc",
            "filename": "third_party/xla/xla/service/hlo_domain_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_domain_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_domain_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_domain_test.cc?ref=aef5f3eedc23fd6cd9f85b4c88474b5413e1719a",
            "patch": "@@ -432,16 +432,16 @@ TEST_F(HloDomainTest, CheckNoDomainAddedOnPureIOComputation) {\n HloModule Module\n \n ENTRY entry {\n-  token0 = token[] after-all(), sharding={maximal device=-1}\n+  token0 = token[] after-all(), sharding={maximal device=1}\n   a = (f32[4], u32[], token[]) recv(token0), channel_id=1,\n-        sharding={{maximal device=-1},{maximal device=-1},{maximal device=-1}}\n+        sharding={{maximal device=1},{maximal device=1},{maximal device=1}}\n   b = (f32[4], token[]) recv-done(a), channel_id=1,\n-        sharding={{maximal device=-1},{maximal device=-1}}\n-  b_element = f32[4] get-tuple-element(b), index=0, sharding={maximal device=-1}\n-  c = f32[4] add(b_element, b_element), sharding={maximal device=-1}\n+        sharding={{maximal device=1},{maximal device=1}}\n+  b_element = f32[4] get-tuple-element(b), index=0, sharding={maximal device=1}\n+  c = f32[4] add(b_element, b_element), sharding={maximal device=1}\n   d = (f32[4], u32[], token[]) send(c, token0), channel_id=2,\n-        sharding={{maximal device=-1},{maximal device=-1},{maximal device=-1}}\n-  ROOT e = token[] send-done(d), channel_id=2, sharding={maximal device=-1}\n+        sharding={{maximal device=1},{maximal device=1},{maximal device=1}}\n+  ROOT e = token[] send-done(d), channel_id=2, sharding={maximal device=1}\n }\n )\";\n "
        },
        {
            "sha": "24d69e257b8d2a413bddaaa3d2fc47451bf090ae",
            "filename": "third_party/xla/xla/service/hlo_verifier_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aef5f3eedc23fd6cd9f85b4c88474b5413e1719a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc?ref=aef5f3eedc23fd6cd9f85b4c88474b5413e1719a",
            "patch": "@@ -3265,7 +3265,30 @@ ENTRY main {\n   auto status = verifier().Run(module.get()).status();\n   ASSERT_FALSE(status.ok());\n   EXPECT_THAT(status.message(),\n-              HasSubstr(\"device 2 > num_devices (2) in tile assignment\"));\n+              HasSubstr(\"device 2 >= num_devices (2) in tile assignment\"));\n+}\n+\n+TEST_F(HloVerifierTest, NegativeDeviceID) {\n+  const char* const hlo = R\"(\n+HloModule Module\n+\n+ENTRY main {\n+  p = f32[4,2] parameter(0), sharding={maximal device=-1}\n+  ROOT r = f32[4,2] copy(p)\n+}\n+)\";\n+\n+  HloModuleConfig config;\n+  config.set_num_partitions(2);\n+  config.set_use_spmd_partitioning(true);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnUnverifiedModule(hlo, config));\n+  ASSERT_TRUE(module->config().use_spmd_partitioning());\n+\n+  auto status = verifier().Run(module.get()).status();\n+  ASSERT_FALSE(status.ok());\n+  EXPECT_THAT(status.message(),\n+              HasSubstr(\"device -1 is negative in tile assignment\"));\n }\n \n TEST_F(HloVerifierTest, InconsistentWhileSharding) {"
        }
    ],
    "stats": {
        "total": 152,
        "additions": 94,
        "deletions": 58
    }
}