{
    "author": "mkuperst",
    "message": "[XLA] Move parameter sharding conflict resolution earlier.\n\nFix up the sharding on called computation parameters for kCall/kWhile/kConditional during preprocessing, instead of doing it on the fly while sharding the caller instruction. This should have no effect on its own, and is just preparation for supporting non-flat graphs.\n\nPiperOrigin-RevId: 797555553",
    "sha": "78fe7e96e512ee95ef71699f596f8e62a4c420eb",
    "files": [
        {
            "sha": "ec2f4f526c2853ca2494608897ae92f196d0c703",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 29,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fe7e96e512ee95ef71699f596f8e62a4c420eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fe7e96e512ee95ef71699f596f8e62a4c420eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=78fe7e96e512ee95ef71699f596f8e62a4c420eb",
            "patch": "@@ -2545,24 +2545,20 @@ std::vector<ReplicaGroup> SpmdPartitioningVisitor::CreateReplicaGroups(\n \n absl::Status SpmdPartitioningVisitor::HandleCall(HloInstruction* hlo) {\n   std::vector<HloInstruction*> call_args;\n-  HloComputation* computation = hlo->called_computations()[0];\n+  call_args.reserve(hlo->operand_count());\n   for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n-    // Shardings of the computation parameter and its argument must be\n-    // the same.\n-    computation->parameter_instruction(i)->set_sharding(\n-        hlo->operand(i)->sharding());\n     call_args.push_back(GetPartitionedHlo(hlo->operand(i)).hlo());\n   }\n \n-  TF_RETURN_IF_ERROR(partitioner_\n-                         ->PartitionComputation(computation, hlo->sharding(),\n-                                                next_channel_id_, logger_,\n-                                                call_graph_)\n-                         .status());\n+  TF_RETURN_IF_ERROR(\n+      partitioner_\n+          ->PartitionComputation(hlo->to_apply(), hlo->sharding(),\n+                                 next_channel_id_, logger_, call_graph_)\n+          .status());\n   SetPartitionedHlo(hlo, [&] {\n     auto* call = b_.AddInstruction(HloInstruction::CreateCall(\n         MakePartitionedShape(hlo->shape(), hlo->sharding()), call_args,\n-        hlo->called_computations()[0]));\n+        hlo->to_apply()));\n     call->set_raw_backend_config_string(hlo->raw_backend_config_string());\n     return call;\n   });\n@@ -4280,18 +4276,8 @@ absl::Status SpmdPartitioningVisitor::HandleReverse(HloInstruction* hlo) {\n \n absl::Status SpmdPartitioningVisitor::HandleWhile(HloInstruction* hlo) {\n   const HloSharding& sharding = hlo->sharding();\n-\n-  // Shardings for the body parameter, body root, and cond parameter must be\n-  // the same.\n-  hlo->while_condition()->parameter_instruction(0)->set_sharding(sharding);\n-  hlo->while_body()->parameter_instruction(0)->set_sharding(sharding);\n-\n-  // The condition root must be replicated so that all partitions follow the\n-  // same control flow.\n   HloInstruction* cond_root = hlo->while_condition()->root_instruction();\n-  const HloSharding cond_root_sharding =\n-      hlo_sharding_util::ReplicateAllDataDims(cond_root->sharding());\n-  cond_root->set_sharding(cond_root_sharding);\n+  const HloSharding cond_root_sharding = cond_root->sharding();\n   TF_RETURN_IF_ERROR(\n       partitioner_\n           ->PartitionComputation(hlo->while_condition(), cond_root_sharding,\n@@ -4315,12 +4301,6 @@ absl::Status SpmdPartitioningVisitor::HandleWhile(HloInstruction* hlo) {\n absl::Status SpmdPartitioningVisitor::HandleConditional(HloInstruction* hlo) {\n   std::vector<HloInstruction*> branch_args;\n   for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n-    HloComputation* computation = hlo->branch_computation(i);\n-\n-    // Shardings of the branch computation parameter and its argument must be\n-    // the same.\n-    computation->parameter_instruction(0)->set_sharding(\n-        hlo->operand(i + 1)->sharding());\n     branch_args.push_back(GetPartitionedHlo(hlo->operand(i + 1)).hlo());\n   }\n \n@@ -4927,7 +4907,7 @@ absl::StatusOr<bool> SpmdPartitioningVisitor::DoPartition(\n     const SpmdPartitionerOptions& options) {\n   VLOG(2) << \"Partitioning computation \" << computation->name() << \" for \"\n           << num_replicas_ << \" replicas and \" << num_partitions_\n-          << \" partitions\";\n+          << \" partitions\" << \" with root sharding \" << root_sharding;\n   TF_RETURN_IF_ERROR(computation->Accept(this));\n \n   HloModule* module = computation->parent();\n@@ -5654,6 +5634,42 @@ absl::Status SpmdPartitioner::PreprocessSharding(\n               HloSharding::Single(hlo->shape(), HloSharding::Replicate()));\n         }\n       }\n+\n+      // For control-flow constructs, we must make sure that the inputs and\n+      // outputs of the called computation have the same sharding as the\n+      // arguments being passed in.\n+      switch (hlo->opcode()) {\n+        case HloOpcode::kWhile: {\n+          hlo->while_condition()->parameter_instruction(0)->set_sharding(\n+              hlo->sharding());\n+          hlo->while_body()->parameter_instruction(0)->set_sharding(\n+              hlo->sharding());\n+          // The condition root must be replicated so that all partitions follow\n+          // the same control flow.\n+          HloInstruction* cond_root =\n+              hlo->while_condition()->root_instruction();\n+          const HloSharding cond_root_sharding =\n+              hlo_sharding_util::ReplicateAllDataDims(cond_root->sharding());\n+          cond_root->set_sharding(cond_root_sharding);\n+          break;\n+        }\n+        case HloOpcode::kConditional: {\n+          for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n+            hlo->branch_computation(i)->parameter_instruction(0)->set_sharding(\n+                hlo->operand(i + 1)->sharding());\n+          }\n+          break;\n+        }\n+        case HloOpcode::kCall: {\n+          for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n+            hlo->to_apply()->parameter_instruction(i)->set_sharding(\n+                hlo->operand(i)->sharding());\n+          }\n+          break;\n+        }\n+        default:\n+          break;\n+      }\n     }\n   }\n "
        }
    ],
    "stats": {
        "total": 74,
        "additions": 45,
        "deletions": 29
    }
}