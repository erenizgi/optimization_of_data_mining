{
    "author": "mehrdadkhani",
    "message": "[XLA:TPU] Modify memory space assignment to not double count the ConcatBitcast shared buffers between input and output operands while exporting its heap simulator trace. MSA adds such ConcatBitcast operations when it adds sliced prefetches.\n\nPiperOrigin-RevId: 843544179",
    "sha": "d040514b80c6197229ebf85fc987af506b4401ad",
    "files": [
        {
            "sha": "0d2933aebbd6b0cc75638adaa3fe1e361fb3f2a2",
            "filename": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment.cc",
            "status": "modified",
            "additions": 67,
            "deletions": 3,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d040514b80c6197229ebf85fc987af506b4401ad/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d040514b80c6197229ebf85fc987af506b4401ad/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc?ref=d040514b80c6197229ebf85fc987af506b4401ad",
            "patch": "@@ -1176,6 +1176,62 @@ absl::Status MemorySpaceAssignment::FixSchedule() {\n   return absl::OkStatus();\n }\n \n+namespace {\n+\n+bool IsConcatBitcastCustomCall(const HloInstruction* instruction) {\n+  return instruction->opcode() == HloOpcode::kCustomCall &&\n+         instruction->custom_call_target() ==\n+             memory_space_assignment::kConcatBitcastCustomCall;\n+}\n+\n+// If a use is a ConcatBitcastCustomCall, we add the uses of the\n+// ConcatBitcastCustomCall's output buffer to the list of uses for the\n+// current value. Since the instructions that have ConcatBitcastCustomCall as a\n+// user are slices, this means we are considering all uses of the concatenated\n+// buffer as uses of the original slices.\n+std::vector<HloUse> GetUsesAndExtendIfConcatBitcast(\n+    const HloValue* value, const HloDataflowAnalysis& dataflow_analysis) {\n+  std::vector<HloUse> uses;\n+  for (const HloUse& use : value->GetUses()) {\n+    if (IsConcatBitcastCustomCall(use.instruction)) {\n+      const HloValue& concat_bitcast_value =\n+          dataflow_analysis.GetUniqueValueAt(use.instruction);\n+      absl::c_copy(concat_bitcast_value.GetUses(), std::back_inserter(uses));\n+    } else {\n+      uses.push_back(use);\n+    }\n+  }\n+  return uses;\n+}\n+\n+// If a value is used by a ConcatBitcastCustomCall, we extend the time bound to\n+// represent the time bound of the concatenated value.\n+HloLiveRange::TimeBound GetTimeBoundAndExtendIfConcatBitcast(\n+    const HloValue* value, const HloDataflowAnalysis& dataflow_analysis,\n+    const HloLiveRange& hlo_live_range) {\n+  HloLiveRange::TimeBound time_bound =\n+      hlo_live_range.buffer_live_ranges().at(value);\n+  for (const HloUse& use : value->GetUses()) {\n+    if (IsConcatBitcastCustomCall(use.instruction)) {\n+      const HloValue& concat_bitcast_value =\n+          dataflow_analysis.GetUniqueValueAt(use.instruction);\n+      const HloLiveRange::TimeBound& concat_time_bound =\n+          hlo_live_range.buffer_live_ranges().at(&concat_bitcast_value);\n+      time_bound.start = std::min(time_bound.start, concat_time_bound.start);\n+      time_bound.end = std::max(time_bound.end, concat_time_bound.end);\n+      if (hlo_live_range.instruction_schedule().at(\n+              time_bound.end_position.instruction) <\n+          hlo_live_range.instruction_schedule().at(\n+              concat_time_bound.end_position.instruction)) {\n+        time_bound.end_position = concat_time_bound.end_position;\n+      }\n+    }\n+  }\n+  return time_bound;\n+}\n+\n+}  // namespace\n+\n absl::Status MemorySpaceAssignment::VerifyAndExportHeapSimulatorTrace(\n     const HloAliasAnalysis& alias_analysis,\n     std::vector<int64_t>* alt_mem_bytes_occupied) {\n@@ -1197,6 +1253,9 @@ absl::Status MemorySpaceAssignment::VerifyAndExportHeapSimulatorTrace(\n   auto add_allocation_and_verify = [&](int64_t start_time, int64_t end_time,\n                                        const HeapSimulator::Chunk& chunk,\n                                        const HloValue* value) -> absl::Status {\n+    if (IsConcatBitcastCustomCall(value->instruction())) {\n+      return absl::OkStatus();\n+    }\n     events[std::make_tuple(start_time, /*is_free=*/false, value->id())] =\n         std::make_tuple(value, chunk, HeapSimulatorTrace::Event::ALLOC);\n     events[std::make_tuple(end_time, /*is_free=*/true, value->id())] =\n@@ -1256,10 +1315,13 @@ absl::Status MemorySpaceAssignment::VerifyAndExportHeapSimulatorTrace(\n \n     for (const HloValue* value : buffer.values()) {\n       const HloLiveRange::TimeBound& time_bound =\n-          hlo_live_range->buffer_live_ranges().at(value);\n+          GetTimeBoundAndExtendIfConcatBitcast(\n+              value, alias_analysis.dataflow_analysis(), *hlo_live_range);\n       const HloInstruction* last_use_instruction = nullptr;\n       int64_t last_use_time = time_bound.start;\n-      for (const HloUse& use : value->GetUses()) {\n+      std::vector<HloUse> uses = GetUsesAndExtendIfConcatBitcast(\n+          value, alias_analysis.dataflow_analysis());\n+      for (const HloUse& use : uses) {\n         int64_t use_time =\n             hlo_live_range->instruction_schedule().at(use.instruction);\n         if (use_time > last_use_time) {\n@@ -1293,7 +1355,9 @@ absl::Status MemorySpaceAssignment::VerifyAndExportHeapSimulatorTrace(\n               std::min(earliest_computation_start_time, computation_start_time);\n           int64_t last_use_time = -1;\n           const HloInstruction* last_use_instruction = nullptr;\n-          for (const HloUse& use : value->GetUses()) {\n+          std::vector<HloUse> uses = GetUsesAndExtendIfConcatBitcast(\n+              value, alias_analysis.dataflow_analysis());\n+          for (const HloUse& use : uses) {\n             int64_t use_time =\n                 hlo_live_range->instruction_schedule().at(use.instruction);\n             if (use.instruction->parent() == called_computation &&"
        },
        {
            "sha": "f97b490e296afbc513a98d204967a58e77735ec8",
            "filename": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment_test.cc",
            "status": "modified",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d040514b80c6197229ebf85fc987af506b4401ad/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d040514b80c6197229ebf85fc987af506b4401ad/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment_test.cc?ref=d040514b80c6197229ebf85fc987af506b4401ad",
            "patch": "@@ -13370,6 +13370,135 @@ ENTRY main {\n   TF_EXPECT_OK(CheckSliceChunks(*assignments, root->operand(1)));\n }\n \n+// This module is optmized as below, which adds two slices followed by a concat\n+// bitcast. Since the concat bitcast uses the same slice buffers for its output,\n+// the heap simulator trace should not have freed any of the slices before the\n+// concat bitcast users are allocated/processed.\n+//\n+// ENTRY main {\n+//   ...\n+//   p1 = f32[8,8]{1,0} parameter(1)\n+//   slice-start = slice-start(p1), slice={[4:8], [0:8]}\n+//   ...\n+//   slice-start.1 = slice-start(p1), slice={[0:4], [0:8]}\n+//   ...\n+//   c = f32[8,8]{1,0:S(1)} tanh(b)\n+//   slice-done = f32[4,8]{1,0:S(1)} slice-done(slice-start)\n+//   slice-done.1 = f32[4,8]{1,0:S(1)} slice-done(slice-start.1)\n+//   custom-call = f32[8,8]{1,0:S(1)} custom-call(slice-done, slice-done.1),\n+//                                    custom_call_target=\"ConcatBitcast\"\n+//   r = f32[8,8]{1,0:S(1)} add(c, custom-call)\n+//   n = f32[8,8]{1,0:S(1)} negate(custom-call)\n+// ROOT f = f32[8,8]{1,0} add(r, n)\n+// }\n+//\n+TEST_F(SlicedPrefetchTest, SlicedPrefetchHeapSimulatorTrace) {\n+  std::string hlo_text = R\"zz(\n+HloModule Slice, is_scheduled=true\n+\n+ENTRY main {\n+  p0 = f32[8,8] parameter(0)\n+  p1 = f32[8,8] parameter(1)\n+\n+  a = f32[8,8] tanh(p0)\n+  b = f32[8,8] tanh(a)\n+  c = f32[8,8] tanh(b)\n+\n+  r = f32[8,8] add(c, p1)\n+  n = f32[8,8] negate(p1)\n+  ROOT f = f32[8,8] add(r, n)\n+})zz\";\n+\n+  SetupProposeSlicesToExpect2SlicesOfF32x8x8();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo_text));\n+  VLOG(1) << \"Original module:\\n\"\n+          << module->ToString(HloPrintOptions::ShortParsable());\n+\n+  std::unique_ptr<PresetAssignments> assignments = AssignMemorySpace(\n+      module.get(), MakeDefaultOptions(),\n+      /*max_prefetch_interval=*/10, /*min_prefetch_interval=*/1);\n+\n+  VLOG(1) << \"Post-MSA module:\\n\"\n+          << module->ToString(HloPrintOptions::ShortParsable());\n+\n+  auto* r_instr = FindInstruction(module.get(), \"r\");\n+  EXPECT_NE(r_instr, nullptr);\n+  auto* n_instr = FindInstruction(module.get(), \"n\");\n+  EXPECT_NE(n_instr, nullptr);\n+\n+  // Expect p1 to be copied via a sliced prefetch for use in r and n.\n+  EXPECT_THAT(\n+      r_instr,\n+      op::Add(_, IsAsyncSlicedCopy(kAlternateMemorySpace, kDefaultMemorySpace,\n+                                   {{{0, 4}, {0, 8}}, {{4, 8}, {0, 8}}},\n+                                   op::Parameter(1))));\n+  EXPECT_THAT(n_instr, op::Negate(r_instr->operand(1)));\n+\n+  // Check the instruction schedule.\n+  TF_EXPECT_OK(\n+      CheckSchedule(*module, r_instr->operand(1),\n+                    /*slices_start_after_instruction_name=*/\"p1\",\n+                    /*slices_done_before_instruction_name=*/\"r\",\n+                    /*expect_slices_started_at_different_times=*/true));\n+\n+  // Check expectations on the chunks assigned to the asynchronous sliced copy.\n+  TF_EXPECT_OK(CheckSliceChunks(*assignments, r_instr->operand(1)));\n+\n+  const HeapSimulatorTrace& heap_trace =\n+      assignments->assignment_information_for_space(kAlternateMemorySpace)\n+          ->heap_simulator_trace;\n+  // Track the set of instructions currently living in the alternate memory\n+  // space.\n+  // - ALLOC event: Instruction should not be in the set. Add it.\n+  // - FREE event: Instruction should be in the set. Remove it.\n+  // - Concat bitcast instruction: Slice operands should remain in the set until\n+  //   all concat bitcast users are allocated.\n+  absl::flat_hash_set<const HloInstruction*> allocated_instructions;\n+  for (const auto& event : heap_trace.events()) {\n+    VLOG(3) << \"event: \" << event.DebugString();\n+    const HloInstruction* instruction =\n+        FindInstruction(module.get(), event.instruction_name());\n+    EXPECT_NE(instruction, nullptr)\n+        << \"Instruction not found: \" << event.instruction_name();\n+    if (instruction->opcode() == HloOpcode::kCustomCall) {\n+      EXPECT_NE(instruction->custom_call_target(),\n+                memory_space_assignment::kConcatBitcastCustomCall)\n+          << \"We do not expect concat bitcast custom call to add any \"\n+             \"independent events to the heap trace.\";\n+    }\n+    if (instruction->opcode() == HloOpcode::kSlice) {\n+      EXPECT_TRUE(event.kind() == HeapSimulatorTrace::Event::ALLOC ||\n+                  event.kind() == HeapSimulatorTrace::Event::FREE);\n+    }\n+    if (event.kind() == HeapSimulatorTrace::Event::ALLOC) {\n+      allocated_instructions.insert(instruction);\n+    } else if (event.kind() == HeapSimulatorTrace::Event::FREE) {\n+      EXPECT_TRUE(allocated_instructions.contains(instruction))\n+          << \"FREE is called on slice instruction before its ALLOC or is its \"\n+             \"being called more than once on the same slice buffer.\";\n+      allocated_instructions.erase(instruction);\n+    } else {\n+      FAIL() << \"Unexpected event kind: \" << event.kind()\n+             << \" for instruction: \" << event.instruction_name();\n+    }\n+    // At the time we allocate the r and n instructions, we should still have\n+    // valid allocations for both slices of the concatbitcast operands, because\n+    // the concatbitcast should share that buffer with its users, i.e. r and n\n+    // instructions.\n+    if ((instruction == r_instr || instruction == n_instr) &&\n+        event.kind() == HeapSimulatorTrace::Event::ALLOC) {\n+      int slice_count = absl::c_count_if(\n+          allocated_instructions, [](const HloInstruction* inst) {\n+            return inst->opcode() == HloOpcode::kSlice;\n+          });\n+      EXPECT_EQ(slice_count, 2)\n+          << \"Did not find enough valid allocations for both slice buffers in \"\n+             \"the trace at the time of allocation for r or n instructions.\";\n+    }\n+  }\n+}\n+\n TEST_F(SlicedPrefetchTest, TwoSlicesWithCopyReplacement) {\n   std::string hlo_text = R\"zz(\n HloModule Slice, is_scheduled=true"
        }
    ],
    "stats": {
        "total": 199,
        "additions": 196,
        "deletions": 3
    }
}