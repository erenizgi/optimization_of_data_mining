{
    "author": "ZixuanJiang",
    "message": "Remove redundant constructors for `TileAssignment`.\n\nPiperOrigin-RevId: 831133271",
    "sha": "fad2b0416a6bb6fffd06bc317e450bc9309a164c",
    "files": [
        {
            "sha": "ee755c1f6d81fe18161aaa7118c917fc90526981",
            "filename": "third_party/xla/xla/hlo/ir/tile_assignment.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fad2b0416a6bb6fffd06bc317e450bc9309a164c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Ftile_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fad2b0416a6bb6fffd06bc317e450bc9309a164c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Ftile_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Ftile_assignment.cc?ref=fad2b0416a6bb6fffd06bc317e450bc9309a164c",
            "patch": "@@ -600,10 +600,8 @@ absl::Status TileAssignment::EachStatus(\n     absl::Span<const int64_t> new_dimensions) const {\n   if (iota_) {\n     CHECK_EQ(Product(new_dimensions), iota_->num_elements());\n-    return TileAssignment(\n-        IotaTileAssignment(new_dimensions, iota_->reshape_dims(),\n-                           iota_->transpose_perm()),\n-        /*shared_array=*/nullptr);\n+    return TileAssignment(new_dimensions, iota_->reshape_dims(),\n+                          iota_->transpose_perm());\n   }\n   std::shared_ptr<Array<int64_t>> reshaped = shared_array_clone();\n   reshaped->Reshape(new_dimensions);"
        },
        {
            "sha": "e7fb204a8c3fefb79188621afd9d032dd6f2a3f9",
            "filename": "third_party/xla/xla/hlo/ir/tile_assignment.h",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fad2b0416a6bb6fffd06bc317e450bc9309a164c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Ftile_assignment.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fad2b0416a6bb6fffd06bc317e450bc9309a164c/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Ftile_assignment.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Ftile_assignment.h?ref=fad2b0416a6bb6fffd06bc317e450bc9309a164c",
            "patch": "@@ -171,14 +171,14 @@ Array<int64_t> ToArray(absl::Span<const int64_t> reshape_dims,\n class TileAssignment {\n  public:\n   TileAssignment() : array_(ReplicatedArray()) {}\n+\n   explicit TileAssignment(std::shared_ptr<const Array<int64_t>> array)\n       : shared_array_(std::move(array)), array_(shared_array_.get()) {}\n   explicit TileAssignment(int64_t device_id)\n       : TileAssignment(std::make_shared<const Array<int64_t>>(\n             std::initializer_list<int64_t>{1}, device_id)) {}\n+\n   explicit TileAssignment(IotaTileAssignment iota) : iota_(std::move(iota)) {}\n-  explicit TileAssignment(std::initializer_list<int64_t> dims)\n-      : iota_(IotaTileAssignment::Create(dims)) {}\n   explicit TileAssignment(absl::Span<const int64_t> dims)\n       : iota_(IotaTileAssignment::Create(dims)) {}\n   explicit TileAssignment(absl::Span<const int64_t> dims,\n@@ -268,13 +268,6 @@ class TileAssignment {\n \n  private:\n   friend class HloSharding;\n-  // TODO(b/281892190): Consider changing int64_t to int32_t since it's unlikely\n-  // to have so many devices to overflow int32_t in practice.\n-  explicit TileAssignment(IotaTileAssignment iota,\n-                          std::shared_ptr<const Array<int64_t>> shared_array)\n-      : iota_(std::move(iota)),\n-        shared_array_(std::move(shared_array)),\n-        array_(shared_array_.get()) {}\n \n   void MaybeMaterializeFullArray() const ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n "
        },
        {
            "sha": "6007cbd53eb9e892a1645ba9c60b51beacaf0a50",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fad2b0416a6bb6fffd06bc317e450bc9309a164c/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fad2b0416a6bb6fffd06bc317e450bc9309a164c/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding_test.cc?ref=fad2b0416a6bb6fffd06bc317e450bc9309a164c",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/python/pjrt_ifrt/xla_sharding.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <optional>\n #include <utility>\n@@ -950,7 +951,8 @@ TEST_P(HloShardingTest, DisassembleFailsWithMismatchingShapeDimsSize) {\n \n TEST_P(HloShardingTest, DisassembleFailsWithDynamicShape) {\n   auto device_list = GetDevices({0, 1});\n-  auto xla_hlo_sharding = xla::HloSharding::Tile(xla::TileAssignment({2}));\n+  auto xla_hlo_sharding =\n+      xla::HloSharding::Tile(xla::TileAssignment(absl::Span<const int64_t>{2}));\n   std::shared_ptr<const HloSharding> sharding =\n       HloSharding::Create(device_list, MemoryKind(), xla_hlo_sharding);\n "
        }
    ],
    "stats": {
        "total": 21,
        "additions": 7,
        "deletions": 14
    }
}