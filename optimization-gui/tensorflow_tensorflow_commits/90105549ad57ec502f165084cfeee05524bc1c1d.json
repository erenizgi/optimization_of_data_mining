{
    "author": "shawnwang18",
    "message": "PR #31341: [XLA:GPU] command buffer ChildCmd and WhileCmd support multiple device\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31341\n\nüìù Summary of Changes\nThis PR fixes an issue that will broken when running the command buffer WhileCmd in the SPMD config. The child_command_buffer_ object should be per device object.\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüß™ Unit Tests:\nxla/service/gpu/tests/command_buffer_test.cc:WhileLoopMultiDevice\nCopybara import of the project:\n\n--\n915f269e4ac1a75396ce4f99f585f405b61fb44a by Shawn Wang <shawnw@nvidia.com>:\n\ncommand buffer ChildCmd and WhileCmd support multiple device\n\nMerging this change closes #31341\n\nPiperOrigin-RevId: 808474308",
    "sha": "90105549ad57ec502f165084cfeee05524bc1c1d",
    "files": [
        {
            "sha": "7eb5f055eeec59c9b6fa9cad0fb22c98633bed63",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 58,
            "changes": 111,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=90105549ad57ec502f165084cfeee05524bc1c1d",
            "patch": "@@ -1347,11 +1347,6 @@ CommandBufferCmd::BufferUseVector ChildCmd::buffers() const {\n absl::Status ChildCmd::Initialize(const Thunk::InitializeParams& params,\n                                   StateManager& state) {\n   TF_RETURN_IF_ERROR(child_commands_.Initialize(params, state));\n-  if (child_command_buffer_ == nullptr) {\n-    TF_ASSIGN_OR_RETURN(child_command_buffer_,\n-                        params.stream->parent()->CreateCommandBuffer(\n-                            se::CommandBuffer::Mode::kNested));\n-  }\n   return absl::OkStatus();\n }\n \n@@ -1360,20 +1355,21 @@ absl::StatusOr<const se::CommandBuffer::Command*> ChildCmd::Record(\n     const RecordParams& record_params, RecordAction record_action,\n     se::CommandBuffer* command_buffer) {\n   VLOG(5) << \"Record ChildCmd \" << child_commands_.size() << \" commands\";\n-  CHECK(child_command_buffer_ != nullptr);\n-  TF_RETURN_IF_ERROR(child_commands_.Record(execute_params, record_params,\n-                                            CommandBufferCmd::RecordCreate{},\n-                                            child_command_buffer_.get()));\n+  auto record_fn = [&](se::CommandBuffer* command_buffer) -> absl::Status {\n+    return child_commands_.Record(execute_params, record_params,\n+                                  CommandBufferCmd::RecordCreate{},\n+                                  command_buffer);\n+  };\n   return Handle(\n       std::move(record_action),\n       [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n         return command_buffer->CreateChildCommand(\n-            se::CommandBuffer::ChildCommandType::kMoved, *child_command_buffer_,\n-            dependencies);\n+            se::CommandBuffer::ChildCommandType::kMoved,\n+            execute_params.stream->parent(), record_fn, dependencies);\n       },\n       [&](const se::CommandBuffer::Command* command) {\n-        // Moved child command does not need to be updated.\n-        return absl::OkStatus();\n+        return command_buffer->UpdateChildCommand(\n+            se::CommandBuffer::ChildCommandType::kMoved, command, record_fn);\n       });\n }\n \n@@ -1470,13 +1466,10 @@ absl::Status WhileCmd::Initialize(const Thunk::InitializeParams& params,\n                                   StateManager& state) {\n   TF_RETURN_IF_ERROR(cond_commands_.Initialize(params, state));\n   TF_RETURN_IF_ERROR(body_commands_.Initialize(params, state));\n+  enable_loop_unroll_ = true;\n   if (enable_loop_unroll_ && body_commands_.support_loop_unroll() &&\n-      cond_commands_.support_loop_unroll() && trip_count_ != std::nullopt &&\n-      child_command_buffer_ == nullptr) {\n+      cond_commands_.support_loop_unroll() && trip_count_ != std::nullopt) {\n     is_unrolled_loop_ = true;\n-    TF_ASSIGN_OR_RETURN(child_command_buffer_,\n-                        params.stream->parent()->CreateCommandBuffer(\n-                            se::CommandBuffer::Mode::kNested));\n   }\n   VLOG(3) << \"while command trip_count: \" << trip_count_.value_or(-1);\n   return absl::OkStatus();\n@@ -1493,55 +1486,57 @@ absl::StatusOr<const se::CommandBuffer::Command*> WhileCmd::Record(\n           << \" body_commands=\" << body_commands_.size();\n   VLOG(5) << \"  pred: \" << pred_ << \" (\" << pred.opaque() << \")\";\n   if (is_unrolled_loop_) {\n-    // When the loop is unrolled, we need to record the body commands for\n-    // `trip_count` times into child_command_buffer_, and implement the While\n-    // command as a child command.\n-    VLOG(3) << \"Recording unrolled loop with trip_count: \"\n-            << trip_count_.value();\n-    CHECK(child_command_buffer_ != nullptr);\n-\n-    // Unroll the while loop body for `trip_count` times.\n-    // Unrolled execution sequence: cond -> body -> cond -> body -> ...\n-    // In the unrolled pattern, we still need to run the cond commands because\n-    // body commands might depends on the value of index variable that is\n-    // updated by condition commands.\n-    auto new_record_params = record_params;\n-    for (int64_t i = 0; i < trip_count_.value(); ++i) {\n-      new_record_params.unroll_iteration = i;\n-      if (i == 0) {\n-        // First iteration, cond_commands_ will not have dependencies.\n-        TF_RETURN_IF_ERROR(cond_commands_.Record(\n-            execute_params, new_record_params, CommandBufferCmd::RecordCreate{},\n-            child_command_buffer_.get(), false));\n-      } else {\n-        // Other iterations, cond_commands_ will have dependencies on the sink\n-        // commands from previous iteration's body.\n-        auto body_sink_commands = body_commands_.SinkCommands(\n-            new_record_params, child_command_buffer_.get(), i - 1);\n-        TF_RETURN_IF_ERROR(cond_commands_.Record(\n+    auto record_fn =\n+        [&](se::CommandBuffer* child_command_buffer) -> absl::Status {\n+      // When the loop is unrolled, we need to record the body commands for\n+      // `trip_count` times into child_command_buffer, and implement the While\n+      // command as a child command.\n+      VLOG(3) << \"Recording unrolled loop with trip_count: \"\n+              << trip_count_.value();\n+\n+      // Unroll the while loop body for `trip_count` times.\n+      // Unrolled execution sequence: cond -> body -> cond -> body -> ...\n+      // In the unrolled pattern, we still need to run the cond commands because\n+      // body commands might depends on the value of index variable that is\n+      // updated by condition commands.\n+      auto new_record_params = record_params;\n+      for (int64_t i = 0; i < trip_count_.value(); ++i) {\n+        new_record_params.unroll_iteration = i;\n+        if (i == 0) {\n+          // First iteration, cond_commands_ will not have dependencies.\n+          TF_RETURN_IF_ERROR(cond_commands_.Record(\n+              execute_params, new_record_params,\n+              CommandBufferCmd::RecordCreate{}, child_command_buffer, false));\n+        } else {\n+          // Other iterations, cond_commands_ will have dependencies on the sink\n+          // commands from previous iteration's body.\n+          auto body_sink_commands = body_commands_.SinkCommands(\n+              new_record_params, child_command_buffer, i - 1);\n+          TF_RETURN_IF_ERROR(\n+              cond_commands_.Record(execute_params, new_record_params,\n+                                    CommandBufferCmd::RecordCreate{\n+                                        absl::MakeSpan(body_sink_commands)},\n+                                    child_command_buffer, false));\n+        }\n+        auto cond_sink_commands = cond_commands_.SinkCommands(\n+            new_record_params, child_command_buffer, i);\n+        TF_RETURN_IF_ERROR(body_commands_.Record(\n             execute_params, new_record_params,\n-            CommandBufferCmd::RecordCreate{absl::MakeSpan(body_sink_commands)},\n-            child_command_buffer_.get(), false));\n+            CommandBufferCmd::RecordCreate{absl::MakeSpan(cond_sink_commands)},\n+            child_command_buffer, i == trip_count_.value() - 1 ? true : false));\n       }\n-      auto cond_sink_commands = cond_commands_.SinkCommands(\n-          new_record_params, child_command_buffer_.get(), i);\n-      TF_RETURN_IF_ERROR(body_commands_.Record(\n-          execute_params, new_record_params,\n-          CommandBufferCmd::RecordCreate{absl::MakeSpan(cond_sink_commands)},\n-          child_command_buffer_.get(),\n-          i == trip_count_.value() - 1 ? true : false));\n-    }\n+      return absl::OkStatus();\n+    };\n     return Handle(\n         std::move(record_action),\n         [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n           return command_buffer->CreateChildCommand(\n               se::CommandBuffer::ChildCommandType::kMoved,\n-              *child_command_buffer_, dependencies);\n+              execute_params.stream->parent(), record_fn, dependencies);\n         },\n         [&](const se::CommandBuffer::Command* command) {\n-          // We do not need to update the child node here, and sub-graph has\n-          // been updated above in the unrooled pattern.\n-          return absl::OkStatus();\n+          return command_buffer->UpdateChildCommand(\n+              se::CommandBuffer::ChildCommandType::kMoved, command, record_fn);\n         });\n   } else {\n     return Handle("
        },
        {
            "sha": "de95bd2cbbd97ef6b9781d779df567f91c477ebc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=90105549ad57ec502f165084cfeee05524bc1c1d",
            "patch": "@@ -822,12 +822,6 @@ class ChildCmd : public CommandBufferCmd {\n \n  private:\n   CommandBufferCmdExecutor child_commands_;\n-\n-  // child command buffer is created at initialization time and then use the\n-  // move semantics to move it to the command buffer implementation. We do not\n-  // use the copy semantics because we will lose track of of the grahp nodes for\n-  // underlying implementation.\n-  std::unique_ptr<se::CommandBuffer> child_command_buffer_;\n };\n \n //===----------------------------------------------------------------------===//\n@@ -897,8 +891,6 @@ class WhileCmd : public CommandBufferCmd {\n   std::optional<int64_t> trip_count_;\n   bool enable_loop_unroll_ = false;\n   bool is_unrolled_loop_ = false;\n-  std::unique_ptr<se::CommandBuffer>\n-      child_command_buffer_;  // The body command buffer for unrolled loop.\n };\n \n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "6c5977a613680fb1f310af7208b0a475afcf3921",
            "filename": "third_party/xla/xla/service/gpu/tests/command_buffer_test.cc",
            "status": "modified",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc?ref=90105549ad57ec502f165084cfeee05524bc1c1d",
            "patch": "@@ -643,6 +643,92 @@ TEST_P(CommandBufferUnrollTest, WhileLoop) {\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n }\n \n+TEST_P(CommandBufferUnrollTest, WhileLoopMultiDevice) {\n+  se::StreamExecutor* stream_executor = GpuExecutor();\n+\n+  if (!IsAtLeastCuda12900(stream_executor)) {\n+    GTEST_SKIP() << \"Child command is not supported for CUDA < 12.9\";\n+  }\n+\n+  // Require at least two visible GPU devices for multi-device execution.\n+  auto name =\n+      absl::AsciiStrToUpper(PlatformUtil::CanonicalPlatformName(\"gpu\").value());\n+  auto* platform = se::PlatformManager::PlatformWithName(name).value();\n+  if (platform->VisibleDeviceCount() < 2) {\n+    GTEST_SKIP() << \"Test requires >= 2 visible GPU devices\";\n+  }\n+\n+  constexpr absl::string_view hlo_text = R\"(\n+  HloModule m, is_scheduled=true\n+\n+  compare_fusion {\n+    p0 = s32[] parameter(0)\n+    ten = s32[] constant(10)\n+    ROOT compare = compare(p0, ten), direction=LT\n+  }\n+\n+  add_one {\n+    p0 = s32[] parameter(0)\n+    one = s32[] constant(1)\n+    ROOT add = add(p0, one)\n+  }\n+\n+  add_two {\n+    p0 = f32[] parameter(0)\n+    two = f32[] constant(2.0)\n+    ROOT add = add(p0, two)\n+  }\n+\n+  body {\n+    p0 = (s32[], f32[]) parameter(0)\n+    cnt = get-tuple-element(p0), index=0\n+    val = get-tuple-element(p0), index=1\n+    add_cnt = s32[] fusion(cnt), kind=kLoop, calls=add_one\n+    add_val = f32[] fusion(val), kind=kLoop, calls=add_two\n+    ROOT tuple = (s32[], f32[]) tuple(add_cnt, add_val)\n+  }\n+\n+  cond {\n+    p0 = (s32[], f32[]) parameter(0)\n+    cnt = get-tuple-element(p0), index=0\n+    ROOT compare = pred[] fusion(cnt), kind=kLoop, calls=compare_fusion\n+  }\n+\n+  command_buffer {\n+    a = s32[] parameter(0)\n+    b = f32[] parameter(1)\n+    tuple = (s32[], f32[]) tuple(a, b)\n+    ROOT while = while(tuple), condition=cond, body=body, backend_config={\"known_trip_count\":{\"n\":\"10\"}}\n+  }\n+\n+  ENTRY main {\n+    a = s32[] parameter(0)\n+    b = f32[] parameter(1)\n+    ROOT call = (s32[], f32[]) call(a, b), to_apply=command_buffer\n+  })\";\n+\n+  // Parse with replica_count=2 to run on two devices.\n+  HloModuleConfig config = GetModuleConfigForTest(/*replica_count=*/2);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(hlo_text, config));\n+\n+  Literal cnt = LiteralUtil::CreateR0<int32_t>(0);\n+  Literal value = LiteralUtil::CreateR0<float>(0.0);\n+\n+  Literal expected_cnt = LiteralUtil::CreateR0<int32_t>(10);\n+  Literal expected_value = LiteralUtil::CreateR0<float>(20.0);\n+  Literal expected = LiteralUtil::MakeTuple({&expected_cnt, &expected_value});\n+\n+  // Flatten tuple parameter into individual leaves for PJRT replicated execute.\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), {&cnt, &value}, /*num_replicas=*/2,\n+                        /*use_threads=*/true, /*run_hlo_passes=*/false));\n+  ASSERT_EQ(results.size(), 2);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected, results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected, results[1]));\n+}\n+\n INSTANTIATE_TEST_SUITE_P(CommandBufferTests, CommandBufferTest,\n                          ::testing::Values(DebugOptions::LHS,\n                                            DebugOptions::CONCURRENT));"
        },
        {
            "sha": "2c10d92287c80c34f6b421a1243b28eb957da950",
            "filename": "third_party/xla/xla/stream_executor/command_buffer.h",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h?ref=90105549ad57ec502f165084cfeee05524bc1c1d",
            "patch": "@@ -186,6 +186,18 @@ class CommandBuffer {\n                                           const Command* command,\n                                           const CommandBuffer& nested) = 0;\n \n+  virtual absl::StatusOr<const Command*> CreateChildCommand(\n+      ChildCommandType type, StreamExecutor* executor,\n+      absl::AnyInvocable<absl::Status(stream_executor::CommandBuffer*)>\n+          record_fn,\n+      absl::Span<const Command* const> dependencies) = 0;\n+\n+  // Updates a command that launches a nested command buffer.\n+  virtual absl::Status UpdateChildCommand(\n+      ChildCommandType type, const Command* command,\n+      absl::AnyInvocable<absl::Status(stream_executor::CommandBuffer*)>\n+          record_fn) = 0;\n+\n   // Creates a device-to-device memory copy.\n   virtual absl::StatusOr<const Command*> CreateMemcpyD2D(\n       DeviceMemoryBase* dst, const DeviceMemoryBase& src, uint64_t size,"
        },
        {
            "sha": "c4f85662dcd6d542e569d6b8a95b4e85af166cbc",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_command_buffer.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc?ref=90105549ad57ec502f165084cfeee05524bc1c1d",
            "patch": "@@ -115,7 +115,8 @@ GpuCommandBuffer::ToGraphNodeDependencies(\n \n     } else if (auto* gpu_command = dynamic_cast<const GpuWhileCommand*>(dep)) {\n       handles.push_back(gpu_command->conditional_node.handle);\n-\n+    } else if (auto* gpu_command = dynamic_cast<const GpuChildCommand*>(dep)) {\n+      handles.push_back(gpu_command->handle);\n     } else {\n       LOG(FATAL) << \"Unsupported command type\";  // Crash OK\n     }\n@@ -239,6 +240,31 @@ absl::Status GpuCommandBuffer::UpdateChildCommand(ChildCommandType type,\n   return UpdateChildNode(type, gpu_command->handle, nested);\n }\n \n+absl::StatusOr<const CommandBuffer::Command*>\n+GpuCommandBuffer::CreateChildCommand(\n+    ChildCommandType type, StreamExecutor* executor,\n+    absl::AnyInvocable<absl::Status(stream_executor::CommandBuffer*)> record_fn,\n+    absl::Span<const Command* const> dependencies) {\n+  TF_RETURN_IF_ERROR(CheckInState(State::kCreate));\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<CommandBuffer> nested,\n+                      executor->CreateCommandBuffer(Mode::kNested));\n+  TF_RETURN_IF_ERROR(record_fn(nested.get()));\n+  TF_ASSIGN_OR_RETURN(\n+      GraphNodeHandle handle,\n+      CreateChildNode(type, ToGraphNodeDependencies(dependencies), *nested));\n+  return AppendCommand(GpuChildCommand{handle, std::move(nested)});\n+}\n+\n+absl::Status GpuCommandBuffer::UpdateChildCommand(\n+    ChildCommandType type, const Command* command,\n+    absl::AnyInvocable<absl::Status(stream_executor::CommandBuffer*)>\n+        record_fn) {\n+  TF_RETURN_IF_ERROR(CheckInState(State::kUpdate));\n+  auto* gpu_command = dynamic_cast<const GpuChildCommand*>(command);\n+  CHECK(gpu_command) << \"Command must be a GpuChildCommand\";\n+  return record_fn(gpu_command->command_buffer.get());\n+}\n+\n absl::StatusOr<const CommandBuffer::Command*> GpuCommandBuffer::CreateMemcpyD2D(\n     DeviceMemoryBase* dst, const DeviceMemoryBase& src, uint64_t size,\n     absl::Span<const Command* const> dependencies) {"
        },
        {
            "sha": "21273ac4e743b29699849c18bbfa5d47c368c136",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_command_buffer.h",
            "status": "modified",
            "additions": 28,
            "deletions": 7,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90105549ad57ec502f165084cfeee05524bc1c1d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.h?ref=90105549ad57ec502f165084cfeee05524bc1c1d",
            "patch": "@@ -105,6 +105,13 @@ class GpuCommandBuffer : public CommandBuffer {\n     GraphConditionalNodeHandle conditional_node;\n   };\n \n+  struct GpuChildCommand : public CommandBuffer::Command {\n+    GpuChildCommand(GraphNodeHandle h, std::unique_ptr<CommandBuffer> cb)\n+        : handle(h), command_buffer(std::move(cb)) {}\n+    GraphNodeHandle handle = nullptr;\n+    std::unique_ptr<CommandBuffer> command_buffer;\n+  };\n+\n   GpuCommandBuffer(Mode mode, StreamExecutor* executor);\n \n   // Bring CreateLaunch and UpdateLaunch template functions into scope.\n@@ -124,13 +131,26 @@ class GpuCommandBuffer : public CommandBuffer {\n                             const BlockDim& blocks, const Kernel& kernel,\n                             const KernelArgs& args) override;\n \n+  // Cloned type child command creation and update.\n   absl::StatusOr<const Command*> CreateChildCommand(\n       ChildCommandType type, CommandBuffer& nested,\n       absl::Span<const Command* const> dependencies) override;\n \n   absl::Status UpdateChildCommand(ChildCommandType type, const Command* command,\n                                   const CommandBuffer& nested) override;\n \n+  // Moved type child command creation and update.\n+  absl::StatusOr<const Command*> CreateChildCommand(\n+      ChildCommandType type, StreamExecutor* executor,\n+      absl::AnyInvocable<absl::Status(stream_executor::CommandBuffer*)>\n+          record_fn,\n+      absl::Span<const Command* const> dependencies) override;\n+\n+  absl::Status UpdateChildCommand(\n+      ChildCommandType type, const Command* command,\n+      absl::AnyInvocable<absl::Status(stream_executor::CommandBuffer*)>\n+          record_fn) override;\n+\n   absl::StatusOr<const Command*> CreateMemcpyD2D(\n       DeviceMemoryBase* dst, const DeviceMemoryBase& src, uint64_t size,\n       absl::Span<const Command* const> dependencies) override;\n@@ -253,8 +273,8 @@ class GpuCommandBuffer : public CommandBuffer {\n   // possible to add new commands to it, otherwise returns internal error.\n   absl::Status CheckNotFinalized();\n \n-  // Return OK status if command buffer is in the given state, otherwise returns\n-  // an error.\n+  // Return OK status if command buffer is in the given state, otherwise\n+  // returns an error.\n   absl::Status CheckInState(State state);\n \n   // Returns OK status if the command buffer can be updated.\n@@ -291,8 +311,8 @@ class GpuCommandBuffer : public CommandBuffer {\n   virtual absl::StatusOr<GraphNodeHandle> CreateEmptyNode(\n       absl::Span<const GraphNodeHandle> dependencies) = 0;\n \n-  // Adds a new conditional node to the graph and creates a corresponding nested\n-  // command buffer.\n+  // Adds a new conditional node to the graph and creates a corresponding\n+  // nested command buffer.\n   virtual absl::StatusOr<GraphConditionalNodeHandle> CreateConditionalNode(\n       absl::Span<const GraphNodeHandle> dependencies,\n       GraphConditionalHandle conditional, ConditionType type) = 0;\n@@ -332,8 +352,8 @@ class GpuCommandBuffer : public CommandBuffer {\n       ChildCommandType type, absl::Span<const GraphNodeHandle> dependencies,\n       CommandBuffer& nested) = 0;\n \n-  // Updates an existing child node. Will return an error if the given node has\n-  // not been created as a child node.\n+  // Updates an existing child node. Will return an error if the given node\n+  // has not been created as a child node.\n   virtual absl::Status UpdateChildNode(ChildCommandType type,\n                                        GraphNodeHandle node_handle,\n                                        const CommandBuffer& nested) = 0;\n@@ -353,7 +373,8 @@ class GpuCommandBuffer : public CommandBuffer {\n \n   //===--------------------------------------------------------------------===//\n \n-  // Launches an instantiated graph. Only supported on primary command buffers.\n+  // Launches an instantiated graph. Only supported on primary command\n+  // buffers.\n   virtual absl::Status LaunchGraph(Stream* stream) = 0;\n \n   // Returns the number of nodes in the graph associated with this command"
        }
    ],
    "stats": {
        "total": 280,
        "additions": 206,
        "deletions": 74
    }
}