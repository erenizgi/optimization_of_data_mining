{
    "author": "tensorflower-gardener",
    "message": "Add support for int8 dots, and allow bf16 to be used on any CPU.\n\nPiperOrigin-RevId: 824272399",
    "sha": "32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
    "files": [
        {
            "sha": "1141987f50762646afbd9ea328330af7a4d67597",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -235,22 +235,18 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/codegen:target_machine_features\",\n         \"//xla/backends/cpu/runtime:dot_lib\",\n         \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/service:pattern_matcher\",\n         \"//xla/tsl/platform:statusor\",\n         \"@XNNPACK//ynnpack\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:no_destructor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n     ],\n )\n "
        },
        {
            "sha": "19dd8256204db6a8ff0e426bdb0d5e15398cd6f5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/ynn_interop.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fynn_interop.cc?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -46,12 +46,22 @@ absl::StatusOr<YnnThreadpool> CreateYnnThreadpool(\n \n absl::StatusOr<ynn_type> YnnType(const PrimitiveType& type) {\n   switch (type) {\n+    case S4:\n+      return ynn_type_int4;\n+    case U4:\n+      return ynn_type_uint4;\n+    case S8:\n+      return ynn_type_int8;\n+    case U8:\n+      return ynn_type_uint8;\n     case BF16:\n       return ynn_type_bf16;\n     case F16:\n       return ynn_type_fp16;\n     case F32:\n       return ynn_type_fp32;\n+    case S32:\n+      return ynn_type_int32;\n     default:\n       return InvalidArgument(\"Unsupported YNNPACK type: %s\",\n                              primitive_util::LowercasePrimitiveTypeName(type));"
        },
        {
            "sha": "8937eb5f6b8e4fc437339742c6556688678fc34e",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -349,31 +349,26 @@ static absl::StatusOr<YnnSubgraph> EmitYnnDotSubgraph(\n   std::vector<size_t> rhs_dims = dims(rhs_shape.dimensions());\n   std::vector<size_t> out_dims = dims(out_shape.dimensions());\n \n-  PrimitiveType dtype = lhs->shape().element_type();\n-  if (dtype != F32 && dtype != BF16) {\n-    return InvalidArgument(\"Unsupported input data type for YnnDotThunk: %s\",\n-                           primitive_util::LowercasePrimitiveTypeName(dtype));\n-  }\n-\n-  ynn_type input_type = (dtype == F32) ? ynn_type_fp32 : ynn_type_bf16;\n-  ynn_type output_type = ynn_type_fp32;\n+  TF_ASSIGN_OR_RETURN(ynn_type ynn_lhs_type, YnnType(lhs_shape.element_type()));\n+  TF_ASSIGN_OR_RETURN(ynn_type ynn_rhs_type, YnnType(rhs_shape.element_type()));\n+  TF_ASSIGN_OR_RETURN(ynn_type ynn_out_type, YnnType(out_shape.element_type()));\n \n   const uint32_t input_tensor_flags = YNN_VALUE_FLAG_EXTERNAL_INPUT;\n   YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n-      subgraph.get(), input_type, lhs_dims.size(), lhs_dims.data(),\n+      subgraph.get(), ynn_lhs_type, lhs_dims.size(), lhs_dims.data(),\n       /*data=*/nullptr,\n       /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n       /*scale_id=*/YNN_INVALID_VALUE_ID, input_tensor_flags, &lhs_id));\n \n   YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n-      subgraph.get(), input_type, rhs_dims.size(), rhs_dims.data(),\n+      subgraph.get(), ynn_rhs_type, rhs_dims.size(), rhs_dims.data(),\n       capture_rhs ? arguments_buffers[1].opaque() : nullptr,\n       /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n       /*scale_id=*/YNN_INVALID_VALUE_ID, input_tensor_flags, &rhs_id));\n \n   const uint32_t output_tensor_flags = YNN_VALUE_FLAG_EXTERNAL_OUTPUT;\n   YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n-      subgraph.get(), output_type, out_dims.size(), out_dims.data(),\n+      subgraph.get(), ynn_out_type, out_dims.size(), out_dims.data(),\n       /*data=*/nullptr,\n       /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n       /*scale_id=*/YNN_INVALID_VALUE_ID, output_tensor_flags, &out_id));"
        },
        {
            "sha": "f9d7805f3e2a4aec569ae87fe902d79b723cf73f",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.cc?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -16,19 +16,24 @@ limitations under the License.\n #include \"xla/backends/cpu/ynn_support.h\"\n \n #include <algorithm>\n+#include <tuple>\n \n #include \"ynnpack/include/ynnpack.h\"\n #include \"absl/base/no_destructor.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"\n #include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/shape.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n \n@@ -138,4 +143,64 @@ bool IsElementwiseOpSupportedByYnn(const HloInstruction* hlo) {\n   }\n }\n \n+absl::StatusOr<bool> IsDotSupportedByYnn(\n+    const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n+    const Shape& rhs_shape, const Shape& out_shape) {\n+  // Stores tuple of allowed (input, output) dtypes.\n+  static const absl::NoDestructor<absl::flat_hash_set<\n+      std::tuple<PrimitiveType, PrimitiveType, PrimitiveType>>>\n+      kAllowedTypes({\n+          {F32, F32, F32},\n+          // TODO(b/449998002): We don't have fast fp16 kernels yet.\n+          // {F16, F16, F32},\n+          {BF16, BF16, F32},\n+          {S8, S8, S32},\n+          {U8, S8, S32},\n+          // TODO(b/441600372): We don't have fast int4 kernels yet. Even the\n+          // reference kernel might be pretty good though?\n+          // {S8, S4, S32},\n+      });\n+\n+  // Types must be in the allowed set.\n+  PrimitiveType lhs_dtype = lhs_shape.element_type();\n+  PrimitiveType rhs_dtype = rhs_shape.element_type();\n+  PrimitiveType out_dtype = out_shape.element_type();\n+  if (!kAllowedTypes->contains({lhs_dtype, rhs_dtype, out_dtype})) {\n+    return false;\n+  }\n+\n+  if (!IsLayoutSupportedByYnn(lhs_shape) ||\n+      !IsLayoutSupportedByYnn(rhs_shape) ||\n+      !IsLayoutSupportedByYnn(out_shape)) {\n+    return false;\n+  }\n+\n+  // Check shapes.\n+  TF_ASSIGN_OR_RETURN(DotShape dot_shape, GetDotShape(dot_dimensions, lhs_shape,\n+                                                      rhs_shape, out_shape));\n+\n+  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n+                      GetDotCanonicalDims(dot_dimensions, dot_shape));\n+\n+  if (dot_canonical_dims.m == 1 && dot_canonical_dims.n == 1 &&\n+      dot_shape.batch_size > 1) {\n+    // TODO(b/430079105): YNNPACK does not handle batch dimensions that are not\n+    // matrix dimensions. We could handle this case by fully implementing dot\n+    // (b/430079105), but we also could just insert dummy dimensions of size 1\n+    // for the matrix dimensions, so the batch dimensions get handled correctly.\n+    return false;\n+  }\n+\n+  // YNNPACK supports transposing the inputs efficiently if possible (they will\n+  // fuse with dot packing), but we don't currently support generating the\n+  // necessary transposes.\n+  if (!dot_canonical_dims.lhs_canonical ||\n+      dot_canonical_dims.lhs_column_major ||\n+      dot_canonical_dims.rhs_column_major) {\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n }  // namespace xla::cpu"
        },
        {
            "sha": "becbcc2c70854289f293ed6abcd1a148a07f170a",
            "filename": "third_party/xla/xla/backends/cpu/ynn_support.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_support.h?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/shape.h\"\n \n namespace xla::cpu {\n \n@@ -55,6 +56,12 @@ bool IsConstantSupportedByYnn(const HloInstruction* hlo);\n // Returns true if the nonconstant elementwise op is supported by YNNPACK.\n bool IsElementwiseOpSupportedByYnn(const HloInstruction* hlo);\n \n+// Returns true if the dot operation is supported by YNNPACK. Returns an error\n+// if the dot operation shape is invalid.\n+absl::StatusOr<bool> IsDotSupportedByYnn(\n+    const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n+    const Shape& rhs_shape, const Shape& out_shape);\n+\n }  // namespace xla::cpu\n \n #endif  // XLA_BACKENDS_CPU_YNN_SUPPORT_H_"
        },
        {
            "sha": "9abe77387d794ac46f3043b3ade369a01e40e916",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -406,6 +406,8 @@ cc_library(\n         \":onednn_contraction_rewriter\",\n         \":onednn_float_support\",\n         \":onednn_ops_rewriter\",\n+    ]) + if_ynnpack([\n+        \"//xla/backends/cpu:ynn_support\",\n     ]),\n )\n "
        },
        {
            "sha": "404f616d382a6fee298aef8de69914b5319e20f6",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 9,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -257,6 +257,10 @@ limitations under the License.\n #include \"xla/service/cpu/onednn_ops_rewriter.h\"\n #endif  // XLA_ONEDNN\n \n+#ifdef XLA_YNNPACK\n+#include \"xla/backends/cpu/ynn_support.h\"\n+#endif  // XLA_YNNPACK\n+\n namespace xla {\n namespace {\n \n@@ -628,15 +632,42 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n     if (!call_library_for_dot(*instr)) {\n       return true;\n     }\n-    bool use_cost_model = module->config()\n-                              .debug_options()\n-                              .xla_cpu_experimental_xnn_graph_fusion_mode() !=\n-                          DebugOptions::XNN_GRAPH_FUSION_MODE_BYPASS_COST_MODEL;\n-    return !IsDotSupportedByXnn(instr->dot_dimension_numbers(),\n-                                instr->operand(0)->shape(),\n-                                instr->operand(1)->shape(), instr->shape(),\n-                                target_machine_features, use_cost_model)\n-                .value_or(false);\n+\n+#ifdef XLA_YNNPACK\n+    if (absl::c_linear_search(\n+            module->config()\n+                .debug_options()\n+                .xla_cpu_experimental_ynn_fusion_type(),\n+            DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_DOT)) {\n+      if (IsDotSupportedByYnn(instr->dot_dimension_numbers(),\n+                              instr->operand(0)->shape(),\n+                              instr->operand(1)->shape(), instr->shape())\n+              .value_or(false)) {\n+        return false;\n+      }\n+    }\n+#endif  // XLA_YNNPACK\n+\n+    auto xnn_graph_fusion_mode =\n+        module->config()\n+            .debug_options()\n+            .xla_cpu_experimental_xnn_graph_fusion_mode();\n+    if (xnn_graph_fusion_mode != DebugOptions::XNN_GRAPH_FUSION_MODE_DISABLED) {\n+      bool use_cost_model =\n+          module->config()\n+              .debug_options()\n+              .xla_cpu_experimental_xnn_graph_fusion_mode() !=\n+          DebugOptions::XNN_GRAPH_FUSION_MODE_BYPASS_COST_MODEL;\n+      if (IsDotSupportedByXnn(instr->dot_dimension_numbers(),\n+                              instr->operand(0)->shape(),\n+                              instr->operand(1)->shape(), instr->shape(),\n+                              target_machine_features, use_cost_model)\n+              .value_or(false)) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n   };\n \n   // xla::cpu::GetDotImplementationStrategy (used by call_library_for_dot)"
        },
        {
            "sha": "55a27735204f730f2bd86804bb7299fd73658d47",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/32c1551f24bdbadd582f7f00d4b269a54ef4d75b/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=32c1551f24bdbadd582f7f00d4b269a54ef4d75b",
            "patch": "@@ -1091,13 +1091,10 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitDotThunk(\n               .xla_cpu_experimental_ynn_fusion_type(),\n           DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_DOT);\n       if (use_ynn) {\n-        // TODO(ashaposhnikov): Replace IsDotSupportedByXnn with\n-        // IsDotSupportedByYnn.\n         TF_ASSIGN_OR_RETURN(\n             auto is_dot_supported,\n-            IsDotSupportedByXnn(dnums, lhs->shape(), rhs->shape(),\n-                                instruction->shape(), &target_machine_features_,\n-                                /*use_cost_model=*/false));\n+            IsDotSupportedByYnn(dnums, lhs->shape(), rhs->shape(),\n+                                instruction->shape()));\n         if (is_dot_supported) {\n           return EmitYnnFusionThunk(instruction);\n         }"
        }
    ],
    "stats": {
        "total": 161,
        "additions": 132,
        "deletions": 29
    }
}