{
    "author": "apivovarov",
    "message": "This change replaces uses of std::iota with absl::c_iota, which operate directly on containers. This simplifies the code by removing the need to pass begin() and end() iterators.\n\nAlso includes minor refactoring of if/else structures and updates function signatures for clarity.\n\nPiperOrigin-RevId: 838924315",
    "sha": "c63f89f014084cecc0354b7996b927c676948059",
    "files": [
        {
            "sha": "2201945eabf1f55e8c2b5917bab9de4a3ba54411",
            "filename": "third_party/xla/xla/pjrt/c/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -537,6 +537,7 @@ cc_library(\n         \"//xla/pjrt:pjrt_executable\",\n         \"//xla/pjrt/proto:compile_options_proto_cc\",\n         \"//xla/service:computation_placer_hdr\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n@@ -584,6 +585,7 @@ xla_test(\n         \"//xla/tests:literal_test_util\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n@@ -656,6 +658,7 @@ cc_library(\n         \"//xla/service:computation_placer_hdr\",\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/tests:literal_test_util\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "49f9094148959627e32a254dd299922f1337ef6d",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_gpu_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <cstring>\n #include <functional>\n #include <memory>\n-#include <numeric>\n #include <string>\n #include <thread>  // NOLINT(build/c++11)\n #include <utility>\n@@ -30,6 +29,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n@@ -171,7 +171,7 @@ TEST_F(PjrtCApiGpuTest, CreateViewOfDeviceBuffer) {\n   CHECK_OK(transfer_to_host.Await());\n   ASSERT_EQ(literal->data<float>().size(), 4);\n   std::vector<float> float_data(4);\n-  std::iota(float_data.begin(), float_data.end(), 41.0f);\n+  absl::c_iota(float_data, 41.0f);\n   EXPECT_TRUE(xla::LiteralTestUtil::Equal(\n       xla::LiteralUtil::CreateR1<float>(float_data), *literal));\n }"
        },
        {
            "sha": "396d99a6b9c4076627a5fbd5291501d59be63bdf",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n #include <cstddef>\n #include <functional>\n #include <memory>\n-#include <numeric>\n #include <set>\n #include <string>\n #include <tuple>\n@@ -27,6 +26,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n@@ -329,7 +329,7 @@ void destroy_executable(PJRT_LoadedExecutable* executable,\n TEST_F(PjrtCApiTest, BufferTransferImmutableUntilTransferCompletes) {\n   xla::Shape shape = xla::ShapeUtil::MakeShapeWithType<float>({4});\n   std::vector<float> float_data(4);\n-  std::iota(float_data.begin(), float_data.end(), 41.0f);\n+  absl::c_iota(float_data, 41.0f);\n \n   PJRT_Client_BufferFromHostBuffer_Args args = CreateBufferFromHostBufferArgs(\n       float_data, shape,\n@@ -683,7 +683,7 @@ TEST_F(PjrtCApiBufferTest, ToHostBufferNoHostLayout) {\n   EXPECT_EQ(error, nullptr);\n   ASSERT_EQ(literal->data<float>().size(), 4);\n   std::vector<float> float_data(4);\n-  std::iota(float_data.begin(), float_data.end(), 41.0f);\n+  absl::c_iota(float_data, 41.0f);\n   EXPECT_TRUE(xla::LiteralTestUtil::Equal(\n       xla::LiteralUtil::CreateR1<float>(float_data), *literal));\n }"
        },
        {
            "sha": "7fb04f460a4fb7bc70a8e9d9e242fa70bbc3ef52",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_test_base.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_test_base.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -16,12 +16,12 @@ limitations under the License.\n #include \"xla/pjrt/c/pjrt_c_api_test_base.h\"\n \n #include <memory>\n-#include <numeric>\n #include <string>\n #include <utility>\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/types/span.h\"\n@@ -188,7 +188,7 @@ std::pair<std::unique_ptr<PJRT_Buffer, ::pjrt::PJRT_BufferDeleter>,\n PjrtCApiTestBase::create_iota_buffer(PJRT_Device* device) {\n   xla::Shape shape = xla::ShapeUtil::MakeShapeWithType<float>({4});\n   std::vector<float> float_data(4);\n-  std::iota(float_data.begin(), float_data.end(), 41.0f);\n+  absl::c_iota(float_data, 41.0f);\n   return create_buffer_from_data(float_data, shape, device);\n }\n "
        },
        {
            "sha": "8eef6118ec07ce802a6f58fe2ac778ec2efa109d",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -265,6 +265,7 @@ xla_test(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:subprocess\",\n         \"//xla/tsl/util:command_line_flags\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "fc54cea0ea9e5f9c257337d9be27866870cb9968",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include <cstdint>\n #include <cstring>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <set>\n #include <string>\n@@ -33,6 +32,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n@@ -145,7 +145,9 @@ absl::StatusOr<std::shared_ptr<xla::Literal>> ExtractSingleResult(\n   std::vector<std::unique_ptr<xla::PjRtBuffer>>& result_buffers = (*result)[0];\n   TF_RET_CHECK(result_buffers.size() == 1);\n   auto literal_or = result_buffers[0]->ToLiteralSync();\n-  if (!literal_or.status().ok()) return literal_or.status();\n+  if (!literal_or.status().ok()) {\n+    return literal_or.status();\n+  }\n   return *literal_or;\n }\n \n@@ -793,7 +795,7 @@ TEST(StreamExecutorGpuClientTest, FromHostAsync) {\n   std::vector<Shape> src_shapes;\n   for (int i = 0; i < 4; ++i) {\n     std::vector<float> data(i + 1);\n-    std::iota(data.begin(), data.end(), static_cast<float>(i + 10));\n+    absl::c_iota(data, static_cast<float>(i + 10));\n     src_literals.emplace_back(LiteralUtil::CreateR1<float>(data));\n     src_shapes.push_back(src_literals.back().shape());\n   }\n@@ -865,7 +867,7 @@ TEST(StreamExecutorGpuClientTest, FromHostAsyncPinnedHost) {\n   std::vector<Shape> src_shapes;\n   for (int i = 0; i < 4; ++i) {\n     std::vector<float> data(i + 1);\n-    std::iota(data.begin(), data.end(), static_cast<float>(i + 10));\n+    absl::c_iota(data, static_cast<float>(i + 10));\n     src_literals.emplace_back(LiteralUtil::CreateR1<float>(data));\n     src_shapes.push_back(src_literals.back().shape());\n   }\n@@ -1147,7 +1149,7 @@ TEST(StreamExecutorGpuClientTest, CreateMixOfErrorBuffers) {\n   std::vector<Shape> src_shapes;\n   for (int i = 0; i < 4; ++i) {\n     std::vector<float> data(i + 1);\n-    std::iota(data.begin(), data.end(), static_cast<float>(i + 10));\n+    absl::c_iota(data, static_cast<float>(i + 10));\n     src_literals.emplace_back(LiteralUtil::CreateR1<float>(data));\n     src_shapes.push_back(src_literals.back().shape());\n   }\n@@ -2618,7 +2620,7 @@ TEST(StreamExecutorGpuClientTest, MultipleDeviceShareDmaMapping) {\n                           GetStreamExecutorGpuClient(DefaultOptions()));\n   ASSERT_GE(client->devices().size(), 2);\n \n-  size_t test_length = 0.5l * 1024 * 1024;\n+  size_t test_length = 512 * 1024;\n   std::vector<int32_t> data(test_length);\n   for (int32_t i = 0; i < test_length; ++i) {\n     data[i] = i;\n@@ -2669,7 +2671,7 @@ TEST(StreamExecutorGpuClientTest, RawBuffer) {\n                           GetStreamExecutorGpuClient(DefaultOptions()));\n \n   std::vector<int32_t> data(256);\n-  std::iota(data.begin(), data.end(), 10);\n+  absl::c_iota(data, 10);\n \n   Shape shape = ShapeUtil::MakeShape(S32, {256});\n   auto buffer =\n@@ -2690,7 +2692,7 @@ TEST(StreamExecutorGpuClientTest, RawBuffer) {\n   ASSERT_EQ(on_device_size, 1024);\n \n   std::vector<int32_t> data2(256);\n-  std::iota(data2.begin(), data2.end(), 47);\n+  absl::c_iota(data2, 47);\n   auto* dst1 = tsl::port::AlignedMalloc(1024, 1024);\n   auto* dst2 = tsl::port::AlignedMalloc(1024, 1024);\n   memcpy(dst1, data2.data(), sizeof(int32_t) * data2.size());\n@@ -2894,7 +2896,7 @@ TEST(StreamExecutorGpuClientTest, FailedCrossHostSendArgsSizeMismatch) {\n \n   // Create a buffer to try to send.\n   std::vector<int32_t> data(256);\n-  std::iota(data.begin(), data.end(), 1);\n+  absl::c_iota(data, 1);\n \n   Shape shape = ShapeUtil::MakeShape(S32, {256});\n \n@@ -3075,7 +3077,7 @@ absl::Status SuccessfulCrossHostTransferTestBody(bool is_sender,\n   if (is_sender) {\n     LOG(INFO) << log_prefix << \": creating buffers\";\n     std::vector<int32_t> data(256);\n-    std::iota(data.begin(), data.end(), 1);\n+    absl::c_iota(data, 1);\n     Shape shape = ShapeUtil::MakeShape(S32, {256});\n \n     // Create the data to send.\n@@ -3121,7 +3123,7 @@ absl::Status SuccessfulCrossHostTransferTestBody(bool is_sender,\n     // Receiver logic.\n     // Expected data to receive.\n     std::vector<int32_t> expected_data(256);\n-    std::iota(expected_data.begin(), expected_data.end(), 1);\n+    absl::c_iota(expected_data, 1);\n     auto expected_literal = LiteralUtil::CreateR1<int32_t>(expected_data);\n \n     // Receive some data.\n@@ -3263,7 +3265,7 @@ absl::Status ShardedAutotuningWorksTestBody(const int node_id,\n         service,\n         xla::GetDistributedRuntimeService(\n             \"[::]:12345\", xla::CoordinationServiceImpl::Options{\n-                              .num_nodes = ShardedAutotuningTest::kNumNodes}));\n+                              /*num_nodes=*/ShardedAutotuningTest::kNumNodes}));\n   }\n \n   xla::DistributedRuntimeClient::Options distributed_options;\n@@ -3305,8 +3307,7 @@ absl::Status ShardedAutotuningWorksTestBody(const int node_id,\n   debug_options.set_xla_gpu_cublas_fallback(false);\n \n   if (node_id < num_nodes_using_cache) {\n-    debug_options.set_xla_gpu_per_fusion_autotune_cache_dir(\n-        std::string(cache_dir));\n+    debug_options.set_xla_gpu_per_fusion_autotune_cache_dir(cache_dir);\n   }\n \n   mlir::MLIRContext context;"
        },
        {
            "sha": "55e94db05bcfc51be25f928820ddbe709681ed91",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -175,6 +175,7 @@ xla_test(\n         \":tfrt_gpu_client\",\n         \":tracked_gpu_device_buffer\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "958a1735c11363127c5eba27ad1136704bb42ff7",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -21,14 +21,14 @@ limitations under the License.\n #include <cstdlib>\n #include <cstring>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n@@ -550,7 +550,7 @@ TEST(TfrtGpuClientTest, ShouldStageHostToDeviceTransfersSetToTrue) {\n   auto* staging_client = tensorflow::down_cast<TfrtGpuClient*>(client.get());\n   EXPECT_TRUE(staging_client->should_stage_host_to_device_transfers());\n   std::vector<int32_t> data(256);\n-  std::iota(data.begin(), data.end(), 10);\n+  absl::c_iota(data, 10);\n   Shape shape = ShapeUtil::MakeShape(S32, {256});\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtBuffer> buffer,\n@@ -575,7 +575,7 @@ TEST(TfrtGpuClientTest, ShouldStageHostToDeviceTransfersSetToFalse) {\n   auto* staging_client = tensorflow::down_cast<TfrtGpuClient*>(client.get());\n   EXPECT_FALSE(staging_client->should_stage_host_to_device_transfers());\n   std::vector<int32_t> data(256);\n-  std::iota(data.begin(), data.end(), 10);\n+  absl::c_iota(data, 10);\n   Shape shape = ShapeUtil::MakeShape(S32, {256});\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<PjRtBuffer> buffer,\n@@ -898,7 +898,7 @@ TEST(TfrtGpuClientTest, FromHostAsync) {\n   std::vector<Shape> src_shapes;\n   for (int i = 0; i < 4; ++i) {\n     std::vector<float> data(i + 1);\n-    std::iota(data.begin(), data.end(), static_cast<float>(i + 10));\n+    absl::c_iota(data, static_cast<float>(i + 10));\n     src_literals.push_back(LiteralUtil::CreateR1<float>(data));\n     src_shapes.push_back(src_literals.back().shape());\n   }\n@@ -971,7 +971,7 @@ TEST(TfrtGpuClientTest, FromHostAsyncPinnedHost) {\n   std::vector<Shape> src_shapes;\n   for (int i = 0; i < 4; ++i) {\n     std::vector<float> data(i + 1);\n-    std::iota(data.begin(), data.end(), static_cast<float>(i + 10));\n+    absl::c_iota(data, static_cast<float>(i + 10));\n     src_literals.emplace_back(LiteralUtil::CreateR1<float>(data));\n     src_shapes.push_back(src_literals.back().shape());\n   }\n@@ -1086,7 +1086,7 @@ TEST(TfrtGpuClientTest, CreateMixOfErrorBuffers) {\n   std::vector<Shape> src_shapes;\n   for (int i = 0; i < 4; ++i) {\n     std::vector<float> data(i + 1);\n-    std::iota(data.begin(), data.end(), static_cast<float>(i + 10));\n+    absl::c_iota(data, static_cast<float>(i + 10));\n     src_literals.push_back(LiteralUtil::CreateR1<float>(data));\n     src_shapes.push_back(src_literals.back().shape());\n   }\n@@ -1794,7 +1794,7 @@ TEST(TfrtGpuClientTest, MultipleDeviceShareDmaMapping) {\n     GTEST_SKIP() << \"Test requires at least two addressable devices.\";\n   }\n \n-  size_t test_length = 0.5l * 1024 * 1024;\n+  size_t test_length = 512 * 1024;\n   std::vector<int32_t> data(test_length);\n   for (int32_t i = 0; i < test_length; ++i) {\n     data[i] = i;"
        },
        {
            "sha": "1b3c1e3b3734c49b71f7b1e89576c741957f8eae",
            "filename": "third_party/xla/xla/pjrt/utils.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 19,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpjrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Futils.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -85,9 +85,8 @@ absl::StatusOr<Shape> GetShardedShape(const Shape& shape,\n           }\n         });\n     return sharded_shape;\n-  } else {\n-    return hlo_sharding.TileShape(shape);\n   }\n+  return hlo_sharding.TileShape(shape);\n }\n \n absl::StatusOr<Shape> GetShardedShape(const HloInstructionProto& instr) {\n@@ -226,11 +225,11 @@ absl::StatusOr<MemorySpaceColor> GetMemorySpaceColor(\n   // unpinned_host?\n   if (memory_kind == \"unpinned_host\" || memory_kind == \"pinned_host\") {\n     return xla::Layout::kHostMemorySpace;\n-  } else if (memory_kind == \"device\") {\n+  }\n+  if (memory_kind == \"device\") {\n     return xla::Layout::kDefaultMemorySpace;\n-  } else {\n-    return InvalidArgument(\"Unknown memory kind %s\", memory_kind);\n   }\n+  return InvalidArgument(\"Unknown memory kind %s\", memory_kind);\n }\n \n // Helper method that takes an ArrayAttr of DictionaryAttrs for each arg or\n@@ -330,7 +329,9 @@ absl::StatusOr<std::vector<LayoutMode>> GetArgLayoutModes(\n   TF_ASSIGN_OR_RETURN(std::optional<std::vector<LayoutMode>> maybe_result,\n                       GetTupleLayoutModes(main.getFunctionType().getInputs(),\n                                           main.getAllArgAttrs()));\n-  if (maybe_result) return *maybe_result;\n+  if (maybe_result) {\n+    return *maybe_result;\n+  }\n \n   return MlirAttrsToLayoutModes(main.getAllArgAttrs(), main.getNumArguments());\n }\n@@ -347,7 +348,9 @@ absl::StatusOr<std::vector<LayoutMode>> GetOutputLayoutModes(\n   TF_ASSIGN_OR_RETURN(std::optional<std::vector<LayoutMode>> maybe_tuple_result,\n                       GetTupleLayoutModes(main.getFunctionType().getResults(),\n                                           main.getAllResultAttrs()));\n-  if (maybe_tuple_result) return *maybe_tuple_result;\n+  if (maybe_tuple_result) {\n+    return *maybe_tuple_result;\n+  }\n \n   return MlirAttrsToLayoutModes(main.getAllResultAttrs(), main.getNumResults());\n }\n@@ -365,7 +368,9 @@ absl::StatusOr<std::vector<MemorySpaceColor>> GetArgMemoryKinds(\n       std::optional<std::vector<MemorySpaceColor>> maybe_tuple_result,\n       GetTupleMemoryKinds(main.getFunctionType().getInputs(),\n                           main.getAllArgAttrs()));\n-  if (maybe_tuple_result) return *maybe_tuple_result;\n+  if (maybe_tuple_result) {\n+    return *maybe_tuple_result;\n+  }\n \n   return MlirAttrsToMemoryKinds(main.getAllArgAttrs(), main.getNumArguments());\n }\n@@ -383,7 +388,9 @@ absl::StatusOr<std::vector<MemorySpaceColor>> GetOutputMemoryKinds(\n       std::optional<std::vector<MemorySpaceColor>> maybe_tuple_result,\n       GetTupleMemoryKinds(main.getFunctionType().getResults(),\n                           main.getAllResultAttrs()));\n-  if (maybe_tuple_result) return *maybe_tuple_result;\n+  if (maybe_tuple_result) {\n+    return *maybe_tuple_result;\n+  }\n \n   return MlirAttrsToMemoryKinds(main.getAllResultAttrs(), main.getNumResults());\n }\n@@ -771,9 +778,8 @@ absl::StatusOr<std::vector<int>> ComputeParametersThatMustBeDonated(\n           computation->parameter_instruction(0)->shape();\n       CHECK(input_tuple_shape.IsTuple());\n       return input_tuple_shape.tuple_shapes().size();\n-    } else {\n-      return computation->num_parameters();\n     }\n+    return computation->num_parameters();\n   }();\n   // If any buffer in a parameter is aliased we will donate the entire input\n   // parameter.\n@@ -903,20 +909,20 @@ absl::Status TestBufferDonationClashes(\n           \"Toy \"\n           \"example for this bug: `f(donate(a), donate(a))`.\",\n           arg_idx, replica, partition, prev_arg_idx);\n-    } else if (is_donated) {\n+    }\n+    if (is_donated) {\n       return InvalidArgument(\n           \"Attempt to donate a buffer which is also used by the same call \"\n           \"to Execute() (flattened argument %d, replica %d, partition %d, \"\n           \"first use: %d). Toy example for this bug: `f(a, donate(a))`.\",\n           arg_idx, replica, partition, prev_arg_idx);\n-    } else {\n-      return InvalidArgument(\n-          \"Attempt to use a buffer that was previously donated in the same \"\n-          \"call to Execute() (flattened argument %d, replica %d, partition \"\n-          \"%d, first use: %d). Toy example for this bug: `f(donate(a), \"\n-          \"a)`.\",\n-          arg_idx, replica, partition, prev_arg_idx);\n     }\n+    return InvalidArgument(\n+        \"Attempt to use a buffer that was previously donated in the same \"\n+        \"call to Execute() (flattened argument %d, replica %d, partition \"\n+        \"%d, first use: %d). Toy example for this bug: `f(donate(a), \"\n+        \"a)`.\",\n+        arg_idx, replica, partition, prev_arg_idx);\n   }\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "c0966506e62166ebf71a4c86e565c84bd5714b3f",
            "filename": "third_party/xla/xla/python/ifrt/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -373,6 +373,7 @@ xla_cc_test(\n         \":serdes_version\",\n         \":shape_proto_cc\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/hash:hash_testing\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n@@ -550,6 +551,7 @@ cc_library(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/types:span\",\n     ],\n@@ -913,6 +915,7 @@ xla_cc_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/types:span\","
        },
        {
            "sha": "b4889982bebb80617f751db53c79768b5cb48512",
            "filename": "third_party/xla/xla/python/ifrt/array_impl_test_lib.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <utility>\n #include <vector>\n@@ -91,7 +90,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBuffer) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n   UserContextScope user_context_scope(test_util::MakeUserContext(100));\n@@ -122,7 +121,7 @@ TEST(ArrayImplTest,\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n \n   std::vector<Device*> devices;\n   devices.reserve(2);\n@@ -160,7 +159,7 @@ TEST_P(ArrayImplWithHostBufferSemanticsTest,\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -208,7 +207,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferImmutableOnlyDuringCall) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -245,7 +244,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferImmutableUntilTransferCompletes) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -269,7 +268,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferZeroCopy) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -296,7 +295,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferDefaultLayout) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices()[0];\n \n   for (Memory* const memory : device->Memories()) {\n@@ -329,7 +328,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferAndCopyToHostBuffer) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -439,7 +438,7 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferReplicated) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n   absl::Span<Device* const> devices = client->addressable_devices();\n   TF_ASSERT_OK_AND_ASSIGN(DeviceListRef device_list,\n                           client->MakeDeviceList(devices));\n@@ -489,9 +488,9 @@ TEST(ArrayImplTest, MakeArraysFromHostBufferShardsAndCopyToHostBuffer) {\n   Shape shape({2, 3});\n   Shape shard_shape({1, 3});\n   auto data0 = std::make_unique<std::vector<float>>(3);\n-  std::iota(data0->begin(), data0->end(), 0);\n+  absl::c_iota(*data0, 0);\n   auto data1 = std::make_unique<std::vector<float>>(3);\n-  std::iota(data1->begin(), data1->end(), 3);\n+  absl::c_iota(*data1, 3);\n   absl::Span<Device* const> devices =\n       client->addressable_devices().subspan(0, 2);\n   TF_ASSERT_OK_AND_ASSIGN(DeviceListRef device_list,\n@@ -579,7 +578,7 @@ TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithDifferentDevices) {\n   Shape shape({2, 3});\n   Shape shard_shape = shape;\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n \n   ShardingRef sharding0 = SingleDeviceSharding::Create(\n       client->addressable_devices()[0], MemoryKind());\n@@ -629,7 +628,7 @@ TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithDifferentMemoryKinds) {\n   Shape shape({2, 3});\n   Shape shard_shape = shape;\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n \n   std::vector<MemoryKind> memory_kinds;\n   for (const Memory* memory :\n@@ -680,7 +679,7 @@ TEST(ArrayImplTest, MakeArraysFromHostBufferShardsWithLayout) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices()[0];\n \n   auto layout = std::make_shared<xla::PjRtLayout>(\n@@ -882,7 +881,7 @@ TEST(ArrayImplTest, HostBufferRoundTripAllMemoryKinds) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices()[0];\n \n   for (Memory* const memory : device->Memories()) {\n@@ -1023,7 +1022,7 @@ TEST(ArrayImplTest, AssembleArray) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device0 = client->addressable_devices().at(0);\n   ShardingRef sharding0 = SingleDeviceSharding::Create(device0, MemoryKind());\n   Device* device1 = client->addressable_devices().at(1);\n@@ -1072,7 +1071,7 @@ TEST(ArrayImplTest, AssembleAndDisassembleArray) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device0 = client->addressable_devices().at(0);\n   ShardingRef sharding0 = SingleDeviceSharding::Create(device0, MemoryKind());\n   Device* device1 = client->addressable_devices().at(1);\n@@ -1197,7 +1196,7 @@ TEST(ArrayImplTest, CopyToSameDevices) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n@@ -1236,7 +1235,7 @@ TEST(ArrayImplTest, AssembleAndDisassembleNonAddressableArray) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device0 = client->addressable_devices().at(0);\n   ShardingRef sharding0 = SingleDeviceSharding::Create(device0, MemoryKind());\n   Device* device1 = client->addressable_devices().at(1);\n@@ -1292,7 +1291,7 @@ TEST(ArrayImplTest, CopyToDifferentDevice) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n   std::vector<ArrayRef> shards;\n   for (auto* device : devices->devices()) {\n@@ -1370,7 +1369,7 @@ TEST(ArrayImplTest, CopyMixedSourceDevices) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n \n   std::vector<ArrayRef> arrays;\n@@ -1404,7 +1403,7 @@ TEST(ArrayImplTest, CopyMixedSourceMemoryKind) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n \n@@ -1434,7 +1433,7 @@ TEST(ArrayImplTest, CopyPreservesDefaultLayouts) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices()[0];\n \n   for (Memory* const src_memory : device->Memories()) {\n@@ -1530,7 +1529,7 @@ TEST(ArrayImplTest, CopyArraysExhaustive) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n \n   for (Memory* const src_memory : src_device->Memories()) {\n     for (Device* const dst_device : client->addressable_devices()) {\n@@ -1583,7 +1582,7 @@ TEST(ArrayImplTest, CopyArraysSubByteDType) {\n   DType dtype(DType::kS4);\n   Shape shape({2, 3});\n   std::vector<int8_t> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n \n   for (Memory* const src_memory : src_device->Memories()) {\n     for (Device* const dst_device : client->addressable_devices()) {\n@@ -1679,7 +1678,7 @@ TEST(ArrayImplTest, GetReadyFuture) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n@@ -1698,7 +1697,7 @@ TEST(ArrayImplTest, BatchedGetReadyFuture) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n@@ -1720,7 +1719,7 @@ TEST(ArrayImplTest, Delete) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n@@ -1739,7 +1738,7 @@ TEST(ArrayImplTest, DeleteIsIdempotent) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;\n@@ -1763,7 +1762,7 @@ TEST(ArrayImplTest, IsDeleted) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n   auto semantics = Client::HostBufferSemantics::kImmutableOnlyDuringCall;"
        },
        {
            "sha": "20c1cce31223797c940455aebc56ba8e3ba5ccf1",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -96,6 +96,7 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:btree\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "639b7fdbbeecf931bfa0627b8f51611aa4fae935",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/ifrt_lower_mpmd_reshard_to_call_pass.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_lower_mpmd_reshard_to_call_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_lower_mpmd_reshard_to_call_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_lower_mpmd_reshard_to_call_pass.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -14,11 +14,11 @@ limitations under the License.\n ==============================================================================*/\n \n #include <cstdint>\n-#include <numeric>\n #include <string>\n #include <tuple>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/btree_set.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n@@ -170,16 +170,15 @@ class IfrtLowerMpmdReshardToCallPass\n       llvm::SmallVector<int32_t> donated_input_indices;\n       if (reshard_op.getDonated()) {\n         donated_input_indices.resize(reshard_op.getInputs().size());\n-        std::iota(donated_input_indices.begin(), donated_input_indices.end(),\n-                  0);\n+        absl::c_iota(donated_input_indices, 0);\n       }\n       auto call_op = CallOp::create(\n           builder, reshard_op.getLoc(),\n           /*outputs=*/reshard_op.getOutputs().getTypes(),\n           /*control_output=*/reshard_op.getControlOutput().getType(),\n           /*inputs=*/reshard_op.getInputs(),\n           /*control_inputs=*/reshard_op.getControlInputs(),\n-          /*args_attrs=*/nullptr,\n+          /*arg_attrs=*/nullptr,\n           /*res_attrs=*/nullptr,\n           /*callee=*/reshard_func_symbol,\n           /*devices=*/devices,"
        },
        {
            "sha": "888b09f5bd37f3d01c6c7d67cfd94803155fff13",
            "filename": "third_party/xla/xla/python/ifrt/remap_impl_test_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_impl_test_lib.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <utility>\n #include <vector>\n@@ -116,7 +115,7 @@ absl::StatusOr<ArrayRef> CreateArray(Client* client,\n \n   for (int i = 0; i < base_values.size(); ++i) {\n     std::vector<ValueType> data(shard_shape.num_elements());\n-    std::iota(data.begin(), data.end(), base_values[i]);\n+    absl::c_iota(data, base_values[i]);\n \n     Device* device = client->addressable_devices().at(device_indices[i]);\n     devices.push_back(device);\n@@ -179,7 +178,7 @@ void AssertArrayContent(Client* client, Array* array,\n                 ElementsAre(expected_device));\n \n     std::vector<ValueType> expected_data(expected_shard_shape.num_elements());\n-    std::iota(expected_data.begin(), expected_data.end(), base_values[i]);\n+    absl::c_iota(expected_data, base_values[i]);\n \n     std::vector<ValueType> actual_data(shards[i]->shape().num_elements());\n     TF_ASSERT_OK(shards[i]"
        },
        {
            "sha": "d7476d2fbc7c76f0c75d84bbd083d2f8ef18e7fe",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -17,12 +17,12 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n-#include <numeric>\n #include <tuple>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/types/span.h\"\n@@ -337,7 +337,7 @@ TEST_P(RemapPlanTest, InvalidShardIndex) {\n       num_shards *= shape[i] / shard_shape[i];\n     }\n     std::vector<int> devices(num_shards);\n-    std::iota(devices.begin(), devices.end(), 0);\n+    absl::c_iota(devices, 0);\n     RemapPlan plan;\n     plan.input_specs.push_back(ArraySpec{\n         /*dtype=*/DType(DType::kS32),"
        },
        {
            "sha": "f3cbd6f9af3ef2ab5b5c1ee004b4e709a939d1ef",
            "filename": "third_party/xla/xla/python/ifrt/shape_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fshape_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fshape_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fshape_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -17,12 +17,12 @@ limitations under the License.\n \n #include <cstdint>\n #include <limits>\n-#include <numeric>\n #include <sstream>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/hash/hash_testing.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n@@ -47,7 +47,7 @@ TEST(ShapeTest, LargeDim) {\n TEST(ShapeTest, ManyDims) {\n   const int kNumDims = 65536;  // Arbitrarily large number.\n   std::vector<int64_t> dims(kNumDims);\n-  std::iota(dims.begin(), dims.end(), 0);\n+  absl::c_iota(dims, 0);\n   Shape shape(dims);\n   EXPECT_THAT(shape.dims(), ElementsAreArray(dims));\n }"
        },
        {
            "sha": "1e495f02291abf69842695e14669452dd9e2ac54",
            "filename": "third_party/xla/xla/python/ifrt/support/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -69,9 +69,9 @@ xla_cc_test(\n         \"//xla/python/ifrt:mock\",\n         \"//xla/python/ifrt:test_util\",\n         \"//xla/python/ifrt/ir:sharding_param\",\n-        \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\","
        },
        {
            "sha": "5f5e2b9a76700e8b9a278e3cf099bc55a147ed51",
            "filename": "third_party/xla/xla/python/ifrt/support/sharding_conversions_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2Fsharding_conversions_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2Fsharding_conversions_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsupport%2Fsharding_conversions_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -16,12 +16,12 @@ limitations under the License.\n #include \"xla/python/ifrt/support/sharding_conversions.h\"\n \n #include <memory>\n-#include <numeric>\n #include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n@@ -41,7 +41,6 @@ limitations under the License.\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/test_util.h\"\n #include \"xla/shape.h\"\n-#include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -332,7 +331,7 @@ TEST_P(HloShardingToShardingParamTest, HloShardingToShardingParam) {\n   EXPECT_EQ(param.hlo_sharding, actual_hlo_sharding);\n   // Verify that the conversion to OpSharding is also correct.\n   std::vector<int> device_ids(param.num_devices);\n-  std::iota(device_ids.begin(), device_ids.end(), 0);\n+  absl::c_iota(device_ids, 0);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto hlo_via_op_sharding,\n       ToHloShardingViaOpSharding(sharding_param,"
        },
        {
            "sha": "7b494d46aeb9a67eb7859b3e1fedbbfe08a5d3e8",
            "filename": "third_party/xla/xla/python/ifrt/tuple_impl_test_lib.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Ftuple_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Ftuple_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Ftuple_impl_test_lib.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -14,17 +14,22 @@ limitations under the License.\n ==============================================================================*/\n \n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/client.h\"\n+#include \"xla/python/ifrt/device.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/test_util.h\"\n #include \"xla/python/ifrt/tuple.h\"\n+#include \"xla/python/ifrt/value.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -38,7 +43,7 @@ absl::StatusOr<ArrayRef> MakeArray(Client* client) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n "
        },
        {
            "sha": "fb430f8c7f33e17203452297296243a41f226f64",
            "filename": "third_party/xla/xla/python/ifrt_proxy/integration_tests/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -142,6 +142,7 @@ ifrt_proxy_cc_test(\n         \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "222457fc1553d3e23646c49becc62e08dc100933",
            "filename": "third_party/xla/xla/python/ifrt_proxy/integration_tests/mock_array_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2Fmock_array_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2Fmock_array_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2Fmock_array_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -15,14 +15,14 @@\n #include <cstdint>\n #include <functional>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/log/check.h\"\n@@ -84,7 +84,7 @@ class MockArrayTest : public testing::Test {\n     DType dtype(DType::kF32);\n     Shape shape({2, 3});\n     auto data = std::make_unique<std::vector<float>>(6);\n-    std::iota(data->begin(), data->end(), 0);\n+    absl::c_iota(*data, 0);\n     xla::ifrt::Device* device = client_->addressable_devices().at(0);\n     ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -132,7 +132,9 @@ class MockArrayTest : public testing::Test {\n                     absl::MutexLock l(mu_);\n                     if (get_ready_hook_) {\n                       absl::Status s = get_ready_hook_();\n-                      if (!s.ok()) return tsl::Future<>(s);\n+                      if (!s.ok()) {\n+                        return tsl::Future<>(s);\n+                      }\n                     }\n                     return delegated->GetReadyFuture();\n                   });\n@@ -142,7 +144,9 @@ class MockArrayTest : public testing::Test {\n                     absl::MutexLock l(mu_);\n                     if (copy_host_hook_) {\n                       absl::Status s = copy_host_hook_();\n-                      if (!s.ok()) return tsl::Future<>(s);\n+                      if (!s.ok()) {\n+                        return tsl::Future<>(s);\n+                      }\n                     }\n                     return delegated->CopyToHostBuffer(data, byte_strides,\n                                                        semantics);"
        },
        {
            "sha": "e64ef5493243d95fb8287ac9b1df9dbe8dd1f6cd",
            "filename": "third_party/xla/xla/python/ifrt_proxy/server/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -158,6 +158,7 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:status_to_from_proto\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "c63193d83c614688042c1d49222c93cbf891f7e2",
            "filename": "third_party/xla/xla/python/ifrt_proxy/server/ifrt_backend.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2Fifrt_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2Fifrt_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fserver%2Fifrt_backend.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -19,13 +19,13 @@\n #include <deque>\n #include <functional>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <string>\n #include <utility>\n #include <variant>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/container/flat_hash_map.h\"\n@@ -685,7 +685,7 @@ uint64_t IfrtBackend::HandleGenerator::GenerateAtServer() {\n void IfrtBackend::HandleGenerator::GenerateAtServerBulk(\n     absl::Span<uint64_t> result_handles) {\n   absl::MutexLock lock(mu_);\n-  std::iota(result_handles.begin(), result_handles.end(), current_);\n+  absl::c_iota(result_handles, current_);\n   current_ += result_handles.size();\n   CHECK_GE(current_, kServerGeneratedHandlesMinValue);\n }"
        },
        {
            "sha": "a0e2a06df54bd7dabc5f18547bee221ad117cc41",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -196,6 +196,7 @@ cc_library(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "dbaaefa9918726c99d18eb57fb2b713a829f30f6",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fbasic_string_array_test.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n #include <cstdint>\n #include <iterator>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <string>\n #include <utility>\n@@ -380,7 +379,7 @@ absl::StatusOr<ArrayRef> MakeSingleDeviceFloatTestArray(Client* client,\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   auto data = std::make_unique<std::vector<float>>(6);\n-  std::iota(data->begin(), data->end(), 0);\n+  absl::c_iota(*data, 0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n   return client->MakeArrayFromHostBuffer("
        },
        {
            "sha": "31cc3645eaead6c8f101f4c21513327a7441fb77",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_executable_impl_test_lib.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_executable_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c63f89f014084cecc0354b7996b927c676948059/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_executable_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_executable_impl_test_lib.cc?ref=c63f89f014084cecc0354b7996b927c676948059",
            "patch": "@@ -15,12 +15,12 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n-#include <numeric>\n #include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -354,7 +354,7 @@ TEST_P(LoadedExecutableImplTest, CompileAndExecute) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -386,7 +386,7 @@ TEST_P(LoadedExecutableImplTest, CompileAndExecute) {\n   TF_ASSERT_OK(future.Await());\n \n   std::vector<float> expected_out_data(6);\n-  std::iota(expected_out_data.begin(), expected_out_data.end(), 1);\n+  absl::c_iota(expected_out_data, 1);\n   EXPECT_THAT(out_data, ElementsAreArray(expected_out_data));\n }\n \n@@ -411,7 +411,7 @@ TEST_P(LoadedExecutableImplTest, CompileAndExecutePortable) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -445,7 +445,7 @@ TEST_P(LoadedExecutableImplTest, CompileAndExecutePortable) {\n   TF_ASSERT_OK(future.Await());\n \n   std::vector<float> expected_out_data(6);\n-  std::iota(expected_out_data.begin(), expected_out_data.end(), 1);\n+  absl::c_iota(expected_out_data, 1);\n   EXPECT_THAT(out_data, ElementsAreArray(expected_out_data));\n }\n \n@@ -464,7 +464,7 @@ TEST_P(LoadedExecutableImplTest, DoNotFillStatus) {\n   DType dtype(DType::kF32);\n   Shape shape({2, 3});\n   std::vector<float> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   Device* device = client->addressable_devices().at(0);\n   ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n \n@@ -493,7 +493,7 @@ TEST_P(LoadedExecutableImplTest, DoNotFillStatus) {\n   TF_ASSERT_OK(future.Await());\n \n   std::vector<float> expected_out_data(6);\n-  std::iota(expected_out_data.begin(), expected_out_data.end(), 1);\n+  absl::c_iota(expected_out_data, 1);\n   EXPECT_THAT(out_data, ElementsAreArray(expected_out_data));\n }\n \n@@ -556,7 +556,7 @@ module @add_sub {\n     ShardingRef sharding = SingleDeviceSharding::Create(device, MemoryKind());\n     for (int i = 0; i < 2; ++i) {\n       std::vector<int32_t> data(6);\n-      std::iota(data.begin(), data.end(), 0);\n+      absl::c_iota(data, 0);\n       TF_ASSERT_OK_AND_ASSIGN(\n           arrays.emplace_back(),\n           client->MakeArrayFromHostBuffer(\n@@ -801,7 +801,7 @@ TEST(ExecutableTest, ExecutableSerialization) {\n   xla::ifrt::Shape shard_shape({1, 3});\n   xla::ifrt::Shape shape({2, 3});\n   std::vector<int32_t> data(6);\n-  std::iota(data.begin(), data.end(), 0);\n+  absl::c_iota(data, 0);\n   std::vector<xla::ifrt::ArrayRef> input_arrays;\n \n   // Input 1 : [0, 1, 2, 3, 4, 5] sharded on device 0 and 1."
        }
    ],
    "stats": {
        "total": 247,
        "additions": 135,
        "deletions": 112
    }
}