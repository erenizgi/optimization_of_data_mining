{
    "author": "EusebioDM",
    "message": "Introduce `GpuAotCompilationResult`\n\nThis will be created by the gpu_compiler, and used for the be used for the new AOT / runtime split logic.\n\nPiperOrigin-RevId: 829445776",
    "sha": "4cbccfc8c59d180434c48f839f47263a0e910b0b",
    "files": [
        {
            "sha": "0160c564b3a421b912dc3c4d25443f51aa2c8c98",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cbccfc8c59d180434c48f839f47263a0e910b0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cbccfc8c59d180434c48f839f47263a0e910b0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=4cbccfc8c59d180434c48f839f47263a0e910b0b",
            "patch": "@@ -2026,6 +2026,53 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"gpu_aot_compilation_result\",\n+    hdrs = [\"gpu_aot_compilation_result.h\"],\n+    deps = [\n+        \":gpu_executable\",\n+        \":gpu_executable_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:executable\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"gpu_aot_compilation_result_test\",\n+    srcs = [\"gpu_aot_compilation_result_test.cc\"],\n+    deps = [\n+        \":gpu_aot_compilation_result\",\n+        \":gpu_executable\",\n+        \":launch_dimensions\",\n+        \"//xla:literal_util\",\n+        \"//xla/backends/gpu/runtime:kernel_thunk\",\n+        \"//xla/backends/gpu/runtime:sequential_thunk\",\n+        \"//xla/backends/gpu/runtime:thunk\",\n+        \"//xla/codegen/emitters:kernel_arguments\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service:executable\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:mock_platform\",\n+        \"//xla/stream_executor:mock_stream_executor\",\n+        \"//xla/stream_executor:semantic_version\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n xla_test(\n     name = \"gpu_offloading_test\",\n     srcs = [\"gpu_offloading_test.cc\"],"
        },
        {
            "sha": "fc0acde8e1646c6c2cc187bf1ea748a0295c654d",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_result.h",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cbccfc8c59d180434c48f839f47263a0e910b0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cbccfc8c59d180434c48f839f47263a0e910b0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result.h?ref=4cbccfc8c59d180434c48f839f47263a0e910b0b",
            "patch": "@@ -0,0 +1,88 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_GPU_GPU_AOT_COMPILATION_RESULT_H_\n+#define XLA_SERVICE_GPU_GPU_AOT_COMPILATION_RESULT_H_\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/memory/memory.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/gpu_executable.h\"\n+#include \"xla/service/gpu/gpu_executable.pb.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+\n+// `AotCompilationResult` implementation for GPU, containing a serialized\n+// `GpuExecutable`.\n+//\n+// Unlike `LegacyGpuAotCompilationResult`, this result contains the entire\n+// optimized executable, including the Thunks, as opposed to just the optimized\n+// HLO.\n+class GpuAotCompilationResult : public AotCompilationResult {\n+ public:\n+  static absl::StatusOr<std::unique_ptr<GpuAotCompilationResult>> Create(\n+      GpuExecutableProto executable) {\n+    TF_ASSIGN_OR_RETURN(\n+        std::unique_ptr<HloModule> module,\n+        HloModule::CreateFromProtoWithConfig(executable.hlo_module()));\n+\n+    return absl::WrapUnique(\n+        new GpuAotCompilationResult(std::move(executable), std::move(module)));\n+  }\n+\n+  absl::StatusOr<std::string> SerializeAsString() const final {\n+    std::string serialized = executable_.SerializeAsString();\n+    if (serialized.empty()) {\n+      return absl::InternalError(\"Failed to serialize GpuExecutableProto.\");\n+    }\n+    return serialized;\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<Executable>> LoadExecutable(\n+      Compiler* compiler, const se::StreamExecutor* stream_exec) &&\n+      final {\n+    return GpuExecutable::FromProto(executable_,\n+                                    stream_exec->GetDeviceDescription(),\n+                                    stream_exec->GetPlatform()->Name());\n+  }\n+\n+  const HloModule* optimized_module() const final { return hlo_module_.get(); };\n+\n+  std::unique_ptr<HloModule> consume_optimized_module() final {\n+    return std::move(hlo_module_);\n+  };\n+\n+ private:\n+  explicit GpuAotCompilationResult(GpuExecutableProto executable,\n+                                   std::unique_ptr<HloModule> hlo_module)\n+      : executable_(std::move(executable)),\n+        hlo_module_(std::move(hlo_module)) {}\n+\n+  GpuExecutableProto executable_;\n+  std::unique_ptr<HloModule> hlo_module_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_SERVICE_GPU_GPU_AOT_COMPILATION_RESULT_H_"
        },
        {
            "sha": "a6f62c1388a49f84f9d67e3975a44a4af025b9d8",
            "filename": "third_party/xla/xla/service/gpu/gpu_aot_compilation_result_test.cc",
            "status": "added",
            "additions": 164,
            "deletions": 0,
            "changes": 164,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cbccfc8c59d180434c48f839f47263a0e910b0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cbccfc8c59d180434c48f839f47263a0e910b0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_aot_compilation_result_test.cc?ref=4cbccfc8c59d180434c48f839f47263a0e910b0b",
            "patch": "@@ -0,0 +1,164 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/gpu_aot_compilation_result.h\"\n+\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n+#include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/gpu_executable.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/mock_platform.h\"\n+#include \"xla/stream_executor/mock_stream_executor.h\"\n+#include \"xla/stream_executor/semantic_version.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::stream_executor::DeviceDescription;\n+using ::stream_executor::GpuComputeCapability;\n+using ::stream_executor::MockPlatform;\n+using ::stream_executor::MockStreamExecutor;\n+using ::testing::Return;\n+using ::testing::ReturnRef;\n+using ::tsl::proto_testing::EqualsProto;\n+\n+DeviceDescription GetDeviceDescription() {\n+  DeviceDescription device_description;\n+  device_description.set_gpu_compute_capability(\n+      GpuComputeCapability{::stream_executor::CudaComputeCapability::Volta()});\n+  device_description.set_driver_version({12, 3, 0});\n+  device_description.set_runtime_version({12, 3, 0});\n+  return device_description;\n+}\n+\n+class GpuAotCompilationResultTest : public ::testing::Test {\n+ public:\n+  GpuAotCompilationResultTest() : device_description_(GetDeviceDescription()) {\n+    EXPECT_CALL(executor_, GetDeviceDescription())\n+        .WillRepeatedly(ReturnRef(device_description_));\n+    EXPECT_CALL(executor_, GetPlatform()).WillRepeatedly(Return(&platform_));\n+    EXPECT_CALL(platform_, Name()).WillRepeatedly(ReturnRef(platform_name_));\n+  }\n+\n+  // Creates a dummy GpuExecutableProto, the actual values don't matter much.\n+  absl::StatusOr<GpuExecutableProto> CreateGpuExecutableProto() {\n+    Thunk::ThunkInfo thunk_info;\n+    thunk_info.thunk_id = 123;\n+\n+    ThunkSequence thunk_sequence;\n+    thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+        thunk_info,\n+        /*kernel_name=*/\"test_kernel\", emitters::KernelArguments({}),\n+        LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0, ::stream_executor::gpu::TmaMetadata()));\n+\n+    auto hlo_module = std::make_unique<HloModule>(\"test_module_with_shape\",\n+                                                  HloModuleConfig());\n+    auto builder = HloComputation::Builder(\"entry\");\n+    auto constant = builder.AddInstruction(\n+        HloInstruction::CreateConstant(LiteralUtil::CreateR0<float>(0.0f)));\n+    hlo_module->AddEntryComputation(builder.Build(constant));\n+\n+    GpuExecutable::Params params;\n+    params.debug_module = std::move(hlo_module);\n+    params.asm_text = \"test_asm_text\";\n+    params.binary = {1, 2, 3};\n+    params.dnn_compiled_graphs = {{\"test_dnn_compiled_graph\", \"test_json\"}};\n+\n+    thunk_info.thunk_id = 456;\n+    params.executable = std::make_unique<SequentialThunk>(\n+        thunk_info, std::move(thunk_sequence));\n+    params.device_description = device_description_;\n+\n+    params.module_name = \"test_module\";\n+    params.enable_debug_info_manager = false;\n+    params.mlir_allocations = {BufferAllocation(0, 1024, 0)};\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<GpuExecutable> executable,\n+                        GpuExecutable::Create(std::move(params)));\n+    return executable->ToProto();\n+  }\n+\n+  DeviceDescription device_description_;\n+  MockStreamExecutor executor_;\n+  MockPlatform platform_;\n+  const std::string platform_name_ = \"gpu\";\n+};\n+\n+TEST_F(GpuAotCompilationResultTest, CreateAndSerialize) {\n+  TF_ASSERT_OK_AND_ASSIGN(GpuExecutableProto reference_executable,\n+                          CreateGpuExecutableProto());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<GpuAotCompilationResult> result,\n+      GpuAotCompilationResult::Create(reference_executable));\n+  TF_ASSERT_OK_AND_ASSIGN(std::string serialized_result,\n+                          result->SerializeAsString());\n+  GpuExecutableProto deserialized_executable;\n+  ASSERT_TRUE(deserialized_executable.ParseFromString(serialized_result))\n+      << \"Failed to parse serialized result.\";\n+\n+  EXPECT_THAT(deserialized_executable, EqualsProto(reference_executable));\n+}\n+\n+TEST_F(GpuAotCompilationResultTest, LoadExecutable) {\n+  TF_ASSERT_OK_AND_ASSIGN(GpuExecutableProto reference_executable,\n+                          CreateGpuExecutableProto());\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<GpuAotCompilationResult> result,\n+      GpuAotCompilationResult::Create(reference_executable));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Executable> executable,\n+      std::move(*result).LoadExecutable(/*compiler=*/nullptr, &executor_));\n+\n+  auto* gpu_executable = dynamic_cast<GpuExecutable*>(executable.get());\n+  ASSERT_NE(gpu_executable, nullptr) << \"Executable is not a GpuExecutable.\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(GpuExecutableProto executable_proto,\n+                          gpu_executable->ToProto());\n+  // HLO module is re-created from proto, and will have a new ID, so we clear\n+  // it for comparison purposes.\n+  executable_proto.mutable_hlo_module()->mutable_hlo_module()->clear_id();\n+  reference_executable.mutable_hlo_module()->mutable_hlo_module()->clear_id();\n+  EXPECT_THAT(executable_proto, EqualsProto(reference_executable));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 299,
        "additions": 299,
        "deletions": 0
    }
}