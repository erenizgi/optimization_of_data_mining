{
    "author": "vwbaker",
    "message": "Add should_autotune as a parameter to the AutotunerPass\n\nWe add backends individually to the pass and as the type of instruction the pass should autotune is dependent on the backends added, it makes more sense for these both to be configurable outside the pass. This also works towards adding an autotuning pass to autotune between BlockLevelEmitters and NativeEmitters.\n\nPiperOrigin-RevId: 799524909",
    "sha": "9a1fc124f011585c286052e6d06eeeb9fb476276",
    "files": [
        {
            "sha": "5b90d04bc9db9af12a5927bf9e902f49d1b58fcf",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=9a1fc124f011585c286052e6d06eeeb9fb476276",
            "patch": "@@ -1343,19 +1343,13 @@ cc_library(\n     srcs = [\"fusion_dispatch_pipeline.cc\"],\n     hdrs = [\"fusion_dispatch_pipeline.h\"],\n     deps = [\n-        \"//xla:shape_util\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n         \"//xla/service:hlo_cost_analysis\",\n-        \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu/transforms:fusion_block_level_rewriter\",\n         \"//xla/service/gpu/transforms:fusion_dynamic_memcpy_rewriter\",\n         \"//xla/stream_executor:device_description\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@llvm-project//llvm:Support\",\n     ],\n )\n \n@@ -1451,6 +1445,8 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/backends/gpu/autotuner:block_level_emitter\",\n         \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n@@ -1577,6 +1573,7 @@ cc_library(\n         \"//xla/service:while_loop_constant_sinking\",\n         \"//xla/service:while_loop_simplifier\",\n         \"//xla/service/debug:unstable_reduction_detector\",\n+        \"//xla/service/gpu/autotuning:autotuner_pass\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n         \"//xla/service/gpu/autotuning:custom_kernel_fusion_autotuner\",\n         \"//xla/service/gpu/model:collective_ptable_stats_collection\",\n@@ -1861,6 +1858,7 @@ cc_library(\n     ],\n     deps = [\n         \":alias_info\",\n+        \":cublas_cudnn\",\n         \":cublas_padding_requirements\",\n         \":gpu_compiler\",\n         \":ir_emission_utils\",\n@@ -1873,9 +1871,10 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/backends/gpu/autotuner:block_level_emitter\",\n         \"//xla/backends/gpu/autotuner:cublas\",\n         \"//xla/backends/gpu/autotuner:cublaslt\",\n-        \"//xla/backends/gpu/autotuner:factory\",\n+        \"//xla/backends/gpu/autotuner:native_emitter\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n@@ -2111,6 +2110,7 @@ cc_library(\n     ],\n     deps = [\n         \":alias_info\",\n+        \":cublas_cudnn\",\n         \":cublas_padding_requirements\",\n         \":gpu_compiler\",\n         \":target_constants\","
        },
        {
            "sha": "89e7dbf2fff803c726df257f6a8a55915c0e2bd5",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=9a1fc124f011585c286052e6d06eeeb9fb476276",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/conv_algorithm_picker.h\"\n #include \"xla/service/gpu/autotuning/gemm_algorithm_picker.h\"\n #include \"xla/service/gpu/autotuning/gemm_fusion_autotuner.h\"\n+#include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/cublas_padding_requirements.h\"\n #include \"xla/service/gpu/gpu_compiler.h\"\n #include \"xla/service/gpu/llvm_gpu_backend/amdgpu_backend.h\"\n@@ -266,11 +267,15 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n   if (debug_options.xla_gpu_experimental_use_autotuner_pass()) {\n     backends.push_back(\n         std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n+    auto should_autotune = [](const HloInstruction& instruction) -> bool {\n+      return instruction.opcode() == HloOpcode::kCustomCall &&\n+             IsCublasGemm(instruction);\n+    };\n     TF_ASSIGN_OR_RETURN(\n         std::unique_ptr<AutotunerPass> autotuner_pass,\n         AutotunerPass::Create(std::move(backends), debug_options,\n                               options.device_allocator, stream_exec,\n-                              thread_pool));\n+                              thread_pool, should_autotune));\n     pipeline->AddPass(std::move(autotuner_pass));\n   } else {\n     pipeline->AddPass<GemmAlgorithmPicker>(autotune_config);"
        },
        {
            "sha": "1a17bc616aede26c424dc177cd21598b9280d2b6",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=9a1fc124f011585c286052e6d06eeeb9fb476276",
            "patch": "@@ -319,13 +319,15 @@ xla_test(\n     tags = [\"cuda-only\"],\n     deps = [\n         \":autotuner_pass\",\n+        \"//xla/backends/autotuner\",\n         \"//xla/backends/autotuner:autotuner_cache_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/backends/gpu/autotuner:cublas\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:cublas_cudnn\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -340,7 +342,6 @@ xla_test(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:path\",\n-        \"@local_tsl//tsl/platform:status\",\n     ],\n )\n \n@@ -796,7 +797,6 @@ cc_library(\n         \"//xla/backends/gpu/autotuner:gpu_profiler\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n-        \"//xla/service/gpu:cublas_cudnn\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "99e0b9fc14592b654f9482aea24fd57809ae8e51",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=9a1fc124f011585c286052e6d06eeeb9fb476276",
            "patch": "@@ -34,8 +34,6 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/gpu_profiler.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -49,7 +47,7 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     std::vector<std::unique_ptr<CodegenBackend>> backends,\n     const DebugOptions& debug_options, se::DeviceMemoryAllocator* allocator,\n     stream_executor::StreamExecutor* stream_executor,\n-    tsl::thread::ThreadPool* thread_pool) {\n+    tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune) {\n   std::unique_ptr<GpuProfiler> profiler =\n       GpuProfiler::Create(stream_executor, allocator, ProfileOptions());\n \n@@ -83,20 +81,16 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n       std::unique_ptr<Autotuner> autotuner,\n       Autotuner::Create(std::move(backends), std::move(profiler),\n                         AutotuneConfig(), std::move(cache), thread_pool));\n-  return absl::WrapUnique(new AutotunerPass(std::move(autotuner)));\n+  return absl::WrapUnique(\n+      new AutotunerPass(std::move(autotuner), should_autotune));\n }\n \n absl::StatusOr<bool> AutotunerPass::Run(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   VLOG(1) << \"Running Autotuner Pass\";\n \n-  auto should_autotune = [](const HloInstruction& instruction) -> bool {\n-    return instruction.opcode() == HloOpcode::kCustomCall &&\n-           IsCublasGemm(instruction);\n-  };\n-\n-  TF_RETURN_IF_ERROR(autotuner_->Autotune(module, should_autotune));\n+  TF_RETURN_IF_ERROR(autotuner_->Autotune(module, should_autotune_));\n   return true;\n }\n "
        },
        {
            "sha": "fc1118be12a4fc0f24b61709a9650c6cc5f55c22",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=9a1fc124f011585c286052e6d06eeeb9fb476276",
            "patch": "@@ -40,8 +40,8 @@ class AutotunerPass : public HloModulePass {\n   static absl::StatusOr<std::unique_ptr<AutotunerPass>> Create(\n       std::vector<std::unique_ptr<CodegenBackend>> backends,\n       const DebugOptions& debug_options, se::DeviceMemoryAllocator* allocator,\n-      se::StreamExecutor* stream_executor,\n-      tsl::thread::ThreadPool* thread_pool);\n+      se::StreamExecutor* stream_executor, tsl::thread::ThreadPool* thread_pool,\n+      InstructionFilterFn should_autotune);\n \n   absl::string_view name() const override { return \"autotuner\"; }\n \n@@ -51,10 +51,12 @@ class AutotunerPass : public HloModulePass {\n       const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n \n  private:\n-  explicit AutotunerPass(std::unique_ptr<Autotuner> autotuner)\n-      : autotuner_(std::move(autotuner)) {}\n+  explicit AutotunerPass(std::unique_ptr<Autotuner> autotuner,\n+                         InstructionFilterFn should_autotune)\n+      : autotuner_(std::move(autotuner)), should_autotune_(should_autotune) {}\n \n   std::unique_ptr<Autotuner> autotuner_;\n+  InstructionFilterFn should_autotune_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "340bc3730c018f0ce5f3b6fd515ddb8f4c07a8aa",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 76,
            "deletions": 49,
            "changes": 125,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=9a1fc124f011585c286052e6d06eeeb9fb476276",
            "patch": "@@ -26,13 +26,16 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/strings/ascii.h\"\n+#include \"xla/backends/autotuner/autotuner.h\"\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n@@ -59,6 +62,11 @@ se::StreamExecutor* GpuExecutor() {\n   return platform->ExecutorForDevice(0).value();\n }\n \n+bool IsCublasGemmInstruction(const HloInstruction& instruction) {\n+  return instruction.opcode() == HloOpcode::kCustomCall &&\n+         IsCublasGemm(instruction);\n+}\n+\n class AutotunerPassTest : public HloHardwareIndependentTestBase {\n  protected:\n   AutotunerPassTest()\n@@ -71,28 +79,29 @@ class AutotunerPassTest : public HloHardwareIndependentTestBase {\n   NVPTXCompiler compiler_;\n };\n \n-TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n-  const char kCublasCustomCallHlo[] = R\"(\n-    HloModule module, entry_computation_layout={(f32[100,100]{1,0}, f32[100,100]{1,0})->f32[100,100]{1,0}}\n-\n-    ENTRY %main (arg0: f32[100,100], arg1: f32[100,100]) -> f32[100,100] {\n-      %arg0 = f32[100,100]{1,0} parameter(0)\n-      %arg1 = f32[100,100]{1,0} parameter(1)\n-      %custom-call.1 = (f32[100,100]{1,0}, s8[80000]{0}) custom-call(%arg0, %arg1),\n-      custom_call_target=\"__cublas$gemm\",\n-      backend_config={\n-        \"gemm_backend_config\":{\n-          \"dot_dimension_numbers\":\n-            {\n-              \"lhs_contracting_dimensions\":[\"1\"],\n-              \"rhs_contracting_dimensions\":[\"0\"],\n-              \"lhs_batch_dimensions\":[],\n-              \"rhs_batch_dimensions\":[]\n-          }\n-        }\n+const char kCublasCustomCallHlo[] = R\"(\n+HloModule module, entry_computation_layout={(f32[100,100]{1,0}, f32[100,100]{1,0})->f32[100,100]{1,0}}\n+\n+ENTRY %main (arg0: f32[100,100], arg1: f32[100,100]) -> f32[100,100] {\n+  %arg0 = f32[100,100]{1,0} parameter(0)\n+  %arg1 = f32[100,100]{1,0} parameter(1)\n+  %custom-call.1 = (f32[100,100]{1,0}, s8[80000]{0}) custom-call(%arg0, %arg1),\n+  custom_call_target=\"__cublas$gemm\",\n+  backend_config={\n+    \"gemm_backend_config\":{\n+      \"dot_dimension_numbers\":\n+        {\n+          \"lhs_contracting_dimensions\":[\"1\"],\n+          \"rhs_contracting_dimensions\":[\"0\"],\n+          \"lhs_batch_dimensions\":[],\n+          \"rhs_batch_dimensions\":[]\n       }\n-      ROOT %get-tuple-element = f32[100,100]{1,0} get-tuple-element(%custom-call.1), index=0\n-    })\";\n+    }\n+  }\n+  ROOT %get-tuple-element = f32[100,100]{1,0} get-tuple-element(%custom-call.1), index=0\n+})\";\n+\n+TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kCublasCustomCallHlo));\n \n@@ -106,33 +115,49 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n       std::unique_ptr<AutotunerPass> pass,\n       AutotunerPass::Create(std::move(backends),\n                             module->config().debug_options(), allocator_.get(),\n-                            stream_executor_, &thread_pool));\n+                            stream_executor_, &thread_pool,\n+                            IsCublasGemmInstruction));\n   EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n               tsl::testing::IsOkAndHolds(true));\n+  // Verify that the backend config has been updated in the HLO.\n+  auto gemm =\n+      module->entry_computation()->GetInstructionWithName(\"custom-call.1\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_backend_config_after_first_run,\n+                          gemm->backend_config<GpuBackendConfig>());\n+  ASSERT_TRUE(gpu_backend_config_after_first_run.gemm_backend_config()\n+                  .has_selected_algorithm());\n+}\n+\n+TEST_F(AutotunerPassTest, CublasGemmIsNotAutotunedWhenFilterReturnsFalse) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kCublasCustomCallHlo));\n+\n+  tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n+                                      /*num_threads=*/4);\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::make_unique<CublasBackend>(\n+      stream_executor_, &module->config().debug_options(), &compiler_));\n+\n+  auto should_autotune = [](const HloInstruction& instruction) {\n+    return false;\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<AutotunerPass> pass,\n+      AutotunerPass::Create(std::move(backends),\n+                            module->config().debug_options(), allocator_.get(),\n+                            stream_executor_, &thread_pool, should_autotune));\n+  EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n+              tsl::testing::IsOkAndHolds(true));\n+  // Verify that the backend config has *not* been updated in the HLO.\n+  auto gemm =\n+      module->entry_computation()->GetInstructionWithName(\"custom-call.1\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_backend_config_after_first_run,\n+                          gemm->backend_config<GpuBackendConfig>());\n+  ASSERT_FALSE(gpu_backend_config_after_first_run.gemm_backend_config()\n+                   .has_selected_algorithm());\n }\n \n TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n-  const char kCublasCustomCallHlo[] = R\"(\n-    HloModule module, entry_computation_layout={(f32[100,100]{1,0}, f32[100,100]{1,0})->f32[100,100]{1,0}}\n-\n-    ENTRY %main (arg0: f32[100,100], arg1: f32[100,100]) -> f32[100,100] {\n-      %arg0 = f32[100,100]{1,0} parameter(0)\n-      %arg1 = f32[100,100]{1,0} parameter(1)\n-      %custom-call.1 = (f32[100,100]{1,0}, s8[80000]{0}) custom-call(%arg0, %arg1),\n-      custom_call_target=\"__cublas$gemm\",\n-      backend_config={\n-        \"gemm_backend_config\":{\n-          \"dot_dimension_numbers\":\n-            {\n-              \"lhs_contracting_dimensions\":[\"1\"],\n-              \"rhs_contracting_dimensions\":[\"0\"],\n-              \"lhs_batch_dimensions\":[],\n-              \"rhs_batch_dimensions\":[]\n-          }\n-        }\n-      }\n-      ROOT %get-tuple-element = f32[100,100]{1,0} get-tuple-element(%custom-call.1), index=0\n-    })\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kCublasCustomCallHlo));\n \n@@ -153,9 +178,10 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n \n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass,\n-        AutotunerPass::Create(\n-            std::move(backends), module->config().debug_options(),\n-            allocator_.get(), stream_executor_, &thread_pool));\n+        AutotunerPass::Create(std::move(backends),\n+                              module->config().debug_options(),\n+                              allocator_.get(), stream_executor_, &thread_pool,\n+                              IsCublasGemmInstruction));\n     EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n                 tsl::testing::IsOkAndHolds(true));\n   }\n@@ -208,9 +234,10 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n         stream_executor_, &module->config().debug_options(), &compiler_));\n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass2,\n-        AutotunerPass::Create(\n-            std::move(backends2), module->config().debug_options(),\n-            allocator_.get(), stream_executor_, &thread_pool));\n+        AutotunerPass::Create(std::move(backends2),\n+                              module->config().debug_options(),\n+                              allocator_.get(), stream_executor_, &thread_pool,\n+                              IsCublasGemmInstruction));\n     EXPECT_THAT(pass2->Run(module.get(), /*execution_threads=*/{}),\n                 tsl::testing::IsOkAndHolds(true));\n   }"
        },
        {
            "sha": "e713c2eda5f69b52ae342251ad817f6cc2aae6d3",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a1fc124f011585c286052e6d06eeeb9fb476276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=9a1fc124f011585c286052e6d06eeeb9fb476276",
            "patch": "@@ -43,7 +43,6 @@ limitations under the License.\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n-#include \"xla/backends/gpu/autotuner/factory.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/pass/hlo_pass_fix.h\"\n@@ -65,6 +64,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/conv_algorithm_picker.h\"\n #include \"xla/service/gpu/autotuning/gemm_algorithm_picker.h\"\n #include \"xla/service/gpu/autotuning/gemm_fusion_autotuner.h\"\n+#include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/cublas_padding_requirements.h\"\n #include \"xla/service/gpu/gpu_compiler.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n@@ -370,11 +370,15 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n         std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n     backends.push_back(\n         std::make_unique<CublasLtBackend>(stream_exec, &debug_options, this));\n+    auto should_autotune = [](const HloInstruction& instruction) -> bool {\n+      return instruction.opcode() == HloOpcode::kCustomCall &&\n+             IsCublasGemm(instruction);\n+    };\n     TF_ASSIGN_OR_RETURN(\n         std::unique_ptr<AutotunerPass> autotuner_pass,\n         AutotunerPass::Create(std::move(backends), debug_options,\n                               options.device_allocator, stream_exec,\n-                              thread_pool));\n+                              thread_pool, should_autotune));\n     pipeline->AddPass(std::move(autotuner_pass));\n   } else {\n     // On Ampere or later, GemmAlgorithmPicker just provides a way to \"warmup\""
        }
    ],
    "stats": {
        "total": 182,
        "additions": 107,
        "deletions": 75
    }
}