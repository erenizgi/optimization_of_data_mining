{
    "author": "basioli-k",
    "message": "[XLA:CPU] Erase legacy compute function path in cpu_executable.\n\nThis has been unused for a while now.\n\nPiperOrigin-RevId: 819276283",
    "sha": "c61dd4a8305ef543814c109748319aa015aa62f1",
    "files": [
        {
            "sha": "b6c518d0f1ff1d71251b58baa903529178c6cc28",
            "filename": "tensorflow/lite/java/src/main/native/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c61dd4a8305ef543814c109748319aa015aa62f1/tensorflow%2Flite%2Fjava%2Fsrc%2Fmain%2Fnative%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c61dd4a8305ef543814c109748319aa015aa62f1/tensorflow%2Flite%2Fjava%2Fsrc%2Fmain%2Fnative%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fjava%2Fsrc%2Fmain%2Fnative%2FBUILD?ref=c61dd4a8305ef543814c109748319aa015aa62f1",
            "patch": "@@ -94,7 +94,6 @@ cc_library_with_tflite(\n     linkopts = [\n         \"-lm\",\n         \"-ldl\",\n-        \"-Wl,-z,max-page-size=16384\",\n     ],\n     tflite_deps = [\n         \":jni_utils\","
        },
        {
            "sha": "65fcde98a47f65f4f39cd2178329e6b0ae6630fa",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 90,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c61dd4a8305ef543814c109748319aa015aa62f1/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c61dd4a8305ef543814c109748319aa015aa62f1/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc?ref=c61dd4a8305ef543814c109748319aa015aa62f1",
            "patch": "@@ -83,34 +83,6 @@ limitations under the License.\n namespace xla {\n namespace cpu {\n \n-absl::StatusOr<std::unique_ptr<CpuExecutable>> CpuExecutable::Create(\n-    std::unique_ptr<FunctionLibrary> function_library,\n-    std::unique_ptr<BufferAssignment> assignment,\n-    std::unique_ptr<HloModule> hlo_module,\n-    const std::string& entry_function_name,\n-    std::unique_ptr<HloProfilePrinterData> hlo_profile_printer_data,\n-    std::unique_ptr<HloProfileIndexMap> hlo_profile_index_map) {\n-  VLOG(2) << \"Create CpuExecutable from a jit compiled function: \"\n-          << entry_function_name << \", module=\" << hlo_module->name();\n-\n-  std::unique_ptr<CpuExecutable> executable(new CpuExecutable(\n-      std::move(hlo_module), std::move(hlo_profile_printer_data),\n-      std::move(hlo_profile_index_map), std::move(assignment)));\n-  executable->function_library_ = std::move(function_library);\n-  executable->module_name_ = entry_function_name;\n-\n-  TF_ASSIGN_OR_RETURN(\n-      executable->compute_function_,\n-      executable->function_library_\n-          ->ResolveFunction<std::remove_pointer_t<ComputeFunctionType>>(\n-              entry_function_name));\n-\n-  VLOG(1) << \"compute_function_ at address \"\n-          << reinterpret_cast<void*>(executable->compute_function_);\n-\n-  return executable;\n-}\n-\n absl::StatusOr<std::unique_ptr<CpuExecutable>> CpuExecutable::Create(\n     std::unique_ptr<FunctionLibrary> function_library,\n     std::unique_ptr<BufferAssignment> assignment,\n@@ -246,60 +218,6 @@ CpuExecutable::CreateBufferTable(se::DeviceMemoryAllocator* memory_allocator,\n   return std::move(buffers);\n }\n \n-absl::Status CpuExecutable::ExecuteComputeFunction(\n-    const ExecutableRunOptions* run_options,\n-    absl::Span<MaybeOwningDeviceMemory const> buffers) {\n-  uint64_t start_micros = tsl::Env::Default()->NowMicros();\n-\n-  size_t profile_counters_size = 0;\n-  int64_t* profile_counters = nullptr;\n-\n-  // Call the computation function following the calling convention. See the\n-  // definition of 'ComputeFunctionType' for the details of the calling\n-  // convention of JITed functions.\n-  std::vector<void*> buffer_pointers;\n-  for (auto& buffer : buffers) {\n-    buffer_pointers.push_back(\n-        const_cast<void*>(buffer.AsDeviceMemoryBase().opaque()));\n-  }\n-\n-  VLOG(3) << \"Executing compute function:\";\n-  VLOG(3) << absl::StrFormat(\"  Number of buffer table entries: %u\",\n-                             buffer_pointers.size());\n-  auto ptr_printer = [](std::string* out, const void* p) {\n-    absl::StrAppend(out, absl::StrFormat(\"%p\", p));\n-  };\n-  VLOG(3) << absl::StrFormat(\"  Buffer table: [%s]\",\n-                             absl::StrJoin(buffer_pointers, \", \", ptr_printer));\n-  VLOG(3) << absl::StrFormat(\"  Number of profile counters: %u\",\n-                             profile_counters_size);\n-  VLOG(3) << absl::StrFormat(\"  Profile counters: %p\", profile_counters);\n-\n-  auto record_profile = [&]() {\n-    uint64_t end_micros = tsl::Env::Default()->NowMicros();\n-    if (run_options->execution_profile()) {\n-      const double nanoseconds = (end_micros - start_micros) * 1000.0;\n-      run_options->execution_profile()->set_compute_time_ns(\n-          std::max(nanoseconds, 1.0));\n-    }\n-  };\n-\n-  XlaCustomCallStatus status;\n-  // For the entry computation (like all global computations), all inputs and\n-  // outputs are in the buffer table, and both the result pointer and args\n-  // array pointers are unused (so we set them to 'nullptr').\n-  compute_function_(nullptr, run_options, nullptr, buffer_pointers.data(),\n-                    &status, profile_counters);\n-  record_profile();\n-  std::optional<absl::string_view> error_message =\n-      CustomCallStatusGetMessage(&status);\n-  if (error_message) {\n-    return Internal(\"CustomCall failed: %s\", *error_message);\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n absl::Status CpuExecutable::ExecuteThunks(\n     const ExecutableRunOptions* run_options,\n     absl::Span<MaybeOwningDeviceMemory const> buffers) {\n@@ -532,14 +450,8 @@ absl::StatusOr<ExecutionOutput> CpuExecutable::ExecuteAsyncOnStream(\n   tsl::port::ScopedFlushDenormal flush;\n   tsl::port::ScopedSetRound round(FE_TONEAREST);\n \n-  if (has_compute_function()) {\n-    TF_RETURN_IF_ERROR(\n-        ExecuteComputeFunction(&run_options->run_options(), buffers));\n-  } else if (has_thunks()) {\n-    TF_RETURN_IF_ERROR(ExecuteThunks(&run_options->run_options(), buffers));\n-  } else {\n-    return Internal(\"No compute function or thunks found.\");\n-  }\n+  DCHECK(has_thunks());\n+  TF_RETURN_IF_ERROR(ExecuteThunks(&run_options->run_options(), buffers));\n \n   MarkToBeReleasedArguments(absl::MakeSpan(arguments), result);\n   return std::move(result);"
        },
        {
            "sha": "8723ad6f6fa39d83d57cc7090220a64f475629cf",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.h",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c61dd4a8305ef543814c109748319aa015aa62f1/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c61dd4a8305ef543814c109748319aa015aa62f1/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h?ref=c61dd4a8305ef543814c109748319aa015aa62f1",
            "patch": "@@ -55,16 +55,6 @@ namespace cpu {\n // architecture, so JIT-ed code and host code share the same ABI.\n class CpuExecutable : public Executable {\n  public:\n-  // Creates a CpuExecutable from JIT compiled cpu function by resolving\n-  // `entry_function_name` in the `jit`.\n-  static absl::StatusOr<std::unique_ptr<CpuExecutable>> Create(\n-      std::unique_ptr<FunctionLibrary> function_library,\n-      std::unique_ptr<BufferAssignment> assignment,\n-      std::unique_ptr<HloModule> hlo_module,\n-      const std::string& entry_function_name,\n-      std::unique_ptr<HloProfilePrinterData> hlo_profile_printer_data,\n-      std::unique_ptr<HloProfileIndexMap> hlo_profile_index_map);\n-\n   // Creates a CpuExecutable from a thunk sequence.\n   static absl::StatusOr<std::unique_ptr<CpuExecutable>> Create(\n       std::unique_ptr<FunctionLibrary> function_library,\n@@ -80,12 +70,6 @@ class CpuExecutable : public Executable {\n       const ServiceExecutableRunOptions* run_options,\n       std::vector<ExecutionInput> arguments) override;\n \n-  // Calls the generated function performing the computation with the given\n-  // arguments using the supplied buffers.\n-  absl::Status ExecuteComputeFunction(\n-      const ExecutableRunOptions* run_options,\n-      absl::Span<MaybeOwningDeviceMemory const> buffers);\n-\n   // Calls emitted thunk sequence with the given arguments using the supplied\n   // buffers.\n   absl::Status ExecuteThunks(const ExecutableRunOptions* run_options,\n@@ -140,15 +124,6 @@ class CpuExecutable : public Executable {\n \n   static int64_t ShapeSizeBytes(const Shape& shape);\n \n-  // Type of the computation function we expect in the JIT.\n-  using ComputeFunctionType =\n-      void (*)(void* /*result*/, const ExecutableRunOptions* /*run_options*/,\n-               const void** /*args*/, void** /*buffer_table*/,\n-               XlaCustomCallStatus* /*status*/, int64_t* /*profile_counters*/);\n-\n-  bool has_compute_function() const { return compute_function_ != nullptr; }\n-  ComputeFunctionType compute_function() const { return compute_function_; }\n-\n   bool has_thunks() const { return thunks_.has_value(); }\n   ThunkExecutor& thunks() { return *thunks_; }\n \n@@ -247,9 +222,6 @@ class CpuExecutable : public Executable {\n   // We are currently transitioning from (1) to (2) with a long term plan to\n   // unify thunk-based runtime with all XLA backends.\n \n-  // A function pointer to the jit-compiled entry function.\n-  ComputeFunctionType compute_function_ = nullptr;\n-\n   // A thunk executor created from the compiled thunk sequence.\n   std::optional<ThunkExecutor> thunks_;\n   // Vector indexed by BufferAllocation::Index for efficient access."
        }
    ],
    "stats": {
        "total": 121,
        "additions": 2,
        "deletions": 119
    }
}