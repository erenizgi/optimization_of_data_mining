{
    "author": "Tixxx",
    "message": "PR #26196: force delay scheduling start to extend overlap interval\n\nImported from GitHub PR https://github.com/openxla/xla/pull/26196\n\nðŸ“ Summary of Changes\nThis pr introduces a new heuristics to delay scheduling start based on the op type when the overlap limit is larger than 1 and default cost model is used.\n\nðŸŽ¯ Justification\nThe goal is to extend the overlap interval as much as possible to include more computes and provide the best out of the box scheduling.\nðŸš€ Kind of Contribution\n âš¡ï¸ Performance Improvement\n\nðŸ“Š Benchmark (for Performance Improvements)\nhlo_llama31_8b_bf16_1x8.hlo -> 1359ms -> 1318.3ms\nhlo_llama31_8b_bf16_2x8.hlo -> 1366ms -> 1321.4ms\nhlo_llama31_8b_fp8_1x8.hlo -> 1123ms -> 1089ms\nhlo_llama31_8b_fp8_2x8.hlo -> 1141ms -> 1.95.5ms\nðŸ§ª Unit Tests:\nAdded unit tests\n\nðŸ§ª Execution Tests:\nincluded\n\nCopybara import of the project:\n\n--\n28d5f58a06b1725fafd981f22fe4b72680c34b0c by TJ Xu <tjx@nvidia.com>:\n\nAdd a new LHS config to priotitize computes nodes over collective starts\nwhen overlap limit is larger than 1 nad LHS is using default cost model\n\n--\nf40aed823cea4bf190fe96860ea59db24e6953a7 by TJ Xu <tjx@nvidia.com>:\n\nrefactor the local lambda in readysetlt to be a proper function\n\n--\n4ec19f629ce116a92b36e06505e1c0df51369d13 by TJ Xu <tjx@nvidia.com>:\n\nupdated the unit test to be more specific\n\nMerging this change closes #26196\n\nPiperOrigin-RevId: 842588981",
    "sha": "1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff",
    "files": [
        {
            "sha": "304da6a2627c0faef40ae30bffc4005c0feb8905",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff",
            "patch": "@@ -582,7 +582,12 @@ absl::Status RunLatencyHidingSchedulerPasses(\n     pipeline.AddPass<PGLEAccuracyChecker>(\n         dynamic_cast<ProfileGuidedLatencyEstimator&>(*estimator));\n   }\n-\n+  // If overlap limit is set to be greater than 1 and the default t-short size\n+  // estimator is used we will tell LHS to extend async-done intervals as much\n+  // as possible to start collectives as early as possible.\n+  if (config.parallel_collective_overlap_limit > 1) {\n+    config.prioritize_compute_over_async_start = true;\n+  }\n   auto async_tracker = std::make_unique<GpuAsyncTracker>(config);\n \n   std::shared_ptr<const SchedulingContext> scheduling_context ="
        },
        {
            "sha": "ac1185896e2eadcb69ce6fad56ceade1f4448901",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc?ref=1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff",
            "patch": "@@ -75,6 +75,8 @@ class GpuLatencyHidingSchedulerBaseTest\n     DebugOptions& options = module->mutable_config().mutable_debug_options();\n     options.set_xla_gpu_experimental_parallel_collective_overlap_limit(\n         num_parallel_resources);\n+    options.set_xla_gpu_enable_analytical_sol_latency_estimator(false);\n+\n     options.set_xla_gpu_pgle_accuracy_checker(strictness);\n \n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, /*pointer_size=*/8,\n@@ -1041,5 +1043,89 @@ TEST_F(GpuLatencyHidingSchedulerBaseTest, ParallelThreadsShouldBeScheduled) {\n   TF_EXPECT_OK(ScheduleModule(module.get()));\n }\n \n+TEST_F(GpuLatencyHidingSchedulerBaseTest,\n+       MultipleParallelAsyncsExtendedOverAllComputes) {\n+  absl::string_view kHloModule = R\"(\n+HloModule m\n+reduce {\n+x = f32[] parameter(0)\n+y = f32[] parameter(1)\n+ROOT _ = f32[] add(x, y)\n+}\n+ENTRY main {\n+p0 = f32[] parameter(0)\n+p1 = f32[2] parameter(1)\n+p2 = f32[2] parameter(2)\n+p3 = f32[2] parameter(3)\n+p4 = f32[2] parameter(4)\n+p5 = f32[2] parameter(5)\n+p6 = f32[2] parameter(6)\n+ar_0 = f32[] all-reduce-start(p0), to_apply=reduce\n+ar_1 = f32[] all-reduce-done(ar_0)\n+add_2 = f32[2] add(p1, p6)\n+\n+ar_2 = f32[2] all-reduce-start(add_2), to_apply=reduce\n+ar_3 = f32[2] all-reduce-done(ar_2)\n+add_3 = f32[2] add(p1, p3)\n+\n+rs_0 = ((f32[2]), f32[1]) reduce-scatter-start(add_3), to_apply=reduce,\n+dimensions={0}\n+rs_1 = f32[1] reduce-scatter-done(rs_0)\n+add_0 = f32[2] add(p1, p2)\n+div_0 = f32[2] divide(p3, p4)\n+mul_0 = f32[2] multiply(p4, p5)\n+ROOT _ = (f32[], f32[2], f32[1], f32[2], f32[2], f32[2]) tuple(ar_1, ar_3, rs_1, add_0, div_0, mul_0)\n+}\n+)\";\n+  absl::string_view kFdoProfile = \"\";\n+\n+  auto config = GetModuleConfig(kFdoProfile);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloModule, config));\n+\n+  TF_EXPECT_OK(ScheduleModule(module.get(), /*num_parallel_resources=*/16));\n+  auto schedule = module->schedule();\n+  std::vector<HloInstruction*> instruction_sequence =\n+      schedule.sequence(module->entry_computation()).instructions();\n+  // With a lot of parallel resources and default latency estimator,\n+  // LHS will try to extend all asyncs as much as possible.\n+  // We expect all computes to be wrapped within all async start-done\n+  // intervals.\n+  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"add_2\") >\n+                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n+              GetIndexByName(instruction_sequence, \"add_3\") >\n+                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n+              GetIndexByName(instruction_sequence, \"add_2\") <\n+                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n+              GetIndexByName(instruction_sequence, \"add_3\") <\n+                  GetIndexByName(instruction_sequence, \"ar_1\"));\n+\n+  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"add_0\") >\n+                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n+              GetIndexByName(instruction_sequence, \"add_0\") >\n+                  GetIndexByName(instruction_sequence, \"rs_0\") &&\n+              GetIndexByName(instruction_sequence, \"add_0\") <\n+                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n+              GetIndexByName(instruction_sequence, \"add_0\") <\n+                  GetIndexByName(instruction_sequence, \"rs_1\"));\n+\n+  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"div_0\") >\n+                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n+              GetIndexByName(instruction_sequence, \"div_0\") >\n+                  GetIndexByName(instruction_sequence, \"rs_0\") &&\n+              GetIndexByName(instruction_sequence, \"div_0\") <\n+                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n+              GetIndexByName(instruction_sequence, \"div_0\") <\n+                  GetIndexByName(instruction_sequence, \"rs_1\"));\n+  EXPECT_TRUE(GetIndexByName(instruction_sequence, \"mul_0\") >\n+                  GetIndexByName(instruction_sequence, \"ar_0\") &&\n+              GetIndexByName(instruction_sequence, \"mul_0\") >\n+                  GetIndexByName(instruction_sequence, \"rs_0\") &&\n+              GetIndexByName(instruction_sequence, \"mul_0\") <\n+                  GetIndexByName(instruction_sequence, \"ar_1\") &&\n+              GetIndexByName(instruction_sequence, \"mul_0\") <\n+                  GetIndexByName(instruction_sequence, \"rs_1\"));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "f7500d9a46267e5b24694fbb25a6a012f07d7c9b",
            "filename": "third_party/xla/xla/service/latency_hiding_scheduler.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.cc?ref=1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff",
            "patch": "@@ -1262,6 +1262,29 @@ class ReadySetLt {\n     return std::nullopt;\n   }\n \n+  inline std::optional<bool> DelayAsyncStartCandidateCondition(\n+      DefaultSchedulerCore::ScheduleCandidate& a,\n+      DefaultSchedulerCore::ScheduleCandidate& b, const HloGraphNode* a_node,\n+      const HloGraphNode* b_node, const char** reason) const {\n+    bool a_has_async_resource =\n+        a_node->DoesReleaseAnyResource() && !IsResourceConstrained(a, a_node);\n+    bool b_has_async_resource =\n+        b_node->DoesReleaseAnyResource() && !IsResourceConstrained(b, b_node);\n+\n+    CMP_EXPLICIT(!a_has_async_resource, !b_has_async_resource,\n+                 \"kDelayAsyncStartForCompute\");\n+    if (a_has_async_resource && b_has_async_resource) {\n+      // If 2 nodes are both async nodes, we prioritize the one\n+      // with more depth to free up more computes to overlap\n+      // with the one with less depth which can be launched\n+      // early\n+      CMP_EXPLICIT(a_node->GetDepth() > b_node->GetDepth(),\n+                   b_node->GetDepth() > a_node->GetDepth(),\n+                   \"kDelayAsyncStartForDepth\");\n+    }\n+    return std::nullopt;\n+  }\n+\n   // The comparison here implements the priority for the nodes in the ready\n   // set. The function compares a and b in a series of prioritized\n   // comparisons. As soon as it finds one that is not equal, it stops.  If\n@@ -1371,6 +1394,16 @@ class ReadySetLt {\n                    AsyncDepth0CandidateCondition(b, bn), \"kStartAtZeroDepth\");\n     }\n \n+    if (sched_state_.config.aggressive_scheduling_policies &&\n+        sched_state_.config.prioritize_compute_over_async_start) {\n+      // If an instruction releasing a resource is not resource constrained,\n+      // delay it as much as possible.\n+      if (auto value =\n+              DelayAsyncStartCandidateCondition(a, b, an, bn, reason)) {\n+        return *value;\n+      }\n+    }\n+\n     auto a_readytime = an->GetReadyTime();\n     auto b_readytime = bn->GetReadyTime();\n     if (a_readytime != b_readytime) {  // Quick test to avoid lots of work"
        },
        {
            "sha": "2930dfd2277e3d434de17713fadcc0e64def1a72",
            "filename": "third_party/xla/xla/service/latency_hiding_scheduler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flatency_hiding_scheduler.h?ref=1144cc69a2b8caa8f2a5f99cd29cc22882fc73ff",
            "patch": "@@ -149,6 +149,9 @@ struct SchedulerConfig {\n   bool use_real_cost_model = false;\n   bool aggressive_scheduling_policies = false;\n   bool prioritize_async_depth_over_stall = false;\n+\n+  bool prioritize_compute_over_async_start = false;\n+\n   bool enable_release_start_policy = false;\n   bool resource_sharing = false;\n   bool resource_serializing = false;"
        }
    ],
    "stats": {
        "total": 129,
        "additions": 128,
        "deletions": 1
    }
}