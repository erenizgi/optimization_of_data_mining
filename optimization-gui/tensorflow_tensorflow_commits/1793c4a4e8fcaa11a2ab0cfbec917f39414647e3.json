{
    "author": "ezhulenev",
    "message": "[xla:pjrt] Migtate PjRt GPU to xla::Future and xla::Promise\n\nPiperOrigin-RevId: 811896949",
    "sha": "1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
    "files": [
        {
            "sha": "b1b57971b881468cfc8e2a3b7a45b811f09893a4",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -60,6 +60,7 @@ cc_library(\n         \":gpu_topology_proto_cc\",\n         \":se_gpu_topology_description\",\n         \"//xla:executable_run_options\",\n+        \"//xla:future\",\n         \"//xla:literal\",\n         \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n@@ -88,7 +89,6 @@ cc_library(\n         \"//xla/pjrt:pjrt_compiler\",\n         \"//xla/pjrt:pjrt_device_description\",\n         \"//xla/pjrt:pjrt_executable\",\n-        \"//xla/pjrt:pjrt_future\",\n         \"//xla/pjrt:pjrt_stream_executor_client\",\n         \"//xla/pjrt:stream_executor_executable\",\n         \"//xla/pjrt:stream_executor_executable_proto_cc\",\n@@ -198,6 +198,7 @@ xla_test(\n         \":se_gpu_pjrt_client\",\n         \":se_gpu_topology_description\",\n         \"//xla:debug_options_flags\",\n+        \"//xla:future\",\n         \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n@@ -218,7 +219,6 @@ xla_test(\n         \"//xla/pjrt:pjrt_compiler\",\n         \"//xla/pjrt:pjrt_device_description\",\n         \"//xla/pjrt:pjrt_executable\",\n-        \"//xla/pjrt:pjrt_future\",\n         \"//xla/pjrt:pjrt_stream_executor_client\",\n         \"//xla/pjrt:raw_buffer\",\n         \"//xla/pjrt/distributed\",\n@@ -318,7 +318,6 @@ xla_test(\n         \"//xla/pjrt:pjrt_client\",\n         \"//xla/pjrt:pjrt_compiler\",\n         \"//xla/pjrt:pjrt_executable\",\n-        \"//xla/pjrt:pjrt_future\",\n         \"//xla/pjrt:pjrt_stream_executor_client\",\n         \"//xla/pjrt/distributed\",\n         \"//xla/pjrt/distributed:client\","
        },
        {
            "sha": "13f750a4b52e418d154e25af9c9d16eab6454d32",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/layout.h\"\n #include \"xla/literal.h\"\n@@ -76,7 +77,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_device_description.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/pjrt_stream_executor_client.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_allocator_config.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n@@ -772,8 +772,8 @@ StreamExecutorGpuClient::GetDefaultDeviceAssignment(int num_replicas,\n                                                               num_partitions);\n }\n \n-PjRtFuture<> StreamExecutorGpuClient::CopyRawSubBufferToHost(\n-    PjRtBuffer* pjrt_buffer, PjRtFuture<void*> dst, int64_t offset,\n+Future<> StreamExecutorGpuClient::CopyRawSubBufferToHost(\n+    PjRtBuffer* pjrt_buffer, Future<void*> dst, int64_t offset,\n     int64_t transfer_size) {\n   auto* buffer = tensorflow::down_cast<PjRtStreamExecutorBuffer*>(pjrt_buffer);\n   DCHECK(buffer);\n@@ -785,16 +785,16 @@ PjRtFuture<> StreamExecutorGpuClient::CopyRawSubBufferToHost(\n   // `dst` is not immediately available.\n   PjRtStreamExecutorBuffer::ScopedHold hold(buffer->GetBufferWithUsageHold());\n   if (!hold.ok()) {\n-    return PjRtFuture<>(hold.status());\n+    return Future<>(hold.status());\n   }\n \n   auto device_memory = hold->device_memory();\n   if (!device_memory) {\n-    return PjRtFuture<>(\n+    return Future<>(\n         InvalidArgument(\"Copy raw buffer called on an invalid buffer\"));\n   }\n \n-  auto [promise, future] = PjRtFuture<>::MakePromise();\n+  auto [promise, future] = Future<>::MakePromise();\n   auto usage_event = BufferSequencingEvent::Create(this->thread_pool());\n \n   auto definition_events = hold->definition_events();\n@@ -917,18 +917,18 @@ PjRtFuture<> StreamExecutorGpuClient::CopyRawSubBufferToHost(\n         });\n       });\n \n-  return PjRtFutureHelpers::WithProfiling(\n+  return FutureHelpers::WithProfiling(\n       std::move(future),\n       /*on_block_start=*/\n       []() {\n         tsl::profiler::TraceMeProducer traceme(\n             \"StreamExecutorGpuClient::CopyRawSubBufferToHost\");\n         VLOG(1) << \"StreamExecutorGpuClient::CopyRawSubBufferToHost\";\n-        return PjRtFutureHelpers::ProfilingKeys(\n+        return FutureHelpers::ProfilingKeys(\n             {/*traceme_context_id =*/traceme.GetContextId()});\n       },\n       /*on_block_end=*/\n-      [](PjRtFutureHelpers::ProfilingKeys keys) {\n+      [](FutureHelpers::ProfilingKeys keys) {\n         tsl::profiler::TraceMeConsumer traceme(\n             \"StreamExecutorGpuClient::CopyRawSubBufferToHost\",\n             keys.traceme_context_id);"
        },
        {
            "sha": "4e7ae5efe365ab2a5cf9eed0180969a8acc7c494",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"xla/client/local_client.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/layout.h\"\n #include \"xla/pjrt/distributed/client.h\"\n@@ -45,7 +46,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/pjrt_stream_executor_client.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n #include \"xla/pjrt/tracked_device_buffer.h\"\n@@ -140,9 +140,9 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n       std::optional<absl::Span<const std::optional<Layout>>> device_layouts,\n       PjRtMemorySpace* memory_space) override;\n \n-  PjRtFuture<> CopyRawSubBufferToHost(PjRtBuffer* buffer, PjRtFuture<void*> dst,\n-                                      int64_t offset,\n-                                      int64_t transfer_size) override;\n+  Future<> CopyRawSubBufferToHost(PjRtBuffer* buffer, Future<void*> dst,\n+                                  int64_t offset,\n+                                  int64_t transfer_size) override;\n \n   void CopyToRemoteDevice(PjRtBuffer* buffer,\n                           absl::string_view serialized_descriptor,"
        },
        {
            "sha": "b546ff35c1d6b65f5c72cff9e081671c4019334a",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -53,6 +53,7 @@ limitations under the License.\n #include \"xla/debug_options_flags.h\"\n #include \"xla/ffi/ffi.h\"\n #include \"xla/ffi/ffi_api.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/test.h\"\n@@ -73,7 +74,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_device_description.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/pjrt_stream_executor_client.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n #include \"xla/pjrt/profiling/device_time_measurement.h\"\n@@ -1067,7 +1067,7 @@ TEST(StreamExecutorGpuClientTest, CopyRawToHostFuture) {\n       std::unique_ptr<PjRtBuffer> buffer,\n       client->BufferFromHostLiteral(literal, client->memory_spaces()[0]));\n \n-  auto [dst_promise, dst_future] = xla::PjRtFuture<void*>::MakePromise();\n+  auto [dst_promise, dst_future] = xla::Future<void*>::MakePromise();\n \n   TF_ASSERT_OK_AND_ASSIGN(int64_t size, buffer->GetOnDeviceSizeInBytes());\n   auto ready = buffer->GetReadyFuture();"
        },
        {
            "sha": "5fae5fefd41bfb7c7f283e1fd735ef2df2a8c6ae",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -41,6 +41,7 @@ cc_library(\n         \":tracked_gpu_device_buffer\",\n         \"//xla:debug_options_flags\",\n         \"//xla:executable_run_options\",\n+        \"//xla:future\",\n         \"//xla:literal\",\n         \"//xla:shape_layout\",\n         \"//xla:shape_tree\",\n@@ -183,6 +184,7 @@ xla_test(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//mlir:IR\",\n+        \"//xla:future\",\n         \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla:shape_util\","
        },
        {
            "sha": "fa3c36a81570786999cf6713db36b2f2d16fa8d1",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 29,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"unsupported/Eigen/CXX11/Tensor\"\n #include \"xla/client/local_client.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/layout.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/literal.h\"\n@@ -48,7 +49,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/pjrt/transpose.h\"\n #include \"xla/primitive_util.h\"\n@@ -167,34 +167,34 @@ absl::StatusOr<Shape> TfrtGpuBuffer::logical_on_device_shape() {\n   return shape_or;\n }\n \n-PjRtFuture<> TfrtGpuBuffer::GetReadyFuture() {\n+Future<> TfrtGpuBuffer::GetReadyFuture() {\n   VLOG(4) << \"TfrtGpuBuffer::GetReadyFuture\";\n   absl::MutexLock lock(mu_);\n   if (!tracked_device_buffer_) {\n-    return PjRtFuture<>(InvalidArgument(\n+    return Future<>(InvalidArgument(\n         \"GetReadyFuture() called on deleted or donated buffer\"));\n   }\n   if (!ready_future_) {\n     ready_future_ = CreateFutureForEvent(tracked_device_buffer_->ready_event());\n   }\n-  return PjRtFutureHelpers::WithProfiling(\n+  return FutureHelpers::WithProfiling(\n       ready_future_,\n       /*on_block_start=*/\n       []() {\n         tsl::profiler::TraceMeProducer traceme(\"TfrtGpuBuffer::Await\");\n         VLOG(4) << \"TfrtGpuBuffer::Await\";\n-        return PjRtFutureHelpers::ProfilingKeys(\n+        return FutureHelpers::ProfilingKeys(\n             {/*traceme_context_id=*/traceme.GetContextId()});\n       },\n       /*on_block_end=*/\n-      [](PjRtFutureHelpers::ProfilingKeys keys) {\n+      [](FutureHelpers::ProfilingKeys keys) {\n         tsl::profiler::TraceMeConsumer traceme(\"TfrtGpuBuffer::Await\",\n                                                keys.traceme_context_id);\n       });\n }\n \n absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n-TfrtGpuBuffer::DonateWithControlDependency(PjRtFuture<> dependency) {\n+TfrtGpuBuffer::DonateWithControlDependency(Future<> dependency) {\n   VLOG(4) << \"TfrtGpuBuffer::DonateWithControlDependency\";\n \n   TF_ASSIGN_OR_RETURN(DonationTransaction donation_transaction,\n@@ -325,16 +325,15 @@ TfrtGpuBuffer::ReleaseDeviceMemoryOwnership(\n   return ref;\n }\n \n-PjRtFuture<> TfrtGpuBuffer::ToLiteral(MutableLiteralBase* literal) {\n+Future<> TfrtGpuBuffer::ToLiteral(MutableLiteralBase* literal) {\n   VLOG(3) << \"TfrtGpuBuffer::ToLiteral for a tensor of shape \"\n           << literal->shape().ToString();\n-  return ToLiteralHelper(PjRtFuture<MutableLiteralBase*>(literal));\n+  return ToLiteralHelper(Future<MutableLiteralBase*>(literal));\n }\n \n-PjRtFuture<> TfrtGpuBuffer::ToLiteralHelper(\n-    PjRtFuture<MutableLiteralBase*> literal) {\n+Future<> TfrtGpuBuffer::ToLiteralHelper(Future<MutableLiteralBase*> literal) {\n   tsl::profiler::TraceMe traceme(\"TfrtGpuBuffer::ToLiteral\");\n-  auto [promise, future] = PjRtFuture<>::MakePromise();\n+  auto [promise, future] = Future<>::MakePromise();\n   auto usage_event = tsl::MakeConstructedAsyncValueRef<GpuEvent>();\n   auto* device_buffer = AcquireUsage(usage_event);\n   if (device_buffer == nullptr) {\n@@ -347,8 +346,8 @@ PjRtFuture<> TfrtGpuBuffer::ToLiteralHelper(\n       client_->xla_client()->backend().transfer_manager()->PackSubbyteTypes();\n \n   auto [literal_and_transpose_promise, literal_and_transpose_future] =\n-      PjRtFuture<std::pair<MutableLiteralBase*,\n-                           std::shared_ptr<TransposePlan>>>::MakePromise();\n+      Future<std::pair<MutableLiteralBase*,\n+                       std::shared_ptr<TransposePlan>>>::MakePromise();\n   literal.OnReady(\n       [client = client_, on_device_shape{on_device_shape_},\n        promise = std::move(literal_and_transpose_promise)](\n@@ -530,46 +529,46 @@ PjRtFuture<> TfrtGpuBuffer::ToLiteralHelper(\n                        {device_buffer->definition_event().CopyRCRef()},\n                        std::move(d2h_copy));\n \n-  return PjRtFutureHelpers::WithProfiling(\n+  return FutureHelpers::WithProfiling(\n       std::move(future),\n       /*on_block_start=*/\n       []() {\n         tsl::profiler::TraceMeProducer traceme(\"TfrtGpuBuffer::ToLiteral\");\n         VLOG(3) << \"TfrtGpuBuffer::ToLiteral::OnBlockStart\";\n-        return PjRtFutureHelpers::ProfilingKeys(\n+        return FutureHelpers::ProfilingKeys(\n             {/*traceme_context_id =*/traceme.GetContextId()});\n       },\n       /*on_block_end=*/\n-      [](PjRtFutureHelpers::ProfilingKeys keys) {\n+      [](FutureHelpers::ProfilingKeys keys) {\n         tsl::profiler::TraceMeConsumer traceme(\"TfrtGpuBuffer::ToLiteral\",\n                                                keys.traceme_context_id);\n       });\n }\n \n-PjRtFuture<> TfrtGpuBuffer::LazyToLiteral(\n-    absl::AnyInvocable<PjRtFuture<MutableLiteralBase*>() &&> generator) {\n+Future<> TfrtGpuBuffer::LazyToLiteral(\n+    absl::AnyInvocable<Future<MutableLiteralBase*>() &&> generator) {\n   VLOG(3) << \"TfrtGpuBuffer::LazyToLiteral\";\n   auto buffer = std::move(generator)();\n   return ToLiteralHelper(std::move(buffer));\n }\n \n-PjRtFuture<> TfrtGpuBuffer::CopyRawToHostFuture(PjRtFuture<void*> dst_future,\n-                                                int64_t offset,\n-                                                int64_t transfer_size) {\n+Future<> TfrtGpuBuffer::CopyRawToHostFuture(Future<void*> dst_future,\n+                                            int64_t offset,\n+                                            int64_t transfer_size) {\n   VLOG(3) << \"TfrtGpuBuffer::CopyRawToHostFuture\";\n   tsl::profiler::TraceMe traceme(\"TfrtGpuBuffer::CopyRawToHostFuture\");\n-  auto [promise, future] = PjRtFuture<>::MakePromise();\n+  auto [promise, future] = Future<>::MakePromise();\n   auto usage_event = tsl::MakeConstructedAsyncValueRef<GpuEvent>();\n   auto* device_buffer = AcquireUsage(usage_event);\n   MarkGpuEventReadyOnExit usage_event_holder(std::move(usage_event));\n   if (device_buffer == nullptr) {\n-    return PjRtFuture<>(\n+    return Future<>(\n         InvalidArgument(\"ToLiteral() called on deleted or donated buffer\"));\n   }\n   auto d2h_copy = [device(device_), device_buffer,\n                    usage_event_holder = std::move(usage_event_holder),\n-                   client = client_, offset, transfer_size](\n-                      PjRtFuture<>::Promise promise, void* dst) mutable {\n+                   client = client_, offset,\n+                   transfer_size](Promise<> promise, void* dst) mutable {\n     if (device_buffer->definition_event().IsError()) {\n       LOG(ERROR) << \"device_buffer->definition_event().GetError(): \"\n                  << device_buffer->definition_event().GetError();\n@@ -658,18 +657,18 @@ PjRtFuture<> TfrtGpuBuffer::CopyRawToHostFuture(PjRtFuture<void*> dst_future,\n             });\n       });\n \n-  return PjRtFutureHelpers::WithProfiling(\n+  return FutureHelpers::WithProfiling(\n       std::move(future),\n       /*on_block_start=*/\n       []() {\n         tsl::profiler::TraceMeProducer traceme(\n             \"TfrtGpuBuffer::CopyRawToHostFuture\");\n         VLOG(3) << \"TfrtGpuBuffer::CopyRawToHostFuture\";\n-        return PjRtFutureHelpers::ProfilingKeys(\n+        return FutureHelpers::ProfilingKeys(\n             {/*traceme_context_id =*/traceme.GetContextId()});\n       },\n       /*on_block_end=*/\n-      [](PjRtFutureHelpers::ProfilingKeys keys) {\n+      [](FutureHelpers::ProfilingKeys keys) {\n         tsl::profiler::TraceMeConsumer traceme(\n             \"TfrtGpuBuffer::CopyRawToHostFuture\", keys.traceme_context_id);\n       });"
        },
        {
            "sha": "178d4b9c2494772446954d175e02f1fa1e59696e",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.h",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.h?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n #include <string>\n \n #include \"absl/log/log.h\"\n+#include \"xla/future.h\"\n #include \"xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n@@ -63,21 +63,20 @@ class TfrtGpuBuffer final : public PjRtBuffer {\n   ReleaseDeviceMemoryOwnership(bool wait_for_operations_to_complete) override;\n \n   using PjRtBuffer::ToLiteralSync;\n-  PjRtFuture<> ToLiteral(MutableLiteralBase* literal) override;\n+  Future<> ToLiteral(MutableLiteralBase* literal) override;\n \n-  PjRtFuture<> LazyToLiteral(\n-      absl::AnyInvocable<PjRtFuture<MutableLiteralBase*>() &&> generator)\n-      override;\n+  Future<> LazyToLiteral(\n+      absl::AnyInvocable<Future<MutableLiteralBase*>() &&> generator) override;\n \n   absl::StatusOr<size_t> GetOnDeviceSizeInBytes() const override;\n \n-  PjRtFuture<> CopyRawToHost(void* dst, int64_t offset,\n-                             int64_t transfer_size) override {\n-    return CopyRawToHostFuture(PjRtFuture<void*>(dst), offset, transfer_size);\n+  Future<> CopyRawToHost(void* dst, int64_t offset,\n+                         int64_t transfer_size) override {\n+    return CopyRawToHostFuture(Future<void*>(dst), offset, transfer_size);\n   }\n \n-  PjRtFuture<> CopyRawToHostFuture(PjRtFuture<void*> dst, int64_t offset,\n-                                   int64_t transfer_size) override;\n+  Future<> CopyRawToHostFuture(Future<void*> dst, int64_t offset,\n+                               int64_t transfer_size) override;\n \n   void Delete() override;\n \n@@ -86,16 +85,16 @@ class TfrtGpuBuffer final : public PjRtBuffer {\n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> CopyToMemorySpace(\n       PjRtMemorySpace* dst_memory_space) override;\n \n-  void CopyToRemoteDevice(PjRtFuture<std::string> serialized_descriptor,\n+  void CopyToRemoteDevice(Future<std::string> serialized_descriptor,\n                           RemoteSendCallback on_done) override {\n     on_done(Unimplemented(\"CopyToRemoteDevice not implemented.\"),\n             /*sends_were_enqueued=*/false);\n   }\n \n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> DonateWithControlDependency(\n-      PjRtFuture<> dependency) override;\n+      Future<> dependency) override;\n \n-  PjRtFuture<> GetReadyFuture() override;\n+  Future<> GetReadyFuture() override;\n \n   bool IsOnCpu() const override;\n \n@@ -198,7 +197,7 @@ class TfrtGpuBuffer final : public PjRtBuffer {\n   std::unique_ptr<TrackedGpuDeviceBuffer> ReleaseBufferLocked()\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n-  PjRtFuture<> ToLiteralHelper(PjRtFuture<MutableLiteralBase*> literal);\n+  Future<> ToLiteralHelper(Future<MutableLiteralBase*> literal);\n \n   TfrtGpuClient* client_;\n   const Shape on_device_shape_;\n@@ -216,7 +215,7 @@ class TfrtGpuBuffer final : public PjRtBuffer {\n   // might fail. Note that concurrent calls to AcquireUsage() and\n   // AcquireDonation() might fail even if the pending donation is aborted later.\n   tsl::AsyncValueRef<bool> donation_event_ ABSL_GUARDED_BY(mu_);\n-  PjRtFuture<> ready_future_ ABSL_GUARDED_BY(mu_);\n+  Future<> ready_future_ ABSL_GUARDED_BY(mu_);\n \n   // This event is triggered when the last external reference is released.\n   // It is used to make sure that the buffer is not deleted before all external"
        },
        {
            "sha": "a1c73f35f0d82f7eb875b7ae54310ceca4376dce",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -46,6 +46,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/ffi/ffi.h\"\n #include \"xla/ffi/ffi_api.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/test.h\"\n@@ -65,7 +66,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_common.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/pjrt/raw_buffer.h\"\n@@ -512,7 +512,7 @@ TEST(TfrtGpuClientTest, DonateWithControlDependency) {\n       std::unique_ptr<PjRtBuffer> buffer,\n       client->BufferFromHostLiteral(literal, client->memory_spaces()[0]));\n \n-  auto [promise, future] = PjRtFuture<>::MakePromise();\n+  auto [promise, future] = Future<>::MakePromise();\n   auto blocked_buffer =\n       std::move(*(buffer->DonateWithControlDependency(future)));\n   EXPECT_TRUE(buffer->IsDeleted());\n@@ -701,7 +701,7 @@ TEST(TfrtGpuClientTest, ToLiteralAsync) {\n   Shape host_shape =\n       ShapeUtil::DeviceShapeToHostShape(buffer->on_device_shape());\n   auto [literal_promise, literal_future] =\n-      PjRtFuture<MutableLiteralBase*>::MakePromise();\n+      Future<MutableLiteralBase*>::MakePromise();\n \n   // Literal is not ready.\n   buffer->LazyToLiteral([f = std::move(literal_future)]() { return f; })\n@@ -756,7 +756,7 @@ TEST(TfrtGpuClientTest, ToLiteralAsyncWithNonCompactLayout) {\n   Shape host_shape =\n       ShapeUtil::DeviceShapeToHostShape(buffer->on_device_shape());\n   auto [literal_promise, literal_future] =\n-      PjRtFuture<MutableLiteralBase*>::MakePromise();\n+      Future<MutableLiteralBase*>::MakePromise();\n \n   absl::Notification n;\n   buffer->LazyToLiteral([f = std::move(literal_future)]() { return f; })\n@@ -1233,7 +1233,7 @@ TEST(TfrtGpuClientTest, CopyRawToHostFuture) {\n       std::unique_ptr<PjRtBuffer> buffer,\n       client->BufferFromHostLiteral(literal, client->memory_spaces()[0]));\n \n-  auto [dst_promise, dst_future] = xla::PjRtFuture<void*>::MakePromise();\n+  auto [dst_promise, dst_future] = xla::Future<void*>::MakePromise();\n \n   TF_ASSERT_OK_AND_ASSIGN(int64_t size, buffer->GetOnDeviceSizeInBytes());\n   auto ready = buffer->GetReadyFuture();\n@@ -1654,7 +1654,7 @@ TEST(TfrtGpuClientTest, AsyncCopyToDevice) {\n \n   auto literal = std::make_shared<Literal>(src_literal.shape());\n \n-  PjRtFuture<> local_recv_literal = local_recv_buffer->ToLiteral(literal.get());\n+  Future<> local_recv_literal = local_recv_buffer->ToLiteral(literal.get());\n   TF_EXPECT_OK(local_recv_literal.Await());\n \n   EXPECT_TRUE(ShapeUtil::Compatible(src_literal.shape(), literal->shape()));"
        },
        {
            "sha": "cbef8f354173df66c3e29541a791d7ac14d9f8a2",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_executable.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"xla/client/executable_build_options.h\"\n #include \"xla/client/local_client.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/layout.h\"\n@@ -108,7 +109,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n         dst_(dst),\n         done_(std::move(done)) {}\n \n-  PjRtFuture<> AddChunk(PjRtChunk chunk) final {\n+  Future<> AddChunk(PjRtChunk chunk) final {\n     tsl::profiler::TraceMe trace([&] {\n       return tsl::profiler::TraceMeEncode(\"TfrtGpuCopyToDeviceStream::AddChunk\",\n                                           {{\"channel_id\", channel_id_}});\n@@ -125,15 +126,15 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n       done_.SetError(absl::InvalidArgumentError(absl::StrFormat(\n           \"Chunk size (%d) was not a multiple of the granule size (%d)\",\n           chunk.size(), granule_size_in_bytes())));\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     if (current_bytes_ + chunk.size() > total_bytes_) {\n       done_.SetError(absl::InvalidArgumentError(\n           absl::StrFormat(\"Adding chunk of size %d would overflow buffer of \"\n                           \"size %d (%d already transferred)\",\n                           chunk.size(), total_bytes_, current_bytes_)));\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     se::DeviceMemoryBase dst(\n@@ -149,7 +150,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n     auto copied = stream_->Memcpy(&dst, chunk.data(), chunk.size());\n     if (!copied.ok()) {\n       done_.SetError(copied);\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     // Delete chunk once the memcpy operation completes.\n@@ -159,7 +160,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n         });\n     if (!deleted.ok()) {\n       done_.SetError(deleted);\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     // Record done event once processed the last chunk. It is the caller\n@@ -169,12 +170,12 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n       auto recorded = stream_->RecordEvent(done_.get().get());\n       if (!recorded.ok()) {\n         done_.SetError(recorded);\n-        return PjRtFuture<>(done_.GetError());\n+        return Future<>(done_.GetError());\n       }\n       done_.SetStateConcrete();\n     }\n \n-    return PjRtFuture<>(absl::OkStatus());\n+    return Future<>(absl::OkStatus());\n   }\n \n  private:\n@@ -875,7 +876,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n                        std::move(prepare_inputs));\n \n   // Create output TFRT buffers.\n-  std::optional<PjRtFuture<>> future;\n+  std::optional<Future<>> future;\n   if (fill_future) {\n     future = CreateFutureForEvent(complete_event);\n   }\n@@ -887,7 +888,7 @@ absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>>\n TfrtGpuExecutable::Execute(\n     absl::Span<const std::vector<PjRtBuffer*>> argument_handles,\n     const ExecuteOptions& options,\n-    std::optional<std::vector<PjRtFuture<>>>& returned_futures) const {\n+    std::optional<std::vector<Future<>>>& returned_futures) const {\n   tsl::profiler::TraceMeProducer activity(\"TfrtGpuExecutable::Execute\",\n                                           tsl::profiler::ContextType::kPjRt,\n                                           options.launch_id);\n@@ -1014,7 +1015,7 @@ TfrtGpuExecutable::Execute(\n absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n TfrtGpuExecutable::ExecuteSharded(\n     absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-    const ExecuteOptions& options, std::optional<PjRtFuture<>>& returned_future,\n+    const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n     bool fill_future) const {\n   tsl::profiler::TraceMeProducer activity(\"TfrtGpuExecutable::ExecuteSharded\",\n                                           tsl::profiler::ContextType::kPjRt,\n@@ -1046,7 +1047,7 @@ TfrtGpuExecutable::ExecuteSharded(\n absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n TfrtGpuExecutable::ExecutePortable(\n     absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-    const ExecuteOptions& options, std::optional<PjRtFuture<>>& returned_future,\n+    const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n     bool fill_future) const {\n   tsl::profiler::TraceMeProducer activity(\"TfrtGpuExecutable::ExecutePortable\",\n                                           tsl::profiler::ContextType::kPjRt,"
        },
        {
            "sha": "dc5909b75060ccf60b928a25426b56552a648d33",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_executable.h",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.h?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -32,14 +32,14 @@ limitations under the License.\n #include \"unsupported/Eigen/CXX11/Tensor\"\n #include \"xla/client/local_client.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/layout.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_client.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_device.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/shape.h\"\n@@ -104,21 +104,18 @@ class TfrtGpuExecutable final : public PjRtLoadedExecutable {\n   absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>> Execute(\n       absl::Span<const std::vector<PjRtBuffer*>> argument_handles,\n       const ExecuteOptions& options,\n-      std::optional<std::vector<PjRtFuture<>>>& returned_futures)\n-      const override;\n+      std::optional<std::vector<Future<>>>& returned_futures) const override;\n \n   using PjRtLoadedExecutable::ExecuteSharded;\n   absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>> ExecuteSharded(\n       absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-      const ExecuteOptions& options,\n-      std::optional<PjRtFuture<>>& returned_future,\n+      const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n       bool fill_future) const override;\n \n   using PjRtLoadedExecutable::ExecutePortable;\n   absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>> ExecutePortable(\n       absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,\n-      const ExecuteOptions& options,\n-      std::optional<PjRtFuture<>>& returned_future,\n+      const ExecuteOptions& options, std::optional<Future<>>& returned_future,\n       bool fill_future) const override;\n \n   void Delete() override { executables_.clear(); }"
        },
        {
            "sha": "a5eb9604e361bd0a3693de2b2ab614f6cb768a4e",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -50,6 +50,7 @@ limitations under the License.\n #include \"xla/core/collectives/collectives.h\"\n #include \"xla/core/collectives/collectives_registry.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/layout.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/maybe_owning.h\"\n@@ -69,7 +70,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_common.h\"\n #include \"xla/pjrt/pjrt_device_description.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_allocator_config.h\"\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/service/compiler.h\"\n@@ -130,8 +130,8 @@ absl::StatusOr<std::shared_ptr<se::Event>> CreateCudaEvent(\n   return absl::ShareUniquePtr(std::move(cuda_event));\n }\n \n-PjRtFuture<> CreateFutureForEvent(tsl::AsyncValueRef<xla::GpuEvent> event) {\n-  auto [promise, future] = PjRtFuture<>::MakePromise();\n+Future<> CreateFutureForEvent(tsl::AsyncValueRef<xla::GpuEvent> event) {\n+  auto [promise, future] = Future<>::MakePromise();\n   auto done_fn = [promise = std::move(promise), event]() mutable {\n     if (const absl::Status* error = event.GetErrorIfPresent()) {\n       VLOG(3) << \"Setting future: \" << *error;\n@@ -313,7 +313,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n         dst_(dst),\n         done_(std::move(done)) {}\n \n-  PjRtFuture<> AddChunk(PjRtChunk chunk) final {\n+  Future<> AddChunk(PjRtChunk chunk) final {\n     tsl::profiler::TraceMe trace([&] {\n       return tsl::profiler::TraceMeEncode(\"TfrtGpuCopyToDeviceStream::AddChunk\",\n                                           {{\"channel_id\", channel_id_}});\n@@ -330,15 +330,15 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n       done_.SetError(absl::InvalidArgumentError(absl::StrFormat(\n           \"Chunk size (%d) was not a multiple of the granule size (%d)\",\n           chunk.size(), granule_size_in_bytes())));\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     if (current_bytes_ + chunk.size() > total_bytes_) {\n       done_.SetError(absl::InvalidArgumentError(\n           absl::StrFormat(\"Adding chunk of size %d would overflow buffer of \"\n                           \"size %d (%d already transferred)\",\n                           chunk.size(), total_bytes_, current_bytes_)));\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     se::DeviceMemoryBase dst(\n@@ -354,7 +354,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n     auto copied = stream_->Memcpy(&dst, chunk.data(), chunk.size());\n     if (!copied.ok()) {\n       done_.SetError(copied);\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     // Delete chunk once the memcpy operation completes.\n@@ -364,7 +364,7 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n         });\n     if (!deleted.ok()) {\n       done_.SetError(deleted);\n-      return PjRtFuture<>(done_.GetError());\n+      return Future<>(done_.GetError());\n     }\n \n     // Record done event once processed the last chunk. It is the caller\n@@ -374,12 +374,12 @@ class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n       auto recorded = stream_->RecordEvent(done_.get().get());\n       if (!recorded.ok()) {\n         done_.SetError(recorded);\n-        return PjRtFuture<>(done_.GetError());\n+        return Future<>(done_.GetError());\n       }\n       done_.SetStateConcrete();\n     }\n \n-    return PjRtFuture<>(absl::OkStatus());\n+    return Future<>(absl::OkStatus());\n   }\n \n  private:"
        },
        {
            "sha": "7357366da2218751a6ee34b3fc185a716a467eb6",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1793c4a4e8fcaa11a2ab0cfbec917f39414647e3/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h?ref=1793c4a4e8fcaa11a2ab0cfbec917f39414647e3",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"unsupported/Eigen/CXX11/Tensor\"\n #include \"xla/client/local_client.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/future.h\"\n #include \"xla/layout.h\"\n #include \"xla/maybe_owning.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n@@ -47,7 +48,6 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_common.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/gpu/gpu_executable_run_options.h\"\n@@ -72,7 +72,7 @@ absl::Status WaitForEventOnStream(se::Stream* stream, se::Event* event);\n absl::StatusOr<std::shared_ptr<se::Event>> CreateCudaEvent(\n     TfrtGpuDevice* device);\n \n-PjRtFuture<> CreateFutureForEvent(tsl::AsyncValueRef<xla::GpuEvent> event);\n+Future<> CreateFutureForEvent(tsl::AsyncValueRef<xla::GpuEvent> event);\n \n absl::StatusOr<Shape> GetDestinationDeviceShape(const Shape& host_shape,\n                                                 TfrtGpuDevice* device,"
        }
    ],
    "stats": {
        "total": 193,
        "additions": 95,
        "deletions": 98
    }
}