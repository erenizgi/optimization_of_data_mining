{
    "author": "EusebioDM",
    "message": "Move `GpuExecutable` dumping logic from the compiler to the executable\n\nPiperOrigin-RevId: 836681917",
    "sha": "a0db2845b15d6bc8957bff9b5501ba49b864f034",
    "files": [
        {
            "sha": "3c865b508f570075b52b4ce1cd50e7af2d611277",
            "filename": "third_party/xla/xla/client/local_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fclient%2Flocal_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fclient%2Flocal_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fclient%2Flocal_client.cc?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -447,6 +447,10 @@ LocalClient::Compile(const XlaComputation& computation,\n   local_executables.reserve(executables.size());\n \n   for (auto& executable : executables) {\n+    if (executable->has_module()) {\n+      TF_RETURN_IF_ERROR(executable->DumpExecutableIfEnabled(\n+          updated_options, executable->module().config().debug_options()));\n+    }\n     local_executables.push_back(std::make_unique<LocalExecutable>(\n         std::move(executable), local_service_->mutable_backend(),\n         updated_options));"
        },
        {
            "sha": "f0a6cf249deeb6b661a4e403c8b62b2ae8a522e8",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -1207,6 +1207,7 @@ cc_library(\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -1553,6 +1554,7 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/client:executable_build_options\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\","
        },
        {
            "sha": "bb8e494db1d64d21cb84cb8738eb648b0f5d4aba",
            "filename": "third_party/xla/xla/service/executable.h",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -26,10 +26,12 @@ limitations under the License.\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/client/executable_build_options.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/computation_layout.h\"\n@@ -444,6 +446,16 @@ class Executable {\n     return {};\n   }\n \n+  // Gives the executable a chance to dump itself with the given\n+  // `ExecutableBuildOptions`\n+  // Whether dumping is enabled, and how/where is determined by the\n+  // `debug_options`.\n+  virtual absl::Status DumpExecutableIfEnabled(\n+      const ExecutableBuildOptions& options,\n+      const DebugOptions& debug_options) const {\n+    return absl::OkStatus();\n+  }\n+\n  protected:\n   // HloModule this was compiled from. BufferAssignment keeps pointers to\n   // HloInstructions owned by the HloModule so we need to keep the HloModule"
        },
        {
            "sha": "45e3b1f79a59483ac08d14325c9f7081451d7ec1",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -769,6 +769,7 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/backends/gpu/runtime:annotation\",\n         \"//xla/backends/gpu/runtime:collective_clique_requests\",\n         \"//xla/backends/gpu/runtime:collective_cliques\",\n@@ -780,7 +781,9 @@ cc_library(\n         \"//xla/backends/gpu/runtime:thunk_buffer_debug_pass\",\n         \"//xla/backends/gpu/runtime:thunk_pass_pipeline\",\n         \"//xla/backends/gpu/runtime:thunk_proto_deserialization\",\n+        \"//xla/client:executable_build_options\",\n         \"//xla/core/collectives:clique_key\",\n+        \"//xla/core/collectives:communicator\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:computation_layout\","
        },
        {
            "sha": "e79ba64a9144d7aa2164d791e1c91514732b1cd0",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -2491,40 +2491,6 @@ GpuCompiler::CompileToBackendResult(\n                                    std::move(compile_module_results)};\n }\n \n-static absl::Status DumpGpuExecutableIfEnabled(\n-    const GpuExecutable& gpu_executable,\n-    const Compiler::CompileOptions& compile_options,\n-    const DebugOptions& debug_options) {\n-  // If we were to dump the GPU executable for autotuning, we would end up\n-  // creating lots of tiny executables that aren't event useful for customers.\n-  if (compile_options.is_autotuning_compilation) {\n-    return absl::OkStatus();\n-  }\n-  if (!debug_options.has_xla_dump_to() ||\n-      !debug_options.xla_gpu_experimental_dump_gpu_executable()) {\n-    return absl::OkStatus();\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(GpuExecutableProto gpu_executable_proto,\n-                      gpu_executable.ToProto());\n-  std::string serialized_proto = gpu_executable_proto.SerializeAsString();\n-  if (serialized_proto.empty()) {\n-    return absl::InternalError(\"Failed to serialize GPU executable proto\");\n-  }\n-\n-  ExecutableAndOptionsProto dump_proto;\n-  *dump_proto.mutable_serialized_executable() = std::move(serialized_proto);\n-  constexpr absl::string_view kDumpFilename = \"gpu_executable\";\n-  if (gpu_executable.has_module()) {\n-    DumpPerModuleProtobufToFile(gpu_executable.module(), dump_proto,\n-                                debug_options, kDumpFilename);\n-  } else {\n-    DumpProtobufToFile(dump_proto, debug_options, kDumpFilename);\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n     std::unique_ptr<HloModule> module, se::StreamExecutor* stream_exec,\n     const CompileOptions& options) {\n@@ -2642,9 +2608,6 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n               : std::move(module),\n           /*enable_debug_info_manager=*/!options.is_autotuning_compilation}));\n \n-  TF_RETURN_IF_ERROR(\n-      DumpGpuExecutableIfEnabled(*gpu_executable, options, debug_opts));\n-\n   if (embed_ir_in_executable) {\n     std::string ir_module_string_before_opt =\n         llvm_ir::DumpToString(res.compile_module_results.llvm_module.get());"
        },
        {
            "sha": "c8a8b9288e127ca8b07cce2ef214d154079170ab",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -404,50 +404,6 @@ ENTRY e {\n   EXPECT_THAT(entry_root, GmockMatch(m::Fusion()));\n }\n \n-TEST_F(GpuCompilerTest, GpuExecutableDump) {\n-  constexpr absl::string_view hlo_text = R\"hlo(\n-    HloModule test\n-\n-    ENTRY main {\n-      p = f32[10]{0} parameter(0)\n-      ROOT neg = f32[10]{0} negate(p)\n-    }\n-)hlo\";\n-  HloModuleConfig config = GetModuleConfigForTest();\n-  DebugOptions& debug_options = config.mutable_debug_options();\n-  debug_options.set_xla_gpu_experimental_dump_gpu_executable(true);\n-  TF_ASSERT_OK_AND_ASSIGN(TemporaryDirectory temp_dir,\n-                          TemporaryDirectory::CreateForCurrentTestcase());\n-  debug_options.set_xla_dump_to(temp_dir.path());\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo_text, config));\n-  std::string module_name = module->name();\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<Executable> executable,\n-      backend().compiler()->RunBackend(std::move(module),\n-                                       backend().default_stream_executor(),\n-                                       Compiler::CompileOptions()));\n-\n-  std::vector<std::string> dump_files;\n-  TF_ASSERT_OK(tsl::Env::Default()->GetMatchingPaths(\n-      tsl::io::JoinPath(debug_options.xla_dump_to(), \"*gpu_executable.txt\"),\n-      &dump_files));\n-  ASSERT_EQ(dump_files.size(), 1);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::string dump_serialized_contents,\n-                          ReadNonEmptyFile(dump_files[0]));\n-  ExecutableAndOptionsProto dump_content;\n-  ASSERT_TRUE(tsl::protobuf::TextFormat::ParseFromString(\n-      dump_serialized_contents, &dump_content));\n-\n-  GpuExecutableProto gpu_executable_proto;\n-  ASSERT_TRUE(gpu_executable_proto.ParseFromString(\n-      dump_content.serialized_executable()));\n-  EXPECT_THAT(gpu_executable_proto.binary(), Not(IsEmpty()));\n-  EXPECT_EQ(gpu_executable_proto.module_name(), module_name);\n-}\n-\n class PersistedAutotuningTest : public HloTestBase {\n  protected:\n   static constexpr absl::string_view kHloText = R\"("
        },
        {
            "sha": "6e60c74db6e9e6898dace36c9eae961c2550b3dd",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/runtime/annotation.h\"\n #include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n #include \"xla/backends/gpu/runtime/collective_cliques.h\"\n@@ -51,7 +52,9 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk_buffer_debug_pass.h\"\n #include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n #include \"xla/backends/gpu/runtime/thunk_proto_deserialization.h\"\n+#include \"xla/client/executable_build_options.h\"\n #include \"xla/core/collectives/clique_key.h\"\n+#include \"xla/core/collectives/communicator.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -1306,5 +1309,48 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::FromProto(\n \n   return Create(std::move(params));\n }\n+\n+static absl::StatusOr<ExecutableBuildOptionsProto>\n+CreateSerializableBuildOptionsProto(const ExecutableBuildOptions& options) {\n+  ExecutableBuildOptions serializable_opts = options;\n+  // These fields are not serializable, and the toProto will fail if they are\n+  // set, but we also don't need them for the dump so just clear them.\n+  serializable_opts.set_layout_canonicalization_callback(nullptr);\n+  serializable_opts.set_compile_thread_pool(nullptr);\n+\n+  return serializable_opts.ToProto();\n+}\n+\n+absl::Status GpuExecutable::DumpExecutableIfEnabled(\n+    const ExecutableBuildOptions& options,\n+    const DebugOptions& debug_options) const {\n+  if (!debug_options.has_xla_dump_to() ||\n+      !debug_options.xla_gpu_experimental_dump_gpu_executable()) {\n+    return absl::OkStatus();\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(GpuExecutableProto gpu_executable_proto, ToProto());\n+  std::string serialized_proto = gpu_executable_proto.SerializeAsString();\n+  if (serialized_proto.empty()) {\n+    return absl::InternalError(\"Failed to serialize GPU executable proto\");\n+  }\n+\n+  ExecutableAndOptionsProto dump_proto;\n+  *dump_proto.mutable_serialized_executable() = std::move(serialized_proto);\n+  TF_ASSIGN_OR_RETURN(\n+      *dump_proto.mutable_compile_options()->mutable_executable_build_options(),\n+      CreateSerializableBuildOptionsProto(options));\n+\n+  constexpr absl::string_view kDumpFilename = \"gpu_executable\";\n+  if (has_module()) {\n+    DumpPerModuleProtobufToFile(module(), dump_proto, debug_options,\n+                                kDumpFilename);\n+  } else {\n+    DumpProtobufToFile(dump_proto, debug_options, kDumpFilename);\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "80023389d649168101d62f824f9bd4049fb21a5c",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/annotation.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/client/executable_build_options.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -223,6 +224,10 @@ class GpuExecutable : public Executable {\n \n   absl::StatusOr<GpuExecutableProto> ToProto() const;\n \n+  absl::Status DumpExecutableIfEnabled(\n+      const ExecutableBuildOptions& options,\n+      const DebugOptions& debug_options) const final;\n+\n  private:\n   // Use GpuExecutable::Create() to create an instance.\n   explicit GpuExecutable(Params params,"
        },
        {
            "sha": "ed0e5f20bdbb2c6c2031bacd44704c81fd450b3c",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 98,
            "deletions": 14,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a0db2845b15d6bc8957bff9b5501ba49b864f034/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=a0db2845b15d6bc8957bff9b5501ba49b864f034",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/client/executable_build_options.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n@@ -54,6 +55,7 @@ limitations under the License.\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/tsl/testing/temporary_directory.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"tsl/platform/path.h\"\n@@ -68,6 +70,14 @@ using ::testing::Property;\n using ::testing::SizeIs;\n using ::testing::UnorderedElementsAre;\n using ::tsl::proto_testing::EqualsProto;\n+using ::tsl::proto_testing::Partially;\n+using ::tsl::testing::TemporaryDirectory;\n+\n+Thunk::ThunkInfo ThunkInfoWithId(int thunk_id) {\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.thunk_id = thunk_id;\n+  return thunk_info;\n+}\n \n TEST(GpuExecutableTest, OuputInfoToAndFromProto) {\n   const GpuExecutable::OutputInfo output_info0{/*allocation_index=*/42,\n@@ -371,28 +381,24 @@ TEST(GpuExecutableTest, DumpsMetadataListProto) {\n \n   int execution_count = 0;\n   auto create_executable = [&]() {\n-    Thunk::ThunkInfo thunk_info;\n-    thunk_info.thunk_id = 123;\n     BufferAllocation alloc(0, 1024, 0);\n     BufferAllocation::Slice slice(&alloc, 0, 1024);\n \n     ThunkSequence thunk_sequence;\n     thunk_sequence.push_back(std::make_unique<KernelThunk>(\n-        thunk_info,\n+        ThunkInfoWithId(123),\n         /*kernel_name=*/\"test_kernel\",\n         /*kernel_arguments=*/emitters::KernelArguments({}),\n         /*launch_dimensions=*/LaunchDimensions(),\n         /*cluster_dim=*/std::nullopt,\n         /*shmem_bytes=*/0,\n         /*tma_metadata=*/se::gpu::TmaMetadata()));\n-    thunk_info.thunk_id = 456;\n     thunk_sequence.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        thunk_info, slice, slice, 1024));\n+        ThunkInfoWithId(456), slice, slice, 1024));\n \n     GpuExecutable::Params params;\n-    thunk_info.thunk_id = 789;\n     params.executable = std::make_unique<SequentialThunk>(\n-        thunk_info, std::move(thunk_sequence));\n+        ThunkInfoWithId(789), std::move(thunk_sequence));\n     params.debug_options = debug_options;\n \n     params.module_name = absl::StrCat(\"test_module\", execution_count++);\n@@ -443,12 +449,9 @@ TEST(GpuExecutableTest, ProtoConversion) {\n   device_description.set_driver_version({12, 3, 0});\n   device_description.set_runtime_version({12, 3, 0});\n \n-  Thunk::ThunkInfo thunk_info;\n-  thunk_info.thunk_id = 123;\n-\n   ThunkSequence thunk_sequence;\n   thunk_sequence.push_back(std::make_unique<KernelThunk>(\n-      thunk_info,\n+      ThunkInfoWithId(123),\n       /*kernel_name=*/\"test_kernel\", emitters::KernelArguments({}),\n       LaunchDimensions(),\n       /*cluster_dim=*/std::nullopt,\n@@ -459,9 +462,8 @@ TEST(GpuExecutableTest, ProtoConversion) {\n   params.binary = {1, 2, 3};\n   params.dnn_compiled_graphs = {{\"test_dnn_compiled_graph\", \"test_json\"}};\n \n-  thunk_info.thunk_id = 456;\n-  params.executable =\n-      std::make_unique<SequentialThunk>(thunk_info, std::move(thunk_sequence));\n+  params.executable = std::make_unique<SequentialThunk>(\n+      ThunkInfoWithId(456), std::move(thunk_sequence));\n   params.device_description = device_description;\n \n   params.module_name = \"test_module\";\n@@ -487,5 +489,87 @@ TEST(GpuExecutableTest, ProtoConversion) {\n   EXPECT_THAT(reconstructed_executable->name(), \"test_module\");\n }\n \n+TEST(GpuExecutableTest, GpuExecutableDump) {\n+  tsl::Env* env = tsl::Env::Default();\n+\n+  DebugOptions debug_options;\n+  debug_options.set_xla_gpu_experimental_dump_gpu_executable(true);\n+  TF_ASSERT_OK_AND_ASSIGN(TemporaryDirectory temp_dir,\n+                          TemporaryDirectory::CreateForCurrentTestcase());\n+  debug_options.set_xla_dump_to(temp_dir.path());\n+  debug_options.set_xla_enable_dumping(true);\n+\n+  auto create_executable = [&]() {\n+    BufferAllocation alloc(0, 1024, 0);\n+    BufferAllocation::Slice slice(&alloc, 0, 1024);\n+\n+    ThunkSequence thunk_sequence;\n+    thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+        ThunkInfoWithId(123),\n+        /*kernel_name=*/\"test_kernel\",\n+        /*kernel_arguments=*/emitters::KernelArguments({}),\n+        /*launch_dimensions=*/LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0,\n+        /*tma_metadata=*/se::gpu::TmaMetadata()));\n+    thunk_sequence.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n+        ThunkInfoWithId(456), slice, slice, 1024));\n+\n+    GpuExecutable::Params params;\n+    params.executable = std::make_unique<SequentialThunk>(\n+        ThunkInfoWithId(789), std::move(thunk_sequence));\n+    params.debug_options = debug_options;\n+\n+    params.module_name = \"test_module\";\n+    params.debug_module =\n+        std::make_unique<HloModule>(params.module_name, HloModuleConfig());\n+    params.debug_module->mutable_config().set_debug_options(debug_options);\n+    return GpuExecutable::Create(std::move(params));\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable,\n+                          create_executable());\n+  ExecutableBuildOptions build_options;\n+  build_options.set_num_replicas(2);\n+  build_options.set_num_partitions(1);\n+  // Thread pool is not serializable, and should be ignored in the dump.\n+  tsl::thread::ThreadPool pool(tsl::Env::Default(), \"test_pool\", 1);\n+  build_options.set_compile_thread_pool(&pool);\n+  TF_ASSERT_OK(\n+      executable->DumpExecutableIfEnabled(build_options, debug_options));\n+\n+  std::vector<std::string> dump_files;\n+  TF_ASSERT_OK(env->GetMatchingPaths(\n+      tsl::io::JoinPath(debug_options.xla_dump_to(), \"*gpu_executable.txt\"),\n+      &dump_files));\n+  ASSERT_EQ(dump_files.size(), 1);\n+\n+  ExecutableAndOptionsProto dump_content;\n+  TF_ASSERT_OK(tsl::ReadTextProto(env, dump_files[0], &dump_content));\n+  EXPECT_THAT(dump_content.compile_options().executable_build_options(),\n+              Partially(EqualsProto(R\"pb(\n+                num_replicas: 2 num_partitions: 1\n+              )pb\")));\n+\n+  GpuExecutableProto gpu_executable_proto;\n+  ASSERT_TRUE(gpu_executable_proto.ParseFromString(\n+      dump_content.serialized_executable()));\n+  ASSERT_THAT(gpu_executable_proto, Partially(EqualsProto(R\"pb(\n+                module_name: \"test_module\"\n+                thunk {\n+                  thunk_info { thunk_id: 789 }\n+                  sequential_thunk: {\n+                    thunks: {\n+                      thunk_info: { thunk_id: 123 }\n+                      kernel_thunk: { kernel_name: \"test_kernel\" }\n+                    }\n+                    thunks: {\n+                      thunk_info: { thunk_id: 456 }\n+                      device_to_device_copy_thunk: {}\n+                    }\n+                  }\n+                }\n+              )pb\")));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 265,
        "additions": 170,
        "deletions": 95
    }
}