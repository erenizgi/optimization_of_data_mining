{
    "author": "junwhanahn",
    "message": "Use `xla::ifrt::HloSharding` for executable outputs\n\nPiperOrigin-RevId: 817726203",
    "sha": "f41c626f12d79bd5bc8e1678a7ef5ca0c17720e1",
    "files": [
        {
            "sha": "0456a1bce16476056972a34c2f0eb0288c918e94",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 15,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f41c626f12d79bd5bc8e1678a7ef5ca0c17720e1/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f41c626f12d79bd5bc8e1678a7ef5ca0c17720e1/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=f41c626f12d79bd5bc8e1678a7ef5ca0c17720e1",
            "patch": "@@ -63,6 +63,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_host_callback.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_memory.h\"\n #include \"xla/python/pjrt_ifrt/xla_compiler.h\"\n+#include \"xla/python/pjrt_ifrt/xla_sharding.h\"\n #include \"xla/service/global_device_id.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/shape.h\"\n@@ -123,20 +124,21 @@ GetFirstModuleOutputDimensions(\n \n // Returns the output shardings of the first module in a\n // `PjRtLoadedExecutable`.\n-absl::StatusOr<std::optional<HloSharding>> GetFirstModuleOutputSharding(\n+absl::StatusOr<std::optional<xla::HloSharding>> GetFirstModuleOutputSharding(\n     xla::PjRtLoadedExecutable* pjrt_loaded_executable,\n     const xla::Shape& shape) {\n   auto output_shardings = pjrt_loaded_executable->GetOutputShardings();\n   std::optional<xla::HloSharding> result_hlo_sharding;\n   if (output_shardings.has_value()) {\n-    std::vector<HloSharding> hlo_shardings;\n+    std::vector<xla::HloSharding> hlo_shardings;\n     hlo_shardings.reserve(output_shardings->size());\n     for (const auto& sharding : *output_shardings) {\n-      TF_ASSIGN_OR_RETURN(auto hlo_sharding, HloSharding::FromProto(sharding));\n+      TF_ASSIGN_OR_RETURN(auto hlo_sharding,\n+                          xla::HloSharding::FromProto(sharding));\n       hlo_shardings.push_back(hlo_sharding);\n     }\n     if (shape.IsTuple()) {\n-      return HloSharding::Tuple(shape, hlo_shardings);\n+      return xla::HloSharding::Tuple(shape, hlo_shardings);\n     } else {\n       return hlo_shardings.front();\n     }\n@@ -381,19 +383,18 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::CreateInternal(\n \n     CHECK(xla::primitive_util::IsArrayType(element_type));\n \n-    xla::DimensionVector tile_shape_dimensions = dimensions;\n     if (sharding != nullptr) {\n-      CHECK(!sharding->IsTuple());\n-      // TODO(yueshengys): Consider overloading `HloSharding::TileShape` to\n-      // directly take `xla::DimensionVector` as inputs.\n-      tile_shape_dimensions =\n-          xla::ShapeUtil::CreateDimensionVectorFromShape(sharding->TileShape(\n-              xla::ShapeUtil::MakeShape(element_type, dimensions)));\n+      output_shardings.push_back(ifrt::HloSharding::Create(\n+          executable_devices, memory_kind, *sharding));\n+    } else {\n+      // Assume a traditional replication computation where tile shapes are\n+      // the same as global shapes.\n+      const xla::DimensionVector& tile_shape_dimensions = dimensions;\n+      output_shardings.push_back(ifrt::ConcreteEvenSharding::Create(\n+          executable_devices, memory_kind,\n+          /*shape=*/ifrt::Shape(dimensions),\n+          /*shard_shape=*/ifrt::Shape(tile_shape_dimensions)));\n     }\n-    output_shardings.push_back(ifrt::ConcreteEvenSharding::Create(\n-        executable_devices, memory_kind,\n-        /*shape=*/ifrt::Shape(dimensions),\n-        /*shard_shape=*/ifrt::Shape(tile_shape_dimensions)));\n     return absl::OkStatus();\n   };\n   auto append_token = [&](MemoryKind memory_kind) {"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 16,
        "deletions": 15
    }
}