{
    "author": "unknown",
    "message": "[XLA:GPU] Make float check more parallel\n\nThe goal is to count NaNs/infs/zeros in a buffer of floats, and append\nthe results to a BufferDebugLog stored in device memory. This used to be\ndone on a single thread block with poor performance.\n\nThis CL changes it to a 2-step process:\n1. Do partial accumulation into a temporary buffer.\n2. Use a second kernel to reduce partial results down into scalars and\n   append them to the log.\n\nThis also includes some optimizations suggested by gflegar:\n* Use array-of-structs over struct-of-arrays for __shared__ memory in\n  step 1\n* Always use 1024 threads per block to avoid switching at kernel runtime\n* Read global memory 128bits a time\n\nPiperOrigin-RevId: 842289188",
    "sha": "19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
    "files": [
        {
            "sha": "27b4149b9e25bdae5c75df5bbbcdf1d63bfd211f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -3259,6 +3259,7 @@ cc_library(\n         \":thunk_id\",\n         \":thunk_pass_pipeline\",\n         \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/gpu:ffi\",\n@@ -3287,6 +3288,7 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_googlesource_code_re2//:re2\",\n+        \"@eigen_archive//:eigen3\",\n     ],\n )\n \n@@ -3463,9 +3465,13 @@ cc_library(\n         \":buffer_debug_log_structs\",\n         \":thunk\",\n         \"//xla:types\",\n+        \"//xla:util\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n@@ -3477,6 +3483,7 @@ cc_library(\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/synchronization\","
        },
        {
            "sha": "9d3492ae964f332b7fb049a11bac03ac581e68f3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_debug_log_structs.h",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_structs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_structs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_debug_log_structs.h?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -54,6 +54,25 @@ static_assert(sizeof(BufferDebugLogEntry) == sizeof(uint32_t) * 2);\n static_assert(offsetof(BufferDebugLogEntry, entry_id) == 0);\n static_assert(offsetof(BufferDebugLogEntry, value) == sizeof(uint32_t));\n \n+struct FloatCheckResult {\n+  uint32_t nan_count;\n+  uint32_t inf_count;\n+  uint32_t zero_count;\n+\n+  template <typename Sink>\n+  friend void AbslStringify(Sink& sink, const FloatCheckResult& result) {\n+    absl::Format(&sink, \"{nan_count: %u, inf_count: %u, zero_count: %u}\",\n+                 result.nan_count, result.inf_count, result.zero_count);\n+  }\n+};\n+\n+// The struct layout must match on both host and device.\n+static_assert(_Alignof(FloatCheckResult) == _Alignof(uint32_t));\n+static_assert(sizeof(FloatCheckResult) == sizeof(uint32_t) * 3);\n+static_assert(offsetof(FloatCheckResult, nan_count) == 0);\n+static_assert(offsetof(FloatCheckResult, inf_count) == sizeof(uint32_t));\n+static_assert(offsetof(FloatCheckResult, zero_count) == sizeof(uint32_t) * 2);\n+\n struct BufferDebugFloatCheckEntry {\n   // An ID that uniquely identifies a log entry within a HLO module execution.\n   BufferDebugLogEntryId entry_id;"
        },
        {
            "sha": "d6b8b04c70c47a2150564e7db064bea05326cc02",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 11,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.cc?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -15,11 +15,15 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/buffers_float_check_thunk.h\"\n \n+#include <algorithm>\n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <string>\n+#include <tuple>\n #include <utility>\n \n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -30,14 +34,18 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n #include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_float_check_kernel.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/types.h\"\n+#include \"xla/util.h\"\n \n namespace xla::gpu {\n \n@@ -73,15 +81,33 @@ absl::Status BuffersDebugFloatCheckThunk::Initialize(\n           auto kernel_bf16,\n           registry.LoadKernel<se::gpu::BufferDebugFloatCheckBf16Kernel>(\n               params.executor));\n+      TF_ASSIGN_OR_RETURN(\n+          auto kernel_reduce,\n+          registry.LoadKernel<\n+              se::gpu::BufferDebugAppendReducedFloatCheckResultsKernel>(\n+              params.executor));\n       kernels_[params.executor] = std::make_unique<Kernels>(\n-          Kernels{std::move(kernel_f32), std::move(kernel_bf16)});\n+          Kernels{std::move(kernel_f32), std::move(kernel_bf16),\n+                  std::move(kernel_reduce)});\n+      VLOG(1) << \"NanCount kernels loaded\";\n     }\n   }\n \n-  VLOG(1) << \"FloatCheck kernel loaded\";\n   return absl::OkStatus();\n }\n \n+template <typename T>\n+se::BlockDim GetBlockDimForBuffer(se::Stream* stream,\n+                                  se::DeviceMemory<T> buffer,\n+                                  int64_t max_blocks) {\n+  const int64_t num_elements = buffer.size() / sizeof(T);\n+  const se::DeviceDescription& desc = stream->parent()->GetDeviceDescription();\n+  const int64_t num_blocks =\n+      std::min(xla::CeilOfRatio(num_elements, desc.threads_per_block_limit()),\n+               max_blocks);\n+  return se::BlockDim(num_blocks);\n+}\n+\n absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   se::StreamExecutor* executor = params.stream->parent();\n@@ -102,15 +128,22 @@ absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n \n   VLOG(1) << \"BuffersDebugFloatCheckThunk::ExecuteOnStream\";\n \n-  const se::ThreadDim thread_dim(\n-      executor->GetDeviceDescription().threads_per_block_limit(), 1, 1);\n+  se::DeviceAddress<xla::gpu::FloatCheckResult> tmp_ptr(\n+      params.buffer_allocations->GetDeviceAddress(tmp_slice_));\n+  const size_t tmp_size_elements =\n+      tmp_slice_.size() / sizeof(xla::gpu::FloatCheckResult);\n+  CHECK_GT(tmp_size_elements, 0)\n+      << \"tmp_slice_ is too small to hold any results, this should have been \"\n+         \"caught during initialization\";\n \n   se::DeviceAddress<uint8_t> log_ptr(\n       params.buffer_allocations->GetDeviceAddress(log_slice_));\n   se::gpu::BufferDebugLog<BufferDebugFloatCheckEntry> buffer_debug_log =\n       se::gpu::BufferDebugLog<\n           BufferDebugFloatCheckEntry>::FromDeviceAddressUnchecked(log_ptr);\n   const uint32_t execution_id = execution_count_.fetch_add(1);\n+  // The kernel assumes 1024 threads per block.\n+  const se::ThreadDim thread_dim(1024);\n \n   for (const auto& [buffer_idx, buffer] : checked_thunk_buffers_) {\n     BufferDebugLogEntryMetadataStore::Metadata metadata{\n@@ -130,22 +163,32 @@ absl::Status BuffersDebugFloatCheckThunk::ExecuteOnStream(\n       VLOG(1) << \"F32 buffer detected with id: \" << entry_id\n               << \" and size: \" << device_buffer.size();\n       se::DeviceAddress<float> f32_buffer(device_buffer);\n-      TF_RETURN_IF_ERROR(kernels->f32.Launch(\n-          thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n-          f32_buffer, f32_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n-          buffer_debug_log.GetDeviceEntries()));\n+      const se::BlockDim block_dim = GetBlockDimForBuffer<float>(\n+          params.stream, f32_buffer, tmp_size_elements);\n+      TF_RETURN_IF_ERROR(\n+          kernels->f32.Launch(thread_dim, block_dim, params.stream, f32_buffer,\n+                              f32_buffer.size(), tmp_ptr, tmp_size_elements));\n     } else if (buffer_type == PrimitiveType::BF16) {\n       VLOG(1) << \"BF16 buffer detected with id: \" << entry_id\n               << \" and size: \" << device_buffer.size();\n       se::DeviceAddress<Eigen::bfloat16> bf16_buffer(device_buffer);\n+      const se::BlockDim block_dim = GetBlockDimForBuffer<Eigen::bfloat16>(\n+          params.stream, bf16_buffer, tmp_size_elements);\n       TF_RETURN_IF_ERROR(kernels->bf16.Launch(\n-          thread_dim, se::BlockDim(1, 1, 1), params.stream, entry_id,\n-          bf16_buffer, bf16_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n-          buffer_debug_log.GetDeviceEntries()));\n+          thread_dim, block_dim, params.stream, bf16_buffer, bf16_buffer.size(),\n+          tmp_ptr, tmp_size_elements));\n     } else {\n       VLOG(1) << \"Unsupported primitive type for float checking: \"\n               << PrimitiveType_Name(buffer_type);\n+      continue;\n     }\n+\n+    // Operations on the same stream perform in sequence, so at this point the\n+    // results of the previous FloatCheck operation are available.\n+    TF_RETURN_IF_ERROR(kernels->reduce.Launch(\n+        thread_dim, se::BlockDim(1, 1, 1), params.stream, tmp_ptr,\n+        tmp_size_elements, entry_id, buffer_debug_log.GetDeviceHeader(),\n+        buffer_debug_log.GetDeviceEntries()));\n   }\n \n   return absl::OkStatus();"
        },
        {
            "sha": "f73c9ef305fde662951775723972676042942db6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk.h?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include <atomic>\n #include <cstddef>\n+#include <cstdint>\n #include <memory>\n #include <string>\n #include <utility>\n@@ -38,18 +39,21 @@ class BuffersDebugFloatCheckThunk : public Thunk {\n  public:\n   explicit BuffersDebugFloatCheckThunk(\n       ThunkInfo info, const ThunkInfo& checked_thunk_info,\n-      BufferAllocation::Slice log_slice,\n+      BufferAllocation::Slice log_slice, BufferAllocation::Slice tmp_slice,\n       absl::flat_hash_map<size_t, BufferAllocation::Slice>\n           checked_thunk_buffers,\n       std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store)\n       : Thunk(Thunk::Kind::kBuffersDebugFloatCheck, std::move(info)),\n         log_slice_(log_slice),\n+        tmp_slice_(tmp_slice),\n         checked_thunk_info_(checked_thunk_info),\n         checked_thunk_buffers_(std::move(checked_thunk_buffers)),\n         metadata_store_(std::move(metadata_store)) {}\n \n-  absl::Status Initialize(const InitializeParams& params) override;\n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+  absl::Status Initialize(const InitializeParams& params) override\n+      ABSL_LOCKS_EXCLUDED(kernels_mutex_);\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override\n+      ABSL_LOCKS_EXCLUDED(kernels_mutex_);\n \n   std::string ToString(int indent) const override;\n \n@@ -67,6 +71,8 @@ class BuffersDebugFloatCheckThunk : public Thunk {\n   struct Kernels {\n     stream_executor::gpu::BufferDebugFloatCheckF32Kernel::KernelType f32;\n     stream_executor::gpu::BufferDebugFloatCheckBf16Kernel::KernelType bf16;\n+    stream_executor::gpu::BufferDebugAppendReducedFloatCheckResultsKernel::\n+        KernelType reduce;\n   };\n   absl::Mutex kernels_mutex_;\n   // Each loaded kernel is associated with a specific device (represented by its\n@@ -79,6 +85,7 @@ class BuffersDebugFloatCheckThunk : public Thunk {\n       kernels_ ABSL_GUARDED_BY(kernels_mutex_);\n \n   BufferAllocation::Slice log_slice_;\n+  BufferAllocation::Slice tmp_slice_;\n   ThunkInfo checked_thunk_info_;\n   absl::flat_hash_map<size_t, BufferAllocation::Slice> checked_thunk_buffers_;\n   std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store_;"
        },
        {
            "sha": "c56538c15a6e396c94cec0b91b688058c333a456",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_float_check_thunk_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 10,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_float_check_thunk_test.cc?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -101,17 +101,20 @@ class BuffersDebugFloatCheckThunkTest : public ::testing::Test {\n TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n   static constexpr size_t kLogSize =\n       BufferDebugLog<BufferDebugFloatCheckEntry>::RequiredSizeForEntries(10);\n+  static constexpr size_t kTmpSizeElems = 1024;\n+  static constexpr size_t kTmpSizeBytes = kTmpSizeElems * sizeof(uint32_t);\n   static constexpr size_t kInputElems = 1024;\n   static constexpr size_t kInputSizeInBytes = kInputElems * sizeof(float);\n   static constexpr size_t kTotalDeviceMemoryBytes =\n-      kLogSize + kInputSizeInBytes * 2;\n+      kLogSize + kTmpSizeBytes + kInputSizeInBytes * 2;\n   // Setup memory allocations for the log and inputs\n   BufferAllocation alloc(/*index=*/0,\n                          /*size=*/kTotalDeviceMemoryBytes,\n                          /*color=*/0);\n   int64_t input_offset = kLogSize;\n   BufferAllocation::Slice log_slice(&alloc, /*offset=*/0, kLogSize);\n-  input_offset += kLogSize;\n+  BufferAllocation::Slice tmp_slice(&alloc, /*offset=*/kLogSize, kTmpSizeBytes);\n+  input_offset += kLogSize + kTmpSizeBytes;\n \n   BufferAllocation::Slice inputs[2];\n   int64_t input_size_bf16 = kInputElems * sizeof(Eigen::bfloat16);\n@@ -159,7 +162,7 @@ TEST_F(BuffersDebugFloatCheckThunkTest, CalculatesNanCounts) {\n   Thunk::ThunkInfo checked_thunk_info;\n   checked_thunk_info.thunk_id = ThunkId(123);\n   BuffersDebugFloatCheckThunk thunk(\n-      Thunk::ThunkInfo(), checked_thunk_info, log_slice,\n+      Thunk::ThunkInfo(), checked_thunk_info, log_slice, tmp_slice,\n       {{/*buffer_idx=*/0, inputs[0]}, {/*buffer_idx=*/1, inputs[1]}},\n       metadata_store);\n   TF_ASSERT_OK(thunk.Initialize(init_params));\n@@ -202,8 +205,13 @@ TEST_F(BuffersDebugFloatCheckThunkTest,\n     GTEST_SKIP() << \"need at least 2 devices for this test\";\n   }\n \n+  static constexpr size_t kLogOffset = 0;\n   static constexpr size_t kLogSizeBytes = 1024;\n+  static constexpr size_t kTmpOffset = kLogOffset + kLogSizeBytes;\n+  static constexpr size_t kTmpSizeBytes = 1024 * sizeof(uint32_t);\n+  static constexpr size_t kInputOffset = kTmpOffset + kTmpSizeBytes;\n   static constexpr size_t kInputSizeBytes = 1024;\n+  static constexpr size_t kTotalDeviceMemory = kInputOffset + kInputSizeBytes;\n \n   struct TestDevice {\n     se::StreamExecutor* executor;\n@@ -219,24 +227,25 @@ TEST_F(BuffersDebugFloatCheckThunkTest,\n     auto allocator =\n         std::make_unique<se::StreamExecutorMemoryAllocator>(executor);\n     BufferAllocations allocations(\n-        {executor->AllocateArray<uint8_t>(kLogSizeBytes + kInputSizeBytes)},\n+        {executor->AllocateArray<uint8_t>(kTotalDeviceMemory)},\n         executor->device_ordinal(), allocator.get());\n \n     return TestDevice{std::move(executor), std::move(stream),\n                       std::move(allocator), std::move(allocations)};\n   };\n   TF_ASSERT_OK_AND_ASSIGN(TestDevice device0, setup_device(0));\n   TF_ASSERT_OK_AND_ASSIGN(TestDevice device1, setup_device(1));\n-  BufferAllocation allocation(0, kLogSizeBytes + kInputSizeBytes, 0);\n-  BufferAllocation::Slice log_slice(&allocation, 0, kLogSizeBytes);\n-  BufferAllocation::Slice f32_slice(&allocation, kLogSizeBytes, kInputSizeBytes,\n+  BufferAllocation allocation(/*index=*/0, kTotalDeviceMemory, /*color=*/0);\n+  BufferAllocation::Slice log_slice(&allocation, kLogOffset, kLogSizeBytes);\n+  BufferAllocation::Slice tmp_slice(&allocation, kTmpOffset, kTmpSizeBytes);\n+  BufferAllocation::Slice f32_slice(&allocation, kInputOffset, kInputSizeBytes,\n                                     PrimitiveType::F32);\n-  BufferAllocation::Slice bf16_slice(&allocation, kLogSizeBytes,\n-                                     kInputSizeBytes, PrimitiveType::BF16);\n+  BufferAllocation::Slice bf16_slice(&allocation, kInputOffset, kInputSizeBytes,\n+                                     PrimitiveType::BF16);\n   Thunk::ThunkInfo checked_thunk_info;\n   checked_thunk_info.thunk_id = ThunkId(123);\n   BuffersDebugFloatCheckThunk thunk(\n-      Thunk::ThunkInfo(), checked_thunk_info, log_slice,\n+      Thunk::ThunkInfo(), checked_thunk_info, log_slice, tmp_slice,\n       {{/*buffer_idx=*/0, f32_slice}, {/*buffer_idx=*/1, bf16_slice}},\n       std::make_shared<BufferDebugLogEntryMetadataStore>());\n "
        },
        {
            "sha": "808142a0e2a45c28e72fb9d26de8c0c4a9c52b2a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_float_check.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 6,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_float_check.cc?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -15,7 +15,10 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/thunk_buffer_debug_float_check.h\"\n \n+#include <algorithm>\n+#include <cmath>\n #include <cstddef>\n+#include <cstdint>\n #include <cstring>\n #include <memory>\n #include <optional>\n@@ -32,6 +35,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"Eigen/Core\"\n #include \"xla/backends/gpu/ffi.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log.pb.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n@@ -59,6 +63,7 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -72,10 +77,41 @@ constexpr size_t kLogSizeBytes = 64 * 1024;\n \n namespace {\n \n-std::unique_ptr<Thunk> WrapWithFloatCheckThunk(\n+size_t CalculateTempBufferSize(const Thunk& thunk) {\n+  size_t max_buffer_size_bytes = 0;\n+  for (const BufferUse& use : thunk.buffer_uses()) {\n+    if (use.HasDefinedContentsOnInput() || use.HasDefinedContentsOnOutput()) {\n+      max_buffer_size_bytes =\n+          std::max<size_t>(max_buffer_size_bytes, use.slice().size());\n+    }\n+  }\n+\n+  // We're doing the float checks in 2 steps:\n+  // - parallel aggregation: one thread block writes partial result into the\n+  //   temp buffer. The number of thread blocks used will be limtied by the size\n+  //   calculated here.\n+  // - reduction of the temp buffer on a single thread block\n+  // To optimize for time, we want to do as much computation in parallel as we\n+  // can, but also consider the overhead of single-block reduction step.\n+\n+  // Avoid making the reduction step use less than a block's worth of data. We\n+  // can't go any faster than that anyway.\n+  static constexpr size_t kMinElements = 1024;\n+  // Arbitrary limit of 1Mi elements. This should be enough to accomodate the\n+  // max number of thread blocks available on any supported GPU.\n+  static constexpr size_t kMaxElements = 1024 * 1024;\n+  const size_t size_elems =\n+      xla::CeilOfRatio(max_buffer_size_bytes, sizeof(uint32_t));\n+  const size_t sqrt_size_elems = std::sqrt(size_elems);\n+  return std::clamp(xla::CeilOfRatio(size_elems, sqrt_size_elems), kMinElements,\n+                    kMaxElements);\n+}\n+\n+absl::StatusOr<std::unique_ptr<Thunk>> WrapWithFloatCheckThunk(\n     std::unique_ptr<Thunk> thunk, BufferAllocation::Slice log_slice,\n     const Thunk& predecessor_thunk, Thunk& successor_thunk,\n-    std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store) {\n+    std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store,\n+    ThunkPassBufferAllocator& allocator) {\n   const auto& thunk_buffers = thunk->buffer_uses();\n   if (thunk_buffers.empty()) {\n     VLOG(1) << \"No buffers in thunk \" << thunk->thunk_info().thunk_id\n@@ -120,6 +156,12 @@ std::unique_ptr<Thunk> WrapWithFloatCheckThunk(\n     return thunk;\n   }\n \n+  const size_t temp_buffer_size_bytes =\n+      CalculateTempBufferSize(*thunk) * sizeof(xla::gpu::FloatCheckResult);\n+  TF_ASSIGN_OR_RETURN(BufferAllocation * tmp_alloc,\n+                      allocator.NewEmptyAllocation(temp_buffer_size_bytes));\n+  BufferAllocation::Slice tmp_slice(tmp_alloc, 0, tmp_alloc->size());\n+\n   VLOG(1) << \"Wrapping thunk \" << thunk->thunk_info().thunk_id\n           << \" with float check thunk due to presence of buffers: \"\n           << buffers_to_check.size();\n@@ -128,7 +170,7 @@ std::unique_ptr<Thunk> WrapWithFloatCheckThunk(\n   thunk_and_checks.push_back(std::move(thunk));\n   auto buffer_debug_float_check_thunk =\n       std::make_unique<BuffersDebugFloatCheckThunk>(\n-          Thunk::ThunkInfo(), thunk_ptr->thunk_info(), log_slice,\n+          Thunk::ThunkInfo(), thunk_ptr->thunk_info(), log_slice, tmp_slice,\n           std::move(buffers_to_check), std::move(metadata_store));\n   buffer_debug_float_check_thunk->add_control_predecessor(thunk_ptr);\n   thunk_and_checks.push_back(std::move(buffer_debug_float_check_thunk));\n@@ -329,16 +371,18 @@ absl::Status RunFloatCheckPassInternal(SequentialThunk* root_thunk,\n       CreateBufferDebugFloatCheckThunk(metadata_store, log_slice, hlo_module));\n \n   ThunkFilter thunk_filter = CreateThunkFilter(debug_options);\n-  TF_RETURN_IF_ERROR(\n-      root_thunk->TransformAllNestedThunks([&](std::unique_ptr<Thunk> thunk) {\n+  TF_RETURN_IF_ERROR(root_thunk->TransformAllNestedThunks(\n+      [&](std::unique_ptr<Thunk> thunk)\n+          -> absl::StatusOr<std::unique_ptr<Thunk>> {\n         if (thunk_filter(*thunk) == InstrumentAction::kSkip) {\n           return thunk;\n         }\n         VLOG(1) << \"Wrapping with float check thunk\";\n         return WrapWithFloatCheckThunk(\n             std::move(thunk), log_slice,\n             /*predecessor_thunk=*/*buffer_debug_init_thunk,\n-            /*successor_thunk=*/*buffer_debug_dump_thunk, metadata_store);\n+            /*successor_thunk=*/*buffer_debug_dump_thunk, metadata_store,\n+            allocator);\n       }));\n \n   ThunkSequence& thunks = root_thunk->thunks();"
        },
        {
            "sha": "62bc737997450a8e2850c7ce880a8177ff01dbe4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_buffer_debug_pass_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_buffer_debug_pass_test.cc?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -58,6 +58,7 @@ namespace {\n \n using testing::ElementsAre;\n using testing::Eq;\n+using testing::IsEmpty;\n using testing::Pair;\n using testing::Pointer;\n using testing::SizeIs;\n@@ -102,17 +103,16 @@ using SliceList =\n class FakeThunkPassBufferAllocator : public ThunkPassBufferAllocator {\n  public:\n   absl::StatusOr<BufferAllocation*> NewEmptyAllocation(int64_t size) override {\n-    if (CreatedAlloc()) {\n-      return absl::InvalidArgumentError(\"Expected only one allocation\");\n-    }\n-    alloc_ = std::make_unique<BufferAllocation>(0, size, 0);\n-    return alloc_.get();\n+    allocs_.push_back(std::make_unique<BufferAllocation>(0, size, 0));\n+    return allocs_.back().get();\n   }\n \n-  bool CreatedAlloc() { return alloc_ != nullptr; }\n+  const std::vector<std::unique_ptr<BufferAllocation>>& allocs() const {\n+    return allocs_;\n+  }\n \n  private:\n-  std::unique_ptr<BufferAllocation> alloc_;\n+  std::vector<std::unique_ptr<BufferAllocation>> allocs_;\n };\n \n class FakeThunk : public Thunk {\n@@ -188,6 +188,7 @@ TEST_F(ThunkBufferDebugPassTest, IsNoOpWhenHloModuleIsNull) {\n                              /*hlo_module=*/nullptr, device_info, allocator));\n   EXPECT_FALSE(changed);\n   EXPECT_THAT(root_thunk->thunks(), ElementsAre(Pointer(fake_thunk_ptr)));\n+  EXPECT_THAT(allocator.allocs(), IsEmpty());\n }\n \n TEST_F(ThunkBufferDebugPassTest, InsertsBuffersDebugChecksumThunks) {\n@@ -256,6 +257,8 @@ TEST_F(ThunkBufferDebugPassTest, InsertsBuffersDebugChecksumThunks) {\n                                                 {2, slice_io},\n                                             }))),\n           IsCustomCallThunkWithTargetName(\"xla_gpu_buffer_debug_log_dump\")));\n+\n+  EXPECT_THAT(allocator.allocs(), SizeIs(1));\n }\n \n TEST_F(ThunkBufferDebugPassTest, RecursivelyInsertsBuffersDebugChecksumThunks) {\n@@ -461,6 +464,8 @@ TEST_F(ThunkBufferDebugPassTest, RecursivelyInsertsBuffersDebugChecksumThunks) {\n                     Pointer(branch1_thunk_ptr),\n                     IsChecksumThunkChecking(SliceList{{0, slice_branch1}})));\n   }\n+\n+  EXPECT_THAT(allocator.allocs(), SizeIs(1));\n }\n \n TEST_F(ThunkBufferDebugPassTest, InsertsBuffersDebugFloatCheckThunks) {\n@@ -544,6 +549,9 @@ TEST_F(ThunkBufferDebugPassTest, InsertsBuffersDebugFloatCheckThunks) {\n       static_cast<const BuffersDebugFloatCheckThunk&>(*sub_thunks[1]);\n   EXPECT_THAT(buffer_debug_after_fake_thunk.buffer_slices(),\n               UnorderedElementsAre(Pair(1, slice_o), Pair(2, slice_io)));\n+\n+  // 1 for the log buffer, 1 per wrapped thunk for the temp buffer\n+  EXPECT_THAT(allocator.allocs(), SizeIs(2));\n }\n \n TEST_F(ThunkBufferDebugPassTest, BufferSaverInserter) {"
        },
        {
            "sha": "1342a13898784c46220348b60a5616e8ed8a3afc",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -453,7 +453,9 @@ cuda_library(\n         \"gpu\",\n     ],\n     deps = [\n+        \":cuda_platform\",\n         \":cuda_platform_id\",\n+        \"//xla:util\",\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n         \"//xla/stream_executor:kernel_spec\",\n         \"//xla/stream_executor/gpu:buffer_debug_float_check_kernel\",\n@@ -475,6 +477,7 @@ xla_test(\n         \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n         \"//xla/backends/gpu/runtime:thunk_id\",\n         \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\","
        },
        {
            "sha": "4f6e94ab7ccce2bf68a6639f25050ceb46ad1cdf",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_float_check_kernel_cuda.cu.cc",
            "status": "modified",
            "additions": 185,
            "deletions": 157,
            "changes": 342,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda.cu.cc?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -13,9 +13,14 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <algorithm>\n+#include <array>\n #include <cassert>\n #include <cmath>\n+#include <cstddef>\n #include <cstdint>\n+#include <optional>\n+#include <tuple>\n \n #include \"absl/base/casts.h\"\n #include \"third_party/gpus/cuda/include/cuda/atomic\"\n@@ -24,11 +29,29 @@ limitations under the License.\n #include \"xla/stream_executor/gpu/buffer_debug_float_check_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n #include \"xla/stream_executor/kernel_spec.h\"\n+#include \"xla/util.h\"\n \n namespace se = stream_executor;\n \n namespace {\n \n+using xla::gpu::FloatCheckResult;\n+\n+// https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/:\n+// > CUDA architecture limits the numbers of threads per block (1024 threads\n+// > per block limit).\n+static constexpr uint64_t kBlockSize = 1024;\n+// warpSize is not a compile time constant on all OSS CI builds, but we need it\n+// to be one for static array initialization. We assert this value matches\n+// warpSize at runtime.\n+static constexpr uint64_t kWarpSize = 32;\n+static constexpr uint64_t kMaxWarpsPerBlock = kBlockSize / kWarpSize;\n+template <typename T>\n+static constexpr uint64_t kElementsPerMemoryAccess =\n+    std::max<uint64_t>(16 / sizeof(T), 1);\n+template <typename T>\n+using Chunk = std::array<T, kElementsPerMemoryAccess<T>>;\n+\n __device__ unsigned int ThreadIdx() {\n   return threadIdx.z * blockDim.y * blockDim.x + threadIdx.y * blockDim.x +\n          threadIdx.x;\n@@ -39,16 +62,57 @@ __device__ unsigned int BlockIdx() {\n          blockIdx.x;\n }\n \n-// Based on\n-// https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf\n-template <unsigned int BLOCK_SIZE>\n-__device__ void WarpReduceSum(unsigned int tid, volatile uint32_t* data) {\n-  if (BLOCK_SIZE >= 64) data[tid] += data[tid + 32];\n-  if (BLOCK_SIZE >= 32) data[tid] += data[tid + 16];\n-  if (BLOCK_SIZE >= 16) data[tid] += data[tid + 8];\n-  if (BLOCK_SIZE >= 8) data[tid] += data[tid + 4];\n-  if (BLOCK_SIZE >= 4) data[tid] += data[tid + 2];\n-  if (BLOCK_SIZE >= 2) data[tid] += data[tid + 1];\n+// Reduce a warp worth of values into a single one and have the 0th thread in\n+// the warp return it.\n+__device__ uint32_t WarpReduceSum(uint32_t value) {\n+  static constexpr uint32_t kFullMask = ~0;\n+  for (unsigned int offset = 1; offset < kWarpSize; offset <<= 1) {\n+    value += __shfl_down_sync(kFullMask, value, offset);\n+  }\n+  return value;\n+}\n+\n+// Sum up a block worth of FloatCheckResults into a single one and have the 0th\n+// thread in the block return it.\n+__device__ FloatCheckResult BlockReduceSum(uint32_t tid,\n+                                           FloatCheckResult value) {\n+  assert(kWarpSize == warpSize);\n+  static_assert(kBlockSize == kWarpSize * kMaxWarpsPerBlock);\n+  // Required to do the second warp reduction.\n+  static_assert(kMaxWarpsPerBlock == kWarpSize);\n+\n+  const size_t warp_idx = tid / kWarpSize;\n+  const size_t lane_idx = tid % kWarpSize;\n+\n+  value.nan_count = WarpReduceSum(value.nan_count);\n+  value.inf_count = WarpReduceSum(value.inf_count);\n+  value.zero_count = WarpReduceSum(value.zero_count);\n+\n+  __shared__ uint32_t scratch_nan[kMaxWarpsPerBlock];\n+  __shared__ uint32_t scratch_inf[kMaxWarpsPerBlock];\n+  __shared__ uint32_t scratch_zero[kMaxWarpsPerBlock];\n+  if (lane_idx == 0) {\n+    scratch_nan[warp_idx] = value.nan_count;\n+    scratch_inf[warp_idx] = value.inf_count;\n+    scratch_zero[warp_idx] = value.zero_count;\n+  }\n+\n+  __syncthreads();\n+  // The first warp reduces the results from all warps.\n+  if (warp_idx == 0) {\n+    value.nan_count = scratch_nan[lane_idx];\n+    value.inf_count = scratch_inf[lane_idx];\n+    value.zero_count = scratch_zero[lane_idx];\n+    value.nan_count = WarpReduceSum(value.nan_count);\n+    value.inf_count = WarpReduceSum(value.inf_count);\n+    value.zero_count = WarpReduceSum(value.zero_count);\n+  } else {\n+    value.nan_count = 0;\n+    value.inf_count = 0;\n+    value.zero_count = 0;\n+  }\n+\n+  return value;\n }\n \n __device__ inline bool IsNan(float v) { return isnan(v); }\n@@ -60,190 +124,149 @@ __device__ inline bool IsZero(__nv_bfloat16 v) {\n   return v == __nv_bfloat16(0.0f);\n }\n \n-// Calculates count of NaNs of all elements of `input` and puts result in\n-// `output`.\n-//\n-// Optimized implementation based on\n-// https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf\n-// that takes advantage of `BLOCK_SIZE` threads.\n-//\n-// `BLOCK_SIZE` must be a power of 2 no larger than 1024.\n-template <typename T, unsigned int BLOCK_SIZE>\n-__device__ void ReduceSum(const T* input, uint64_t input_size,\n-                          uint32_t* nan_counter, uint32_t* inf_counter,\n-                          uint32_t* zero_counter) {\n-  __shared__ uint32_t nan_count[BLOCK_SIZE];\n-  __shared__ uint32_t inf_count[BLOCK_SIZE];\n-  __shared__ uint32_t zero_count[BLOCK_SIZE];\n+// Get a part of the input buffer current thread block is responsible for\n+// processing, assuming the load is spread up to max_blocks across the entire\n+// grid. If max_blocks is not provided, the entire grid is used.\n+template <typename T>\n+__device__ inline std::tuple<const T*, uint64_t> GetBlockInput(\n+    const T* input, uint64_t input_size,\n+    std::optional<uint64_t> max_blocks = std::nullopt) {\n+  size_t grid_size = gridDim.x * gridDim.y * gridDim.z;\n+  if (max_blocks.has_value()) {\n+    grid_size = std::min<size_t>(grid_size, *max_blocks);\n+  }\n+  const uint64_t max_block_input_size = xla::RoundUpTo(\n+      xla::CeilOfRatio(input_size, grid_size), kElementsPerMemoryAccess<T>);\n+  const uint64_t block_input_offset = BlockIdx() * max_block_input_size;\n+  const uint64_t block_input_size =\n+      std::min(max_block_input_size, input_size - block_input_offset);\n+  return {input + block_input_offset, block_input_size};\n+}\n \n-  assert(BlockIdx() == 0);\n+template <typename T>\n+__device__ FloatCheckResult CheckFloats(const T* input, uint64_t input_size,\n+                                        uint64_t max_blocks) {\n   const unsigned int tid = ThreadIdx();\n+  const auto [block_input, block_input_size] =\n+      GetBlockInput(input, input_size, max_blocks);\n \n-  nan_count[tid] = 0;\n-  inf_count[tid] = 0;\n-  zero_count[tid] = 0;\n-  for (unsigned int i = tid; i < input_size; i += BLOCK_SIZE) {\n-    if (IsNan(input[i])) {\n-      nan_count[tid]++;\n-    }\n-    if (IsInf(input[i])) {\n-      inf_count[tid]++;\n-    }\n-    if (IsZero(input[i])) {\n-      zero_count[tid]++;\n-    }\n-  }\n-\n-  __syncthreads();\n+  const Chunk<T>* chunked_input =\n+      reinterpret_cast<const Chunk<T>*>(block_input);\n+  const uint64_t input_chunks =\n+      xla::FloorOfRatio(block_input_size, kElementsPerMemoryAccess<T>);\n+  // This may be less than block_input_size only for the last block.\n+  const uint64_t chunked_input_size =\n+      xla::RoundDownTo(block_input_size, kElementsPerMemoryAccess<T>);\n \n-  if (BLOCK_SIZE >= 1024) {\n-    if (tid < 512) {\n-      nan_count[tid] += nan_count[tid + 512];\n-      inf_count[tid] += inf_count[tid + 512];\n-      zero_count[tid] += zero_count[tid + 512];\n+  FloatCheckResult result{};\n+  for (uint64_t i = tid; i < input_chunks; i += kBlockSize) {\n+    Chunk<T> values = chunked_input[i];\n+    for (const T value : values) {\n+      result.nan_count += IsNan(value);\n+      result.inf_count += IsInf(value);\n+      result.zero_count += IsZero(value);\n     }\n-    __syncthreads();\n   }\n-  if (BLOCK_SIZE >= 512) {\n-    if (tid < 256) {\n-      nan_count[tid] += nan_count[tid + 256];\n-      inf_count[tid] += inf_count[tid + 256];\n-      zero_count[tid] += zero_count[tid + 256];\n-    }\n-    __syncthreads();\n-  }\n-  if (BLOCK_SIZE >= 256) {\n-    if (tid < 128) {\n-      nan_count[tid] += nan_count[tid + 128];\n-      inf_count[tid] += inf_count[tid + 128];\n-      zero_count[tid] += zero_count[tid + 128];\n-    }\n-    __syncthreads();\n-  }\n-  if (BLOCK_SIZE >= 128) {\n-    if (tid < 64) {\n-      nan_count[tid] += nan_count[tid + 64];\n-      inf_count[tid] += inf_count[tid + 64];\n-      zero_count[tid] += zero_count[tid + 64];\n+\n+  if (tid == 0 && chunked_input_size < block_input_size) {\n+    const size_t rest = block_input_size - chunked_input_size;\n+    for (uint64_t j = 0; j < rest; ++j) {\n+      const T value = block_input[input_chunks + j];\n+      result.nan_count += IsNan(value);\n+      result.inf_count += IsInf(value);\n+      result.zero_count += IsZero(value);\n     }\n-    __syncthreads();\n-  }\n-  if (tid < 32) {\n-    WarpReduceSum<BLOCK_SIZE>(tid, nan_count);\n-    WarpReduceSum<BLOCK_SIZE>(tid, inf_count);\n-    WarpReduceSum<BLOCK_SIZE>(tid, zero_count);\n   }\n-  if (tid == 0) {\n-    *nan_counter = nan_count[0];\n-    *inf_counter = inf_count[0];\n-    *zero_counter = zero_count[0];\n+\n+  return BlockReduceSum(tid, result);\n+}\n+\n+__device__ FloatCheckResult ReduceResults(const FloatCheckResult* input,\n+                                          uint64_t input_size) {\n+  const unsigned int tid = ThreadIdx();\n+  const auto [block_input, block_input_size] = GetBlockInput(input, input_size);\n+\n+  FloatCheckResult result{};\n+  for (uint64_t i = tid; i < input_size; i += kBlockSize) {\n+    const FloatCheckResult value = block_input[i];\n+    result.nan_count += value.nan_count;\n+    result.inf_count += value.inf_count;\n+    result.zero_count += value.zero_count;\n   }\n+\n+  // Now reduce a block worth of values into a single one.\n+  return BlockReduceSum(tid, result);\n }\n \n-// Attempts to append the NaN count of the `input` buffer to the\n-// `float_check_entries`, using `log_header` to track available capacity and\n-// used space.\n-//\n-// The log entry is tagged with `entry_id`. The NaN count is parallelized as\n-// much as block dimensions allow it.\n-//\n-// If the log does not have enough space for the new entry, the entry is\n-// discarded.\n-//\n-// `input_size_in_bytes` is the size of the input buffer in bytes.\n-//\n-// LIMITATIONS:\n-// - Only a single thread block is supported.\n-// - Block dimensions must be a power of 2.\n+// Count the number of floats for NaNs, Infs and zeros in input buffer and store\n+// partially accumulated results in the tmp array.\n template <typename T>\n-__global__ void AppendFloatCheck(\n-    xla::gpu::BufferDebugLogEntryId entry_id, const T* input,\n-    uint64_t input_size_in_bytes, xla::gpu::BufferDebugLogHeader* log_header,\n-    xla::gpu::BufferDebugFloatCheckEntry* float_check_entries) {\n-  const uint32_t block_size = blockDim.x * blockDim.y * blockDim.z;\n-  const uint64_t input_size = input_size_in_bytes / sizeof(T);\n-  uint32_t nan_count = 0;\n-  uint32_t inf_count = 0;\n-  uint32_t zero_count = 0;\n-\n-  assert(gridDim.x == 1 && gridDim.y == 1 && gridDim.z == 1);\n-  if (BlockIdx() != 0) {\n+__global__ void FloatCheck(const T* input, uint64_t input_size,\n+                           xla::gpu::FloatCheckResult* tmp, uint64_t tmp_size) {\n+  assert(blockDim.x * blockDim.y * blockDim.z == kBlockSize);\n+  assert(BlockIdx() < tmp_size);\n+  if (BlockIdx() >= tmp_size) {\n     return;\n   }\n \n-  // https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/:\n-  // > CUDA architecture limits the numbers of threads per block (1024 threads\n-  // > per block limit).\n-  switch (block_size) {\n-    case 1024:\n-      ReduceSum<T, 1024>(input, input_size, &nan_count, &inf_count,\n-                         &zero_count);\n-      break;\n-    case 512:\n-      ReduceSum<T, 512>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 256:\n-      ReduceSum<T, 256>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 128:\n-      ReduceSum<T, 128>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 64:\n-      ReduceSum<T, 64>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 32:\n-      ReduceSum<T, 32>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 16:\n-      ReduceSum<T, 16>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 8:\n-      ReduceSum<T, 8>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 4:\n-      ReduceSum<T, 4>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 2:\n-      ReduceSum<T, 2>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    case 1:\n-      ReduceSum<T, 1>(input, input_size, &nan_count, &inf_count, &zero_count);\n-      break;\n-    default:\n-      // Unsupported block size.\n-      assert(false);\n-      return;\n+  const FloatCheckResult result = CheckFloats(input, input_size, tmp_size);\n+  if (ThreadIdx() == 0) {\n+    tmp[BlockIdx()] = result;\n   }\n+}\n \n-  if (ThreadIdx() == 0) {\n-    cuda::atomic_ref<uint32_t, cuda::thread_scope_system>\n-        nan_count_log_write_idx(log_header->write_idx);\n+// Reduce the partially accumulated results from `FloatCheck` invocations and\n+// append the result to the buffer debug log.\n+__global__ void ReduceFloatCheckResults(\n+    xla::gpu::FloatCheckResult* tmp, uint64_t tmp_size,\n+    xla::gpu::BufferDebugLogEntryId entry_id,\n+    xla::gpu::BufferDebugLogHeader* log_header,\n+    xla::gpu::BufferDebugFloatCheckEntry* log_entries) {\n+  assert(blockDim.x * blockDim.y * blockDim.z == kBlockSize);\n+  assert(BlockIdx() == 0);\n+  if (BlockIdx() >= 1) {\n+    return;\n+  }\n+\n+  assert(tmp_size > 0);\n+  FloatCheckResult total = ReduceResults(tmp, tmp_size);\n+\n+  if (BlockIdx() == 0 && ThreadIdx() == 0) {\n+    cuda::atomic_ref<uint32_t, cuda::thread_scope_system> log_write_idx(\n+        log_header->write_idx);\n #if __CUDA_ARCH__ >= 600\n-    const uint32_t write_idx = nan_count_log_write_idx.fetch_add(1);\n-    if (nan_count_log_write_idx.load() < log_header->capacity) {\n-      float_check_entries[write_idx] = xla::gpu::BufferDebugFloatCheckEntry{\n-          entry_id, nan_count, inf_count, zero_count};\n+    const uint32_t write_idx = log_write_idx.fetch_add(1);\n+    if (write_idx < log_header->capacity) {\n+      log_entries[write_idx] = xla::gpu::BufferDebugFloatCheckEntry{\n+          entry_id, total.nan_count, total.inf_count, total.zero_count};\n     }\n #else\n     // Our toolchains generate a fetch_add PTX instructions with system scope,\n     // which is not supported on pre-Pascal architectures.\n+    (void)total;\n     assert(false);\n #endif\n   }\n }\n \n se::KernelLoaderSpec GetFloatCheckF32KernelSpec(int arity) {\n   return se::KernelLoaderSpec::CreateInProcessSymbolSpec(\n-      absl::bit_cast<void*>(&AppendFloatCheck<float>),\n+      absl::bit_cast<void*>(&FloatCheck<float>),\n       \"BufferDebugFloatCheckF32Kernel\", arity);\n }\n \n se::KernelLoaderSpec GetFloatCheckBf16KernelSpec(int arity) {\n   return se::KernelLoaderSpec::CreateInProcessSymbolSpec(\n-      absl::bit_cast<void*>(&AppendFloatCheck<__nv_bfloat16>),\n+      absl::bit_cast<void*>(&FloatCheck<__nv_bfloat16>),\n       \"BufferDebugFloatCheckBf16Kernel\", arity);\n }\n \n+se::KernelLoaderSpec GetReduceFloatCheckResultsKernelSpec(int arity) {\n+  return se::KernelLoaderSpec::CreateInProcessSymbolSpec(\n+      absl::bit_cast<void*>(&ReduceFloatCheckResults),\n+      \"BufferDebugReduceFloatCheckResultsKernel\", arity);\n+}\n+\n }  // namespace\n \n GPU_KERNEL_REGISTRY_REGISTER_KERNEL_STATICALLY(\n@@ -253,3 +276,8 @@ GPU_KERNEL_REGISTRY_REGISTER_KERNEL_STATICALLY(\n GPU_KERNEL_REGISTRY_REGISTER_KERNEL_STATICALLY(\n     BufferDebugFloatCheckBf16Kernel, se::gpu::BufferDebugFloatCheckBf16Kernel,\n     se::cuda::kCudaPlatformId, GetFloatCheckBf16KernelSpec);\n+\n+GPU_KERNEL_REGISTRY_REGISTER_KERNEL_STATICALLY(\n+    BufferDebugReduceFloatCheckResultsKernel,\n+    se::gpu::BufferDebugAppendReducedFloatCheckResultsKernel,\n+    se::cuda::kCudaPlatformId, GetReduceFloatCheckResultsKernelSpec);"
        },
        {
            "sha": "a1ab9cbb610482b46a88eedd31824593b53c0183",
            "filename": "third_party/xla/xla/stream_executor/cuda/buffer_debug_float_check_kernel_cuda_test.cc",
            "status": "modified",
            "additions": 113,
            "deletions": 22,
            "changes": 135,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fbuffer_debug_float_check_kernel_cuda_test.cc?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -13,7 +13,9 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <algorithm>\n #include <cstdint>\n+#include <cstdlib>\n #include <limits>\n #include <memory>\n #include <optional>\n@@ -29,6 +31,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_float_check_kernel.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n@@ -86,11 +89,17 @@ class FloatCheckKernelTest : public ::testing::Test {\n   absl::Status AppendFloatCheckOnDevice(\n       BufferDebugLogEntryId entry_id, const std::vector<InputType>& input,\n       se::gpu::BufferDebugLog<BufferType>& buffer_debug_log,\n-      stream_executor::ThreadDim dim = stream_executor::ThreadDim(1, 1, 1)) {\n+      stream_executor::BlockDim block_dim = stream_executor::BlockDim(1, 1, 1),\n+      size_t temp_buffer_size_elements = 1024) {\n     // Load kernel\n     gpu::GpuKernelRegistry registry =\n         gpu::GpuKernelRegistry::GetGlobalRegistry();\n     TF_ASSIGN_OR_RETURN(auto kernel, registry.LoadKernel<Kernel>(executor_));\n+    TF_ASSIGN_OR_RETURN(\n+        auto reduce_kernel,\n+        registry\n+            .LoadKernel<gpu::BufferDebugAppendReducedFloatCheckResultsKernel>(\n+                executor_));\n \n     // Setup device buffers\n     TF_ASSIGN_OR_RETURN(\n@@ -100,13 +109,27 @@ class FloatCheckKernelTest : public ::testing::Test {\n     auto cleanup_input =\n         absl::MakeCleanup([&]() { executor_->Deallocate(&device_input); });\n \n+    TF_ASSIGN_OR_RETURN(\n+        se::DeviceAddress<xla::gpu::FloatCheckResult> device_tmp,\n+        CheckNotNull(executor_->AllocateArray<xla::gpu::FloatCheckResult>(\n+                         temp_buffer_size_elements),\n+                     \"tmp\"));\n+    auto cleanup_tmp =\n+        absl::MakeCleanup([&]() { executor_->Deallocate(&device_tmp); });\n+\n+    const se::ThreadDim thread_dim(1024, 1, 1);\n+\n     // Call kernel\n     TF_RETURN_IF_ERROR(stream_->Memcpy(&device_input, input.data(),\n                                        input.size() * sizeof(input[0])));\n-    TF_RETURN_IF_ERROR(kernel.Launch(\n-        dim, stream_executor::BlockDim(1, 1, 1), stream_.get(), entry_id,\n-        device_input, device_input.ElementCount() * sizeof(InputType),\n-        buffer_debug_log.GetDeviceHeader(),\n+    TF_RETURN_IF_ERROR(kernel.Launch(thread_dim, block_dim, stream_.get(),\n+                                     device_input, device_input.ElementCount(),\n+                                     device_tmp, device_tmp.ElementCount()));\n+    TF_RETURN_IF_ERROR(reduce_kernel.Launch(\n+        thread_dim, se::BlockDim(1, 1, 1), stream_.get(), device_tmp,\n+        std::min(device_tmp.ElementCount(),\n+                 block_dim.x * block_dim.y * block_dim.z),\n+        entry_id, buffer_debug_log.GetDeviceHeader(),\n         buffer_debug_log.GetDeviceEntries()));\n     TF_RETURN_IF_ERROR(stream_->BlockHostUntilDone());\n \n@@ -170,33 +193,101 @@ TEST_F(FloatCheckKernelTest, ChecksFloatsForBf16) {\n }\n \n TEST_F(FloatCheckKernelTest, ChecksFloatsInParallel) {\n-  se::DeviceAddress<uint8_t> mem = executor_->AllocateArray<uint8_t>(1024);\n-  std::vector<float> input(1024, 1.0f);\n-  input[100] = std::numeric_limits<float>::quiet_NaN();\n-  input[200] = std::numeric_limits<float>::quiet_NaN();\n-  input[300] = std::numeric_limits<float>::quiet_NaN();\n-  input[400] = 0.0f;\n-  input[600] = std::numeric_limits<float>::infinity();\n-  input[700] = std::numeric_limits<float>::infinity();\n+  static constexpr size_t kNumNaNs = 100;\n+  static constexpr size_t kNumInfs = 200;\n+  static constexpr size_t kNumZeros = 300;\n+  static constexpr size_t kMaxTestValues =\n+      std::max(std::max(kNumNaNs, kNumInfs), kNumZeros);\n+\n+  const se::DeviceDescription& device_desc = executor_->GetDeviceDescription();\n+  const size_t threads_per_core = device_desc.threads_per_core_limit();\n+  const size_t num_cores = device_desc.core_count();\n+  const size_t input_size = num_cores * threads_per_core * 3 / 2;\n+  const size_t test_value_stride = input_size / (kMaxTestValues + 1);\n+  ASSERT_GT(input_size, kMaxTestValues);\n+  ASSERT_GT(test_value_stride, 2);\n+\n+  std::vector<float> input(input_size, 1.0f);\n+  for (size_t i = 0; i < kNumNaNs; ++i) {\n+    input[i * test_value_stride] = std::numeric_limits<float>::quiet_NaN();\n+  }\n+  for (size_t i = 0; i < kNumInfs; ++i) {\n+    input[i * test_value_stride + 1] = std::numeric_limits<float>::infinity();\n+  }\n+  for (size_t i = 0; i < kNumZeros; ++i) {\n+    input[i * test_value_stride + 2] = 0.0f;\n+  }\n \n+  se::DeviceAddress<uint8_t> log_mem = executor_->AllocateArray<uint8_t>(1024);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto device_log,\n       se::gpu::BufferDebugLog<BufferDebugFloatCheckEntry>::CreateOnDevice(\n-          *stream_, mem));\n+          *stream_, log_mem));\n \n+  int64_t threads_per_block;\n+  int64_t num_blocks;\n+  CalculateDimensionality(executor_->GetDeviceDescription(), input.size(),\n+                          &threads_per_block, &num_blocks);\n+  const se::BlockDim block_dim(num_blocks);\n   TF_EXPECT_OK(AppendFloatCheckOnDevice<gpu::BufferDebugFloatCheckF32Kernel>(\n-      BufferDebugLogEntryId{0}, input, device_log, se::ThreadDim(2, 4, 8)));\n+      BufferDebugLogEntryId{0}, input, device_log, block_dim));\n   TF_EXPECT_OK(AppendFloatCheckOnDevice<gpu::BufferDebugFloatCheckF32Kernel>(\n-      BufferDebugLogEntryId{0}, input, device_log, se::ThreadDim(2, 4, 8)));\n+      BufferDebugLogEntryId{0}, input, device_log, block_dim));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n   ASSERT_GE(host_log.size(), 2);\n-  EXPECT_EQ(host_log[0].nan_count, 3);\n-  EXPECT_EQ(host_log[0].inf_count, 2);\n-  EXPECT_EQ(host_log[0].zero_count, 1);\n-  EXPECT_EQ(host_log[1].nan_count, 3);\n-  EXPECT_EQ(host_log[1].inf_count, 2);\n-  EXPECT_EQ(host_log[1].zero_count, 1);\n+  EXPECT_EQ(host_log[0].nan_count, kNumNaNs);\n+  EXPECT_EQ(host_log[0].inf_count, kNumInfs);\n+  EXPECT_EQ(host_log[0].zero_count, kNumZeros);\n+  EXPECT_EQ(host_log[1].nan_count, kNumNaNs);\n+  EXPECT_EQ(host_log[1].inf_count, kNumInfs);\n+  EXPECT_EQ(host_log[1].zero_count, kNumZeros);\n+}\n+\n+TEST_F(FloatCheckKernelTest, ReduceFloatCheckResults) {\n+  static constexpr size_t kNumNaNs = 100;\n+  static constexpr size_t kNumInfs = 200;\n+  static constexpr size_t kNumZeros = 300;\n+  static constexpr size_t kIntermediateResults = 16 * 1024;\n+\n+  std::vector<xla::gpu::FloatCheckResult> results(kIntermediateResults);\n+  for (size_t i = 0; i < kIntermediateResults; ++i) {\n+    results[i].nan_count = i < kNumNaNs ? 1 : 0;\n+    results[i].inf_count = i < kNumInfs ? 1 : 0;\n+    results[i].zero_count = i < kNumZeros ? 1 : 0;\n+  }\n+\n+  gpu::GpuKernelRegistry registry = gpu::GpuKernelRegistry::GetGlobalRegistry();\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto reduce_kernel,\n+      registry.LoadKernel<gpu::BufferDebugAppendReducedFloatCheckResultsKernel>(\n+          executor_));\n+\n+  se::DeviceAddress<uint8_t> log_mem = executor_->AllocateArray<uint8_t>(1024);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto device_log,\n+      se::gpu::BufferDebugLog<BufferDebugFloatCheckEntry>::CreateOnDevice(\n+          *stream_, log_mem));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      se::DeviceAddress<xla::gpu::FloatCheckResult> device_results,\n+      CheckNotNull(executor_->AllocateArray<xla::gpu::FloatCheckResult>(\n+                       kIntermediateResults),\n+                   \"results\"));\n+  auto cleanup_results =\n+      absl::MakeCleanup([&]() { executor_->Deallocate(&device_results); });\n+\n+  TF_ASSERT_OK(stream_->Memcpy(&device_results, results.data(),\n+                               results.size() * sizeof(results[0])));\n+  TF_ASSERT_OK(reduce_kernel.Launch(\n+      se::ThreadDim(1024, 1, 1), se::BlockDim(1, 1, 1), stream_.get(),\n+      device_results, device_results.ElementCount(), BufferDebugLogEntryId{0},\n+      device_log.GetDeviceHeader(), device_log.GetDeviceEntries()));\n+  TF_ASSERT_OK_AND_ASSIGN(auto host_log, device_log.ReadFromDevice(*stream_));\n+\n+  ASSERT_GE(host_log.size(), 1);\n+  EXPECT_EQ(host_log[0].nan_count, kNumNaNs);\n+  EXPECT_EQ(host_log[0].inf_count, kNumInfs);\n+  EXPECT_EQ(host_log[0].zero_count, kNumZeros);\n }\n \n }  // namespace"
        },
        {
            "sha": "af0b687d6f9578f92096faf51c9b5d3444741267",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_debug_float_check_kernel.h",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_float_check_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19a899a5b8475fa9d72fbc16b09c5a90381d7c18/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_float_check_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_debug_float_check_kernel.h?ref=19a899a5b8475fa9d72fbc16b09c5a90381d7c18",
            "patch": "@@ -25,21 +25,32 @@ limitations under the License.\n \n namespace stream_executor::gpu {\n \n-// Trait for a kernel that computes the NaN count of given input buffer and\n-// appends it to the buffer debug log.\n-//\n-// This kernel MUST execute on a single thread block.\n+// Counts the number of NaNs, Infs and zeros in a buffer of floats in parallel,\n+// and stores partially accumulated results in the FloatCheckResult array.\n struct BufferDebugFloatCheckF32Kernel {\n   using KernelType =\n-      TypedKernel<xla::gpu::BufferDebugLogEntryId, DeviceAddress<float>,\n-                  uint64_t, DeviceAddress<xla::gpu::BufferDebugLogHeader>,\n-                  DeviceAddress<xla::gpu::BufferDebugFloatCheckEntry>>;\n+      TypedKernel<DeviceAddress<float>, uint64_t,\n+                  DeviceAddress<xla::gpu::FloatCheckResult>, uint64_t>;\n };\n \n+// Counts the number of NaNs, Infs and zeros in a buffer of bfloat16s in\n+// parallel, and stores partially accumulated results in the FloatCheckResult\n+// array.\n struct BufferDebugFloatCheckBf16Kernel {\n   using KernelType =\n-      TypedKernel<xla::gpu::BufferDebugLogEntryId,\n-                  DeviceAddress<Eigen::bfloat16>, uint64_t,\n+      TypedKernel<DeviceAddress<Eigen::bfloat16>, uint64_t,\n+                  DeviceAddress<xla::gpu::FloatCheckResult>, uint64_t>;\n+};\n+\n+// Trait for a kernel that reduces the partially accumulated results from\n+// `BufferDebugFloatCheckF32Kernel` or `BufferDebugFloatCheckBf16Kernel`\n+// invocations and appends the result to the buffer debug log.\n+//\n+// This kernel MUST execute on a single thread block.\n+struct BufferDebugAppendReducedFloatCheckResultsKernel {\n+  using KernelType =\n+      TypedKernel<DeviceAddress<xla::gpu::FloatCheckResult>, uint64_t,\n+                  xla::gpu::BufferDebugLogEntryId,\n                   DeviceAddress<xla::gpu::BufferDebugLogHeader>,\n                   DeviceAddress<xla::gpu::BufferDebugFloatCheckEntry>>;\n };"
        }
    ],
    "stats": {
        "total": 720,
        "additions": 495,
        "deletions": 225
    }
}