{
    "author": "pschuh",
    "message": "Manual rollback of \"Implement AllocateDestinationBuffer in terms of raw buffer APIs.\"\n\nPiperOrigin-RevId: 799408305",
    "sha": "499c531bb102711cef73e204552179af7bfaa802",
    "files": [
        {
            "sha": "e1641513ee3d10035bf33879249c9a3cc8c28838",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 297,
            "deletions": 32,
            "changes": 329,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/499c531bb102711cef73e204552179af7bfaa802/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/499c531bb102711cef73e204552179af7bfaa802/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=499c531bb102711cef73e204552179af7bfaa802",
            "patch": "@@ -607,56 +607,95 @@ AllocateDestinationBuffer(const Shape& on_host_shape, PjRtDevice* device,\n         \"Cannot allocate a PjRtStreamExecutorBuffer for a tuple.\");\n   }\n \n+  PjRtMemorySpace* default_memory_space =\n+      device->default_memory_space().value_or(nullptr);\n   if (!memory_space) {\n-    memory_space = device->default_memory_space().value_or(nullptr);\n+    memory_space = default_memory_space;\n+  }\n+  bool is_pinned_host_memory =\n+      memory_space && (memory_space->kind() == PinnedHostMemorySpace::kKind);\n+  // Only allow pinned host memory or device memory.\n+  if (memory_space != default_memory_space && !is_pinned_host_memory) {\n+    return InvalidArgument(\"Buffer allocation: invalid memory space\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(\n-      Shape on_device_shape,\n-      client->MakeDefaultShapeForMemorySpace(\n-          memory_space, on_host_shape,\n-          on_host_shape.has_layout() ? &on_host_shape.layout() : nullptr));\n-  TF_ASSIGN_OR_RETURN(\n-      size_t on_device_bytes_count,\n-      client->GetOnDeviceBytesCount(memory_space, on_device_shape));\n-  TF_ASSIGN_OR_RETURN(\n-      tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,\n-      client->AllocateRawBuffer(memory_space, on_device_bytes_count,\n-                                /*retry_on_oom=*/true,\n-                                /*allocate_after=*/{}));\n+  auto* se_client = tensorflow::down_cast<PjRtStreamExecutorClient*>(client);\n+  TransferManager* transfer_manager =\n+      se_client->client()->backend().transfer_manager();\n+\n+  // Communicate the desired memory space to the allocator via the shape\n+  // callback.\n+  auto memory_space_shape_fn = [is_pinned_host_memory,\n+                                transfer_manager](const Shape& shape) {\n+    Shape result = transfer_manager->HostShapeToDeviceShape(shape);\n+    if (is_pinned_host_memory) {\n+      result.mutable_layout()->set_memory_space(Layout::kHostMemorySpace);\n+    }\n+    return result;\n+  };\n \n-  absl::InlinedVector<tsl::RCReference<PjRtDeviceEvent>, 4> definition_events;\n-  // Record the caller's definition event.\n-  if (definition_event) {\n-    definition_events.push_back(tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(\n-        std::move(definition_event)));\n+  TF_ASSIGN_OR_RETURN(\n+      ScopedShapedBuffer dst_buffer,\n+      transfer_manager->AllocateScopedShapedBuffer(\n+          on_host_shape, se_client->allocator(),\n+          local_device->local_device_id().value(),\n+          local_device->local_hardware_id().value(), memory_space_shape_fn));\n+  if (local_device->allocation_model() ==\n+      LocalDeviceState::kComputeSynchronized) {\n+    if (copy_stream == nullptr) {\n+      CHECK(is_uninitialized_create);\n+    } else {\n+      CHECK(copy_stream->WaitFor(local_device->compute_stream()).ok());\n+    }\n+  } else {\n+    DCHECK(transfer_manager->CanShapedBufferBeAccessedNow(\n+        local_device->compute_stream()->parent(), dst_buffer));\n   }\n+  Shape on_device_shape = dst_buffer.on_device_shape();\n+\n+  absl::InlinedVector<BufferSequencingEventRef, 2> definition_events;\n   if (is_uninitialized_create) {\n     // There is not going to be any copy into the buffer so in general we don't\n     // need a definition event.\n     // But if the caller provided a definition event then we record that. Also\n     // put it as the first definition event so that we can guarantee only the\n     // first one might not have event recorded.\n+    if (definition_event) {\n+      definition_events.push_back(definition_event);\n+    }\n     if (local_device->allocation_model() ==\n         LocalDeviceState::kComputeSynchronized) {\n-      TF_ASSIGN_OR_RETURN(auto allocation_ready_event,\n-                          raw_buffer->MakeAllocationReadyEvent());\n-      definition_events.push_back(std::move(allocation_ready_event));\n+      // The allocation is not valid until the compute stream passes this point,\n+      // so add a definition event in the compute stream.\n+      definition_events.emplace_back(\n+          BufferSequencingEvent::Create(client->thread_pool()));\n+      TF_RETURN_IF_ERROR(\n+          client->AllocateAndRecordEvent(definition_events.back(), local_device,\n+                                         local_device->compute_stream()));\n     }\n   } else {\n-    client->WaitForAllocation(copy_stream, *raw_buffer);\n-    // Callers expect at least one event.\n-    if (definition_events.empty()) {\n+    // We have at least one definition event, for the copy completing to\n+    // the device buffers.\n+    if (definition_event) {\n+      definition_events.push_back(definition_event);\n+    } else {\n       definition_events.emplace_back(\n-          tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(\n-              BufferSequencingEvent::Create(client->thread_pool())));\n+          BufferSequencingEvent::Create(client->thread_pool()));\n     }\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      auto buffer, client->DefineBuffer(on_device_shape, std::move(raw_buffer),\n-                                        std::move(definition_events), true));\n-  return std::unique_ptr<PjRtStreamExecutorBuffer>(\n-      tensorflow::down_cast<PjRtStreamExecutorBuffer*>(buffer.release()));\n+\n+  auto mem = RawSEDeviceMemory::Create(dst_buffer.buffer({}),\n+                                       device->local_device_id(),\n+                                       dst_buffer.memory_allocator());\n+  dst_buffer.clear();\n+\n+  auto dst_device_buffer = std::make_unique<TrackedDeviceBuffer>(\n+      device, std::move(mem), definition_events);\n+\n+  auto py_buffer = std::make_unique<PjRtStreamExecutorBuffer>(\n+      on_device_shape, std::move(dst_device_buffer), client, device,\n+      memory_space);\n+  return py_buffer;\n }\n \n void PjRtStreamExecutorBuffer::ScopedHold::ConvertUsageHold(\n@@ -1030,6 +1069,232 @@ PjRtStreamExecutorClient::LinearizeHostBufferInto(\n   return definition_event;\n }\n \n+// BufferFromHostBuffer() is used to create a buffer either for a device, or\n+// for a host memory, depending on `memory_space`. The memory copy is needed\n+// for both cases, either from the unpinned host memory to device, or from\n+// the unpinned host memory to the pinned host memory.\n+absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n+PjRtStreamExecutorClient::BufferFromHostBufferInternal(\n+    const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n+    std::optional<absl::Span<int64_t const>> byte_strides,\n+    HostBufferSemantics host_buffer_semantics,\n+    absl::AnyInvocable<void() &&> on_done_with_host_buffer, PjRtDevice* device,\n+    const Layout* device_layout, PjRtMemorySpace* memory_space) {\n+  tsl::profiler::TraceMe traceme(\n+      \"PjRtStreamExecutorClient::BufferFromHostBuffer\");\n+  Shape device_shape = ShapeUtil::MakeShape(type, dims);\n+  VLOG(1) << \"PjRtStreamExecutorClient::BufferFromHostBuffer: shape: \"\n+          << device_shape.ToString() << \" device: \" << device->DebugString();\n+  TF_ASSIGN_OR_RETURN(LocalDeviceState * local_device,\n+                      tensorflow::down_cast<PjRtStreamExecutorDevice*>(device)\n+                          ->GetLocalDeviceState());\n+\n+  absl::InlinedVector<int64_t, 4> tmp_strides;\n+  if (!byte_strides) {\n+    tmp_strides.resize(dims.size());\n+    TF_RETURN_IF_ERROR(\n+        ShapeUtil::ByteStrides(device_shape, absl::MakeSpan(tmp_strides)));\n+    byte_strides = tmp_strides;\n+  }\n+  int64_t size = ShapeUtil::ByteSizeOf(device_shape);\n+\n+  TransferManager* transfer_manager = client()->backend().transfer_manager();\n+  if (device_layout != nullptr) {\n+    *(device_shape.mutable_layout()) = *device_layout;\n+  } else {\n+    TF_ASSIGN_OR_RETURN(\n+        device_shape,\n+        transfer_manager->ChooseCompactLayoutForShape(device_shape));\n+  }\n+  absl::InlinedVector<int64_t, 4> shape_strides(\n+      device_shape.dimensions().size());\n+  TF_RETURN_IF_ERROR(\n+      ShapeUtil::ByteStrides(device_shape, absl::MakeSpan(shape_strides)));\n+  bool host_and_device_strides_equal =\n+      (size == 0 || *byte_strides == shape_strides);\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<PjRtStreamExecutorBuffer> py_buffer,\n+      AllocateDestinationBuffer(device_shape, device, local_device,\n+                                local_device->host_to_device_stream(),\n+                                /*is_uninitialized_create=*/false, this,\n+                                /*definition_event=*/nullptr, memory_space));\n+\n+  PjRtStreamExecutorBuffer::ScopedHold device_buffer(\n+      py_buffer->GetBufferWithUsageHold());\n+  CHECK(device_buffer.ok());\n+\n+  std::shared_ptr<TransposePlan> transpose;\n+  if (!host_and_device_strides_equal) {\n+    absl::InlinedVector<int64_t, 4> permutation(dims.size());\n+    absl::c_reverse_copy(device_shape.layout().minor_to_major(),\n+                         permutation.begin());\n+    TransposePlan::Options options;\n+    options.elem_size_in_bytes = primitive_util::ByteWidth(type);\n+    options.dims = dims;\n+    options.permutation = permutation;\n+    options.input_layout = TransposePlan::Striding{*byte_strides};\n+    absl::MutexLock lock(&transpose_mu_);\n+    TF_ASSIGN_OR_RETURN(transpose, transpose_cache_.GetOrCreate(options));\n+  }\n+\n+  bool should_pack = primitive_util::IsSubByteNonPredType(type) &&\n+                     transfer_manager->PackSubbyteTypes();\n+  int64_t packed_size;\n+  if (should_pack) {\n+    packed_size =\n+        CeilOfRatio<int64_t>(size, 8 / primitive_util::BitWidth(type));\n+  } else {\n+    packed_size = size;\n+  }\n+\n+  // If necessary, allocate a host-side buffer for staging host-to-device\n+  // transfers. On GPU this is a buffer in pinned memory.\n+  std::shared_ptr<void> staging_buffer;\n+  bool must_use_staging_buffer =\n+      host_buffer_semantics == HostBufferSemantics::kImmutableOnlyDuringCall ||\n+      !host_and_device_strides_equal || packed_size != size;\n+  // Allocating multigigabyte pinned buffers can be very slow. In that case,\n+  // using a staging buffer is probably worse than not using one.\n+  // TODO(phawkins): add chunking for transfers.\n+  if (must_use_staging_buffer || (!IsDmaMapped(data, packed_size) &&\n+                                  (should_stage_host_to_device_transfers() &&\n+                                   packed_size < (int64_t{1} << 30)))) {\n+    void* ptr = host_memory_allocator()->AllocateRaw(\n+        tsl::Allocator::kAllocatorAlignment, transpose ? size : packed_size);\n+    staging_buffer = std::shared_ptr<void>(\n+        ptr, [host_memory_allocator = host_memory_allocator()](void* ptr) {\n+          host_memory_allocator->DeallocateRaw(ptr);\n+        });\n+  }\n+\n+  // Copy the buffer into a staging buffer before returning control to the\n+  // caller if the caller only guaranteed that the buffer is valid for the\n+  // duration of the call. Otherwise, we stage (if necessary) on a separate\n+  // thread.\n+  if (host_buffer_semantics == HostBufferSemantics::kImmutableOnlyDuringCall) {\n+    if (transpose) {\n+      transpose->Execute(data, staging_buffer.get());\n+      if (should_pack) {\n+        primitive_util::PackIntN(\n+            type,\n+            absl::MakeConstSpan(static_cast<const char*>(staging_buffer.get()),\n+                                size),\n+            absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n+                           packed_size));\n+      }\n+    } else {\n+      if (should_pack) {\n+        primitive_util::PackIntN(\n+            type, absl::MakeConstSpan(static_cast<const char*>(data), size),\n+            absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n+                           packed_size));\n+      } else {\n+        std::memcpy(staging_buffer.get(), data, size);\n+      }\n+    }\n+    if (on_done_with_host_buffer) {\n+      std::move(on_done_with_host_buffer)();\n+      on_done_with_host_buffer = nullptr;\n+    }\n+  }\n+\n+  BufferSequencingEventRef event = device_buffer->definition_events()[0];\n+\n+  // The host to device transfer is performed on a thread pool, mostly because\n+  // it includes linearization that may be slow. It is OK to capture the\n+  // py_buffer pointer because the py_buffer can't be deleted until all the\n+  // usage holds have gone away.\n+  // TODO(misard) assess if it would be preferable to introduce a heuristic to\n+  // put the transfer into the calling thread for small literals.\n+  auto transfer_h2d =\n+      [this, local_client = client(), local_device, data, size, type,\n+       packed_size, event, device_memory_owned = device_buffer->device_memory(),\n+       device_shape, should_pack, py_buffer{py_buffer.get()},\n+       on_device_shape{py_buffer->on_device_shape()},\n+       staging_buffer{std::move(staging_buffer)},\n+       on_done_with_host_buffer =\n+           on_done_with_host_buffer\n+               ? std::make_shared<absl::AnyInvocable<void() &&>>(\n+                     std::move(on_done_with_host_buffer))\n+               : nullptr,\n+       host_buffer_semantics, transpose{std::move(transpose)}]() mutable {\n+        // This function uses TF_CHECK_OK and value() since we have no way\n+        // to report failures from a callback. However, the operations here are\n+        // unlikely to fail and not recoverable even if we were to fail: DMAs to\n+        // memory that has already been allocated, and a possible Event\n+        // allocation.\n+\n+        se::DeviceMemoryBase device_memory = device_memory_owned->mem();\n+\n+        // If applicable on the backend, stage the transfer via host memory\n+        // allocated via the host_memory_allocator. On GPU, this is pinned\n+        // memory.\n+        if (staging_buffer) {\n+          // If we didn't already copy the input buffer into the staging buffer,\n+          // do so now.\n+          if (host_buffer_semantics !=\n+              HostBufferSemantics::kImmutableOnlyDuringCall) {\n+            if (transpose) {\n+              transpose->Execute(data, staging_buffer.get());\n+              if (should_pack) {\n+                primitive_util::PackIntN(\n+                    type,\n+                    absl::MakeConstSpan(\n+                        static_cast<const char*>(staging_buffer.get()), size),\n+                    absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n+                                   packed_size));\n+              }\n+            } else {\n+              if (should_pack) {\n+                primitive_util::PackIntN(\n+                    type,\n+                    absl::MakeConstSpan(static_cast<const char*>(data), size),\n+                    absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n+                                   packed_size));\n+              } else {\n+                std::memcpy(staging_buffer.get(), data, size);\n+              }\n+            }\n+          }\n+          TF_CHECK_OK(local_device->host_to_device_stream()->Memcpy(\n+              &device_memory, staging_buffer.get(), packed_size));\n+        } else {\n+          TF_CHECK_OK(local_device->host_to_device_stream()->Memcpy(\n+              &device_memory, data, packed_size));\n+        }\n+\n+        TF_CHECK_OK(AddDestinationBufferSynchronization(\n+            this, local_device, event, local_device->host_to_device_stream()));\n+\n+        event.AndThen([device_memory_owned = std::move(device_memory_owned),\n+                       staging_buffer{std::move(staging_buffer)},\n+                       on_done_with_host_buffer{\n+                           std::move(on_done_with_host_buffer)}]() mutable {\n+          if (on_done_with_host_buffer) {\n+            std::move (*on_done_with_host_buffer)();\n+          }\n+        });\n+      };\n+  thread_pool()->Schedule(WrapClosureAsCopyable(std::move(transfer_h2d)));\n+  RecordUsage(std::move(device_buffer), local_device, local_device, event,\n+              local_device->host_to_device_stream());\n+  return std::unique_ptr<PjRtBuffer>(std::move(py_buffer));\n+}\n+\n+absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n+PjRtStreamExecutorClient::BufferFromHostBuffer(\n+    const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n+    std::optional<absl::Span<int64_t const>> byte_strides,\n+    HostBufferSemantics host_buffer_semantics,\n+    absl::AnyInvocable<void() &&> on_done_with_host_buffer,\n+    PjRtMemorySpace* memory_space, const Layout* device_layout) {\n+  return BufferFromHostBufferInternal(\n+      data, type, dims, byte_strides, host_buffer_semantics,\n+      std::move(on_done_with_host_buffer), memory_space->devices()[0],\n+      device_layout, memory_space);\n+}\n+\n absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n PjRtStreamExecutorClient::CreateUninitializedBuffer(\n     const Shape& shape, PjRtMemorySpace* memory_space) {"
        },
        {
            "sha": "391bd23d31252625a6747f8c53e7367ec182e7bb",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.h",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/499c531bb102711cef73e204552179af7bfaa802/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/499c531bb102711cef73e204552179af7bfaa802/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h?ref=499c531bb102711cef73e204552179af7bfaa802",
            "patch": "@@ -331,6 +331,13 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> CreateErrorBuffer(\n       absl::Status error, const Shape& shape, PjRtMemorySpace* memory) override;\n \n+  absl::StatusOr<std::unique_ptr<PjRtBuffer>> BufferFromHostBuffer(\n+      const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n+      std::optional<absl::Span<int64_t const>> byte_strides,\n+      HostBufferSemantics host_buffer_semantics,\n+      absl::AnyInvocable<void() &&> on_done_with_host_buffer,\n+      PjRtMemorySpace* memory_space, const Layout* device_layout) override;\n+\n   using PjRtClient::BufferFromHostLiteral;\n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> BufferFromHostLiteral(\n       const LiteralSlice& literal, PjRtMemorySpace* memory_space,\n@@ -510,6 +517,14 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n       std::vector<std::unique_ptr<LocalExecutable>> local_executables,\n       CompileOptions compile_options, bool dump);\n \n+  absl::StatusOr<std::unique_ptr<PjRtBuffer>> BufferFromHostBufferInternal(\n+      const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n+      std::optional<absl::Span<int64_t const>> byte_strides,\n+      HostBufferSemantics host_buffer_semantics,\n+      absl::AnyInvocable<void() &&> on_done_with_host_buffer,\n+      PjRtDevice* device, const Layout* device_layout,\n+      PjRtMemorySpace* memory_space);\n+\n   const PjRtPlatformId platform_id_;\n   const std::string platform_name_;\n   LocalClient* client_;"
        }
    ],
    "stats": {
        "total": 344,
        "additions": 312,
        "deletions": 32
    }
}