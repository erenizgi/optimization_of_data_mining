{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Provide functions to setup multicast from a single process.\n\nPiperOrigin-RevId: 819790003",
    "sha": "b545b61c0d1030587169ea30a3e6cb00b6208700",
    "files": [
        {
            "sha": "6faba2b4cc2e058033c47f2ff21ce68edb5a2c04",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -1240,6 +1240,31 @@ xla_test(\n     ],\n )\n \n+xla_test(\n+    name = \"cuda_executor_multigpu_test\",\n+    srcs = [\"cuda_executor_multigpu_test.cc\"],\n+    backend_tags = {\n+        \"gpu\": [\n+            \"multi_gpu\",\n+            \"no_oss\",\n+        ],\n+    },\n+    backends = [\"gpu\"],\n+    tags = [\"cuda-only\"],\n+    deps = [\n+        \":cuda_executor\",\n+        \":cuda_platform\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/gpu:gpu_init\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"all_runtime\",\n     copts = tsl_copts(),"
        },
        {
            "sha": "570ca01f6d7828f9801225921d9248b95cc97240",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 148,
            "deletions": 0,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -641,6 +641,14 @@ absl::StatusOr<bool> IsRdmaSupported(CUdevice device) {\n   return rdma_supported;\n }\n \n+absl::StatusOr<bool> IsMulticastSupported(CUdevice device) {\n+  int is_multicast_supported = 0;\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+      cuDeviceGetAttribute(&is_multicast_supported,\n+                           CU_DEVICE_ATTRIBUTE_MULTICAST_SUPPORTED, device)));\n+  return is_multicast_supported;\n+}\n+\n CUmemAllocationProp GetVmmAllocationProperties(CUdevice device,\n                                                bool is_rdma_supported) {\n   CUmemAllocationProp properties = {};\n@@ -661,6 +669,26 @@ CUmemAccessDesc GetVmmAccessDescriptor(int device) {\n   return descriptor;\n }\n \n+absl::StatusOr<CUmulticastObjectProp> CreateMulticastObjectProperties(\n+    int num_devices, size_t size) {\n+  CUmulticastObjectProp multicast_properties;\n+  memset(&multicast_properties, 0, sizeof(CUmulticastObjectProp));\n+  multicast_properties.numDevices = num_devices;\n+\n+  multicast_properties.handleTypes = CU_MEM_HANDLE_TYPE_NONE;\n+  multicast_properties.flags = 0;\n+\n+  size_t multicast_granularity = 0;\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+      cuMulticastGetGranularity(&multicast_granularity, &multicast_properties,\n+                                CU_MULTICAST_GRANULARITY_RECOMMENDED)));\n+\n+  // Align up the size to the multicast granularity.\n+  multicast_properties.size =\n+      xla::RoundUpTo<size_t>(size, multicast_granularity);\n+  return multicast_properties;\n+}\n+\n }  // namespace\n \n // Given const GPU memory, returns a libcuda device pointer datatype, suitable\n@@ -896,6 +924,7 @@ absl::Status CudaExecutor::Init() {\n   TF_ASSIGN_OR_RETURN(device_, GetDevice(device_ordinal()));\n   TF_ASSIGN_OR_RETURN(is_vmm_supported_, IsVmmSupported(device_));\n   TF_ASSIGN_OR_RETURN(is_rdma_supported_, IsRdmaSupported(device_));\n+  TF_ASSIGN_OR_RETURN(is_multicast_supported_, IsMulticastSupported(device_));\n   TF_ASSIGN_OR_RETURN(CudaContext * context,\n                       CudaContext::Create(device_ordinal(), device_));\n   cuda_context_ = context;\n@@ -1718,5 +1747,124 @@ absl::StatusOr<TensorMap> CudaExecutor::CreateTensorMap(\n   return absl::bit_cast<TensorMap>(tensor_map);\n }\n \n+CudaExecutor::MulticastMemory::~MulticastMemory() {\n+  if (handle_ != 0) {\n+    for (auto const& [device_ordinal, mapped_memory_ptr] : mapped_devices_) {\n+      VLOG(3) << \"[\" << device_ordinal << \"] Unbind multicast: \" << handle_;\n+      TF_CHECK_OK(stream_executor::cuda::ToStatus(cuMulticastUnbind(\n+          handle_, device_ordinal, /*mcOffset=*/0, padded_size_)));\n+\n+      VLOG(3) << \"[\" << device_ordinal << \"] Unmap ptr: \" << mapped_memory_ptr;\n+      TF_CHECK_OK(stream_executor::cuda::ToStatus(\n+          cuMemUnmap(mapped_memory_ptr, padded_size_)));\n+      VLOG(3) << \"[\" << device_ordinal\n+              << \"] Release address space: \" << mapped_memory_ptr;\n+      TF_CHECK_OK(stream_executor::cuda::ToStatus(\n+          cuMemAddressFree(mapped_memory_ptr, padded_size_)));\n+    }\n+    TF_CHECK_OK(stream_executor::cuda::ToStatus(\n+        cuMemRelease(static_cast<CUmemGenericAllocationHandle>(handle_))));\n+  }\n+}\n+\n+absl::Status CudaExecutor::MulticastMemory::Initialize(\n+    uint64_t size, int num_devices, CudaExecutor& cuda_executor) {\n+  if (handle_ != 0) {\n+    return absl::FailedPreconditionError(\n+        \"Multicast memory is already initialized.\");\n+  }\n+\n+  if (num_devices <= 1) {\n+    return absl::InvalidArgumentError(\n+        absl::StrCat(\"Number of devices must be greater than 1, but got \",\n+                     num_devices, \".\"));\n+  }\n+\n+  CUmemAllocationProp properties = GetVmmAllocationProperties(\n+      cuda_executor.device_, cuda_executor.is_rdma_supported_);\n+  TF_RETURN_IF_ERROR(\n+      stream_executor::cuda::ToStatus(cuMemGetAllocationGranularity(\n+          &granularity_, &properties, CU_MEM_ALLOC_GRANULARITY_RECOMMENDED)));\n+\n+  padded_size_ = xla::RoundUpTo<size_t>(size, granularity_);\n+  num_devices_ = num_devices;\n+  TF_ASSIGN_OR_RETURN(CUmulticastObjectProp multicast_properties,\n+                      CreateMulticastObjectProperties(num_devices_, size));\n+\n+  VLOG(3) << \"[\" << static_cast<int>(cuda_executor.device_)\n+          << \"] Create multicast memory: \" << static_cast<uint64_t>(handle_)\n+          << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n+          << \" for \" << num_devices_ << \" devices.\";\n+  return stream_executor::cuda::ToStatus(\n+      cuMulticastCreate(&handle_, &multicast_properties));\n+}\n+\n+absl::Status CudaExecutor::MulticastMemory::SubscribeDevice(int device_number) {\n+  if (handle_ == 0) {\n+    return absl::FailedPreconditionError(\n+        \"Multicast memory is not initialized.\");\n+  }\n+\n+  if (subscribed_devices_ >= num_devices_) {\n+    return absl::InvalidArgumentError(\"All devices are already subscribed.\");\n+  }\n+\n+  VLOG(3) << \"[\" << device_number << \"] Subscribe to multicast: \" << handle_;\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+      cuMulticastAddDevice(handle_, device_number)));\n+  subscribed_devices_++;\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<void*> CudaExecutor::MulticastMemory::MapMemory(\n+    void* device_ptr, CudaExecutor& cuda_executor) {\n+  if (device_ptr == nullptr) {\n+    return absl::InvalidArgumentError(\"Device pointer is null.\");\n+  }\n+\n+  if (handle_ == 0) {\n+    return absl::FailedPreconditionError(\n+        \"Multicast memory is not initialized.\");\n+  }\n+\n+  if (subscribed_devices_ != num_devices_) {\n+    return absl::FailedPreconditionError(\"All devices should be subscribed.\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      stream_executor::gpu::CudaExecutor::VmmMemoryHandle memory_handle,\n+      cuda_executor.RetainVmmMemoryHandle(device_ptr));\n+\n+  CUmemGenericAllocationHandle retained_memory_handle =\n+      static_cast<CUmemGenericAllocationHandle>(memory_handle.handle());\n+\n+  // Bind the memory to the multicast object.\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+      cuMulticastBindMem(handle_, /*mcOffset=*/0, retained_memory_handle,\n+                         /*memOffset=*/0, padded_size_, /*flags=*/0)));\n+\n+  VLOG(3) << \"[\" << static_cast<int>(cuda_executor.device_)\n+          << \"] Mapped multicast memory: \" << static_cast<uint64_t>(handle_)\n+          << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n+          << \" to address: \" << device_ptr;\n+\n+  // Map a virtual address range for the multicast memory. Multicast\n+  // memory is used to reduce the data stored in the multicast object.\n+  CUdeviceptr multicast_device_ptr;\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(cuMemAddressReserve(\n+      &multicast_device_ptr, padded_size_, granularity_, 0, 0)));\n+\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+      cuMemMap(multicast_device_ptr, padded_size_, 0, handle_, 0)));\n+\n+  CUmemAccessDesc accessDesc = GetVmmAccessDescriptor(cuda_executor.device_);\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+      cuMemSetAccess(multicast_device_ptr, padded_size_, &accessDesc, 1)));\n+\n+  absl::MutexLock subscription_lock(mapped_devices_mu_);\n+  mapped_devices_.emplace(cuda_executor.device_, multicast_device_ptr);\n+  return reinterpret_cast<void*>(multicast_device_ptr);\n+}\n+\n }  // namespace gpu\n }  // namespace stream_executor"
        },
        {
            "sha": "ea7e589764f7bd598ea3d38573be66d01db8cd76",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_STREAM_EXECUTOR_CUDA_CUDA_EXECUTOR_H_\n #define XLA_STREAM_EXECUTOR_CUDA_CUDA_EXECUTOR_H_\n \n+#include <atomic>\n #include <cstdint>\n #include <map>\n #include <memory>\n@@ -154,9 +155,40 @@ class CudaExecutor : public GpuExecutor {\n     uint64_t handle_;\n   };\n \n+  class MulticastMemory {\n+   public:\n+    MulticastMemory()\n+        : handle_(0),\n+          padded_size_(0),\n+          granularity_(0),\n+          num_devices_(0),\n+          subscribed_devices_(0) {};\n+    ~MulticastMemory();\n+\n+    absl::Status Initialize(uint64_t size, int num_devices,\n+                            CudaExecutor& cuda_executor);\n+\n+    absl::Status SubscribeDevice(int device_number);\n+\n+    absl::StatusOr<void*> MapMemory(void* device_ptr,\n+                                    CudaExecutor& cuda_executor);\n+\n+   private:\n+    CUmemGenericAllocationHandle handle_;\n+    uint64_t padded_size_;\n+    uint64_t granularity_;\n+    int num_devices_;\n+    std::atomic<int> subscribed_devices_;\n+    absl::flat_hash_map<int, CUdeviceptr> mapped_devices_\n+        ABSL_GUARDED_BY(mapped_devices_mu_);\n+    absl::Mutex mapped_devices_mu_;\n+  };\n+\n   // Returns a handle to the given memory if it was allocated with VMM API.\n   absl::StatusOr<VmmMemoryHandle> RetainVmmMemoryHandle(void* ptr);\n \n+  bool is_multicast_supported() const { return is_multicast_supported_; }\n+\n  private:\n   absl::Status VmmDeallocateMemory(void* ptr);\n \n@@ -180,6 +212,8 @@ class CudaExecutor : public GpuExecutor {\n \n   bool is_rdma_supported_ = false;\n \n+  bool is_multicast_supported_ = false;\n+\n   // Guards the in-memory-module mapping.\n   absl::Mutex in_memory_modules_mu_;\n "
        },
        {
            "sha": "6ce94686eb09e6c24b15754568766a69031065f8",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test.cc",
            "status": "added",
            "additions": 194,
            "deletions": 0,
            "changes": 194,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b545b61c0d1030587169ea30a3e6cb00b6208700/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc?ref=b545b61c0d1030587169ea30a3e6cb00b6208700",
            "patch": "@@ -0,0 +1,194 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"xla/stream_executor/cuda/cuda_executor.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/gpu_init.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace stream_executor::gpu {\n+namespace {\n+using ::absl_testing::IsOk;\n+using ::absl_testing::IsOkAndHolds;\n+using ::absl_testing::StatusIs;\n+using ::testing::NotNull;\n+\n+StreamExecutor* GetGpuExecutor(int64_t device_ordinal) {\n+  auto* platform =\n+      PlatformManager::PlatformWithName(stream_executor::GpuPlatformName())\n+          .value();\n+  return platform->ExecutorForDevice(device_ordinal).value();\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, MultimemCanBeInitializedOnce) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(multicast_memory.Initialize(1024, 2, *executors[0]), IsOk());\n+  EXPECT_THAT(multicast_memory.Initialize(1024, 2, *executors[0]),\n+              StatusIs(absl::StatusCode::kFailedPrecondition,\n+                       \"Multicast memory is already initialized.\"));\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, UnitializedMulticastCanNotBeSubscribed) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(\n+      multicast_memory.SubscribeDevice(0),\n+      absl::FailedPreconditionError(\"Multicast memory is not initialized.\"));\n+  EXPECT_THAT(multicast_memory.MapMemory(nullptr, *executors[0]),\n+              absl::InvalidArgumentError(\"Device pointer is null.\"));\n+  EXPECT_THAT(\n+      multicast_memory.MapMemory(reinterpret_cast<void*>(1), *executors[0]),\n+      StatusIs(absl::StatusCode::kFailedPrecondition,\n+               \"Multicast memory is not initialized.\"));\n+}\n+\n+TEST(CudaExecutorMultiGpuTest,\n+     MulticastMemoryCanNotBeInitializedWithOneDevice) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(multicast_memory.Initialize(1024, 1, *executors[0]),\n+              StatusIs(absl::StatusCode::kInvalidArgument,\n+                       \"Number of devices must be greater than 1, but got 1.\"));\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, MulticastMemoryResubscriptionFails) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(multicast_memory.Initialize(1024, 2, *executors[0]), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(0), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(0),\n+              StatusIs(absl::StatusCode::kInternal,\n+                       \"CUDA error: : CUDA_ERROR_UNKNOWN: unknown error\"));\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, AllDevicesMustBeSubscribedBeforeMapping) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(multicast_memory.Initialize(1024, 2, *executors[0]), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(0), IsOk());\n+  EXPECT_THAT(\n+      multicast_memory.MapMemory(reinterpret_cast<void*>(1), *executors[0]),\n+      StatusIs(absl::StatusCode::kFailedPrecondition,\n+               \"All devices should be subscribed.\"));\n+  ;\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, MulticastMemorySubscribeMoreDevices) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(multicast_memory.Initialize(1024, 2, *executors[0]), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(0), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(1), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(2),\n+              StatusIs(absl::StatusCode::kInvalidArgument,\n+                       \"All devices are already subscribed.\"));\n+  ;\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, MulticastMemoryUsingNonVmmMemory) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  const int64_t kNumDevices = 2;\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(multicast_memory.Initialize(1024, kNumDevices, *executors[0]),\n+              IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(0), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(1), IsOk());\n+\n+  DeviceMemoryBase device_memory = executors[0]->Allocate(1, 0);\n+  EXPECT_THAT(\n+      multicast_memory.MapMemory(device_memory.opaque(), *executors[0]),\n+      StatusIs(absl::StatusCode::kInternal,\n+               \"CUDA error: : CUDA_ERROR_INVALID_VALUE: invalid argument\"));\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, MulticastMemoryUsingVmmMemory) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  const int64_t kNumDevices = 2;\n+  const int64_t kMemorySize = 1024;\n+  CudaExecutor::MulticastMemory multicast_memory;\n+  EXPECT_THAT(\n+      multicast_memory.Initialize(kMemorySize, kNumDevices, *executors[0]),\n+      IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(0), IsOk());\n+  EXPECT_THAT(multicast_memory.SubscribeDevice(1), IsOk());\n+\n+  stream_executor::DeviceMemoryBase first_device_memory =\n+      executors[0]->Allocate(\n+          kMemorySize, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n+  EXPECT_THAT(\n+      multicast_memory.MapMemory(first_device_memory.opaque(), *executors[0]),\n+      IsOkAndHolds(NotNull()));\n+\n+  stream_executor::DeviceMemoryBase second_device_memory =\n+      executors[1]->Allocate(\n+          kMemorySize, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n+  EXPECT_THAT(\n+      multicast_memory.MapMemory(second_device_memory.opaque(), *executors[1]),\n+      IsOkAndHolds(NotNull()));\n+}\n+\n+}  // namespace\n+}  // namespace stream_executor::gpu"
        }
    ],
    "stats": {
        "total": 401,
        "additions": 401,
        "deletions": 0
    }
}