{
    "author": "KanishAnand",
    "message": "This is a mechanical change to replace `tile_assignment().num_dimensions()` by `num_dimensions()`.\nThis is 3/N cl's to privatize `tile_assignment()` method.\n\nPiperOrigin-RevId: 840218216",
    "sha": "0a0dcda405e32252ba4cd957644599a440b9f9e8",
    "files": [
        {
            "sha": "bac3485ccf1746928545298052ff5aef26dbc404",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -1296,12 +1296,10 @@ absl::StatusOr<std::unique_ptr<StrategyGroup>> CreateAllStrategiesGroup(\n // Two shardings shard the same dimension of a given tensor.\n bool ShardingIsConsistent(const HloSharding& partial_sharding,\n                           const HloSharding& complete_sharding, bool strict) {\n-  if (partial_sharding.tile_assignment().num_dimensions() >\n-      complete_sharding.tile_assignment().num_dimensions()) {\n+  if (partial_sharding.num_dimensions() > complete_sharding.num_dimensions()) {\n     return false;\n   }\n-  for (size_t i = 0; i < partial_sharding.tile_assignment().num_dimensions();\n-       ++i) {\n+  for (size_t i = 0; i < partial_sharding.num_dimensions(); ++i) {\n     if (strict && partial_sharding.dimension(i) > 1 &&\n         partial_sharding.dimension(i) == complete_sharding.dimension(i)) {\n       return true;"
        },
        {
            "sha": "bda908e326ea402d088658b548d580c0d33474e2",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_util.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -1082,7 +1082,7 @@ int64_t NumTileDimensions(const HloSharding& sharding) {\n     return -1;\n   }\n   int64_t num_tile_dims = 0;\n-  for (int i = 0; i < sharding.tile_assignment().num_dimensions(); i++) {\n+  for (int i = 0; i < sharding.num_dimensions(); i++) {\n     if (sharding.dimension(i) != 1) {\n       num_tile_dims++;\n     }\n@@ -1093,7 +1093,7 @@ int64_t NumTileDimensions(const HloSharding& sharding) {\n bool TileAssignmentMatchesMesh(const HloSharding& sharding,\n                                const DeviceMesh& mesh) {\n   int sharded_dims = 0;\n-  for (int i = 0; i < sharding.tile_assignment().num_dimensions(); ++i) {\n+  for (int i = 0; i < sharding.num_dimensions(); ++i) {\n     if (sharding.dimension(i) > 1) {\n       sharded_dims++;\n     }\n@@ -1166,7 +1166,7 @@ GetTensorDimToMeshDimMixedMeshSharding(int64_t tensor_shape_rank,\n \n   std::vector<absl::btree_set<int64_t>> tensor_dim_to_mesh_axis_mapping;\n   int mesh_axis_idx = 0;\n-  for (int i = 0; i < sharding.tile_assignment().num_dimensions(); ++i) {\n+  for (int i = 0; i < sharding.num_dimensions(); ++i) {\n     if (sharding.dimension(i) == 1) {\n       tensor_dim_to_mesh_axis_mapping.push_back({});\n       continue;\n@@ -2189,9 +2189,8 @@ AdjustShardingWithPartialMeshShapePerElement(\n           /*consider_reverse_device_meshes=*/true));\n \n   int mesh_axis_idx = 0;\n-  int end = sharding.ReplicateOnLastTileDim()\n-                ? sharding.tile_assignment().num_dimensions() - 1\n-                : sharding.tile_assignment().num_dimensions();\n+  int end = sharding.ReplicateOnLastTileDim() ? sharding.num_dimensions() - 1\n+                                              : sharding.num_dimensions();\n   for (int i = 0; i < end; ++i) {\n     if (sharding.dimension(i) == 1) {\n       new_tile_assignment_dimensions.push_back(1);"
        },
        {
            "sha": "1874dbceb05362b0b15f4f3bbf73a68e89057717",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -1178,10 +1178,10 @@ int64_t HloSharding::NumTiles(absl::Span<const int64_t> dims) const {\n   }\n   CHECK(!IsManual());\n   CHECK(!ReplicateOnLastTileDim() ||\n-        !absl::c_linear_search(dims, tile_assignment().num_dimensions() - 1));\n+        !absl::c_linear_search(dims, num_dimensions() - 1));\n   int64_t num_tiles = 1;\n   for (auto d : dims) {\n-    CHECK(d < tile_assignment().num_dimensions());\n+    CHECK(d < num_dimensions());\n     num_tiles *= dimension(d);\n   }\n   return num_tiles;"
        },
        {
            "sha": "c5b1b6ff27af460f6d10f4c726d13927d607b50e",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -518,6 +518,9 @@ class HloSharding {\n   // REQUIRES: !IsReplicated() && !IsTuple()\n   const TileAssignment& tile_assignment() const { return tile_assignment_; }\n \n+  // Returns the number of dimensions.\n+  int64_t num_dimensions() const { return tile_assignment().num_dimensions(); }\n+\n   // Returns number of shards in the given dimension.\n   int64_t dimension(int64_t dim_index) const {\n     return tile_assignment().dim(dim_index);"
        },
        {
            "sha": "00c20f8927eba25db22e276441956a0df931705b",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 52,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -451,15 +451,15 @@ bool MergeShardingIfCompatible(const HloSharding& to_merge,\n   }\n \n   // Combine the tile dimension sizes from dst and to_merge.\n-  DimensionVector perm_merge(dst->tile_assignment().num_dimensions(), -1);\n-  DimensionVector perm_dst(dst->tile_assignment().num_dimensions(), -1);\n+  DimensionVector perm_merge(dst->num_dimensions(), -1);\n+  DimensionVector perm_dst(dst->num_dimensions(), -1);\n   int64_t perm_merge_counter = 0;\n   int64_t perm_dst_counter = 0;\n   DimensionVector merge_old_tile_dim, dst_old_tile_dim;\n   DimensionVector merge_new_tile_dim, dst_new_tile_dim;\n   DimensionVector merge_new_tile_index, dst_new_tile_index;\n   DimensionVector merged_tile_dims;\n-  merged_tile_dims.reserve(dst->tile_assignment().num_dimensions());\n+  merged_tile_dims.reserve(dst->num_dimensions());\n   int64_t num_merge_groups = 1;\n   int64_t num_dst_groups = 1;\n   for (int64_t i = 0; i < to_merge.TiledDataRank(); ++i) {\n@@ -554,7 +554,7 @@ bool MergeShardingIfCompatible(const HloSharding& to_merge,\n         perm.pop_back();\n       } else {\n         new_tile_dims.push_back(replication);\n-        new_tile_indices.push_back(dst->tile_assignment().num_dimensions() - 1);\n+        new_tile_indices.push_back(dst->num_dimensions() - 1);\n       }\n \n       std::vector<TileAssignment> result;\n@@ -637,9 +637,8 @@ bool MergeShardingIfCompatible(const HloSharding& to_merge,\n     absl::Status compatible =\n         new_tile_array.EachStatus([&](absl::Span<const int64_t> indices,\n                                       int64_t* device) -> absl::Status {\n-          DimensionVector to_merge_index(\n-              to_merge.tile_assignment().num_dimensions());\n-          DimensionVector dst_index(dst->tile_assignment().num_dimensions());\n+          DimensionVector to_merge_index(to_merge.num_dimensions());\n+          DimensionVector dst_index(dst->num_dimensions());\n           for (int64_t i = 0; i < to_merge.TiledDataRank(); ++i) {\n             if (to_merge.dimension(i) == 1) {\n               to_merge_index[i] = 0;\n@@ -852,12 +851,12 @@ HloSharding TransposeSharding(const HloSharding& sharding,\n   std::vector<int> perm_dimensions(dimensions.begin(), dimensions.end());\n   // Add subgroup dims if missing.\n   if (sharding.TiledDataRank() == dimensions.size()) {\n-    for (int64_t i = sharding.TiledDataRank();\n-         i < sharding.tile_assignment().num_dimensions(); ++i) {\n+    for (int64_t i = sharding.TiledDataRank(); i < sharding.num_dimensions();\n+         ++i) {\n       perm_dimensions.push_back(i);\n     }\n   } else {\n-    CHECK_EQ(sharding.tile_assignment().num_dimensions(), dimensions.size());\n+    CHECK_EQ(sharding.num_dimensions(), dimensions.size());\n   }\n   auto tile_assignment = sharding.tile_assignment().Transpose(perm_dimensions);\n   if (!sharding.ReplicateOnLastTileDim()) {\n@@ -994,8 +993,8 @@ std::optional<HloSharding> ReshapeSharding(const Shape& source_shape,\n   const HloSharding sharding = PartiallyReplicateTiledShardingOnDims(\n       source_sharding, source_dims_to_replicate);\n \n-  for (int64_t i = sharding.TiledDataRank();\n-       i < sharding.tile_assignment().num_dimensions(); ++i) {\n+  for (int64_t i = sharding.TiledDataRank(); i < sharding.num_dimensions();\n+       ++i) {\n     target_tile_dims.push_back(\n         i == sharding.SubgroupReplicationDim() ? 1 : sharding.dimension(i));\n   }\n@@ -1061,15 +1060,14 @@ HloSharding PropagateShardingThroughReshape(const Shape& source_shape,\n       if (auto reshaped =\n               ReshapeSharding(source_shape, target_shape, grouped_sharding)) {\n         std::vector<int> perm;\n-        perm.reserve(sharding.tile_assignment().num_dimensions());\n+        perm.reserve(sharding.num_dimensions());\n         for (int64_t i = start_dim; i < end_dim; i++) {\n           perm.push_back(i);\n         }\n         for (int64_t i = 0; i < start_dim; i++) {\n           perm.push_back(i);\n         }\n-        for (int64_t i = end_dim;\n-             i < sharding.tile_assignment().num_dimensions(); i++) {\n+        for (int64_t i = end_dim; i < sharding.num_dimensions(); i++) {\n           perm.push_back(i);\n         }\n \n@@ -1155,12 +1153,12 @@ HloSharding PropagateShardingAlongDimsAndReplicateOthers(\n   std::vector<int64_t> argsort_target_dims = ArgSort(target_dims);\n   if (argsort_source_dims != argsort_target_dims) {\n     std::vector<int64_t> perm;\n-    perm.reserve(replicate_other_dims.tile_assignment().num_dimensions());\n+    perm.reserve(replicate_other_dims.num_dimensions());\n     for (int64_t i = 0; i < source_dims.size(); ++i) {\n       perm.push_back(source_dims[argsort_target_dims[i]]);\n     }\n     for (int64_t non_source_dim = 0, i = source_dims.size();\n-         i < replicate_other_dims.tile_assignment().num_dimensions(); ++i) {\n+         i < replicate_other_dims.num_dimensions(); ++i) {\n       while (absl::c_linear_search(source_dims, non_source_dim)) {\n         non_source_dim++;\n       }\n@@ -1175,7 +1173,7 @@ HloSharding PropagateShardingAlongDimsAndReplicateOthers(\n         source_sharding.dimension(source_dims[i]);\n   }\n   for (int64_t i = replicate_other_dims.TiledDataRank();\n-       i < replicate_other_dims.tile_assignment().num_dimensions(); ++i) {\n+       i < replicate_other_dims.num_dimensions(); ++i) {\n     target_tile_dims.push_back(replicate_other_dims.dimension(i));\n   }\n \n@@ -1672,8 +1670,7 @@ HloSharding ReplicateAllDataDims(const HloSharding& sharding,\n   if (data_rank >= 0 && data_rank != result.TiledDataRank() &&\n       !result.IsTileMaximal()) {\n     DimensionVector new_tile_shape(data_rank, 1);\n-    for (int64_t i = result.TiledDataRank();\n-         i < result.tile_assignment().num_dimensions(); ++i) {\n+    for (int64_t i = result.TiledDataRank(); i < result.num_dimensions(); ++i) {\n       new_tile_shape.push_back(result.dimension(i));\n     }\n     auto tile = result.tile_assignment().Reshape(new_tile_shape);\n@@ -1688,9 +1685,8 @@ HloSharding RemoveShapeDimensions(const HloSharding& sharding,\n     return sharding;\n   }\n   DimensionVector new_tile_shape;\n-  new_tile_shape.reserve(sharding.tile_assignment().num_dimensions() -\n-                         dims_to_remove.size());\n-  for (int64_t i = 0; i < sharding.tile_assignment().num_dimensions(); ++i) {\n+  new_tile_shape.reserve(sharding.num_dimensions() - dims_to_remove.size());\n+  for (int64_t i = 0; i < sharding.num_dimensions(); ++i) {\n     if (absl::c_linear_search(dims_to_remove, i)) {\n       CHECK_EQ(sharding.dimension(i), 1);\n     } else {\n@@ -1710,13 +1706,11 @@ std::optional<HloSharding> TransposeShardingWithCollapsedDims(\n   if (source.IsTileMaximal() || source.IsManual()) {\n     return source;\n   }\n-  if (src_to_tgt.size() < source.tile_assignment().num_dimensions()) {\n+  if (src_to_tgt.size() < source.num_dimensions()) {\n     // Add missing subgroup dims.\n     DimensionVector new_src_to_tgt(src_to_tgt.begin(), src_to_tgt.end());\n     DimensionVector new_tgt_to_src(tgt_to_src.begin(), tgt_to_src.end());\n-    for (int64_t i = 0;\n-         i < source.tile_assignment().num_dimensions() - src_to_tgt.size();\n-         ++i) {\n+    for (int64_t i = 0; i < source.num_dimensions() - src_to_tgt.size(); ++i) {\n       new_src_to_tgt.push_back(tgt_to_src.size() + i);\n       new_tgt_to_src.push_back(src_to_tgt.size() + i);\n     }\n@@ -2111,7 +2105,7 @@ std::string GroupedSharding::ToString() const {\n GroupedSharding GroupShardingOnAllDimsExcept(\n     const HloSharding& sharding, absl::Span<const int64_t> non_group_dims,\n     bool subgroup_manual) {\n-  std::vector<int64_t> group_dims(sharding.tile_assignment().num_dimensions());\n+  std::vector<int64_t> group_dims(sharding.num_dimensions());\n   absl::c_iota(group_dims, 0);\n \n   group_dims.erase(\n@@ -2139,7 +2133,7 @@ GroupedSharding GroupShardingOnDims(const HloSharding& sharding,\n   // The first item of the pair is the group_dim_size. The second item is the\n   // group_dim_shard.\n   std::vector<std::pair<int64_t, int64_t>> decomposed_tiling_dims(\n-      sharding.tile_assignment().num_dimensions());\n+      sharding.num_dimensions());\n   for (int64_t i = 0; i < decomposed_tiling_dims.size(); ++i) {\n     // Set default values for group_dim_size and group_dim_shard.\n     decomposed_tiling_dims[i] = std::make_pair(1, sharding.dimension(i));\n@@ -2195,11 +2189,11 @@ GroupedSharding GroupShardingOnDims(const HloSharding& sharding,\n   DeviceGroupTileAssignment device_groups = DeviceGroupTileAssignment(\n       grouped_tile_assignment.Reshape({num_device_groups, device_group_size}));\n \n-  auto grouped = GroupedSharding(\n-      std::move(device_groups),\n-      DimensionVector(group_dims.begin(), group_dims.end()),\n-      std::move(group_dim_sizes), sharding.tile_assignment().num_dimensions(),\n-      HloSharding::Replicate(), subgroup_manual);\n+  auto grouped =\n+      GroupedSharding(std::move(device_groups),\n+                      DimensionVector(group_dims.begin(), group_dims.end()),\n+                      std::move(group_dim_sizes), sharding.num_dimensions(),\n+                      HloSharding::Replicate(), subgroup_manual);\n   if (sharding.ReplicateOnLastTileDim()) {\n     grouped.data_rank--;\n   }\n@@ -2212,9 +2206,8 @@ GroupedSharding GroupShardingOnDims(const HloSharding& sharding,\n     return grouped;\n   }\n   if (sharding.IsManualSubgroup()) {\n-    int64_t tile_dimensions = sharding.tile_assignment().num_dimensions();\n     int64_t subgroup_size = sharding.subgroup_types().size();\n-    int64_t rank = tile_dimensions - subgroup_size;\n+    int64_t rank = sharding.num_dimensions() - subgroup_size;\n     int num_dims_erase = 0;\n     for (int i = 0; i < subgroup_size; i++) {\n       if (sharding.subgroup_types()[i] == OpSharding::MANUAL) {\n@@ -2230,8 +2223,7 @@ GroupedSharding GroupShardingOnDims(const HloSharding& sharding,\n   TileAssignment grouped_tiling(grouped_tiling_dims);\n   grouped.sharding =\n       sharding.ReplicateOnLastTileDim() &&\n-              grouped_tiling_dims.size() ==\n-                  sharding.tile_assignment().num_dimensions()\n+              grouped_tiling_dims.size() == sharding.num_dimensions()\n           ? HloSharding::PartialTile(grouped_tiling, sharding.metadata())\n           : HloSharding::Tile(grouped_tiling, sharding.metadata());\n   return grouped;\n@@ -2264,9 +2256,8 @@ GroupedSharding GroupShardingOnReplicatedDim(\n       sharding.tile_assignment().dimensions().back() % num_groups == 0) {\n     absl::InlinedVector<int64_t, 1> group_dim_shards = {\n         sharding.tile_assignment().dimensions().back() / num_groups};\n-    return GroupShardingOnDims(\n-        sharding, {sharding.tile_assignment().num_dimensions() - 1},\n-        group_dim_shards);\n+    return GroupShardingOnDims(sharding, {sharding.num_dimensions() - 1},\n+                               group_dim_shards);\n   }\n \n   // 2. Try borrow dimensions from replicable_dims in order, and group sharding.\n@@ -2311,9 +2302,8 @@ GroupedSharding GroupShardingOnReplicatedDim(\n         HloSharding partial_sharding = HloSharding::PartialTile(\n             tile_assignment.value(), sharding.metadata());\n         if (!partial_sharding.IsReplicated()) {\n-          return GroupShardingOnDims(\n-              partial_sharding,\n-              {partial_sharding.tile_assignment().num_dimensions() - 1});\n+          return GroupShardingOnDims(partial_sharding,\n+                                     {partial_sharding.num_dimensions() - 1});\n         }\n       }\n     }\n@@ -2336,9 +2326,8 @@ GroupedSharding GetGroupedReplicatedSharding(const int64_t num_groups,\n \n GroupedSharding GetManualSubgroupSharding(const HloSharding& sharding) {\n   CHECK(sharding.IsManualSubgroup());\n-  int64_t tile_dimensions = sharding.tile_assignment().num_dimensions();\n   int64_t subgroup_size = sharding.subgroup_types().size();\n-  int64_t rank = tile_dimensions - subgroup_size;\n+  int64_t rank = sharding.num_dimensions() - subgroup_size;\n   DimensionVector group_dims;\n   bool last_tile_dim_replicate = false;\n \n@@ -2354,7 +2343,7 @@ GroupedSharding GetManualSubgroupSharding(const HloSharding& sharding) {\n       GroupShardingOnDims(sharding, group_dims, /*subgroup_manual=*/true);\n \n   if (last_tile_dim_replicate ||\n-      group_sharding.sharding.tile_assignment().num_dimensions() > rank) {\n+      group_sharding.sharding.num_dimensions() > rank) {\n     group_sharding.sharding = HloSharding::PartialTile(\n         group_sharding.sharding.tile_assignment(), sharding.metadata());\n   }\n@@ -2374,7 +2363,7 @@ PartialReplicatedGroupShardingWithAssignedDeviceGroups(\n   }\n   std::vector<DimensionVector> device_to_index(\n       Product(sharding.tile_assignment().dimensions()),\n-      DimensionVector(sharding.tile_assignment().num_dimensions()));\n+      DimensionVector(sharding.num_dimensions()));\n   sharding.tile_assignment().Each(\n       [&device_to_index](absl::Span<const int64_t> indices, int64_t device) {\n         device_to_index[device].assign(indices.begin(), indices.end());\n@@ -2429,11 +2418,10 @@ PartialReplicatedGroupShardingWithAssignedDeviceGroups(\n       return std::nullopt;\n     }\n   }\n-  return GroupedSharding(device_groups,\n-                         {sharding.tile_assignment().num_dimensions() - 1},\n+  return GroupedSharding(device_groups, {sharding.num_dimensions() - 1},\n                          {shard_size_on_replicated_dim},\n-                         sharding.tile_assignment().num_dimensions() - 1,\n-                         *final_sharding, /*subgroup_manual=*/false);\n+                         sharding.num_dimensions() - 1, *final_sharding,\n+                         /*subgroup_manual=*/false);\n }\n \n HloSharding UngroupSharding(const GroupedSharding& grouped_sharding) {"
        },
        {
            "sha": "ecaad635b7a440e266c623f95de11ff4c7e301f6",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -661,10 +661,10 @@ TEST(HloShardingUtilTest, UngroupSharding_ManualOnly) {\n   DimensionVector group_dims = {2};\n   DimensionVector group_dim_sizes = {2};\n \n-  auto grouped = GroupedSharding(\n-      std::move(device_groups), std::move(group_dims),\n-      std::move(group_dim_sizes), sharding.tile_assignment().num_dimensions(),\n-      sharding, /*subgroup_manual=*/true);\n+  auto grouped =\n+      GroupedSharding(std::move(device_groups), std::move(group_dims),\n+                      std::move(group_dim_sizes), sharding.num_dimensions(),\n+                      sharding, /*subgroup_manual=*/true);\n \n   HloSharding ungroup_sharding = UngroupSharding(grouped);\n \n@@ -678,11 +678,10 @@ TEST(HloShardingUtilTest, UngroupSharding_ReplicatedAndManual) {\n   DimensionVector group_dims = {3};\n   DimensionVector group_dim_sizes = {2};\n \n-  auto grouped =\n-      GroupedSharding(std::move(device_groups), std::move(group_dims),\n-                      std::move(group_dim_sizes),\n-                      sharding.tile_assignment().num_dimensions() - 1, sharding,\n-                      /*subgroup_manual=*/true);\n+  auto grouped = GroupedSharding(\n+      std::move(device_groups), std::move(group_dims),\n+      std::move(group_dim_sizes), sharding.num_dimensions() - 1, sharding,\n+      /*subgroup_manual=*/true);\n \n   HloSharding ungroup_sharding = UngroupSharding(grouped);\n   VLOG(1) << \"ungroup_sharding: \" << ungroup_sharding.ToString();\n@@ -698,11 +697,10 @@ TEST(HloShardingUtilTest, UngroupSharding_ManualAndReplicated) {\n   DimensionVector group_dims = {2};\n   DimensionVector group_dim_sizes = {2};\n \n-  auto grouped =\n-      GroupedSharding(std::move(device_groups), std::move(group_dims),\n-                      std::move(group_dim_sizes),\n-                      sharding.tile_assignment().num_dimensions() - 1, sharding,\n-                      /*subgroup_manual=*/true);\n+  auto grouped = GroupedSharding(\n+      std::move(device_groups), std::move(group_dims),\n+      std::move(group_dim_sizes), sharding.num_dimensions() - 1, sharding,\n+      /*subgroup_manual=*/true);\n \n   HloSharding ungroup_sharding = UngroupSharding(grouped);\n   VLOG(1) << \"ungroup_sharding: \" << ungroup_sharding.ToString();"
        },
        {
            "sha": "b6589ed82a60c73e5ab5ecef05e46d2fb2e232f9",
            "filename": "third_party/xla/xla/service/sharding_propagation.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_propagation.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -2294,7 +2294,7 @@ bool ShardingPropagation::InferShardingFromOperands(\n         }\n       }\n       for (int64_t i = op->sharding().TiledDataRank();\n-           i < op->sharding().tile_assignment().num_dimensions(); ++i) {\n+           i < op->sharding().num_dimensions(); ++i) {\n         target_tile_assignment_dimensions.push_back(\n             op->sharding().dimension(i));\n       }"
        },
        {
            "sha": "1731e723402460bc25647734128addd3ac0080b2",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 17,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -906,8 +906,7 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n   CHECK_EQ(Product(slice_sharding->tile_assignment().dimensions()),\n            num_partitions);\n   int64_t slice_sharding_dim = -1;\n-  for (int64_t i = 0; i < slice_sharding->tile_assignment().num_dimensions();\n-       ++i) {\n+  for (int64_t i = 0; i < slice_sharding->num_dimensions(); ++i) {\n     if (slice_sharding->dimension(i) > 1) {\n       slice_sharding_dim = i;\n       break;\n@@ -2422,16 +2421,13 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n     // output device groups. If fails then try find a dimension to swap instead\n     // of reordering the mesh with collective permutes that can create weird\n     // patterns. If that fails also do the traditional replication matching.\n-    for (int64_t i = other_sharding.tile_assignment().num_dimensions() - 1;\n-         i >= 0; --i) {\n+    for (int64_t i = other_sharding.num_dimensions() - 1; i >= 0; --i) {\n       if (other_sharding.tile_assignment().dimensions()[i] % group_count == 0) {\n-        std::vector<int64_t> perm(\n-            other_sharding.tile_assignment().num_dimensions(), 0);\n+        std::vector<int64_t> perm(other_sharding.num_dimensions(), 0);\n         absl::c_iota(perm, 0);\n-        std::swap(perm[i],\n-                  perm[other_sharding.tile_assignment().num_dimensions() - 1]);\n+        std::swap(perm[i], perm[other_sharding.num_dimensions() - 1]);\n         auto sharding_to_match =\n-            i == other_sharding.tile_assignment().num_dimensions() - 1\n+            i == other_sharding.num_dimensions() - 1\n                 ? other_sharding\n                 : hlo_sharding_util::TransposeSharding(other_sharding, perm);\n         if (auto grouped_sharding = hlo_sharding_util::\n@@ -2444,8 +2440,7 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n         }\n       }\n     }\n-    other_group_dims.push_back(\n-        other_sharding.tile_assignment().num_dimensions() - 1);\n+    other_group_dims.push_back(other_sharding.num_dimensions() - 1);\n   }\n   if (other_group_dims.empty()) {\n     const bool may_replicate_other_contracting_dims =\n@@ -2465,8 +2460,7 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n                other_sharding.tile_assignment().dimensions().back() %\n                        group_count ==\n                    0) {\n-      other_group_dims.push_back(\n-          other_sharding.tile_assignment().num_dimensions() - 1);\n+      other_group_dims.push_back(other_sharding.num_dimensions() - 1);\n     } else if (may_replicate_other_contracting_dims &&\n                (!may_replicate_other_non_contracting_dims ||\n                 ShapeUtil::ByteSizeOf(other_shape) <=\n@@ -2484,8 +2478,7 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n     }\n   }\n   if (other_group_dims.size() == 1 &&\n-      other_group_dims[0] ==\n-          other_sharding.tile_assignment().num_dimensions() - 1) {\n+      other_group_dims[0] == other_sharding.num_dimensions() - 1) {\n     std::vector<int64_t> group_dim_shards = {\n         other_sharding.tile_assignment().dimensions().back() / group_count};\n     return AlignGroupsWith(\n@@ -2698,8 +2691,7 @@ GetDotGroupPartitionContractingOutputShardings(\n         output_sharding.tile_assignment().dimensions().back() / group_count};\n     auto grouped = AlignGroupsWith(\n         hlo_sharding_util::GroupShardingOnDims(\n-            output_sharding,\n-            {output_sharding.tile_assignment().num_dimensions() - 1},\n+            output_sharding, {output_sharding.num_dimensions() - 1},\n             group_dim_shards),\n         lhs_grouped,\n         /*ignore_group_order=*/true);"
        },
        {
            "sha": "c50b3245897ab1168e01d4b1fd2252b1414a6c46",
            "filename": "third_party/xla/xla/service/spmd/gather_scatter_handler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fgather_scatter_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fgather_scatter_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fgather_scatter_handler.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -1495,8 +1495,7 @@ absl::StatusOr<HloInstruction*> PartitionScatterIndexPassthroughDimensions(\n   auto partition_id = indices.state().partition_id;\n   if (indices.sharding().ReplicateOnLastTileDim()) {\n     auto sharding_grouped = hlo_sharding_util::GroupShardingOnDims(\n-        indices.sharding(),\n-        {indices.sharding().tile_assignment().num_dimensions() - 1});\n+        indices.sharding(), {indices.sharding().num_dimensions() - 1});\n     auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n         indices.state(), sharding_grouped.device_groups, b);\n     partition_id = per_group_partitioner_state.partition_id;"
        },
        {
            "sha": "588d9c776263b5c25ceeafec21f68dc47387c2d3",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 17,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -673,8 +673,7 @@ PartitionedHlo PartitionedHlo::ReshardNoCache(\n \n   // 'Replicated' to partial replicated.\n   if (target.ReplicateOnLastTileDim()) {\n-    std::vector<int64_t> group_dims(target.tile_assignment().num_dimensions() -\n-                                    1);\n+    std::vector<int64_t> group_dims(target.num_dimensions() - 1);\n     absl::c_iota(group_dims, 0);\n     auto target_grouped =\n         hlo_sharding_util::GroupShardingOnDims(target, group_dims);\n@@ -1374,17 +1373,15 @@ HloInstruction* PartitionedHlo::ReplicatePartial(\n   HloInstruction* broadcast = hlo_;\n   if (!broadcast_dims.empty()) {\n     std::vector<int64_t> other_dims;\n-    for (int64_t i = 0; i < sharding().tile_assignment().num_dimensions();\n-         ++i) {\n+    for (int64_t i = 0; i < sharding().num_dimensions(); ++i) {\n       if (!absl::c_linear_search(broadcast_dims, i)) {\n         other_dims.push_back(i);\n       }\n     }\n     HloSharding original_sharding = sharding();\n     auto grouped =\n         hlo_sharding_util::GroupShardingOnDims(original_sharding, other_dims);\n-    std::vector<int64_t> dev_indices(\n-        grouped.sharding.tile_assignment().num_dimensions(), 0);\n+    std::vector<int64_t> dev_indices(grouped.sharding.num_dimensions(), 0);\n     hlo_->set_sharding(HloSharding::AssignDevice(\n         grouped.sharding.tile_assignment()(dev_indices)));\n     auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n@@ -1558,7 +1555,7 @@ PartitionedHlo::ReshardFromPartialReplicateWithDynamicSlice(\n   std::vector<int64_t> expand_tile_dims;\n   std::vector<int64_t> tiling_dim_factors;\n   int64_t rank = hlo_->shape().dimensions().size();\n-  tiling_dim_factors.reserve(target.tile_assignment().num_dimensions());\n+  tiling_dim_factors.reserve(target.num_dimensions());\n   const auto& temp_target_sharding = target_compatible_sharding.value();\n   for (int64_t dim = 0; dim < rank; dim++) {\n     if (temp_target_sharding.dimension(dim) > sharding().dimension(dim)) {\n@@ -2238,7 +2235,7 @@ std::optional<PartitionedHlo> PartitionedHlo::TryComplexReshardHandling(\n     // but in the target it is not and some other dimension got moved or\n     // modified. Try to remove the partial replication to simplify the step from\n     // source to target sharding.\n-    for (int64_t i = 0; i < target.tile_assignment().num_dimensions(); ++i) {\n+    for (int64_t i = 0; i < target.num_dimensions(); ++i) {\n       if (target.dimension(i) != sharding().dimension(i) &&\n           sharding().dimension(i) == 1 &&\n           target.dimension(i) % partial_repl_amount == 0) {\n@@ -2251,8 +2248,7 @@ std::optional<PartitionedHlo> PartitionedHlo::TryComplexReshardHandling(\n     }\n     VLOG(5) << \"Matched partially replicated to non partially replicated: \"\n             << sharding().ToString();\n-    std::vector<int64_t> transpose_dims(\n-        sharding().tile_assignment().num_dimensions(), 0);\n+    std::vector<int64_t> transpose_dims(sharding().num_dimensions(), 0);\n     absl::c_iota(transpose_dims, 0);\n     std::swap(transpose_dims[first_different_dimension], transpose_dims.back());\n     auto intermediate_sharding =\n@@ -2294,14 +2290,13 @@ PartitionedHlo::ReshardPartialReplicateWithAllToAll(\n   // the last tile dim will be sharded after all-to-all.\n   const int num_replicas =\n       partial_replicate_sharding.tile_assignment().dimensions().back();\n-  if (((tile_sharding.tile_assignment().num_dimensions() + 1) !=\n-       partial_replicate_sharding.tile_assignment().num_dimensions()) ||\n+  if (((tile_sharding.num_dimensions() + 1) !=\n+       partial_replicate_sharding.num_dimensions()) ||\n       (partial_replicate_sharding.dimension(0) != 1)) {\n     return std::nullopt;\n   }\n   int to_replicate_dim = -1;\n-  for (int i = tile_sharding.tile_assignment().num_dimensions() - 1; i >= 0;\n-       --i) {\n+  for (int i = tile_sharding.num_dimensions() - 1; i >= 0; --i) {\n     if (tile_sharding.dimension(i) > 1 && (to_replicate_dim == -1)) {\n       if (tile_sharding.dimension(i) != num_replicas) {\n         return std::nullopt;\n@@ -4581,8 +4576,7 @@ absl::Status SpmdPartitioningVisitor::HandleRng(HloInstruction* hlo) {\n                           MakePartitionedShape(hlo->shape(), hlo->sharding()),\n                           hlo->random_distribution(), new_operands)));\n   } else {\n-    std::vector<int64_t> group_dims(\n-        hlo->sharding().tile_assignment().num_dimensions() - 1);\n+    std::vector<int64_t> group_dims(hlo->sharding().num_dimensions() - 1);\n     absl::c_iota(group_dims, 0);\n     auto sharding_grouped =\n         hlo_sharding_util::GroupShardingOnDims(hlo->sharding(), group_dims);\n@@ -5293,7 +5287,7 @@ SpmdPartitioner::AllGatherShardsInternal(\n   ag = result;\n   // If n > 1 dimensions are partitioned, split the leading dimension to n.\n   std::vector<int64_t> tiled_dims;\n-  for (int64_t i = 0; i < sharding.tile_assignment().num_dimensions(); ++i) {\n+  for (int64_t i = 0; i < sharding.num_dimensions(); ++i) {\n     if (sharding.dimension(i) > 1 && absl::c_linear_search(selected_dims, i)) {\n       tiled_dims.push_back(i);\n     }"
        },
        {
            "sha": "905659e92cf04669da1f37069aedb9b3f759ab66",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 12,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -929,8 +929,8 @@ std::optional<int64_t> UniqueTiledDim(const HloSharding& sharding) {\n   }\n   int64_t dim = -1;\n   int64_t rank = sharding.ReplicateOnLastTileDim()\n-                     ? sharding.tile_assignment().num_dimensions() - 1\n-                     : sharding.tile_assignment().num_dimensions();\n+                     ? sharding.num_dimensions() - 1\n+                     : sharding.num_dimensions();\n   for (int64_t i = 0; i < rank; ++i) {\n     if (sharding.dimension(i) > 1) {\n       if (dim != -1) {\n@@ -2153,16 +2153,15 @@ std::optional<std::vector<std::pair<int64_t, int64_t>>>\n GetReshardAllToAllSourceTargetDims(const HloSharding& source,\n                                    const HloSharding& target) {\n   if (source.IsTileMaximal() || target.IsTileMaximal() ||\n-      source.tile_assignment().num_dimensions() !=\n-          target.tile_assignment().num_dimensions() ||\n+      source.num_dimensions() != target.num_dimensions() ||\n       source.NumTiles() != target.NumTiles()) {\n     return std::nullopt;\n   }\n   // Record partition count to index for indices that have different partition\n   // counts on source and target.\n   std::map<int64_t, std::vector<int64_t>> source_size_to_dim;\n   std::map<int64_t, std::vector<int64_t>> target_size_to_dim;\n-  for (int64_t i = 0; i < source.tile_assignment().num_dimensions(); ++i) {\n+  for (int64_t i = 0; i < source.num_dimensions(); ++i) {\n     if (source.dimension(i) == target.dimension(i)) {\n       continue;\n     }\n@@ -2466,7 +2465,7 @@ std::optional<std::vector<int64_t>> FindMatchingPartitionedDimsForGrouping(\n   std::vector<int64_t> dims;\n   if (device_groups.num_devices_per_group() < 2) {\n     // Trivial case: single member groups\n-    for (int64_t i = 0; i < sharding.tile_assignment().num_dimensions(); ++i) {\n+    for (int64_t i = 0; i < sharding.num_dimensions(); ++i) {\n       if (sharding.dimension(i) > 1) {\n         dims.push_back(i);\n       }\n@@ -2475,14 +2474,13 @@ std::optional<std::vector<int64_t>> FindMatchingPartitionedDimsForGrouping(\n   }\n \n   std::vector<std::vector<int64_t>> device_to_index(\n-      num_devices,\n-      std::vector<int64_t>(sharding.tile_assignment().num_dimensions()));\n+      num_devices, std::vector<int64_t>(sharding.num_dimensions()));\n   sharding.tile_assignment().Each(\n       [&](absl::Span<const int64_t> index, int64_t device) {\n         device_to_index[device].assign(index.begin(), index.end());\n       });\n   int64_t group_count = 1;\n-  for (int64_t i = 0; i < sharding.tile_assignment().num_dimensions(); ++i) {\n+  for (int64_t i = 0; i < sharding.num_dimensions(); ++i) {\n     if (device_to_index[device_groups(0, 0)][i] ==\n         device_to_index[device_groups(0, 1)][i]) {\n       dims.push_back(i);\n@@ -2960,10 +2958,9 @@ std::optional<IotaReplicaGroupList> GetIotaPartitionGroupsAcrossTargetDims(\n       sharding.tile_assignment().num_elements() / total_group_size;\n \n   std::vector<int64_t> reshape_dimensions;\n-  reshape_dimensions.reserve(sharding.tile_assignment().num_dimensions());\n+  reshape_dimensions.reserve(sharding.num_dimensions());\n   std::vector<int64_t> target_dim_locations;\n-  for (int64_t dim = 0; dim < sharding.tile_assignment().num_dimensions();\n-       ++dim) {\n+  for (int64_t dim = 0; dim < sharding.num_dimensions(); ++dim) {\n     if (auto it = absl::c_find(target_dims, dim); it != target_dims.end()) {\n       int64_t current_val = sharding.dimension(dim);\n       int64_t group_size = group_sizes[std::distance(target_dims.begin(), it)];"
        },
        {
            "sha": "de4d6360d01764226b9426f737822c8cfc6316b0",
            "filename": "third_party/xla/xla/tests/collective_ops_sharded_unsharded_e2e_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_sharded_unsharded_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0a0dcda405e32252ba4cd957644599a440b9f9e8/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_sharded_unsharded_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_sharded_unsharded_e2e_test.cc?ref=0a0dcda405e32252ba4cd957644599a440b9f9e8",
            "patch": "@@ -240,7 +240,7 @@ class CollectiveOpsTestE2EShardedUnsharded : public CollectiveOpsE2ETestBase {\n                            std::vector<int64_t>& sharded_dims,\n                            const HloSharding& sharding) {\n     if (!sharding.IsReplicated()) {\n-      for (int k = 0; k < sharding.tile_assignment().num_dimensions(); ++k) {\n+      for (int k = 0; k < sharding.num_dimensions(); ++k) {\n         if (sharding.dimension(k) > 1) {\n           dims_per_shard[k] /= sharding.dimension(k);\n           sharded_dims.push_back(k);"
        }
    ],
    "stats": {
        "total": 224,
        "additions": 96,
        "deletions": 128
    }
}