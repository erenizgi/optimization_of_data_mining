{
    "author": "ezhulenev",
    "message": "[xla:cpu:ynn] Do not track work stealing workers\n\n```\nname                                                               cpu/op         cpu/op      vs base\nBM_ParallelFor/8/1/process_time   [#threads=8, #threadpools=1  ]    5.470m ±  5%   5.095m ± 3%  -6.87% (p=0.000 n=80)\nBM_ParallelFor/8/2/process_time   [#threads=8, #threadpools=2  ]    2.857m ±  1%   2.595m ± 2%  -9.15% (n=80)\nBM_ParallelFor/8/4/process_time   [#threads=8, #threadpools=4  ]    1.447m ± 10%   1.328m ± 1%  -8.23% (p=0.000 n=80)\nBM_ParallelFor/8/8/process_time   [#threads=8, #threadpools=8  ]   1058.1µ ± 20%   974.5µ ± 1%  -7.90% (p=0.000 n=80)\nBM_ParallelFor/8/16/process_time  [#threads=8, #threadpools=16 ]    741.5µ ± 26%   705.8µ ± 1%  -4.81% (p=0.000 n=80)\nBM_ParallelFor/16/1/process_time  [#threads=16, #threadpools=1 ]    9.796m ± 29%   9.972m ± 2%       ~ (p=0.312 n=80)\nBM_ParallelFor/16/2/process_time  [#threads=16, #threadpools=2 ]    7.871m ± 28%   7.706m ± 1%  -2.10% (p=0.030 n=80)\nBM_ParallelFor/16/4/process_time  [#threads=16, #threadpools=4 ]    4.330m ±  2%   4.157m ± 1%  -3.99% (p=0.000 n=80)\nBM_ParallelFor/16/8/process_time  [#threads=16, #threadpools=8 ]    2.678m ±  2%   2.638m ± 1%  -1.49% (p=0.014 n=80)\nBM_ParallelFor/16/16/process_time [#threads=16, #threadpools=16]    1.791m ±  1%   1.807m ± 1%       ~ (p=0.325 n=80)\nBM_ParallelFor/32/1/process_time  [#threads=32, #threadpools=1 ]    15.33m ±  1%   15.41m ± 1%       ~ (p=0.215 n=80)\nBM_ParallelFor/32/2/process_time  [#threads=32, #threadpools=2 ]    13.99m ±  1%   13.80m ± 2%       ~ (p=0.400 n=80)\nBM_ParallelFor/32/4/process_time  [#threads=32, #threadpools=4 ]    9.415m ±  1%   9.172m ± 1%  -2.58% (p=0.000 n=80)\nBM_ParallelFor/32/8/process_time  [#threads=32, #threadpools=8 ]    5.759m ±  1%   5.647m ± 1%  -1.95% (p=0.004 n=80)\nBM_ParallelFor/32/16/process_time [#threads=32, #threadpools=16]    3.932m ±  1%   3.864m ± 1%  -1.72% (p=0.006 n=80)\ngeomean                                                            4.051m         3.916m       -3.32%\n\nname                                                               time/op        time/op     vs base\nBM_ParallelFor/8/1/process_time   [#threads=8, #threadpools=1  ]    651.2µ ±  3%   600.3µ ± 4%  -7.80% (p=0.000 n=80)\nBM_ParallelFor/8/2/process_time   [#threads=8, #threadpools=2  ]    329.4µ ±  0%   298.6µ ± 2%  -9.35% (n=80)\nBM_ParallelFor/8/4/process_time   [#threads=8, #threadpools=4  ]    169.3µ ± 12%   155.7µ ± 1%  -8.05% (p=0.000 n=80)\nBM_ParallelFor/8/8/process_time   [#threads=8, #threadpools=8  ]    125.8µ ± 21%   115.7µ ± 1%  -8.08% (p=0.000 n=80)\nBM_ParallelFor/8/16/process_time  [#threads=8, #threadpools=16 ]    95.41µ ± 24%   89.56µ ± 1%  -6.13% (p=0.000 n=80)\nBM_ParallelFor/16/1/process_time  [#threads=16, #threadpools=1 ]   1015.8µ ±  1%   952.0µ ± 1%  -6.29% (n=80)\nBM_ParallelFor/16/2/process_time  [#threads=16, #threadpools=2 ]    556.5µ ±  1%   522.6µ ± 1%  -6.09% (n=80)\nBM_ParallelFor/16/4/process_time  [#threads=16, #threadpools=4 ]    289.7µ ±  2%   274.4µ ± 1%  -5.30% (p=0.000 n=80)\nBM_ParallelFor/16/8/process_time  [#threads=16, #threadpools=8 ]    178.8µ ±  2%   174.1µ ± 1%  -2.59% (p=0.000 n=80)\nBM_ParallelFor/16/16/process_time [#threads=16, #threadpools=16]    123.9µ ±  2%   123.0µ ± 1%       ~ (p=0.098 n=80)\nBM_ParallelFor/32/1/process_time  [#threads=32, #threadpools=1 ]    1.526m ±  3%   1.433m ± 3%  -6.07% (p=0.000 n=80)\nBM_ParallelFor/32/2/process_time  [#threads=32, #threadpools=2 ]    835.2µ ±  2%   783.5µ ± 2%  -6.19% (p=0.000 n=80)\nBM_ParallelFor/32/4/process_time  [#threads=32, #threadpools=4 ]    471.6µ ±  2%   455.1µ ± 1%  -3.52% (p=0.000 n=80)\nBM_ParallelFor/32/8/process_time  [#threads=32, #threadpools=8 ]    296.1µ ±  2%   287.0µ ± 2%  -3.08% (p=0.000 n=80)\nBM_ParallelFor/32/16/process_time [#threads=32, #threadpools=16]    215.0µ ±  2%   211.6µ ± 1%  -1.59% (p=0.018 n=80)\ngeomean                                                            330.2µ         312.3µ       -5.42%\n```\n\nPiperOrigin-RevId: 824259124",
    "sha": "5edcd28152d182dcda22806ceb332ab4033dd455",
    "files": [
        {
            "sha": "2513c09ef2f577e8810d003641aa9141f0cfa885",
            "filename": "third_party/xla/xla/backends/cpu/runtime/work_queue.h",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5edcd28152d182dcda22806ceb332ab4033dd455/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5edcd28152d182dcda22806ceb332ab4033dd455/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h?ref=5edcd28152d182dcda22806ceb332ab4033dd455",
            "patch": "@@ -93,7 +93,11 @@ class Worker {\n  public:\n   Worker(size_t worker_index, WorkQueue* queue);\n \n-  std::optional<size_t> Pop();\n+  // Pops a work item from the work queue. If `notify_work_stealing` is true,\n+  // the worker will notify the work queue when it switches to the work\n+  // stealing mode. Worker parallelization has an optimization to avoid\n+  // scheduling more workers if there are workers in the work stealing mode.\n+  std::optional<size_t> Pop(bool notify_work_stealing = true);\n \n   // Schedule `num_workers` workers into the Eigen thread pool that process\n   // `num_work_items` parallel work items and return an async value that becomes\n@@ -182,15 +186,16 @@ inline Worker::Worker(size_t worker_index, WorkQueue* queue)\n       partition_index_(worker_index),\n       queue_(queue) {}\n \n-inline std::optional<size_t> Worker::Pop() {\n+inline std::optional<size_t> Worker::Pop(bool notify_work_stealing) {\n   std::optional<size_t> work_item = queue_->Pop(partition_index_);\n   if (ABSL_PREDICT_TRUE(work_item)) {\n     return work_item;\n   }\n \n   // If we didn't find a work item in the initially assigned partition, notify\n   // the work queue that we are switching to work stealing mode.\n-  if (ABSL_PREDICT_FALSE(partition_index_ == worker_index_)) {\n+  if (ABSL_PREDICT_FALSE(notify_work_stealing &&\n+                         partition_index_ == worker_index_)) {\n     queue_->NotifyWorkStealingWorker();\n   }\n "
        },
        {
            "sha": "d3408241aca86094b8128f88b655bf7fc9f40d0d",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5edcd28152d182dcda22806ceb332ab4033dd455/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5edcd28152d182dcda22806ceb332ab4033dd455/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2FBUILD?ref=5edcd28152d182dcda22806ceb332ab4033dd455",
            "patch": "@@ -28,6 +28,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@eigen_archive//:eigen3\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n         \"@slinky//slinky/base:thread_pool\",\n     ],\n )"
        },
        {
            "sha": "018d7102b13230517706a325093d89e20bf379f0",
            "filename": "third_party/xla/xla/backends/cpu/runtime/ynnpack/slinky_threadpool.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 9,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5edcd28152d182dcda22806ceb332ab4033dd455/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5edcd28152d182dcda22806ceb332ab4033dd455/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fynnpack%2Fslinky_threadpool.cc?ref=5edcd28152d182dcda22806ceb332ab4033dd455",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"slinky/base/ref_count.h\"\n #include \"slinky/base/thread_pool.h\"\n #include \"xla/backends/cpu/runtime/work_queue.h\"\n+#include \"tsl/profiler/lib/traceme.h\"\n \n #define EIGEN_USE_THREADS\n #include \"Eigen/ThreadPool\"\n@@ -66,10 +67,14 @@ class Task final : public SlinkyThreadPool::task {\n   // Runs this task by processing work items in the current thread.\n   TaskState Run();\n \n+  // Returns true if the work queue is empty. It doesn't mean that the task is\n+  // complete, as some threads might still be working on this task.\n+  bool IsEmptyWorkQueue() const;\n+\n   // Returns the number of workers that are currently working on this task.\n   int64_t num_workers() const;\n \n-  bool is_empty_work_queue() const;\n+  // Returns true if the task is done.\n   bool done() const final;\n \n  private:\n@@ -100,13 +105,13 @@ TaskState Task::Run() {\n   Worker w(worker_index, &work_queue_);\n   size_t num_processed_work_items = 0;\n \n-  if (std::optional<size_t> item = w.Pop()) {\n+  if (std::optional<size_t> item = w.Pop(/*notify_work_stealing=*/false)) {\n     SlinkyThreadPool::task_body body = body_;\n \n     do {\n       body(*item);\n       ++num_processed_work_items;\n-    } while ((item = w.Pop()).has_value());\n+    } while ((item = w.Pop(/*notify_work_stealing=*/false)).has_value());\n   }\n \n   // The number of pending work items should never go below zero.\n@@ -128,7 +133,7 @@ int64_t Task::num_workers() const {\n   return worker_index_.load(std::memory_order_relaxed);\n }\n \n-bool Task::is_empty_work_queue() const { return work_queue_.IsEmpty(); }\n+bool Task::IsEmptyWorkQueue() const { return work_queue_.IsEmpty(); }\n \n bool Task::done() const {\n   return pending_work_items_.load(std::memory_order_acquire) == 0;\n@@ -231,7 +236,7 @@ slinky::ref_count<Task> SlinkyThreadPool::Impl::Dequeue() {\n     slinky::ref_count<Task>& task = *i;\n \n     // Task doesn't have any more work items to process.\n-    if (ABSL_PREDICT_FALSE(task->is_empty_work_queue())) {\n+    if (ABSL_PREDICT_FALSE(task->IsEmptyWorkQueue())) {\n       i = tasks_.erase(i);\n       continue;\n     }\n@@ -278,6 +283,7 @@ void SlinkyThreadPool::Impl::WorkOnTasks(const absl::Condition& condition) {\n \n void SlinkyThreadPool::Impl::Await(const absl::Condition& condition) {\n   if (ABSL_PREDICT_FALSE(!condition.Eval())) {\n+    tsl::profiler::TraceMe trace(\"SlinkyThreadPool::Await\");\n     absl::MutexLock lock(waiter_mutex_);\n     waiter_mutex_.Await(condition);\n   }\n@@ -303,7 +309,7 @@ void SlinkyThreadPool::Impl::ScheduleWorkers(int64_t num_workers,\n   if (ABSL_PREDICT_TRUE(num_workers > 0 && CanScheduleWorkers())) {\n     slinky::ref_count<ScheduleState> state(\n         new ScheduleState(num_workers - 1, std::move(task), {this}));\n-    threadpool_->Schedule([state = state.take()]() {\n+    threadpool_->Schedule([state = state.take()] {\n       ScheduleWorkers</*release_impl_ref=*/false>(state);\n     });\n   }\n@@ -321,8 +327,7 @@ void SlinkyThreadPool::Impl::ScheduleWorkers(ScheduleState* context) {\n \n   for (size_t i = 0; i < kNumRecursiveWorkers; ++i) {\n     bool schedule_worker =\n-        state->impl->CanScheduleWorkers() &&\n-        !state->task->is_empty_work_queue() &&\n+        state->impl->CanScheduleWorkers() && !state->task->IsEmptyWorkQueue() &&\n         state->remaining_workers.fetch_sub(1, std::memory_order_relaxed) > 0;\n \n     if (ABSL_PREDICT_TRUE(!schedule_worker)) {\n@@ -333,7 +338,7 @@ void SlinkyThreadPool::Impl::ScheduleWorkers(ScheduleState* context) {\n     // reference count to track the number of active workers.\n     state->impl->add_ref();\n     state->impl->threadpool_->Schedule(\n-        [state = slinky::ref_count<ScheduleState>(state).take()]() {\n+        [state = slinky::ref_count<ScheduleState>(state).take()] {\n           SlinkyThreadPool::Impl::ScheduleWorkers</*release_impl_ref=*/true>(\n               state);\n         });"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 23,
        "deletions": 12
    }
}