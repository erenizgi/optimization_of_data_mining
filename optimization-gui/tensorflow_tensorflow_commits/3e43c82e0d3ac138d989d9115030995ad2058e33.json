{
    "author": "ezhulenev",
    "message": "[xla:cpu] Refactor convolution_lib into reusable functions\n\nPiperOrigin-RevId: 832764749",
    "sha": "3e43c82e0d3ac138d989d9115030995ad2058e33",
    "files": [
        {
            "sha": "441d9bbaba561545c7752e6a1bd728ec01cb9687",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -22,8 +22,8 @@ package_group(\n filegroup(\n     name = \"runtime_srcs\",\n     srcs = [\n-        \"convolution_thunk_f16.cc\",\n-        \"convolution_thunk_f32.cc\",\n+        \"convolution_lib_f16.cc\",\n+        \"convolution_lib_f32.cc\",\n         \"rng_state_lib.cc\",\n         \"sort_lib.cc\",\n     ],\n@@ -33,7 +33,7 @@ filegroup(\n filegroup(\n     name = \"runtime_hdrs\",\n     srcs = [\n-        \"convolution_thunk_internal.h\",\n+        \"convolution_lib.h\",\n         \"kernel_c_api.h\",\n         \"rng_state_lib.h\",\n         \"sort_lib.h\",\n@@ -393,20 +393,22 @@ cc_library(\n )\n \n cc_library(\n-    name = \"convolution_thunk_internal\",\n+    name = \"convolution_lib\",\n     srcs = [\n-        \"convolution_thunk_f16.cc\",\n-        \"convolution_thunk_f32.cc\",\n+        \"convolution_lib_f16.cc\",\n+        \"convolution_lib_f32.cc\",\n     ],\n-    hdrs = [\"convolution_thunk_internal.h\"],\n+    hdrs = [\"convolution_lib.h\"],\n     copts = runtime_copts(),\n     visibility = internal_visibility([\":friends\"]),\n     deps = [\n-        \":work_queue\",\n+        \"//xla/backends/cpu/runtime:work_queue\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/framework/contraction:eigen_contraction_kernel\",\n         \"//xla/tsl/framework/convolution:eigen_helpers\",\n-        \"//xla/tsl/platform:logging\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@eigen_archive//:eigen3\",\n     ],\n )\n@@ -415,10 +417,9 @@ cc_library(\n     name = \"convolution_thunk\",\n     srcs = [\"convolution_thunk.cc\"],\n     hdrs = [\"convolution_thunk.h\"],\n-    copts = runtime_copts(),\n     deps = [\n         \":convolution_dims\",\n-        \":convolution_thunk_internal\",\n+        \":convolution_lib\",\n         \":thunk\",\n         \"//xla:executable_run_options\",\n         \"//xla:shape_util\","
        },
        {
            "sha": "cc42cfd2c079cd339664af87f3b8fe75b71e811e",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_lib.h",
            "status": "renamed",
            "additions": 251,
            "deletions": 244,
            "changes": 495,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib.h?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -13,28 +13,69 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#ifndef XLA_BACKENDS_CPU_RUNTIME_CONVOLUTION_THUNK_INTERNAL_H_\n-#define XLA_BACKENDS_CPU_RUNTIME_CONVOLUTION_THUNK_INTERNAL_H_\n+#ifndef XLA_BACKENDS_CPU_RUNTIME_CONVOLUTION_LIB_H_\n+#define XLA_BACKENDS_CPU_RUNTIME_CONVOLUTION_LIB_H_\n \n #include <algorithm>\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <utility>\n \n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n #include \"xla/backends/cpu/runtime/work_queue.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/chain.h\"\n #include \"xla/tsl/framework/convolution/eigen_spatial_convolutions.h\"  // IWYU pragma: keep\n-#include \"xla/tsl/platform/logging.h\"\n \n #define EIGEN_USE_THREADS\n #include \"Eigen/Core\"\n #include \"unsupported/Eigen/CXX11/Tensor\"\n \n namespace xla::cpu::internal {\n \n-constexpr auto kMaxConvMatrixSize = static_cast<size_t>(8) << 30;  // 8 GiB\n+// Done callback is called when the Convolution computation is complete.\n+using DoneCallback = absl::AnyInvocable<void()>;\n+\n+template <typename ScalarType>\n+void EigenConv2D(const Eigen::ThreadPoolDevice* device, ScalarType* out,\n+                 const ScalarType* lhs, const ScalarType* rhs,\n+                 Eigen::Index input_batch, Eigen::Index input_x,\n+                 Eigen::Index input_y, Eigen::Index input_channels,\n+                 Eigen::Index kernel_x, Eigen::Index kernel_y,\n+                 Eigen::Index kernel_channels, Eigen::Index kernel_filters,\n+                 Eigen::Index output_x, Eigen::Index output_y,\n+                 Eigen::Index x_stride, Eigen::Index y_stride,\n+                 Eigen::Index padding_x_before, Eigen::Index padding_x_after,\n+                 Eigen::Index padding_y_before, Eigen::Index padding_y_after,\n+                 Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,\n+                 Eigen::Index rhs_x_dilation, Eigen::Index rhs_y_dilation,\n+                 Eigen::Index feature_group_count, DoneCallback done);\n+\n+template <typename ScalarType>\n+void EigenConv3D(const Eigen::ThreadPoolDevice* device, ScalarType* out,\n+                 const ScalarType* lhs, const ScalarType* rhs,\n+                 Eigen::Index input_batch, Eigen::Index input_x,\n+                 Eigen::Index input_y, Eigen::Index input_z,\n+                 Eigen::Index input_channels, Eigen::Index kernel_x,\n+                 Eigen::Index kernel_y, Eigen::Index kernel_z,\n+                 Eigen::Index kernel_channels, Eigen::Index kernel_filters,\n+                 Eigen::Index output_x, Eigen::Index output_y,\n+                 Eigen::Index output_z, Eigen::Index x_stride,\n+                 Eigen::Index y_stride, Eigen::Index z_stride,\n+                 Eigen::Index padding_x_before, Eigen::Index padding_x_after,\n+                 Eigen::Index padding_y_before, Eigen::Index padding_y_after,\n+                 Eigen::Index padding_z_before, Eigen::Index padding_z_after,\n+                 Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,\n+                 Eigen::Index lhs_z_dilation, Eigen::Index rhs_x_dilation,\n+                 Eigen::Index rhs_y_dilation, Eigen::Index rhs_z_dilation,\n+                 Eigen::Index feature_group_count, DoneCallback done);\n+\n+//===----------------------------------------------------------------------===//\n+// Convolution 2D implementation details.\n+//===----------------------------------------------------------------------===//\n \n // Returns in 'out_data' (assumes to be zero-initialized) image patch in storage\n // order (width, height, depth), constructed from patches in 'conv_matrix',\n@@ -95,23 +136,48 @@ void Pack2DPatches(const T* conv_matrix, const int depth, const int height,\n   }\n }\n \n+template <typename ScalarType>\n+bool CanUseCustomTransposedConv(\n+    Eigen::Index input_batch, Eigen::Index input_x, Eigen::Index input_y,\n+    Eigen::Index kernel_x, Eigen::Index kernel_y, Eigen::Index kernel_filters,\n+    Eigen::Index output_x, Eigen::Index output_y, Eigen::Index x_stride,\n+    Eigen::Index y_stride, Eigen::Index lhs_x_dilation,\n+    Eigen::Index lhs_y_dilation, Eigen::Index rhs_x_dilation,\n+    Eigen::Index rhs_y_dilation, Eigen::Index feature_group_count) {\n+  // Total spatial dimensions.\n+  const int input_image_size = input_x * input_y;\n+  const int kernel_total_size = kernel_x * kernel_y * kernel_filters;\n+\n+  // Don't use custom transposed convolutions with intermediate buffers.\n+  constexpr auto kMaxConvMatrixSize = static_cast<size_t>(8) << 30;  // 8 GiB\n+\n+  // Intermediate buffer (convolution matrix)\n+  const size_t buffer_size = input_batch * input_image_size * kernel_total_size;\n+  if (buffer_size * sizeof(ScalarType) > kMaxConvMatrixSize) {\n+    return false;\n+  }\n+\n+  return (lhs_x_dilation > 1 || lhs_y_dilation > 1) && rhs_x_dilation == 1 &&\n+         rhs_y_dilation == 1 && feature_group_count == 1 && x_stride == 1 &&\n+         y_stride == 1;\n+}\n+\n // This implementation is based on TF algorithm with parallel contraction.\n // TODO(adambanas): There are other variants of this algorithm, 10% performance\n // improvement was observed on 1D case when not using parallel contraction.\n // Explore these alternatives.\n // TODO(adambanas): Add support for feature group count.\n-template <typename EigenDevice, typename ScalarType>\n-bool EigenTransposedConv2D(\n-    const EigenDevice& device, ScalarType* out, ScalarType* lhs,\n-    ScalarType* rhs, Eigen::Index input_batch, Eigen::Index input_x,\n-    Eigen::Index input_y, Eigen::Index input_channels, Eigen::Index kernel_x,\n-    Eigen::Index kernel_y, Eigen::Index kernel_channels,\n+template <typename ScalarType>\n+void EigenTransposedConv2D(\n+    const Eigen::ThreadPoolDevice* device, ScalarType* out,\n+    const ScalarType* lhs, const ScalarType* rhs, Eigen::Index input_batch,\n+    Eigen::Index input_x, Eigen::Index input_y, Eigen::Index input_channels,\n+    Eigen::Index kernel_x, Eigen::Index kernel_y, Eigen::Index kernel_channels,\n     Eigen::Index kernel_filters, Eigen::Index output_x, Eigen::Index output_y,\n     Eigen::Index padding_x_before, Eigen::Index padding_x_after,\n     Eigen::Index padding_y_before, Eigen::Index padding_y_after,\n     Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,\n-    tsl::CountDownAsyncValueRef<tsl::Chain> count_down,\n-    bool use_thunk_runtime) {\n+    DoneCallback done) {\n   // Grouped convolutions are not supported yet.\n   CHECK(kernel_channels == input_channels);\n \n@@ -133,15 +199,7 @@ bool EigenTransposedConv2D(\n \n   // Intermediate buffer (convolution matrix)\n   const size_t buffer_size = input_batch * input_image_size * kernel_total_size;\n-  if (buffer_size * sizeof(ScalarType) > kMaxConvMatrixSize) {\n-    LOG(WARNING)\n-        << \"Falling back to generic convolution implementation, because custom \"\n-           \"transposed convolution algorithm needs too much memory (\"\n-        << buffer_size * sizeof(ScalarType)\n-        << \" bytes, exceeding the threshold of \" << kMaxConvMatrixSize\n-        << \" bytes).\";\n-    return false;\n-  }\n+\n   auto conv_matrix = std::make_unique<ScalarType[]>(buffer_size);\n   ScalarType* conv_matrix_data = conv_matrix.get();\n \n@@ -163,25 +221,15 @@ bool EigenTransposedConv2D(\n   ConstTensorMap2D A(lhs, input_batch * input_image_size, input_channels);\n   ConstTensorMap3D B(rhs, kernel_x * kernel_y, kernel_channels, kernel_filters);\n \n-  // Use concurrent execution if we have a thread pool device.\n-  constexpr bool use_thread_pool =\n-      std::is_same_v<EigenDevice, Eigen::ThreadPoolDevice>;\n-\n-  // For thunk runtime, `count_down` must be provided only if we use a thread\n-  // pool device. This check is not true for classic runtime which does not\n-  // support async execution.\n-  if (use_thunk_runtime) {\n-    CHECK_EQ(use_thread_pool, static_cast<bool>(count_down));  // Crash OK\n-  }\n-\n   const int input_offset = input_image_size * kernel_total_size;\n   const int output_offset = output_image_size * kernel_filters;\n \n   // Pack the calculated patches into the output buffer.\n   // NOTE: The ownership of the conv_matrix is transferred to the lambda without\n   // data copy or reallocation. Thanks to that, conv_matrix_data pointer remains\n   // valid, and that is important because 'C' matrix is referencing it.\n-  auto pack_patches = [=, conv_matrix = std::move(conv_matrix)]() mutable {\n+  auto pack_patches = [=, conv_matrix = std::move(conv_matrix),\n+                       done = std::move(done)]() mutable {\n     // Using local pointers to buffers, because lambda is not mutable.\n     const ScalarType* conv_matrix_data = conv_matrix.get();\n     ScalarType* local_out_data = out_data;\n@@ -197,11 +245,8 @@ bool EigenTransposedConv2D(\n       local_out_data += output_offset;\n     }\n \n-    // If `count_down` is provided, we need to count it down after the work is\n-    // done.\n-    if (count_down) {\n-      count_down.CountDown();\n-    }\n+    // Signal completion of the work once the patches are packed.\n+    done();\n   };\n \n   // Molds the output of the contraction into the shape expected by packing\n@@ -213,44 +258,31 @@ bool EigenTransposedConv2D(\n   post_contract_dims[0] = input_batch * input_image_size;\n   post_contract_dims[1] = kernel_total_size;\n \n-  if (count_down) {\n-    // Schedule the work in the thread pool and return.\n-    C.device(device, std::move(pack_patches)) =\n+  if (device != nullptr) {\n+    C.device(*device, std::move(pack_patches)) =\n         A.contract(B, contract_dims).reshape(post_contract_dims);\n   } else {\n-    // Run synchronously in the current thread.\n-    C.device(device) = A.contract(B, contract_dims).reshape(post_contract_dims);\n+    C = A.contract(B, contract_dims).reshape(post_contract_dims);\n     pack_patches();\n   }\n-  return true;\n-}\n-\n-inline bool CanUseCustomTransposedConv(\n-    Eigen::Index x_stride, Eigen::Index y_stride, Eigen::Index lhs_x_dilation,\n-    Eigen::Index lhs_y_dilation, Eigen::Index rhs_x_dilation,\n-    Eigen::Index rhs_y_dilation, Eigen::Index feature_group_count) {\n-  return (lhs_x_dilation > 1 || lhs_y_dilation > 1) && rhs_x_dilation == 1 &&\n-         rhs_y_dilation == 1 && feature_group_count == 1 && x_stride == 1 &&\n-         y_stride == 1;\n }\n \n // Algorithm that works for all types of 2D convolutions. Even though it works\n // for transposed convolutions, the custom algorithm should be used whenever\n // applicable, because it is faster.\n-template <bool is_grouped, typename EigenDevice, typename ScalarType>\n+template <bool is_grouped, typename ScalarType>\n void EigenGenericConv2D(\n-    const EigenDevice& device, ScalarType* out, ScalarType* lhs,\n-    ScalarType* rhs, Eigen::Index input_batch, Eigen::Index input_x,\n-    Eigen::Index input_y, Eigen::Index input_channels, Eigen::Index kernel_x,\n-    Eigen::Index kernel_y, Eigen::Index kernel_channels,\n+    const Eigen::ThreadPoolDevice* device, ScalarType* out,\n+    const ScalarType* lhs, const ScalarType* rhs, Eigen::Index input_batch,\n+    Eigen::Index input_x, Eigen::Index input_y, Eigen::Index input_channels,\n+    Eigen::Index kernel_x, Eigen::Index kernel_y, Eigen::Index kernel_channels,\n     Eigen::Index kernel_filters, Eigen::Index output_x, Eigen::Index output_y,\n     Eigen::Index x_stride, Eigen::Index y_stride, Eigen::Index padding_x_before,\n     Eigen::Index padding_x_after, Eigen::Index padding_y_before,\n     Eigen::Index padding_y_after, Eigen::Index lhs_x_dilation,\n     Eigen::Index lhs_y_dilation, Eigen::Index rhs_x_dilation,\n     Eigen::Index rhs_y_dilation, Eigen::Index feature_group_count,\n-    tsl::CountDownAsyncValueRef<tsl::Chain> count_down,\n-    bool use_thunk_runtime) {\n+    DoneCallback done) {\n   // For non-grouped convolutions, we can optimize Eigen expressions and avoid\n   // introducing an extra dimension of size `1`.\n   if constexpr (!is_grouped) {\n@@ -348,105 +380,80 @@ void EigenGenericConv2D(\n     return std::make_pair(output, convolved);\n   };\n \n-  // Use concurrent execution if we have a thread pool device.\n-  constexpr bool use_thread_pool =\n-      std::is_same_v<EigenDevice, Eigen::ThreadPoolDevice>;\n-\n-  // For thunk runtime, `count_down` must be provided only if we use a thread\n-  // pool device. This check is not true for classic runtime which does not\n-  // support async execution.\n-  if (use_thunk_runtime) {\n-    CHECK_EQ(use_thread_pool, static_cast<bool>(count_down));  // Crash OK\n-  }\n-\n   // For non-grouped convolutions, we need to execute only one Eigen expression.\n   if constexpr (!is_grouped) {\n     auto [output, convolved] = convolve();\n \n-    if (count_down) {\n-      auto on_done = [count_down]() mutable { count_down.CountDown(); };\n-      output.device(device, std::move(on_done)) = convolved;\n+    if (device != nullptr) {\n+      output.device(*device, std::move(done)) = convolved;\n     } else {\n-      output.device(device) = convolved;\n+      output = convolved;\n+      done();\n     }\n \n     return;\n   }\n \n   // For grouped convolutions, we need to execute multiple Eigen expressions and\n   // we might use thread pool to run them concurrently.\n-  if constexpr (use_thread_pool) {\n+  if (device != nullptr) {\n     // Although we schedule at most one tasks for each thread, individual\n     // convolution might also schedule more tasks into the same thread pool.\n-    auto max_tasks = static_cast<Eigen::Index>(device.numThreads());\n+    auto max_tasks = static_cast<Eigen::Index>(device->numThreads());\n     auto task_size = Eigen::numext::div_ceil(feature_group_count, max_tasks);\n     auto num_tasks = Eigen::numext::div_ceil(feature_group_count, task_size);\n \n-    if (use_thunk_runtime) {\n-      Worker::Parallelize(\n-          device.getPool(), /*num_workers=*/num_tasks, num_tasks,\n-          [=, &device](Eigen::Index task_index) mutable {\n-            Eigen::Index start = task_index * task_size;\n-            Eigen::Index end = std::min(start + task_size, feature_group_count);\n-            for (Eigen::Index i = start; i < end; ++i) {\n-              auto on_done = [count_down]() mutable { count_down.CountDown(); };\n-              auto [output, convolved] = convolve_group(i);\n-              output.device(device, std::move(on_done)) = convolved;\n-            }\n-          });\n-    } else {\n-      tsl::BlockUntilReady(Worker::Parallelize(\n-          device.getPool(), /*num_workers=*/num_tasks, num_tasks,\n-          [=, &device](Eigen::Index task_index) {\n-            Eigen::Index start = task_index * task_size;\n-            Eigen::Index end = std::min(start + task_size, feature_group_count);\n-            for (Eigen::Index i = start; i < end; ++i) {\n-              auto [output, convolved] = convolve_group(i);\n-              output.device(device) = convolved;\n-            }\n-          }));\n-    }\n+    // Signal done callback when all feature groups are done.\n+    tsl::CountDownAsyncValueRef<tsl::Chain> count_down(feature_group_count);\n+    count_down.AsPtr().AndThen([done = std::move(done)]() mutable { done(); });\n+\n+    Worker::Parallelize(\n+        device->getPool(), /*num_workers=*/num_tasks, num_tasks,\n+        [=](Eigen::Index task_index) {\n+          Eigen::Index start = task_index * task_size;\n+          Eigen::Index end = std::min(start + task_size, feature_group_count);\n+          for (Eigen::Index i = start; i < end; ++i) {\n+            auto on_done = [count_down]() mutable { count_down.CountDown(); };\n+            auto [output, convolved] = convolve_group(i);\n+            output.device(*device, std::move(on_done)) = convolved;\n+          }\n+        });\n \n   } else {\n     // Convolve all feature groups sequentially in the caller thread.\n     for (Eigen::Index i = 0; i < feature_group_count; ++i) {\n       auto [output, convolved] = convolve_group(i);\n-      output.device(device) = convolved;\n+      output = convolved;\n     }\n+    done();\n   }\n }\n \n-// TODO(ezhulenev): Make internal implementation a private static method of\n-// ConvolutionThunk (for consistency with DotThunk). Today we keep it as a\n-// free function to use it in the legacy XLA CPU runtime.\n-template <typename EigenDevice, typename ScalarType>\n-void EigenConv2D(const EigenDevice& device, ScalarType* out, ScalarType* lhs,\n-                 ScalarType* rhs, Eigen::Index input_batch,\n-                 Eigen::Index input_x, Eigen::Index input_y,\n-                 Eigen::Index input_channels, Eigen::Index kernel_x,\n-                 Eigen::Index kernel_y, Eigen::Index kernel_channels,\n-                 Eigen::Index kernel_filters, Eigen::Index output_x,\n-                 Eigen::Index output_y, Eigen::Index x_stride,\n-                 Eigen::Index y_stride, Eigen::Index padding_x_before,\n-                 Eigen::Index padding_x_after, Eigen::Index padding_y_before,\n-                 Eigen::Index padding_y_after, Eigen::Index lhs_x_dilation,\n-                 Eigen::Index lhs_y_dilation, Eigen::Index rhs_x_dilation,\n-                 Eigen::Index rhs_y_dilation, Eigen::Index feature_group_count,\n-                 tsl::CountDownAsyncValueRef<tsl::Chain> count_down,\n-                 bool use_thunk_runtime) {\n-  DCHECK(!count_down || (feature_group_count == count_down.count()));\n-\n-  if (CanUseCustomTransposedConv(x_stride, y_stride, lhs_x_dilation,\n-                                 lhs_y_dilation, rhs_x_dilation, rhs_y_dilation,\n-                                 feature_group_count)) {\n-    if (EigenTransposedConv2D(device, out, lhs, rhs, input_batch, input_x,\n-                              input_y, input_channels, kernel_x, kernel_y,\n-                              kernel_channels, kernel_filters, output_x,\n-                              output_y, padding_x_before, padding_x_after,\n-                              padding_y_before, padding_y_after, lhs_x_dilation,\n-                              lhs_y_dilation, count_down, use_thunk_runtime)) {\n-      return;\n-    }\n+template <typename ScalarType>\n+void EigenConv2D(const Eigen::ThreadPoolDevice* device, ScalarType* out,\n+                 const ScalarType* lhs, const ScalarType* rhs,\n+                 Eigen::Index input_batch, Eigen::Index input_x,\n+                 Eigen::Index input_y, Eigen::Index input_channels,\n+                 Eigen::Index kernel_x, Eigen::Index kernel_y,\n+                 Eigen::Index kernel_channels, Eigen::Index kernel_filters,\n+                 Eigen::Index output_x, Eigen::Index output_y,\n+                 Eigen::Index x_stride, Eigen::Index y_stride,\n+                 Eigen::Index padding_x_before, Eigen::Index padding_x_after,\n+                 Eigen::Index padding_y_before, Eigen::Index padding_y_after,\n+                 Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,\n+                 Eigen::Index rhs_x_dilation, Eigen::Index rhs_y_dilation,\n+                 Eigen::Index feature_group_count, DoneCallback done) {\n+  if (CanUseCustomTransposedConv<ScalarType>(\n+          input_batch, input_x, input_y, kernel_x, kernel_y, kernel_filters,\n+          output_x, output_y, x_stride, y_stride, lhs_x_dilation,\n+          lhs_y_dilation, rhs_x_dilation, rhs_y_dilation,\n+          feature_group_count)) {\n+    EigenTransposedConv2D(device, out, lhs, rhs, input_batch, input_x, input_y,\n+                          input_channels, kernel_x, kernel_y, kernel_channels,\n+                          kernel_filters, output_x, output_y, padding_x_before,\n+                          padding_x_after, padding_y_before, padding_y_after,\n+                          lhs_x_dilation, lhs_y_dilation, std::move(done));\n+    return;\n   }\n \n   if (feature_group_count == 1) {\n@@ -455,40 +462,40 @@ void EigenConv2D(const EigenDevice& device, ScalarType* out, ScalarType* lhs,\n         kernel_x, kernel_y, kernel_channels, kernel_filters, output_x, output_y,\n         x_stride, y_stride, padding_x_before, padding_x_after, padding_y_before,\n         padding_y_after, lhs_x_dilation, lhs_y_dilation, rhs_x_dilation,\n-        rhs_y_dilation, feature_group_count, std::move(count_down),\n-        use_thunk_runtime);\n+        rhs_y_dilation, feature_group_count, std::move(done));\n \n   } else {\n     EigenGenericConv2D</*is_grouped=*/true>(\n         device, out, lhs, rhs, input_batch, input_x, input_y, input_channels,\n         kernel_x, kernel_y, kernel_channels, kernel_filters, output_x, output_y,\n         x_stride, y_stride, padding_x_before, padding_x_after, padding_y_before,\n         padding_y_after, lhs_x_dilation, lhs_y_dilation, rhs_x_dilation,\n-        rhs_y_dilation, feature_group_count, std::move(count_down),\n-        use_thunk_runtime);\n+        rhs_y_dilation, feature_group_count, std::move(done));\n   }\n }\n \n-template <typename EigenDevice, typename ScalarType>\n-void EigenConv3D(const EigenDevice& device, ScalarType* out, ScalarType* lhs,\n-                 ScalarType* rhs, Eigen::Index input_batch,\n-                 Eigen::Index input_x, Eigen::Index input_y,\n-                 Eigen::Index input_z, Eigen::Index input_channels,\n-                 Eigen::Index kernel_x, Eigen::Index kernel_y,\n-                 Eigen::Index kernel_z, Eigen::Index kernel_channels,\n-                 Eigen::Index kernel_filters, Eigen::Index output_x,\n-                 Eigen::Index output_y, Eigen::Index output_z,\n-                 Eigen::Index x_stride, Eigen::Index y_stride,\n-                 Eigen::Index z_stride, Eigen::Index padding_x_before,\n-                 Eigen::Index padding_x_after, Eigen::Index padding_y_before,\n-                 Eigen::Index padding_y_after, Eigen::Index padding_z_before,\n-                 Eigen::Index padding_z_after, Eigen::Index lhs_x_dilation,\n-                 Eigen::Index lhs_y_dilation, Eigen::Index lhs_z_dilation,\n-                 Eigen::Index rhs_x_dilation, Eigen::Index rhs_y_dilation,\n-                 Eigen::Index rhs_z_dilation, Eigen::Index feature_group_count,\n-                 tsl::CountDownAsyncValueRef<tsl::Chain> count_down) {\n-  DCHECK(!count_down || (feature_group_count == count_down.count()));\n+//===----------------------------------------------------------------------===//\n+// Convolution 3D implementation details.\n+//===----------------------------------------------------------------------===//\n \n+template <typename ScalarType>\n+void EigenConv3D(const Eigen::ThreadPoolDevice* device, ScalarType* out,\n+                 const ScalarType* lhs, const ScalarType* rhs,\n+                 Eigen::Index input_batch, Eigen::Index input_x,\n+                 Eigen::Index input_y, Eigen::Index input_z,\n+                 Eigen::Index input_channels, Eigen::Index kernel_x,\n+                 Eigen::Index kernel_y, Eigen::Index kernel_z,\n+                 Eigen::Index kernel_channels, Eigen::Index kernel_filters,\n+                 Eigen::Index output_x, Eigen::Index output_y,\n+                 Eigen::Index output_z, Eigen::Index x_stride,\n+                 Eigen::Index y_stride, Eigen::Index z_stride,\n+                 Eigen::Index padding_x_before, Eigen::Index padding_x_after,\n+                 Eigen::Index padding_y_before, Eigen::Index padding_y_after,\n+                 Eigen::Index padding_z_before, Eigen::Index padding_z_after,\n+                 Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,\n+                 Eigen::Index lhs_z_dilation, Eigen::Index rhs_x_dilation,\n+                 Eigen::Index rhs_y_dilation, Eigen::Index rhs_z_dilation,\n+                 Eigen::Index feature_group_count, DoneCallback done) {\n   using ConstTType =\n       Eigen::TensorMap<Eigen::Tensor<const ScalarType, 5, Eigen::RowMajor>,\n                        Eigen::Aligned>;\n@@ -542,6 +549,10 @@ void EigenConv3D(const EigenDevice& device, ScalarType* out, ScalarType* lhs,\n   kernel_dims[1] = feature_group_count;\n   kernel_dims[2] = kernel_filters / feature_group_count;\n \n+  // Signal done callback when all feature groups are done.\n+  tsl::CountDownAsyncValueRef<tsl::Chain> count_down(feature_group_count);\n+  count_down.AsPtr().AndThen([done = std::move(done)]() mutable { done(); });\n+\n   for (Eigen::Index i = 0; i < feature_group_count; ++i) {\n     // The dimension order must be flipped when passed to Eigen.\n     auto input_chip = input.reshape(input_reshaped_dims).chip(i, 4);\n@@ -560,99 +571,95 @@ void EigenConv3D(const EigenDevice& device, ScalarType* out, ScalarType* lhs,\n             .reshape(post_contract_dims);\n \n     auto output_reshaped = output.reshape(output_reshaped_dims).chip(i, 4);\n-    if (count_down) {\n+\n+    if (device != nullptr) {\n       auto on_done = [count_down]() mutable { count_down.CountDown(); };\n-      output_reshaped.device(device, std::move(on_done)) = convolved;\n+      output_reshaped.device(*device, std::move(on_done)) = convolved;\n     } else {\n-      output_reshaped.device(device) = convolved;\n+      output_reshaped = convolved;\n+      count_down.CountDown();\n     }\n   }\n }\n \n-// Extern Conv2D template for all supported devices and data types.\n-#define CONV2D_EXTERN_TEMPLATE(DEVICE, SCALAR_TYPE)                        \\\n-  extern template void EigenConv2D<DEVICE, SCALAR_TYPE>(                   \\\n-      const DEVICE& device, SCALAR_TYPE* out, SCALAR_TYPE* lhs,            \\\n-      SCALAR_TYPE* rhs, Eigen::Index input_batch, Eigen::Index input_x,    \\\n-      Eigen::Index input_y, Eigen::Index input_channels,                   \\\n-      Eigen::Index kernel_x, Eigen::Index kernel_y,                        \\\n-      Eigen::Index kernel_channels, Eigen::Index kernel_filters,           \\\n-      Eigen::Index output_x, Eigen::Index output_y, Eigen::Index x_stride, \\\n-      Eigen::Index y_stride, Eigen::Index padding_x_before,                \\\n-      Eigen::Index padding_x_after, Eigen::Index padding_y_before,         \\\n-      Eigen::Index padding_y_after, Eigen::Index lhs_x_dilation,           \\\n-      Eigen::Index lhs_y_dilation, Eigen::Index rhs_x_dilation,            \\\n-      Eigen::Index rhs_y_dilation, Eigen::Index feature_group_count,       \\\n-      tsl::CountDownAsyncValueRef<tsl::Chain> count_down,                  \\\n-      bool use_thunk_runtime)\n-\n-CONV2D_EXTERN_TEMPLATE(Eigen::DefaultDevice, Eigen::half);\n-CONV2D_EXTERN_TEMPLATE(Eigen::DefaultDevice, float);\n-CONV2D_EXTERN_TEMPLATE(Eigen::ThreadPoolDevice, Eigen::half);\n-CONV2D_EXTERN_TEMPLATE(Eigen::ThreadPoolDevice, float);\n-\n-#undef CONV2D_EXTERN_TEMPLATE\n-\n-// Extern Conv3D template for all supported devices and data types.\n-#define CONV3D_EXTERN_TEMPLATE(DEVICE, SCALAR_TYPE)                            \\\n-  extern template void EigenConv3D<DEVICE, SCALAR_TYPE>(                       \\\n-      const DEVICE& device, SCALAR_TYPE* out, SCALAR_TYPE* lhs,                \\\n-      SCALAR_TYPE* rhs, Eigen::Index input_batch, Eigen::Index input_x,        \\\n-      Eigen::Index input_y, Eigen::Index input_z, Eigen::Index input_channels, \\\n-      Eigen::Index kernel_x, Eigen::Index kernel_y, Eigen::Index kernel_z,     \\\n-      Eigen::Index kernel_channels, Eigen::Index kernel_filters,               \\\n-      Eigen::Index output_x, Eigen::Index output_y, Eigen::Index output_z,     \\\n-      Eigen::Index x_stride, Eigen::Index y_stride, Eigen::Index z_stride,     \\\n-      Eigen::Index padding_x_before, Eigen::Index padding_x_after,             \\\n-      Eigen::Index padding_y_before, Eigen::Index padding_y_after,             \\\n-      Eigen::Index padding_z_before, Eigen::Index padding_z_after,             \\\n-      Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,                \\\n-      Eigen::Index lhs_z_dilation, Eigen::Index rhs_x_dilation,                \\\n-      Eigen::Index rhs_y_dilation, Eigen::Index rhs_z_dilation,                \\\n-      Eigen::Index feature_group_count,                                        \\\n-      tsl::CountDownAsyncValueRef<tsl::Chain> count_down)\n-\n-CONV3D_EXTERN_TEMPLATE(Eigen::DefaultDevice, Eigen::half);\n-CONV3D_EXTERN_TEMPLATE(Eigen::DefaultDevice, float);\n-CONV3D_EXTERN_TEMPLATE(Eigen::ThreadPoolDevice, Eigen::half);\n-CONV3D_EXTERN_TEMPLATE(Eigen::ThreadPoolDevice, float);\n-\n-#undef CONV3D_EXTERN_TEMPLATE\n-\n }  // namespace xla::cpu::internal\n \n-#define CONV2D_INSTANTIATE_TEMPLATE(DEVICE, SCALAR_TYPE)                   \\\n-  template void xla::cpu::internal::EigenConv2D<DEVICE, SCALAR_TYPE>(      \\\n-      const DEVICE& device, SCALAR_TYPE* out, SCALAR_TYPE* lhs,            \\\n-      SCALAR_TYPE* rhs, Eigen::Index input_batch, Eigen::Index input_x,    \\\n-      Eigen::Index input_y, Eigen::Index input_channels,                   \\\n-      Eigen::Index kernel_x, Eigen::Index kernel_y,                        \\\n-      Eigen::Index kernel_channels, Eigen::Index kernel_filters,           \\\n-      Eigen::Index output_x, Eigen::Index output_y, Eigen::Index x_stride, \\\n-      Eigen::Index y_stride, Eigen::Index padding_x_before,                \\\n-      Eigen::Index padding_x_after, Eigen::Index padding_y_before,         \\\n-      Eigen::Index padding_y_after, Eigen::Index lhs_x_dilation,           \\\n-      Eigen::Index lhs_y_dilation, Eigen::Index rhs_x_dilation,            \\\n-      Eigen::Index rhs_y_dilation, Eigen::Index feature_group_count,       \\\n-      tsl::CountDownAsyncValueRef<tsl::Chain> count_down,                  \\\n-      bool use_thunk_runtime)\n-\n-#define CONV3D_INSTANTIATE_TEMPLATE(DEVICE, SCALAR_TYPE)                       \\\n-  template void xla::cpu::internal::EigenConv3D<DEVICE, SCALAR_TYPE>(          \\\n-      const DEVICE& device, SCALAR_TYPE* out, SCALAR_TYPE* lhs,                \\\n-      SCALAR_TYPE* rhs, Eigen::Index input_batch, Eigen::Index input_x,        \\\n-      Eigen::Index input_y, Eigen::Index input_z, Eigen::Index input_channels, \\\n-      Eigen::Index kernel_x, Eigen::Index kernel_y, Eigen::Index kernel_z,     \\\n-      Eigen::Index kernel_channels, Eigen::Index kernel_filters,               \\\n-      Eigen::Index output_x, Eigen::Index output_y, Eigen::Index output_z,     \\\n-      Eigen::Index x_stride, Eigen::Index y_stride, Eigen::Index z_stride,     \\\n-      Eigen::Index padding_x_before, Eigen::Index padding_x_after,             \\\n-      Eigen::Index padding_y_before, Eigen::Index padding_y_after,             \\\n-      Eigen::Index padding_z_before, Eigen::Index padding_z_after,             \\\n-      Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,                \\\n-      Eigen::Index lhs_z_dilation, Eigen::Index rhs_x_dilation,                \\\n-      Eigen::Index rhs_y_dilation, Eigen::Index rhs_z_dilation,                \\\n-      Eigen::Index feature_group_count,                                        \\\n-      tsl::CountDownAsyncValueRef<tsl::Chain> count_down)\n-\n-#endif  // XLA_BACKENDS_CPU_RUNTIME_CONVOLUTION_THUNK_INTERNAL_H_\n+// Define Conv2D template for all supported data types.\n+#define XLA_CPU_DECLARE_CONV2D(SCALAR_TYPE)                                 \\\n+  extern template void xla::cpu::internal::EigenConv2D<SCALAR_TYPE>(        \\\n+      const Eigen::ThreadPoolDevice* device, SCALAR_TYPE* out,              \\\n+      const SCALAR_TYPE* lhs, const SCALAR_TYPE* rhs,                       \\\n+      Eigen::Index input_batch, Eigen::Index input_x, Eigen::Index input_y, \\\n+      Eigen::Index input_channels, Eigen::Index kernel_x,                   \\\n+      Eigen::Index kernel_y, Eigen::Index kernel_channels,                  \\\n+      Eigen::Index kernel_filters, Eigen::Index output_x,                   \\\n+      Eigen::Index output_y, Eigen::Index x_stride, Eigen::Index y_stride,  \\\n+      Eigen::Index padding_x_before, Eigen::Index padding_x_after,          \\\n+      Eigen::Index padding_y_before, Eigen::Index padding_y_after,          \\\n+      Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,             \\\n+      Eigen::Index rhs_x_dilation, Eigen::Index rhs_y_dilation,             \\\n+      Eigen::Index feature_group_count, DoneCallback done)\n+\n+XLA_CPU_DECLARE_CONV2D(Eigen::half);\n+XLA_CPU_DECLARE_CONV2D(float);\n+\n+#undef XLA_CPU_DECLARE_CONV2D\n+\n+// Define Conv3D template for all supported data types.\n+#define XLA_CPU_DECLARE_CONV3D(SCALAR_TYPE)                                 \\\n+  extern template void xla::cpu::internal::EigenConv3D<SCALAR_TYPE>(        \\\n+      const Eigen::ThreadPoolDevice* device, SCALAR_TYPE* out,              \\\n+      const SCALAR_TYPE* lhs, const SCALAR_TYPE* rhs,                       \\\n+      Eigen::Index input_batch, Eigen::Index input_x, Eigen::Index input_y, \\\n+      Eigen::Index input_z, Eigen::Index input_channels,                    \\\n+      Eigen::Index kernel_x, Eigen::Index kernel_y, Eigen::Index kernel_z,  \\\n+      Eigen::Index kernel_channels, Eigen::Index kernel_filters,            \\\n+      Eigen::Index output_x, Eigen::Index output_y, Eigen::Index output_z,  \\\n+      Eigen::Index x_stride, Eigen::Index y_stride, Eigen::Index z_stride,  \\\n+      Eigen::Index padding_x_before, Eigen::Index padding_x_after,          \\\n+      Eigen::Index padding_y_before, Eigen::Index padding_y_after,          \\\n+      Eigen::Index padding_z_before, Eigen::Index padding_z_after,          \\\n+      Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,             \\\n+      Eigen::Index lhs_z_dilation, Eigen::Index rhs_x_dilation,             \\\n+      Eigen::Index rhs_y_dilation, Eigen::Index rhs_z_dilation,             \\\n+      Eigen::Index feature_group_count, DoneCallback done)\n+\n+XLA_CPU_DECLARE_CONV3D(Eigen::half);\n+XLA_CPU_DECLARE_CONV3D(float);\n+\n+#undef XLA_CPU_DECLARE_CONV3D\n+\n+#define XLA_CPU_DEFINE_CONV2D(SCALAR_TYPE)                                  \\\n+  template void xla::cpu::internal::EigenConv2D<SCALAR_TYPE>(               \\\n+      const Eigen::ThreadPoolDevice* device, SCALAR_TYPE* out,              \\\n+      const SCALAR_TYPE* lhs, const SCALAR_TYPE* rhs,                       \\\n+      Eigen::Index input_batch, Eigen::Index input_x, Eigen::Index input_y, \\\n+      Eigen::Index input_channels, Eigen::Index kernel_x,                   \\\n+      Eigen::Index kernel_y, Eigen::Index kernel_channels,                  \\\n+      Eigen::Index kernel_filters, Eigen::Index output_x,                   \\\n+      Eigen::Index output_y, Eigen::Index x_stride, Eigen::Index y_stride,  \\\n+      Eigen::Index padding_x_before, Eigen::Index padding_x_after,          \\\n+      Eigen::Index padding_y_before, Eigen::Index padding_y_after,          \\\n+      Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,             \\\n+      Eigen::Index rhs_x_dilation, Eigen::Index rhs_y_dilation,             \\\n+      Eigen::Index feature_group_count, DoneCallback done)\n+\n+#define XLA_CPU_DEFINE_CONV3D(SCALAR_TYPE)                                  \\\n+  template void xla::cpu::internal::EigenConv3D<SCALAR_TYPE>(               \\\n+      const Eigen::ThreadPoolDevice* device, SCALAR_TYPE* out,              \\\n+      const SCALAR_TYPE* lhs, const SCALAR_TYPE* rhs,                       \\\n+      Eigen::Index input_batch, Eigen::Index input_x, Eigen::Index input_y, \\\n+      Eigen::Index input_z, Eigen::Index input_channels,                    \\\n+      Eigen::Index kernel_x, Eigen::Index kernel_y, Eigen::Index kernel_z,  \\\n+      Eigen::Index kernel_channels, Eigen::Index kernel_filters,            \\\n+      Eigen::Index output_x, Eigen::Index output_y, Eigen::Index output_z,  \\\n+      Eigen::Index x_stride, Eigen::Index y_stride, Eigen::Index z_stride,  \\\n+      Eigen::Index padding_x_before, Eigen::Index padding_x_after,          \\\n+      Eigen::Index padding_y_before, Eigen::Index padding_y_after,          \\\n+      Eigen::Index padding_z_before, Eigen::Index padding_z_after,          \\\n+      Eigen::Index lhs_x_dilation, Eigen::Index lhs_y_dilation,             \\\n+      Eigen::Index lhs_z_dilation, Eigen::Index rhs_x_dilation,             \\\n+      Eigen::Index rhs_y_dilation, Eigen::Index rhs_z_dilation,             \\\n+      Eigen::Index feature_group_count, DoneCallback done)\n+\n+#endif  // XLA_BACKENDS_CPU_RUNTIME_CONVOLUTION_LIB_H_",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk_internal.h"
        },
        {
            "sha": "e374aefae767483e2a04195561f307f742571de7",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_lib_f16.cc",
            "status": "renamed",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib_f16.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib_f16.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib_f16.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -13,10 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/convolution_thunk_internal.h\"\n+#include \"xla/backends/cpu/runtime/convolution_lib.h\"\n \n-CONV2D_INSTANTIATE_TEMPLATE(Eigen::DefaultDevice, Eigen::half);\n-CONV2D_INSTANTIATE_TEMPLATE(Eigen::ThreadPoolDevice, Eigen::half);\n-\n-CONV3D_INSTANTIATE_TEMPLATE(Eigen::DefaultDevice, Eigen::half);\n-CONV3D_INSTANTIATE_TEMPLATE(Eigen::ThreadPoolDevice, Eigen::half);\n+XLA_CPU_DEFINE_CONV2D(Eigen::half);\n+XLA_CPU_DEFINE_CONV3D(Eigen::half);",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk_f16.cc"
        },
        {
            "sha": "4caddbb902cfcdff243049e4d9d2c4288da20916",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_lib_f32.cc",
            "status": "renamed",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib_f32.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib_f32.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_lib_f32.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -13,14 +13,11 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/cpu/runtime/convolution_thunk_internal.h\"\n+#include \"xla/backends/cpu/runtime/convolution_lib.h\"\n \n #if defined(TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL)\n #include \"xla/tsl/framework/contraction/eigen_contraction_kernel.h\"  // IWYU pragma: keep\n #endif\n \n-CONV2D_INSTANTIATE_TEMPLATE(Eigen::DefaultDevice, float);\n-CONV2D_INSTANTIATE_TEMPLATE(Eigen::ThreadPoolDevice, float);\n-\n-CONV3D_INSTANTIATE_TEMPLATE(Eigen::DefaultDevice, float);\n-CONV3D_INSTANTIATE_TEMPLATE(Eigen::ThreadPoolDevice, float);\n+XLA_CPU_DEFINE_CONV2D(float);\n+XLA_CPU_DEFINE_CONV3D(float);",
            "previous_filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk_f32.cc"
        },
        {
            "sha": "986676020c9946ad2c02070b49807a9172447e4e",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 65,
            "changes": 106,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -12,6 +12,7 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n+\n #include \"xla/backends/cpu/runtime/convolution_thunk.h\"\n \n #include <cstdint>\n@@ -23,9 +24,10 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/cpu/runtime/convolution_dims.h\"\n-#include \"xla/backends/cpu/runtime/convolution_thunk_internal.h\"\n+#include \"xla/backends/cpu/runtime/convolution_lib.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -101,12 +103,6 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> ConvolutionThunk::Execute(\n                                 convolution_slices_.output_buffer.ToString(),\n                                 output_data.opaque());\n \n-  if (options_.multi_threaded && params.intra_op_threadpool == nullptr) {\n-    return Internal(\n-        \"Intra-op threadpool must be provided for ConvolutionThunk in \"\n-        \"multi-threaded mode.\");\n-  }\n-\n   // Eigen convolution\n   if (convolution_canonical_dims_.convolution_rank() == 2) {\n     return HandleEigen2DConvolution(params, input_data, kernel_data,\n@@ -122,13 +118,14 @@ ConvolutionThunk::HandleEigen2DConvolution(const ExecuteParams& params,\n                                            se::DeviceMemoryBase input,\n                                            se::DeviceMemoryBase kernel,\n                                            se::DeviceMemoryBase output) {\n-  auto dispatch = [&](auto type_tag, const auto& eigen_device,\n-                      tsl::CountDownAsyncValueRef<ExecuteEvent> count_down) {\n-    using scalar_type = decltype(type_tag);\n+  auto dispatch = [&](auto type_tag) {\n+    auto execute_event = tsl::MakeConstructedAsyncValueRef<ExecuteEvent>();\n+\n+    using ScalarType = decltype(type_tag);\n     internal::EigenConv2D(\n-        eigen_device, static_cast<scalar_type*>(output.opaque()),\n-        static_cast<scalar_type*>(input.opaque()),\n-        static_cast<scalar_type*>(kernel.opaque()),\n+        params.intra_op_threadpool, static_cast<ScalarType*>(output.opaque()),\n+        static_cast<const ScalarType*>(input.opaque()),\n+        static_cast<const ScalarType*>(kernel.opaque()),\n         convolution_canonical_dims_.input_batch,\n         convolution_canonical_dims_.input_dims.x,\n         convolution_canonical_dims_.input_dims.y,\n@@ -149,49 +146,38 @@ ConvolutionThunk::HandleEigen2DConvolution(const ExecuteParams& params,\n         convolution_canonical_dims_.base_dilation.y,\n         convolution_canonical_dims_.window_dilation.x,\n         convolution_canonical_dims_.window_dilation.y,\n-        convolution_canonical_dims_.feature_group_count, std::move(count_down),\n-        /*use_thunk_runtime=*/true);\n+        convolution_canonical_dims_.feature_group_count,\n+        [execute_event] { execute_event.SetStateConcrete(); });\n+\n+    return execute_event;\n   };\n \n   PrimitiveType input_type = convolution_slices_.input_shape.element_type();\n \n-  // Execute convolution in the intra-op threadpool.\n-  if (options_.multi_threaded) {\n-    tsl::CountDownAsyncValueRef<ExecuteEvent> count_down(\n-        convolution_canonical_dims_.feature_group_count);\n-    auto execute_event = count_down.AsRef();\n-\n-    if (input_type == PrimitiveType::F16) {\n-      dispatch(Eigen::half{}, *params.intra_op_threadpool,\n-               std::move(count_down));\n-    } else {\n-      dispatch(float{}, *params.intra_op_threadpool, std::move(count_down));\n-    }\n-    return execute_event;\n-  }\n-\n-  // Execute convolution in the caller thread.\n-  if (input_type == PrimitiveType::F16) {\n-    dispatch(Eigen::half{}, Eigen::DefaultDevice(), /*count_down=*/{});\n-  } else {\n-    dispatch(float{}, Eigen::DefaultDevice(), /*count_down=*/{});\n+  switch (input_type) {\n+    case PrimitiveType::F16:\n+      return dispatch(Eigen::half{});\n+    case PrimitiveType::F32:\n+      return dispatch(float{});\n+    default:\n+      return Internal(\"Unsupported Conv2D input type: %s\",\n+                      primitive_util::LowercasePrimitiveTypeName(input_type));\n   }\n-\n-  return OkExecuteEvent();\n }\n \n tsl::AsyncValueRef<Thunk::ExecuteEvent>\n ConvolutionThunk::HandleEigen3DConvolution(const ExecuteParams& params,\n                                            se::DeviceMemoryBase input,\n                                            se::DeviceMemoryBase kernel,\n                                            se::DeviceMemoryBase output) {\n-  auto dispatch = [&](auto type_tag, const auto& eigen_device,\n-                      tsl::CountDownAsyncValueRef<ExecuteEvent> count_down) {\n-    using scalar_type = decltype(type_tag);\n+  auto dispatch = [&](auto type_tag) {\n+    auto execute_event = tsl::MakeConstructedAsyncValueRef<ExecuteEvent>();\n+\n+    using ScalarType = decltype(type_tag);\n     internal::EigenConv3D(\n-        eigen_device, static_cast<scalar_type*>(output.opaque()),\n-        static_cast<scalar_type*>(input.opaque()),\n-        static_cast<scalar_type*>(kernel.opaque()),\n+        params.intra_op_threadpool, static_cast<ScalarType*>(output.opaque()),\n+        static_cast<const ScalarType*>(input.opaque()),\n+        static_cast<const ScalarType*>(kernel.opaque()),\n         convolution_canonical_dims_.input_batch,\n         convolution_canonical_dims_.input_dims.x,\n         convolution_canonical_dims_.input_dims.y,\n@@ -220,33 +206,23 @@ ConvolutionThunk::HandleEigen3DConvolution(const ExecuteParams& params,\n         convolution_canonical_dims_.window_dilation.x,\n         convolution_canonical_dims_.window_dilation.y,\n         convolution_canonical_dims_.window_dilation.z,\n-        convolution_canonical_dims_.feature_group_count, std::move(count_down));\n+        convolution_canonical_dims_.feature_group_count,\n+        [execute_event] { execute_event.SetStateConcrete(); });\n+\n+    return execute_event;\n   };\n \n   PrimitiveType input_type = convolution_slices_.input_shape.element_type();\n \n-  // Execute convolution in the intra-op threadpool.\n-  if (options_.multi_threaded) {\n-    tsl::CountDownAsyncValueRef<ExecuteEvent> count_down(\n-        convolution_canonical_dims_.feature_group_count);\n-    auto execute_event = count_down.AsRef();\n-\n-    if (input_type == PrimitiveType::F16) {\n-      dispatch(Eigen::half{}, *params.intra_op_threadpool,\n-               std::move(count_down));\n-    } else {\n-      dispatch(float{}, *params.intra_op_threadpool, std::move(count_down));\n-    }\n-    return execute_event;\n-  }\n-\n-  // Execute convolution in the caller thread.\n-  if (input_type == PrimitiveType::F16) {\n-    dispatch(Eigen::half{}, Eigen::DefaultDevice(), /*count_down=*/{});\n-  } else {\n-    dispatch(float{}, Eigen::DefaultDevice(), /*count_down=*/{});\n+  switch (input_type) {\n+    case PrimitiveType::F16:\n+      return dispatch(Eigen::half{});\n+    case PrimitiveType::F32:\n+      return dispatch(float{});\n+    default:\n+      return Internal(\"Unsupported Conv3D input type: %s\",\n+                      primitive_util::LowercasePrimitiveTypeName(input_type));\n   }\n-  return OkExecuteEvent();\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "5f639d5dbdd0ffa3d825997ea9a1fd1ade93051c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk.h?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -33,8 +33,9 @@ namespace xla::cpu {\n // Performs 1D, 2D or 3D convolution.\n class ConvolutionThunk final : public Thunk {\n  public:\n+  // TODO(ezhulenev): Remove this struct as we always use thread pool.\n   struct Options {\n-    bool multi_threaded = false;\n+    bool multi_threaded = true;\n   };\n \n   static absl::StatusOr<std::unique_ptr<ConvolutionThunk>> Create("
        },
        {
            "sha": "80004ffc47139142839abe6d188937b00ca9ab78",
            "filename": "third_party/xla/xla/backends/cpu/runtime/convolution_thunk_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fconvolution_thunk_test.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -182,34 +182,5 @@ TEST(ConvolutionThunkTest, CreationErrorOnOutputChannelsMismatch) {\n                            \"should be the same as output channels count (4)\"));\n }\n \n-TEST(ConvolutionThunkTest,\n-     ExecutionErrorOnMissingThreadPoolInMultiThreadedMode) {\n-  ConvolutionThunkBuilder<float> builder;\n-\n-  auto options = MakeConvolutionOptions();\n-  options.multi_threaded = true;\n-  builder.SetOptions(options);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto thunk, builder.Build());\n-  BufferAllocations allocations = builder.GetAllocations();\n-\n-  // Execute thunk and wait for completion.\n-  Thunk::ExecuteParams params;\n-  params.intra_op_threadpool = nullptr;\n-  params.buffer_allocations = &allocations;\n-\n-  auto execute_event = thunk->Execute(params);\n-  tsl::BlockUntilReady(execute_event);\n-\n-  // Verify that the execution was not successful.\n-  ASSERT_TRUE(execute_event.IsError());\n-  auto status = execute_event.GetError();\n-  EXPECT_EQ(absl::StatusCode::kInternal, status.code());\n-  EXPECT_EQ(\n-      \"Intra-op threadpool must be provided for ConvolutionThunk in \"\n-      \"multi-threaded mode.\",\n-      status.message());\n-}\n-\n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "30c43246fa98463806b21e6f5c37870af9377a88",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -1052,7 +1052,7 @@ cc_library(\n     deps = [\n         \":runtime_lightweight_check\",\n         \"//xla:executable_run_options\",\n-        \"//xla/backends/cpu/runtime:convolution_thunk_internal\",\n+        \"//xla/backends/cpu/runtime:convolution_lib\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/synchronization\",  # build_cleaner: keep\n         \"@eigen_archive//:eigen3\",\n@@ -1068,7 +1068,7 @@ cc_library(\n     deps = [\n         \":runtime_lightweight_check\",\n         \"//xla:executable_run_options\",\n-        \"//xla/backends/cpu/runtime:convolution_thunk_internal\",\n+        \"//xla/backends/cpu/runtime:convolution_lib\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/synchronization\",  # build_cleaner: keep\n         \"@eigen_archive//:eigen3\",\n@@ -1107,7 +1107,7 @@ cc_library(\n     copts = runtime_copts(),\n     visibility = [\"//visibility:public\"],\n     deps = [\n-        \"//xla/backends/cpu/runtime:convolution_thunk_internal\",\n+        \"//xla/backends/cpu/runtime:convolution_lib\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/synchronization\",  # build_cleaner: keep\n         \"@eigen_archive//:eigen3\",\n@@ -1121,7 +1121,7 @@ cc_library(\n     copts = runtime_copts(),\n     visibility = [\"//visibility:public\"],\n     deps = [\n-        \"//xla/backends/cpu/runtime:convolution_thunk_internal\",\n+        \"//xla/backends/cpu/runtime:convolution_lib\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/synchronization\",  # build_cleaner: keep\n         \"@eigen_archive//:eigen3\","
        },
        {
            "sha": "785360dcc86a0d61097d0402471a8f8f7f47362f",
            "filename": "third_party/xla/xla/service/cpu/runtime_conv2d.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv2d.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -21,7 +21,7 @@ limitations under the License.\n \n #define EIGEN_USE_THREADS\n \n-#include \"xla/backends/cpu/runtime/convolution_thunk_internal.h\"\n+#include \"xla/backends/cpu/runtime/convolution_lib.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/service/cpu/runtime_lightweight_check.h\"\n \n@@ -39,12 +39,12 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenConv2DF32(\n       static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n   XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n   xla::cpu::internal::EigenConv2D(\n-      *run_options->intra_op_thread_pool(), out, lhs, rhs, input_batch,\n-      input_rows, input_cols, input_channels, kernel_rows, kernel_cols,\n-      kernel_channels, kernel_filters, output_rows, output_cols, row_stride,\n-      col_stride, padding_top, padding_bottom, padding_left, padding_right,\n-      lhs_row_dilation, lhs_col_dilation, rhs_row_dilation, rhs_col_dilation,\n-      feature_group_count, /*count_down=*/{}, /*use_thunk_runtime=*/false);\n+      nullptr, out, lhs, rhs, input_batch, input_rows, input_cols,\n+      input_channels, kernel_rows, kernel_cols, kernel_channels, kernel_filters,\n+      output_rows, output_cols, row_stride, col_stride, padding_top,\n+      padding_bottom, padding_left, padding_right, lhs_row_dilation,\n+      lhs_col_dilation, rhs_row_dilation, rhs_col_dilation, feature_group_count,\n+      /*done=*/[] {});\n }\n \n ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenConv2DF16(\n@@ -61,10 +61,10 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenConv2DF16(\n       static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n   XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n   xla::cpu::internal::EigenConv2D(\n-      *run_options->intra_op_thread_pool(), out, lhs, rhs, input_batch,\n-      input_rows, input_cols, input_channels, kernel_rows, kernel_cols,\n-      kernel_channels, kernel_filters, output_rows, output_cols, row_stride,\n-      col_stride, padding_top, padding_bottom, padding_left, padding_right,\n-      lhs_row_dilation, lhs_col_dilation, rhs_row_dilation, rhs_col_dilation,\n-      feature_group_count, /*count_down=*/{}, /*use_thunk_runtime=*/false);\n+      nullptr, out, lhs, rhs, input_batch, input_rows, input_cols,\n+      input_channels, kernel_rows, kernel_cols, kernel_channels, kernel_filters,\n+      output_rows, output_cols, row_stride, col_stride, padding_top,\n+      padding_bottom, padding_left, padding_right, lhs_row_dilation,\n+      lhs_col_dilation, rhs_row_dilation, rhs_col_dilation, feature_group_count,\n+      /*done=*/[] {});\n }"
        },
        {
            "sha": "530a079ec959a1507c88d43119063c4abb833c0a",
            "filename": "third_party/xla/xla/service/cpu/runtime_conv3d.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv3d.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv3d.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_conv3d.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -21,7 +21,7 @@ limitations under the License.\n \n #define EIGEN_USE_THREADS\n \n-#include \"xla/backends/cpu/runtime/convolution_thunk_internal.h\"\n+#include \"xla/backends/cpu/runtime/convolution_lib.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/service/cpu/runtime_lightweight_check.h\"\n \n@@ -41,13 +41,13 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenConv3DF32(\n       static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n   XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n   xla::cpu::internal::EigenConv3D(\n-      *run_options->intra_op_thread_pool(), out, lhs, rhs, input_batch, input_x,\n-      input_y, input_z, input_channels, kernel_x, kernel_y, kernel_z,\n-      kernel_channels, kernel_filters, output_x, output_y, output_z, x_stride,\n-      y_stride, z_stride, padding_x_before, padding_x_after, padding_y_before,\n+      nullptr, out, lhs, rhs, input_batch, input_x, input_y, input_z,\n+      input_channels, kernel_x, kernel_y, kernel_z, kernel_channels,\n+      kernel_filters, output_x, output_y, output_z, x_stride, y_stride,\n+      z_stride, padding_x_before, padding_x_after, padding_y_before,\n       padding_y_after, padding_z_before, padding_z_after, lhs_x_dilation,\n       lhs_y_dilation, lhs_z_dilation, rhs_x_dilation, rhs_y_dilation,\n-      rhs_z_dilation, feature_group_count, {});\n+      rhs_z_dilation, feature_group_count, /*done=*/[] {});\n }\n \n ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenConv3DF16(\n@@ -66,11 +66,11 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenConv3DF16(\n       static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n   XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n   xla::cpu::internal::EigenConv3D(\n-      *run_options->intra_op_thread_pool(), out, lhs, rhs, input_batch, input_x,\n-      input_y, input_z, input_channels, kernel_x, kernel_y, kernel_z,\n-      kernel_channels, kernel_filters, output_x, output_y, output_z, x_stride,\n-      y_stride, z_stride, padding_x_before, padding_x_after, padding_y_before,\n+      nullptr, out, lhs, rhs, input_batch, input_x, input_y, input_z,\n+      input_channels, kernel_x, kernel_y, kernel_z, kernel_channels,\n+      kernel_filters, output_x, output_y, output_z, x_stride, y_stride,\n+      z_stride, padding_x_before, padding_x_after, padding_y_before,\n       padding_y_after, padding_z_before, padding_z_after, lhs_x_dilation,\n       lhs_y_dilation, lhs_z_dilation, rhs_x_dilation, rhs_y_dilation,\n-      rhs_z_dilation, feature_group_count, {});\n+      rhs_z_dilation, feature_group_count, /*done=*/[] {});\n }"
        },
        {
            "sha": "e392770525ee893311dd26571cd11552f84c11ae",
            "filename": "third_party/xla/xla/service/cpu/runtime_single_threaded_conv2d.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_single_threaded_conv2d.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_single_threaded_conv2d.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_single_threaded_conv2d.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n #include <cstdint>\n \n #include \"absl/base/attributes.h\"\n-#include \"xla/backends/cpu/runtime/convolution_thunk_internal.h\"\n+#include \"xla/backends/cpu/runtime/convolution_lib.h\"\n \n ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void\n __xla_cpu_runtime_EigenSingleThreadedConv2DF16(\n@@ -32,12 +32,12 @@ __xla_cpu_runtime_EigenSingleThreadedConv2DF16(\n     int64_t lhs_col_dilation, int64_t rhs_row_dilation,\n     int64_t rhs_col_dilation, int64_t feature_group_count) {\n   xla::cpu::internal::EigenConv2D(\n-      Eigen::DefaultDevice(), out, lhs, rhs, input_batch, input_rows,\n-      input_cols, input_channels, kernel_rows, kernel_cols, kernel_channels,\n-      kernel_filters, output_rows, output_cols, row_stride, col_stride,\n-      padding_top, padding_bottom, padding_left, padding_right,\n-      lhs_row_dilation, lhs_col_dilation, rhs_row_dilation, rhs_col_dilation,\n-      feature_group_count, /*count_down=*/{}, /*use_thunk_runtime=*/false);\n+      nullptr, out, lhs, rhs, input_batch, input_rows, input_cols,\n+      input_channels, kernel_rows, kernel_cols, kernel_channels, kernel_filters,\n+      output_rows, output_cols, row_stride, col_stride, padding_top,\n+      padding_bottom, padding_left, padding_right, lhs_row_dilation,\n+      lhs_col_dilation, rhs_row_dilation, rhs_col_dilation, feature_group_count,\n+      /*done=*/[] {});\n }\n \n ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void\n@@ -52,10 +52,10 @@ __xla_cpu_runtime_EigenSingleThreadedConv2DF32(\n     int64_t rhs_row_dilation, int64_t rhs_col_dilation,\n     int64_t feature_group_count) {\n   xla::cpu::internal::EigenConv2D(\n-      Eigen::DefaultDevice(), out, lhs, rhs, input_batch, input_rows,\n-      input_cols, input_channels, kernel_rows, kernel_cols, kernel_channels,\n-      kernel_filters, output_rows, output_cols, row_stride, col_stride,\n-      padding_top, padding_bottom, padding_left, padding_right,\n-      lhs_row_dilation, lhs_col_dilation, rhs_row_dilation, rhs_col_dilation,\n-      feature_group_count, /*count_down=*/{}, /*use_thunk_runtime=*/false);\n+      nullptr, out, lhs, rhs, input_batch, input_rows, input_cols,\n+      input_channels, kernel_rows, kernel_cols, kernel_channels, kernel_filters,\n+      output_rows, output_cols, row_stride, col_stride, padding_top,\n+      padding_bottom, padding_left, padding_right, lhs_row_dilation,\n+      lhs_col_dilation, rhs_row_dilation, rhs_col_dilation, feature_group_count,\n+      /*done=*/[] {});\n }"
        },
        {
            "sha": "e2b278fafe5dbb30b27ea39dda744b492e1cb202",
            "filename": "third_party/xla/xla/service/cpu/runtime_single_threaded_conv3d.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_single_threaded_conv3d.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_single_threaded_conv3d.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_single_threaded_conv3d.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n #include <cstdint>\n \n #include \"absl/base/attributes.h\"\n-#include \"xla/backends/cpu/runtime/convolution_thunk_internal.h\"\n+#include \"xla/backends/cpu/runtime/convolution_lib.h\"\n \n ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void\n __xla_cpu_runtime_EigenSingleThreadedConv3DF32(\n@@ -34,13 +34,13 @@ __xla_cpu_runtime_EigenSingleThreadedConv3DF32(\n     int64_t rhs_y_dilation, int64_t rhs_z_dilation,\n     int64_t feature_group_count) {\n   xla::cpu::internal::EigenConv3D(\n-      Eigen::DefaultDevice(), out, lhs, rhs, input_batch, input_x, input_y,\n-      input_z, input_channels, kernel_x, kernel_y, kernel_z, kernel_channels,\n+      nullptr, out, lhs, rhs, input_batch, input_x, input_y, input_z,\n+      input_channels, kernel_x, kernel_y, kernel_z, kernel_channels,\n       kernel_filters, output_x, output_y, output_z, x_stride, y_stride,\n       z_stride, padding_x_before, padding_x_after, padding_y_before,\n       padding_y_after, padding_z_before, padding_z_after, lhs_x_dilation,\n       lhs_y_dilation, lhs_z_dilation, rhs_x_dilation, rhs_y_dilation,\n-      rhs_z_dilation, feature_group_count, {});\n+      rhs_z_dilation, feature_group_count, /*done=*/[] {});\n }\n \n ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void\n@@ -57,11 +57,11 @@ __xla_cpu_runtime_EigenSingleThreadedConv3DF16(\n     int64_t rhs_y_dilation, int64_t rhs_z_dilation,\n     int64_t feature_group_count) {\n   xla::cpu::internal::EigenConv3D(\n-      Eigen::DefaultDevice(), out, lhs, rhs, input_batch, input_x, input_y,\n-      input_z, input_channels, kernel_x, kernel_y, kernel_z, kernel_channels,\n+      nullptr, out, lhs, rhs, input_batch, input_x, input_y, input_z,\n+      input_channels, kernel_x, kernel_y, kernel_z, kernel_channels,\n       kernel_filters, output_x, output_y, output_z, x_stride, y_stride,\n       z_stride, padding_x_before, padding_x_after, padding_y_before,\n       padding_y_after, padding_z_before, padding_z_after, lhs_x_dilation,\n       lhs_y_dilation, lhs_z_dilation, rhs_x_dilation, rhs_y_dilation,\n-      rhs_z_dilation, feature_group_count, {});\n+      rhs_z_dilation, feature_group_count, /*done=*/[] {});\n }"
        },
        {
            "sha": "e618f47ae3d29790e0d6f6591841279c463b12da",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3e43c82e0d3ac138d989d9115030995ad2058e33/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=3e43c82e0d3ac138d989d9115030995ad2058e33",
            "patch": "@@ -785,8 +785,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConvolutionThunk(\n       TF_ASSIGN_OR_RETURN(auto output_buffer, GetAllocationSlice(instruction));\n \n       ConvolutionThunk::Options options;\n-      options.multi_threaded =\n-          hlo_module_config_.debug_options().xla_cpu_multi_thread_eigen();\n       return ThunkSequence::Of<ConvolutionThunk>(\n           ThunkInfo(instruction), options, input_buffer, input_shape,\n           kernel_buffer, kernel_shape, output_buffer, output_shape,"
        }
    ],
    "stats": {
        "total": 772,
        "additions": 360,
        "deletions": 412
    }
}