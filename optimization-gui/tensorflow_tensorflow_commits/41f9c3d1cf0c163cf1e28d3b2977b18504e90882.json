{
    "author": "ermilovmaxim",
    "message": "Add Shape to CopyThunk buffer_uses\n\nModify Thunk's serialization\n\nPiperOrigin-RevId: 843308614",
    "sha": "41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
    "files": [
        {
            "sha": "506a39ee7b28b8c953312ac9657f4af8b67a1c64",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -25,6 +25,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/runtime:copy_thunk\",\n+        \"//xla/backends/gpu/runtime:shaped_slice\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\","
        },
        {
            "sha": "2997e07de0e0efee1ef2837f109eff9e9ce11406",
            "filename": "third_party/xla/xla/backends/gpu/codegen/copy.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -64,13 +65,15 @@ absl::StatusOr<FusionEmissionResult> MemcpyFusion::Emit(\n     IrEmitterContext& ir_emitter_context,\n     const HloFusionInstruction& fusion) const {\n   std::vector<BufferAllocation::Slice> src_buffers;\n+  std::vector<Shape> src_shapes;\n   for (const HloInstructionAdaptor& root_adaptor : analysis_.fusion_roots()) {\n     const HloInstruction* root = &root_adaptor.instruction();\n     const HloInstruction* src_instr =\n         fusion.operand(root->operand(0)->parameter_number());\n     TF_ASSIGN_OR_RETURN(BufferAllocation::Slice slice,\n                         buffer_assignment_->GetUniqueSlice(src_instr, {}));\n     src_buffers.push_back(slice);\n+    src_shapes.push_back(root->operand(0)->shape());\n   }\n \n   std::vector<BufferAllocation::Slice> dst_buffers;\n@@ -91,8 +94,8 @@ absl::StatusOr<FusionEmissionResult> MemcpyFusion::Emit(\n       result.thunks.emplace_back(std::make_unique<DeviceToDeviceCopyThunk>(\n           Thunk::ThunkInfo::WithProfileAnnotation(\n               &fusion, ir_emitter_context.GetNextThunkId()),\n-          /*source_buffer=*/src_buffers[i],\n-          /*destination_buffer=*/dst_buffers[i],\n+          /*source_buffer=*/ShapedSlice{src_buffers[i], src_shapes[i]},\n+          /*destination_buffer=*/ShapedSlice{dst_buffers[i], src_shapes[i]},\n           /*mem_size=*/src_buffers[i].size()));\n     }\n   }"
        },
        {
            "sha": "6cbc17cc246fe015c2b73ccaa9e018c528a552bc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -1289,8 +1289,8 @@ absl::StatusOr<FusionEmissionResult> EmitCollective(\n              \"collective\";\n       seq.emplace_back(std::make_unique<DeviceToDeviceCopyThunk>(\n           thunk_info,\n-          /*source_buffer=*/src.value(),\n-          /*destination_buffer=*/dst.value(),\n+          /*source_buffer=*/ShapedSlice{src.value(), shape},\n+          /*destination_buffer=*/ShapedSlice{dst.value(), shape},\n           /*mem_size=*/ShapeUtil::ByteSizeOf(shape)));\n     }\n   } else if (implementable_status.ok()) {"
        },
        {
            "sha": "3876537b50c5b214505a024ace4f6627230f49ea",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -592,9 +592,11 @@ cc_library(\n     srcs = [\"copy_thunk.cc\"],\n     hdrs = [\"copy_thunk.h\"],\n     deps = [\n+        \":shaped_slice\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n         \":while_thunk\",\n+        \"//xla:shape_util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n@@ -610,6 +612,7 @@ cc_library(\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/container:node_hash_map\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/synchronization\",\n@@ -624,6 +627,7 @@ xla_cc_test(\n         \":copy_thunk\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n+        \"//xla:shape_util\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:parse_text_proto\",\n@@ -3205,6 +3209,7 @@ xla_test(\n         \":gemm_thunk\",\n         \":replica_id_thunk\",\n         \":sequential_thunk\",\n+        \":shaped_slice\",\n         \":thunk\",\n         \":thunk_pass_pipeline\",\n         \":while_thunk\","
        },
        {
            "sha": "5412864c82397729efc97925fb06d028ea3eddd3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -1324,23 +1324,29 @@ CommandBufferCmd::BufferUseVector CustomKernelLaunchCmd::buffers() const {\n // MemcpyDeviceToDeviceCmd\n //===----------------------------------------------------------------------===//\n \n-MemcpyDeviceToDeviceCmd::MemcpyDeviceToDeviceCmd(BufferAllocation::Slice dst,\n-                                                 BufferAllocation::Slice src,\n+MemcpyDeviceToDeviceCmd::MemcpyDeviceToDeviceCmd(ShapedSlice dst,\n+                                                 ShapedSlice src,\n                                                  int64_t num_bytes)\n     : CommandBufferCmd(CommandBufferCmdType::kMemcpyDeviceToDeviceCmd),\n       dst_(dst),\n       src_(src),\n-      num_bytes_(num_bytes) {}\n+      num_bytes_(num_bytes) {\n+  CHECK_EQ(ShapeUtil::ByteSizeOfElements(src_.shape),\n+           ShapeUtil::ByteSizeOfElements(dst_.shape));\n+  CHECK_LE(num_bytes, dst_.slice.size());\n+  CHECK_LE(num_bytes, src_.slice.size());\n+  CHECK_GE(src_.slice.size(), ShapeUtil::ByteSizeOf(src_.shape));\n+}\n \n absl::StatusOr<const se::CommandBuffer::Command*>\n MemcpyDeviceToDeviceCmd::Record(const Thunk::ExecuteParams& execute_params,\n                                 const RecordParams& record_params,\n                                 RecordAction record_action,\n                                 se::CommandBuffer* command_buffer) {\n   se::DeviceAddressBase dst =\n-      execute_params.buffer_allocations->GetDeviceAddress(dst_);\n+      execute_params.buffer_allocations->GetDeviceAddress(dst_.slice);\n   se::DeviceAddressBase src =\n-      execute_params.buffer_allocations->GetDeviceAddress(src_);\n+      execute_params.buffer_allocations->GetDeviceAddress(src_.slice);\n \n   VLOG(5) << \"MemcpyDeviceToDeviceCmd: num_bytes = \" << num_bytes_;\n   VLOG(5) << \"  Dst: \" << dst_ << \" (\" << dst.opaque() << \")\";\n@@ -1363,7 +1369,8 @@ MemcpyDeviceToDeviceCmd::Record(const Thunk::ExecuteParams& execute_params,\n }\n \n CommandBufferCmd::BufferUseVector MemcpyDeviceToDeviceCmd::buffers() const {\n-  return {BufferUse::Write(dst_), BufferUse::Read(src_)};\n+  return {BufferUse::Write(dst_.slice, dst_.shape),\n+          BufferUse::Read(src_.slice, src_.shape)};\n }\n \n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "abeb0971888d2204845dad878a35a9c72289a0f7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -781,8 +781,7 @@ class CustomKernelLaunchCmd : public CommandBufferCmd {\n \n class MemcpyDeviceToDeviceCmd : public CommandBufferCmd {\n  public:\n-  MemcpyDeviceToDeviceCmd(BufferAllocation::Slice dst,\n-                          BufferAllocation::Slice src, int64_t num_bytes);\n+  MemcpyDeviceToDeviceCmd(ShapedSlice dst, ShapedSlice src, int64_t num_bytes);\n \n   absl::StatusOr<const se::CommandBuffer::Command*> Record(\n       const Thunk::ExecuteParams& execute_params,\n@@ -792,9 +791,9 @@ class MemcpyDeviceToDeviceCmd : public CommandBufferCmd {\n   BufferUseVector buffers() const override;\n \n  private:\n-  BufferAllocation::Slice dst_;\n-  BufferAllocation::Slice src_;\n-  int64_t num_bytes_;\n+  ShapedSlice dst_;\n+  ShapedSlice src_;\n+  uint64_t num_bytes_;\n };\n \n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "c995a45d181cac096e38b9fa080bf60f32169aa3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -252,6 +252,7 @@ TEST(CommandBufferCmdTest, MemcpyCmd) {\n   auto stream = stream_executor->CreateStream().value();\n   int64_t length = 4;\n   int64_t byte_length = sizeof(int32_t) * length;\n+  Shape shape = ShapeUtil::MakeShape(S32, {length});\n \n   // Prepare arguments: a=42, b=0\n   se::DeviceAddress<int32_t> a =\n@@ -271,7 +272,8 @@ TEST(CommandBufferCmdTest, MemcpyCmd) {\n \n   // Prepare commands sequence for constructing command buffer.\n   CommandBufferCmdSequence commands;\n-  commands.Emplace<MemcpyDeviceToDeviceCmd>(slice_b, slice_a, byte_length);\n+  commands.Emplace<MemcpyDeviceToDeviceCmd>(\n+      ShapedSlice{slice_b, shape}, ShapedSlice{slice_a, shape}, byte_length);\n   TF_ASSERT_OK_AND_ASSIGN(\n       CommandBufferCmdExecutor executor,\n       CommandBufferCmdExecutor::Create(std::move(commands), serialize));\n@@ -609,6 +611,7 @@ TEST(CommandBufferCmdTest, RecordExecutorsWithDependencies) {\n   auto stream = stream_executor->CreateStream().value();\n   int64_t length = 4;\n   int64_t byte_length = sizeof(int32_t) * length;\n+  Shape shape = ShapeUtil::MakeShape(S32, {length});\n \n   // Device buffers: a, b, c\n   se::DeviceAddress<int32_t> a =\n@@ -654,7 +657,8 @@ TEST(CommandBufferCmdTest, RecordExecutorsWithDependencies) {\n \n   // Executor C: c = b (memcpy)\n   CommandBufferCmdSequence seq_c;\n-  seq_c.Emplace<MemcpyDeviceToDeviceCmd>(slice_c, slice_b, byte_length);\n+  seq_c.Emplace<MemcpyDeviceToDeviceCmd>(\n+      ShapedSlice{slice_c, shape}, ShapedSlice{slice_b, shape}, byte_length);\n   TF_ASSERT_OK_AND_ASSIGN(\n       CommandBufferCmdExecutor exec_c,\n       CommandBufferCmdExecutor::Create(std::move(seq_c), serialize));\n@@ -749,7 +753,8 @@ TEST(CommandBufferCmdTest, NestedChildCmdCreateAndUpdate) {\n \n   // Inner child: c = a (device-to-device memcpy)\n   CommandBufferCmdSequence inner_seq;\n-  inner_seq.Emplace<MemcpyDeviceToDeviceCmd>(slice_c, slice_a, byte_length);\n+  inner_seq.Emplace<MemcpyDeviceToDeviceCmd>(\n+      ShapedSlice{slice_c, shape}, ShapedSlice{slice_a, shape}, byte_length);\n   TF_ASSERT_OK_AND_ASSIGN(\n       CommandBufferCmdExecutor inner_executor,\n       CommandBufferCmdExecutor::Create(std::move(inner_seq), serialize));\n@@ -759,7 +764,8 @@ TEST(CommandBufferCmdTest, NestedChildCmdCreateAndUpdate) {\n   middle_seq.Emplace<ChildCmd>(std::move(inner_executor));\n   // Add a couple of extra commands that don't affect `c`.\n   middle_seq.Emplace<Memset32Cmd>(slice_b, /*bit_pattern=*/3);\n-  middle_seq.Emplace<MemcpyDeviceToDeviceCmd>(slice_b, slice_b, byte_length);\n+  middle_seq.Emplace<MemcpyDeviceToDeviceCmd>(\n+      ShapedSlice{slice_b, shape}, ShapedSlice{slice_b, shape}, byte_length);\n   TF_ASSERT_OK_AND_ASSIGN(\n       CommandBufferCmdExecutor middle_executor,\n       CommandBufferCmdExecutor::Create(std::move(middle_seq), serialize));"
        },
        {
            "sha": "19a47a6ff0f5df724e7ea95f88ad4e8df773c4ba",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/replica_id_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n@@ -51,6 +52,7 @@ limitations under the License.\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/platform_util.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -134,8 +136,10 @@ std::unique_ptr<AllGatherStartThunk> CreateAllGatherStartThunk(\n std::unique_ptr<DeviceToDeviceCopyThunk> CreateCopyThunk(\n     const BufferAllocation& alloc0) {\n   BufferAllocation::Slice slice0(&alloc0, 0, 1024);\n-  return std::make_unique<DeviceToDeviceCopyThunk>(Thunk::ThunkInfo(), slice0,\n-                                                   slice0, 1024);\n+  Shape shape = ShapeUtil::MakeShape(S32, {256});\n+  return std::make_unique<DeviceToDeviceCopyThunk>(\n+      Thunk::ThunkInfo(), ShapedSlice{slice0, shape},\n+      ShapedSlice{slice0, shape}, 1024);\n }\n \n std::unique_ptr<GemmThunk> CreateGemmThunk(const BufferAllocation& alloc1) {"
        },
        {
            "sha": "9c80bec9cba67c21409a0ed230c6b5a8de7fb986",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -159,7 +159,7 @@ TEST(CommandBufferThunkTest, MemcpyCmd) {\n \n   int64_t length = 4;\n   int64_t byte_length = sizeof(int32_t) * length;\n-\n+  Shape shape = ShapeUtil::MakeShape(S32, {length});\n   // Prepare arguments: a=42, b=0\n   se::DeviceAddress<int32_t> a =\n       stream_executor->AllocateArray<int32_t>(length, 0);\n@@ -178,7 +178,8 @@ TEST(CommandBufferThunkTest, MemcpyCmd) {\n \n   // Prepare commands sequence for constructing command buffer.\n   CommandBufferCmdSequence commands;\n-  commands.Emplace<MemcpyDeviceToDeviceCmd>(slice_b, slice_a, byte_length);\n+  commands.Emplace<MemcpyDeviceToDeviceCmd>(\n+      ShapedSlice{slice_b, shape}, ShapedSlice{slice_a, shape}, byte_length);\n   TF_ASSERT_OK_AND_ASSIGN(\n       CommandBufferCmdExecutor executor,\n       CommandBufferCmdExecutor::Create(std::move(commands), serialize));"
        },
        {
            "sha": "a5aba50e3451565c940b03e280c281874d1c82bd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/copy_thunk.cc",
            "status": "modified",
            "additions": 74,
            "deletions": 48,
            "changes": 122,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -22,15 +22,18 @@ limitations under the License.\n \n #include \"absl/base/casts.h\"\n #include \"absl/container/node_hash_map.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -42,22 +45,31 @@ namespace xla {\n namespace gpu {\n \n DeviceToDeviceCopyThunk::DeviceToDeviceCopyThunk(\n-    ThunkInfo thunk_info, const BufferAllocation::Slice& source_buffer,\n-    const BufferAllocation::Slice& destination_buffer, uint64_t mem_size)\n+    ThunkInfo thunk_info, const ShapedSlice& source_buffer,\n+    const ShapedSlice& destination_buffer, int64_t mem_size)\n     : Thunk(Kind::kCopy, std::move(thunk_info)),\n       source_buffer_(source_buffer),\n       destination_buffer_(destination_buffer),\n-      mem_size_(mem_size) {}\n+      mem_size_(mem_size) {\n+  // TODO(b/460846009): Determine size based on shape.\n+  // Bounded dynamic shape contains extra header after data.\n+  // Header size needs to be accounted for.\n+  CHECK_EQ(ShapeUtil::ByteSizeOf(source_buffer_.shape),\n+           ShapeUtil::ByteSizeOf(destination_buffer_.shape));\n+\n+  CHECK_GE(source_buffer_.slice.size(), mem_size);\n+  CHECK_GE(destination_buffer_.slice.size(), mem_size);\n+}\n \n absl::Status DeviceToDeviceCopyThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   se::DeviceAddressBase destination_data =\n-      params.buffer_allocations->GetDeviceAddress(destination_buffer_);\n+      params.buffer_allocations->GetDeviceAddress(destination_buffer_.slice);\n   se::DeviceAddressBase source_data =\n-      params.buffer_allocations->GetDeviceAddress(source_buffer_);\n-  VLOG(3) << \"Memcpy D2D of size \" << mem_size_ << \" from \"\n+      params.buffer_allocations->GetDeviceAddress(source_buffer_.slice);\n+  VLOG(3) << \"Memcpy D2D of size \" << size_bytes() << \" from \"\n           << source_data.opaque() << \" to \" << destination_data.opaque();\n-  return params.stream->Memcpy(&destination_data, source_data, mem_size_);\n+  return params.stream->Memcpy(&destination_data, source_data, size_bytes());\n }\n \n absl::StatusOr<ThunkProto> DeviceToDeviceCopyThunk::ToProto() const {\n@@ -67,9 +79,9 @@ absl::StatusOr<ThunkProto> DeviceToDeviceCopyThunk::ToProto() const {\n       proto.mutable_device_to_device_copy_thunk();\n   CopyThunkProto* copy_thunk_proto = d2d_copy_thunk_proto->mutable_copy_thunk();\n   TF_ASSIGN_OR_RETURN(*copy_thunk_proto->mutable_source_buffer(),\n-                      source().ToProto());\n+                      source_buffer_.ToProto());\n   TF_ASSIGN_OR_RETURN(*copy_thunk_proto->mutable_destination_buffer(),\n-                      destination().ToProto());\n+                      destination_buffer_.ToProto());\n   copy_thunk_proto->set_mem_size(size_bytes());\n   return proto;\n }\n@@ -79,13 +91,18 @@ DeviceToDeviceCopyThunk::FromProto(\n     ThunkInfo thunk_info, const DeviceToDeviceCopyThunkProto& thunk_proto,\n     absl::Span<const BufferAllocation> buffer_allocations) {\n   TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice src_slice,\n-      BufferAllocation::Slice::FromProto(\n-          thunk_proto.copy_thunk().source_buffer(), buffer_allocations));\n+      ShapedSlice src_slice,\n+      ShapedSlice::FromProto(thunk_proto.copy_thunk().source_buffer(),\n+                             buffer_allocations));\n   TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice dst_slice,\n-      BufferAllocation::Slice::FromProto(\n-          thunk_proto.copy_thunk().destination_buffer(), buffer_allocations));\n+      ShapedSlice dst_slice,\n+      ShapedSlice::FromProto(thunk_proto.copy_thunk().destination_buffer(),\n+                             buffer_allocations));\n+  if (ShapeUtil::ByteSizeOfElements(src_slice.shape) !=\n+      ShapeUtil::ByteSizeOfElements(dst_slice.shape)) {\n+    return absl::FailedPreconditionError(\n+        \"DeviceToDeviceCopyThunkProto with incompatible shapes.\");\n+  }\n   return std::make_unique<DeviceToDeviceCopyThunk>(\n       std::move(thunk_info), src_slice, dst_slice,\n       thunk_proto.copy_thunk().mem_size());\n@@ -95,14 +112,18 @@ DeviceToDeviceCopyThunk::FromProto(\n // CopyThunk\n //===----------------------------------------------------------------------===//\n \n-CopyThunk::CopyThunk(ThunkInfo thunk_info,\n-                     const BufferAllocation::Slice& source_buffer,\n-                     const BufferAllocation::Slice& destination_buffer,\n-                     uint64_t mem_size)\n+CopyThunk::CopyThunk(ThunkInfo thunk_info, const ShapedSlice& source_buffer,\n+                     const ShapedSlice& destination_buffer, int64_t mem_size)\n     : Thunk(Kind::kCopy, std::move(thunk_info)),\n       source_buffer_(source_buffer),\n       destination_buffer_(destination_buffer),\n-      mem_size_(mem_size) {}\n+      mem_size_(mem_size) {\n+  CHECK_EQ(ShapeUtil::ByteSizeOfElements(source_buffer_.shape),\n+           ShapeUtil::ByteSizeOfElements(destination_buffer_.shape));\n+\n+  CHECK_GE(source_buffer_.slice.size(), mem_size);\n+  CHECK_GE(destination_buffer_.slice.size(), mem_size);\n+}\n \n absl::Status CopyThunk::ExecuteOnStream(const ExecuteParams& params) {\n   return absl::OkStatus();\n@@ -146,23 +167,28 @@ absl::StatusOr<ThunkProto> CopyThunk::ToProto() const {\n \n   CopyThunkProto* copy_thunk_proto = proto.mutable_copy_thunk();\n   TF_ASSIGN_OR_RETURN(*copy_thunk_proto->mutable_source_buffer(),\n-                      source().ToProto());\n+                      source_buffer_.ToProto());\n   TF_ASSIGN_OR_RETURN(*copy_thunk_proto->mutable_destination_buffer(),\n-                      destination().ToProto());\n+                      destination_buffer_.ToProto());\n   copy_thunk_proto->set_mem_size(size_bytes());\n   return proto;\n }\n \n absl::StatusOr<std::unique_ptr<CopyThunk>> CopyThunk::FromProto(\n     ThunkInfo thunk_info, const CopyThunkProto& thunk_proto,\n     absl::Span<const BufferAllocation> buffer_allocations) {\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice src_slice,\n-                      BufferAllocation::Slice::FromProto(\n-                          thunk_proto.source_buffer(), buffer_allocations));\n   TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice dst_slice,\n-      BufferAllocation::Slice::FromProto(thunk_proto.destination_buffer(),\n-                                         buffer_allocations));\n+      ShapedSlice src_slice,\n+      ShapedSlice::FromProto(thunk_proto.source_buffer(), buffer_allocations));\n+  TF_ASSIGN_OR_RETURN(ShapedSlice dst_slice,\n+                      ShapedSlice::FromProto(thunk_proto.destination_buffer(),\n+                                             buffer_allocations));\n+  if (ShapeUtil::ByteSizeOfElements(src_slice.shape) !=\n+      ShapeUtil::ByteSizeOfElements(dst_slice.shape)) {\n+    return absl::FailedPreconditionError(\n+        \"DeviceToDeviceCopyThunkProto with incompatible shapes.\");\n+  }\n+\n   return std::make_unique<CopyThunk>(std::move(thunk_info), src_slice,\n                                      dst_slice, thunk_proto.mem_size());\n }\n@@ -171,8 +197,8 @@ absl::StatusOr<std::unique_ptr<CopyThunk>> CopyThunk::FromProto(\n // DeviceToHostCopyThunk\n //===----------------------------------------------------------------------===//\n DeviceToHostCopyThunk::DeviceToHostCopyThunk(\n-    ThunkInfo thunk_info, const BufferAllocation::Slice& source_buffer,\n-    const BufferAllocation::Slice& destination_buffer, uint64_t mem_size,\n+    ThunkInfo thunk_info, const ShapedSlice& source_buffer,\n+    const ShapedSlice& destination_buffer, int64_t mem_size,\n     std::shared_ptr<CopyThunk::AsyncEvents> async_events,\n     const HloInstruction* instr)\n     : CopyThunk(std::move(thunk_info), source_buffer, destination_buffer,\n@@ -183,9 +209,9 @@ DeviceToHostCopyThunk::DeviceToHostCopyThunk(\n absl::Status DeviceToHostCopyThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   se::DeviceAddressBase destination_data =\n-      params.buffer_allocations->GetDeviceAddress(destination());\n+      params.buffer_allocations->GetDeviceAddress(destination().slice);\n   se::DeviceAddressBase source_data =\n-      params.buffer_allocations->GetDeviceAddress(source());\n+      params.buffer_allocations->GetDeviceAddress(source().slice);\n   void* cpu_dst = destination_data.opaque();\n   TF_ASSIGN_OR_RETURN(\n       se::Stream * stream,\n@@ -225,13 +251,13 @@ DeviceToHostCopyThunk::FromProto(\n     ThunkInfo thunk_info, const DeviceToHostCopyThunkProto& thunk_proto,\n     absl::Span<const BufferAllocation> buffer_allocations) {\n   TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice src_slice,\n-      BufferAllocation::Slice::FromProto(\n-          thunk_proto.copy_thunk().source_buffer(), buffer_allocations));\n+      ShapedSlice src_slice,\n+      ShapedSlice::FromProto(thunk_proto.copy_thunk().source_buffer(),\n+                             buffer_allocations));\n   TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice dst_slice,\n-      BufferAllocation::Slice::FromProto(\n-          thunk_proto.copy_thunk().destination_buffer(), buffer_allocations));\n+      ShapedSlice dst_slice,\n+      ShapedSlice::FromProto(thunk_proto.copy_thunk().destination_buffer(),\n+                             buffer_allocations));\n   return std::make_unique<DeviceToHostCopyThunk>(\n       std::move(thunk_info), src_slice, dst_slice,\n       thunk_proto.copy_thunk().mem_size(),\n@@ -252,8 +278,8 @@ DeviceToHostCopyThunk::GetAsyncEventsUniqueId() const {\n // HostToDeviceCopyThunk\n //===----------------------------------------------------------------------===//\n HostToDeviceCopyThunk::HostToDeviceCopyThunk(\n-    ThunkInfo thunk_info, const BufferAllocation::Slice& source_buffer,\n-    const BufferAllocation::Slice& destination_buffer, uint64_t mem_size,\n+    ThunkInfo thunk_info, const ShapedSlice& source_buffer,\n+    const ShapedSlice& destination_buffer, int64_t mem_size,\n     std::shared_ptr<CopyThunk::AsyncEvents> async_events,\n     const HloInstruction* instr)\n     : CopyThunk(std::move(thunk_info), source_buffer, destination_buffer,\n@@ -264,9 +290,9 @@ HostToDeviceCopyThunk::HostToDeviceCopyThunk(\n absl::Status HostToDeviceCopyThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   se::DeviceAddressBase destination_data =\n-      params.buffer_allocations->GetDeviceAddress(destination());\n+      params.buffer_allocations->GetDeviceAddress(destination().slice);\n   se::DeviceAddressBase source_data =\n-      params.buffer_allocations->GetDeviceAddress(source());\n+      params.buffer_allocations->GetDeviceAddress(source().slice);\n   void* cpu_src = source_data.opaque();\n   TF_ASSIGN_OR_RETURN(\n       se::Stream * stream,\n@@ -306,13 +332,13 @@ HostToDeviceCopyThunk::FromProto(\n     ThunkInfo thunk_info, const HostToDeviceCopyThunkProto& thunk_proto,\n     absl::Span<const BufferAllocation> buffer_allocations) {\n   TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice src_slice,\n-      BufferAllocation::Slice::FromProto(\n-          thunk_proto.copy_thunk().source_buffer(), buffer_allocations));\n+      ShapedSlice src_slice,\n+      ShapedSlice::FromProto(thunk_proto.copy_thunk().source_buffer(),\n+                             buffer_allocations));\n   TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice dst_slice,\n-      BufferAllocation::Slice::FromProto(\n-          thunk_proto.copy_thunk().destination_buffer(), buffer_allocations));\n+      ShapedSlice dst_slice,\n+      ShapedSlice::FromProto(thunk_proto.copy_thunk().destination_buffer(),\n+                             buffer_allocations));\n   return std::make_unique<HostToDeviceCopyThunk>(\n       std::move(thunk_info), src_slice, dst_slice,\n       thunk_proto.copy_thunk().mem_size(),"
        },
        {
            "sha": "a6afb8f0e3c7e4da59cf81ac599b011512bd8e3c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/copy_thunk.h",
            "status": "modified",
            "additions": 32,
            "deletions": 38,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.h?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -30,11 +30,13 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n@@ -45,28 +47,25 @@ namespace gpu {\n class DeviceToDeviceCopyThunk : public Thunk {\n  public:\n   // Constructs a CopyThunk that copies host data from `source_buffer` to the\n-  // device buffer `destination_buffer`. `mem_size` is the size of the data in\n-  // bytes.\n+  // device buffer `destination_buffer`.\n   DeviceToDeviceCopyThunk(ThunkInfo thunk_info,\n-                          const BufferAllocation::Slice& source_buffer,\n-                          const BufferAllocation::Slice& destination_buffer,\n-                          uint64_t mem_size);\n+                          const ShapedSlice& source_buffer,\n+                          const ShapedSlice& destination_buffer,\n+                          int64_t mem_size);\n \n   DeviceToDeviceCopyThunk(const DeviceToDeviceCopyThunk&) = delete;\n   DeviceToDeviceCopyThunk& operator=(const DeviceToDeviceCopyThunk&) = delete;\n \n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n \n-  const BufferAllocation::Slice& source() const { return source_buffer_; }\n-  const BufferAllocation::Slice& destination() const {\n-    return destination_buffer_;\n-  }\n-  uint64_t size_bytes() const { return mem_size_; }\n+  const ShapedSlice& source() const { return source_buffer_; }\n+  const ShapedSlice& destination() const { return destination_buffer_; }\n+  int64_t size_bytes() const { return mem_size_; }\n \n   BufferUses buffer_uses() const override {\n     return {\n-        BufferUse::Read(source_buffer_),\n-        BufferUse::Write(destination_buffer_),\n+        BufferUse::Read(source_buffer_.slice, source_buffer_.shape),\n+        BufferUse::Write(destination_buffer_.slice, destination_buffer_.shape),\n     };\n   }\n \n@@ -78,9 +77,11 @@ class DeviceToDeviceCopyThunk : public Thunk {\n \n   friend bool operator==(const DeviceToDeviceCopyThunk& lhs,\n                          const DeviceToDeviceCopyThunk& rhs) {\n-    return std::tie(lhs.source_buffer_, lhs.destination_buffer_,\n-                    lhs.mem_size_) ==\n-           std::tie(rhs.source_buffer_, rhs.destination_buffer_, rhs.mem_size_);\n+    if (lhs.size_bytes() != rhs.size_bytes()) {\n+      return false;\n+    }\n+    return std::tie(lhs.source_buffer_, lhs.destination_buffer_) ==\n+           std::tie(rhs.source_buffer_, rhs.destination_buffer_);\n   }\n \n   friend bool operator!=(const DeviceToDeviceCopyThunk& lhs,\n@@ -89,9 +90,9 @@ class DeviceToDeviceCopyThunk : public Thunk {\n   }\n \n  private:\n-  const BufferAllocation::Slice source_buffer_;\n-  const BufferAllocation::Slice destination_buffer_;\n-  const uint64_t mem_size_;\n+  const ShapedSlice source_buffer_;\n+  const ShapedSlice destination_buffer_;\n+  const int64_t mem_size_;\n };\n \n //===----------------------------------------------------------------------===//\n@@ -117,20 +118,17 @@ class CopyThunk : public Thunk {\n     absl::flat_hash_map<Key, std::unique_ptr<se::Event>> events_\n         ABSL_GUARDED_BY(mutex_);\n   };\n-  CopyThunk(ThunkInfo thunk_info, const BufferAllocation::Slice& source_buffer,\n-            const BufferAllocation::Slice& destination_buffer,\n-            uint64_t mem_size);\n+  CopyThunk(ThunkInfo thunk_info, const ShapedSlice& source_buffer,\n+            const ShapedSlice& destination_buffer, int64_t mem_size);\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n-  const BufferAllocation::Slice& source() const { return source_buffer_; }\n-  const BufferAllocation::Slice& destination() const {\n-    return destination_buffer_;\n-  }\n+  const ShapedSlice& source() const { return source_buffer_; }\n+  const ShapedSlice& destination() const { return destination_buffer_; }\n   uint64_t size_bytes() const { return mem_size_; }\n \n   BufferUses buffer_uses() const override {\n     return {\n-        BufferUse::Read(source_buffer_),\n-        BufferUse::Write(destination_buffer_),\n+        BufferUse::Read(source_buffer_.slice, source_buffer_.shape),\n+        BufferUse::Write(destination_buffer_.slice, destination_buffer_.shape),\n     };\n   }\n \n@@ -146,9 +144,9 @@ class CopyThunk : public Thunk {\n       absl::Span<const BufferAllocation> buffer_allocations);\n \n  private:\n-  const BufferAllocation::Slice source_buffer_;\n-  const BufferAllocation::Slice destination_buffer_;\n-  const uint64_t mem_size_;\n+  const ShapedSlice source_buffer_;\n+  const ShapedSlice destination_buffer_;\n+  const int64_t mem_size_;\n };\n \n //===----------------------------------------------------------------------===//\n@@ -163,10 +161,8 @@ class DeviceToHostCopyThunk : public CopyThunk {\n   // the device buffer `destination_buffer`. `mem_size` is the size of the data\n   // in bytes. `events` are the cuda record/wait events.\n   // `instr` is the copy-start instruction.\n-  DeviceToHostCopyThunk(ThunkInfo thunk_info,\n-                        const BufferAllocation::Slice& source_buffer,\n-                        const BufferAllocation::Slice& destination_buffer,\n-                        uint64_t mem_size,\n+  DeviceToHostCopyThunk(ThunkInfo thunk_info, const ShapedSlice& source_buffer,\n+                        const ShapedSlice& destination_buffer, int64_t mem_size,\n                         std::shared_ptr<CopyThunk::AsyncEvents> events,\n                         const HloInstruction* instr);\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n@@ -198,10 +194,8 @@ class HostToDeviceCopyThunk : public CopyThunk {\n   // the host buffer `destination_buffer`. `mem_size` is the size of the data\n   // in bytes. `events` are the cuda record/wait events.\n   // `instr` is the copy-start instruction.\n-  HostToDeviceCopyThunk(ThunkInfo thunk_info,\n-                        const BufferAllocation::Slice& source_buffer,\n-                        const BufferAllocation::Slice& destination_buffer,\n-                        uint64_t mem_size,\n+  HostToDeviceCopyThunk(ThunkInfo thunk_info, const ShapedSlice& source_buffer,\n+                        const ShapedSlice& destination_buffer, int64_t mem_size,\n                         std::shared_ptr<CopyThunk::AsyncEvents> events,\n                         const HloInstruction* instr);\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;"
        },
        {
            "sha": "3b556de2685d4d56056a18bb41be2ba8993d515b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/copy_thunk_test.cc",
            "status": "modified",
            "additions": 231,
            "deletions": 59,
            "changes": 290,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk_test.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -23,6 +23,8 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/parse_text_proto.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n@@ -41,22 +43,40 @@ TEST(CopyThunkTest, ToProto) {\n   BufferAllocation alloc0(/*index=*/0, /*size=*/1024, /*color=*/0);\n   BufferAllocation alloc1(/*index=*/1, /*size=*/1024, /*color=*/0);\n   auto src_slice =\n-      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/384);\n+      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/256);\n   auto dst_slice = BufferAllocation::Slice(&alloc1, /*offset=*/0, /*size=*/256);\n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n \n-  CopyThunk thunk(thunk_info, src_slice, dst_slice, /*mem_size=*/256);\n+  CopyThunk thunk(thunk_info, {src_slice, shape}, {dst_slice, shape}, 256);\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, thunk.ToProto());\n-  EXPECT_THAT(proto, EqualsProto(R\"pb(\n-                thunk_info {\n-                  profile_annotation: \"profile_annotation\"\n-                  execution_stream_id: 123\n-                }\n-                copy_thunk {\n-                  source_buffer { offset: 128 size: 384 }\n-                  destination_buffer { size: 256 buffer_allocation_index: 1 }\n-                  mem_size: 256\n-                }\n-              )pb\"));\n+  EXPECT_THAT(\n+      proto, EqualsProto(R\"pb(\n+        thunk_info {\n+          profile_annotation: \"profile_annotation\"\n+          execution_stream_id: 123\n+        }\n+        copy_thunk {\n+          source_buffer {\n+            slice { offset: 128 size: 256 }\n+            shape {\n+              dimensions: 64\n+              element_type: S32\n+              is_dynamic_dimension: false\n+              layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            }\n+          }\n+          destination_buffer {\n+            slice { size: 256 buffer_allocation_index: 1 }\n+            shape {\n+              dimensions: 64\n+              element_type: S32\n+              is_dynamic_dimension: false\n+              layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            }\n+          }\n+          mem_size: 256\n+        }\n+      )pb\"));\n }\n \n TEST(CopyThunkTest, FromProto) {\n@@ -67,8 +87,24 @@ TEST(CopyThunkTest, FromProto) {\n           execution_stream_id: 123\n         }\n         copy_thunk {\n-          source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n-          destination_buffer { offset: 0 size: 256 buffer_allocation_index: 1 }\n+          source_buffer {\n+            slice { offset: 128 size: 256 }\n+            shape {\n+              dimensions: 64\n+              element_type: S32\n+              is_dynamic_dimension: false\n+              layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            }\n+          }\n+          destination_buffer {\n+            slice { size: 256 buffer_allocation_index: 1 }\n+            shape {\n+              dimensions: 64\n+              element_type: S32\n+              is_dynamic_dimension: false\n+              layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            }\n+          }\n           mem_size: 256\n         }\n       )pb\");\n@@ -83,15 +119,18 @@ TEST(CopyThunkTest, FromProto) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<CopyThunk> thunk,\n       CopyThunk::FromProto(thunk_info, proto.copy_thunk(), buffer_allocations));\n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n \n   EXPECT_EQ(\n       *thunk.get(),\n       CopyThunk(thunk_info,\n-                BufferAllocation::Slice(&buffer_allocations[0],\n-                                        /*offset=*/128, /*size=*/384),\n-                BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n-                                        /*size=*/256),\n-                /*mem_size=*/256));\n+                {BufferAllocation::Slice(&buffer_allocations[0],\n+                                         /*offset=*/128, /*size=*/256),\n+                 shape},\n+                {BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n+                                         /*size=*/256),\n+                 shape},\n+                256));\n }\n \n TEST(DeviceToHostCopyThunkProtoTest, ToProto) {\n@@ -102,11 +141,12 @@ TEST(DeviceToHostCopyThunkProtoTest, ToProto) {\n   BufferAllocation alloc0(/*index=*/0, /*size=*/1024, /*color=*/0);\n   BufferAllocation alloc1(/*index=*/1, /*size=*/1024, /*color=*/0);\n   auto src_slice =\n-      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/384);\n+      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/256);\n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n   auto dst_slice = BufferAllocation::Slice(&alloc1, /*offset=*/0, /*size=*/256);\n \n-  DeviceToHostCopyThunk thunk(thunk_info, src_slice, dst_slice,\n-                              /*mem_size=*/256,\n+  DeviceToHostCopyThunk thunk(thunk_info, {src_slice, shape},\n+                              {dst_slice, shape}, 256,\n                               /*events=*/nullptr,\n                               /*instr=*/nullptr);\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, thunk.ToProto());\n@@ -117,8 +157,30 @@ TEST(DeviceToHostCopyThunkProtoTest, ToProto) {\n                 }\n                 device_to_host_copy_thunk {\n                   copy_thunk {\n-                    source_buffer { offset: 128 size: 384 }\n-                    destination_buffer { size: 256 buffer_allocation_index: 1 }\n+                    source_buffer {\n+                      slice { offset: 128 size: 256 }\n+                      shape {\n+                        dimensions: 64\n+                        element_type: S32\n+                        is_dynamic_dimension: false\n+                        layout {\n+                          minor_to_major: 0\n+                          tail_padding_alignment_in_elements: 1\n+                        }\n+                      }\n+                    }\n+                    destination_buffer {\n+                      slice { size: 256 buffer_allocation_index: 1 }\n+                      shape {\n+                        dimensions: 64\n+                        element_type: S32\n+                        is_dynamic_dimension: false\n+                        layout {\n+                          minor_to_major: 0\n+                          tail_padding_alignment_in_elements: 1\n+                        }\n+                      }\n+                    }\n                     mem_size: 256\n                   }\n                 }\n@@ -134,11 +196,29 @@ TEST(DeviceToHostCopyThunkProtoTest, FromProto) {\n         }\n         device_to_host_copy_thunk {\n           copy_thunk {\n-            source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n+            source_buffer {\n+              slice { offset: 128 size: 256 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n+            }\n             destination_buffer {\n-              offset: 0\n-              size: 256\n-              buffer_allocation_index: 1\n+              slice { size: 256 buffer_allocation_index: 1 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n             }\n             mem_size: 256\n           }\n@@ -156,14 +236,17 @@ TEST(DeviceToHostCopyThunkProtoTest, FromProto) {\n       std::unique_ptr<DeviceToHostCopyThunk> thunk,\n       DeviceToHostCopyThunk::FromProto(\n           thunk_info, proto.device_to_host_copy_thunk(), buffer_allocations));\n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n \n   EXPECT_EQ(*thunk.get(),\n             DeviceToHostCopyThunk(\n                 thunk_info,\n-                BufferAllocation::Slice(&buffer_allocations[0],\n-                                        /*offset=*/128, /*size=*/384),\n-                BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n-                                        /*size=*/256),\n+                {BufferAllocation::Slice(&buffer_allocations[0],\n+                                         /*offset=*/128, /*size=*/256),\n+                 shape},\n+                {BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n+                                         /*size=*/256),\n+                 shape},\n                 /*mem_size=*/256,\n                 /*events=*/nullptr,\n                 /*instr=*/nullptr));\n@@ -177,10 +260,12 @@ TEST(HostToDeviceCopyThunkProtoTest, ToProto) {\n   BufferAllocation alloc0(/*index=*/0, /*size=*/1024, /*color=*/0);\n   BufferAllocation alloc1(/*index=*/1, /*size=*/1024, /*color=*/0);\n   auto src_slice =\n-      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/384);\n+      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/256);\n   auto dst_slice = BufferAllocation::Slice(&alloc1, /*offset=*/0, /*size=*/256);\n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n \n-  HostToDeviceCopyThunk thunk(thunk_info, src_slice, dst_slice,\n+  HostToDeviceCopyThunk thunk(thunk_info, {src_slice, shape},\n+                              {dst_slice, shape},\n                               /*mem_size=*/256,\n                               /*events=*/nullptr,\n                               /*instr=*/nullptr);\n@@ -192,8 +277,30 @@ TEST(HostToDeviceCopyThunkProtoTest, ToProto) {\n                 }\n                 host_to_device_copy_thunk {\n                   copy_thunk {\n-                    source_buffer { offset: 128 size: 384 }\n-                    destination_buffer { size: 256 buffer_allocation_index: 1 }\n+                    source_buffer {\n+                      slice { offset: 128 size: 256 }\n+                      shape {\n+                        dimensions: 64\n+                        element_type: S32\n+                        is_dynamic_dimension: false\n+                        layout {\n+                          minor_to_major: 0\n+                          tail_padding_alignment_in_elements: 1\n+                        }\n+                      }\n+                    }\n+                    destination_buffer {\n+                      slice { size: 256 buffer_allocation_index: 1 }\n+                      shape {\n+                        dimensions: 64\n+                        element_type: S32\n+                        is_dynamic_dimension: false\n+                        layout {\n+                          minor_to_major: 0\n+                          tail_padding_alignment_in_elements: 1\n+                        }\n+                      }\n+                    }\n                     mem_size: 256\n                   }\n                 }\n@@ -209,11 +316,29 @@ TEST(HostToDeviceCopyThunkProtoTest, FromProto) {\n         }\n         host_to_device_copy_thunk {\n           copy_thunk {\n-            source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n+            source_buffer {\n+              slice { offset: 128 size: 256 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n+            }\n             destination_buffer {\n-              offset: 0\n-              size: 256\n-              buffer_allocation_index: 1\n+              slice { size: 256 buffer_allocation_index: 1 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n             }\n             mem_size: 256\n           }\n@@ -226,6 +351,7 @@ TEST(HostToDeviceCopyThunkProtoTest, FromProto) {\n   std::vector<BufferAllocation> buffer_allocations = {\n       BufferAllocation(/*index=*/0, /*size=*/1024, /*color=*/0),\n       BufferAllocation(/*index=*/1, /*size=*/1024, /*color=*/0)};\n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<HostToDeviceCopyThunk> thunk,\n@@ -235,10 +361,12 @@ TEST(HostToDeviceCopyThunkProtoTest, FromProto) {\n   EXPECT_EQ(*thunk.get(),\n             HostToDeviceCopyThunk(\n                 thunk_info,\n-                BufferAllocation::Slice(&buffer_allocations[0],\n-                                        /*offset=*/128, /*size=*/384),\n-                BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n-                                        /*size=*/256),\n+                {BufferAllocation::Slice(&buffer_allocations[0],\n+                                         /*offset=*/128, /*size=*/256),\n+                 shape},\n+                {BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n+                                         /*size=*/256),\n+                 shape},\n                 /*mem_size=*/256,\n                 /*events=*/nullptr,\n                 /*instr=*/nullptr));\n@@ -252,11 +380,12 @@ TEST(DeviceToDeviceCopyThunkProtoTest, ToProto) {\n   BufferAllocation alloc0(/*index=*/0, /*size=*/1024, /*color=*/0);\n   BufferAllocation alloc1(/*index=*/1, /*size=*/1024, /*color=*/0);\n   auto src_slice =\n-      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/384);\n+      BufferAllocation::Slice(&alloc0, /*offset=*/128, /*size=*/256);\n   auto dst_slice = BufferAllocation::Slice(&alloc1, /*offset=*/0, /*size=*/256);\n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n \n-  DeviceToDeviceCopyThunk thunk(thunk_info, src_slice, dst_slice,\n-                                /*mem_size=*/256);\n+  DeviceToDeviceCopyThunk thunk(thunk_info, {src_slice, shape},\n+                                {dst_slice, shape}, 256);\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, thunk.ToProto());\n   EXPECT_THAT(proto, EqualsProto(R\"pb(\n                 thunk_info {\n@@ -265,8 +394,30 @@ TEST(DeviceToDeviceCopyThunkProtoTest, ToProto) {\n                 }\n                 device_to_device_copy_thunk {\n                   copy_thunk {\n-                    source_buffer { offset: 128 size: 384 }\n-                    destination_buffer { size: 256 buffer_allocation_index: 1 }\n+                    source_buffer {\n+                      slice { offset: 128 size: 256 }\n+                      shape {\n+                        dimensions: 64\n+                        element_type: S32\n+                        is_dynamic_dimension: false\n+                        layout {\n+                          minor_to_major: 0\n+                          tail_padding_alignment_in_elements: 1\n+                        }\n+                      }\n+                    }\n+                    destination_buffer {\n+                      slice { size: 256 buffer_allocation_index: 1 }\n+                      shape {\n+                        dimensions: 64\n+                        element_type: S32\n+                        is_dynamic_dimension: false\n+                        layout {\n+                          minor_to_major: 0\n+                          tail_padding_alignment_in_elements: 1\n+                        }\n+                      }\n+                    }\n                     mem_size: 256\n                   }\n                 }\n@@ -282,11 +433,29 @@ TEST(DeviceToDeviceCopyThunkProtoTest, FromProto) {\n         }\n         device_to_device_copy_thunk {\n           copy_thunk {\n-            source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n+            source_buffer {\n+              slice { offset: 128 size: 256 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n+            }\n             destination_buffer {\n-              offset: 0\n-              size: 256\n-              buffer_allocation_index: 1\n+              slice { size: 256 buffer_allocation_index: 1 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n             }\n             mem_size: 256\n           }\n@@ -305,14 +474,17 @@ TEST(DeviceToDeviceCopyThunkProtoTest, FromProto) {\n       DeviceToDeviceCopyThunk::FromProto(\n           thunk_info, proto.device_to_device_copy_thunk(), buffer_allocations));\n \n+  Shape shape = ShapeUtil::MakeShape(S32, {64});\n   EXPECT_EQ(*thunk.get(),\n             DeviceToDeviceCopyThunk(\n                 thunk_info,\n-                BufferAllocation::Slice(&buffer_allocations[0],\n-                                        /*offset=*/128, /*size=*/384),\n-                BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n-                                        /*size=*/256),\n-                /*mem_size=*/256));\n+                {BufferAllocation::Slice(&buffer_allocations[0],\n+                                         /*offset=*/128, /*size=*/256),\n+                 shape},\n+                {BufferAllocation::Slice(&buffer_allocations[1], /*offset=*/0,\n+                                         /*size=*/256),\n+                 shape},\n+                256));\n }\n \n }  // namespace"
        },
        {
            "sha": "a84798192bfe651283ff572572d6d94ad94b2814",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -53,8 +53,8 @@ message ThunkMetadataListProto {\n }\n \n message CopyThunkProto {\n-  xla.buffer_assignment.BufferAllocationSliceProto source_buffer = 1;\n-  xla.buffer_assignment.BufferAllocationSliceProto destination_buffer = 2;\n+  ShapedSliceProto source_buffer = 1;\n+  ShapedSliceProto destination_buffer = 2;\n   int64 mem_size = 3;\n }\n "
        },
        {
            "sha": "b3afd91438161a9c93820693df448f7326d731a6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization_test.cc",
            "status": "modified",
            "additions": 244,
            "deletions": 46,
            "changes": 290,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization_test.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -101,8 +101,24 @@ TEST(ThunkProtoDeserializationTest, CopyThunk) {\n           execution_stream_id: 123\n         }\n         copy_thunk {\n-          source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n-          destination_buffer { offset: 0 size: 256 buffer_allocation_index: 1 }\n+          source_buffer {\n+            slice { offset: 128 size: 384 buffer_allocation_index: 0 }\n+            shape {\n+              dimensions: 64\n+              element_type: S32\n+              is_dynamic_dimension: false\n+              layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            }\n+          }\n+          destination_buffer {\n+            slice { offset: 0 size: 256 buffer_allocation_index: 1 }\n+            shape {\n+              dimensions: 64\n+              element_type: S32\n+              is_dynamic_dimension: false\n+              layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            }\n+          }\n           mem_size: 256\n         }\n       )pb\");\n@@ -130,11 +146,29 @@ TEST(ThunkProtoDeserializationTest, DeviceToHostCopyThunk) {\n         }\n         device_to_host_copy_thunk {\n           copy_thunk {\n-            source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n+            source_buffer {\n+              slice { offset: 128 size: 384 buffer_allocation_index: 0 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n+            }\n             destination_buffer {\n-              offset: 0\n-              size: 256\n-              buffer_allocation_index: 1\n+              slice { offset: 0 size: 256 buffer_allocation_index: 1 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n             }\n             mem_size: 256\n           }\n@@ -164,11 +198,29 @@ TEST(ThunkProtoDeserializationTest, HostToDeviceCopyThunk) {\n         }\n         host_to_device_copy_thunk {\n           copy_thunk {\n-            source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n+            source_buffer {\n+              slice { offset: 128 size: 384 buffer_allocation_index: 0 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n+            }\n             destination_buffer {\n-              offset: 0\n-              size: 256\n-              buffer_allocation_index: 1\n+              slice { offset: 0 size: 256 buffer_allocation_index: 1 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n             }\n             mem_size: 256\n           }\n@@ -198,11 +250,29 @@ TEST(ThunkProtoDeserializationTest, DeviceToDeviceCopyThunk) {\n         }\n         device_to_device_copy_thunk {\n           copy_thunk {\n-            source_buffer { offset: 128 size: 384 buffer_allocation_index: 0 }\n+            source_buffer {\n+              slice { offset: 128 size: 384 buffer_allocation_index: 0 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n+            }\n             destination_buffer {\n-              offset: 0\n-              size: 256\n-              buffer_allocation_index: 1\n+              slice { offset: 0 size: 256 buffer_allocation_index: 1 }\n+              shape {\n+                dimensions: 64\n+                element_type: S32\n+                is_dynamic_dimension: false\n+                layout {\n+                  minor_to_major: 0\n+                  tail_padding_alignment_in_elements: 1\n+                }\n+              }\n             }\n             mem_size: 256\n           }\n@@ -243,18 +313,30 @@ TEST(ThunkProtoDeserializationTest, WhileThunk) {\n                 execution_stream_id: 123\n               }\n               copy_thunk {\n-                source_buffer { buffer_allocation_index: 0 }\n-                destination_buffer { buffer_allocation_index: 1 }\n-              }\n-            }\n-            thunks {\n-              thunk_info {\n-                profile_annotation: \"profile_annotation\"\n-                execution_stream_id: 123\n-              }\n-              copy_thunk {\n-                source_buffer { buffer_allocation_index: 1 }\n-                destination_buffer { buffer_allocation_index: 2 }\n+                source_buffer {\n+                  slice { offset: 128 size: 384 buffer_allocation_index: 0 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n+                destination_buffer {\n+                  slice { offset: 0 size: 256 buffer_allocation_index: 1 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n               }\n             }\n           }\n@@ -265,8 +347,30 @@ TEST(ThunkProtoDeserializationTest, WhileThunk) {\n                 execution_stream_id: 123\n               }\n               copy_thunk {\n-                source_buffer { buffer_allocation_index: 2 }\n-                destination_buffer { buffer_allocation_index: 3 }\n+                source_buffer {\n+                  slice { offset: 128 size: 384 buffer_allocation_index: 2 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n+                destination_buffer {\n+                  slice { offset: 0 size: 256 buffer_allocation_index: 3 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n               }\n             }\n             thunks {\n@@ -275,8 +379,30 @@ TEST(ThunkProtoDeserializationTest, WhileThunk) {\n                 execution_stream_id: 123\n               }\n               copy_thunk {\n-                source_buffer { buffer_allocation_index: 3 }\n-                destination_buffer { buffer_allocation_index: 4 }\n+                source_buffer {\n+                  slice { offset: 128 size: 384 buffer_allocation_index: 3 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n+                destination_buffer {\n+                  slice { offset: 0 size: 256 buffer_allocation_index: 4 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n               }\n             }\n           }\n@@ -318,11 +444,29 @@ TEST(ThunkProtoDeserializationTest, ConditionalThunk) {\n                 execution_stream_id: 123\n               }\n               copy_thunk {\n-                source_buffer { offset: 0 size: 256 buffer_allocation_index: 0 }\n+                source_buffer {\n+                  slice { offset: 0 size: 256 buffer_allocation_index: 0 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n                 destination_buffer {\n-                  offset: 1\n-                  size: 257\n-                  buffer_allocation_index: 1\n+                  slice { offset: 1 size: 257 buffer_allocation_index: 1 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n                 }\n               }\n             }\n@@ -332,11 +476,29 @@ TEST(ThunkProtoDeserializationTest, ConditionalThunk) {\n                 execution_stream_id: 123\n               }\n               copy_thunk {\n-                source_buffer { offset: 2 size: 258 buffer_allocation_index: 1 }\n+                source_buffer {\n+                  slice { offset: 2 size: 258 buffer_allocation_index: 1 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n                 destination_buffer {\n-                  offset: 3\n-                  size: 259\n-                  buffer_allocation_index: 2\n+                  slice { offset: 3 size: 259 buffer_allocation_index: 2 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n                 }\n               }\n             }\n@@ -348,11 +510,29 @@ TEST(ThunkProtoDeserializationTest, ConditionalThunk) {\n                 execution_stream_id: 123\n               }\n               copy_thunk {\n-                source_buffer { offset: 4 size: 260 buffer_allocation_index: 2 }\n+                source_buffer {\n+                  slice { offset: 4 size: 260 buffer_allocation_index: 3 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n                 destination_buffer {\n-                  offset: 5\n-                  size: 261\n-                  buffer_allocation_index: 3\n+                  slice { offset: 5 size: 261 buffer_allocation_index: 3 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n                 }\n               }\n             }\n@@ -362,11 +542,29 @@ TEST(ThunkProtoDeserializationTest, ConditionalThunk) {\n                 execution_stream_id: 123\n               }\n               copy_thunk {\n-                source_buffer { offset: 6 size: 262 buffer_allocation_index: 3 }\n+                source_buffer {\n+                  slice { offset: 6 size: 262 buffer_allocation_index: 3 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n+                }\n                 destination_buffer {\n-                  offset: 7\n-                  size: 263\n-                  buffer_allocation_index: 4\n+                  slice { offset: 7 size: 263 buffer_allocation_index: 4 }\n+                  shape {\n+                    dimensions: 64\n+                    element_type: S32\n+                    is_dynamic_dimension: false\n+                    layout {\n+                      minor_to_major: 0\n+                      tail_padding_alignment_in_elements: 1\n+                    }\n+                  }\n                 }\n               }\n             }"
        },
        {
            "sha": "b5bde9888fb0f82066d9dc80bc85a5461f07cf8b",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -784,6 +784,7 @@ xla_cc_test(\n         \"//xla/backends/gpu/runtime:custom_kernel_thunk\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n+        \"//xla/backends/gpu/runtime:shaped_slice\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/client:executable_build_options\",\n         \"//xla/codegen/emitters:kernel_arguments\","
        },
        {
            "sha": "d4e884d50898b0fc030b1d539d4d4a4f7fffae2b",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/custom_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/client/executable_build_options.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -49,6 +50,7 @@ limitations under the License.\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/logical_buffer.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_layout.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n@@ -145,6 +147,7 @@ TEST(GpuExecutableTest, RunThunkPasses) {\n   auto create_executable = [&]() {\n     Thunk::ThunkInfo thunk_info;\n     BufferAllocation alloc(0, 1024, 0);\n+    Shape shape = ShapeUtil::MakeShape(S32, {256});\n     BufferAllocation::Slice slice(&alloc, 0, 1024);\n \n     ThunkSequence thunk_sequence;\n@@ -157,7 +160,8 @@ TEST(GpuExecutableTest, RunThunkPasses) {\n         /*shmem_bytes=*/0,\n         /*tma_metadata=*/se::gpu::TmaMetadata()));\n     thunk_sequence.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        thunk_info, slice, slice, 1024));\n+        thunk_info, ShapedSlice{slice, shape}, ShapedSlice{slice, shape},\n+        1024));\n \n     GpuExecutable::Params params;\n     params.executable = std::make_unique<SequentialThunk>(\n@@ -391,6 +395,7 @@ TEST(GpuExecutableTest, DumpsMetadataListProto) {\n   auto create_executable = [&]() {\n     BufferAllocation alloc(0, 1024, 0);\n     BufferAllocation::Slice slice(&alloc, 0, 1024);\n+    Shape shape = ShapeUtil::MakeShape(S32, {256});\n \n     ThunkSequence thunk_sequence;\n     thunk_sequence.push_back(std::make_unique<KernelThunk>(\n@@ -402,7 +407,8 @@ TEST(GpuExecutableTest, DumpsMetadataListProto) {\n         /*shmem_bytes=*/0,\n         /*tma_metadata=*/se::gpu::TmaMetadata()));\n     thunk_sequence.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        ThunkInfoWithId(456), slice, slice, 1024));\n+        ThunkInfoWithId(456), ShapedSlice{slice, shape},\n+        ShapedSlice{slice, shape}, 1024));\n \n     GpuExecutable::Params params;\n     params.executable = std::make_unique<SequentialThunk>(\n@@ -511,6 +517,8 @@ TEST(GpuExecutableTest, GpuExecutableDump) {\n   auto create_executable = [&]() {\n     ThunkSequence thunk_sequence;\n     BufferAllocation::Slice slice(&alloc, 0, 1024);\n+    Shape shape = ShapeUtil::MakeShape(S32, {256});\n+\n     thunk_sequence.push_back(std::make_unique<KernelThunk>(\n         ThunkInfoWithId(123),\n         /*kernel_name=*/\"test_kernel\",\n@@ -520,7 +528,8 @@ TEST(GpuExecutableTest, GpuExecutableDump) {\n         /*shmem_bytes=*/0,\n         /*tma_metadata=*/se::gpu::TmaMetadata()));\n     thunk_sequence.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        ThunkInfoWithId(456), slice, slice, 1024));\n+        ThunkInfoWithId(456), ShapedSlice{slice, shape},\n+        ShapedSlice{slice, shape}, 1024));\n \n     GpuExecutable::Params params;\n     params.executable = std::make_unique<SequentialThunk>("
        },
        {
            "sha": "890f1c660302fa9203b9e7e0d910b920420d0640",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 18,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/41f9c3d1cf0c163cf1e28d3b2977b18504e90882/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=41f9c3d1cf0c163cf1e28d3b2977b18504e90882",
            "patch": "@@ -1151,8 +1151,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTriangularSolveCustomCall(\n     thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             instr, ir_emitter_context_->GetNextThunkId()),\n-        /*source_buffer=*/b_slice,\n-        /*destination_buffer=*/result_slice,\n+        /*source_buffer=*/ShapedSlice{b_slice, b_shape},\n+        /*destination_buffer=*/ShapedSlice{result_slice, b_shape},\n         /*mem_size=*/ShapeUtil::ByteSizeOf(b_shape)));\n   }\n \n@@ -1438,8 +1438,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCopy(\n   return GetThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n-      /*source_buffer=*/src_buffer,\n-      /*destination_buffer=*/dst_buffer,\n+      /*source_buffer=*/ShapedSlice{src_buffer, instr->operand(0)->shape()},\n+      /*destination_buffer=*/ShapedSlice{dst_buffer, instr->shape()},\n       /*mem_size=*/src_buffer.size()));\n }\n \n@@ -1589,9 +1589,10 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSort(\n       thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n           Thunk::ThunkInfo::WithProfileAnnotation(\n               sort, ir_emitter_context_->GetNextThunkId()),\n-          /*source_buffer=*/source_address,\n-          /*destination_buffer=*/destination_buffer,\n-          /*mem_size=*/\n+          /*source_buffer=*/\n+          ShapedSlice{source_address, sort->operand(i)->shape()},\n+          /*destination_buffer=*/\n+          ShapedSlice{destination_buffer, sort->operand(i)->shape()},\n           ShapeUtil::ByteSizeOf(sort->operand(i)->shape())));\n     }\n   }\n@@ -1676,10 +1677,12 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectivePermute(\n                                                           : normal_shape_idx));\n \n     const int64_t src_memory_space = operand_shape.layout().memory_space();\n+    Shape result_buffer_shape = (result_shape.IsTuple())\n+                                    ? result_shape.tuple_shapes(oprd_idx)\n+                                    : result_shape;\n+\n     const int64_t dst_memory_space =\n-        (result_shape.IsTuple())\n-            ? result_shape.tuple_shapes(0).layout().memory_space()\n-            : result_shape.layout().memory_space();\n+        result_buffer_shape.layout().memory_space();\n \n     TF_ASSIGN_OR_RETURN(BufferAllocation::Slice source_slice,\n                         GetAllocationSliceForHlo(operand));\n@@ -1690,8 +1693,9 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectivePermute(\n       thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n           Thunk::ThunkInfo::WithProfileAnnotation(\n               instr, ir_emitter_context_->GetNextThunkId()),\n-          /*source_buffer=*/source_slice,\n-          /*destination_buffer=*/result_slice,\n+          /*source_buffer=*/ShapedSlice{source_slice, operand_shape},\n+          /*destination_buffer=*/\n+          ShapedSlice{result_slice, result_buffer_shape},\n           /*mem_size=*/ShapeUtil::ByteSizeOf(operand_shape)));\n       // Signal that start thunk not created with nullptr.\n       GetCollectivesAsyncEvents().try_emplace(instr, nullptr);\n@@ -2097,8 +2101,9 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitDegeneratedCollectiveThunk(\n     thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             inst, ir_emitter_context_->GetNextThunkId()),\n-        /*source_buffer=*/buffers[i].source_buffer,\n-        /*destination_buffer=*/buffers[i].destination_buffer,\n+        /*source_buffer=*/ShapedSlice{buffers[i].source_buffer, shape},\n+        /*destination_buffer=*/\n+        ShapedSlice{buffers[i].destination_buffer, shape},\n         /*mem_size=*/ShapeUtil::ByteSizeOf(shape)));\n   }\n   if (thunks.size() == 1) {\n@@ -2233,8 +2238,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCopyStartThunk(\n     auto thunk = std::make_unique<DeviceToHostCopyThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n-        /*source_buffer=*/src_buffer,\n-        /*destination_buffer=*/dst_buffer,\n+        /*source_buffer=*/ShapedSlice{src_buffer, input_shape},\n+        /*destination_buffer=*/ShapedSlice{dst_buffer, input_shape},\n         /*mem_size=*/ShapeUtil::ByteSizeOf(input_shape),\n         /*copy_events=*/copy_events_,\n         /*copy_start_instr=*/copy_start_instr);\n@@ -2244,8 +2249,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCopyStartThunk(\n     auto thunk = std::make_unique<HostToDeviceCopyThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n-        /*source_buffer=*/src_buffer,\n-        /*destination_buffer=*/dst_buffer,\n+        /*source_buffer=*/ShapedSlice{src_buffer, input_shape},\n+        /*destination_buffer=*/ShapedSlice{dst_buffer, input_shape},\n         /*mem_size=*/ShapeUtil::ByteSizeOf(input_shape),\n         /*copy_events=*/copy_events_,\n         /*copy_start_instr=*/copy_start_instr);"
        }
    ],
    "stats": {
        "total": 905,
        "additions": 668,
        "deletions": 237
    }
}