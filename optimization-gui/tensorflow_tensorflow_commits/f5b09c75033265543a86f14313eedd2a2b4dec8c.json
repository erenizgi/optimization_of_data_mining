{
    "author": "KanishAnand",
    "message": "Close parameter shardings to respect `allow_spmd_sharding_propagation_to_parameters` flag set to default `{false}` value.\n\nPiperOrigin-RevId: 797932617",
    "sha": "f5b09c75033265543a86f14313eedd2a2b4dec8c",
    "files": [
        {
            "sha": "4126182c74f06872081e1fc941091bf6f2663584",
            "filename": "third_party/xla/xla/service/spmd/shardy/utils.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 19,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f5b09c75033265543a86f14313eedd2a2b4dec8c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f5b09c75033265543a86f14313eedd2a2b4dec8c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc?ref=f5b09c75033265543a86f14313eedd2a2b4dec8c",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <memory>\n+#include <optional>\n #include <string>\n \n #include \"mhlo/IR/register.h\"\n@@ -157,6 +158,31 @@ void setFuncArgFrontendAttrs(FuncOp funcOp, unsigned int index,\n                     DictionaryAttr::get(funcOp.getContext(), frontendAttrs));\n }\n \n+std::optional<TensorShardingAttr> adjustShardingInternal(\n+    mlir::MLIRContext* context, int idx, TensorShardingAttr sharding,\n+    int64_t rank, absl::Span<const bool> allowSpmdShardingPropagation) {\n+  bool allowPropagation = false;\n+  if (!allowSpmdShardingPropagation.empty()) {\n+    allowPropagation = allowSpmdShardingPropagation.size() == 1\n+                           ? allowSpmdShardingPropagation[0]\n+                           : allowSpmdShardingPropagation[idx];\n+  }\n+\n+  if (allowPropagation) {\n+    return std::nullopt;\n+  }\n+\n+  // Close all dimensions if sharding propagation is not allowed.\n+  if (sharding) {\n+    sharding = sharding.getClosedLike(sharding);\n+  } else {\n+    sharding = TensorShardingAttr::getFullyClosed(context, rank,\n+                                                  MeshAttr::get(context, {}));\n+  }\n+\n+  return sharding;\n+}\n+\n }  // namespace\n \n void setFrontendAttribute(Operation* op, StringRef name, Attribute value) {\n@@ -210,29 +236,24 @@ void loadAllRequiredDialects(mlir::MLIRContext* context) {\n   context->loadAllAvailableDialects();\n }\n \n-void adjustOutputSharding(\n+void adjustInputSharding(\n     FuncOp func, int idx, TensorShardingAttr sharding, int64_t rank,\n-    absl::Span<const bool> allowSpmdShardingPropagationToOutput) {\n-  bool allowPropagation = false;\n-  if (!allowSpmdShardingPropagationToOutput.empty()) {\n-    allowPropagation = allowSpmdShardingPropagationToOutput.size() == 1\n-                           ? allowSpmdShardingPropagationToOutput[0]\n-                           : allowSpmdShardingPropagationToOutput[idx];\n-  }\n-\n-  if (allowPropagation) {\n-    return;\n+    absl::Span<const bool> allowSpmdShardingPropagationToParameters) {\n+  if (std::optional<TensorShardingAttr> adjustedSharding =\n+          adjustShardingInternal(func.getContext(), idx, sharding, rank,\n+                                 allowSpmdShardingPropagationToParameters)) {\n+    mlir::sdy::setSharding(func.getArgument(idx), *adjustedSharding);\n   }\n+}\n \n-  // Close all dimensions if sharding propagation to outputs is not allowed.\n-  if (sharding) {\n-    sharding = sharding.getClosedLike(sharding);\n-  } else {\n-    sharding = TensorShardingAttr::getFullyClosed(\n-        func.getContext(), rank,\n-        MeshAttr::get(func.getContext(), mlir::ArrayRef<MeshAxisAttr>{}));\n+void adjustOutputSharding(\n+    FuncOp func, int idx, TensorShardingAttr sharding, int64_t rank,\n+    absl::Span<const bool> allowSpmdShardingPropagationToOutput) {\n+  if (std::optional<TensorShardingAttr> adjustedSharding =\n+          adjustShardingInternal(func.getContext(), idx, sharding, rank,\n+                                 allowSpmdShardingPropagationToOutput)) {\n+    setFuncResultSharding(func, idx, *adjustedSharding);\n   }\n-  setFuncResultSharding(func, idx, sharding);\n }\n \n CustomCallOp cloneCustomCallWithNewResultTypes(CustomCallOp op,"
        },
        {
            "sha": "4f64beca4c238ad4b0e9499e95435c769d6186d5",
            "filename": "third_party/xla/xla/service/spmd/shardy/utils.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f5b09c75033265543a86f14313eedd2a2b4dec8c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f5b09c75033265543a86f14313eedd2a2b4dec8c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.h?ref=f5b09c75033265543a86f14313eedd2a2b4dec8c",
            "patch": "@@ -80,6 +80,13 @@ bool hasKey(mlir::DictionaryAttr dictAttr, mlir::StringRef key);\n \n void loadAllRequiredDialects(mlir::MLIRContext* context);\n \n+// Adjusts the input sharding based on allowSpmdShardingPropagationToParameters\n+// flag.\n+void adjustInputSharding(\n+    mlir::func::FuncOp func, int idx, mlir::sdy::TensorShardingAttr sharding,\n+    int64_t rank,\n+    absl::Span<const bool> allowSpmdShardingPropagationToParameters);\n+\n // Adjusts the output sharding based on allowSpmdShardingPropagationToOutput\n // flag.\n void adjustOutputSharding("
        }
    ],
    "stats": {
        "total": 66,
        "additions": 47,
        "deletions": 19
    }
}