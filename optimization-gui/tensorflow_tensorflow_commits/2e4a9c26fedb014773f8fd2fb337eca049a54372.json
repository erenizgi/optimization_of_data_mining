{
    "author": "tensorflower-gardener",
    "message": "Don't wrap all host-to-host copies as host computation in HostOffloader.\nSome copies such as transpose copies can be done faster via an itermediate\non device copy.\n\nPiperOrigin-RevId: 805570561",
    "sha": "2e4a9c26fedb014773f8fd2fb337eca049a54372",
    "files": [
        {
            "sha": "d96b6a806db952cd80671765514aa725e3f7d92c",
            "filename": "third_party/xla/xla/hlo/transforms/host_offloader.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2e4a9c26fedb014773f8fd2fb337eca049a54372/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2e4a9c26fedb014773f8fd2fb337eca049a54372/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc?ref=2e4a9c26fedb014773f8fd2fb337eca049a54372",
            "patch": "@@ -256,6 +256,12 @@ absl::StatusOr<bool> HostOffloader::WalkDownHostMemoryOffloadPaths(\n         // compute happening on host memory, convert it to host compute.\n         need_to_wrap_instruction_as_host_compute = true;\n       }\n+    } else if (instruction->opcode() == HloOpcode::kCopy) {\n+      if (instruction->shape() == instruction->operand(0)->shape()) {\n+        need_to_wrap_instruction_as_host_compute = true;\n+      } else {\n+        // For copies that change layout, etc., don't rewrite here.\n+      }\n     } else {\n       // This is some unaccounted for instruction. Since it is unaccounted for,\n       // it must be something which is not legal to do with device compute."
        }
    ],
    "stats": {
        "total": 6,
        "additions": 6,
        "deletions": 0
    }
}