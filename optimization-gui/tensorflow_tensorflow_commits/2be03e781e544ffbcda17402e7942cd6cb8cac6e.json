{
    "author": "sergachev",
    "message": "PR #31395: Fix layout assignment for bitcast-convert operands.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31395\n\nBitcast-convert is expected to not change layout between input and output. For conversions between different type widths which add or remove one dimension this can be further specified as 'layouts do not introduce a transpose of the preserved dimensions'. These have not been handled in the output to input layout propagation so far.\n\nCopybara import of the project:\n\n--\n5c36f2949f791646ee63f64041dcc904abaefb67 by Ilia Sergachev <isergachev@nvidia.com>:\n\nFix layout assignment for bitcast-convert operands.\n\nBitcast-convert is expected to not change layout between input to\noutput. For conversions between different type widths which add or\nremove one dimension this can be further specified as 'layouts do not\nintroduce a transpose of the preserved dimensions'. These have not been\nhandled in the output to input layout propagation so far.\n\nMerging this change closes #31395\n\nPiperOrigin-RevId: 808154421",
    "sha": "2be03e781e544ffbcda17402e7942cd6cb8cac6e",
    "files": [
        {
            "sha": "e97d78136dca6336f01842ce7a4825d44e31e6e3",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2be03e781e544ffbcda17402e7942cd6cb8cac6e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2be03e781e544ffbcda17402e7942cd6cb8cac6e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc?ref=2be03e781e544ffbcda17402e7942cd6cb8cac6e",
            "patch": "@@ -497,8 +497,10 @@ e {\n   EXPECT_THAT(layout_assignment.Run(module.get()),\n               absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              GmockMatch(m::Copy(m::BitcastConvert(m::Parameter())\n-                                     .WithShape(S4, {3, 5, 2}, {2, 0, 1}))));\n+              GmockMatch(m::Copy(\n+                  m::BitcastConvert(\n+                      m::Copy(m::Parameter()).WithShape(S8, {3, 5}, {0, 1}))\n+                      .WithShape(S4, {3, 5, 2}, {2, 0, 1}))));\n }\n \n TEST_F(LayoutAssignmentTest, FftLayout) {"
        },
        {
            "sha": "0a5d11b18366ffe9b7ea4b7fba8524f34ceec97f",
            "filename": "third_party/xla/xla/service/layout_assignment.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 8,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2be03e781e544ffbcda17402e7942cd6cb8cac6e/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2be03e781e544ffbcda17402e7942cd6cb8cac6e/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment.cc?ref=2be03e781e544ffbcda17402e7942cd6cb8cac6e",
            "patch": "@@ -1150,8 +1150,8 @@ absl::StatusOr<HloInstruction*> LayoutAssignment::CreateCopyWithNewLayout(\n     return copy;\n   }\n \n-    return FailedPrecondition(\n-        \"Can only copy array and tuple shaped instructions\");\n+  return FailedPrecondition(\n+      \"Can only copy array and tuple shaped instructions\");\n }\n \n // Creates a copy of the given operand if the operand's layout does not match\n@@ -1935,13 +1935,28 @@ absl::Status LayoutAssignment::PropagateBufferConstraintToOperands(\n     }\n     if (!InstructionCanChangeLayoutInstance(instruction)) {\n       // Copy the layout to the operand.\n-      if (buffer.IsArray() && operand->shape().IsArray() &&\n-          operand->shape().dimensions().size() ==\n-              LayoutUtil::MinorToMajor(buffer_constraint.layout()).size()) {\n-        TF_RETURN_IF_ERROR(SetArrayOperandLayout(\n-            buffer_constraint.layout(), instruction, operand_no,\n-            /*mandatory=*/true, /*dfs=*/true, current_priority_));\n+      if (buffer.IsArray() && operand->shape().IsArray()) {\n+        if (operand->shape().dimensions().size() ==\n+            LayoutUtil::MinorToMajor(buffer_constraint.layout()).size()) {\n+          TF_RETURN_IF_ERROR(SetArrayOperandLayout(\n+              buffer_constraint.layout(), instruction, operand_no,\n+              /*mandatory=*/true, /*dfs=*/true, current_priority_));\n+        } else if (instruction->opcode() == HloOpcode::kBitcastConvert) {\n+          Shape shape = instruction->shape();\n+          if (operand->shape().dimensions().size() <\n+              instruction->shape().dimensions().size()) {\n+            shape = ShapeUtil::DeleteDimension(shape.dimensions().size() - 1,\n+                                               shape);\n+          } else {\n+            ShapeUtil::AppendMinorDimension(\n+                operand->shape().dimensions().back(), &shape);\n+          }\n+          TF_RETURN_IF_ERROR(SetArrayOperandLayout(\n+              shape.layout(), instruction, operand_no,\n+              /*mandatory=*/true, /*dfs=*/true, current_priority_));\n+        }\n       }\n+\n     } else if (instruction->opcode() == HloOpcode::kBroadcast) {\n       Layout layout =\n           GetBroadcastLayoutFromOutput(buffer_constraint.layout(), instruction);"
        },
        {
            "sha": "e39858a6daf7ce3e65dfaf49e897442d8fb5fbb1",
            "filename": "third_party/xla/xla/service/layout_assignment_test.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 12,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2be03e781e544ffbcda17402e7942cd6cb8cac6e/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2be03e781e544ffbcda17402e7942cd6cb8cac6e/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Flayout_assignment_test.cc?ref=2be03e781e544ffbcda17402e7942cd6cb8cac6e",
            "patch": "@@ -59,6 +59,18 @@ namespace {\n namespace m = xla::match;\n using ::testing::ElementsAre;\n \n+absl::Status AssignLayoutsToComputation(\n+    HloModule* m, ChannelLayoutConstraints* channel_constraints = nullptr) {\n+  if (!m->entry_computation_layout().result_layout().LayoutIsSet()) {\n+    m->mutable_entry_computation_layout()\n+        ->mutable_result_layout()\n+        ->SetToDefaultLayout();\n+  }\n+  LayoutAssignment layout_assignment(m->mutable_entry_computation_layout(),\n+                                     channel_constraints);\n+  return layout_assignment.Run(m).status();\n+}\n+\n class LayoutAssignmentTest : public HloTestBase {\n  protected:\n   void AssignLayouts(HloModule* m, ComputationLayout* entry_computation_layout,\n@@ -990,6 +1002,34 @@ TEST_F(LayoutAssignmentTest, CopySliceOperandToAvoidImplicitLayoutChange) {\n           m::Slice(m::Copy(m::Parameter(1)).WithShapeEqualTo(&shape_copy)))));\n }\n \n+TEST_F(LayoutAssignmentTest, BitcastConvertAddingDimensionDoesNotChangeLayout) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+e {\n+  a = f32[2,64]{0,1} parameter(0)\n+  b = u4[2,64,8]{1,2,0:E(4)} bitcast-convert(a)\n+})\"));\n+  TF_ASSERT_OK(AssignLayoutsToComputation(module.get()));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::BitcastConvert(\n+                  m::Copy(m::Parameter()).WithShape(F32, {2, 64}, {1, 0}))));\n+}\n+\n+TEST_F(LayoutAssignmentTest,\n+       BitcastConvertRemovingDimensionDoesNotChangeLayout) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+e {\n+  a = s8[16,3,2]{2,1,0} parameter(0)\n+  b = u16[16,3]{0,1} bitcast-convert(a)\n+})\"));\n+  TF_ASSERT_OK(AssignLayoutsToComputation(module.get()));\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(m::BitcastConvert(\n+          m::Copy(m::Parameter()).WithShape(S8, {16, 3, 2}, {2, 0, 1}))));\n+}\n+\n TEST_F(LayoutAssignmentTest, CopyDSliceOperandToAvoidImplicitLayoutChange) {\n   const char* module_str = R\"(\n     HloModule CopyDSliceOperandToAvoidImplicitLayoutChange\n@@ -1469,18 +1509,6 @@ ENTRY %MixedHostDeviceResult {\n   ExpectTupleLayoutIs(result_shape, {{1, 0}, {0, 1}});\n }\n \n-absl::Status AssignLayoutsToComputation(\n-    HloModule* m, ChannelLayoutConstraints* channel_constraints = nullptr) {\n-  if (!m->entry_computation_layout().result_layout().LayoutIsSet()) {\n-    m->mutable_entry_computation_layout()\n-        ->mutable_result_layout()\n-        ->SetToDefaultLayout();\n-  }\n-  LayoutAssignment layout_assignment(m->mutable_entry_computation_layout(),\n-                                     channel_constraints);\n-  return layout_assignment.Run(m).status();\n-}\n-\n TEST_F(LayoutAssignmentTest, OverwriteDiamondShapedConstraintsX) {\n   // Check that we handle a diamond-shaped graph correctly.\n   //      transpose"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 67,
        "deletions": 22
    }
}