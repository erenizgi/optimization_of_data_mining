{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 845757249",
    "sha": "f40e47ecab285fc5117a468e540ab09e4374b64d",
    "files": [
        {
            "sha": "993b98e61dc0ed57cf7522e4d71127a70af48da2",
            "filename": "tensorflow/compiler/tf2xla/light_outside_compilation_kernels_for_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 32,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Flight_outside_compilation_kernels_for_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Flight_outside_compilation_kernels_for_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Flight_outside_compilation_kernels_for_test.cc?ref=f40e47ecab285fc5117a468e540ab09e4374b64d",
            "patch": "@@ -64,14 +64,15 @@ class TestStaticTfOp : public OpKernel {\n \n     // Just pass the value through.\n     uint64_t size = input.AllocatedBytes();\n-    se::DeviceMemoryBase gpu_dst{out_tensor->data(), size};\n+    stream_executor::DeviceAddressBase gpu_dst{out_tensor->data(), size};\n     se::Stream* stream = ctx->op_device_context()->stream();\n \n-    OP_REQUIRES_OK(ctx,\n-                   stream->MemcpyD2D(\n-                       /*gpu_dst=*/&gpu_dst,\n-                       /*gpu_src=*/se::DeviceMemoryBase{input.data(), size},\n-                       /*size=*/input.AllocatedBytes()));\n+    OP_REQUIRES_OK(\n+        ctx,\n+        stream->MemcpyD2D(\n+            /*gpu_dst=*/&gpu_dst,\n+            /*gpu_src=*/stream_executor::DeviceAddressBase{input.data(), size},\n+            /*size=*/input.AllocatedBytes()));\n   }\n };\n \n@@ -105,21 +106,23 @@ class TestStaticMultipleOutputTfOp : public OpKernel {\n \n     // Just pass the value through.\n     uint64_t size = input.AllocatedBytes();\n-    se::DeviceMemoryBase gpu_dst1{out_tensor1->data(), size};\n-    se::DeviceMemoryBase gpu_dst2{out_tensor2->data(), size};\n+    stream_executor::DeviceAddressBase gpu_dst1{out_tensor1->data(), size};\n+    stream_executor::DeviceAddressBase gpu_dst2{out_tensor2->data(), size};\n     se::Stream* stream =\n         ctx->device()->tensorflow_accelerator_device_info()->stream;\n \n-    OP_REQUIRES_OK(ctx,\n-                   stream->MemcpyD2D(\n-                       /*gpu_dst=*/&gpu_dst1,\n-                       /*gpu_src=*/se::DeviceMemoryBase{input.data(), size},\n-                       /*size=*/input.AllocatedBytes()));\n-    OP_REQUIRES_OK(ctx,\n-                   stream->MemcpyD2D(\n-                       /*gpu_dst=*/&gpu_dst2,\n-                       /*gpu_src=*/se::DeviceMemoryBase{input.data(), size},\n-                       /*size=*/input.AllocatedBytes()));\n+    OP_REQUIRES_OK(\n+        ctx,\n+        stream->MemcpyD2D(\n+            /*gpu_dst=*/&gpu_dst1,\n+            /*gpu_src=*/stream_executor::DeviceAddressBase{input.data(), size},\n+            /*size=*/input.AllocatedBytes()));\n+    OP_REQUIRES_OK(\n+        ctx,\n+        stream->MemcpyD2D(\n+            /*gpu_dst=*/&gpu_dst2,\n+            /*gpu_src=*/stream_executor::DeviceAddressBase{input.data(), size},\n+            /*size=*/input.AllocatedBytes()));\n   }\n };\n \n@@ -165,12 +168,12 @@ class TestDynamicTfOp : public OpKernel {\n     se::Stream* stream =\n         ctx->device()->tensorflow_accelerator_device_info()->stream;\n \n-    se::DeviceMemoryBase gpu_dst{out_tensor->data(), size_to_cpy};\n+    stream_executor::DeviceAddressBase gpu_dst{out_tensor->data(), size_to_cpy};\n     OP_REQUIRES_OK(ctx, stream->MemcpyD2D(\n                             /*gpu_dst=*/&gpu_dst,\n                             /*gpu_src=*/\n-                            se::DeviceMemoryBase{input.data(),\n-                                                 static_cast<uint64_t>(size)},\n+                            stream_executor::DeviceAddressBase{\n+                                input.data(), static_cast<uint64_t>(size)},\n                             /*size=*/size_to_cpy));\n   }\n \n@@ -211,7 +214,7 @@ class DynamicMultidimOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     TensorShape output_shape;\n-    auto vec = ctx->input(0).flat<int32>();\n+    auto vec = ctx->input(0).flat<int32_t>();\n     for (int i = 0; i < vec.size(); i++) {\n       OP_REQUIRES_OK(ctx, output_shape.AddDimWithStatus(vec(i)));\n     }\n@@ -225,8 +228,8 @@ class DynamicMultidimOp : public OpKernel {\n     for (int i = 0; i < output_shape.num_elements(); i++) {\n       host_data[i] = 1.0;\n     }\n-    se::DeviceMemoryBase gpu_dst{out_tensor->data(),\n-                                 static_cast<uint64_t>(num_elements)};\n+    stream_executor::DeviceAddressBase gpu_dst{\n+        out_tensor->data(), static_cast<uint64_t>(num_elements)};\n \n     se::Stream* stream =\n         ctx->device()->tensorflow_accelerator_device_info()->stream;\n@@ -302,10 +305,10 @@ class TestTfMustBeConstantOp : public OpKernel {\n     TF_CHECK_OK(ctx->allocate_temp(input.dtype(), input.shape(), &tmp,\n                                    pinned_alloc_attrs));\n \n-    OP_REQUIRES_OK(\n-        ctx, stream->Memcpy(tmp.data(),\n-                            se::DeviceMemoryBase{input.data(), allocated_size},\n-                            allocated_size));\n+    OP_REQUIRES_OK(ctx, stream->Memcpy(tmp.data(),\n+                                       stream_executor::DeviceAddressBase{\n+                                           input.data(), allocated_size},\n+                                       allocated_size));\n \n     OP_REQUIRES_OK(ctx, stream->BlockHostUntilDone());\n \n@@ -316,8 +319,8 @@ class TestTfMustBeConstantOp : public OpKernel {\n     Tensor* out_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", ctx->input(0).shape(),\n                                              &out_tensor));\n-    se::DeviceMemoryBase gpu_dst{out_tensor->data(),\n-                                 static_cast<uint64_t>(allocated_size)};\n+    stream_executor::DeviceAddressBase gpu_dst{\n+        out_tensor->data(), static_cast<uint64_t>(allocated_size)};\n     OP_REQUIRES_OK(ctx, stream->Memcpy(&gpu_dst, tmp.data(), allocated_size));\n   }\n };\n@@ -361,11 +364,12 @@ class TestDynamicTfWithBoundOp : public OpKernel {\n \n     se::Stream* stream =\n         ctx->device()->tensorflow_accelerator_device_info()->stream;\n-    se::DeviceMemoryBase gpu_dst{out_tensor->data(), size_to_cpy};\n+    stream_executor::DeviceAddressBase gpu_dst{out_tensor->data(), size_to_cpy};\n     OP_REQUIRES_OK(\n         ctx, stream->MemcpyD2D(\n                  /*gpu_dst=*/&gpu_dst,\n-                 /*gpu_src=*/se::DeviceMemoryBase{input.data(), size_to_cpy},\n+                 /*gpu_src=*/\n+                 stream_executor::DeviceAddressBase{input.data(), size_to_cpy},\n                  /*size=*/size_to_cpy));\n   }\n "
        },
        {
            "sha": "e8c7dc1a579b6baf9a89fb788889fe490abac79c",
            "filename": "tensorflow/compiler/tf2xla/literal_util_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fliteral_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fliteral_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fliteral_util_test.cc?ref=f40e47ecab285fc5117a468e540ab09e4374b64d",
            "patch": "@@ -52,9 +52,9 @@ TEST(LiteralUtil, LiteralToHostTensor) {\n template <class T>\n using LiteralUtilTest = ::testing::Test;\n using Types =\n-    ::testing::Types<std::pair<int8, qint8>, std::pair<uint8, quint8>,\n-                     std::pair<int16, qint16>, std::pair<uint16, quint16>,\n-                     std::pair<int32, qint32>>;\n+    ::testing::Types<std::pair<int8_t, qint8>, std::pair<uint8_t, quint8>,\n+                     std::pair<int16_t, qint16>, std::pair<uint16_t, quint16>,\n+                     std::pair<int32_t, qint32>>;\n \n TYPED_TEST_SUITE(LiteralUtilTest, Types);\n "
        },
        {
            "sha": "114905925cbf20602ec24406c8efe4a1ff5d0a04",
            "filename": "tensorflow/compiler/tf2xla/mlir_tf2xla.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fmlir_tf2xla.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fmlir_tf2xla.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fmlir_tf2xla.cc?ref=f40e47ecab285fc5117a468e540ab09e4374b64d",
            "patch": "@@ -127,7 +127,7 @@ absl::Status ConvertGraphDefToXlaViaMlir(\n   // with a placeholder node that contains a single output.\n   FunctionLibraryDefinition flib_def(OpRegistry::Global(), graph_def.library());\n   std::unique_ptr<Graph> graph(new Graph(flib_def));\n-  std::unordered_map<string, string> feed_name_remap;\n+  std::unordered_map<std::string, std::string> feed_name_remap;\n   TF_RETURN_IF_ERROR(AddPlaceholdersForFeeds(config, graph->op_registry(),\n                                              &feed_name_remap, &graph_def));\n "
        },
        {
            "sha": "e7925a011f9eb56efaad206ee54bf3f77412014f",
            "filename": "tensorflow/compiler/tf2xla/xla_compilation_device.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compilation_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compilation_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compilation_device.cc?ref=f40e47ecab285fc5117a468e540ab09e4374b64d",
            "patch": "@@ -45,7 +45,8 @@ class XlaCompilationAllocator : public Allocator {\n     // Regardless of the size requested, always allocates an XlaExpression.\n     // Respects the alignment request because there is alignment checking even\n     // for Tensors whose data is never accessed.\n-    void* p = port::AlignedMalloc(sizeof(XlaExpression), alignment);\n+    void* p = tsl::port::AlignedMalloc(\n+        sizeof(XlaExpression), static_cast<std::align_val_t>(alignment));\n     XlaExpression* expression = reinterpret_cast<XlaExpression*>(p);\n     new (expression) XlaExpression();\n     return expression;"
        },
        {
            "sha": "b9abd5006a958a5b4ee8508b3c412ec34c5d5f85",
            "filename": "tensorflow/compiler/tf2xla/xla_compiler.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f40e47ecab285fc5117a468e540ab09e4374b64d/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.h?ref=f40e47ecab285fc5117a468e540ab09e4374b64d",
            "patch": "@@ -226,7 +226,7 @@ class XlaCompiler {\n     // This must be a shared_ptr, as this is passed all the way down to the\n     // cluster compilation. This allows asynchronous compilation to hold a\n     // reference until the compilation is finished.\n-    std::shared_ptr<se::DeviceMemoryAllocator> device_allocator;\n+    std::shared_ptr<stream_executor::DeviceAddressAllocator> device_allocator;\n \n     // Alias input and output buffers for parameters that are passed-through XLA\n     // modules without being changed."
        }
    ],
    "stats": {
        "total": 81,
        "additions": 43,
        "deletions": 38
    }
}