{
    "author": "matthiaskramm",
    "message": "Fix memory leak in MakeShapesInfo() and re-enable layout conversion.\n\nPiperOrigin-RevId: 843790085",
    "sha": "619f7362044e25c37127e50edcba86cc627f94ea",
    "files": [
        {
            "sha": "08592d5a9c7776ede62859f6e5b5dfbd2c1db0d2",
            "filename": "third_party/xla/xla/pjrt/c_api_client/pjrt_c_api_client.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 13,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/619f7362044e25c37127e50edcba86cc627f94ea/third_party%2Fxla%2Fxla%2Fpjrt%2Fc_api_client%2Fpjrt_c_api_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/619f7362044e25c37127e50edcba86cc627f94ea/third_party%2Fxla%2Fxla%2Fpjrt%2Fc_api_client%2Fpjrt_c_api_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc_api_client%2Fpjrt_c_api_client.cc?ref=619f7362044e25c37127e50edcba86cc627f94ea",
            "patch": "@@ -1013,15 +1013,15 @@ absl::Status PjRtCApiClient::DmaUnmap(void* data) {\n // Helper struct and method used to serialize shapes past the C API boundary.\n struct ShapesInfo {\n   std::vector<size_t> shape_num_dims;\n-  std::vector<PJRT_Buffer_MemoryLayout*> layout_list;\n+  std::vector<std::optional<pjrt::BufferMemoryLayoutData>> layout_list;\n   std::vector<const int64_t*> num_dims;\n   std::vector<PJRT_Buffer_Type> element_type_list;\n };\n \n ShapesInfo MakeShapesInfo(absl::Span<const Shape> shapes) {\n   std::vector<size_t> shape_num_dims;\n   shape_num_dims.reserve(shapes.size());\n-  std::vector<PJRT_Buffer_MemoryLayout*> layout_list;\n+  std::vector<std::optional<pjrt::BufferMemoryLayoutData>> layout_list;\n   layout_list.reserve(shapes.size());\n   std::vector<const int64_t*> num_dims;\n   num_dims.reserve(shapes.size());\n@@ -1034,15 +1034,20 @@ ShapesInfo MakeShapesInfo(absl::Span<const Shape> shapes) {\n     num_dims.push_back(shapes[i].dimensions().data());\n     element_type_list.push_back(\n         pjrt::ConvertToPjRtBufferType(shapes[i].element_type()));\n-    // TODO(b/434246423): Enable this once ASAN failure is fixed.\n-    // if (shapes[i].has_layout()) {\n-    //   // this is messed up\n-    //   auto& layout = shapes[i].layout();\n-    //   TF_ASSIGN_OR_RETURN(\n-    //       pjrt::BufferMemoryLayoutData c_layout_data,\n-    //       pjrt::ConvertToBufferMemoryLayoutData(layout));\n-    //   layout_list.push_back(&(c_layout_data.c_layout));\n-    layout_list.push_back(nullptr);\n+\n+    if (shapes[i].has_layout()) {\n+      auto& layout = shapes[i].layout();\n+      absl::StatusOr<pjrt::BufferMemoryLayoutData> c_layout_data =\n+          pjrt::ConvertToBufferMemoryLayoutData(layout);\n+      if (c_layout_data.ok()) {\n+        layout_list.push_back(std::optional<pjrt::BufferMemoryLayoutData>(\n+            std::move(*c_layout_data)));\n+      } else {\n+        layout_list.push_back({});\n+      }\n+    } else {\n+      layout_list.push_back({});\n+    }\n   }\n \n   return ShapesInfo{\n@@ -1088,7 +1093,16 @@ PjRtCApiClient::MakeCrossHostReceiveBuffers(\n   args.shape_num_dims = shapes_info.shape_num_dims.data();\n   args.num_dims = shapes_info.num_dims.data();\n   args.element_types = shapes_info.element_type_list.data();\n-  args.layouts = shapes_info.layout_list.data();\n+\n+  std::vector<PJRT_Buffer_MemoryLayout*> layout_list;\n+  for (int i = 0; i < shapes_info.layout_list.size(); i++) {\n+    if (shapes_info.layout_list[i].has_value()) {\n+      layout_list.push_back(&shapes_info.layout_list[i]->c_layout);\n+    } else {\n+      layout_list.push_back(nullptr);\n+    }\n+  }\n+  args.layouts = layout_list.data();\n \n   args.notifier = pjrt::CppCrossHostRecvNotifierToC(c_api, std::move(notifier));\n   args.device = tensorflow::down_cast<PjRtCApiDevice*>(device)->c_device();\n@@ -1179,7 +1193,16 @@ PjRtCApiClient::CrossHostReceiveBuffers(\n   args.shape_num_dims = shapes_info.shape_num_dims.data();\n   args.num_dims = shapes_info.num_dims.data();\n   args.element_types = shapes_info.element_type_list.data();\n-  args.layouts = shapes_info.layout_list.data();\n+\n+  std::vector<PJRT_Buffer_MemoryLayout*> layout_list;\n+  for (int i = 0; i < shapes_info.layout_list.size(); i++) {\n+    if (shapes_info.layout_list[i].has_value()) {\n+      layout_list.push_back(&shapes_info.layout_list[i]->c_layout);\n+    } else {\n+      layout_list.push_back(nullptr);\n+    }\n+  }\n+  args.layouts = layout_list.data();\n \n   args.device = tensorflow::down_cast<PjRtCApiDevice*>(device)->c_device();\n   args.src_global_device_ids = src_global_device_ids.data();"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 36,
        "deletions": 13
    }
}