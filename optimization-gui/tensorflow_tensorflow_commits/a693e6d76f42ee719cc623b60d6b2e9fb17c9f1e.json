{
    "author": "Cjkkkk",
    "message": "PR #34789: [XLA:GPU] Fix cuDNN SDPA test to use 0 as workspace size to work universally on all archs\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34789\n\nüìù Summary of Changes\nuse 0 as default workspace size and query later so it works universally on all archs, cuDNN paged attention reference doesn't do this like other cuDNN sdpa tests, it fails on B200 in NV internal CI. Therefore the fix.\n\nüéØ Justification\nuse 0 as default workspace size and query later so it works universally on all archs, cuDNN paged attention reference doesn't do this like other cuDNN sdpa tests, it fails on B200 in NV internal CI. Therefore the fix.\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nNone\n\nüß™ Unit Tests:\nNone\n\nüß™ Execution Tests:\nNone\n\nCopybara import of the project:\n\n--\n7c53e935fcb424970da1ffed4c18a95e08835d57 by Cjkkkk <ske@nvidia.com>:\n\nuse 0 as workspace to work universally on all arch\n\nMerging this change closes #34789\n\nPiperOrigin-RevId: 841696508",
    "sha": "a693e6d76f42ee719cc623b60d6b2e9fb17c9f1e",
    "files": [
        {
            "sha": "8a0329b6a1fbf6f5c8d5909045ac2b82255e09ec",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_fused_mha_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a693e6d76f42ee719cc623b60d6b2e9fb17c9f1e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a693e6d76f42ee719cc623b60d6b2e9fb17c9f1e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc?ref=a693e6d76f42ee719cc623b60d6b2e9fb17c9f1e",
            "patch": "@@ -1372,7 +1372,7 @@ class FlashAttentionPagedAttention : public MultiHeadedAttentionTest {\n     ENTRY %main.7 (Arg_0.1: bf16[1,128,2,128], Arg_1.2: bf16[1,128,2,128]) -> bf16[1,128,2,128] {\n       %Arg_1.2 = bf16[1,128,2,128]{3,2,1,0} parameter(1)\n       %Arg_0.1 = bf16[1,128,2,128]{3,2,1,0} parameter(0)\n-      %custom-call.3 = (bf16[1,2,128,128]{3,1,2,0}, u8[256]{0}) custom-call(%Arg_0.1, %Arg_1.2, %Arg_1.2), custom_call_target=\"__cudnn$fmhaSoftmax\", operand_layout_constraints={bf16[1,128,2,128]{3,2,1,0}, bf16[1,128,2,128]{3,2,1,0}, bf16[1,128,2,128]{3,2,1,0}}, api_version=API_VERSION_STATUS_RETURNING, backend_config={\"cudnn_fmha_backend_config\": {\"algorithm\": {\"algo_id\": \"0\", \"math_type\": \"TENSOR_OP_MATH\", \"tuning_knobs\": {\"17\": \"1\", \"24\": \"0\"}, \"workspace_size\": \"0\"}, \"fmha_scale\": 1.0, \"intermediate_tensor_shape\": {\"element_type\": \"BF16\", \"dimensions\": [\"1\", \"2\", \"128\", \"128\"], \"tuple_shapes\": [], \"layout\": {\"dim_level_types\": [], \"dim_unique\": [], \"dim_ordered\": [], \"minor_to_major\": [\"3\", \"2\", \"1\", \"0\"], \"tiles\": [], \"element_size_in_bits\": \"0\", \"memory_space\": \"0\", \"index_primitive_type\": \"PRIMITIVE_TYPE_INVALID\", \"pointer_primitive_type\": \"PRIMITIVE_TYPE_INVALID\", \"dynamic_shape_metadata_prefix_bytes\": \"0\"}, \"is_dynamic_dimension\": [false, false, false, false]}, \"is_flash_attention\": true, \"mask_type\": \"NO_MASK\", \"bmm1_dot_dimension_numbers\": {\"lhs_contracting_dimensions\": [\"3\"], \"rhs_contracting_dimensions\": [\"3\"], \"lhs_batch_dimensions\": [\"0\", \"2\"], \"rhs_batch_dimensions\": [\"0\", \"2\"]}, \"bmm2_dot_dimension_numbers\": {\"lhs_contracting_dimensions\": [\"3\"], \"rhs_contracting_dimensions\": [\"1\"], \"lhs_batch_dimensions\": [\"0\", \"1\"], \"rhs_batch_dimensions\": [\"0\", \"2\"]}, \"dropout_rate\": 0, \"seed\": 42, \"sliding_window_length\": 0, \"max_seg_per_batch\": 1, \"is_paged_attention\": false}}\n+      %custom-call.3 = (bf16[1,2,128,128]{3,1,2,0}, u8[0]{0}) custom-call(%Arg_0.1, %Arg_1.2, %Arg_1.2), custom_call_target=\"__cudnn$fmhaSoftmax\", operand_layout_constraints={bf16[1,128,2,128]{3,2,1,0}, bf16[1,128,2,128]{3,2,1,0}, bf16[1,128,2,128]{3,2,1,0}}, api_version=API_VERSION_STATUS_RETURNING, backend_config={\"cudnn_fmha_backend_config\": {\"algorithm\": {\"algo_id\": \"0\", \"math_type\": \"TENSOR_OP_MATH\", \"tuning_knobs\": {\"17\": \"1\", \"24\": \"0\"}, \"workspace_size\": \"0\"}, \"fmha_scale\": 1.0, \"intermediate_tensor_shape\": {\"element_type\": \"BF16\", \"dimensions\": [\"1\", \"2\", \"128\", \"128\"], \"tuple_shapes\": [], \"layout\": {\"dim_level_types\": [], \"dim_unique\": [], \"dim_ordered\": [], \"minor_to_major\": [\"3\", \"2\", \"1\", \"0\"], \"tiles\": [], \"element_size_in_bits\": \"0\", \"memory_space\": \"0\", \"index_primitive_type\": \"PRIMITIVE_TYPE_INVALID\", \"pointer_primitive_type\": \"PRIMITIVE_TYPE_INVALID\", \"dynamic_shape_metadata_prefix_bytes\": \"0\"}, \"is_dynamic_dimension\": [false, false, false, false]}, \"is_flash_attention\": true, \"mask_type\": \"NO_MASK\", \"bmm1_dot_dimension_numbers\": {\"lhs_contracting_dimensions\": [\"3\"], \"rhs_contracting_dimensions\": [\"3\"], \"lhs_batch_dimensions\": [\"0\", \"2\"], \"rhs_batch_dimensions\": [\"0\", \"2\"]}, \"bmm2_dot_dimension_numbers\": {\"lhs_contracting_dimensions\": [\"3\"], \"rhs_contracting_dimensions\": [\"1\"], \"lhs_batch_dimensions\": [\"0\", \"1\"], \"rhs_batch_dimensions\": [\"0\", \"2\"]}, \"dropout_rate\": 0, \"seed\": 42, \"sliding_window_length\": 0, \"max_seg_per_batch\": 1, \"is_paged_attention\": false}}\n       %get-tuple-element.4.0 = bf16[1,2,128,128]{3,1,2,0} get-tuple-element(%custom-call.3), index=0\n       ROOT %bitcast.6.0 = bf16[1,128,2,128]{3,2,1,0} bitcast(%get-tuple-element.4.0)\n     }"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}