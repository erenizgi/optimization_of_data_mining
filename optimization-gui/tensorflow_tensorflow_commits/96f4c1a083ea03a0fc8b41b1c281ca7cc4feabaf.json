{
    "author": "hawkinsp",
    "message": "Reverts 4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c\n\nPiperOrigin-RevId: 832484477",
    "sha": "96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf",
    "files": [
        {
            "sha": "cf93b80b8d2e6ef6ef6bfb0bb902d26ef27aa3b0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #include <memory>\n #include <string>\n #include <utility>\n+#include <variant>\n #include <vector>\n \n #include <gmock/gmock.h>\n@@ -1565,9 +1566,7 @@ ENTRY e {\n   EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{/*aabs=*/1e-6, /*arel=*/1e-6}));\n }\n \n-// Dynamic slice is not supported by the generic Triton emitter yet and disabled\n-// in the triton gemm fusion pass.\n-TEST_F(TritonGemmTest, DISABLED_DynamicSliceIsSupportedInLhsEndToEnd) {\n+TEST_F(TritonGemmTest, DynamicSliceIsSupportedInLhsEndToEnd) {\n   // The select is used to restrict the start index to values that make sense.\n   // If it was constant, then the dynamic-slice would be optimized to slice. It\n   // is not strictly needed, because we also support clamping the indices."
        },
        {
            "sha": "0ceae13fce9ec4fa0b81913cc9c390fa9988789e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 66,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n \n #include <string>\n+#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -40,6 +41,7 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -260,13 +262,12 @@ bool IsTritonSupportedElementwise(HloOpcode opcode, PrimitiveType element_type,\n }\n \n CodegenDecision IsTritonSupportedInstructionImpl(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n-    bool is_fused_computation);\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version);\n \n // Filters Reduces which can be handled using Triton.\n CodegenDecision CanTritonHandleReduce(\n     const HloReduceInstruction& reduce,\n-    const se::GpuComputeCapability& gpu_version, bool is_fused_computation) {\n+    const se::GpuComputeCapability& gpu_version) {\n   if (reduce.shape().element_type() == PrimitiveType::F8E4M3FN ||\n       reduce.shape().element_type() == PrimitiveType::F8E5M2) {\n     return CodegenDecision::Forbid(\n@@ -275,9 +276,7 @@ CodegenDecision CanTritonHandleReduce(\n \n   bool is_triton_supported_reduction_computation = absl::c_all_of(\n       reduce.to_apply()->instructions(), [&](const HloInstruction* instr) {\n-        return IsTritonSupportedInstructionImpl(*instr, gpu_version,\n-                                                is_fused_computation)\n-            .CanFuse();\n+        return IsTritonSupportedInstructionImpl(*instr, gpu_version).CanFuse();\n       });\n   if (!is_triton_supported_reduction_computation) {\n     return CodegenDecision::Forbid(\n@@ -343,14 +342,6 @@ CodegenDecision AreTypesSupportedByAlgUnsetDot(\n         \"Dot operation only supports F64 result type for F64 input type.\");\n   }\n \n-  if (input_type == F8E5M2 || result_type == F8E5M2) {\n-    if (auto* cuda_cc = gpu_version.cuda_compute_capability();\n-        cuda_cc && !cuda_cc->IsAtLeastAmpere()) {\n-      return CodegenDecision::Forbid(\n-          \"Dot operation for F8E5M2 is not supported before Ampere.\");\n-    }\n-  }\n-\n   if (input_type == F8E4M3FN || result_type == F8E4M3FN) {\n     if (auto* cuda_cc = gpu_version.cuda_compute_capability();\n         cuda_cc && !cuda_cc->IsAtLeastHopper()) {\n@@ -455,33 +446,25 @@ CodegenDecision AreDotAlgorithmInputAndOutputConversionsSupported(\n }\n \n CodegenDecision IsTritonSupportedDot(\n-    const HloDotInstruction& dot, const se::GpuComputeCapability& gpu_version,\n-    bool is_fused_computation) {\n-  // Check that dot matches the pattern expected by the emitter: is in a nested\n-  // GEMM fusion and its operands are fusions.\n-\n-  const HloInstruction* lhs = dot.operand(0);\n-  const HloInstruction* rhs = dot.operand(1);\n-  if (is_fused_computation) {\n-    if (!IsInTritonNestedGemmFusion(dot)) {\n-      return CodegenDecision::Forbid(\n-          \"Dot operation is only supported in nested GEMM fusions.\");\n-    }\n-    if (lhs->opcode() != HloOpcode::kFusion ||\n-        rhs->opcode() != HloOpcode::kFusion) {\n-      return CodegenDecision::Forbid(\n-          \"Only operands that are fusions are supported.\");\n-    }\n+    const HloDotInstruction& dot, const se::GpuComputeCapability& gpu_version) {\n+  if (!IsInTritonNestedGemmFusion(dot)) {\n+    return CodegenDecision::Forbid(\n+        \"Dot operation is only supported in nested GEMM fusions.\");\n   }\n-\n   PrimitiveType result_type = dot.shape().element_type();\n-  const Shape& lhs_shape = lhs->shape();\n-  const Shape& rhs_shape = rhs->shape();\n+  const Shape& lhs_shape = dot.operand(0)->shape();\n+  const Shape& rhs_shape = dot.operand(1)->shape();\n   PrimitiveType lhs_type = lhs_shape.element_type();\n   PrimitiveType rhs_type = rhs_shape.element_type();\n \n-  if (lhs_type != rhs_type && !(primitive_util::IsF8Type(lhs_type) &&\n-                                primitive_util::IsF8Type(rhs_type))) {\n+  if (dot.operand(0)->opcode() != HloOpcode::kFusion ||\n+      dot.operand(1)->opcode() != HloOpcode::kFusion) {\n+    return CodegenDecision::Forbid(\n+        \"Only operands that are fusions are supported.\");\n+  }\n+\n+  // TODO(b/393299275): add support tests for mixed types.\n+  if (lhs_type != rhs_type) {\n     return CodegenDecision::Forbid(\n         \"Dot operation only supports same types for lhs and rhs.\");\n   }\n@@ -568,31 +551,27 @@ CodegenDecision IsTritonSupportedFusion(\n                    \" is not supported: \", decision.Explain()));\n }\n \n-CodegenDecision IsTritonSupportedConcatenate(const HloInstruction& hlo,\n-                                             bool is_fused_computation) {\n+CodegenDecision IsTritonSupportedConcatenate(const HloInstruction& hlo) {\n   CHECK(hlo.opcode() == HloOpcode::kConcatenate);\n-  if (is_fused_computation) {\n-    if (!IsInTritonNestedGemmFusion(hlo)) {\n-      return CodegenDecision::Forbid(\n-          \"Only concatenates in nested GEMM fusions are supported.\");\n-    }\n-    // TODO(b/393299275): remove this operand filter once migration is\n-    // complete and priority fusion can produce nests.\n-    if (absl::c_any_of(hlo.operands(), [](const HloInstruction* operand) {\n-          return operand->opcode() != HloOpcode::kFusion;\n-        })) {\n-      return CodegenDecision::Forbid(\n-          \"Only support concatenates with nested GEMM fusions as a \"\n-          \"parameter.\");\n-    }\n+  if (!IsInTritonNestedGemmFusion(hlo)) {\n+    return CodegenDecision::Forbid(\n+        \"Only concatenates in nested GEMM fusions are supported.\");\n+  }\n+  // TODO(b/393299275): remove this operand filter once migration is\n+  // complete and priority fusion can produce nests.\n+  if (absl::c_any_of(hlo.operands(), [](const HloInstruction* operand) {\n+        return operand->opcode() != HloOpcode::kFusion;\n+      })) {\n+    return CodegenDecision::Forbid(\n+        \"Only support concatenates with nested GEMM fusions as a \"\n+        \"parameter.\");\n   }\n   return CodegenDecision(hlo.shape().element_type() != S4,\n                          \"S4 is not supported.\");\n }\n \n CodegenDecision IsTritonSupportedInstructionImpl(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n-    bool is_fused_computation) {\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version) {\n   if (internal::IsTritonUnsupportedOpcode(instr.opcode())) {\n     return CodegenDecision::Forbid(\n         absl::StrCat(\"Unsupported opcode \", HloOpcodeString(instr.opcode())));\n@@ -624,7 +603,7 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n   }\n \n   if (instr.opcode() == HloOpcode::kConcatenate) {\n-    return IsTritonSupportedConcatenate(instr, is_fused_computation);\n+    return IsTritonSupportedConcatenate(instr);\n   }\n \n   // Special handling for the kPad instruction. Right now we only support \"high\"\n@@ -668,7 +647,7 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n   switch (instr.opcode()) {\n     case HloOpcode::kReduce: {\n       return CanTritonHandleReduce(*Cast<HloReduceInstruction>(&instr),\n-                                   gpu_version, is_fused_computation);\n+                                   gpu_version);\n     }\n     case HloOpcode::kParameter:\n       return CodegenDecision::Allow();\n@@ -692,8 +671,8 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n       return CodegenDecision(instr.shape().element_type() != S4,\n                              \"S4 is not supported.\");\n     case HloOpcode::kDot:\n-      return IsTritonSupportedDot(*Cast<HloDotInstruction>(&instr), gpu_version,\n-                                  is_fused_computation);\n+      return IsTritonSupportedDot(*Cast<HloDotInstruction>(&instr),\n+                                  gpu_version);\n     case HloOpcode::kFusion:\n       return IsTritonSupportedFusion(*Cast<HloFusionInstruction>(&instr),\n                                      gpu_version);\n@@ -764,10 +743,9 @@ absl::Status EnsureTritonSupportsComputeCapability(\n }\n \n CodegenDecision IsTritonSupportedInstruction(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n-    bool is_fused_computation) {\n-  CodegenDecision decision = IsTritonSupportedInstructionImpl(\n-      instr, gpu_version, is_fused_computation);\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version) {\n+  CodegenDecision decision =\n+      IsTritonSupportedInstructionImpl(instr, gpu_version);\n   VLOG(2) << absl::StrCat(\"IsTritonSupportedInstruction: \", instr.ToString(),\n                           \" \",\n                           (decision.CanFuse() ? \"yes\" : decision.Explain()));\n@@ -776,8 +754,7 @@ CodegenDecision IsTritonSupportedInstruction(\n \n CodegenDecision IsTritonSupportedComputation(\n     const HloComputation& computation,\n-    const se::GpuComputeCapability& gpu_compute_capability,\n-    bool is_fused_computation) {\n+    const se::GpuComputeCapability& gpu_compute_capability) {\n   VLOG(3) << \"IsTritonSupportedComputation: \" << computation.ToString();\n   for (const auto* instruction : computation.instructions()) {\n     // TODO(b/452478982): This check can be removed if we support Tuple ops\n@@ -788,8 +765,8 @@ CodegenDecision IsTritonSupportedComputation(\n       // supported for fusion roots.\n       continue;\n     }\n-    if (CodegenDecision can_codegen = IsTritonSupportedInstruction(\n-            *instruction, gpu_compute_capability, is_fused_computation);\n+    if (CodegenDecision can_codegen =\n+            IsTritonSupportedInstruction(*instruction, gpu_compute_capability);\n         !can_codegen) {\n       return can_codegen;\n     }"
        },
        {
            "sha": "de2c15c6c470118438748cd944d901c410c6caa0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.h",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h?ref=96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/shape.h\"\n@@ -40,32 +39,26 @@ absl::Status EnsureTritonSupportsComputeCapability(\n     const se::GpuComputeCapability& gpu_compute_capability);\n \n // Return `CodegenDecision`'s equivalent of `true` if the parameter instruction\n-// is supported by the Triton emitters for the given compute capability/\n-// `is_fused_computation` indicates that fusions are already formed and\n-// we should run additional checks for them.\n+// is supported by the Triton emitters for the given compute capability. Note\n+// that this function makes no assumption about what happens if\n+// `FloatNormalization` is run, unlike the legacy Triton utils.\n //\n-// TODO(b/393299275): remove this comment.\n // Note: this function is entirely dissociated from the legacy Triton emitters.\n // If you intend to add a feature to the legacy Triton emitters (which you\n // probably shouldn't), use `legacy_triton::IsTritonSupportedInstruction`\n // instead.\n CodegenDecision IsTritonSupportedInstruction(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n-    bool is_fused_computation = true);\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version);\n \n // Returns `CodegenDecision`'s equivalent of `true` if all the instructions in\n // the parameter computation are supported by the Triton emitters for the given\n // compute capability.\n-// `is_fused_computation` indicates that fusions are already formed and\n-// we should run additional checks for them.\n //\n-// TODO(b/393299275): remove this comment.\n // This function has the same caveats as `IsTritonSupportedInstruction` as\n // defined in the present namespace.\n CodegenDecision IsTritonSupportedComputation(\n     const HloComputation& computation,\n-    const se::GpuComputeCapability& gpu_compute_capability,\n-    bool is_fused_computation = true);\n+    const se::GpuComputeCapability& gpu_compute_capability);\n \n // Returns `true` if the parameter computation is a Triton fused computation,\n // i.e. the calling fusion instruction has `FusionKind::kCustom` and"
        },
        {
            "sha": "15fdbe7a8646b8855b116e3a55c186a445e88793",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 28,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc?ref=96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf",
            "patch": "@@ -57,7 +57,6 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/tensor_float_32_utils.h\"\n \n namespace xla {\n namespace gpu {\n@@ -409,17 +408,6 @@ FusionPlanAndRequirements BuildFusionPlanTowardOperands(\n       continue;\n     }\n \n-    // TODO(b/393299275): this check cannot be replaced by a\n-    // `IsTritonSupportedComputation` because we will do some rewrites\n-    // later that might change the decision. For example 'scaled-dot-rewriter'\n-    // replaces unsupported F8E8M0FNU with u8. We should have a more principled\n-    // way check if we will be able to emit the triton code for the fusion.\n-    if (original_hlo.opcode() == HloOpcode::kDynamicSlice) {\n-      // TODO(b/417172838): support dynamic slice op.\n-      fusion_builder.SetShouldFuseNode(node_id, false);\n-      continue;\n-    }\n-\n     auto opt_result = GetOperandDimOrdersAndCombinedReqsIfProfitable(\n         original_hlo, dim_order, properties, gpu_version, combined_reqs);\n     if (!opt_result.has_value()) {\n@@ -716,27 +704,13 @@ absl::StatusOr<Decision> CreateDotFusion(\n     std::vector<HloInstruction*>& fusion_inputs,\n     HloInstruction** fusion_output_ptr) {\n   VLOG(5) << dot.ToString();\n-  if (CodegenDecision is_supported = IsTritonSupportedInstruction(\n-          dot, gpu_version, /*is_fused_computation=*/false);\n+  if (CodegenDecision is_supported =\n+          legacy_triton::IsTritonSupportedInstruction(dot, gpu_version);\n       !is_supported) {\n     VLOG(3) << is_supported.Explain();\n     return Decision::Deny(is_supported.Explain());\n   }\n \n-  // TODO(b/393299275): legacy triton emitter only accepted dots with unset\n-  // algorithm when tf32 was enabled. Keeping this check for now to avoid\n-  // performance regressions. We should investigate how to improve performance,\n-  // or move this check under IsTritonSupportedInstruction.\n-  if (dot.precision_config().algorithm() == PrecisionConfig::ALG_UNSET) {\n-    if (!tsl::tensor_float_32_execution_enabled() ||\n-        absl::c_any_of(dot.precision_config().operand_precision(),\n-                       [](int x) { return x != PrecisionConfig::DEFAULT; })) {\n-      return Decision::Deny(\n-          \"Having non-default operand precisions or TensorFloat-32 disabled \"\n-          \"for Dot op with unset algorithm.\");\n-    }\n-  }\n-\n   std::vector<HlosAndRequirements> hlos_and_reqs;\n   hlos_and_reqs.reserve(dot.operand_count());\n   TF_ASSIGN_OR_RETURN(HlosAndRequirements lhs_hlos_and_reqs,"
        },
        {
            "sha": "a701b59f4b6742a9fd560fc08f125b99c8eba45d",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=96f4c1a083ea03a0fc8b41b1c281ca7cc4feabaf",
            "patch": "@@ -264,8 +264,7 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest, DISABLED_DynamicSliceIsFused) {\n+TEST_F(GemmFusionTest, DynamicSliceIsFused) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -289,8 +288,7 @@ ENTRY e {\n                                     m::Parameter(), m::Constant()))));\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest, DISABLED_DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n+TEST_F(GemmFusionTest, DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -321,8 +319,7 @@ ENTRY e {\n                             m::Parameter(), m::Parameter()))));\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest, DISABLED_DoNotFuseDynamicSliceOfNonMajorFragments) {\n+TEST_F(GemmFusionTest, DoNotFuseDynamicSliceOfNonMajorFragments) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -341,9 +338,7 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-// TODO(b/417172838): support dynamic slice op.\n-TEST_F(GemmFusionTest,\n-       DISABLED_CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n+TEST_F(GemmFusionTest, CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {"
        }
    ],
    "stats": {
        "total": 174,
        "additions": 56,
        "deletions": 118
    }
}