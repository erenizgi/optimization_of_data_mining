{
    "author": "shawnwang18",
    "message": "PR #30695: [XLA:GPU] Lowering command buffer's `WhileCmd` into an unrolled cuda-graph.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30695\n\nüìù Summary of Changes\n\nThis PR enables lowering the `WhileCmd` of `CommandBufferCmd`  into an unrolled command buffer (sub cuda-graph without the loop node).\n\nThe main idea includes below:\n\n1. We now assume that `CommandBufferCmdExecutor` (`CommandBufferCmd` sequence) can be lowered in the unrolling way, and the `RecordParams` struct has a new member variable `unroll_iteration`, which indicates the current unroll iteration.  Original normal `CommandBufferCmdExecutor::Record` lowering will just be assumed as a special case,  i.e, it just has the unrolling factor 1 .\n\n2.  Original, the `StateManager` class maintains a one-one map from `CommandBufferCmd` to the set of cuda-graph nodes lowered from the command.  Our new assumption breaks this one-one mapping , now it means that one `CommandBufferCmd`  might be lowered into multiple sets of command buffer nodes (cuda-graph nodes), each set is associated with one unrolling iteration.   To support it, we update the `StateManager`, and now each `<const CommandBufferCmd*, const se::CommandBuffer*, unroll_itearation_value>` tuple will maps to one set of cuda-graph nodes:\n\n```\nusing Key = std::tuple<const CommandBufferCmd*, const se::CommandBuffer*,\n                           TypeId, int64_t>;\nabsl::flat_hash_map<Key, std::unique_ptr<State>> state_;\n\nCommandBufferCmd::State* CommandBufferCmd::StateManager::GetOrCreate(\n    const CommandBufferCmd* cmd, const se::CommandBuffer* command_buffer,\n    TypeId type_id, int64_t unroll_iteration,\n    absl::FunctionRef<std::unique_ptr<State>()> create) {\n  Key key = {cmd, command_buffer, type_id, unroll_iteration};\n  if (auto it = state_.find(key); it != state_.end()) {\n    return it->second.get();\n  }\n  return state_.try_emplace(key, create()).first->second.get();\n}\n\n```\n\n3.  To lowering the `WhileCmd` into an unrolled cuda-graph, we needs to pre-known the trip count, this depends on whether the `WhileThunk` has pre-known trip count.\n\n4.  To unrolled the `WhileCmd` when the trip count is pre-known, we can implement the `WhileCmd:Record` function like below,  i.e, it lowers into cuda graph `ChildNode`, whose child graph is lowered from a sequence of `CommandBufferCmdExecutor`:  `cond_iteration0` ->`body-iteration0` ->`cond_iteration1` -> `body_iteration1` ......\n\n```\n   // Unroll the while loop body for `trip_count` times.\n    // Unrolled execution sequence: cond -> body -> cond -> body -> ...\n    // In the unrolled pattern, we still need to run the cond commands because\n    // body commands might depends on the value of index variable that is\n    // updated by condition commands.\n    auto new_record_params = record_params;\n\n    for (int64_t i = 0; i < trip_count_.value(); ++i) {\n      new_record_params.unroll_iteration = i;\n      if (i == 0) {\n        // First iteration, cond_commands_ will not have dependencies.\n        TF_RETURN_IF_ERROR(cond_commands_.Record(\n            execute_params, new_record_params, CommandBufferCmd::RecordCreate{},\n            child_command_buffer_.get(), false));\n      } else {\n        // Other iterations, cond_commands_ will have dependencies on the sink\n        // commands from previous iteration's body.\n        auto body_sink_commands = body_commands_.SinkCommands(\n            new_record_params, child_command_buffer_.get(), i - 1);\n        TF_RETURN_IF_ERROR(cond_commands_.Record(\n            execute_params, new_record_params,\n            CommandBufferCmd::RecordCreate{absl::MakeSpan(body_sink_commands)},\n            child_command_buffer_.get(), false));\n      }\n      auto cond_sink_commands = cond_commands_.SinkCommands(\n          new_record_params, child_command_buffer_.get(), i);\n      TF_RETURN_IF_ERROR(body_commands_.Record(\n          execute_params, new_record_params,\n          CommandBufferCmd::RecordCreate{absl::MakeSpan(cond_sink_commands)},\n          child_command_buffer_.get(),\n          i == trip_count_.value() - 1 ? true : false));\n    }\n    return Handle(\n        std::move(record_action),\n        [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n          return command_buffer->CreateChildCommand(\n              se::CommandBuffer::ChildCommandType::kMoved,\n              *child_command_buffer_, dependencies);\n        },\n        [&](const se::CommandBuffer::Command* command) {\n          // Moved child command does not need to be updated.\n          return absl::OkStatus();\n        });\n\n```\n\nüéØ Justification\n\n`WhileCmd` unrolling is a good way to work around several patterns that is not supported by cuda-graph `loop` operator,  e.g.:\n\n1.  nccl kernels can not be included in the body sub-graph of cuda graph `loop` node, because capturing NCCL custom will get un-supported loop body graph pattern.\n2.  DynamicSliceFusion command can not be wrapped by cuda-graph `loop` node, because memory pointers will change across loop iterations.\n\nAbove issues are very difficult by fixing cuda. As scan layer is widely used in JAX LLM models, we need to find a work around. We hope that we can use the loop unrolling to lower the whole model (full loop, not just loop body) into one cuda-graph.\n\nüöÄ Kind of Contribution\nNew feature.\n\nüß™ Unit Tests:\ncommand_buffer_test.cc: CommandBufferUnrollTest.WhileLoop\n\nCopybara import of the project:\n\n--\n042cb211fb825f44f95af3c4d629aa9653ee35ec by Shawn Wang <shawnw@nvidia.com>:\n\nSupport while loop unrolling in command buffer\n\nMerging this change closes #30695\n\nPiperOrigin-RevId: 806211906",
    "sha": "5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
    "files": [
        {
            "sha": "43abc3db7fed18980b272bc48ca70e6cb324f2d4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 113,
            "deletions": 41,
            "changes": 154,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -212,8 +212,8 @@ CommandBufferCmd::StateManager::GetNextTypeId() {\n \n CommandBufferCmd::State* CommandBufferCmd::StateManager::GetOrNull(\n     const CommandBufferCmd* cmd, const se::CommandBuffer* command_buffer,\n-    TypeId type_id) {\n-  Key key = {cmd, command_buffer, type_id};\n+    TypeId type_id, int64_t unroll_iteration) {\n+  Key key = {cmd, command_buffer, type_id, unroll_iteration};\n   if (auto it = state_.find(key); it != state_.end()) {\n     return it->second.get();\n   }\n@@ -222,8 +222,9 @@ CommandBufferCmd::State* CommandBufferCmd::StateManager::GetOrNull(\n \n CommandBufferCmd::State* CommandBufferCmd::StateManager::GetOrCreate(\n     const CommandBufferCmd* cmd, const se::CommandBuffer* command_buffer,\n-    TypeId type_id, absl::FunctionRef<std::unique_ptr<State>()> create) {\n-  Key key = {cmd, command_buffer, type_id};\n+    TypeId type_id, int64_t unroll_iteration,\n+    absl::FunctionRef<std::unique_ptr<State>()> create) {\n+  Key key = {cmd, command_buffer, type_id, unroll_iteration};\n   if (auto it = state_.find(key); it != state_.end()) {\n     return it->second.get();\n   }\n@@ -470,6 +471,7 @@ absl::Status CommandBufferCmdExecutor::Record(\n                                     command_buffer, create->dependencies)\n                            .status());\n   }\n+\n   if (finalize) {\n     return command_buffer->Finalize();\n   }\n@@ -513,10 +515,11 @@ CommandBufferCmdExecutor::RecordCreate(\n     }\n \n     // Create new commands by recording them into the command buffer.\n-    DCHECK(!state.GetOrNull<RecordState>(command, command_buffer))\n+    DCHECK(!state.GetOrNull<RecordState>(command, command_buffer,\n+                                         record_params.unroll_iteration))\n         << \"Record state must be null for \" << command->ToString();\n-    auto* record_state =\n-        state.GetOrCreate<RecordState>(command, command_buffer);\n+    auto* record_state = state.GetOrCreate<RecordState>(\n+        command, command_buffer, record_params.unroll_iteration);\n \n     std::vector<const se::CommandBuffer::Command*> command_dependencies =\n         Dependencies(record_params, command_buffer, id);\n@@ -627,7 +630,8 @@ absl::Status CommandBufferCmdExecutor::RecordUpdate(\n     }\n \n     // Update existing commands in the command buffer.\n-    auto* record_state = state.GetOrNull<RecordState>(command, command_buffer);\n+    auto* record_state = state.GetOrNull<RecordState>(\n+        command, command_buffer, record_params.unroll_iteration);\n     DCHECK(record_state) << \"Record state must be not null for \"\n                          << command->ToString();\n \n@@ -666,9 +670,9 @@ bool CommandBufferCmdExecutor::IsSink(CommandId id) const {\n }\n \n std::vector<const se::CommandBuffer::Command*>\n-CommandBufferCmdExecutor::SinkCommands(\n-    const RecordParams& record_params,\n-    se::CommandBuffer* command_buffer) const {\n+CommandBufferCmdExecutor::SinkCommands(const RecordParams& record_params,\n+                                       se::CommandBuffer* command_buffer,\n+                                       int64_t unroll_iteration) const {\n   std::vector<CommandId> sink_ids;\n   if (execution_graph_) {\n     auto sink_span = execution_graph_->sink();\n@@ -680,16 +684,16 @@ CommandBufferCmdExecutor::SinkCommands(\n   std::vector<const se::CommandBuffer::Command*> sink_commands;\n   for (CommandId id : sink_ids) {\n     auto* record_state = record_params.state.GetOrNull<RecordState>(\n-        commands_[id].get(), command_buffer);\n+        commands_[id].get(), command_buffer, unroll_iteration);\n     sink_commands.push_back(record_state->command);\n   }\n   return sink_commands;\n }\n \n std::vector<const se::CommandBuffer::Command*>\n-CommandBufferCmdExecutor::SourceCommands(\n-    const RecordParams& record_params,\n-    se::CommandBuffer* command_buffer) const {\n+CommandBufferCmdExecutor::SourceCommands(const RecordParams& record_params,\n+                                         se::CommandBuffer* command_buffer,\n+                                         int64_t unroll_iteration) const {\n   std::vector<CommandId> source_ids;\n   if (execution_graph_) {\n     auto source_span = execution_graph_->source();\n@@ -701,7 +705,7 @@ CommandBufferCmdExecutor::SourceCommands(\n   std::vector<const se::CommandBuffer::Command*> source_commands;\n   for (CommandId id : source_ids) {\n     auto* record_state = record_params.state.GetOrNull<RecordState>(\n-        commands_[id].get(), command_buffer);\n+        commands_[id].get(), command_buffer, unroll_iteration);\n     source_commands.push_back(record_state->command);\n   }\n   return source_commands;\n@@ -711,15 +715,15 @@ std::vector<const se::CommandBuffer::Command*>\n CommandBufferCmdExecutor::Dependencies(const RecordParams& record_params,\n                                        se::CommandBuffer* command_buffer,\n                                        CommandId id) const {\n-  // Source commands have no dependencies.\n+  // Collect commands that are dependencies of the command `id`.\n+  absl::InlinedVector<CommandId, 4> dependencies_ids;\n+\n   if (IsSource(id)) {\n     VLOG(2) << \"Command ID \" << id\n             << \" is a source command, empty dependencies\";\n     return {};\n   }\n \n-  // Collect commands that are dependencies of the command `id`.\n-  absl::InlinedVector<CommandId, 4> dependencies_ids;\n   if (execution_graph_) {\n     for (const ExecutionGraph::NodeEdge& in_edge :\n          execution_graph_->in_edges(id)) {\n@@ -733,7 +737,8 @@ CommandBufferCmdExecutor::Dependencies(const RecordParams& record_params,\n   std::vector<const se::CommandBuffer::Command*> dependencies;\n   for (CommandId dependency_id : dependencies_ids) {\n     auto* record_state = record_params.state.GetOrNull<RecordState>(\n-        commands_[dependency_id].get(), command_buffer);\n+        commands_[dependency_id].get(), command_buffer,\n+        record_params.unroll_iteration);\n     DCHECK(record_state) << \"Record state must be not null for \"\n                          << commands_[dependency_id]->ToString();\n \n@@ -882,11 +887,13 @@ TracedCommandBufferCmd::RecordTracedCommand(\n     se::CommandBuffer* command_buffer,\n     absl::FunctionRef<absl::Status(se::Stream*)> trace) {\n   auto traced_cmd = record_params.state.GetOrCreate<TracedCommandBuffer>(\n-      this, command_buffer, [&] {\n+      this, command_buffer,\n+      [&] {\n         const auto& debug_options = xla::GetDebugOptionsFromFlags();\n         return std::make_unique<TracedCommandBuffer>(\n             this, buffers(), debug_options.xla_cmd_buffer_trace_cache_size());\n-      });\n+      },\n+      record_params.unroll_iteration);\n \n   TF_ASSIGN_OR_RETURN(\n       auto nested_cmd,\n@@ -1450,16 +1457,29 @@ CommandBufferCmd::BufferUseVector CaseCmd::buffers() const {\n \n WhileCmd::WhileCmd(BufferAllocation::Slice pred,\n                    CommandBufferCmdExecutor cond_commands,\n-                   CommandBufferCmdExecutor body_commands)\n+                   CommandBufferCmdExecutor body_commands,\n+                   std::optional<int64_t> trip_count, bool enable_loop_unroll)\n     : CommandBufferCmd(CommandBufferCmdType::kWhileCmd),\n       pred_(pred),\n       cond_commands_(std::move(cond_commands)),\n-      body_commands_(std::move(body_commands)) {}\n+      body_commands_(std::move(body_commands)),\n+      trip_count_(trip_count),\n+      enable_loop_unroll_(enable_loop_unroll) {}\n \n absl::Status WhileCmd::Initialize(const Thunk::InitializeParams& params,\n                                   StateManager& state) {\n   TF_RETURN_IF_ERROR(cond_commands_.Initialize(params, state));\n-  return body_commands_.Initialize(params, state);\n+  TF_RETURN_IF_ERROR(body_commands_.Initialize(params, state));\n+  if (enable_loop_unroll_ && body_commands_.support_loop_unroll() &&\n+      cond_commands_.support_loop_unroll() && trip_count_ != std::nullopt &&\n+      child_command_buffer_ == nullptr) {\n+    is_unrolled_loop_ = true;\n+    TF_ASSIGN_OR_RETURN(child_command_buffer_,\n+                        params.stream->parent()->CreateCommandBuffer(\n+                            se::CommandBuffer::Mode::kNested));\n+  }\n+  VLOG(3) << \"while command trip_count: \" << trip_count_.value_or(-1);\n+  return absl::OkStatus();\n }\n \n absl::StatusOr<const se::CommandBuffer::Command*> WhileCmd::Record(\n@@ -1472,22 +1492,74 @@ absl::StatusOr<const se::CommandBuffer::Command*> WhileCmd::Record(\n   VLOG(5) << \"WhileCmd: cond_commands=\" << cond_commands_.size()\n           << \" body_commands=\" << body_commands_.size();\n   VLOG(5) << \"  pred: \" << pred_ << \" (\" << pred.opaque() << \")\";\n-\n-  return Handle(\n-      std::move(record_action),\n-      [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n-        return command_buffer->CreateWhile(\n-            se::DeviceMemory<bool>(pred),\n-            CreateCommands(&cond_commands_, &execute_params, &record_params),\n-            CreateCommands(&body_commands_, &execute_params, &record_params),\n-            dependencies);\n-      },\n-      [&](const se::CommandBuffer::Command* command) {\n-        return command_buffer->UpdateWhile(\n-            command, se::DeviceMemory<bool>(pred),\n-            UpdateCommands(&cond_commands_, &execute_params, &record_params),\n-            UpdateCommands(&body_commands_, &execute_params, &record_params));\n-      });\n+  if (is_unrolled_loop_) {\n+    // When the loop is unrolled, we need to record the body commands for\n+    // `trip_count` times into child_command_buffer_, and implement the While\n+    // command as a child command.\n+    VLOG(3) << \"Recording unrolled loop with trip_count: \"\n+            << trip_count_.value();\n+    CHECK(child_command_buffer_ != nullptr);\n+\n+    // Unroll the while loop body for `trip_count` times.\n+    // Unrolled execution sequence: cond -> body -> cond -> body -> ...\n+    // In the unrolled pattern, we still need to run the cond commands because\n+    // body commands might depends on the value of index variable that is\n+    // updated by condition commands.\n+    auto new_record_params = record_params;\n+    for (int64_t i = 0; i < trip_count_.value(); ++i) {\n+      new_record_params.unroll_iteration = i;\n+      if (i == 0) {\n+        // First iteration, cond_commands_ will not have dependencies.\n+        TF_RETURN_IF_ERROR(cond_commands_.Record(\n+            execute_params, new_record_params, CommandBufferCmd::RecordCreate{},\n+            child_command_buffer_.get(), false));\n+      } else {\n+        // Other iterations, cond_commands_ will have dependencies on the sink\n+        // commands from previous iteration's body.\n+        auto body_sink_commands = body_commands_.SinkCommands(\n+            new_record_params, child_command_buffer_.get(), i - 1);\n+        TF_RETURN_IF_ERROR(cond_commands_.Record(\n+            execute_params, new_record_params,\n+            CommandBufferCmd::RecordCreate{absl::MakeSpan(body_sink_commands)},\n+            child_command_buffer_.get(), false));\n+      }\n+      auto cond_sink_commands = cond_commands_.SinkCommands(\n+          new_record_params, child_command_buffer_.get(), i);\n+      TF_RETURN_IF_ERROR(body_commands_.Record(\n+          execute_params, new_record_params,\n+          CommandBufferCmd::RecordCreate{absl::MakeSpan(cond_sink_commands)},\n+          child_command_buffer_.get(),\n+          i == trip_count_.value() - 1 ? true : false));\n+    }\n+    return Handle(\n+        std::move(record_action),\n+        [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n+          return command_buffer->CreateChildCommand(\n+              se::CommandBuffer::ChildCommandType::kMoved,\n+              *child_command_buffer_, dependencies);\n+        },\n+        [&](const se::CommandBuffer::Command* command) {\n+          // We do not need to update the child node here, and sub-graph has\n+          // been updated above in the unrooled pattern.\n+          return absl::OkStatus();\n+        });\n+  } else {\n+    return Handle(\n+        std::move(record_action),\n+        [&](absl::Span<const se::CommandBuffer::Command* const> dependencies) {\n+          return command_buffer->CreateWhile(\n+              se::DeviceMemory<bool>(pred),\n+              CreateCommands(&cond_commands_, &execute_params, &record_params),\n+              CreateCommands(&body_commands_, &execute_params, &record_params),\n+              dependencies);\n+        },\n+        [&](const se::CommandBuffer::Command* command) {\n+          return command_buffer->UpdateWhile(\n+              command, se::DeviceMemory<bool>(pred),\n+              UpdateCommands(&cond_commands_, &execute_params, &record_params),\n+              UpdateCommands(&body_commands_, &execute_params, &record_params));\n+        });\n+  }\n }\n \n bool WhileCmd::requires_initialization() {"
        },
        {
            "sha": "b57b6737e922e483370f765a85b0daac92095253",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 60,
            "deletions": 21,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -162,28 +162,31 @@ class CommandBufferCmd {\n \n     template <typename ConcreteState>\n     ConcreteState* GetOrNull(const CommandBufferCmd* cmd,\n-                             const se::CommandBuffer* command_buffer) {\n+                             const se::CommandBuffer* command_buffer,\n+                             int64_t unroll_iteration = 0) {\n       static_assert(std::is_base_of_v<State, ConcreteState>);\n-      return static_cast<ConcreteState*>(\n-          GetOrNull(cmd, command_buffer, GetTypeId<ConcreteState>()));\n+      return static_cast<ConcreteState*>(GetOrNull(\n+          cmd, command_buffer, GetTypeId<ConcreteState>(), unroll_iteration));\n     }\n \n     template <typename ConcreteState>\n     ConcreteState* GetOrCreate(\n         const CommandBufferCmd* cmd, const se::CommandBuffer* command_buffer,\n-        absl::FunctionRef<std::unique_ptr<ConcreteState>()> create) {\n+        absl::FunctionRef<std::unique_ptr<ConcreteState>()> create,\n+        int64_t unroll_iteration = 0) {\n       static_assert(std::is_base_of_v<State, ConcreteState>);\n-      return static_cast<ConcreteState*>(GetOrCreate(cmd, command_buffer,\n-                                                     GetTypeId<ConcreteState>(),\n-                                                     [&] { return create(); }));\n+      return static_cast<ConcreteState*>(\n+          GetOrCreate(cmd, command_buffer, GetTypeId<ConcreteState>(),\n+                      unroll_iteration, [&] { return create(); }));\n     }\n \n     template <typename ConcreteState>\n     ConcreteState* GetOrCreate(const CommandBufferCmd* cmd,\n-                               const se::CommandBuffer* command_buffer) {\n-      return GetOrCreate<ConcreteState>(cmd, command_buffer, [] {\n-        return std::make_unique<ConcreteState>();\n-      });\n+                               const se::CommandBuffer* command_buffer,\n+                               int64_t unroll_iteration = 0) {\n+      return GetOrCreate<ConcreteState>(\n+          cmd, command_buffer, [] { return std::make_unique<ConcreteState>(); },\n+          unroll_iteration);\n     }\n \n    private:\n@@ -199,14 +202,16 @@ class CommandBufferCmd {\n     static TypeId GetNextTypeId();\n \n     State* GetOrNull(const CommandBufferCmd* cmd,\n-                     const se::CommandBuffer* command_buffer, TypeId type_id);\n+                     const se::CommandBuffer* command_buffer, TypeId type_id,\n+                     int64_t unroll_iteration);\n \n     State* GetOrCreate(const CommandBufferCmd* cmd,\n                        const se::CommandBuffer* command_buffer, TypeId type_id,\n+                       int64_t unroll_iteration,\n                        absl::FunctionRef<std::unique_ptr<State>()> create);\n \n-    using Key =\n-        std::tuple<const CommandBufferCmd*, const se::CommandBuffer*, TypeId>;\n+    using Key = std::tuple<const CommandBufferCmd*, const se::CommandBuffer*,\n+                           TypeId, int64_t>;\n     absl::flat_hash_map<Key, std::unique_ptr<State>> state_;\n   };\n \n@@ -224,6 +229,11 @@ class CommandBufferCmd {\n     // A flag indicating whether we record comands at command buffer thunk\n     // initialization time.\n     bool is_initialization = false;\n+\n+    // The command sequence might be recorded in the loop unrolling pattern, so\n+    // the command sequence might be instantiated multiple times, we uses\n+    // unroll_iteration to locate the commands for current unroll iteration.\n+    int64_t unroll_iteration = 0;\n   };\n \n   // Create new commands in the command buffer using the given dependencies.\n@@ -284,6 +294,11 @@ class CommandBufferCmd {\n   // ensure that all ranks execute NCCL command update.\n   virtual bool requires_initialization() { return false; }\n \n+  // Returns true if command supports loop unroll, the while loop can be\n+  // unrolled only if it has pre-known trip count and also all commands from the\n+  // body commands are unrollable..\n+  virtual bool support_loop_unroll() { return true; }\n+\n   // This is only true for DynamicSliceCopyFusionCmd when offset is dependents\n   // on loop iteration. As the command of slice operation is access the sliced\n   // memory region that varies across loop iterations, so even the original\n@@ -468,15 +483,20 @@ class CommandBufferCmdExecutor {\n                           [](const auto& cmd) { return cmd->force_update(); });\n   }\n \n-  // Returns all source commands for current command executor.\n+  bool support_loop_unroll() const {\n+    return absl::c_all_of(\n+        commands_, [](const auto& cmd) { return cmd->support_loop_unroll(); });\n+  }\n+\n+  // Returns all commands associated with the given ids on a certain unroll\n+  // iteration.\n   std::vector<const se::CommandBuffer::Command*> SourceCommands(\n-      const RecordParams& record_params,\n-      se::CommandBuffer* command_buffer) const;\n+      const RecordParams& record_params, se::CommandBuffer* command_buffer,\n+      int64_t unroll_iteration) const;\n \n-  // Returns all sink commands for current command executor.\n   std::vector<const se::CommandBuffer::Command*> SinkCommands(\n-      const RecordParams& record_params,\n-      se::CommandBuffer* command_buffer) const;\n+      const RecordParams& record_params, se::CommandBuffer* command_buffer,\n+      int64_t unroll_iteration) const;\n \n   // Renders the execution graph using default renderer. Returns url of the\n   // rendered graph, or an error if rendering failed.\n@@ -796,6 +816,8 @@ class ChildCmd : public CommandBufferCmd {\n \n   bool force_update() override;\n \n+  bool support_loop_unroll() override { return false; }\n+\n   BufferUseVector buffers() const override;\n \n  private:\n@@ -829,6 +851,8 @@ class CaseCmd : public CommandBufferCmd {\n \n   bool force_update() override;\n \n+  bool support_loop_unroll() override { return false; }\n+\n   BufferUseVector buffers() const override;\n \n  private:\n@@ -844,7 +868,9 @@ class CaseCmd : public CommandBufferCmd {\n class WhileCmd : public CommandBufferCmd {\n  public:\n   WhileCmd(BufferAllocation::Slice pred, CommandBufferCmdExecutor cond_commands,\n-           CommandBufferCmdExecutor body_commands);\n+           CommandBufferCmdExecutor body_commands,\n+           std::optional<int64_t> trip_count = std::nullopt,\n+           bool enable_loop_unroll = false);\n \n   absl::Status Initialize(const Thunk::InitializeParams& params,\n                           StateManager& state) override;\n@@ -858,12 +884,21 @@ class WhileCmd : public CommandBufferCmd {\n \n   bool force_update() override;\n \n+  // We have not tried unrolling the loop inside another loop, so marking it\n+  // unsupported for now.\n+  bool support_loop_unroll() override { return false; }\n+\n   BufferUseVector buffers() const override;\n \n  private:\n   BufferAllocation::Slice pred_;\n   CommandBufferCmdExecutor cond_commands_;\n   CommandBufferCmdExecutor body_commands_;\n+  std::optional<int64_t> trip_count_;\n+  bool enable_loop_unroll_ = false;\n+  bool is_unrolled_loop_ = false;\n+  std::unique_ptr<se::CommandBuffer>\n+      child_command_buffer_;  // The body command buffer for unrolled loop.\n };\n \n //===----------------------------------------------------------------------===//\n@@ -1209,6 +1244,8 @@ class DynamicSliceFusionCmd : public CommandBufferCmd {\n \n   bool requires_initialization() override;\n \n+  bool support_loop_unroll() override { return false; }\n+\n   bool IsNestedCommandBuffer() const final { return true; }\n \n  private:\n@@ -1254,6 +1291,8 @@ class DynamicSliceCopyFusionCmd : public CommandBufferCmd {\n \n   bool force_update() override { return offsets_.depends_on_loop; }\n \n+  bool support_loop_unroll() override { return false; }\n+\n   BufferUseVector buffers() const override;\n \n  private:"
        },
        {
            "sha": "3351d3dd55095e868757b06be1830b5e901db11c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -116,8 +116,9 @@ static absl::StatusOr<Command> Convert(\n       CommandBufferCmdExecutor body_cmds,\n       ConvertToCommands(thunk.body_thunk_sequence()->thunks(), options));\n \n-  return std::make_unique<WhileCmd>(thunk.condition_result_buffer(),\n-                                    std::move(cond_cmds), std::move(body_cmds));\n+  return std::make_unique<WhileCmd>(\n+      thunk.condition_result_buffer(), std::move(cond_cmds),\n+      std::move(body_cmds), thunk.trip_count(), options.enable_loop_unroll);\n }\n \n static absl::StatusOr<Command> Convert(const GemmThunk& thunk) {"
        },
        {
            "sha": "f9c7792a26c47b49e65a5b6e4deeb65c5e6b4c0e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.h?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -26,6 +26,7 @@ namespace xla::gpu {\n struct ConvertToCommandsOptions {\n   CommandBufferCmdExecutor::SynchronizationMode synchronization_mode =\n       CommandBufferCmdExecutor::SynchronizationMode::kSerialize;\n+  bool enable_loop_unroll = false;\n };\n \n // Converts thunk sequence to a command buffer cmd sequence."
        },
        {
            "sha": "bf3bf8b67dc290ac3b599744dbb2b8cf26469c85",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_test.cc?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -683,13 +683,15 @@ TEST(CommandBufferCmdTest, RecordExecutorsWithDependencies) {\n                              CommandBufferCmd::RecordCreate{},\n                              command_buffer.get(), /*finalize=*/false));\n \n-  auto a_sinks = exec_a.SinkCommands(record_params, command_buffer.get());\n+  auto a_sinks = exec_a.SinkCommands(record_params, command_buffer.get(),\n+                                     /*unroll_iteration=*/0);\n   TF_ASSERT_OK(\n       exec_b.Record(exec_params, record_params,\n                     CommandBufferCmd::RecordCreate{absl::MakeSpan(a_sinks)},\n                     command_buffer.get(), /*finalize=*/false));\n \n-  auto b_sinks = exec_b.SinkCommands(record_params, command_buffer.get());\n+  auto b_sinks = exec_b.SinkCommands(record_params, command_buffer.get(),\n+                                     /*unroll_iteration=*/0);\n   TF_ASSERT_OK(\n       exec_c.Record(exec_params, record_params,\n                     CommandBufferCmd::RecordCreate{absl::MakeSpan(b_sinks)},"
        },
        {
            "sha": "67dd12be825fdf4934f49baebcfc5ec7c9a838d4",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -240,6 +240,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.add_xla_gpu_enable_command_buffer(DebugOptions::CUDNN);\n   opts.set_xla_gpu_graph_min_graph_size(5);\n   opts.set_xla_gpu_command_buffer_scheduling_mode(DebugOptions::LHS);\n+  opts.set_xla_gpu_command_buffer_unroll_loops(false);\n   opts.set_xla_cmd_buffer_trace_cache_size(16);\n \n   opts.set_xla_gpu_collectives_use_persistent_cliques(false);\n@@ -2026,6 +2027,12 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       \"(AutotuneResult::TritonGemmKey) textproto configuration for all Triton \"\n       \"GEMM fusions. (You can get such textprotos from the debug logs of the \"\n       \"GEMM autotuner.) \"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_command_buffer_unroll_loops\",\n+      bool_setter_for(&DebugOptions::set_xla_gpu_command_buffer_unroll_loops),\n+      debug_options->xla_gpu_command_buffer_unroll_loops(),\n+      \"During command buffer lowering, unroll the loop command if loop has \"\n+      \"known loop count.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_copy_insertion_use_region_analysis\",\n       bool_setter_for("
        },
        {
            "sha": "2a8412230dc567e5f16b902b30664146d3bbc025",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -604,10 +604,13 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(\n       return Internal(\"Unsupported command buffer scheduling mode: %d\", mode);\n   }\n \n+  bool enable_loop_unroll = ir_emitter_context_->debug_options()\n+                                .xla_gpu_command_buffer_unroll_loops();\n   TF_ASSIGN_OR_RETURN(\n       CommandBufferCmdExecutor cmd_executor,\n-      ConvertToCommands(thunk_sequence->thunks(),\n-                        ConvertToCommandsOptions{synchronization_mode}));\n+      ConvertToCommands(\n+          thunk_sequence->thunks(),\n+          ConvertToCommandsOptions{synchronization_mode, enable_loop_unroll}));\n \n   AddThunkToThunkSequence(std::make_unique<CommandBufferThunk>(\n       std::move(cmd_executor), Thunk::ThunkInfo::WithProfileAnnotation(instr),"
        },
        {
            "sha": "05009413ce1699f3a23b7810015b72b67391253f",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -159,6 +159,10 @@ xla_test(\n         \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla/service:hlo_module_config\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_pjrt_interpreter_reference_mixin\",\n         \"//xla/tests:hlo_pjrt_test_base\",\n         \"//xla/tests:literal_test_util\","
        },
        {
            "sha": "372c1cd5956d0e12d82da18c80571f49aeac64b4",
            "filename": "third_party/xla/xla/service/gpu/tests/command_buffer_test.cc",
            "status": "modified",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -21,6 +21,10 @@ limitations under the License.\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tests/hlo_pjrt_interpreter_reference_mixin.h\"\n #include \"xla/tests/hlo_pjrt_test_base.h\"\n #include \"xla/tests/literal_test_util.h\"\n@@ -30,6 +34,28 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n+se::StreamExecutor* GpuExecutor() {\n+  auto name =\n+      absl::AsciiStrToUpper(PlatformUtil::CanonicalPlatformName(\"gpu\").value());\n+  auto* platform = se::PlatformManager::PlatformWithName(name).value();\n+  return platform->ExecutorForDevice(0).value();\n+}\n+\n+bool IsAtLeastCuda12900(const se::StreamExecutor* stream_executor) {\n+  const auto& device_description = stream_executor->GetDeviceDescription();\n+  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n+      &device_description.gpu_compute_capability());\n+  if (cuda_cc != nullptr) {\n+    if (device_description.driver_version() >=\n+            stream_executor::SemanticVersion(12, 9, 0) &&\n+        device_description.runtime_version() >=\n+            stream_executor::SemanticVersion(12, 9, 0)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n class CommandBufferTest\n     : public HloPjRtInterpreterReferenceMixin<HloPjRtTestBase>,\n       public ::testing::WithParamInterface<\n@@ -42,6 +68,16 @@ class CommandBufferTest\n   }\n };\n \n+// Test fixture that enables loop unrolling for command buffers.\n+class CommandBufferUnrollTest : public CommandBufferTest {\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = HloPjRtTestBase::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_command_buffer_scheduling_mode(GetParam());\n+    debug_options.set_xla_gpu_command_buffer_unroll_loops(true);\n+    return debug_options;\n+  }\n+};\n+\n TEST_P(CommandBufferTest, Fusions) {\n   constexpr absl::string_view hlo_text = R\"(\n   HloModule m, is_scheduled=true\n@@ -538,9 +574,81 @@ TEST_P(CommandBufferTest, DynamicSliceCopyFusionCmd) {\n       RunAndCompareNoHloPasses(std::move(module), ErrorSpec{1e-3, 2e-3}));\n }\n \n+TEST_P(CommandBufferUnrollTest, WhileLoop) {\n+  se::StreamExecutor* stream_executor = GpuExecutor();\n+\n+  if (!IsAtLeastCuda12900(stream_executor)) {\n+    GTEST_SKIP() << \"Child command is not supported for CUDA < 12.9\";\n+  }\n+\n+  constexpr absl::string_view hlo_text = R\"(\n+  HloModule m, is_scheduled=true\n+\n+  compare_fusion {\n+    p0 = s32[] parameter(0)\n+    ten = s32[] constant(10)\n+    ROOT compare = compare(p0, ten), direction=LT\n+  }\n+\n+  add_one {\n+    p0 = s32[] parameter(0)\n+    one = s32[] constant(1)\n+    ROOT add = add(p0, one)\n+  }\n+\n+  add_two {\n+    p0 = f32[] parameter(0)\n+    two = f32[] constant(2.0)\n+    ROOT add = add(p0, two)\n+  }\n+\n+  body {\n+    p0 = (s32[], f32[]) parameter(0)\n+    cnt = get-tuple-element(p0), index=0\n+    val = get-tuple-element(p0), index=1\n+    add_cnt = s32[] fusion(cnt), kind=kLoop, calls=add_one\n+    add_val = f32[] fusion(val), kind=kLoop, calls=add_two\n+    ROOT tuple = (s32[], f32[]) tuple(add_cnt, add_val)\n+  }\n+\n+  cond {\n+    p0 = (s32[], f32[]) parameter(0)\n+    cnt = get-tuple-element(p0), index=0\n+    ROOT compare = pred[] fusion(cnt), kind=kLoop, calls=compare_fusion\n+  }\n+\n+  command_buffer {\n+    p0 = (s32[], f32[]) parameter(0)\n+    ROOT while = while(p0), condition=cond, body=body, backend_config={\"known_trip_count\":{\"n\":\"10\"}}\n+  }\n+\n+  ENTRY main {\n+    p0 = (s32[], f32[]) parameter(0)\n+    ROOT call = (s32[], f32[]) call(p0), to_apply=command_buffer\n+  })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo_text));\n+\n+  Literal cnt = LiteralUtil::CreateR0<int32_t>(0);\n+  Literal value = LiteralUtil::CreateR0<float>(0.0);\n+  Literal argument = LiteralUtil::MakeTuple({&cnt, &value});\n+\n+  Literal expected_cnt = LiteralUtil::CreateR0<int32_t>(10);\n+  Literal expected_value = LiteralUtil::CreateR0<float>(20.0);\n+  Literal expected = LiteralUtil::MakeTuple({&expected_cnt, &expected_value});\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      Literal result,\n+      Execute(std::move(module), {&argument}, /*run_hlo_passes=*/false));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n+}\n+\n INSTANTIATE_TEST_SUITE_P(CommandBufferTests, CommandBufferTest,\n                          ::testing::Values(DebugOptions::LHS,\n                                            DebugOptions::CONCURRENT));\n+INSTANTIATE_TEST_SUITE_P(CommandBufferTestsUnroll, CommandBufferUnrollTest,\n+                         ::testing::Values(DebugOptions::LHS,\n+                                           DebugOptions::CONCURRENT));\n \n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "fb72f724b3d48907dd2d37a41a413d5c1a332e7c",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ed06f11a042e8ef7b86f25d099605a04b9f2c70/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=5ed06f11a042e8ef7b86f25d099605a04b9f2c70",
            "patch": "@@ -395,6 +395,10 @@ message DebugOptions {\n   optional CommandBufferSchedulingMode xla_gpu_command_buffer_scheduling_mode =\n       404;\n \n+  // During command buffer lowering, unroll the loop command if loop has known\n+  // loop count and the loop body does not have unsupported commands.\n+  optional bool xla_gpu_command_buffer_unroll_loops = 411;\n+\n   optional bool xla_gpu_copy_insertion_use_region_analysis = 236;\n \n   // Crashes the program when any kind of verification fails, instead of just\n@@ -1333,7 +1337,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 411\n+  // Next id: 412\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 379,
        "additions": 310,
        "deletions": 69
    }
}