{
    "author": "pschuh",
    "message": "Remove read references to untuple_result now that everyone always sets true.\n\nPiperOrigin-RevId: 828700891",
    "sha": "7e6a5445f545192b4050eb6a33fbedb213ea2665",
    "files": [
        {
            "sha": "97c5be8cf9e0017725c099847d5083c4458b3921",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc?ref=7e6a5445f545192b4050eb6a33fbedb213ea2665",
            "patch": "@@ -1924,14 +1924,6 @@ PjRtCpuExecutable::Execute(\n     std::optional<std::vector<Future<>>>& returned_futures) const {\n   RunId run_id(options.launch_id);\n   tsl::profiler::TraceMe trace_me(\"PjRtCpuExecutable::Execute\");\n-  if (!options.untuple_result && cpu_executable_->module()\n-                                     .config()\n-                                     .entry_computation_layout()\n-                                     .result_shape()\n-                                     .IsTuple()) {\n-    return InvalidArgument(\n-        \"Tuple results must be untupled using ExecuteOptions::untuple_result.\");\n-  }\n   if (device_assignment_ == nullptr) {\n     return InvalidArgument(\"Execute expects a non-null device_assignment\");\n   }\n@@ -2065,14 +2057,6 @@ PjRtCpuExecutable::ExecuteSharded(\n   if (device_assignment_ == nullptr) {\n     return InvalidArgument(\"ExecuteShard expects a non-null device_assignment\");\n   }\n-  if (!options.untuple_result && cpu_executable_->module()\n-                                     .config()\n-                                     .entry_computation_layout()\n-                                     .result_shape()\n-                                     .IsTuple()) {\n-    return InvalidArgument(\n-        \"Tuple results must be untupled using ExecuteOptions::untuple_result.\");\n-  }\n   for (int i = 0; i < addressable_devices_.size(); ++i) {\n     if (addressable_devices_[i] == device) {\n       VLOG(1) << \"ExecuteShard executes computation \" << name()"
        },
        {
            "sha": "35add53ba48380f86c0eab0a6c46317844032923",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_executable.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc?ref=7e6a5445f545192b4050eb6a33fbedb213ea2665",
            "patch": "@@ -385,10 +385,6 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n \n   TF_ASSIGN_OR_RETURN(std::vector<Shape> output_shapes, GetOutputShapes());\n   const Shape& result_shape = output_shapes[executable_idx];\n-  if (!options.untuple_result && result_shape.IsTuple()) {\n-    return InvalidArgument(\n-        \"Tuple results must be untupled using ExecuteOptions::untuple_result.\");\n-  }\n \n   // `scheduled_event` indicates whether gpu computation is dispatched to the\n   // stream and whether there was an error.\n@@ -521,9 +517,8 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n   std::vector<tsl::AsyncValueRef<GpuDeviceMemory>> output_buffers;\n   std::vector<std::unique_ptr<PjRtBuffer>> outputs;\n   auto gpu_executable = executables_[executable_idx];\n-  bool untuple_result = options.untuple_result;\n   bool result_is_tuple = result_shape.IsTuple();\n-  if (options.untuple_result && result_shape.IsTuple()) {\n+  if (result_shape.IsTuple()) {\n     output_buffers.reserve(result_shape.tuple_shapes().size());\n     outputs.reserve(output_buffers.size());\n     for (int i = 0; i < result_shape.tuple_shapes().size(); ++i) {\n@@ -592,7 +587,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n       [replica, partition, device, launch_id(options.launch_id),\n        output_buffers(output_buffers), complete_event(complete_event.CopyRef()),\n        scheduled_event(scheduled_event.CopyRef()),\n-       untuple_result(untuple_result), result_is_tuple(result_is_tuple),\n+       result_is_tuple(result_is_tuple),\n        donation_transactions(std::move(donation_transactions)),\n        parameter_shapes(on_device_executable_parameter_shapes_[executable_idx]),\n        gpu_executable(std::move(gpu_executable)),\n@@ -745,7 +740,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n \n         ExecutionOutput& execution_output = result_buffer_or_status.value();\n         ScopedShapedBuffer output = execution_output.ConsumeResult();\n-        if (untuple_result && result_is_tuple) {\n+        if (result_is_tuple) {\n           for (int i = 0; i < output_buffers.size(); ++i) {\n             ScopedShapedBuffer tuple_buffer = output.TakeSubTree({i});\n             stream_executor::DeviceMemoryBase* elem ="
        },
        {
            "sha": "ece4b088dda58c8f57ef1c2005e55e4b3b15e500",
            "filename": "third_party/xla/xla/pjrt/interpreter/interpreter_client.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Finterpreter%2Finterpreter_client.cc?ref=7e6a5445f545192b4050eb6a33fbedb213ea2665",
            "patch": "@@ -282,8 +282,7 @@ InterpreterLoadedExecutable::ExecuteSharded(\n   // Transform the result literal back into a one or more\n   // InterpreterLiteralWrapperBuffer.\n   std::vector<std::unique_ptr<PjRtBuffer>> result;\n-  // Untuple result if requested.\n-  if (options.untuple_result && result_literal.shape().IsTuple()) {\n+  if (result_literal.shape().IsTuple()) {\n     const int tuple_count = result_literal.shape().tuple_shapes().size();\n     result.reserve(tuple_count);\n     // DecomposeTuple invalidates result_literal. move(...) to make it obvious."
        },
        {
            "sha": "1c5e1521047f7bfdf309d2fcc6e6c1a5429741e3",
            "filename": "third_party/xla/xla/pjrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7e6a5445f545192b4050eb6a33fbedb213ea2665/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc?ref=7e6a5445f545192b4050eb6a33fbedb213ea2665",
            "patch": "@@ -149,7 +149,7 @@ absl::StatusOr<ExecuteOptionsProto> ExecuteOptions::ToProto() const {\n   ExecuteOptionsProto proto;\n \n   proto.set_arguments_are_tupled(false);\n-  proto.set_untuple_result(untuple_result);\n+  proto.set_untuple_result(true);\n   proto.set_launch_id(launch_id);\n   if (context != nullptr) {\n     return absl::UnimplementedError("
        }
    ],
    "stats": {
        "total": 32,
        "additions": 5,
        "deletions": 27
    }
}