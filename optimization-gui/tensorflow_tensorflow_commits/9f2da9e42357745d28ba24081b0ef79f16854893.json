{
    "author": "tensorflower-gardener",
    "message": "Add convenience method to RemapPlan to compute `input_devices_for_output_map` given `mappings`.\n\nPiperOrigin-RevId: 805920275",
    "sha": "9f2da9e42357745d28ba24081b0ef79f16854893",
    "files": [
        {
            "sha": "6fd64a8a52813ab0c5d36f8f7cf82c8248009cce",
            "filename": "third_party/xla/xla/python/ifrt/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2FBUILD?ref=9f2da9e42357745d28ba24081b0ef79f16854893",
            "patch": "@@ -910,7 +910,6 @@ xla_cc_test(\n         \"//xla:shape_util\",\n         \"//xla/pjrt:pjrt_layout\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/status\",\n@@ -935,7 +934,6 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/container:inlined_vector\","
        },
        {
            "sha": "fe4426fb4bc85236d9f252748bb69da8a7303ee1",
            "filename": "third_party/xla/xla/python/ifrt/remap_impl_test_lib.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_impl_test_lib.cc?ref=9f2da9e42357745d28ba24081b0ef79f16854893",
            "patch": "@@ -40,7 +40,6 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n \n@@ -52,7 +51,6 @@ using ::testing::ElementsAre;\n using ::testing::ElementsAreArray;\n using ::testing::HasSubstr;\n using ::testing::SizeIs;\n-using ::tsl::testing::StatusIs;\n \n // Returns a shape for an array whose first dimension is fully sharded across\n // `num_shards` devices. For example, [2, 3] with num_shards=5 becomes [10, 3].\n@@ -207,6 +205,7 @@ TEST(RemapImplTest, ExtractSingleShard) {\n       RemapPlan::Mapping{/*in_array=*/0, /*out_array=*/0,\n                          /*from=*/{RemapPlan::Interval{1, 2, 1}},\n                          /*to=*/{RemapPlan::Interval{0, 1, 1}}});\n+  TF_ASSERT_OK(plan.ComputeInputDevicesForOutputMap(client.get()));\n   TF_ASSERT_OK(plan.Validate());\n \n   std::vector<ArrayRef> arrays;\n@@ -259,6 +258,7 @@ TEST(RemapImplTest, InterleaveArraysDonate) {\n       RemapPlan::Mapping{/*in_array=*/1, /*out_array=*/0,\n                          /*from=*/{RemapPlan::Interval{0, 2, 1}},\n                          /*to=*/{RemapPlan::Interval{1, 4, 2}}});\n+  TF_ASSERT_OK(plan.ComputeInputDevicesForOutputMap(client.get()));\n   TF_ASSERT_OK(plan.Validate());\n \n   std::vector<ArrayRef> arrays;\n@@ -307,6 +307,7 @@ TEST(RemapImplTest, InterleaveArraysReuse) {\n       RemapPlan::Mapping{/*in_array=*/1, /*out_array=*/0,\n                          /*from=*/{RemapPlan::Interval{0, 2, 1}},\n                          /*to=*/{RemapPlan::Interval{1, 4, 2}}});\n+  TF_ASSERT_OK(plan.ComputeInputDevicesForOutputMap(client.get()));\n   TF_ASSERT_OK(plan.Validate());\n \n   std::vector<ArrayRef> arrays;\n@@ -349,6 +350,7 @@ TEST(RemapImplTest, DeinterleaveArrays) {\n       RemapPlan::Mapping{/*in_array=*/0, /*out_array=*/1,\n                          /*from=*/{RemapPlan::Interval{1, 4, 2}},\n                          /*to=*/{RemapPlan::Interval{0, 2, 1}}});\n+  TF_ASSERT_OK(plan.ComputeInputDevicesForOutputMap(client.get()));\n   TF_ASSERT_OK(plan.Validate());\n \n   std::vector<ArrayRef> arrays;\n@@ -423,6 +425,7 @@ TEST(RemapImplTest, BatchMappingIdentity) {\n                          /*out_array=*/1,\n                          /*from=*/{RemapPlan::Interval{0, 2, 1}},\n                          /*to=*/{RemapPlan::Interval{0, 2, 1}}});\n+  TF_ASSERT_OK(plan.ComputeInputDevicesForOutputMap(client.get()));\n   TF_ASSERT_OK(plan.Validate());\n \n   std::vector<ArrayRef> inputs;\n@@ -508,6 +511,7 @@ TEST(RemapImplTest, BatchMappingDeinterleave) {\n                          /*out_array=*/3,\n                          /*from=*/{RemapPlan::Interval{1, 2, 1}},\n                          /*to=*/{RemapPlan::Interval{0, 1, 1}}});\n+  TF_ASSERT_OK(plan.ComputeInputDevicesForOutputMap(client.get()));\n   TF_ASSERT_OK(plan.Validate());\n \n   std::vector<ArrayRef> inputs;\n@@ -554,6 +558,7 @@ TEST(RemapImplTest, DetectBadInput) {\n       RemapPlan::Mapping{/*in_array=*/0, /*out_array=*/0,\n                          /*from=*/{RemapPlan::Interval{0, 1, 1}},\n                          /*to=*/{RemapPlan::Interval{0, 1, 1}}});\n+  TF_ASSERT_OK(plan.ComputeInputDevicesForOutputMap(client.get()));\n   TF_ASSERT_OK(plan.Validate());\n \n   {"
        },
        {
            "sha": "6079dd9b1a6a93b1be8fea56e59bfd360bc88f30",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 0,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.cc?ref=9f2da9e42357745d28ba24081b0ef79f16854893",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/python/ifrt/device_list.h\"\n #include \"xla/python/ifrt/remap_plan.pb.h\"\n #include \"xla/python/ifrt/serdes_version.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -168,6 +169,22 @@ bool CheckOneInputForOneOutput(const xla::ifrt::RemapPlan& plan) {\n   return true;\n }\n \n+absl::StatusOr<DeviceListRef> ComputeDeviceListFromIntervals(\n+    Client* client, const DeviceListRef& device_list, int64_t count,\n+    absl::Span<const RemapPlan::Interval> intervals) {\n+  std::vector<Device*> devices;\n+  devices.reserve(count);\n+  for (const RemapPlan::Interval& interval : intervals) {\n+    int64_t index = interval.start;\n+    while (index < interval.end) {\n+      TF_RET_CHECK(index < device_list->size());\n+      devices.push_back(device_list->devices()[index]);\n+      index += interval.step;\n+    }\n+  }\n+  return client->MakeDeviceList(devices);\n+}\n+\n }  // namespace\n \n std::string RemapPlan::Interval::DebugString() const {\n@@ -190,6 +207,60 @@ std::string RemapPlan::Mapping::DebugString() const {\n                       \",to=\", format_intervals(to), \")\");\n }\n \n+absl::Status RemapPlan::ComputeInputDevicesForOutputMap(Client* client) {\n+  TF_RET_CHECK(mappings);\n+  TF_RET_CHECK(input_devices_for_output_map.empty());\n+  // A list of intervals along with the sum of entries across all the intervals.\n+  struct IntervalsAndCount {\n+    std::vector<Interval> intervals;\n+    int64_t count = 0;\n+  };\n+\n+  // Map from output array index to all its input contributors.\n+  //\n+  // The value is a map fron input array index to the intervals of that input\n+  // array that contribute to the given output.\n+  absl::flat_hash_map<int, absl::flat_hash_map<int, IntervalsAndCount>>\n+      output_to_inputs_and_intervals;\n+  for (const Mapping& mapping : *mappings) {\n+    IntervalsAndCount& intervals =\n+        output_to_inputs_and_intervals[mapping.out_array][mapping.in_array];\n+    for (const Interval& interval : mapping.from) {\n+      intervals.intervals.push_back(interval);\n+      intervals.count += GetNumberOfSteps(interval);\n+    }\n+  }\n+\n+  for (const auto& [out_array, input_intervals] :\n+       output_to_inputs_and_intervals) {\n+    TF_RET_CHECK(out_array < output_specs.size());\n+    const DeviceListRef& out_devices =\n+        output_specs[out_array].sharding->devices();\n+    auto [it, inserted] = input_devices_for_output_map.insert({out_array, {}});\n+    TF_RET_CHECK(inserted);\n+    for (const auto& [in_array, intervals] : input_intervals) {\n+      TF_RET_CHECK(in_array < input_specs.size());\n+      const DeviceListRef& in_devices =\n+          input_specs[in_array].sharding->devices();\n+      TF_RET_CHECK(intervals.count <= out_devices->size());\n+      TF_RET_CHECK(intervals.count <= in_devices->size());\n+      DeviceListRef interval_device_list;\n+      if (intervals.count == in_devices->size()) {\n+        interval_device_list = in_devices;\n+      } else if (intervals.count == out_devices->size()) {\n+        interval_device_list = out_devices;\n+      } else {\n+        TF_ASSIGN_OR_RETURN(\n+            interval_device_list,\n+            ComputeDeviceListFromIntervals(client, in_devices, intervals.count,\n+                                           intervals.intervals));\n+      }\n+      it->second.push_back({in_array, interval_device_list});\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n absl::Status RemapPlan::Validate() const {\n   const int num_inputs = input_specs.size();\n   if (num_inputs == 0) {"
        },
        {
            "sha": "b1b1f6354216506adf10f58798cb8f7fcfbb5635",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan.h?ref=9f2da9e42357745d28ba24081b0ef79f16854893",
            "patch": "@@ -121,6 +121,9 @@ struct RemapPlan {\n   // plan's validity, delegating the role to this method.\n   absl::Status Validate() const;\n \n+  // Fills in `input_devices_for_output_map` from `mappings`.\n+  absl::Status ComputeInputDevicesForOutputMap(Client* client);\n+\n   // Constructs `RemapPlan` from `RemapPlanProto`. Devices are looked up\n   // using `lookup_device`. Device ids in the proto must be consistent with\n   // the devices returned by `lookup_device`."
        },
        {
            "sha": "5897ca8d003c2dad3a9da2d79c188216e02c7cf6",
            "filename": "third_party/xla/xla/python/ifrt/remap_plan_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2da9e42357745d28ba24081b0ef79f16854893/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fremap_plan_test.cc?ref=9f2da9e42357745d28ba24081b0ef79f16854893",
            "patch": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n \n@@ -52,7 +51,6 @@ namespace {\n using ::testing::ElementsAreArray;\n using ::testing::HasSubstr;\n using ::testing::SizeIs;\n-using ::tsl::testing::StatusIs;\n \n class RemapPlanTest\n     : public testing::TestWithParam<test_util::DeviceTestParam> {\n@@ -135,6 +133,7 @@ TEST_P(RemapPlanTest, MixedDtype) {\n                          /*from=*/{RemapPlan::Interval{0, 1, 1}},\n                          /*to=*/{RemapPlan::Interval{0, 1, 1}}});\n \n+  TF_EXPECT_OK(plan.ComputeInputDevicesForOutputMap(client()));\n   TF_EXPECT_OK(plan.Validate());\n }\n \n@@ -639,8 +638,7 @@ TEST_P(RemapPlanTest, InvalidInputDevicesForOutputMap) {\n                              HasSubstr(\"does not reference that device\")));\n \n   plan.input_devices_for_output_map.clear();\n-  plan.input_devices_for_output_map.insert(\n-      {0, {{0, GetDevices({0})}, {1, GetDevices({1})}}});\n+  TF_EXPECT_OK(plan.ComputeInputDevicesForOutputMap(client()));\n   TF_EXPECT_OK(plan.Validate());\n }\n "
        }
    ],
    "stats": {
        "total": 91,
        "additions": 83,
        "deletions": 8
    }
}