{
    "author": "loislo",
    "message": "[XLA:GPU] Add basic support for HloOpcode::kScaledDot to HloEvaluator.\n\nThis change introduces `HandleScaledDot` and `EvaluateScaledDotOp` to the HloEvaluator. The `HandleScaledDotSlowPathWithLiterals` function is added to perform the dot product computation.\n\nPiperOrigin-RevId: 799586208",
    "sha": "183083f0ef349085f2192da5616efee3f75de546",
    "files": [
        {
            "sha": "f42aa27dae212428b238e14e5713f4c16b76be64",
            "filename": "third_party/xla/xla/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2FBUILD?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -867,8 +867,7 @@ cc_library(\n     visibility = internal_visibility([\":friends\"]),\n     deps = [\n         \":array\",\n-        \":types\",\n-        \"//xla/tsl/platform:logging\",\n+        \":util\",\n     ],\n )\n "
        },
        {
            "sha": "f75dbd38ed65a30c4a7e580056f800d672dd35c9",
            "filename": "third_party/xla/xla/array2d.h",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Farray2d.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Farray2d.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Farray2d.h?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -18,17 +18,12 @@ limitations under the License.\n \n #include <algorithm>\n #include <cstdint>\n-#include <functional>\n #include <initializer_list>\n-#include <iterator>\n #include <memory>\n-#include <random>\n #include <vector>\n \n #include \"absl/functional/function_ref.h\"\n-#include \"absl/strings/str_cat.h\"\n #include \"xla/array.h\"\n-#include \"xla/types.h\"\n #include \"xla/util.h\"\n \n namespace xla {"
        },
        {
            "sha": "e5b5eac65c4897a932371aece5bf5cb3c2a303b4",
            "filename": "third_party/xla/xla/array3d.h",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Farray3d.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Farray3d.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Farray3d.h?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -16,17 +16,12 @@ limitations under the License.\n #ifndef XLA_ARRAY3D_H_\n #define XLA_ARRAY3D_H_\n \n-#include <algorithm>\n-#include <functional>\n+#include <cstdint>\n #include <initializer_list>\n-#include <iterator>\n-#include <memory>\n-#include <numeric>\n-#include <random>\n+#include <vector>\n \n #include \"xla/array.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/types.h\"\n+#include \"xla/util.h\"\n \n namespace xla {\n \n@@ -66,6 +61,19 @@ class Array3D : public Array<T> {\n   int64_t n1() const { return this->dim(0); }\n   int64_t n2() const { return this->dim(1); }\n   int64_t n3() const { return this->dim(2); }\n+\n+  void FillUnique(T start_value = 0) {\n+    int shift2 = Log2Ceiling<uint64_t>(n2());\n+    int shift3 = Log2Ceiling<uint64_t>(n3());\n+    for (int64_t i0 = 0; i0 < n1(); ++i0) {\n+      for (int64_t i1 = 0; i1 < n2(); ++i1) {\n+        for (int64_t i2 = 0; i2 < n3(); ++i2) {\n+          (*this)(i0, i1, i2) =\n+              ((i0 << (shift3 + shift2)) | (i1 << shift2) | i2) + start_value;\n+        }\n+      }\n+    }\n+  }\n };\n \n }  // namespace xla"
        },
        {
            "sha": "ac0dfdf03f5da62549704754bb2d90b5903f0f08",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -3677,6 +3677,27 @@ INSTANTIATE_TEST_SUITE_P(\n                        ::testing::ValuesIn(AllXlaDataTypes())),\n     DotUnsetAlgorithmEmitterTest::ParamToString);\n \n+TEST_F(TritonEmitterTest, ScaledDotIsSupportedByReferencePlatform) {\n+  if (!std::get_if<se::CudaComputeCapability>(&GpuComputeCapability())) {\n+    GTEST_SKIP() << \"Ignore scaled dot test on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule ScaledDotIsSupportedByReferencePlatform\n+\n+    ENTRY entry {\n+     lhs = bf16[4,4] parameter(0)\n+     lhs_scale = bf16[1,1] parameter(1)\n+     rhs = bf16[4,4] parameter(2)\n+     rhs_scale = bf16[1,1] parameter(3)\n+     ROOT dot = bf16[4,4] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+         lhs_contracting_dims={1},\n+         rhs_contracting_dims={1}\n+    }\n+  )\";\n+\n+  EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   if (std::get_if<se::CudaComputeCapability>(&GpuComputeCapability())) {\n     GTEST_SKIP() << \"Warp size is always 32 on CUDA\";"
        },
        {
            "sha": "2420a3bdbc3f606478197db46a253b9585c872b4",
            "filename": "third_party/xla/xla/hlo/evaluator/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2FBUILD?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -105,6 +105,7 @@ cc_library(\n         \"@eigen_archive//:eigen3\",\n         \"@local_tsl//tsl/platform:ml_dtypes\",\n         \"@local_tsl//tsl/platform:platform_port\",\n+        \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n "
        },
        {
            "sha": "d47bbe46ff530c45ad0eab7681a83126dfa2876e",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.cc?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -1128,6 +1128,31 @@ absl::StatusOr<Literal> HloEvaluator::EvaluateDotOp(\n   return Evaluate(cloned_instruction.get());\n }\n \n+absl::StatusOr<Literal> HloEvaluator::EvaluateScaledDotOp(\n+    const DotDimensionNumbers& dim_numbers,\n+    const PrecisionConfig& precision_config, const Literal& lhs,\n+    const Literal& lhs_scale, const Literal& rhs, const Literal& rhs_scale) {\n+  std::unique_ptr<HloInstruction> lhs_instr =\n+      HloInstruction::CreateConstant(lhs.Clone());\n+  std::unique_ptr<HloInstruction> lhs_scale_instr =\n+      HloInstruction::CreateConstant(lhs_scale.Clone());\n+  std::unique_ptr<HloInstruction> rhs_instr =\n+      HloInstruction::CreateConstant(rhs.Clone());\n+  std::unique_ptr<HloInstruction> rhs_scale_instr =\n+      HloInstruction::CreateConstant(rhs_scale.Clone());\n+\n+  TF_ASSIGN_OR_RETURN(\n+      Shape dot_shape,\n+      ShapeInference::InferDotOpShape(lhs.shape(), rhs.shape(), dim_numbers,\n+                                      /*preferred_element_type=*/std::nullopt));\n+\n+  std::unique_ptr<HloInstruction> cloned_instruction =\n+      HloInstruction::CreateScaledDot(\n+          dot_shape, lhs_instr.get(), lhs_scale_instr.get(), rhs_instr.get(),\n+          rhs_scale_instr.get(), dim_numbers, precision_config);\n+  return Evaluate(cloned_instruction.get());\n+}\n+\n absl::Status HloEvaluator::EvaluateParameterFromCallerArgument(\n     const HloInstruction* parameter, const ShapeIndex& shape_index,\n     PrecomputedAnalyses analyses) {"
        },
        {
            "sha": "3e9755627d39f09ac46701df73624c4214d14e1d",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator.h?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -193,6 +193,10 @@ class HloEvaluator : public ConstDfsHloVisitorWithDefault,\n   absl::StatusOr<Literal> EvaluateDotOp(const DotDimensionNumbers& dim_numbers,\n                                         const PrecisionConfig& precision_config,\n                                         const Literal& lhs, const Literal& rhs);\n+  absl::StatusOr<Literal> EvaluateScaledDotOp(\n+      const DotDimensionNumbers& dim_numbers,\n+      const PrecisionConfig& precision_config, const Literal& lhs,\n+      const Literal& lhs_scale, const Literal& rhs, const Literal& rhs_scale);\n \n   void set_dynamic_dimension_inference(\n       DynamicDimensionInference* dynamic_dimension_inference) override {"
        },
        {
            "sha": "05f671e5e3b46ef101a2adc8c1169aa994cd75a0",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator_test.cc",
            "status": "modified",
            "additions": 180,
            "deletions": 0,
            "changes": 180,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_test.cc?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -1260,6 +1260,186 @@ TEST_F(HloEvaluatorTest, RaggedDotNonContractingWithBatchDimensions) {\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected, result));\n }\n \n+HloInstruction* BF16Array2D(HloComputation::Builder& b, int rows, int cols,\n+                            float value) {\n+  auto array = std::make_unique<Array2D<float>>(rows, cols);\n+  array->FillUnique(value);\n+  auto literal = LiteralUtil::CreateR2FromArray2D<float>(*array);\n+  auto bf16_literal = LiteralUtil::ConvertF32ToBF16(literal);\n+  return b.AddInstruction(\n+      HloInstruction::CreateConstant(std::move(bf16_literal)));\n+}\n+\n+HloInstruction* BF16Array3D(HloComputation::Builder& b, int batch, int rows,\n+                            int cols, float value) {\n+  auto array = std::make_unique<Array3D<float>>(batch, rows, cols);\n+  array->FillUnique(value);\n+  auto literal = LiteralUtil::CreateR3FromArray3D<float>(*array);\n+  auto bf16_literal = LiteralUtil::ConvertF32ToBF16(literal);\n+  return b.AddInstruction(\n+      HloInstruction::CreateConstant(std::move(bf16_literal)));\n+}\n+\n+TEST_F(HloEvaluatorTest, ScaledDot) {\n+  HloComputation::Builder b(TestName());\n+\n+  auto lhs_instr = BF16Array2D(b, 1, 4, 1.0f);\n+  auto lhs_scale_instr = BF16Array2D(b, 1, 2, 2.0f);\n+  auto rhs_instr = BF16Array2D(b, 4, 1, 1.0f);\n+  auto rhs_scale_instr = BF16Array2D(b, 2, 1, 3.0f);\n+\n+  Shape shape = ShapeUtil::MakeShape(BF16, {1, 1});\n+  DotDimensionNumbers dot_dnums;\n+  dot_dnums.add_lhs_contracting_dimensions(1);\n+  dot_dnums.add_rhs_contracting_dimensions(0);\n+  b.AddInstruction(HloInstruction::CreateScaledDot(\n+      shape, lhs_instr, lhs_scale_instr, rhs_instr, rhs_scale_instr, dot_dnums,\n+      DefaultPrecisionConfig(4)));\n+  m_->AddEntryComputation(b.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result, Evaluate());\n+\n+  // lhs[1,4] = {{1, 2, 3, 4}}\n+  // lhs_scale[1,2] = {{2, 3}}\n+  // rhs[4,1] = {\n+  //   {1},\n+  //   {2},\n+  //   {3},\n+  //   {4}\n+  // }\n+  // rhs_scale[2,1] = {\n+  //   {3},\n+  //   {4}\n+  // }\n+\n+  // lhs * lhs_scale * rhs * rhs_scale\n+  // 1 * 2 * 1 * 3 = 6\n+  // 2 * 2 * 2 * 3 = 24\n+  // 3 * 3 * 3 * 4 = 108\n+  // 4 * 3 * 4 * 4 = 192\n+  //           sum = 330\n+  auto expected_array = Array2D<float>({{330.f}});\n+  auto expected = LiteralUtil::CreateR2FromArray2D<float>(expected_array);\n+  auto expected_bf16 = LiteralUtil::ConvertF32ToBF16(expected);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_bf16, result));\n+}\n+\n+TEST_F(HloEvaluatorTest, ScaledDotWithOneMissingScale) {\n+  HloComputation::Builder b(TestName());\n+\n+  auto lhs_instr = BF16Array2D(b, 1, 4, 1.0f);\n+  auto lhs_scale_instr = b.AddInstruction(\n+      HloInstruction::CreateConstant(LiteralUtil::CreateR0(BF16, 1.0f)));\n+  auto rhs_instr = BF16Array2D(b, 4, 1, 1.0f);\n+  auto rhs_scale_instr = BF16Array2D(b, 2, 1, 3.0f);\n+\n+  Shape shape = ShapeUtil::MakeShape(BF16, {1, 1});\n+  DotDimensionNumbers dot_dnums;\n+  dot_dnums.add_lhs_contracting_dimensions(1);\n+  dot_dnums.add_rhs_contracting_dimensions(0);\n+  b.AddInstruction(HloInstruction::CreateScaledDot(\n+      shape, lhs_instr, lhs_scale_instr, rhs_instr, rhs_scale_instr, dot_dnums,\n+      DefaultPrecisionConfig(4)));\n+  m_->AddEntryComputation(b.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result, Evaluate());\n+\n+  // lhs[1,4] = {{1, 2, 3, 4}}\n+  // lhs_scal = 1.0f\n+  // rhs[4,1] = {\n+  //   {1},\n+  //   {2},\n+  //   {3},\n+  //   {4}\n+  // }\n+  // rhs_scale[2,1] = {\n+  //   {3},\n+  //   {4}\n+  // }\n+\n+  // lhs * lhs_scale * rhs * rhs_scale\n+  // 1 * 1 * 1 * 3 = 3\n+  // 2 * 1 * 2 * 3 = 12\n+  // 3 * 1 * 3 * 4 = 36\n+  // 4 * 1 * 4 * 4 = 64\n+  //           sum = 115\n+  auto expected_array = Array2D<float>({{115.f}});\n+  auto expected = LiteralUtil::CreateR2FromArray2D<float>(expected_array);\n+  auto expected_bf16 = LiteralUtil::ConvertF32ToBF16(expected);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_bf16, result));\n+}\n+\n+TEST_F(HloEvaluatorTest, ScaledDotWithBatchDim) {\n+  HloComputation::Builder b(TestName());\n+\n+  auto lhs_instr = BF16Array3D(b, 2, 1, 4, 1.0f);\n+  auto lhs_scale_instr = b.AddInstruction(\n+      HloInstruction::CreateConstant(LiteralUtil::CreateR0(BF16, 1.0f)));\n+  auto rhs_instr = BF16Array3D(b, 2, 4, 1, 1.0f);\n+  auto rhs_scale_instr = BF16Array3D(b, 1, 2, 1, 3.0f);\n+\n+  Shape shape = ShapeUtil::MakeShape(BF16, {2, 1, 1});\n+  DotDimensionNumbers dot_dnums;\n+  dot_dnums.add_lhs_batch_dimensions(0);\n+  dot_dnums.add_rhs_batch_dimensions(0);\n+\n+  dot_dnums.add_lhs_contracting_dimensions(2);\n+  dot_dnums.add_rhs_contracting_dimensions(1);\n+  b.AddInstruction(HloInstruction::CreateScaledDot(\n+      shape, lhs_instr, lhs_scale_instr, rhs_instr, rhs_scale_instr, dot_dnums,\n+      DefaultPrecisionConfig(4)));\n+  m_->AddEntryComputation(b.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(Literal result, Evaluate());\n+  // lhs[2,1,4] = {\n+  //   {{ 1, 2, 3, 4 }},\n+  //   {{ 5, 6, 7, 8 }}\n+  // }\n+\n+  // rhs[2,4,1] = {\n+  //   {\n+  //     {1},\n+  //     {5},\n+  //     {9},\n+  //     {13}\n+  //   },\n+  //   {\n+  //     {5},\n+  //     {5},\n+  //     {13},\n+  //     {13}\n+  //   }\n+  // }\n+  // rhs_scale[2,2,1] = {\n+  //   {\n+  //     {3},\n+  //     {5}\n+  //   },\n+  //   {\n+  //     {3},\n+  //     {5}\n+  //   }\n+  // }\n+  // 1 * 1 * 1 * 3 = 3\n+  // 2 * 1 * 5 * 3 = 30\n+  // 3 * 1 * 9 * 5 = 135\n+  // 4 * 1 * 13 * 5 = 260\n+  // result_val: 428\n+  //\n+  // 5 * 1 * 5 * 3 = 75\n+  // 6 * 1 * 5 * 3 = 90\n+  // 7 * 1 * 13 * 5 = 455\n+  // 8 * 1 * 13 * 5 = 520\n+  // result_val: 1140\n+\n+  // The expectation does not match exact value the result due to the rounding\n+  // to bf16.\n+  auto expected_array = Array3D<float>({{{428.f}}, {{1136.f}}});\n+  auto expected = LiteralUtil::CreateR3FromArray3D<float>(expected_array);\n+  auto expected_bf16 = LiteralUtil::ConvertF32ToBF16(expected);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_bf16, result));\n+}\n+\n TEST_P(HloEvaluatorBf16Test, DotRank2AndRank1) {\n   HloComputation::Builder b(TestName());\n "
        },
        {
            "sha": "01b162696976f93b3d4621ec4f144f04b12a74b7",
            "filename": "third_party/xla/xla/hlo/evaluator/hlo_evaluator_typed_visitor.h",
            "status": "modified",
            "additions": 224,
            "deletions": 1,
            "changes": 225,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fevaluator%2Fhlo_evaluator_typed_visitor.h?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -28,11 +28,11 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <random>\n+#include <tuple>\n #include <type_traits>\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/base/attributes.h\"\n #include \"absl/base/casts.h\"\n #include \"absl/log/check.h\"\n@@ -59,6 +59,7 @@ limitations under the License.\n #include \"xla/types.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/protobuf.h\"  // IWYU pragma: keep\n \n namespace xla {\n \n@@ -1456,6 +1457,228 @@ class HloEvaluatorTypedVisitor : public ConstDfsHloVisitorWithDefault {\n                       gs_literal.Convert(PrimitiveType::S64).value()));\n   }\n \n+  absl::Status HandleScaledDot(const HloInstruction* dot) override {\n+    auto lhs = dot->operand(0);\n+    auto lhs_scale = dot->operand(1);\n+    auto rhs = dot->operand(2);\n+    auto rhs_scale = dot->operand(3);\n+    CHECK(dot->shape().IsArray());\n+    CHECK(lhs->shape().IsArray());\n+    CHECK(rhs->shape().IsArray());\n+    CHECK(lhs_scale->shape().IsArray());\n+    CHECK(rhs_scale->shape().IsArray());\n+    CHECK(lhs_scale->shape().dimensions().size() == 0 ||\n+          lhs_scale->shape().dimensions().size() ==\n+              lhs->shape().dimensions().size());\n+    CHECK(rhs_scale->shape().dimensions().size() == 0 ||\n+          rhs_scale->shape().dimensions().size() ==\n+              rhs->shape().dimensions().size());\n+    TF_ASSIGN_OR_RETURN(const Literal lhs_literal,\n+                        parent_->GetEvaluatedLiteralFor(lhs).Convert(\n+                            dot->shape().element_type()));\n+    TF_ASSIGN_OR_RETURN(const Literal rhs_literal,\n+                        parent_->GetEvaluatedLiteralFor(rhs).Convert(\n+                            dot->shape().element_type()));\n+\n+    // If the scale is a scalar, we can just use 1.0. Otherwise, we need to\n+    // evaluate the scale.\n+    auto evaluate_scale =\n+        [&](const HloInstruction* operand,\n+            const HloInstruction* scale) -> absl::StatusOr<Literal> {\n+      if (scale->shape().IsArray() && scale->shape().dimensions().size() > 0) {\n+        TF_ASSIGN_OR_RETURN(Literal scale_literal,\n+                            parent_->GetEvaluatedLiteralFor(scale).Convert(\n+                                dot->shape().element_type()));\n+        return scale_literal;\n+      }\n+      std::vector<int64_t> ones(operand->shape().dimensions().size(), 1);\n+      Shape scale_shape =\n+          ShapeUtil::MakeShape(dot->shape().element_type(), ones);\n+      Literal scale_literal = Literal::CreateFromShape(scale_shape);\n+      scale_literal.PopulateWithValue(static_cast<ReturnT>(1.0f));\n+      return scale_literal;\n+    };\n+    TF_ASSIGN_OR_RETURN(Literal lhs_scale_literal,\n+                        evaluate_scale(lhs, lhs_scale));\n+    TF_ASSIGN_OR_RETURN(Literal rhs_scale_literal,\n+                        evaluate_scale(rhs, rhs_scale));\n+    return HandleScaledDotSlowPathWithLiterals(\n+        dot, lhs_literal, lhs_scale_literal, rhs_literal, rhs_scale_literal);\n+  }\n+\n+ private:\n+  struct ShapeInfo {\n+    static std::pair<DimensionVector, DimensionVector> dims(\n+        const DimensionVector& dim_indexes, const Shape& literal_shape,\n+        const Shape& scale_shape) {\n+      DimensionVector dim_sizes;\n+      DimensionVector dim_scale_divisors;\n+      for (int64_t i = 0; i < dim_indexes.size(); ++i) {\n+        dim_sizes.push_back(literal_shape.dimensions(dim_indexes[i]));\n+        dim_scale_divisors.push_back(literal_shape.dimensions(dim_indexes[i]) /\n+                                     scale_shape.dimensions(dim_indexes[i]));\n+      }\n+      return {dim_sizes, dim_scale_divisors};\n+    }\n+\n+    ShapeInfo(\n+        const Literal& literal, const Literal& scale_literal,\n+        const tsl::protobuf::RepeatedField<int64_t>& contracting_dims_field,\n+        const tsl::protobuf::RepeatedField<int64_t>& batch_dims_field)\n+        : rank(literal.shape().dimensions().size()) {\n+      batch_dim_indexes =\n+          DimensionVector(batch_dims_field.begin(), batch_dims_field.end());\n+      std::tie(batch_dim_sizes, batch_dim_scale_divisors) =\n+          dims(batch_dim_indexes, literal.shape(), scale_literal.shape());\n+\n+      non_contracting_dim_indexes =\n+          GetNonContractingDims(rank, contracting_dims_field, batch_dims_field);\n+      std::tie(non_contracting_dim_sizes, non_contracting_dim_scale_divisors) =\n+          dims(non_contracting_dim_indexes, literal.shape(),\n+               scale_literal.shape());\n+\n+      contracting_dim_indexes = DimensionVector(contracting_dims_field.begin(),\n+                                                contracting_dims_field.end());\n+      std::tie(contracting_dim_sizes, contracting_dim_scale_divisors) =\n+          dims(contracting_dim_indexes, literal.shape(), scale_literal.shape());\n+    }\n+\n+    const int64_t rank;\n+    DimensionVector batch_dim_indexes;\n+    DimensionVector batch_dim_sizes;\n+    DimensionVector batch_dim_scale_divisors;\n+\n+    DimensionVector non_contracting_dim_indexes;\n+    DimensionVector non_contracting_dim_sizes;\n+    DimensionVector non_contracting_dim_scale_divisors;\n+\n+    DimensionVector contracting_dim_indexes;\n+    DimensionVector contracting_dim_sizes;\n+    DimensionVector contracting_dim_scale_divisors;\n+  };\n+\n+  absl::Status HandleScaledDotSlowPathWithLiterals(\n+      const HloInstruction* dot, const Literal& lhs_literal,\n+      const Literal& lhs_scale_literal, const Literal& rhs_literal,\n+      const Literal& rhs_scale_literal) {\n+    const auto& dnums = dot->dot_dimension_numbers();\n+    CHECK(ShapeUtil::SameElementType(lhs_literal.shape(), rhs_literal.shape()));\n+    CHECK(ShapeUtil::SameElementType(lhs_literal.shape(), dot->shape()));\n+\n+    CHECK_EQ(dnums.lhs_batch_dimensions_size(),\n+             dnums.rhs_batch_dimensions_size());\n+\n+    ShapeInfo lhs_info(lhs_literal, lhs_scale_literal,\n+                       dnums.lhs_contracting_dimensions(),\n+                       dnums.lhs_batch_dimensions());\n+    ShapeInfo rhs_info(rhs_literal, rhs_scale_literal,\n+                       dnums.rhs_contracting_dimensions(),\n+                       dnums.rhs_batch_dimensions());\n+    const int64_t total_contraction_size =\n+        Product(lhs_info.contracting_dim_sizes);\n+    Shape dot_shape = GetShapeWithLayout(dot->shape());\n+\n+    TF_ASSIGN_OR_RETURN(Literal result, Literal::Make(dot_shape));\n+    TF_RETURN_IF_ERROR(result.PopulateParallel<ReturnT>(\n+        [&](absl::Span<const int64_t> result_index, int /*thread_id*/) {\n+          // Locations in LHS and RHS that we read from.\n+          DimensionVector lhs_index(lhs_info.rank);\n+          DimensionVector lhs_scale_index(lhs_info.rank);\n+          DimensionVector rhs_index(rhs_info.rank);\n+          DimensionVector rhs_scale_index(rhs_info.rank);\n+\n+          // First come the batch dimensions.\n+          int64_t idx = 0;\n+          for (int64_t i = 0; i < dnums.lhs_batch_dimensions_size(); i++) {\n+            lhs_index[dnums.lhs_batch_dimensions(i)] = result_index[idx];\n+            rhs_index[dnums.rhs_batch_dimensions(i)] = result_index[idx];\n+            lhs_scale_index[dnums.lhs_batch_dimensions(i)] =\n+                result_index[idx] / lhs_info.batch_dim_scale_divisors[i];\n+            rhs_scale_index[dnums.rhs_batch_dimensions(i)] =\n+                result_index[idx] / rhs_info.batch_dim_scale_divisors[i];\n+            idx++;\n+          }\n+\n+          // Next we have non-contracting dimensions, if any.\n+          for (int64_t i = 0; i < lhs_info.non_contracting_dim_indexes.size();\n+               i++) {\n+            lhs_index[lhs_info.non_contracting_dim_indexes[i]] =\n+                result_index[idx];\n+            lhs_scale_index[lhs_info.non_contracting_dim_indexes[i]] =\n+                result_index[idx] /\n+                lhs_info.non_contracting_dim_scale_divisors[i];\n+            idx++;\n+          }\n+          for (int64_t i = 0; i < rhs_info.non_contracting_dim_indexes.size();\n+               i++) {\n+            rhs_index[rhs_info.non_contracting_dim_indexes[i]] =\n+                result_index[idx];\n+            rhs_scale_index[rhs_info.non_contracting_dim_indexes[i]] =\n+                result_index[idx] /\n+                rhs_info.non_contracting_dim_scale_divisors[i];\n+            idx++;\n+          }\n+\n+          auto get_val = [](const Literal& literal,\n+                            const DimensionVector& index) {\n+            return ToArithmeticSafeType(\n+                static_cast<ElementwiseT>(literal.Get<ReturnT>(index)));\n+          };\n+          // Accumulate resulting product along the contracting dimensions.\n+          ElementwiseT result_val = static_cast<ElementwiseT>(0);\n+          for (int64_t k = 0; k < total_contraction_size; k++) {\n+            const auto lhs = get_val(lhs_literal, lhs_index);\n+            const auto lhs_scale = get_val(lhs_scale_literal, lhs_scale_index);\n+            const auto rhs = get_val(rhs_literal, rhs_index);\n+            const auto rhs_scale = get_val(rhs_scale_literal, rhs_scale_index);\n+            result_val += lhs * lhs_scale * rhs * rhs_scale;\n+\n+            if (parent_->trace_mac_handler_ != nullptr) {\n+              const int64_t result_linear_index =\n+                  IndexUtil::MultidimensionalIndexToLinearIndex(dot_shape,\n+                                                                result_index);\n+              const int64_t lhs_linear_index =\n+                  IndexUtil::MultidimensionalIndexToLinearIndex(\n+                      lhs_literal.shape(), lhs_index);\n+              const int64_t rhs_linear_index =\n+                  IndexUtil::MultidimensionalIndexToLinearIndex(\n+                      rhs_literal.shape(), rhs_index);\n+\n+              parent_->trace_mac_handler_(result_linear_index, lhs_linear_index,\n+                                          rhs_linear_index);\n+            }\n+\n+            // If there are no contracting dimensions, do not try to count down\n+            // from -1 to 0; that's an infinite loop.\n+            if (!lhs_info.contracting_dim_sizes.empty()) {\n+              for (int64_t i = lhs_info.contracting_dim_sizes.size() - 1;\n+                   i >= 0; --i) {\n+                lhs_index[lhs_info.contracting_dim_indexes[i]]++;\n+                lhs_scale_index[lhs_info.contracting_dim_indexes[i]] =\n+                    lhs_index[lhs_info.contracting_dim_indexes[i]] /\n+                    lhs_info.contracting_dim_scale_divisors[i];\n+                rhs_index[rhs_info.contracting_dim_indexes[i]]++;\n+                rhs_scale_index[rhs_info.contracting_dim_indexes[i]] =\n+                    rhs_index[rhs_info.contracting_dim_indexes[i]] /\n+                    rhs_info.contracting_dim_scale_divisors[i];\n+                if (lhs_index[lhs_info.contracting_dim_indexes[i]] !=\n+                    lhs_info.contracting_dim_sizes[i]) {\n+                  break;\n+                }\n+                lhs_index[lhs_info.contracting_dim_indexes[i]] = 0;\n+                rhs_index[rhs_info.contracting_dim_indexes[i]] = 0;\n+              }\n+            }\n+          }\n+\n+          return static_cast<ReturnT>(result_val);\n+        }));\n+\n+    parent_->SetEvaluatedLiteralFor(dot, std::move(result));\n+    return absl::OkStatus();\n+  }\n+\n+ public:\n   absl::Status HandlePad(const HloInstruction* pad) override {\n     CHECK(pad->operand(0)->shape().IsArray());\n     // Padding value must be scalar."
        },
        {
            "sha": "bd4ff48e154c89082e6c62d1e1f392ec7e66db40",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/183083f0ef349085f2192da5616efee3f75de546/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.cc?ref=183083f0ef349085f2192da5616efee3f75de546",
            "patch": "@@ -2700,6 +2700,7 @@ std::unique_ptr<HloInstruction> HloInstruction::CloneWithNewOperands(\n     case HloOpcode::kIota:\n     case HloOpcode::kDot:\n     case HloOpcode::kRaggedDot:\n+    case HloOpcode::kScaledDot:\n     case HloOpcode::kDomain:\n     case HloOpcode::kGetDimensionSize:\n     case HloOpcode::kSetDimensionSize:"
        }
    ],
    "stats": {
        "total": 489,
        "additions": 473,
        "deletions": 16
    }
}