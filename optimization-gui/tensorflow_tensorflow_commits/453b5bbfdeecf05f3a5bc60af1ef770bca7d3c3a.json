{
    "author": "fengwuyao",
    "message": "Internal changes only.\n\nPiperOrigin-RevId: 846835877",
    "sha": "453b5bbfdeecf05f3a5bc60af1ef770bca7d3c3a",
    "files": [
        {
            "sha": "94484350f68bcd19403ed31267bc3576a3b11cfd",
            "filename": "tensorflow/lite/types/fp16.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/453b5bbfdeecf05f3a5bc60af1ef770bca7d3c3a/tensorflow%2Flite%2Ftypes%2Ffp16.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/453b5bbfdeecf05f3a5bc60af1ef770bca7d3c3a/tensorflow%2Flite%2Ftypes%2Ffp16.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftypes%2Ffp16.h?ref=453b5bbfdeecf05f3a5bc60af1ef770bca7d3c3a",
            "patch": "@@ -27,6 +27,13 @@ limitations under the License.\n // - https://github.com/google/XNNPACK/issues/6989\n // We also don't need a lot of the functionality in the upstream library.\n \n+// If building with a library that uses //third_party/FP16, that library\n+// provides its own fp16 conversion functions. Avoid redefining them here to\n+// prevent build errors.\n+// FP16_H and FP16_BITCASTS_H are defined by //third_party/FP16/fp16.h and\n+// //third_party/FP16/bitcasts.h respectively.\n+#if !defined(FP16_H) && !defined(FP16_BITCASTS_H)\n+\n static inline float fp32_from_bits(uint32_t w) {\n   union {\n     uint32_t as_bits;\n@@ -216,4 +223,6 @@ static inline uint16_t fp16_ieee_from_fp32_value(float f) {\n          (shl1_w > UINT32_C(0xFF000000) ? UINT16_C(0x7E00) : nonsign);\n }\n \n+#endif  // !defined(FP16_H) && !defined(FP16_BITCASTS_H)\n+\n #endif  // TENSORFLOW_LITE_TYPES_FP16_H_"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 9,
        "deletions": 0
    }
}