{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Collect requested multimem objects in thunks in prepare state and construct them once per HLO run.\n\nMultimem object should be allocated and sets up multimem object once per clique and device address.\n\nPiperOrigin-RevId: 844799607",
    "sha": "b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
    "files": [
        {
            "sha": "53daeea846b443ec67b5428a3f4df8338b4df6dd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 28,
            "deletions": 1,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -1269,6 +1269,7 @@ cc_library(\n         \":collective_cliques\",\n         \":collective_metadata_thunk\",\n         \":collective_multimem\",\n+        \":collective_params\",\n         \":collective_thunk\",\n         \":thunk\",\n         \"//xla:shape_util\",\n@@ -1278,7 +1279,7 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/core/collectives:reduction_kind\",\n-        \"//xla/service:collective_ops_utils\",\n+        \"//xla/runtime:device_id\",\n         \"//xla/service/gpu:gpu_constants\",\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:stream_executor_util\",\n@@ -1316,12 +1317,15 @@ xla_test(\n     backends = [\"h100\"],\n     deps = [\n         \":collective_kernel_thunk\",\n+        \":collective_multimem_registry\",\n         \":collective_params\",\n         \":collective_thunk\",\n         \":thunk\",\n         \"//xla:array\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/gpu/runtime:collective_clique_requests\",\n+        \"//xla/core/collectives:reduction_kind\",\n         \"//xla/pjrt:worker_thread\",\n         \"//xla/runtime:device_id\",\n         \"//xla/service:buffer_assignment\",\n@@ -1675,6 +1679,26 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"collective_multimem_registry\",\n+    srcs = [\"collective_multimem_registry.cc\"],\n+    hdrs = [\"collective_multimem_registry.h\"],\n+    compatible_with = get_compatible_with_portable(),\n+    deps = [\n+        \":collective_multimem\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/runtime:device_id\",\n+        \"//xla/stream_executor:device_address\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:nullability\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+    ],\n+)\n+\n cc_library(\n     name = \"collective_params\",\n     srcs = [\"collective_params.cc\"],\n@@ -2034,6 +2058,7 @@ cc_library(\n     hdrs = [\"collective_metadata_thunk.h\"],\n     deps = [\n         \":collective_multimem\",\n+        \":collective_multimem_registry\",\n         \":collective_thunk\",\n         \":thunk\",\n         \"//xla:shape_util\",\n@@ -2052,6 +2077,7 @@ cc_library(\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n@@ -2159,6 +2185,7 @@ cc_library(\n     deps = [\n         \":collective_clique_requests\",\n         \":collective_cliques\",\n+        \":collective_multimem_registry\",\n         \":collective_params\",\n         \":thunk_id\",\n         \":thunk_proto_cc\","
        },
        {
            "sha": "624e0124c086fb45bb5913da1509dea3cb65d76e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -202,6 +202,7 @@ bool IsAllReduceKernelSupported(int64_t num_ranks, int64_t num_elements,\n                                 ReductionKind reduction_kind,\n                                 AllReduceStrategy all_reduce_strategy) {\n   if (!IsElementReductionSupported(element_type, reduction_kind)) {\n+    VLOG(3) << \"Element type and reduction kind combination is not supported.\";\n     return false;\n   }\n   const int64_t alignment_requirement =\n@@ -211,6 +212,8 @@ bool IsAllReduceKernelSupported(int64_t num_ranks, int64_t num_elements,\n           : se::gpu::kNumElementsPerThread * num_ranks;\n \n   if (num_elements % alignment_requirement != 0) {\n+    VLOG(3)\n+        << \"Number of elements is not aligned to the alignment requirement.\";\n     return false;\n   }\n "
        },
        {
            "sha": "c8efc4c3c933cdcf84b554336a2687281253b055",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -153,9 +153,10 @@ absl::Status AllReduceStartThunk::Initialize(const InitializeParams& params) {\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, config()));\n-  TF_ASSIGN_OR_RETURN(bool use_collective_kernel,\n-                      collective_kernel_thunk_->IsSupported(\n-                          clique_key, params.collective_cliques));\n+  TF_ASSIGN_OR_RETURN(\n+      bool use_collective_kernel,\n+      collective_kernel_thunk_->IsSupported(clique_key, *params.executor,\n+                                            *params.collective_params));\n   if (use_collective_kernel) {\n     TF_RETURN_IF_ERROR(collective_kernel_thunk_->Initialize(params));\n   }\n@@ -170,9 +171,10 @@ absl::StatusOr<bool> AllReduceStartThunk::RunCollective(\n       ConvertToDeviceBuffers(params, buffers_,\n                              config_.config.operand_element_type));\n \n-  TF_ASSIGN_OR_RETURN(bool use_collective_kernel,\n-                      collective_kernel_thunk_->IsSupported(\n-                          clique_key, params.collective_cliques));\n+  TF_ASSIGN_OR_RETURN(\n+      bool use_collective_kernel,\n+      collective_kernel_thunk_->IsSupported(\n+          clique_key, *params.stream->parent(), *params.collective_params));\n \n   if (use_collective_kernel) {\n     TF_RETURN_IF_ERROR(collective_kernel_thunk_->ExecuteOnStream(params));"
        },
        {
            "sha": "703babbbb6f64887a5c92cc300d35bf048f67c68",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 124,
            "deletions": 77,
            "changes": 201,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -33,12 +33,12 @@ limitations under the License.*/\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/all_reduce.h\"\n-#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n #include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n-#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/runtime/device_id.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n@@ -86,49 +86,125 @@ absl::StatusOr<se::DeviceAddressHandle> AllocateMemory(\n   return local_buffer_alloc;\n };\n \n+absl::StatusOr<int> GetLocalDeviceId(\n+    const GlobalDeviceId& global_device_id,\n+    const CollectiveParams& collective_params) {\n+  // If the global device id map is not provided, then we can assume that\n+  // execution is local.\n+  if (!collective_params.global_device_id_map) {\n+    return global_device_id.value();\n+  }\n+\n+  for (const auto& local_device : *collective_params.global_device_id_map) {\n+    if (local_device.second == global_device_id) {\n+      return local_device.first.value();\n+    }\n+  }\n+  return absl::NotFoundError(\n+      absl::StrFormat(\"Global device id %d not found in global device id map.\",\n+                      global_device_id.value()));\n+}\n+\n }  // namespace\n \n absl::StatusOr<bool> CollectiveKernelThunk::IsSupported(\n-    const GpuCliqueKey& clique_key,\n-    const CollectiveCliques* collective_cliques) const {\n+    const GpuCliqueKey& clique_key, se::StreamExecutor& executor,\n+    const CollectiveParams& collective_params) const {\n   if (!collective_kernel_enabled_) {\n+    XLA_VLOG_DEVICE(3, executor.device_ordinal())\n+        << \"Collective kernel is not enabled.\";\n     return false;\n   }\n \n   // TODO(b/407736956): Support variadic all-reduce.\n   if (buffers_.size() != 1) {\n+    XLA_VLOG_DEVICE(3, executor.device_ordinal())\n+        << \"Variadic arguments are not implemented for collective kernels.\";\n     return false;\n   }\n-\n   const int64_t num_elements = buffers_[0].element_count;\n   const int64_t input_size_bytes = GetInputSizeBytes();\n   const AllReduceStrategy strategy =\n       GetAllReduceStrategy(input_size_bytes, is_multimem_enabled_);\n   // Custom all-reduce strategy is only supported for small inputs.\n   if (input_size_bytes > GetMaxSupportedAllReduceSizeBytes(strategy)) {\n+    XLA_VLOG_DEVICE(3, executor.device_ordinal())\n+        << \"Custom all-reduce strategy is only supported for small inputs.\";\n     return false;\n   }\n \n-  TF_ASSIGN_OR_RETURN(bool peer_access_enabled,\n-                      collective_cliques->peer_access_enabled(clique_key));\n-\n-  // Check that peer access is enabled.\n-  if (!peer_access_enabled) {\n+  // Only single-host collectives are supported for now.\n+  if (!clique_key.is_local()) {\n+    XLA_VLOG_DEVICE(3, executor.device_ordinal())\n+        << \"Cross-host symmetric memory collectives are not supported.\";\n     return false;\n   }\n \n+  for (const GlobalDeviceId& device : clique_key.devices()) {\n+    TF_ASSIGN_OR_RETURN(const int peer_device_id,\n+                        GetLocalDeviceId(device, collective_params));\n+    if (!executor.CanEnablePeerAccessTo(peer_device_id)) {\n+      XLA_VLOG_DEVICE(3, executor.device_ordinal())\n+          << \"Peer access is not supported with device \" << peer_device_id;\n+      return false;\n+    }\n+  }\n+\n   return IsAllReduceKernelSupported(\n       clique_key.num_local_participants(), num_elements,\n       collective_config_.operand_element_type[0], reduction_kind_, strategy);\n }\n \n absl::Status CollectiveKernelThunk::Prepare(const PrepareParams& params) {\n-  TF_RET_CHECK(params.collective_params != nullptr);\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n                                 /*include_participant_groups=*/false));\n-  return params.clique_requests->RequestClique(clique_key);\n+  TF_ASSIGN_OR_RETURN(\n+      bool use_collective_kernel,\n+      IsSupported(clique_key, *params.executor, *params.collective_params));\n+  if (!use_collective_kernel) {\n+    return absl::OkStatus();\n+  }\n+  TF_RETURN_IF_ERROR(params.clique_requests->RequestClique(clique_key));\n+\n+  absl::MutexLock lock(mutex_);\n+  if (!per_stream_memory_.contains(params.executor)) {\n+    // Allocate scratch buffers.\n+    const AllReduceStrategy strategy =\n+        GetAllReduceStrategy(GetInputSizeBytes(), is_multimem_enabled_);\n+    const LaunchDimensions launch_dimensions = AllReduceLaunchDimensions(\n+        buffers_[0].element_count, clique_key.num_local_participants(),\n+        strategy);\n+    const int64_t kNumSignalFlags =\n+        clique_key.num_local_participants() * launch_dimensions.num_blocks();\n+    const int64_t kSignalBufferSize = xla::RoundUpTo<uint64_t>(\n+        kNumSignalFlags * sizeof(int32_t), kXlaAllocatedBufferAlignBytes);\n+    const int64_t kLocalBufferSize = xla::RoundUpTo<uint64_t>(\n+        buffers_[0].source_buffer.size(), kXlaAllocatedBufferAlignBytes);\n+    TF_ASSIGN_OR_RETURN(\n+        se::DeviceAddressHandle local_buffers_handle,\n+        AllocateMemory(params.executor, kLocalBufferSize * kNumBuffers,\n+                       \"Local buffers\"));\n+\n+    TF_ASSIGN_OR_RETURN(\n+        se::DeviceAddressHandle signal_buffers_handle,\n+        AllocateMemory(params.executor, kSignalBufferSize * kNumBuffers,\n+                       \"Signal buffers\"));\n+\n+    se::DeviceAddressBase local_buffers_ptr = local_buffers_handle.address();\n+    per_stream_memory_.emplace(\n+        params.executor,\n+        std::make_unique<StreamMemory>(StreamMemory{\n+            std::move(local_buffers_handle), std::move(signal_buffers_handle),\n+            strategy, kLocalBufferSize, kSignalBufferSize}));\n+    if (is_multimem_enabled_ && strategy == AllReduceStrategy::kMultimem) {\n+      params.multimem_registry->Register(\n+          {clique_key, /*map_to=*/local_buffers_ptr});\n+    }\n+  }\n+\n+  return absl::OkStatus();\n }\n \n int64_t CollectiveKernelThunk::GetInputSizeBytes() const {\n@@ -137,30 +213,6 @@ int64_t CollectiveKernelThunk::GetInputSizeBytes() const {\n              collective_config_.operand_element_type[0]);\n }\n \n-absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n-    const GpuCliqueKey& clique_key, const InitializeParams& params,\n-    StreamState& state) {\n-  const std::optional<RankId> rank =\n-      clique_key.rank(params.collective_params->global_device_id);\n-  TF_RET_CHECK(rank.has_value())\n-      << \"Device \" << params.collective_params->global_device_id\n-      << \"is not in the clique.\";\n-\n-  std::vector<se::DeviceAddressBase> parameters{\n-      state.local_buffers_handle.memory(),\n-      state.signal_buffers_handle.memory()};\n-  TF_RET_CHECK(parameters.size() == kNumParameters);\n-\n-  const size_t param_to_peers_ptrs_size_bytes =\n-      parameters.size() * clique_key.num_devices() * sizeof(uint64_t);\n-  state.metadata = params.executor->Allocate(\n-      sizeof(CollectiveKernelMetadata) + param_to_peers_ptrs_size_bytes, 0);\n-\n-  return CollectiveMetadataThunk::ConstructCollectiveMetadata(\n-      clique_key, state.rank, params.stream, std::move(parameters),\n-      state.collective_multimem, state.metadata);\n-}\n-\n absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n   TF_ASSIGN_OR_RETURN(\n       const GpuCliqueKey clique_key,\n@@ -171,43 +223,21 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n   TF_RET_CHECK(rank.has_value())\n       << \"Device \" << params.collective_params->global_device_id\n       << \"is not in the clique.\";\n-  const AllReduceStrategy strategy =\n-      GetAllReduceStrategy(GetInputSizeBytes(), is_multimem_enabled_);\n-  const LaunchDimensions launch_dimensions = AllReduceLaunchDimensions(\n-      buffers_[0].element_count, clique_key.num_local_participants(), strategy);\n \n   StreamState* state = nullptr;\n   {\n     absl::MutexLock lock(mutex_);\n     if (!per_stream_state_.contains(params.executor)) {\n-      // Step1: Allocate signal and local buffers.\n-      const int64_t kNumSignalFlags =\n-          clique_key.num_local_participants() * launch_dimensions.num_blocks();\n-\n-      int64_t kSignalBufferSize = xla::RoundUpTo<uint64_t>(\n-          kNumSignalFlags * sizeof(int32_t), kXlaAllocatedBufferAlignBytes);\n-      const int64_t kLocalBufferSize = xla::RoundUpTo<uint64_t>(\n-          buffers_[0].source_buffer.size(), kXlaAllocatedBufferAlignBytes);\n-\n-      TF_ASSIGN_OR_RETURN(\n-          se::DeviceAddressHandle local_buffers_handle,\n-          AllocateMemory(params.executor, kLocalBufferSize * kNumBuffers,\n-                         \"Local buffers\"));\n-\n-      TF_ASSIGN_OR_RETURN(\n-          se::DeviceAddressHandle signal_buffers_handle,\n-          AllocateMemory(params.executor, kSignalBufferSize * kNumBuffers,\n-                         \"Signal buffers\"));\n-\n-      // Step2: We needs 1 atomic flag per block per device on each device.\n+      StreamMemory* memory_state = per_stream_memory_.at(params.executor).get();\n+      // Step1: We needs 1 atomic flag per block per device on each device.\n       // One-shot kernel expects that the signal flags buffer is zeroed out.\n       // Initial state of device memory is undefined, so we need to zero out\n       // the buffer. The kernel will take care of leaving the buffer in\n       // correct state after use, so we don't need to zero out after\n       // initialization.\n       TF_RETURN_IF_ERROR(params.executor->SynchronousMemZero(\n-          signal_buffers_handle.memory_ptr(),\n-          signal_buffers_handle.memory().size()));\n+          memory_state->signal_buffers_handle.memory_ptr(),\n+          memory_state->signal_buffers_handle.memory().size()));\n       // Create a kernel for execution.\n       std::unique_ptr<se::Kernel> kernel = nullptr;\n       if (!kernel_name_.empty()) {\n@@ -224,42 +254,59 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n                            params.executor, shmem_bytes_));\n         }\n       }\n-      // Step3: Emplace into the stream state.\n+      // Step2: Emplace into the stream state.\n       per_stream_state_.emplace(\n           params.executor,\n-          std::make_unique<StreamState>(\n-              params.executor->device_ordinal(), rank.value(),\n-              std::move(local_buffers_handle), std::move(signal_buffers_handle),\n-              std::move(kernel)));\n+          std::make_unique<StreamState>(params.executor->device_ordinal(),\n+                                        rank.value(), std::move(kernel)));\n \n       state = per_stream_state_.at(params.executor).get();\n \n       // NB: This is a double buffer allocation. So size of a single buffer is\n       // half of the total allocation.\n       for (int i = 0; i < kNumBuffers; ++i) {\n         state->remote_buffer_ptrs[i] =\n-            state->local_buffers_handle.memory_ptr()->GetByteSlice(\n-                /*offset_bytes=*/i * kLocalBufferSize,\n-                /*size_bytes=*/kLocalBufferSize);\n+            memory_state->local_buffers_handle.memory_ptr()->GetByteSlice(\n+                /*offset_bytes=*/i * memory_state->local_buffer_size_bytes,\n+                /*size_bytes=*/memory_state->local_buffer_size_bytes);\n \n         state->signal_buffer_ptrs[i] =\n-            state->signal_buffers_handle.memory_ptr()->GetByteSlice(\n-                /*offset_bytes=*/i * kSignalBufferSize,\n-                /*size_bytes=*/kSignalBufferSize);\n+            memory_state->signal_buffers_handle.memory_ptr()->GetByteSlice(\n+                /*offset_bytes=*/i * memory_state->signal_buffer_size_bytes,\n+                /*size_bytes=*/memory_state->signal_buffer_size_bytes);\n       }\n     }\n   }\n \n+  StreamMemory* memory_state = nullptr;\n+  {\n+    absl::MutexLock lock(mutex_);\n+    memory_state = per_stream_memory_.at(params.executor).get();\n+  }\n+\n   if (state != nullptr) {\n-    if (strategy == AllReduceStrategy::kMultimem) {\n+    if (memory_state->strategy == AllReduceStrategy::kMultimem) {\n       TF_ASSIGN_OR_RETURN(\n           state->collective_multimem,\n-          CollectiveMultimem::Allocate(params.executor, clique_key, *rank,\n-                                       state->local_buffers_handle.memory()));\n+          params.multicast_memory_registry->Get(\n+              {clique_key, memory_state->local_buffers_handle.memory()}));\n       state->multicast_device_ptr =\n           state->collective_multimem->mapped_ptr(*rank);\n     }\n-    TF_RETURN_IF_ERROR(ExchangeStateMetadata(clique_key, params, *state));\n+\n+    std::vector<se::DeviceAddressBase> parameters{\n+        memory_state->local_buffers_handle.memory(),\n+        memory_state->signal_buffers_handle.memory()};\n+    TF_RET_CHECK(parameters.size() == kNumParameters);\n+\n+    const size_t param_to_peers_ptrs_size_bytes =\n+        parameters.size() * clique_key.num_devices() * sizeof(uint64_t);\n+    state->metadata = params.executor->Allocate(\n+        sizeof(CollectiveKernelMetadata) + param_to_peers_ptrs_size_bytes, 0);\n+\n+    return CollectiveMetadataThunk::ConstructCollectiveMetadata(\n+        clique_key, state->rank, params.stream, std::move(parameters),\n+        state->collective_multimem, state->metadata);\n   }\n \n   return absl::OkStatus();"
        },
        {
            "sha": "8476cf19e90f64285c7281da252ca1fe7dbe0500",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -28,16 +28,14 @@ limitations under the License.*/\n #include \"absl/status/status.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/collective_cliques.h\"\n-#include \"xla/backends/gpu/runtime/collective_metadata_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_multimem.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/core/collectives/reduction_kind.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n@@ -82,8 +80,8 @@ class CollectiveKernelThunk : public Thunk {\n \n   // Returns true if the collective kernel is supported for the given clique.\n   absl::StatusOr<bool> IsSupported(\n-      const GpuCliqueKey& clique_key,\n-      const CollectiveCliques* collective_cliques) const;\n+      const GpuCliqueKey& clique_key, se::StreamExecutor& executor,\n+      const CollectiveParams& collective_params) const;\n \n   // The single host collective thunk actually requires a clique key.\n   absl::Status Prepare(const PrepareParams& params) final;\n@@ -100,10 +98,9 @@ class CollectiveKernelThunk : public Thunk {\n   // We use a double buffering strategy for the buffers.\n   // See docs on struct StreamState for more details.\n   static constexpr int64_t kNumBuffers = 2;\n-  // Per-executor state that needs to be synchronized for access.\n-  struct StreamState {\n-    int device_ordinal;\n-    RankId rank;\n+\n+  // Per-executor scratch memory.\n+  struct StreamMemory {\n     // Buffers allocated for the collective.\n     // Buffers are double buffered to allow for consecutive invocation\n     // of the kernel on different GPUs.\n@@ -118,6 +115,17 @@ class CollectiveKernelThunk : public Thunk {\n     // Also double buffered for the same reason as local buffers.\n     se::DeviceAddressHandle signal_buffers_handle;\n \n+    se::gpu::AllReduceStrategy strategy;\n+\n+    const int64_t local_buffer_size_bytes = 0;\n+    const int64_t signal_buffer_size_bytes = 0;\n+  };\n+\n+  // Per-executor state that needs to be synchronized for access.\n+  struct StreamState {\n+    int device_ordinal = 0;\n+    RankId rank = RankId(0);\n+\n     // Pointer to the collective kernel metadata on device.\n     se::DeviceAddressBase metadata;\n \n@@ -136,25 +144,15 @@ class CollectiveKernelThunk : public Thunk {\n     // Constructor to make OSS builds happy.\n     StreamState() = default;\n     StreamState(int device_ordinal_arg, RankId rank_arg,\n-                se::DeviceAddressHandle local_buffers_handle_arg,\n-                se::DeviceAddressHandle signal_buffers_handle_arg,\n                 std::unique_ptr<se::Kernel> kernel_arg)\n         : device_ordinal(device_ordinal_arg),\n           rank(rank_arg),\n-          local_buffers_handle(std::move(local_buffers_handle_arg)),\n-          signal_buffers_handle(std::move(signal_buffers_handle_arg)),\n           kernel(std::move(kernel_arg)) {}\n   };\n \n   // Returns the input size in bytes for the collective.\n   int64_t GetInputSizeBytes() const;\n \n-  // Internal method to sync thread after Initialize.\n-  // Returns the collective kernel metadata for the given clique key.\n-  absl::Status ExchangeStateMetadata(const GpuCliqueKey& clique_key,\n-                                     const InitializeParams& params,\n-                                     StreamState& state);\n-\n   // Whether the one-shot kernel is enabled.\n   const bool collective_kernel_enabled_;\n   // Whether the collective is run on an async stream.\n@@ -177,6 +175,8 @@ class CollectiveKernelThunk : public Thunk {\n   absl::Mutex mutex_;\n   absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<StreamState>>\n       per_stream_state_ ABSL_GUARDED_BY(mutex_);\n+  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<StreamMemory>>\n+      per_stream_memory_ ABSL_GUARDED_BY(mutex_);\n   const bool is_multimem_enabled_;\n };\n }  // namespace xla::gpu"
        },
        {
            "sha": "43631845f37ad3ca9d3dbca3a6d3f6b132f6e19b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk_test.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -26,12 +26,14 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem_registry.h\"\n #include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n@@ -274,12 +276,26 @@ absl::StatusOr<se::DeviceAddressBase> RunCollectiveKernelThunk(\n     TF_RETURN_IF_ERROR(stream->BlockHostUntilDone());\n   }\n \n+  Thunk::PrepareParams prepare_params;\n+  CollectiveMultimemRegistry multimem_registry(\n+      executor, collective_params.global_device_id);\n+  CollectiveCliqueRequests clique_requests;\n+  prepare_params.executor = executor;\n+  prepare_params.buffer_allocations = &buffer_allocations;\n+  prepare_params.collective_params = &collective_params;\n+  prepare_params.clique_requests = &clique_requests;\n+  prepare_params.multimem_registry = &multimem_registry;\n+  TF_RETURN_IF_ERROR(metadata.thunk->Prepare(prepare_params));\n+\n+  TF_RETURN_IF_ERROR(multimem_registry.Build());\n+\n   Thunk::InitializeParams initialize_params;\n   initialize_params.executor = executor;\n   initialize_params.stream = stream.get();\n   initialize_params.buffer_allocations = &buffer_allocations;\n   initialize_params.collective_params = &collective_params;\n   initialize_params.src = {kKernelSource};\n+  initialize_params.multicast_memory_registry = &multimem_registry;\n \n   GpuExecutableRunOptions::DeviceIdMap global_device_id_map = {\n       {LocalDeviceId(0), GlobalDeviceId(0)}};"
        },
        {
            "sha": "d6fb24c4b595817b4a13ab1075c550ff58e166e2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 9,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -23,13 +23,15 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"google/protobuf/repeated_ptr_field.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_rendezvous.h\"\n #include \"xla/backends/gpu/runtime/collective_multimem.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem_registry.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -143,6 +145,36 @@ CollectiveMetadataThunk::GetParameterDeviceMemoryBase(\n       /*size_bytes=*/num_devices * sizeof(void*));\n }\n \n+absl::Status CollectiveMetadataThunk::Prepare(const PrepareParams& params) {\n+  // We currently support only a single memory space for multimem parameters.\n+  // So we just pick the first one here.\n+  auto fast_memory_parameter =\n+      absl::c_find_if(parameters_, [](const Buffer& parameter) {\n+        return parameter.memory_space == xla::Layout::kGenericFastMemorySpace;\n+      });\n+  if (fast_memory_parameter == parameters_.end()) {\n+    return absl::OkStatus();\n+  }\n+\n+  se::DeviceAddressBase memory_range;\n+  TF_ASSIGN_OR_RETURN(memory_range,\n+                      params.executor->GetMemoryRange(\n+                          params.buffer_allocations->GetDeviceAddress(\n+                              fast_memory_parameter->slice)));\n+\n+  // Since there is no parameter in the collective memory space, we don't need\n+  // to set up the collective multimem.\n+  if (memory_range.is_null()) {\n+    return absl::OkStatus();\n+  }\n+  TF_ASSIGN_OR_RETURN(\n+      const GpuCliqueKey clique_key,\n+      GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n+                                /*include_participant_groups=*/false));\n+  params.multimem_registry->Register({clique_key, /*map_to=*/memory_range});\n+  return absl::OkStatus();\n+}\n+\n absl::Status CollectiveMetadataThunk::Initialize(\n     const InitializeParams& params) {\n   TF_ASSIGN_OR_RETURN(\n@@ -164,11 +196,11 @@ absl::Status CollectiveMetadataThunk::Initialize(\n       params.buffer_allocations->GetDeviceAddress(result_);\n \n   GlobalDeviceId global_device_id = params.collective_params->global_device_id;\n-  std::optional<RankId> rank = clique_key.rank(global_device_id);\n \n-  TF_ASSIGN_OR_RETURN(auto multimem,\n-                      AllocateMultimem(clique_key, *rank, params));\n+  TF_ASSIGN_OR_RETURN(auto multimem, GetCollectiveMultimem(clique_key, params));\n \n+  std::optional<RankId> rank = clique_key.rank(global_device_id);\n+  TF_RET_CHECK(rank.has_value());\n   return ConstructCollectiveMetadata(clique_key, *rank, params.stream,\n                                      std::move(parameters), std::move(multimem),\n                                      result_ptr);\n@@ -180,9 +212,8 @@ absl::Status CollectiveMetadataThunk::ExecuteOnStream(\n }\n \n absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n-CollectiveMetadataThunk::AllocateMultimem(const GpuCliqueKey& clique_key,\n-                                          RankId rank,\n-                                          const InitializeParams& params) {\n+CollectiveMetadataThunk::GetCollectiveMultimem(const GpuCliqueKey& clique_key,\n+                                               const InitializeParams& params) {\n   se::DeviceAddressBase memory_range;\n   for (const Buffer& parameter : parameters_) {\n     if (parameter.memory_space == xla::Layout::kGenericFastMemorySpace) {\n@@ -200,10 +231,9 @@ CollectiveMetadataThunk::AllocateMultimem(const GpuCliqueKey& clique_key,\n     return nullptr;\n   }\n \n+  const MultimemRequest request{clique_key, memory_range};\n   TF_ASSIGN_OR_RETURN(std::shared_ptr<CollectiveMultimem> collective_multimem,\n-                      CollectiveMultimem::Allocate(params.executor, clique_key,\n-                                                   rank, memory_range));\n-\n+                      params.multicast_memory_registry->Get(request));\n   absl::MutexLock lock(mutex_);\n   return (collective_multimem_[params.executor] =\n               std::move(collective_multimem));"
        },
        {
            "sha": "01677b001866c74d8374415a748100d22097c5bc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -45,15 +45,16 @@ class CollectiveMetadataThunk : public Thunk {\n     int64_t memory_space;\n   };\n \n-  explicit CollectiveMetadataThunk(ThunkInfo thunk_info,\n-                                   CollectiveConfig collective_config,\n-                                   std::vector<Buffer> parameters,\n-                                   BufferAllocation::Slice result)\n+  CollectiveMetadataThunk(ThunkInfo thunk_info,\n+                          CollectiveConfig collective_config,\n+                          std::vector<Buffer> parameters,\n+                          BufferAllocation::Slice result)\n       : Thunk(Thunk::Kind::kCollectiveMetadata, thunk_info),\n         collective_config_(std::move(collective_config)),\n         parameters_(std::move(parameters)),\n         result_(result) {}\n \n+  absl::Status Prepare(const PrepareParams& params) override;\n   absl::Status Initialize(const InitializeParams& params) override;\n   absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n \n@@ -75,9 +76,8 @@ class CollectiveMetadataThunk : public Thunk {\n       int64_t num_devices, int64_t parameter_index);\n \n  private:\n-  absl::StatusOr<std::shared_ptr<CollectiveMultimem>> AllocateMultimem(\n-      const GpuCliqueKey& clique_key, RankId rank,\n-      const InitializeParams& params);\n+  absl::StatusOr<std::shared_ptr<CollectiveMultimem>> GetCollectiveMultimem(\n+      const GpuCliqueKey& clique_key, const InitializeParams& params);\n \n   const CollectiveConfig collective_config_;\n   std::vector<Buffer> parameters_;"
        },
        {
            "sha": "ad17e9b3b8a45b1cd34b923672d1e507a4214a31",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 16,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/collective_multimem.h\"\n \n-#include <any>\n #include <cstdint>\n #include <memory>\n #include <optional>\n@@ -74,7 +73,6 @@ struct AllocateParams {\n   se::StreamExecutor* executor;\n   RankId rank;\n   se::DeviceAddressBase map_to;\n-  std::any payload;\n };\n \n struct RankCmp {\n@@ -100,9 +98,9 @@ struct MappedPtrFormatter {\n }  // namespace\n \n absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n-CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n+CollectiveMultimem::Allocate(se::StreamExecutor& executor,\n                              const GpuCliqueKey& clique_key, RankId rank,\n-                             se::DeviceAddressBase map_to, std::any payload) {\n+                             se::DeviceAddressBase map_to) {\n   VLOG(3) << absl::StrFormat(\n       \"rank=[%d] Allocate collective multimem for clique: %s\", rank.value(),\n       clique_key.ToString());\n@@ -112,13 +110,13 @@ CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n   if (!clique_key.is_local()) {\n     return Unimplemented(\n         \"%sMultimem is not supported in multi-process mode in clique %s\",\n-        XlaFormatDevice(executor->device_ordinal()), clique_key.ToString());\n+        XlaFormatDevice(executor.device_ordinal()), clique_key.ToString());\n   }\n \n   std::string rendezvous_name = absl::StrFormat(\n       \"CollectiveMultimem::Allocate for clique %s\", clique_key.ToString());\n   AllocateRendezvousKey rendezvous_key = {clique_key};\n-  AllocateParams params = {executor, rank, map_to, std::move(payload)};\n+  AllocateParams params = {&executor, rank, map_to};\n \n   // A callback for rendezvous to allocate and map the multicast memory.\n   auto allocate = [&](absl::Span<const AllocateParams*> params)\n@@ -159,12 +157,6 @@ CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n               dynamic_cast<se::gpu::GpuExecutor*>(param->executor)));\n     }\n \n-    // For all participating devices move payloads to the collective multimem.\n-    absl::btree_map<RankId, std::any> payloads;\n-    for (const auto* param : params) {\n-      payloads[param->rank] = std::move(param->payload);\n-    }\n-\n     VLOG(3) << absl::StrFormat(\n         \"Allocated collective multimem for clique: %s; mapped_ptrs: [%s]\",\n         clique_key.ToString(),\n@@ -182,14 +174,13 @@ CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n }\n \n absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n-CollectiveMultimem::Allocate(se::StreamExecutor* executor,\n+CollectiveMultimem::Allocate(se::StreamExecutor& executor,\n                              const GpuCliqueKey& clique_key,\n                              GlobalDeviceId global_device_id,\n-                             se::DeviceAddressBase map_to, std::any payload) {\n+                             se::DeviceAddressBase map_to) {\n   if (std::optional<RankId> rank = clique_key.rank(global_device_id)) {\n-    return Allocate(executor, clique_key, *rank, map_to, std::move(payload));\n+    return Allocate(executor, clique_key, *rank, map_to);\n   }\n   return InvalidArgument(\"Rank not found for device %v\", global_device_id);\n }\n-\n }  // namespace xla::gpu"
        },
        {
            "sha": "a2ee82ea77e1021ac7c84355ab15e0d0d8dcdce1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem.h",
            "status": "modified",
            "additions": 4,
            "deletions": 26,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem.h?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_\n \n #include <any>\n-#include <functional>\n #include <memory>\n \n #include \"absl/container/btree_map.h\"\n@@ -52,25 +51,19 @@ class CollectiveMultimem {\n   // The optional `payload` argument is captured by the returned shared pointer\n   // to allow callers to associate arbitrary data with the collective multimem.\n   static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n-      se::StreamExecutor* executor, const GpuCliqueKey& clique_key, RankId rank,\n-      se::DeviceAddressBase map_to, std::any payload = {});\n+      se::StreamExecutor& executor, const GpuCliqueKey& clique_key, RankId rank,\n+      se::DeviceAddressBase map_to);\n \n   // Allocates a CollectiveMultimem for the given global device id.\n   static absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Allocate(\n-      se::StreamExecutor* executor, const GpuCliqueKey& clique_key,\n-      GlobalDeviceId global_device_id, se::DeviceAddressBase map_to,\n-      std::any payload = {});\n+      se::StreamExecutor& executor, const GpuCliqueKey& clique_key,\n+      GlobalDeviceId global_device_id, se::DeviceAddressBase map_to);\n \n   const GpuCliqueKey& clique_key() const { return clique_key_; }\n \n   // Returns the device pointer to the multicast memory for the given rank.\n   void* mapped_ptr(RankId rank) const { return mapped_ptrs_.at(rank); }\n \n-  // Returns the payload associated with the given rank. If payload type is not\n-  // the same as `T`, returns an error.\n-  template <typename T>\n-  absl::StatusOr<std::reference_wrapper<T>> payload(RankId rank) const;\n-\n  private:\n   CollectiveMultimem(\n       GpuCliqueKey clique_key, absl::btree_map<RankId, void*> mapped_ptrs,\n@@ -89,21 +82,6 @@ class CollectiveMultimem {\n   std::unique_ptr<se::gpu::MulticastMemory> multicast_memory_;\n };\n \n-template <typename T>\n-absl::StatusOr<std::reference_wrapper<T>> CollectiveMultimem::payload(\n-    RankId rank) const {\n-  auto it = payload_.find(rank);\n-  if (it == payload_.end()) {\n-    return NotFound(\"Payload not found for rank %d\", rank.value());\n-  }\n-\n-  if (std::any_cast<T>(&it->second) == nullptr) {\n-    return InvalidArgument(\"Payload type mismatch for rank %d\", rank.value());\n-  }\n-\n-  return std::ref(std::any_cast<T&>(&it->second));\n-}\n-\n }  // namespace xla::gpu\n \n #endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_H_"
        },
        {
            "sha": "dee42a121083750fb69a089899afd79981ab2417",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem_registry.cc",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem_registry.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem_registry.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem_registry.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -0,0 +1,55 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_multimem_registry.h\"\n+\n+#include <memory>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+\n+void CollectiveMultimemRegistry::Register(const MultimemRequest& request) {\n+  requests_.push_back(request);\n+}\n+\n+absl::Status CollectiveMultimemRegistry::Build() {\n+  for (const MultimemRequest& request : requests_) {\n+    TF_ASSIGN_OR_RETURN(\n+        std::shared_ptr<CollectiveMultimem> multimem,\n+        CollectiveMultimem::Allocate(executor_, request.key, global_device_id_,\n+                                     request.map_to));\n+    multimems_[request] = multimem;\n+  }\n+\n+  requests_.clear();\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<std::shared_ptr<CollectiveMultimem>>\n+CollectiveMultimemRegistry::Get(const MultimemRequest& request) const {\n+  auto it = multimems_.find(request);\n+  if (it == multimems_.end()) {\n+    return absl::NotFoundError(absl::StrFormat(\n+        \"Multimem not found for request: %s\", request.key.ToString()));\n+  }\n+  return it->second;\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "46e7069cf18e44114e401658ca6bef740d479be1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_multimem_registry.h",
            "status": "added",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem_registry.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem_registry.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_multimem_registry.h?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -0,0 +1,83 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_REGISTRY_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_REGISTRY_H_\n+\n+#include <cstdint>\n+#include <memory>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/base/nullability.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/stream_executor/device_address.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla::gpu {\n+\n+// A request for a multimem for a given clique on a given address space.\n+struct MultimemRequest {\n+  static std::tuple<GpuCliqueKey, void*, uint64_t> CmpKey(\n+      const MultimemRequest& key) {\n+    return {key.key, key.map_to.opaque(), key.map_to.size()};\n+  }\n+\n+  friend bool operator==(const MultimemRequest& a, const MultimemRequest& b) {\n+    return a.key == b.key && a.map_to == b.map_to;\n+  }\n+\n+  template <typename H>\n+  friend H AbslHashValue(H h, const MultimemRequest& key) {\n+    return H::combine(std::move(h), key.key, key.map_to.opaque(),\n+                      key.map_to.size());\n+  }\n+\n+  GpuCliqueKey key;\n+  se::DeviceAddressBase map_to;\n+};\n+\n+// Allocates and provides thunks requested multimem objects.\n+class CollectiveMultimemRegistry {\n+ public:\n+  // Does not take ownership of `executor`, which must outlive this object.\n+  CollectiveMultimemRegistry(se::StreamExecutor* absl_nonnull executor,\n+                             GlobalDeviceId global_device_id)\n+      : executor_(*executor), global_device_id_(global_device_id) {}\n+\n+  void Register(const MultimemRequest& request);\n+\n+  absl::Status Build();\n+\n+  absl::StatusOr<std::shared_ptr<CollectiveMultimem>> Get(\n+      const MultimemRequest& request) const;\n+\n+ private:\n+  std::vector<MultimemRequest> requests_;\n+  absl::flat_hash_map<MultimemRequest, std::shared_ptr<CollectiveMultimem>>\n+      multimems_;\n+  se::StreamExecutor& executor_;\n+  GlobalDeviceId global_device_id_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_MULTIMEM_REGISTRY_H_"
        },
        {
            "sha": "e8b8471e7403e913158ff9ab1d4859089136482e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n #include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem_registry.h\"\n #include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n@@ -250,6 +251,12 @@ class Thunk {\n     const CollectiveParams* collective_params = nullptr;\n     // Clique requests for preparing collective communicators.\n     CollectiveCliqueRequests* clique_requests = nullptr;\n+    // Multimem registry for preparing multimem objects.\n+    CollectiveMultimemRegistry* absl_nonnull multimem_registry = nullptr;\n+    // Stream executor for the thunk.\n+    se::StreamExecutor* absl_nonnull executor = nullptr;\n+    // Buffer allocations for the thunk.\n+    const BufferAllocations* absl_nonnull buffer_allocations = nullptr;\n   };\n \n   //===--------------------------------------------------------------------===//\n@@ -282,6 +289,9 @@ class Thunk {\n     // Collective cliques acquired based on resource requests.\n     CollectiveCliques* collective_cliques = nullptr;\n \n+    // Multimem registry for preparing collective communicators.\n+    CollectiveMultimemRegistry* multicast_memory_registry = nullptr;\n+\n     // XLA FFI execution context.\n     const ffi::ExecutionContext* ffi_execution_context = nullptr;\n "
        },
        {
            "sha": "e20aee3d7818af26e89f00b42e2a399228ee5091",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -483,6 +483,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:collective_group_thunk\",\n         \"//xla/backends/gpu/runtime:collective_kernel_thunk\",\n         \"//xla/backends/gpu/runtime:collective_metadata_thunk\",\n+        \"//xla/backends/gpu/runtime:collective_multimem\",\n         \"//xla/backends/gpu/runtime:collective_permute_thunk\",\n         \"//xla/backends/gpu/runtime:collective_thunk\",\n         \"//xla/backends/gpu/runtime:command_buffer_cmd\",\n@@ -691,6 +692,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:annotation\",\n         \"//xla/backends/gpu/runtime:collective_clique_requests\",\n         \"//xla/backends/gpu/runtime:collective_cliques\",\n+        \"//xla/backends/gpu/runtime:collective_multimem_registry\",\n         \"//xla/backends/gpu/runtime:collective_params\",\n         \"//xla/backends/gpu/runtime:command_buffer_conversion_pass\",\n         \"//xla/backends/gpu/runtime:nvshmem_collective_thunk\","
        },
        {
            "sha": "1c73b3a5a8002e4efa99c867a4aa16d2bc62aadd",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/annotation.h\"\n #include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n #include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_multimem_registry.h\"\n #include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_conversion_pass.h\"\n #include \"xla/backends/gpu/runtime/nvshmem_collective_thunk.h\"\n@@ -460,9 +461,13 @@ absl::Status ExecuteThunksImpl(\n           collective_max_nchannels, p2p_max_nchannels));\n \n   CollectiveCliqueRequests clique_requests;\n+  CollectiveMultimemRegistry multimem_registry(\n+      executor, collective_params.global_device_id);\n \n   {  // Prepare thunks for execution and collect requested GPU cliques.\n-    Thunk::PrepareParams prepare_params{&collective_params, &clique_requests};\n+    Thunk::PrepareParams prepare_params{&collective_params, &clique_requests,\n+                                        &multimem_registry, executor,\n+                                        &buffer_allocations};\n \n     tsl::profiler::TraceMe trace_prepare(\"Thunks::Prepare\");\n     TF_RETURN_IF_ERROR(thunk_sequence.Prepare(prepare_params));\n@@ -488,6 +493,8 @@ absl::Status ExecuteThunksImpl(\n                 : false));\n   }\n \n+  TF_RETURN_IF_ERROR(multimem_registry.Build());\n+\n   {  // Initialize thunks using prepared resources before execution.\n     Thunk::InitializeParams initialize_params{\n         executor,\n@@ -497,6 +504,7 @@ absl::Status ExecuteThunksImpl(\n         command_buffer_trace_stream,\n         &collective_params,\n         &collective_cliques,\n+        &multimem_registry,\n         run_options->run_options().ffi_execution_context(),\n         run_options->local_device_count()};\n "
        },
        {
            "sha": "1dbcf2181562837d12087f6dcaa93f2f76428764",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -412,6 +412,7 @@ bool CanEnablePeerAccess(CUdevice from, CUdevice to) {\n     LOG(ERROR) << \"failed to detect peer access capability: \" << status;\n     return false;\n   }\n+\n   return can_access_peer;\n }\n \n@@ -949,7 +950,7 @@ absl::StatusOr<void*> CudaExecutor::VmmAllocateMemory(uint64_t bytes) {\n   int device_count = 0;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cudaGetDeviceCount(&device_count)));\n   for (int peer = 0; peer < device_count; peer++) {\n-    if (peer == device_ordinal() || CanEnablePeerAccess(peer, device_)) {\n+    if (peer == device_ordinal() || CanEnablePeerAccessTo(peer)) {\n       CUmemAccessDesc accessDesc = GetVmmAccessDescriptor(peer);\n       TF_RETURN_IF_ERROR(\n           cuda::ToStatus(cuMemSetAccess(ptr, padded_size, &accessDesc, 1)));\n@@ -1602,11 +1603,29 @@ fft::FftSupport* CudaExecutor::AsFft() {\n   return fft_.get();\n }\n \n+// TODO(468297175): Precalculate peer access in stream executor constructor.\n bool CudaExecutor::CanEnablePeerAccessTo(StreamExecutor* other) {\n   CudaExecutor* cuda_other = static_cast<CudaExecutor*>(other);\n   return CanEnablePeerAccess(cuda_context_, cuda_other->cuda_context_);\n }\n \n+bool CudaExecutor::CanEnablePeerAccessTo(int other_device_ordinal) {\n+  if (other_device_ordinal == device_ordinal()) {\n+    // Self-access is always allowed.\n+    return true;\n+  }\n+\n+  auto it = peer_access_cache_.find(other_device_ordinal);\n+  if (it != peer_access_cache_.end()) {\n+    return it->second;\n+  }\n+\n+  const bool result =\n+      CanEnablePeerAccess(device_ordinal(), other_device_ordinal);\n+  peer_access_cache_[other_device_ordinal] = result;\n+  return result;\n+}\n+\n absl::Status CudaExecutor::EnablePeerAccessTo(StreamExecutor* other) {\n   CudaExecutor* cuda_other = static_cast<CudaExecutor*>(other);\n   return EnablePeerAccess(cuda_context_, cuda_other->cuda_context_);"
        },
        {
            "sha": "b115fc95d07e137a22e6a06cfeea249bad1e8e96",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -90,6 +90,7 @@ class CudaExecutor : public GpuExecutor {\n   void DeallocateStream(Stream* stream) override;\n   absl::Status EnablePeerAccessTo(StreamExecutor* other) override;\n   bool CanEnablePeerAccessTo(StreamExecutor* other) override;\n+  bool CanEnablePeerAccessTo(int other_device_ordinal) override;\n   bool DeviceMemoryUsage(int64_t* free_out, int64_t* total_out) const override;\n   absl::StatusOr<std::unique_ptr<Kernel>> LoadKernel(\n       const KernelLoaderSpec& spec) override;\n@@ -318,6 +319,7 @@ class CudaExecutor : public GpuExecutor {\n   int stream_priority_lowest_ = 0;\n   int stream_priority_highest_ = 0;\n   bool stream_priority_query_ok_ = false;\n+  absl::flat_hash_map<int, bool> peer_access_cache_;\n };\n \n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "c3e8bd145fc7fed27247a745c43f0a32c2efa92e",
            "filename": "third_party/xla/xla/stream_executor/stream_executor.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -235,6 +235,9 @@ class StreamExecutor {\n   // StreamExecutor to memory allocated by another.\n   virtual bool CanEnablePeerAccessTo(StreamExecutor* other) = 0;\n \n+  // Same as above, but takes the device ordinal of the other device.\n+  virtual bool CanEnablePeerAccessTo(int other_device_ordinal) { return false; }\n+\n   // Returns the underlying device memory usage information, if it is available.\n   // If it is not available (false is returned), free/total may not be\n   // initialized."
        },
        {
            "sha": "b5655496b18eb80c2f3a83f0f558e3697650c788",
            "filename": "third_party/xla/xla/tests/collective_metadata_test.cc",
            "status": "modified",
            "additions": 73,
            "deletions": 12,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_metadata_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_metadata_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_metadata_test.cc?ref=b8da0f5494deac8f9cec3966b62f6b5a03cd0d0b",
            "patch": "@@ -84,25 +84,86 @@ TEST_F(CollectiveMetadataTest, ConstructCollectiveMetadata) {\n   ASSERT_EQ(first_result_data.size(), kNumElements);\n   ASSERT_EQ(second_result_data.size(), kNumElements);\n \n-  // Check the rank in the first position.\n-  EXPECT_EQ(first_result_data[0], 0);\n-  EXPECT_EQ(second_result_data[0], 1);\n+  EXPECT_EQ(first_result_data[0], 0) << \"First result rank is not 0.\";\n+  EXPECT_EQ(second_result_data[0], 1) << \"Second result rank is not 1.\";\n \n-  // Check pointer to peers in the second position.\n-  EXPECT_NE(first_result_data[1], 0);\n-  EXPECT_NE(second_result_data[1], 0);\n+  EXPECT_NE(first_result_data[1], 0)\n+      << \"First result pointer to peers is NULL.\";\n+  EXPECT_NE(second_result_data[1], 0)\n+      << \"Second result pointer to peers is NULL.\";\n \n-  // Check pointer to multimem metadata in the third position.\n-  EXPECT_NE(first_result_data[2], 0);\n-  EXPECT_NE(second_result_data[2], 0);\n+  EXPECT_NE(first_result_data[2], 0)\n+      << \"First result pointer to multimem metadata is not set.\";\n+  EXPECT_NE(second_result_data[2], 0)\n+      << \"Second result pointer to multimem metadata is not set.\";\n \n-  // Check param_to_peers structure.\n   for (int i = 3; i < kNumElements; ++i) {\n-    EXPECT_NE(first_result_data[i], 0);\n-    EXPECT_EQ(second_result_data[i], first_result_data[i]);\n+    EXPECT_NE(first_result_data[i], 0)\n+        << \"First result param_to_peers is NULL.\";\n+    EXPECT_EQ(second_result_data[i], first_result_data[i])\n+        << \"Param_to_peers mismatch at index \" << i\n+        << \" in the first result: \" << first_result_data[i]\n+        << \" and in the second result: \" << second_result_data[i];\n   }\n }\n \n+TEST_F(CollectiveMetadataTest, BuildMultimemOnlyOncePerModuleExecution) {\n+  const absl::string_view kModuleStr = R\"(\n+  HloModule test, replica_count=2\n+\n+  ENTRY test_computation {\n+    param_0 = f32[1] parameter(0)\n+    copy_1 = f32[1]{0:S(1)} copy(param_0)\n+\n+    first_result_tuple = (f32[1]{0:S(1)}, u64[5]) custom-call(copy_1), custom_call_target=\"CollectiveMetadata\", output_to_operand_aliasing={{0}: (0, {})}\n+    first_result = u64[5] get-tuple-element(first_result_tuple), index=1\n+    second_result_tuple = (f32[1]{0:S(1)}, u64[5]) custom-call(copy_1), custom_call_target=\"CollectiveMetadata\", output_to_operand_aliasing={{0}: (0, {})}\n+    second_result = u64[5] get-tuple-element(second_result_tuple), index=1\n+    ROOT result_tuple = (u64[5], u64[5]) tuple(first_result, second_result)\n+  })\";\n+\n+  constexpr int kNumReplicas = 2;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n+\n+  Literal input_0 = LiteralUtil::CreateR1<float>({1.0f});\n+  Literal input_1 = LiteralUtil::CreateR1<float>({1.0f});\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ExecutionResult execution_result,\n+      ExecuteReplicated(std::move(module),\n+                        /*arguments=*/std::vector<Literal*>{&input_0, &input_1},\n+                        /*run_hlo_passes=*/false));\n+\n+  std::vector<Literal>& literals = execution_result.results;\n+  ASSERT_EQ(literals.size(), kNumReplicas);\n+\n+  std::vector<Literal> first_result = literals[0].DecomposeTuple();\n+  std::vector<Literal> second_result = literals[1].DecomposeTuple();\n+\n+  absl::Span<const uint64_t> first_device_first_result =\n+      first_result[0].data<uint64_t>();\n+  absl::Span<const uint64_t> first_device_second_result =\n+      first_result[1].data<uint64_t>();\n+  absl::Span<const uint64_t> second_device_first_result =\n+      second_result[0].data<uint64_t>();\n+  absl::Span<const uint64_t> second_device_second_result =\n+      second_result[1].data<uint64_t>();\n+  constexpr int kNumElements = 5;\n+  ASSERT_EQ(first_device_first_result.size(), kNumElements);\n+  ASSERT_EQ(first_device_second_result.size(), kNumElements);\n+  ASSERT_EQ(second_device_first_result.size(), kNumElements);\n+  ASSERT_EQ(second_device_second_result.size(), kNumElements);\n+\n+  EXPECT_EQ(first_device_first_result[2], first_device_second_result[2])\n+      << \"Multimem metadata should be the same for both results.\";\n+  EXPECT_EQ(second_device_first_result[2], second_device_second_result[2])\n+      << \"Multimem metadata should be the same for both results.\";\n+}\n+\n TEST_F(CollectiveMetadataTest, ConstructCollectiveMetadataWithReplicaGroup) {\n   const absl::string_view kModuleStr = R\"(\n   HloModule test, replica_count=4"
        }
    ],
    "stats": {
        "total": 689,
        "additions": 513,
        "deletions": 176
    }
}