{
    "author": "chsigg",
    "message": "Remove temporary Triton patches which have since been upstreamed.\n\nPiperOrigin-RevId: 799923988",
    "sha": "1d7d604de0575d314fd6080d21199e1c146d41bb",
    "files": [
        {
            "sha": "6ce6392617228c7e9f46ebd95e315fb13970b209",
            "filename": "third_party/xla/third_party/triton/temporary/add_set_insertion_point.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch?ref=944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675",
            "patch": "@@ -1,29 +0,0 @@\n-// Remove once it is upstreamed. Tracking bug: b/440003867\n---- a/lib/Dialect/TritonNvidiaGPU/Transforms/InterleaveTMem.cpp\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/lib/Dialect/TritonNvidiaGPU/Transforms/InterleaveTMem.cpp\t2025-08-19 22:42:39.000000000 -0700\n-@@ -63,6 +63,7 @@\n- std::pair<Value, AccessRange>\n- findBufferAccessMemdescSubview(Operation *subview) {\n-   OpBuilder builder(subview->getContext());\n-+  builder.setInsertionPoint(subview);\n-   Location loc = subview->getLoc();\n-   TypedValue<ttg::MemDescType> src;\n-   SmallVector<int64_t> shape;\n-\n---- a/test/TritonNvidiaGPU/interleave_tmem.mlir\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/test/TritonNvidiaGPU/interleave_tmem.mlir\t2025-08-19 22:57:53.000000000 -0700\n-@@ -124,12 +124,12 @@\n-   %subview0 = ttg.memdesc_index %alloc0, %c0 : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   // CHECK: [[ALLOC1:%.+]] = ttng.tmem_alloc\n-   %alloc1 = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n--  // CHECK-NEXT: [[SUBVIEW1:%.+]] = ttg.memdesc_index [[ALLOC1]]\n-+  // CHECK: [[SUBVIEW1:%.+]] = ttg.memdesc_index [[ALLOC1]]\n-   %subview1 = ttg.memdesc_index %alloc1, %c0 : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   // CHECK-NEXT: tmem_store %arg0, [[SUBVIEW1]]\n-   ttng.tmem_store %arg0, %subview1, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   // CHECK-NEXT: [[ALLOC0:%.+]] = ttng.tmem_alloc\n--  // CHECK-NEXT: [[SUBVIEW0:%.+]] = ttg.memdesc_index [[ALLOC0]]\n-+  // CHECK: [[SUBVIEW0:%.+]] = ttg.memdesc_index [[ALLOC0]]\n-   // CHECK-NEXT: tmem_store %arg0, [[SUBVIEW0]]\n-   ttng.tmem_store %arg0, %subview0, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   tt.return"
        },
        {
            "sha": "3c00850fcae0e07f75abf69315bcffed05506b62",
            "filename": "third_party/xla/third_party/triton/temporary/ws_fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_fix.patch?ref=944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675",
            "patch": "@@ -1,22 +0,0 @@\n-Upstreamed in https://github.com/triton-lang/triton/pull/7796\n-\n-diff --git a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp\n---- a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp\n-+++ b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp\n-@@ -13,6 +13,7 @@\n- #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n- #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n- #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n-+#include \"llvm/ADT/STLExtras.h\"\n- \n- namespace tt = mlir::triton;\n- namespace ttg = mlir::triton::gpu;\n-@@ -266,7 +267,7 @@ void lowerTokenOperations(Operation *par\n-     if (auto tokenOp = dyn_cast<ttnvws::CreateTokenOp>(op)) {\n-       // Check to see if it is used by warpSpec. If yes, eraseOperand and\n-       // eraseArgument.\n--      for (OpOperand &use : tokenOp->getUses()) {\n-+      for (OpOperand &use : llvm::make_early_inc_range(tokenOp->getUses())) {\n-         Operation *user = use.getOwner();\n-         if (auto wsOp = dyn_cast<ttg::WarpSpecializeOp>(user)) {\n-           unsigned opndNum = use.getOperandNumber();"
        },
        {
            "sha": "7fb3c6eb8daed564e8860ed522df6820d3e9b8cd",
            "filename": "third_party/xla/third_party/triton/temporary/ws_ub_fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 59,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_ub_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_ub_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_ub_fix.patch?ref=944e62dea9b9a5f6a02e0e8b33ddaa44f24cb675",
            "patch": "@@ -1,59 +0,0 @@\n-Being upstreamed in https://github.com/triton-lang/triton/pull/7828\n-\n-diff --git a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp\n---- a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp\n-+++ b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp\n-@@ -359,25 +359,16 @@ void groupChannels(\n- \n-   // Reorder channels associated with one entry based on program order of the\n-   // producers.\n--  for (auto &kv : consumerChannels) {\n--    if (kv.second.size() > 1) {\n--      auto &allOps = kv.second.front()->getSrcOp()->getBlock()->getOperations();\n--      std::sort(\n--          kv.second.begin(), kv.second.end(), [&](Channel *a, Channel *b) {\n--            auto itrA =\n--                std::find_if(allOps.begin(), allOps.end(), [&](Operation &op) {\n--                  Operation *opPointer = &op;\n--                  return opPointer == a->getSrcOp();\n--                });\n--            auto itrB =\n--                std::find_if(allOps.begin(), allOps.end(), [&](Operation &op) {\n--                  Operation *opPointer = &op;\n--                  return opPointer == b->getSrcOp();\n--                });\n--            assert(itrA != allOps.end() && itrB != allOps.end());\n--            return std::distance(itrA, itrB) < 0;\n--          });\n-+  for (auto &group : make_second_range(consumerChannels)) {\n-+    auto &allOps = group.front()->getSrcOp()->getBlock()->getOperations();\n-+    DenseMap<Operation *, size_t> opIdx;\n-+    opIdx.reserve(allOps.size());\n-+    for (auto [idx, op] : enumerate(allOps)) {\n-+      opIdx[&op] = idx;\n-     }\n-+    sort(group, [&](Channel *a, Channel *b) {\n-+      return opIdx[a->getSrcOp()] < opIdx[b->getSrcOp()];\n-+    });\n-   }\n- \n-   // Switch to using channel as the key instead of ops as ops can be volatile.\n-@@ -587,6 +578,18 @@ void createToken(\n-   DenseMap<ttng::TCGen5MMAOp, Channel *> gen5Barriers;\n-   for (auto *key : orderedChannels) {\n-     auto it = channelsGroupedByConsumers.find(key);\n-+    LLVM_DEBUG({\n-+      LDBG(\"createToken key:\");\n-+      LDBG(\"consumer: \");\n-+      key->getDstOp()->dump();\n-+\n-+      LDBG(\"createToken channelsGroupedByConsumers:\");\n-+      for (auto map_key : make_first_range(channelsGroupedByConsumers)) {\n-+        LDBG(\"representative consumer: \");\n-+        map_key->getDstOp()->dump();\n-+      }\n-+    });\n-+    assert(it != channelsGroupedByConsumers.end());\n-     Channel *channel = it->second.front();\n- \n-     CommChannel commChannel;"
        }
    ],
    "stats": {
        "total": 110,
        "additions": 0,
        "deletions": 110
    }
}