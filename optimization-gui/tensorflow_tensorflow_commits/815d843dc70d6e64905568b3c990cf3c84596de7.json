{
    "author": "pineapplejuice233",
    "message": "Move the d2h copy to a separate stream.\n\nPiperOrigin-RevId: 809120840",
    "sha": "815d843dc70d6e64905568b3c990cf3c84596de7",
    "files": [
        {
            "sha": "f4fe1e7ca03846efd51c8d6e9624501c5f774cfa",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -263,11 +263,11 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/framework:allocator\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "e00ca31fded15c6c4e5cd9d0a038954005a3f0f5",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 26,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -231,7 +231,8 @@ TfrtGpuBuffer::DonateWithControlDependency(PjRtFuture<> dependency) {\n   auto new_tracked_buffer = std::make_unique<TrackedGpuDeviceBuffer>(\n       tracked_buffer->buffer(), std::move(new_definition_event),\n       tracked_buffer->ready_event(),\n-      std::move(tracked_buffer->on_delete_callback_));\n+      std::move(tracked_buffer->on_delete_callback_),\n+      std::move(tracked_buffer->cuda_event_));\n \n   auto new_pjrt_buffer = std::make_unique<TfrtGpuBuffer>(\n       on_device_shape_, std::move(new_tracked_buffer), client_, device_,\n@@ -461,19 +462,28 @@ PjRtFuture<> TfrtGpuBuffer::ToLiteralHelper(\n             });\n             MarkGpuEventReadyOnExit ready_on_exit(std::move(usage_event));\n \n-            auto stream = device->stream();\n+            auto d2h_stream = device->d2h_stream();\n+\n+            absl::Status cuda_event_wait_status =\n+                WaitForEventOnStream(d2h_stream, device_buffer->GetCudaEvent());\n+            if (!cuda_event_wait_status.ok()) {\n+              LOG(ERROR) << \"Failed to wait for cuda event: \"\n+                         << cuda_event_wait_status;\n+              promise.Set(cuda_event_wait_status);\n+              return;\n+            }\n \n             VLOG(3) << \"D2H copy: \"\n                     << device_buffer->buffer()->buffer().opaque() << \" -> \"\n                     << buffer_ptr << \" (\" << byte_size << \" bytes)\";\n-            CHECK_OK(stream->Memcpy(\n+            CHECK_OK(d2h_stream->Memcpy(\n                 buffer_ptr, device_buffer->buffer()->buffer(), byte_size))\n                 << \"stream->Memcpy failed copying from GPU to host\";\n \n             absl::Status status;\n             {\n               tsl::profiler::TraceMe traceme(\"BlockHostUntilDone\");\n-              status = stream->BlockHostUntilDone();\n+              status = d2h_stream->BlockHostUntilDone();\n             }\n             VLOG(3) << \"D2H copy done. \" << status;\n             if (!status.ok()) {\n@@ -596,41 +606,39 @@ PjRtFuture<> TfrtGpuBuffer::CopyRawToHostFuture(PjRtFuture<void*> dst_future,\n \n     void* host_ptr = use_staging ? staging_buffer.get() : dst;\n \n-    auto stream = device->stream();\n+    auto d2h_stream = device->d2h_stream();\n+    absl::Status cuda_event_wait_status =\n+        WaitForEventOnStream(d2h_stream, device_buffer->GetCudaEvent());\n+    if (!cuda_event_wait_status.ok()) {\n+      LOG(ERROR) << \"Failed to wait for cuda event: \" << cuda_event_wait_status;\n+      promise.Set(cuda_event_wait_status);\n+      return;\n+    }\n \n     VLOG(3) << \"D2H copy: \" << sub_buffer.opaque() << \" -> \" << host_ptr << \" (\"\n             << transfer_size << \" bytes)\";\n-    absl::Status status = stream->Memcpy(host_ptr, sub_buffer, transfer_size);\n+    absl::Status status =\n+        d2h_stream->Memcpy(host_ptr, sub_buffer, transfer_size);\n     if (!status.ok()) {\n       LOG(ERROR) << \"stream->Memcpy failed: \" << status;\n       promise.Set(status);\n       return;\n     }\n \n-    if (use_staging) {\n-      status = stream->DoHostCallback(\n-          [dst, staging_buffer = std::move(staging_buffer), transfer_size,\n-           promise = std::move(promise),\n-           usage_event_holder = std::move(usage_event_holder)]() mutable {\n-            tsl::profiler::TraceMe traceme3(\n-                \"CopyRawToHostFuture::D2H_staging_copy\");\n-            std::memcpy(dst, staging_buffer.get(), transfer_size);\n-            VLOG(3) << \"D2H staging copy done: \" << staging_buffer.get()\n-                    << \" -> \" << dst << \" (\" << transfer_size << \" bytes)\";\n-            promise.Set(absl::OkStatus());\n-          });\n-    } else {\n-      status = stream->DoHostCallback(\n-          [promise = std::move(promise),\n-           usage_event_holder = std::move(usage_event_holder)]() mutable {\n-            promise.Set(absl::OkStatus());\n-          });\n-    }\n+    status = d2h_stream->BlockHostUntilDone();\n \n     if (!status.ok()) {\n-      LOG(ERROR) << \"stream->DoHostCallback failed: \" << status;\n+      LOG(ERROR) << \"d2h_stream->BlockHostUntilDone() failed: \" << status;\n       promise.Set(status);\n+      return;\n+    }\n+    if (use_staging) {\n+      tsl::profiler::TraceMe traceme3(\"CopyRawToHostFuture::D2H_staging_copy\");\n+      std::memcpy(dst, staging_buffer.get(), transfer_size);\n+      VLOG(3) << \"D2H staging copy done: \" << staging_buffer.get() << \" -> \"\n+              << dst << \" (\" << transfer_size << \" bytes)\";\n     }\n+    promise.Set(absl::OkStatus());\n   };\n \n   dst_future.OnReady("
        },
        {
            "sha": "f73b7f152b29d396bfe12cbb94165193e257c4ab",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_device.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.cc?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/pjrt/gpu/gpu_topology.pb.h\"\n #include \"xla/pjrt/gpu/tfrt/gpu_event.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_client.h\"\n+#include \"xla/pjrt/gpu/tfrt/utils.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_common.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n@@ -73,9 +74,8 @@ TfrtGpuDevice::TfrtGpuDevice(Options&& options)\n       local_device_id_(options.local_device_id),\n       local_hardware_id_(options.local_hardware_id),\n       executor_(options.executor),\n-      stream_(options.executor == nullptr\n-                  ? nullptr\n-                  : options.executor->CreateStream().value()),\n+      stream_(MaybeCreateStream(options.executor)),\n+      d2h_stream_(MaybeCreateStream(options.executor)),\n       prng_seed_generator_(prng_seed_device_()),\n       prng_seed_distribution_(std::numeric_limits<int>::min(),\n                               std::numeric_limits<int>::max()),\n@@ -114,6 +114,12 @@ TfrtGpuDevice::~TfrtGpuDevice() {\n       LOG(ERROR) << \"Failed to wait for stream to finish: \" << status;\n     }\n   }\n+  if (d2h_stream_ != nullptr) {\n+    absl::Status status = d2h_stream_->BlockHostUntilDone();\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Failed to wait for d2h stream to finish: \" << status;\n+    }\n+  }\n }\n \n PjRtClient* TfrtGpuDevice::client() const { return client_; }"
        },
        {
            "sha": "39529177b683f7197bd4ee3a13c5b01644147fc0",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_device.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_device.h?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -134,6 +134,8 @@ class TfrtGpuDevice final : public PjRtDevice {\n \n   se::Stream* stream() const { return stream_.get(); }\n \n+  se::Stream* d2h_stream() const { return d2h_stream_.get(); }\n+\n   se::StreamExecutor* executor() const { return executor_; }\n \n   tsl::AsyncValueRef<GpuEvent> SetLastCollectiveLaunchEvent(\n@@ -152,6 +154,7 @@ class TfrtGpuDevice final : public PjRtDevice {\n   const PjRtLocalHardwareId local_hardware_id_;\n   se::StreamExecutor* executor_;\n   std::unique_ptr<se::Stream> stream_;\n+  std::unique_ptr<se::Stream> d2h_stream_;\n   absl::InlinedVector<PjRtMemorySpace*, 1> memory_spaces_;\n   absl::flat_hash_map<int, PjRtMemorySpace*> memory_spaces_by_kind_id_;\n "
        },
        {
            "sha": "422b7795aa4d235d928094ec08c0e8e38aa2ab13",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_executable.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -513,6 +513,8 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n     input_deps.push_back(std::move(ordering_event));\n   }\n \n+  TF_ASSIGN_OR_RETURN(auto output_cuda_execute_event, CreateCudaEvent(device));\n+\n   std::vector<tsl::AsyncValueRef<GpuDeviceMemory>> output_buffers;\n   std::vector<std::unique_ptr<PjRtBuffer>> outputs;\n   auto gpu_executable = executables_[executable_idx];\n@@ -531,7 +533,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n       auto leaf_tracked_device_buffer =\n           std::make_unique<TrackedGpuDeviceBuffer>(\n               output_buffers.back().CopyRef(), scheduled_event.CopyRef(),\n-              complete_event.CopyRef());\n+              complete_event.CopyRef(), nullptr, output_cuda_execute_event);\n       VLOG(4) << \"created leaf_tracked_device_buffer: \"\n               << leaf_tracked_device_buffer.get();\n \n@@ -556,7 +558,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n     auto tracked_device_buffer = std::make_unique<TrackedGpuDeviceBuffer>(\n         output_buffers.back().CopyRef(),\n         /*definition_event=*/scheduled_event.CopyRef(),\n-        complete_event.CopyRef());\n+        complete_event.CopyRef(), nullptr, output_cuda_execute_event);\n     VLOG(4) << \"created tracked_device_buffer: \" << tracked_device_buffer.get();\n \n     const Shape& shape = result_shape;\n@@ -598,6 +600,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n        execution_profile(options.execution_profile),\n        send_device_memory(std::move(send_device_memory)),\n        recv_device_memory(std::move(recv_device_memory)),\n+       output_cuda_execute_event(std::move(output_cuda_execute_event)),\n        compute_reservation(std::move(compute_reservation)),\n        client = client_](std::vector<ExecutionInput> execution_inputs) mutable {\n         VLOG(1) << \"execute_fn for \" << executable_name\n@@ -714,6 +717,15 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n           return;\n         }\n \n+        auto record_event_status =\n+            stream->RecordEvent(output_cuda_execute_event.get());\n+        if (!record_event_status.ok()) {\n+          LOG(ERROR) << \"Failed to record cuda event: \" << record_event_status;\n+          scheduled_event.SetError(record_event_status);\n+          complete_event.SetError(record_event_status);\n+          return;\n+        }\n+\n         ExecutionOutput& execution_output = result_buffer_or_status.value();\n         ScopedShapedBuffer output = execution_output.ConsumeResult();\n         if (untuple_result && result_is_tuple) {"
        },
        {
            "sha": "32543f080947b11f907fcb330d1719018cedbd86",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer.cc?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n \n #include <cstddef>\n #include <cstdint>\n+#include <memory>\n #include <utility>\n \n #include \"absl/functional/any_invocable.h\"\n@@ -29,6 +30,7 @@ limitations under the License.\n #include \"xla/shape_tree.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/framework/allocator.h\"\n@@ -80,11 +82,13 @@ TrackedGpuDeviceBuffer::TrackedGpuDeviceBuffer(\n     tsl::AsyncValueRef<GpuDeviceMemory> buffer,\n     tsl::AsyncValueRef<GpuEvent> definition_event,\n     tsl::AsyncValueRef<GpuEvent> ready_event,\n-    absl::AnyInvocable<void() &&> on_delete_callback)\n+    absl::AnyInvocable<void() &&> on_delete_callback,\n+    std::shared_ptr<stream_executor::Event> cuda_event)\n     : buffer_(std::move(buffer)),\n       definition_event_(std::move(definition_event)),\n       ready_event_(std::move(ready_event)),\n-      on_delete_callback_(std::move(on_delete_callback)) {\n+      on_delete_callback_(std::move(on_delete_callback)),\n+      cuda_event_(std::move(cuda_event)) {\n   VLOG(4) << \"TrackedGpuDeviceBuffer::TrackedGpuDeviceBuffer: \" << this << \"\\n \"\n           << tsl::CurrentStackTrace();\n   DCHECK(definition_event_);\n@@ -128,6 +132,7 @@ void TrackedGpuDeviceBuffer::ReleaseDeviceMemory() {\n   buffer_.reset();\n   definition_event_.reset();\n   usage_events_.Clear();\n+  cuda_event_.reset();\n }\n \n void TrackedGpuDeviceBuffer::SetUnOwned() {"
        },
        {
            "sha": "3a1b1bc186f1e96fcbc737bed08e114f724a0726",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.h",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftracked_gpu_device_buffer.h?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -18,11 +18,9 @@ limitations under the License.\n \n #include <cstddef>\n #include <cstdint>\n-#include <functional>\n-#include <type_traits>\n+#include <memory>\n #include <utility>\n \n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -33,8 +31,8 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/event.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/framework/allocator.h\"\n \n namespace xla {\n // TODO(b/400541410): Refactor and Merge this with MaybeOwningDeviceMemory.\n@@ -88,7 +86,8 @@ class TrackedGpuDeviceBuffer {\n       tsl::AsyncValueRef<GpuDeviceMemory> buffer,\n       tsl::AsyncValueRef<GpuEvent> definition_event,\n       tsl::AsyncValueRef<GpuEvent> ready_event,\n-      absl::AnyInvocable<void() &&> on_delete_callback = nullptr);\n+      absl::AnyInvocable<void() &&> on_delete_callback = nullptr,\n+      std::shared_ptr<stream_executor::Event> cuda_event = nullptr);\n \n   TrackedGpuDeviceBuffer(TrackedGpuDeviceBuffer&&) = default;\n   TrackedGpuDeviceBuffer& operator=(TrackedGpuDeviceBuffer&&) = default;\n@@ -128,6 +127,10 @@ class TrackedGpuDeviceBuffer {\n \n   friend class TfrtGpuBuffer;\n \n+  // Gets the cuda execute event to wait if this buffer depends on executions\n+  // from other cuda streams.\n+  stream_executor::Event* GetCudaEvent() const { return cuda_event_.get(); }\n+\n  private:\n   tsl::AsyncValueRef<GpuDeviceMemory> buffer_;\n \n@@ -150,6 +153,8 @@ class TrackedGpuDeviceBuffer {\n   // A callback to call when the TrackedGpuDeviceBuffer is about to be\n   // destroyed.\n   absl::AnyInvocable<void() &&> on_delete_callback_;\n+\n+  std::shared_ptr<stream_executor::Event> cuda_event_;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "f08a5e0b147c98ffffcaeba4895e8880450d7f04",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/memory/memory.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -61,6 +62,7 @@ limitations under the License.\n #include \"xla/pjrt/gpu/se_gpu_topology_description.h\"\n #include \"xla/pjrt/gpu/tfrt/gpu_event.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_client.h\"\n+#include \"xla/pjrt/gpu/tfrt/tfrt_gpu_device.h\"\n #include \"xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.h\"\n #include \"xla/pjrt/host_memory_spaces.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n@@ -108,6 +110,26 @@ limitations under the License.\n \n namespace xla {\n \n+std::unique_ptr<se::Stream> MaybeCreateStream(se::StreamExecutor* executor) {\n+  if (executor == nullptr) {\n+    return nullptr;\n+  }\n+  return executor->CreateStream().value();\n+}\n+\n+absl::Status WaitForEventOnStream(se::Stream* stream, se::Event* event) {\n+  if (!event) {\n+    return absl::OkStatus();\n+  }\n+  return stream->WaitFor(event);\n+}\n+\n+absl::StatusOr<std::shared_ptr<se::Event>> CreateCudaEvent(\n+    TfrtGpuDevice* device) {\n+  TF_ASSIGN_OR_RETURN(auto cuda_event, device->executor()->CreateEvent());\n+  return absl::ShareUniquePtr(std::move(cuda_event));\n+}\n+\n PjRtFuture<> CreateFutureForEvent(tsl::AsyncValueRef<xla::GpuEvent> event) {\n   auto [promise, future] = PjRtFuture<>::MakePromise();\n   auto done_fn = [promise = std::move(promise), event]() mutable {"
        },
        {
            "sha": "1bb89b63f80a055fc88cd24a7b2b86ca591584f4",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/815d843dc70d6e64905568b3c990cf3c84596de7/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h?ref=815d843dc70d6e64905568b3c990cf3c84596de7",
            "patch": "@@ -65,6 +65,13 @@ limitations under the License.\n \n namespace xla {\n \n+std::unique_ptr<se::Stream> MaybeCreateStream(se::StreamExecutor* executor);\n+\n+absl::Status WaitForEventOnStream(se::Stream* stream, se::Event* event);\n+\n+absl::StatusOr<std::shared_ptr<se::Event>> CreateCudaEvent(\n+    TfrtGpuDevice* device);\n+\n PjRtFuture<> CreateFutureForEvent(tsl::AsyncValueRef<xla::GpuEvent> event);\n \n absl::StatusOr<Shape> GetDestinationDeviceShape(const Shape& host_shape,"
        }
    ],
    "stats": {
        "total": 146,
        "additions": 107,
        "deletions": 39
    }
}