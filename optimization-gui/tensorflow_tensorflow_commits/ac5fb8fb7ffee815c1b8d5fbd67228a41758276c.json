{
    "author": "tensorflower-gardener",
    "message": "Introduce and connect an XLA:TPU shardy option to fully deduplicate functions in Shardy.\n\nThis change is a no-op since both newly introduced XLA:TPU option and the corresponding option on ExportNamedComputation pass is false by default.\n\nPiperOrigin-RevId: 821039969",
    "sha": "ac5fb8fb7ffee815c1b8d5fbd67228a41758276c",
    "files": [
        {
            "sha": "e25c6c59e649b59d2efed22b08ed55b3c6d32a0c",
            "filename": "third_party/xla/xla/service/spmd/shardy/shardy_xla_pass.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac5fb8fb7ffee815c1b8d5fbd67228a41758276c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac5fb8fb7ffee815c1b8d5fbd67228a41758276c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc?ref=ac5fb8fb7ffee815c1b8d5fbd67228a41758276c",
            "patch": "@@ -313,6 +313,7 @@ absl::Status runShardingPropagation(HloModule* hloModule,\n                                     mlir::ModuleOp mlirModule,\n                                     bool importMhloShardings,\n                                     mlir::sdy::PropagationOptions options,\n+                                    bool dedupFunctionsFully,\n                                     absl::string_view passName) {\n   LOG(INFO) << \"Using Shardy for XLA SPMD propagation.\";\n \n@@ -386,7 +387,10 @@ absl::Status runShardingPropagation(HloModule* hloModule,\n   options.conservativePropagation = hloModule->use_auto_spmd_partitioning();\n   options.enableAutoPartitioning = hloModule->use_auto_spmd_partitioning();\n   mlir::sdy::addPropagationPipeline(pm, dumpIndex, options);\n-  addStablehloExportPipeline(pm);\n+\n+  xla::sdy::StablehloExportPipelineOptions stablehloExportPipelineOptions;\n+  stablehloExportPipelineOptions.dedupFunctionsFully = dedupFunctionsFully;\n+  addStablehloExportPipeline(pm, stablehloExportPipelineOptions);\n   pm.addPass(mlir::sdy::createSaveModuleOpPass(shardyDir, \"output_module\",\n                                                dumpIndex++));\n   tsl::StatusScopedDiagnosticHandler diagnosticHandler(\n@@ -470,9 +474,9 @@ absl::StatusOr<bool> ShardyXLA::Run(\n                                      useTupleArgs);\n \n   if (runSdyShardingPropagation) {\n-    TF_RETURN_IF_ERROR(runShardingPropagation(hloModule, mlirModule.get(),\n-                                              importMhloShardings,\n-                                              defaultOptions, name()));\n+    TF_RETURN_IF_ERROR(\n+        runShardingPropagation(hloModule, mlirModule.get(), importMhloShardings,\n+                               defaultOptions, dedupFunctionsFully, name()));\n   }\n \n   // TODO(b/431836696): Remove once issue is fixed."
        },
        {
            "sha": "4cac123b6de2282da566d3f970184625d6d40cea",
            "filename": "third_party/xla/xla/service/spmd/shardy/shardy_xla_pass.h",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ac5fb8fb7ffee815c1b8d5fbd67228a41758276c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ac5fb8fb7ffee815c1b8d5fbd67228a41758276c/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.h?ref=ac5fb8fb7ffee815c1b8d5fbd67228a41758276c",
            "patch": "@@ -34,9 +34,11 @@ class ShardyXLA : public xla::HloModulePass {\n  public:\n   explicit ShardyXLA(bool runSdyShardingPropagation = true,\n                      mlir::sdy::PropagationOptions defaultOptions =\n-                         mlir::sdy::PropagationOptions{})\n+                         mlir::sdy::PropagationOptions{},\n+                     bool dedupFunctionsFully = false)\n       : runSdyShardingPropagation(runSdyShardingPropagation),\n-        defaultOptions(defaultOptions) {}\n+        defaultOptions(defaultOptions),\n+        dedupFunctionsFully(dedupFunctionsFully) {}\n \n   absl::string_view name() const override { return \"shardy-xla\"; }\n \n@@ -48,6 +50,11 @@ class ShardyXLA : public xla::HloModulePass {\n  private:\n   bool runSdyShardingPropagation;\n   mlir::sdy::PropagationOptions defaultOptions;\n+  // Whether to deduplicate functions fully, regardless of the input and output\n+  // shardings of functions, and it keeps one callee function for each caller\n+  // function. The default is false, meaning it will deduplicate only if the\n+  // input and output shardings are the same.\n+  bool dedupFunctionsFully;\n   // TODO. Run other SDY passes with flags.\n };\n "
        }
    ],
    "stats": {
        "total": 23,
        "additions": 17,
        "deletions": 6
    }
}