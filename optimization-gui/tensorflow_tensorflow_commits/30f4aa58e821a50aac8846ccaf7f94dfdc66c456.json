{
    "author": "beckerhe",
    "message": "Introduce ThunkId\n\nThis change introduces a new field `thunk_id` in ThunkInfo. It is meant to contain a unique ID which allows to identify each thunk in a thunk graph.\n\nThe IDs are generated and assigned by the IR Emitter (where the thunks are being created).\n\nThunk IDs will allow us to reference thunks in the upcoming SDC log.\n\nPiperOrigin-RevId: 810733732",
    "sha": "30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
    "files": [
        {
            "sha": "901103333277b7f3de719e91ad6c4fa9b6d04af2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/copy.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -89,7 +89,8 @@ absl::StatusOr<FusionEmissionResult> MemcpyFusion::Emit(\n   for (int i = 0; i < src_buffers.size(); ++i) {\n     if (src_buffers[i] != dst_buffers[i]) {\n       result.thunks.emplace_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(&fusion),\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              &fusion, ir_emitter_context.GetNextThunkId()),\n           /*source_buffer=*/src_buffers[i],\n           /*destination_buffer=*/dst_buffers[i],\n           /*mem_size=*/src_buffers[i].size()));\n@@ -152,7 +153,8 @@ absl::StatusOr<FusionEmissionResult> DynamicMemcpyFusion::Emit(\n                std::back_inserter(offsets.dst_offsets));\n \n   result.thunks.emplace_back(std::make_unique<DynamicMemcpyThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(&fusion),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          &fusion, ir_emitter_context.GetNextThunkId()),\n       /*source_buffer=*/src_buffer,\n       /*destination_buffer=*/dst_buffer,\n       /*mem_size=*/ShapeUtil::ByteSizeOfElements(*copy_shape), offsets));"
        },
        {
            "sha": "41c08f2f6677bcb9c0a611292a8890ee3dc4ce44",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -45,7 +45,8 @@ absl::StatusOr<FusionEmissionResult> CuDnnFusion::Emit(\n   result.thunks.emplace_back(std::make_unique<CuDnnThunk>(\n       emitters::GetComputationFingerprint(\n           fusion.fused_instructions_computation(), {}),\n-      Thunk::ThunkInfo::WithProfileAnnotation(&fusion),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          &fusion, ir_emitter_context.GetNextThunkId()),\n       kernel_arguments.GetArgumentBufferSlices(),\n       kernel_arguments.GetArgumentOutputFlags()));\n   return result;"
        },
        {
            "sha": "37d2a908e10f2a42ca349179d3d31d08d1728fc5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -99,8 +99,9 @@ absl::StatusOr<std::unique_ptr<Thunk>> BuildCustomKernelThunkForFusion(\n       emitters::KernelArguments::Create(ir_emitter_context.buffer_assignment(),\n                                         GetDefaultBufferAlignment(), &fusion));\n \n-  return std::make_unique<CustomKernelThunk>(&fusion, std::move(custom_kernel),\n-                                             std::move(kernel_arguments));\n+  return std::make_unique<CustomKernelThunk>(\n+      &fusion, std::move(custom_kernel), std::move(kernel_arguments),\n+      ir_emitter_context.GetNextThunkId());\n }\n \n absl::StatusOr<BufferAllocation::Slice> GetOperandSlice(\n@@ -642,7 +643,8 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n                       ir_emitter_context.gpu_compute_capability()));\n \n   std::unique_ptr<Thunk> thunk;\n-  auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(&fusion);\n+  auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      &fusion, ir_emitter_context.GetNextThunkId());\n \n   if (absl::c_any_of(slice_instrs, IsDynamicSliceOrDynamicUpdateSlice)) {\n     // Creating embedded GEMM thunk.\n@@ -880,7 +882,8 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n   }\n \n   std::unique_ptr<Thunk> thunk;\n-  auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(&fusion);\n+  auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      &fusion, ir_emitter_context.GetNextThunkId());\n \n   auto ffi_thunk =\n       [&](Slices ops,\n@@ -1213,7 +1216,8 @@ absl::StatusOr<FusionEmissionResult> EmitCollective(\n       NcclThunkType::CheckImplementable(instr, replica_count, partition_count);\n   bool is_degenerate = GetCollectiveConfig(instr, use_global_device_ids)\n                            .IsDegenerate(replica_count, partition_count);\n-  Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(instr);\n+  Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context.GetNextThunkId());\n \n   FusionEmissionResult result;\n \n@@ -1280,7 +1284,9 @@ absl::StatusOr<FusionEmissionResult> EmitCollective(\n     } else {\n       auto collective_done_thunk = std::make_unique<CollectiveDoneThunk>(\n           /*kind=*/collective_done_thunk_kind,\n-          /*thunk_info=*/Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+          /*thunk_info=*/\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context.GetNextThunkId()),\n           /*async_events=*/async_events,\n           /*async_stream_kind=*/AsyncStreamKind::kCollective);\n       seq.emplace_back(std::move(collective_done_thunk));"
        },
        {
            "sha": "aea689d5279efd861e8cb2422b12ea3704c6ba62",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2FBUILD?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -46,7 +46,6 @@ cc_library(\n     srcs = [\"emitter_base.cc\"],\n     hdrs = [\"emitter_base.h\"],\n     deps = [\n-        \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n@@ -79,10 +78,10 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/framework/mlir:status_scoped_diagnostic_handler\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "37f4d4edc6138ad8492ac7f1762c0b6bb19ac550",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -16,15 +16,13 @@ limitations under the License.\n \n #include <cstdint>\n #include <functional>\n-#include <iterator>\n #include <memory>\n #include <optional>\n #include <string>\n #include <utility>\n #include <variant>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -67,7 +65,6 @@ limitations under the License.\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/DialectRegistry.h\"\n-#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n #include \"mlir/IR/Location.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n@@ -111,10 +108,10 @@ limitations under the License.\n #include \"xla/service/gpu/llvm_gpu_backend/ptx_version_util.h\"\n #include \"xla/service/gpu/target_util.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n-#include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/framework/mlir/status_scoped_diagnostic_handler.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -319,8 +316,10 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n \n   FusionEmissionResult result;\n   result.thunks.emplace_back(std::make_unique<KernelThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(&fusion), entry->kernel_name,\n-      args, launch_dims, entry->cluster_dim, entry->shmem_bytes,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          &fusion, ir_emitter_context.GetNextThunkId()),\n+      entry->kernel_name, args, launch_dims, entry->cluster_dim,\n+      entry->shmem_bytes,\n       /*tma_metadata=*/se::gpu::TmaMetadata()));\n   return result;\n }"
        },
        {
            "sha": "411cd8575e563fffc7153f871cfa7b24aaaae723",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -265,9 +265,10 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n \n   FusionEmissionResult result;\n   result.thunks.emplace_back(std::make_unique<KernelThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(&fusion), entry->kernel_name,\n-      kernel_arguments, entry->launch_dimensions, entry->cluster_dim,\n-      entry->shmem_bytes, entry->tma_metadata));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          &fusion, ir_emitter_context.GetNextThunkId()),\n+      entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n+      entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata));\n \n   return result;\n }"
        },
        {
            "sha": "ff683e4866b96930dc0a6e31a10c665e8b36902c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -749,6 +749,7 @@ xla_test(\n     deps = [\n         \":gpublas_lt_matmul_thunk\",\n         \":thunk\",\n+        \":thunk_id\",\n         \"//xla:error_spec\",\n         \"//xla:executable_run_options\",\n         \"//xla:shape_util\",\n@@ -830,6 +831,7 @@ cc_library(\n     hdrs = [\"kernel_thunk.h\"],\n     deps = [\n         \":thunk\",\n+        \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n@@ -981,6 +983,7 @@ cc_library(\n     hdrs = [\"select_k_thunk.h\"],\n     deps = [\n         \":thunk\",\n+        \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n@@ -1008,6 +1011,7 @@ xla_cc_test(\n     deps = [\n         \":select_k_thunk\",\n         \":thunk\",\n+        \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n@@ -1450,6 +1454,7 @@ cc_library(\n     deps = [\n         \":collective_thunk\",\n         \":thunk\",\n+        \":thunk_id\",\n         \"//xla:util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_communicator\",\n@@ -1541,6 +1546,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@local_tsl//tsl/profiler/lib:scoped_annotation\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n     ],\n@@ -1552,6 +1558,7 @@ xla_cc_test(\n     deps = [\n         \":sequential_thunk\",\n         \":thunk\",\n+        \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n@@ -1592,11 +1599,29 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"thunk_id\",\n+    hdrs = [\"thunk_id.h\"],\n+    deps = [\n+        \"//xla/tsl/lib/gtl:int_type\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"thunk_id_test\",\n+    srcs = [\"thunk_id_test.cc\"],\n+    deps = [\n+        \":thunk_id\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"thunk\",\n     srcs = [\"thunk.cc\"],\n     hdrs = [\"thunk.h\"],\n     deps = [\n+        \":thunk_id\",\n         \":thunk_proto_cc\",\n         \"//xla:executable_run_options\",\n         \"//xla:status_macros\","
        },
        {
            "sha": "6f65bc866beeaba6f4969f820739d5424f0322d0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_group_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/stream_executor/event.h\"\n@@ -43,8 +44,9 @@ namespace gpu {\n \n CollectiveGroupThunk::CollectiveGroupThunk(\n     const HloInstruction* instruction, Thunk::Kind kind,\n-    std::vector<std::unique_ptr<Thunk>> thunks, AsyncStreamKind stream_kind)\n-    : Thunk(kind, ThunkInfo::WithProfileAnnotation(instruction)),\n+    std::vector<std::unique_ptr<Thunk>> thunks, AsyncStreamKind stream_kind,\n+    ThunkId thunk_id)\n+    : Thunk(kind, ThunkInfo::WithProfileAnnotation(instruction, thunk_id)),\n       stream_kind_(stream_kind),\n       async_events_(new CollectiveThunk::AsyncEvents()) {\n   for (auto& thunk : thunks) {"
        },
        {
            "sha": "dff9a04c7cfa4e34af51cc7afe21a394645444f7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_group_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_group_thunk.h?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -38,7 +38,7 @@ class CollectiveGroupThunk : public Thunk {\n  public:\n   CollectiveGroupThunk(const HloInstruction* instruction, Thunk::Kind kind,\n                        std::vector<std::unique_ptr<Thunk>> thunks,\n-                       AsyncStreamKind stream_kind);\n+                       AsyncStreamKind stream_kind, ThunkId thunk_id);\n   absl::Status Prepare(const PrepareParams& params,\n                        ResourceRequestsInterface& resource_requests) override;\n   absl::Status ExecuteOnStream(const Thunk::ExecuteParams& params) override;"
        },
        {
            "sha": "06a8778df6043074e39079d1779fed3e717d8c2f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/gpublas_lt_matmul_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fgpublas_lt_matmul_thunk_test.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"absl/time/clock.h\"\n #include \"absl/time/time.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -145,7 +146,8 @@ class GpuBlasLtThunkBuilder {\n       bias = slices[has_matrix_bias ? 3 : 2];\n     }\n \n-    Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(gemm);\n+    Thunk::ThunkInfo thunk_info =\n+        Thunk::ThunkInfo::WithProfileAnnotation(gemm, ThunkId(1));\n     std::string canonical_hlo = gemm->ToString(\n         HloPrintOptions::Fingerprint().set_print_backend_config(true));\n "
        },
        {
            "sha": "67fee7b622596c71d889016a0171574566c69442",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"llvm/ADT/STLExtras.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -255,9 +256,9 @@ absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n CustomKernelThunk::CustomKernelThunk(\n     const HloInstruction* instr, CustomKernel custom_kernel,\n-    const emitters::KernelArguments& kernel_arguments)\n+    const emitters::KernelArguments& kernel_arguments, ThunkId thunk_id)\n     : Thunk(Kind::kCustomKernel,\n-            Thunk::ThunkInfo::WithProfileAnnotation(instr)),\n+            Thunk::ThunkInfo::WithProfileAnnotation(instr, thunk_id)),\n       args_(kernel_arguments.GetArgumentBufferSlices()),\n       written_(kernel_arguments.GetArgumentOutputFlags()),\n       custom_kernel_(std::move(custom_kernel)) {}"
        },
        {
            "sha": "656dceca0040d7f9a9b0bd3d0f58d45fbe682163",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -148,7 +148,8 @@ class KernelThunk : public Thunk {\n class CustomKernelThunk : public Thunk {\n  public:\n   CustomKernelThunk(const HloInstruction* inst, CustomKernel custom_kernel,\n-                    const emitters::KernelArguments& kernel_arguments);\n+                    const emitters::KernelArguments& kernel_arguments,\n+                    ThunkId thunk_id);\n \n   std::string ToString(int indent) const override;\n "
        },
        {
            "sha": "7ac3c63dd600c4b9c7bb412261aede4fdf7ecf47",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/select_k_exec.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/primitive_util.h\"\n@@ -45,8 +46,10 @@ namespace xla::gpu {\n SelectKThunk::SelectKThunk(const HloInstruction* inst, std::uint32_t batch_size,\n                            std::uint32_t num_elements, std::uint32_t k,\n                            xla::PrimitiveType dtype,\n-                           const emitters::KernelArguments& kernel_arguments)\n-    : Thunk(Kind::kSelectK, Thunk::ThunkInfo::WithProfileAnnotation(inst)),\n+                           const emitters::KernelArguments& kernel_arguments,\n+                           ThunkId thunk_id)\n+    : Thunk(Kind::kSelectK,\n+            Thunk::ThunkInfo::WithProfileAnnotation(inst, thunk_id)),\n       batch_size_(batch_size),\n       num_elements_(num_elements),\n       k_(k),"
        },
        {
            "sha": "94eb9c5f822a618792b62c0865939da2920d69cc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk.h?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -50,7 +50,8 @@ class SelectKThunk : public Thunk {\n   SelectKThunk(const HloInstruction* inst, std::uint32_t batch_size,\n                std::uint32_t num_elements, std::uint32_t k,\n                xla::PrimitiveType dtype,\n-               const emitters::KernelArguments& kernel_arguments);\n+               const emitters::KernelArguments& kernel_arguments,\n+               ThunkId thunk_id);\n \n   std::string ToString(int indent) const override;\n "
        },
        {
            "sha": "ba469be381d3ed78c54f19844f2e49b0075be7a0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/select_k_thunk_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fselect_k_thunk_test.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/literal_util.h\"\n@@ -39,6 +40,7 @@ TEST(SelectKThunkTest, ToProto) {\n   Thunk::ThunkInfo thunk_info;\n   thunk_info.profile_annotation = \"profile_annotation\";\n   thunk_info.execution_stream_id = 123;\n+  thunk_info.thunk_id = 456;\n \n   BufferAllocation alloc0(/*index=*/0, /*size=*/20, /*color=*/0);\n   BufferAllocation::Slice slice0(&alloc0, /*offset=*/0, /*size=*/20);\n@@ -63,10 +65,11 @@ TEST(SelectKThunkTest, ToProto) {\n   auto topKInst = HloInstruction::CreateCustomCall(\n       ShapeUtil::MakeShape(F32, {1, 5}), {c1.get()}, \"__gpu$TopK\");\n \n-  SelectKThunk thunk(topKInst.get(), 1, 5, 3, F32, kernel_arguments);\n+  SelectKThunk thunk(topKInst.get(), 1, 5, 3, F32, kernel_arguments,\n+                     ThunkId(456));\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, thunk.ToProto());\n   EXPECT_THAT(proto, EqualsProto(R\"pb(\n-                thunk_info { profile_annotation: \"custom-call\" }\n+                thunk_info { profile_annotation: \"custom-call\" thunk_id: 456 }\n                 select_k_thunk {}\n               )pb\"));\n }"
        },
        {
            "sha": "c5a5b4bf3347cc313b23d2cc2d6ba609b69ed4b6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/sequential_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/annotation.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -57,6 +58,8 @@ std::string SequentialThunk::ToString(int indent) const {\n       Thunk::KindToString(thunk_with_longest_kind->get()->kind()).length();\n   std::string result;\n   for (const std::unique_ptr<Thunk>& thunk : thunks_) {\n+    absl::StrAppendFormat(&result,\n+                          \"%03d: \", thunk->thunk_info().thunk_id.value());\n     // Write out the thunk kind, padded out to max_thunk_kind_len.\n     absl::string_view kind_str = Thunk::KindToString(thunk->kind());\n     absl::StrAppend(&result, indent_str, kind_str,"
        },
        {
            "sha": "b3ddfc19db1273a29318e076c965089729c29ce0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/sequential_thunk_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 3,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk_test.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <memory>\n #include <string>\n+#include <utility>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n@@ -25,12 +26,21 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n-#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n namespace {\n \n+class DummyThunk : public Thunk {\n+ public:\n+  explicit DummyThunk(Thunk::Kind kind, Thunk::ThunkInfo thunk_info)\n+      : Thunk(kind, std::move(thunk_info)) {}\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override {\n+    return absl::OkStatus();\n+  }\n+};\n+\n using ::testing::IsEmpty;\n \n constexpr ExecutionStreamId kExecutionStreamId{123};\n@@ -40,6 +50,7 @@ Thunk::ThunkInfo GetExampleThunkInfo() {\n   Thunk::ThunkInfo thunk_info{};\n   thunk_info.execution_stream_id = kExecutionStreamId;\n   thunk_info.profile_annotation = kProfileAnnotation;\n+  thunk_info.thunk_id = ThunkId(1);\n   return thunk_info;\n }\n \n@@ -78,8 +89,7 @@ TEST(SequentialThunkTest, SequentialThunkChainFromProto) {\n   // sequential thunk.\n   ThunkProto* inner_proto = outer_proto.add_thunks();\n   inner_proto->mutable_sequential_thunk();\n-  inner_proto->mutable_thunk_info()->set_profile_annotation(\n-      std::string{kProfileAnnotation});\n+  inner_proto->mutable_thunk_info()->set_profile_annotation(kProfileAnnotation);\n   inner_proto->mutable_thunk_info()->set_execution_stream_id(\n       kExecutionStreamId.value());\n \n@@ -116,5 +126,31 @@ TEST(SequentialThunkTest, SequentialThunkChainFromProto) {\n   EXPECT_EQ(inner_thunk->profile_annotation(), kProfileAnnotation);\n }\n \n+TEST(SequentialThunkTest, ToString) {\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.profile_annotation = \"profile_annotation\";\n+  thunk_info.execution_stream_id = 123;\n+  thunk_info.thunk_id = ThunkId(1);\n+\n+  ThunkSequence thunks;\n+  thunks.push_back(\n+      std::make_unique<DummyThunk>(Thunk::Kind::kGemm, thunk_info));\n+\n+  thunk_info.thunk_id = ThunkId(2);\n+  thunks.push_back(\n+      std::make_unique<DummyThunk>(Thunk::Kind::kGemm, thunk_info));\n+\n+  thunk_info.thunk_id = ThunkId(3);\n+  thunks.push_back(\n+      std::make_unique<DummyThunk>(Thunk::Kind::kGemm, thunk_info));\n+\n+  thunk_info.thunk_id = ThunkId(4);\n+  SequentialThunk sequential_thunk(thunk_info, std::move(thunks));\n+  EXPECT_EQ(sequential_thunk.ToString(/*indent=*/0),\n+            \"001: kGemm\\t\\n\"\n+            \"002: kGemm\\t\\n\"\n+            \"003: kGemm\\t\\n\");\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "80812b3eed9435f1bc5d11e1e03c998cb0ca1b31",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/executable_run_options.h\"\n@@ -107,14 +108,17 @@ using GlobalDeviceIdMap = Thunk::CollectiveExecuteParams::GlobalDeviceIdMap;\n static absl::StatusOr<GlobalDeviceId> GetGlobalDeviceId(\n     const GlobalDeviceIdMap* device_id_map, int64_t local_device_ordinal) {\n   // No local -> global mapping was provided; assume the identity mapping.\n-  if (!device_id_map) return GlobalDeviceId(local_device_ordinal);\n+  if (!device_id_map) {\n+    return GlobalDeviceId(local_device_ordinal);\n+  }\n \n   // Find a global device id in a global device id map.\n   auto it = device_id_map->find(local_device_ordinal);\n-  if (it == device_id_map->end())\n+  if (it == device_id_map->end()) {\n     return absl::NotFoundError(\n         absl::StrCat(\"No global device id found for local device ordinal: \",\n                      local_device_ordinal));\n+  }\n \n   return it->second;\n }\n@@ -353,13 +357,15 @@ absl::StatusOr<Thunk::ThunkInfo> Thunk::ThunkInfo::FromProto(\n   Thunk::ThunkInfo thunk_info;\n   thunk_info.profile_annotation = proto.profile_annotation();\n   thunk_info.execution_stream_id = proto.execution_stream_id();\n+  thunk_info.thunk_id = ThunkId(proto.thunk_id());\n   return thunk_info;\n }\n \n Thunk::ThunkInfo Thunk::ThunkInfo::WithProfileAnnotation(\n-    const HloInstruction* instr) {\n+    const HloInstruction* instr, ThunkId thunk_id) {\n   ThunkInfo thunk_info;\n   thunk_info.profile_annotation = instr->name();\n+  thunk_info.thunk_id = thunk_id;\n   auto gpu_backend_config = instr->backend_config<GpuBackendConfig>();\n   if (gpu_backend_config.ok()) {\n     thunk_info.execution_stream_id =\n@@ -428,6 +434,7 @@ ThunkInfoProto Thunk::ThunkInfo::ToProto() const {\n   ThunkInfoProto proto;\n   proto.set_profile_annotation(profile_annotation);\n   proto.set_execution_stream_id(execution_stream_id.value());\n+  proto.set_thunk_id(thunk_id.value());\n   return proto;\n }\n "
        },
        {
            "sha": "e742c05132dd1e1ecb2f8a5de11e6a11254fb5c7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.h",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/executable_run_options.h\"\n@@ -87,7 +88,7 @@ TSL_LIB_GTL_DEFINE_INT_TYPE(ExecutionStreamId, uint64_t);\n // Unique identifier for async events. The same identifier is expected to be\n // shared between a pair of StartThunk and corresponding DoneThunk. It is used\n // to collect async regions for a CommandBufferThunk.\n-TSL_LIB_GTL_DEFINE_INT_TYPE(AsyncEventsUniqueId, uint64_t)\n+TSL_LIB_GTL_DEFINE_INT_TYPE(AsyncEventsUniqueId, uint64_t);\n \n // Thunk acts as the bridge between IrEmitter and GpuExecutable. It stores the\n // metadata IrEmitter generates for GpuExecutable to invoke an HloInstruction.\n@@ -220,12 +221,15 @@ class Thunk {\n     static absl::StatusOr<Thunk::ThunkInfo> FromProto(\n         const ThunkInfoProto& proto);\n \n-    static ThunkInfo WithProfileAnnotation(const HloInstruction* instr);\n+    static ThunkInfo WithProfileAnnotation(const HloInstruction* instr,\n+                                           ThunkId thunk_id);\n \n     std::string profile_annotation;\n \n     ExecutionStreamId execution_stream_id = kDefaultExecutionStreamId;\n \n+    ThunkId thunk_id = ThunkId{0};\n+\n     // Serializes a ThunkInfo to a ThunkInfoProto.\n     ThunkInfoProto ToProto() const;\n   };"
        },
        {
            "sha": "5f63b509c4841e618284b763aae5622460f4be8f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -29,6 +29,7 @@ import \"xla/xla_data.proto\";\n message ThunkInfoProto {\n   string profile_annotation = 1;\n   int64 execution_stream_id = 2;\n+  int64 thunk_id = 3;\n }\n \n message CopyThunkProto {"
        },
        {
            "sha": "0dfa9999954ee6c0edcdcb273e1b4da7ab28b97b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_id.h",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_id.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_id.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_id.h?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -0,0 +1,40 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_THUNK_ID_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_THUNK_ID_H_\n+\n+#include <cstdint>\n+\n+#include \"xla/tsl/lib/gtl/int_type.h\"\n+\n+namespace xla::gpu {\n+\n+// Unique identifier for a thunk. When creating a thunk graph it's up to the\n+// emitter to generate unique IDs.\n+TSL_LIB_GTL_DEFINE_INT_TYPE(ThunkId, uint64_t);\n+\n+// Generates unique IDs for thunks. This class is thread-compatible.\n+class ThunkIdGenerator {\n+ public:\n+  ThunkId GetNextThunkId() { return ThunkId(next_thunk_id_++); }\n+\n+ private:\n+  uint64_t next_thunk_id_ = 1;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_THUNK_ID_H_"
        },
        {
            "sha": "60d5b3f4ee2b4a098eec8c1f947b264a02fe04fd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_id_test.cc",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_id_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_id_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_id_test.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -0,0 +1,38 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n+\n+#include <gtest/gtest.h>\n+\n+namespace xla::gpu {\n+namespace {\n+\n+TEST(ThunkIdTest, GeneratesUniqueIds) {\n+  ThunkIdGenerator generator;\n+\n+  ThunkId id1 = generator.GetNextThunkId();\n+  ThunkId id2 = generator.GetNextThunkId();\n+  ThunkId id3 = generator.GetNextThunkId();\n+\n+  // The only property that we guarantee is uniqueness, no ordering, no\n+  // consecutiveness, etc.\n+  EXPECT_NE(id1, id2);\n+  EXPECT_NE(id1, id3);\n+  EXPECT_NE(id2, id3);\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "5da477db2cf5a45e1b80d6984cbfe3cb92c39422",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -324,10 +324,12 @@ cc_library(\n         \":kernel_reuse_cache\",\n         \"//xla/backends/gpu/runtime:collective_thunk\",\n         \"//xla/backends/gpu/runtime:host_execute_thunk\",\n+        \"//xla/backends/gpu/runtime:thunk_id\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:name_uniquer\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/strings:string_view\","
        },
        {
            "sha": "b458e76889e7b64ab4e0ecdbb053426e8ced0df0",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"mlir/IR/Operation.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/host_execute_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -39,6 +40,7 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/kernel_reuse_cache.h\"\n #include \"xla/service/name_uniquer.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {\n@@ -139,6 +141,8 @@ class IrEmitterContext {\n \n   bool emit_kernels() const { return emit_kernels_; }\n \n+  ThunkId GetNextThunkId() { return thunk_id_generator_.GetNextThunkId(); }\n+\n  private:\n   const HloModule* hlo_module_;\n   const BufferAssignment* buffer_assignment_;\n@@ -157,6 +161,9 @@ class IrEmitterContext {\n \n   // We should not emit kernels when loading thunks from a compilation result.\n   const bool emit_kernels_;\n+\n+  // Generates unique IDs for thunk creation.\n+  ThunkIdGenerator thunk_id_generator_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "773237692ae154760e0b79378d80f245ffe8eef2",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 170,
            "deletions": 94,
            "changes": 264,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -248,7 +248,8 @@ absl::Status IrEmitterUnnested::EmitConditional(const HloInstruction* instr) {\n     auto ir_emitter = IrEmitterUnnested::Create(ir_emitter_context_);\n     TF_RETURN_IF_ERROR(ir_emitter->EmitHloComputation(comp));\n     Thunk::ThunkInfo branch_thunk_info =\n-        Thunk::ThunkInfo::WithProfileAnnotation(instr);\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            instr, ir_emitter_context_->GetNextThunkId());\n     branch_thunk_info.profile_annotation +=\n         absl::StrCat(\"_branch_\", comp->name());\n     branch_thunks.push_back(\n@@ -259,8 +260,9 @@ absl::Status IrEmitterUnnested::EmitConditional(const HloInstruction* instr) {\n                       GetAllocationSliceForHlo(instr->operand(0), {}));\n   bool branch_index_is_bool = instr->operand(0)->shape().element_type() == PRED;\n   AddThunkToThunkSequence(std::unique_ptr<Thunk>(new ConditionalThunk(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), slice,\n-      std::move(branch_thunks), branch_index_is_bool)));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      slice, std::move(branch_thunks), branch_index_is_bool)));\n   return absl::OkStatus();\n }\n \n@@ -615,7 +617,9 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(\n           ConvertToCommandsOptions{synchronization_mode, enable_loop_unroll}));\n \n   AddThunkToThunkSequence(std::make_unique<CommandBufferThunk>(\n-      std::move(cmd_executor), Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      std::move(cmd_executor),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       std::move(thunk_sequence),\n       ir_emitter_context_->debug_options()\n           .xla_enable_command_buffers_during_profiling()));\n@@ -663,8 +667,10 @@ absl::Status IrEmitterUnnested::EmitConvolutionThunk(\n \n   TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, \"\"));\n   AddThunkToThunkSequence(std::make_unique<ConvolutionThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(config),\n-      std::move(operand_slices), std::move(result_slices), scratch_slice));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      std::move(config), std::move(operand_slices), std::move(result_slices),\n+      scratch_slice));\n   return absl::OkStatus();\n }\n \n@@ -694,8 +700,9 @@ absl::Status IrEmitterUnnested::EmitGemmThunk(\n       GemmConfig::For(static_cast<const HloInstruction*>(instr),\n                       ir_emitter_context_->gpu_compute_capability()));\n   auto thunk = std::make_unique<GemmThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(config), a, b,\n-      c, workspace,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      std::move(config), a, b, c, workspace,\n       RequireDeterminism(ir_emitter_context_->hlo_module().config()));\n   AddThunkToThunkSequence(std::move(thunk));\n   return absl::OkStatus();\n@@ -771,7 +778,8 @@ absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunk(\n   BufferAllocation::Slice a_scale, b_scale, c_scale, d_scale, d_amax;\n   TF_ASSIGN_OR_RETURN(se::gpu::BlasLt::Epilogue blas_lt_epilogue,\n                       gpublas_lt::AsBlasLtEpilogue(epilogue));\n-  Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(instr);\n+  Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n   std::string canonical_hlo = instr->ToString(\n       HloPrintOptions::Fingerprint().set_print_backend_config(true));\n   auto thunk = std::make_unique<CublasLtMatmulThunk>(\n@@ -867,7 +875,8 @@ absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunkF8(\n \n   TF_ASSIGN_OR_RETURN(se::gpu::BlasLt::Epilogue blas_lt_epilogue,\n                       gpublas_lt::AsBlasLtEpilogue(epilogue));\n-  Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(instr);\n+  Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n   std::string canonical_hlo = instr->ToString(\n       HloPrintOptions::Fingerprint().set_print_backend_config(true));\n   auto thunk = std::make_unique<CublasLtMatmulThunk>(\n@@ -915,7 +924,8 @@ absl::Status IrEmitterUnnested::EmitConvolutionReorderThunk(\n   }\n \n   auto thunk = std::make_unique<ConvolutionReorderThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       absl::MakeSpan(filter_dims), operand_slices, result_slices);\n   AddThunkToThunkSequence(std::move(thunk));\n   return absl::OkStatus();\n@@ -993,9 +1003,11 @@ absl::Status IrEmitterUnnested::EmitNormThunk(\n   TF_ASSIGN_OR_RETURN(GpuNormConfig config, GpuNormConfig::For(descriptor));\n \n   auto thunk = std::make_unique<NormThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(config),\n-      x_slice, scale_slice, y_or_dx_slice, bias_slice, expectation_slice,\n-      norm_factor_slice, dy_slice, dscale_slice, dbias_slice, scratch_slice);\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      std::move(config), x_slice, scale_slice, y_or_dx_slice, bias_slice,\n+      expectation_slice, norm_factor_slice, dy_slice, dscale_slice, dbias_slice,\n+      scratch_slice);\n   AddThunkToThunkSequence(std::move(thunk));\n   return absl::OkStatus();\n }\n@@ -1016,7 +1028,9 @@ absl::Status IrEmitterUnnested::EmitCuDnnThunk(\n     dropout_seed = gpu_config.cudnn_fmha_backend_config().seed();\n   }\n   AddThunkToThunkSequence(std::make_unique<CuDnnThunk>(\n-      fingerprint, Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      fingerprint,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       kernel_arguments.GetArgumentBufferSlices(),\n       kernel_arguments.GetArgumentOutputFlags(), dropout_seed));\n   return absl::OkStatus();\n@@ -1061,7 +1075,8 @@ absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(\n                       instr->backend_config<xla::SortOptions>());\n   const Shape& operand_shape = instr->operand(0)->shape();\n   auto thunk = std::make_unique<CubSortThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       operand_shape.element_type(),\n       instr->operand_count() == 2\n           ? std::optional(instr->operand(1)->shape().element_type())\n@@ -1100,7 +1115,8 @@ absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {\n \n   if (operand_buffer != a_buffer) {\n     thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            instr, ir_emitter_context_->GetNextThunkId()),\n         /*source_buffer=*/operand_buffer,\n         /*destination_buffer=*/a_buffer,\n         /*mem_size=*/ShapeUtil::ByteSizeOf(shape)));\n@@ -1119,16 +1135,19 @@ absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {\n               platform->id()));\n \n   thunks.push_back(std::make_unique<CholeskyThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), options, a_buffer,\n-      workspace_buffer, info_buffer, shape.element_type(), batch_size, n,\n-      std::move(solver_creator)));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      options, a_buffer, workspace_buffer, info_buffer, shape.element_type(),\n+      batch_size, n, std::move(solver_creator)));\n \n   // Elide the sequential thunk if there's no copy.\n   if (thunks.size() == 1) {\n     AddThunkToThunkSequence(std::move(thunks[0]));\n   } else {\n     AddThunkToThunkSequence(std::make_unique<SequentialThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(thunks)));\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            instr, ir_emitter_context_->GetNextThunkId()),\n+        std::move(thunks)));\n   }\n \n   return absl::OkStatus();\n@@ -1286,9 +1305,10 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n       TF_ASSIGN_OR_RETURN(attributes, xla::ffi::BuildAttributesMap(dict));\n     }\n     return CustomCallThunk::Create(\n-        Thunk::ThunkInfo::WithProfileAnnotation(instr), call_target_name,\n-        registration->bundle, std::move(operands), std::move(results),\n-        std::move(attributes),\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            instr, ir_emitter_context_->GetNextThunkId()),\n+        call_target_name, registration->bundle, std::move(operands),\n+        std::move(results), std::move(attributes),\n         called_computations.empty() ? nullptr : called_computations[0]);\n   };\n \n@@ -1299,9 +1319,10 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n             ? backend_config->custom_call_backend_config().opaque()\n             : instr->raw_backend_config_string();\n     return CustomCallThunk::Create(\n-        Thunk::ThunkInfo::WithProfileAnnotation(instr), call_target_name,\n-        std::move(custom_call_target), std::move(operands), std::move(results),\n-        std::move(opaque));\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            instr, ir_emitter_context_->GetNextThunkId()),\n+        call_target_name, std::move(custom_call_target), std::move(operands),\n+        std::move(results), std::move(opaque));\n   };\n \n   TF_ASSIGN_OR_RETURN(std::unique_ptr<CustomCallThunk> custom_call_thunk,\n@@ -1316,13 +1337,14 @@ absl::Status IrEmitterUnnested::EmitFftThunk(const HloFftInstruction* instr) {\n                       GetAllocationSliceForHlo(instr->operand(0)));\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice dest_slice,\n                       GetAllocationSliceForHlo(instr));\n-  AddThunkToThunkSequence(\n-      std::make_unique<FftThunk>(Thunk::ThunkInfo::WithProfileAnnotation(instr),\n-                                 instr->fft_type(), instr->fft_length(),\n-                                 /*input_buffer=*/arg_slice,\n-                                 /*output_buffer=*/dest_slice,\n-                                 /*input_shape=*/instr->operand(0)->shape(),\n-                                 /*output_shape=*/instr->shape()));\n+  AddThunkToThunkSequence(std::make_unique<FftThunk>(\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      instr->fft_type(), instr->fft_length(),\n+      /*input_buffer=*/arg_slice,\n+      /*output_buffer=*/dest_slice,\n+      /*input_shape=*/instr->operand(0)->shape(),\n+      /*output_shape=*/instr->shape()));\n   return absl::OkStatus();\n }\n \n@@ -1371,7 +1393,8 @@ absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(\n   // if they aren't the same buffer.\n   if (b_slice != result_slice) {\n     thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            instr, ir_emitter_context_->GetNextThunkId()),\n         /*source_buffer=*/b_slice,\n         /*destination_buffer=*/result_slice,\n         /*mem_size=*/ShapeUtil::ByteSizeOf(b_shape)));\n@@ -1387,15 +1410,18 @@ absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(\n       backend_config.left_side() ? m * m * elem_size : n * n * elem_size;\n   int64_t b_batch_stride = m * n * elem_size;\n   thunks.push_back(std::make_unique<TriangularSolveThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), backend_config,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      backend_config,\n       /*a_buffer=*/a_slice, /*b_buffer=*/result_slice, temp_slice, elem_ty,\n       batch_size, m, n, a_batch_stride, b_batch_stride));\n \n   // Elide the sequential thunk if there's no copy.\n   if (thunks.size() == 1) {\n     AddThunkToThunkSequence(std::move(thunks[0]));\n   } else {\n-    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(instr);\n+    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+        instr, ir_emitter_context_->GetNextThunkId());\n     // Don't repeat the annotation from inside thunks\n     thunk_info.profile_annotation = {};\n     AddThunkToThunkSequence(\n@@ -1463,7 +1489,8 @@ absl::Status IrEmitterUnnested::EmitTopKCustomCall(\n \n     if (use_raft_select_k) {\n       AddThunkToThunkSequence(std::make_unique<SelectKThunk>(\n-          instr, batch_size, n, k, dtype, kernel_arguments));\n+          instr, batch_size, n, k, dtype, kernel_arguments,\n+          ir_emitter_context_->GetNextThunkId()));\n       return absl::OkStatus();\n     }\n   }\n@@ -1479,7 +1506,8 @@ absl::Status IrEmitterUnnested::EmitTopKCustomCall(\n                                   platform_name(), wavefront_size));\n \n   AddThunkToThunkSequence(std::make_unique<CustomKernelThunk>(\n-      instr, std::move(kernel), kernel_arguments));\n+      instr, std::move(kernel), kernel_arguments,\n+      ir_emitter_context_->GetNextThunkId()));\n   return absl::OkStatus();\n }\n \n@@ -1611,9 +1639,10 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n                           GetDefaultBufferAlignment(), instr));\n \n   AddThunkToThunkSequence(std::make_unique<KernelThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), entry->kernel_name,\n-      kernel_arguments, entry->launch_dimensions, entry->cluster_dim,\n-      entry->shmem_bytes, entry->tma_metadata));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n+      entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata));\n   return absl::OkStatus();\n }\n \n@@ -1643,7 +1672,8 @@ absl::Status IrEmitterUnnested::EmitAsyncComputation(\n   // used as input. We enforce this by inlining a `WaitForStreams`\n   // thunk on the main stream.\n   AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       async_streams.destination_stream_id, async_streams.source_stream_id));\n   AddThunkToThunkSequence(std::move(thunk_sequence));\n   return absl::OkStatus();\n@@ -1685,7 +1715,8 @@ absl::Status IrEmitterUnnested::EmitCopy(const HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice dst_buffer,\n                       GetAllocationSliceForHlo(instr));\n   AddThunkToThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       /*source_buffer=*/src_buffer,\n       /*destination_buffer=*/dst_buffer,\n       /*mem_size=*/src_buffer.size()));\n@@ -1702,7 +1733,8 @@ absl::Status IrEmitterUnnested::EmitAsyncCustomCallStart(\n       ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n       stream_assignment.GetAsyncExecutionStreamIds(async_start));\n   AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       streams.destination_stream_id, streams.source_stream_id));\n   TF_ASSIGN_OR_RETURN(ExecutionStreamId execution_stream_id,\n                       stream_assignment.GetSyncExecutionStreamId(wrapped));\n@@ -1756,7 +1788,9 @@ absl::Status IrEmitterUnnested::EmitWhile(const HloInstruction* instr) {\n \n   TF_ASSIGN_OR_RETURN(\n       auto thunk,\n-      BuildWhileThunk(instr, Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      BuildWhileThunk(instr,\n+                      Thunk::ThunkInfo::WithProfileAnnotation(\n+                          instr, ir_emitter_context_->GetNextThunkId()),\n                       trip_count));\n \n   AddThunkToThunkSequence(std::move(thunk));\n@@ -1811,7 +1845,8 @@ absl::Status IrEmitterUnnested::EmitSort(const HloSortInstruction* sort) {\n       // buffers for key/value sort.\n       VLOG(2) << op_name << \" requires initial D2D copy for operand \" << i;\n       AddThunkToThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(sort),\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              sort, ir_emitter_context_->GetNextThunkId()),\n           /*source_buffer=*/source_address,\n           /*destination_buffer=*/destination_buffer,\n           /*mem_size=*/\n@@ -1979,7 +2014,9 @@ absl::Status IrEmitterUnnested::EmitReplicaOrPartitionId(\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice result_slice,\n                       GetAllocationSliceForHlo(instr, {}));\n   auto thunk = std::make_unique<ThunkType>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), result_slice);\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      result_slice);\n   AddThunkToThunkSequence(std::move(thunk));\n   return absl::OkStatus();\n }\n@@ -2031,7 +2068,8 @@ absl::Status IrEmitterUnnested::EmitCollectivePermute(\n       // For a degenerate collective permute, just generate a copy\n       // thunk.\n       AddThunkToThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n           /*source_buffer=*/source_slice,\n           /*destination_buffer=*/result_slice,\n           /*mem_size=*/ShapeUtil::ByteSizeOf(operand_shape)));\n@@ -2054,16 +2092,18 @@ absl::Status IrEmitterUnnested::EmitCollectivePermute(\n       // NVSHMEM collective permute thunk doesn't perform any memcpy operations\n       // at the moment.\n       auto thunk = std::make_unique<NvshmemCollectivePermuteStartThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(instr), instr, replica_count,\n-          partition_count, buffers,\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n+          instr, replica_count, partition_count, buffers,\n           ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p(),\n           GetStreamKindForP2P(instr));\n       GetCollectivesAsyncEvents().try_emplace(instr, thunk->async_events());\n       AddThunkToThunkSequence(std::move(thunk));\n     } else {\n       auto thunk = std::make_unique<CollectivePermuteStartThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(instr), instr, replica_count,\n-          partition_count, buffers,\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n+          instr, replica_count, partition_count, buffers,\n           ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p(),\n           GetStreamKindForP2P(instr));\n       GetCollectivesAsyncEvents().try_emplace(instr, thunk->async_events());\n@@ -2184,7 +2224,8 @@ absl::Status IrEmitterUnnested::EmitCollectiveThunk(\n   }\n \n   if (should_use_nccl_thunk) {\n-    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(inst);\n+    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+        inst, ir_emitter_context_->GetNextThunkId());\n     // The wrapper name is used when syntactic sugar is turned on.\n     if (ir_emitter_context_->debug_options().xla_syntax_sugar_async_ops()) {\n       thunk_info.profile_annotation = async_start->name();\n@@ -2385,7 +2426,8 @@ absl::Status IrEmitterUnnested::EmitCollectiveGroupStartThunk(\n   }\n   auto thunk = std::make_unique<CollectiveGroupThunk>(\n       instr, Thunk::Kind::kGroupStart, std::move(scoped_thunk_sequence_),\n-      stream_kind.value_or(AsyncStreamKind::kCollective));\n+      stream_kind.value_or(AsyncStreamKind::kCollective),\n+      ir_emitter_context_->GetNextThunkId());\n   emit_group_thunks_ = false;\n \n   GetCollectivesAsyncEvents().insert({instr, thunk->async_events()});\n@@ -2420,7 +2462,9 @@ absl::Status IrEmitterUnnested::EmitCollectiveAsyncDone(\n   }\n \n   AddThunkToThunkSequence(std::make_unique<CollectiveDoneThunk>(\n-      kind, Thunk::ThunkInfo::WithProfileAnnotation(inst),\n+      kind,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          inst, ir_emitter_context_->GetNextThunkId()),\n       async_events_it->second, stream_kind));\n   return absl::OkStatus();\n }\n@@ -2452,11 +2496,14 @@ absl::Status IrEmitterUnnested::EmitNvshmemAsyncDone(\n \n   if (kind == Thunk::Kind::kNvshmemCollectivePermuteDone) {\n     AddThunkToThunkSequence(std::make_unique<NvshmemCollectivePermuteDoneThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(inst), async_events_it->second,\n-        stream_kind));\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            inst, ir_emitter_context_->GetNextThunkId()),\n+        async_events_it->second, stream_kind));\n   } else {\n     AddThunkToThunkSequence(std::make_unique<NvshmemCollectiveDoneThunk>(\n-        kind, Thunk::ThunkInfo::WithProfileAnnotation(inst),\n+        kind,\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            inst, ir_emitter_context_->GetNextThunkId()),\n         async_events_it->second, stream_kind));\n   }\n   return absl::OkStatus();\n@@ -2520,7 +2567,8 @@ absl::Status IrEmitterUnnested::EmitNvshmemThunk(\n   }\n \n   if (should_use_nvshmem_thunk) {\n-    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(inst);\n+    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+        inst, ir_emitter_context_->GetNextThunkId());\n     // The wrapper name is used when syntactic sugar is turned on.\n     if (ir_emitter_context_->debug_options().xla_syntax_sugar_async_ops()) {\n       thunk_info.profile_annotation = async_start->name();\n@@ -2553,7 +2601,8 @@ absl::Status IrEmitterUnnested::EmitDegeneratedCollectiveThunk(\n   for (int64_t i = 0; i < buffers.size(); i++) {\n     const Shape shape = inst->operand(i)->shape();\n     thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(inst),\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            inst, ir_emitter_context_->GetNextThunkId()),\n         /*source_buffer=*/buffers[i].source_buffer,\n         /*destination_buffer=*/buffers[i].destination_buffer,\n         /*mem_size=*/ShapeUtil::ByteSizeOf(shape)));\n@@ -2562,7 +2611,9 @@ absl::Status IrEmitterUnnested::EmitDegeneratedCollectiveThunk(\n     AddThunkToThunkSequence(std::move(thunks[0]));\n   } else {\n     AddThunkToThunkSequence(std::make_unique<SequentialThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(inst), std::move(thunks)));\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            inst, ir_emitter_context_->GetNextThunkId()),\n+        std::move(thunks)));\n   }\n   return absl::OkStatus();\n }\n@@ -2588,7 +2639,9 @@ absl::Status IrEmitterUnnested::EmitInfeed(const HloInfeedInstruction* instr) {\n       }));\n \n   auto thunk = std::make_unique<InfeedThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(shaped_slices));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      std::move(shaped_slices));\n   AddThunkToThunkSequence(std::move(thunk));\n   return absl::OkStatus();\n }\n@@ -2615,7 +2668,9 @@ absl::Status IrEmitterUnnested::EmitOutfeed(\n       }));\n \n   auto thunk = std::make_unique<OutfeedThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(shaped_slices));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      std::move(shaped_slices));\n   AddThunkToThunkSequence(std::move(thunk));\n   return absl::OkStatus();\n }\n@@ -2639,8 +2694,9 @@ IrEmitterUnnested::BuildKernelThunkForNonFusionOp(\n                            launch_dimensions, &b_));\n \n   AddThunkToThunkSequence(std::make_unique<KernelThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), kernel->getName().str(),\n-      kernel_arguments, launch_dimensions,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      kernel->getName().str(), kernel_arguments, launch_dimensions,\n       /*cluster_dim=*/std::nullopt,\n       /*shmem_bytes=*/0,\n       /*tma_metadata=*/se::gpu::TmaMetadata()));\n@@ -2681,11 +2737,11 @@ absl::StatusOr<std::unique_ptr<Thunk>> IrEmitterUnnested::BuildWhileThunk(\n   TF_ASSIGN_OR_RETURN(\n       auto pred, GetAllocationSliceForHlo(condition->root_instruction(), {}));\n \n-  Thunk::ThunkInfo cond_thunk_info =\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr);\n+  Thunk::ThunkInfo cond_thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n   cond_thunk_info.profile_annotation += \"_condition\";\n-  Thunk::ThunkInfo body_thunk_info =\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr);\n+  Thunk::ThunkInfo body_thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n   body_thunk_info.profile_annotation += \"_body\";\n \n   return std::unique_ptr<Thunk>(new WhileThunk(\n@@ -2759,12 +2815,14 @@ absl::Status IrEmitterUnnested::EmitCopyStartThunk(\n   // same, the memcpy operation is synchronous within that stream.\n   if (streams.destination_stream_id != streams.source_stream_id) {\n     AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(copy_start_instr),\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n         streams.destination_stream_id, streams.source_stream_id));\n   }\n   if (is_dst_host_memory) {\n     auto thunk = std::make_unique<DeviceToHostCopyThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(copy_start_instr),\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n         /*source_buffer=*/src_buffer,\n         /*destination_buffer=*/dst_buffer,\n         /*mem_size=*/ShapeUtil::ByteSizeOf(input_shape),\n@@ -2774,7 +2832,8 @@ absl::Status IrEmitterUnnested::EmitCopyStartThunk(\n     AddThunkToThunkSequence(std::move(thunk));\n   } else {\n     auto thunk = std::make_unique<HostToDeviceCopyThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(copy_start_instr),\n+        Thunk::ThunkInfo::WithProfileAnnotation(\n+            copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n         /*source_buffer=*/src_buffer,\n         /*destination_buffer=*/dst_buffer,\n         /*mem_size=*/ShapeUtil::ByteSizeOf(input_shape),\n@@ -2793,7 +2852,8 @@ absl::Status IrEmitterUnnested::EmitCopyDoneThunk(const HloInstruction* instr) {\n \n   auto thunk = std::make_unique<CopyDoneThunk>(\n       Thunk::kCopyDone,\n-      Thunk::ThunkInfo::WithProfileAnnotation(copy_start_instr),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n       /*copy_events=*/copy_events_,\n       /*copy_start_instr=*/copy_start_instr);\n   AddThunkToThunkSequence(std::move(thunk));\n@@ -2822,12 +2882,15 @@ absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {\n         /*destination_memory_space=*/memory_space};\n     if (IsNvshmemCollective(instr)) {\n       thunk = std::make_unique<NvshmemSendThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(instr), instr, replica_count,\n-          partition_count, buffer, nvshmem_buffer_addresses_);\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n+          instr, replica_count, partition_count, buffer,\n+          nvshmem_buffer_addresses_);\n     } else {\n       thunk = std::make_unique<SendThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(instr), instr, replica_count,\n-          partition_count, buffer);\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n+          instr, replica_count, partition_count, buffer);\n     }\n     CollectivesAsyncEvents& collectives_async_events =\n         GetCollectivesAsyncEvents();\n@@ -2868,8 +2931,9 @@ absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {\n   }\n \n   AddThunkToThunkSequence(std::make_unique<HostSendThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), src->shape(), slice,\n-      *instr->channel_id(), send_recv_events_,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      src->shape(), slice, *instr->channel_id(), send_recv_events_,\n       ConvertFrontendAttributes(instr->frontend_attributes()),\n       DeviceConstraint(instr)));\n \n@@ -2892,8 +2956,9 @@ absl::Status IrEmitterUnnested::EmitSendDoneThunk(\n   }\n \n   AddThunkToThunkSequence(std::make_unique<HostSendDoneThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), *instr->channel_id(),\n-      send_recv_events_, DeviceConstraint(instr)));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      *instr->channel_id(), send_recv_events_, DeviceConstraint(instr)));\n   return absl::OkStatus();\n }\n \n@@ -2921,12 +2986,15 @@ absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {\n         /*destination_memory_space=*/memory_space};\n     if (IsNvshmemCollective(instr)) {\n       thunk = std::make_unique<NvshmemRecvThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(instr), instr, replica_count,\n-          partition_count, buffer, nvshmem_buffer_addresses_);\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n+          instr, replica_count, partition_count, buffer,\n+          nvshmem_buffer_addresses_);\n     } else {\n       thunk = std::make_unique<RecvThunk>(\n-          Thunk::ThunkInfo::WithProfileAnnotation(instr), instr, replica_count,\n-          partition_count, buffer);\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n+          instr, replica_count, partition_count, buffer);\n     }\n     CollectivesAsyncEvents& collectives_async_events =\n         GetCollectivesAsyncEvents();\n@@ -2966,7 +3034,8 @@ absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {\n   }\n \n   AddThunkToThunkSequence(std::make_unique<HostRecvThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n       instr->shape().tuple_shapes()[0], slice, *instr->channel_id(),\n       send_recv_events_,\n       ConvertFrontendAttributes(instr->frontend_attributes()),\n@@ -2989,8 +3058,9 @@ absl::Status IrEmitterUnnested::EmitRecvDoneThunk(\n         \"instruction\");\n   }\n   AddThunkToThunkSequence(std::make_unique<HostRecvDoneThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(instr), *instr->channel_id(),\n-      send_recv_events_, DeviceConstraint(instr)));\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          instr, ir_emitter_context_->GetNextThunkId()),\n+      *instr->channel_id(), send_recv_events_, DeviceConstraint(instr)));\n \n   return absl::OkStatus();\n }\n@@ -3084,7 +3154,9 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n                 GetInstructionToHostExecuteAsyncEvents().at(custom_call);\n \n             AddThunkToThunkSequence(std::make_unique<HostExecuteDoneThunk>(\n-                Thunk::ThunkInfo::WithProfileAnnotation(instr), async_events));\n+                Thunk::ThunkInfo::WithProfileAnnotation(\n+                    instr, ir_emitter_context_->GetNextThunkId()),\n+                async_events));\n             return absl::OkStatus();\n           }\n           // Wait until the concurrent stream has finished.\n@@ -3095,7 +3167,8 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n               ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n               stream_assignment.GetAsyncExecutionStreamIds(async_done));\n           AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n-              Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+              Thunk::ThunkInfo::WithProfileAnnotation(\n+                  instr, ir_emitter_context_->GetNextThunkId()),\n               streams.source_stream_id, streams.destination_stream_id));\n           return absl::OkStatus();\n         }\n@@ -3152,7 +3225,8 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n               ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n               stream_assignment.GetAsyncExecutionStreamIds(async_start));\n           AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n-              Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+              Thunk::ThunkInfo::WithProfileAnnotation(\n+                  instr, ir_emitter_context_->GetNextThunkId()),\n               streams.destination_stream_id, streams.source_stream_id));\n           return EmitFusion(Cast<HloFusionInstruction>(wrapped));\n         }\n@@ -3200,8 +3274,10 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n             }\n \n             auto thunk = std::make_unique<HostExecuteStartThunk>(\n-                Thunk::ThunkInfo::WithProfileAnnotation(instr), *hlo_module,\n-                std::move(operand_slices), std::move(result_slices));\n+                Thunk::ThunkInfo::WithProfileAnnotation(\n+                    instr, ir_emitter_context_->GetNextThunkId()),\n+                *hlo_module, std::move(operand_slices),\n+                std::move(result_slices));\n \n             auto async_events = thunk->async_events();\n "
        },
        {
            "sha": "f22e4b2f06698ca114c1c238de167c76e695e4d8",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/30f4aa58e821a50aac8846ccaf7f94dfdc66c456/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h?ref=30f4aa58e821a50aac8846ccaf7f94dfdc66c456",
            "patch": "@@ -28,10 +28,10 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n #include \"llvm/IR/Type.h\"\n #include \"llvm/IR/Value.h\"\n #include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/host_send_recv_thunk.h\"\n #include \"xla/backends/gpu/runtime/nvshmem_collective_thunk.h\""
        }
    ],
    "stats": {
        "total": 530,
        "additions": 395,
        "deletions": 135
    }
}