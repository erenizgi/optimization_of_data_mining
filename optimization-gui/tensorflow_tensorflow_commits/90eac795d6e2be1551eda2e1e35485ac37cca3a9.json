{
    "author": "mooskagh",
    "message": "Reverts ef67b29200668cad08743921c41b78bcff12f15a\n\nPiperOrigin-RevId: 810811463",
    "sha": "90eac795d6e2be1551eda2e1e35485ac37cca3a9",
    "files": [
        {
            "sha": "5bdf893d36c6157f235762f1dde364a8173e0f0b",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=90eac795d6e2be1551eda2e1e35485ac37cca3a9",
            "patch": "@@ -147,27 +147,8 @@ absl::Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization(\n   pipeline.AddPass<HloPassFix<GpuAlgebraicSimplifier>>(algsimp_options,\n                                                        gpu_version);\n \n-  // tf2xla bridge, DepthwiseConvolutionConverter, ConvRewriter, and\n-  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover\n-  // to a fixed point.  Include algsimp because ReshapeMover relies on it.\n-  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n-          \"reshape_mover_after_conv_canonicalization\")] {\n-    ReshapeMoverOptions reshape_mover_options;\n-    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n-    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n-    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n-  }();\n-\n-  // The reshapes and transposes can possibly be eliminated using\n-  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.\n-  // ConvertMover wants to move some converts down the graph, but ReshapeMover\n-  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed\n-  // point.\n-  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n-          \"simplify_after_conv_canonicalization\")] {\n-    pipeline.AddPass<ConvertMover>();\n-    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n-  }();\n+  pipeline.AddPass<ConvertMover>();\n+  pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n \n   // ConvRewriter, ConvPaddingLegalization and\n   // CudnnConvPadForTensorCores may add instructions which can be simplified"
        },
        {
            "sha": "390569bea7fc1fb7c89f92796b18a480751ff101",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=90eac795d6e2be1551eda2e1e35485ac37cca3a9",
            "patch": "@@ -921,23 +921,12 @@ absl::Status RunOptimizationPasses(\n     pipeline.AddPass<WhileLoopSimplifier>();\n     pipeline.AddPass<SliceSinker>();\n \n-    ReshapeMoverOptions reshape_mover_options;\n-    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n-    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n     pipeline.AddPass<HloConstantFolding>();\n     pipeline.AddPass<ConditionalSimplifier>();\n     pipeline.AddPass<RealImagExpander>();\n     pipeline.AddPass<TransposeFolding>(CanFoldTransposeOperandIntoDot);\n     pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/false);\n     pipeline.AddPass<HloDCE>();\n-  }();\n-\n-  // ConvertMover and ReshapeMover fight with each other: ConvertMover wants\n-  // to move some converts down the graph, but ReshapeMover wants to move them\n-  // up the graph.  As a compromise, let ReshapeMover run to a fixed point,\n-  // and then run ConvertMover + algsimp to a fixed point.\n-  [&, &pipeline =\n-          pipeline.AddPass<HloPassFix<HloPassPipeline>>(\"simplification-2\")] {\n     pipeline.AddPass<ConvertMover>();\n     pipeline.AddPass<GpuAlgebraicSimplifier>(layout_insensitive_algsimp_opts,\n                                              gpu_version);"
        },
        {
            "sha": "cae3c97108cffc4a405f2a0109df5eb56f4e9899",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=90eac795d6e2be1551eda2e1e35485ac37cca3a9",
            "patch": "@@ -63,7 +63,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[] constant({...})\\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n+  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0)\\n  tmp_2 = bf16[] constant({...})\\n  tmp_3 = bf16[4,32,1024,1024]{3,2,1,0} broadcast(bf16[] tmp_2), dimensions={}\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} multiply(bf16[4,32,1024,1024]{3,2,1,0} tmp_1, bf16[4,32,1024,1024]{3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n   result {\n     gemm {\n       algorithm: -1"
        },
        {
            "sha": "ce3d40166eac4be502b64e39d635030dee0d0341",
            "filename": "third_party/xla/xla/service/gpu/gpu_spmd_pipeline.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc?ref=90eac795d6e2be1551eda2e1e35485ac37cca3a9",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_folding.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_splitter.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n-#include \"xla/hlo/transforms/simplifiers/reshape_mover.h\"\n #include \"xla/hlo/transforms/simplifiers/sort_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/service/call_graph.h\"\n@@ -79,9 +78,6 @@ void AddSPMDPasses(\n   spmd_simplify.AddPass<WhileLoopConstantSinking>();\n   spmd_simplify.AddPass<WhileLoopSimplifier>();\n \n-  ReshapeMoverOptions reshape_mover_options;\n-  reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n-  spmd_simplify.AddPass<ReshapeMover>(reshape_mover_options);\n   // Run AlgebraicSimplifier directly before HloConstantFolding, because we\n   // need to simplify DynamicSlice(Broadcast) away. Constant folding of\n   // DynamicSlice can be quite costly, as the whole operand will be evaluated."
        },
        {
            "sha": "7e058e7cda2365c3fed8af4988884369f8aa7bbe",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=90eac795d6e2be1551eda2e1e35485ac37cca3a9",
            "patch": "@@ -241,27 +241,8 @@ absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n     pipeline.AddPass<CudnnSimplifyPadding>();\n   }\n \n-  // tf2xla bridge, DepthwiseConvolutionConverter, ConvRewriter, and\n-  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover\n-  // to a fixed point.  Include algsimp because ReshapeMover relies on it.\n-  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n-          \"reshape_mover_after_conv_canonicalization\")] {\n-    ReshapeMoverOptions reshape_mover_options;\n-    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n-    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n-    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n-  }();\n-\n-  // The reshapes and transposes can possibly be eliminated using\n-  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.\n-  // ConvertMover wants to move some converts down the graph, but ReshapeMover\n-  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed\n-  // point.\n-  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n-          \"simplify_after_conv_canonicalization\")] {\n-    pipeline.AddPass<ConvertMover>();\n-    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n-  }();\n+  pipeline.AddPass<ConvertMover>();\n+  pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n \n   // ConvRewriter, ConvPaddingLegalization and\n   // CudnnConvPadForTensorCores may add instructions which can be simplified"
        },
        {
            "sha": "4b2f33858492760a3b4433a9746ef6920cfaac9c",
            "filename": "third_party/xla/xla/tests/multioutput_fusion_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/90eac795d6e2be1551eda2e1e35485ac37cca3a9/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc?ref=90eac795d6e2be1551eda2e1e35485ac37cca3a9",
            "patch": "@@ -47,9 +47,12 @@ class MultiOutputFusionTest : public HloTestBase {\n   // Layout assignment assumes that there are no fusions in the input graph.\n   // Since the purpose of this test is to send pre-fused graphs to XLA, we have\n   // to do layout assignment ourselves.\n+  // Dot strength reduction replaces dot with a multiply and it does require\n+  // layout assignment to ensure compatible physical layouts.\n   DebugOptions GetDebugOptionsForTest() const override {\n     auto opts = HloTestBase::GetDebugOptionsForTest();\n     opts.add_xla_disable_hlo_passes(\"layout-assignment\");\n+    opts.add_xla_disable_hlo_passes(\"dot-strength-reduction\");\n     return opts;\n   }\n "
        }
    ],
    "stats": {
        "total": 66,
        "additions": 8,
        "deletions": 58
    }
}