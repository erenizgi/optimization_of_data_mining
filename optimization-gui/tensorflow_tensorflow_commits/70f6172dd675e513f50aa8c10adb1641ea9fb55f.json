{
    "author": "chsigg",
    "message": "Remove legacy Triton pointer ops from int4 passes.\n\nThis change removes conversion patterns for `tt.make_tensor_ptr`, `tt.addptr`, `tt.advance`, and `tt.load` from the int4-to-packed-int4 rewrite pass. These operations are no longer generated by the generic Triton emitter. Corresponding tests using these ops are also removed.\n\nPiperOrigin-RevId: 836585357",
    "sha": "70f6172dd675e513f50aa8c10adb1641ea9fb55f",
    "files": [
        {
            "sha": "f36de1d59943c5bb8d9db7bf98ca09b66273319b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/int4_passes.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 225,
            "changes": 287,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70f6172dd675e513f50aa8c10adb1641ea9fb55f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70f6172dd675e513f50aa8c10adb1641ea9fb55f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc?ref=70f6172dd675e513f50aa8c10adb1641ea9fb55f",
            "patch": "@@ -171,14 +171,14 @@ class I4ToI8Converter : public TypeConverter {\n };\n \n // Divides a value by an integer constant.\n-Value div(ConversionPatternRewriter &r, Value value, int64_t constant) {\n+Value div(ConversionPatternRewriter& r, Value value, int64_t constant) {\n   auto const_attr = r.getIntegerAttr(value.getType(), constant);\n   auto const_op = r.template create<ma::ConstantOp>(value.getLoc(), const_attr);\n   return r.template create<ma::DivSIOp>(value.getLoc(), value, const_op);\n }\n \n // Divides a value by an integer constant.\n-Value ceilDiv(ConversionPatternRewriter &r, Value value, int64_t constant) {\n+Value ceilDiv(ConversionPatternRewriter& r, Value value, int64_t constant) {\n   auto const_attr = r.getIntegerAttr(value.getType(), constant);\n   auto const_op = r.template create<ma::ConstantOp>(value.getLoc(), const_attr);\n   return r.template create<ma::CeilDivSIOp>(value.getLoc(), value, const_op);\n@@ -201,14 +201,14 @@ class TritonXlaExtractOpConversionPattern\n  public:\n   using OpConversionPattern<mtx::ExtractOp>::OpConversionPattern;\n \n-  TritonXlaExtractOpConversionPattern(const I4ToI8Converter &converter,\n-                                      MLIRContext *context)\n+  TritonXlaExtractOpConversionPattern(const I4ToI8Converter& converter,\n+                                      MLIRContext* context)\n       : OpConversionPattern<mtx::ExtractOp>(converter, context),\n         converter_(converter) {}\n \n   LogicalResult matchAndRewrite(\n       mtx::ExtractOp op, OpConversionPattern<mtx::ExtractOp>::OpAdaptor adaptor,\n-      ConversionPatternRewriter &r) const override {\n+      ConversionPatternRewriter& r) const override {\n     // Convert the tensor type using the TypeConverter\n     auto new_result_type = mlir::cast<mlir::RankedTensorType>(\n         getTypeConverter()->convertType(op.getType()));\n@@ -257,149 +257,15 @@ class TritonXlaExtractOpConversionPattern\n   }\n \n  private:\n-  const I4ToI8Converter &converter_;\n-};\n-\n-class MakeTensorPtrOpConversionPattern\n-    : public OpConversionPattern<MakeTensorPtrOp> {\n- public:\n-  using OpConversionPattern<MakeTensorPtrOp>::OpConversionPattern;\n-\n-  MakeTensorPtrOpConversionPattern(const I4ToI8Converter &converter,\n-                                   MLIRContext *context)\n-      : OpConversionPattern<MakeTensorPtrOp>(converter, context),\n-        converter_(converter) {}\n-\n-  LogicalResult matchAndRewrite(\n-      MakeTensorPtrOp op,\n-      OpConversionPattern<MakeTensorPtrOp>::OpAdaptor adaptor,\n-      ConversionPatternRewriter &r) const override {\n-    // Convert the tensor type using the TypeConverter\n-    auto new_type = getTypeConverter()->convertType(op.getType());\n-    if (op.getType() == new_type) {\n-      return r.notifyMatchFailure(op, \"no conversion needed\");\n-    }\n-\n-    SmallVector<Value, 2> shape = adaptor.getShape();\n-    int packed_dimension = converter_.packed_dimension();\n-    // The shape of the i8 tensor is half of the i4 tensor but at least 1.\n-    shape[packed_dimension] = ceilDiv(r, shape[packed_dimension], 2);\n-\n-    // The stride of the i8 tensor is half of the i4 tensor but at least 1.\n-    SmallVector<Value, 2> new_strides = adaptor.getStrides();\n-    for (int i = 0; i < new_strides.size(); ++i) {\n-      new_strides[i] = ceilDiv(r, new_strides[i], 2);\n-    }\n-\n-    r.replaceOpWithNewOp<MakeTensorPtrOp>(\n-        op, new_type, adaptor.getBase(), shape, new_strides,\n-        adaptor.getOffsets(), adaptor.getOrderAttr());\n-\n-    return success();\n-  }\n-\n- private:\n-  const I4ToI8Converter &converter_;\n-};\n-\n-class AddPtrOpConversionPattern : public OpConversionPattern<AddPtrOp> {\n- public:\n-  using OpConversionPattern<AddPtrOp>::OpConversionPattern;\n-\n-  LogicalResult matchAndRewrite(\n-      AddPtrOp op, OpConversionPattern<AddPtrOp>::OpAdaptor adaptor,\n-      ConversionPatternRewriter &r) const override {\n-    // Convert the tensor type using the TypeConverter\n-    auto new_type = getTypeConverter()->convertType(op.getType());\n-    if (op.getType() == new_type) {\n-      return r.notifyMatchFailure(op, \"no conversion needed\");\n-    }\n-\n-    // The increment for the next stripe of tiles along K dimension should be\n-    // twice smaller.\n-    auto ptr = adaptor.getOperands()[0];\n-    auto offset = adaptor.getOperands()[1];\n-    auto new_offset = div(r, offset, 2);\n-\n-    r.replaceOpWithNewOp<AddPtrOp>(op, new_type, ptr, new_offset);\n-\n-    return success();\n-  }\n-};\n-\n-class AdvanceOpConversionPattern : public OpConversionPattern<AdvanceOp> {\n- public:\n-  using OpConversionPattern<AdvanceOp>::OpConversionPattern;\n-\n-  AdvanceOpConversionPattern(const I4ToI8Converter &converter,\n-                             MLIRContext *context)\n-      : OpConversionPattern<AdvanceOp>(converter, context),\n-        converter_(converter) {}\n-  LogicalResult matchAndRewrite(\n-      AdvanceOp op, typename OpConversionPattern<AdvanceOp>::OpAdaptor adaptor,\n-      ConversionPatternRewriter &r) const override {\n-    VLOG(5) << \"AvanceOpConversionPattern: matching\\n\"\n-            << DumpToString(op.getOperation());\n-    // Convert the tensor type using the TypeConverter\n-    auto new_type = converter_.convertType(op.getType());\n-    if (op.getType() == new_type) {\n-      VLOG(5) << \"AdvanceOpConversionPattern: no conversion needed for \"\n-              << DumpToString(op.getType());\n-      return r.notifyMatchFailure(op, \"no conversion needed\");\n-    }\n-    SmallVector<Value, 2> offsets = adaptor.getOffsets();\n-    int packed_dimension = converter_.packed_dimension();\n-    offsets[packed_dimension] = div(r, offsets[packed_dimension], 2);\n-    auto new_op = r.replaceOpWithNewOp<AdvanceOp>(op, new_type,\n-                                                  adaptor.getPtr(), offsets);\n-    VLOG(5) << \"AdvanceOpConversionPattern: replaced \"\n-            << DumpToString(op.getOperation()) << \" with \"\n-            << DumpToString(new_op.getOperation());\n-    return success();\n-  }\n-\n- private:\n-  const I4ToI8Converter &converter_;\n-};\n-\n-// The generic converter for the ops that requires only type conversion.\n-template <typename OpType>\n-class OpTypeConversionPattern : public OpConversionPattern<OpType> {\n- public:\n-  using OpConversionPattern<OpType>::OpConversionPattern;\n-\n-  OpTypeConversionPattern(const I4ToI8Converter &converter,\n-                          MLIRContext *context)\n-      : OpConversionPattern<OpType>(converter, context),\n-        converter_(converter) {}\n-  LogicalResult matchAndRewrite(\n-      OpType op, typename OpConversionPattern<OpType>::OpAdaptor adaptor,\n-      ConversionPatternRewriter &r) const override {\n-    VLOG(5) << \"OpTypeConversionPattern: matching\\n\"\n-            << DumpToString(static_cast<Operation *>(op.getOperation()));\n-    // Convert the tensor type using the TypeConverter\n-    auto new_type = converter_.convertType(op.getType());\n-    if (op.getType() == new_type) {\n-      VLOG(5) << \"OpTypeConversionPattern: no conversion needed for \"\n-              << DumpToString(op.getType());\n-      return r.notifyMatchFailure(op, \"no conversion needed\");\n-    }\n-\n-    r.replaceOpWithNewOp<OpType>(op, new_type, adaptor.getOperands(),\n-                                 op->getAttrs());\n-    return success();\n-  }\n-\n- private:\n-  const I4ToI8Converter &converter_;\n+  const I4ToI8Converter& converter_;\n };\n \n // The pattern converts the ExtSIOp that converts i4 tensor to i8 tensor to an\n // unpack sequence that uses ShLIOp, ShRSIOp, JoinOp, TransOp and ReshapeOp to\n // do the same thing.\n class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n  public:\n-  ExtSIInt4ToInt8Pattern(const I4ToI8Converter &converter, MLIRContext *context,\n+  ExtSIInt4ToInt8Pattern(const I4ToI8Converter& converter, MLIRContext* context,\n                          bool bf16x2_enabled)\n       : OpConversionPattern<ma::ExtSIOp>(converter, context),\n         converter_(converter),\n@@ -410,7 +276,7 @@ class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n   LogicalResult RewriteI4ToI8(\n       ma::ExtSIOp ext_si_op,\n       OpConversionPattern<ma::ExtSIOp>::OpAdaptor adaptor,\n-      ConversionPatternRewriter &r) const {\n+      ConversionPatternRewriter& r) const {\n     auto loc = ext_si_op.getLoc();\n     auto input_type = cast<RankedTensorType>(ext_si_op.getIn().getType());\n     auto packed_type = converter_.convertType(input_type);\n@@ -443,7 +309,7 @@ class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n   LogicalResult RewriteI4ToBf16(\n       ma::ExtSIOp ext_si_op, ma::SIToFPOp si_to_fp_op,\n       OpConversionPattern<ma::ExtSIOp>::OpAdaptor adaptor,\n-      ConversionPatternRewriter &r) const {\n+      ConversionPatternRewriter& r) const {\n     VLOG(5) << \"RewriteI4ToBf16: Using inline asm i4 to bf16 conversion\";\n     auto loc = ext_si_op.getLoc();\n     auto input_type = cast<RankedTensorType>(ext_si_op.getIn().getType());\n@@ -500,7 +366,7 @@ class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n   }\n \n   LogicalResult matchAndRewrite(ma::ExtSIOp ext_si_op, OpAdaptor adaptor,\n-                                ConversionPatternRewriter &r) const override {\n+                                ConversionPatternRewriter& r) const override {\n     VLOG(5) << \"ExtSIInt4ToInt8Pattern: matching\\n\"\n             << DumpToString(ext_si_op.getOperation());\n     auto input_type = dyn_cast<RankedTensorType>(ext_si_op.getIn().getType());\n@@ -528,7 +394,7 @@ class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n           auto tensor_type = dyn_cast<RankedTensorType>(result_type);\n           if (!tensor_type || !tensor_type.getElementType().isBF16()) {\n             VLOG(5) << \"ExtSIInt4ToInt8Pattern: no conversion needed for \"\n-                    << DumpToString(static_cast<Operation *>(si_to_fp_op));\n+                    << DumpToString(static_cast<Operation*>(si_to_fp_op));\n             continue;\n           }\n           if (RewriteI4ToBf16(ext_si_op, si_to_fp_op, adaptor, r).failed()) {\n@@ -541,14 +407,14 @@ class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n   }\n \n  private:\n-  const I4ToI8Converter &converter_;\n+  const I4ToI8Converter& converter_;\n   const bool bf16x2_enabled_;\n };\n \n // Traverses the operands of the op passing though the forOp and returns the\n // list of ops that belong to the same argument.\n-std::vector<Operation *> TraverseUpwards(Operation *op) {\n-  std::vector<Operation *> result;\n+std::vector<Operation*> TraverseUpwards(Operation* op) {\n+  std::vector<Operation*> result;\n   while (op != nullptr) {\n     VLOG(5) << \"op: \\n\" << DumpToString(op);\n     result.push_back(op);\n@@ -578,12 +444,12 @@ std::vector<Operation *> TraverseUpwards(Operation *op) {\n }\n \n // Finds all the ExtSIOp that require the type conversion.\n-std::vector<Operation *> FindInt4ExtSIOp(const ModuleOp &module) {\n+std::vector<Operation*> FindInt4ExtSIOp(const ModuleOp& module) {\n   // It does not matter which packed dimension idx we use here, because use the\n   // converter to detect that the conversion is needed.\n   I4ToI8Converter converter(/*packed_dimension=*/0);\n-  std::vector<Operation *> result;\n-  module->walk([&](Operation *op) {\n+  std::vector<Operation*> result;\n+  module->walk([&](Operation* op) {\n     if (auto extSI = dyn_cast<ma::ExtSIOp>(op)) {\n       VLOG(5) << \"found ExtSI: \" << DumpToString(op);\n       auto input_type = extSI.getIn().getType();\n@@ -596,83 +462,62 @@ std::vector<Operation *> FindInt4ExtSIOp(const ModuleOp &module) {\n   return result;\n }\n \n-// Finds the packed dimension from MakeTensorPtrOp or mtx::ExtractOp.\n+// Finds the packed dimension from mtx::ExtractOp.\n // The packed dimension is the most minor dimension that has a unit stride and a\n // shape that is > 1.\n-// TODO(b/393299275): MakeTensorPtrOp will not be emitted by the generic Triton\n-// emitter. Remove this once the legacy Triton emitter is deprecated.\n-absl::StatusOr<int> GetPackedDimension(MLIRContext *ctx,\n-                                       const std::vector<Operation *> &ops) {\n-  for (auto *op : ops) {\n-    auto make_tensor_ptr_op = dyn_cast<MakeTensorPtrOp>(op);\n+absl::StatusOr<int> GetPackedDimension(MLIRContext* ctx,\n+                                       const std::vector<Operation*>& ops) {\n+  for (auto* op : ops) {\n     auto extract_op = dyn_cast<mtx::ExtractOp>(op);\n-    if (!make_tensor_ptr_op && !extract_op) {\n+    if (!extract_op) {\n       continue;\n     }\n \n-    if (make_tensor_ptr_op) {\n-      // The order attribute is ignored in Triton, check for default order here.\n-      CHECK(\n-          absl::c_is_sorted(make_tensor_ptr_op.getOrder(), std::greater<int>()))\n-          << \"Not default order: \" << DumpToString(op);\n-      auto shape = make_tensor_ptr_op.getShape();\n-      auto strides = make_tensor_ptr_op.getStrides();\n-      for (auto dim : make_tensor_ptr_op.getOrder()) {\n-        if (GetConstValue(strides[dim]).value_or(1) == 1 &&\n-            GetConstValue(shape[dim]).value_or(0) != 1) {\n-          return dim;\n-        }\n-      }\n-    }\n+    // Make sure the packed dimension is not dynamic and has a stride of 1.\n+    auto offsets = extract_op.getStaticOffsets();\n+    auto sizes = extract_op.getStaticSizes();\n+    auto strides = extract_op.getStaticStrides();\n \n-    if (extract_op) {\n-      // Make sure the packed dimension is not dynamic and has a stride of 1.\n-      auto offsets = extract_op.getStaticOffsets();\n-      auto sizes = extract_op.getStaticSizes();\n-      auto strides = extract_op.getStaticStrides();\n+    if (ShapedType::isDynamicShape(strides) ||\n+        ShapedType::isDynamicShape(sizes)) {\n+      return absl::InvalidArgumentError(\n+          \"dynamic shapes, tile strides, and tile sizes not supported\");\n+    }\n \n-      if (ShapedType::isDynamicShape(strides) ||\n-          ShapedType::isDynamicShape(sizes)) {\n+    for (auto dim : extract_op.getSrcLayout()) {\n+      if (extract_op.getSrcShape()[dim] == 1) {\n+        continue;\n+      }\n+      if (strides[dim] != 1) {\n         return absl::InvalidArgumentError(\n-            \"dynamic shapes, tile strides, and tile sizes not supported\");\n+            \"Minor-most non-unit dimension has non-unit stride.\");\n       }\n-\n-      for (auto dim : extract_op.getSrcLayout()) {\n-        if (extract_op.getSrcShape()[dim] == 1) {\n-          continue;\n-        }\n-        if (strides[dim] != 1) {\n-          return absl::InvalidArgumentError(\n-              \"Minor-most non-unit dimension has non-unit stride.\");\n-        }\n-        if (sizes[dim] % 2 != 0) {\n-          return absl::InvalidArgumentError(\n-              \"Minor-most non-unit dimension has odd size.\");\n-        }\n-        if (!ShapedType::isDynamic(offsets[dim]) && offsets[dim] % 2 != 0) {\n-          return absl::InvalidArgumentError(\n-              \"Minor-most non-unit dimension has odd offset.\");\n-        }\n-        std::optional<llvm::SmallDenseSet<unsigned>> optional_mask =\n-            computeRankReductionMask(sizes, extract_op.getType().getShape());\n-        if (!optional_mask) {\n-          return absl::InvalidArgumentError(\"Unsupported rank reduction.\");\n-        }\n-        auto mask = llvm::to_vector(*optional_mask);\n-        // Convert the packed dimension to the rank-reduced dst type.\n-        return dim - (absl::c_upper_bound(mask, dim) - mask.begin());\n+      if (sizes[dim] % 2 != 0) {\n+        return absl::InvalidArgumentError(\n+            \"Minor-most non-unit dimension has odd size.\");\n       }\n-\n-      return absl::InvalidArgumentError(\"Failed to find a packed dimension.\");\n+      if (!ShapedType::isDynamic(offsets[dim]) && offsets[dim] % 2 != 0) {\n+        return absl::InvalidArgumentError(\n+            \"Minor-most non-unit dimension has odd offset.\");\n+      }\n+      std::optional<llvm::SmallDenseSet<unsigned>> optional_mask =\n+          computeRankReductionMask(sizes, extract_op.getType().getShape());\n+      if (!optional_mask) {\n+        return absl::InvalidArgumentError(\"Unsupported rank reduction.\");\n+      }\n+      auto mask = llvm::to_vector(*optional_mask);\n+      // Convert the packed dimension to the rank-reduced dst type.\n+      return dim - (absl::c_upper_bound(mask, dim) - mask.begin());\n     }\n+\n+    return absl::InvalidArgumentError(\"Failed to find a packed dimension.\");\n   }\n-  std::string not_found_message =\n-      \"No MakeTensorPtrOp or mlir::triton::xla::ExtractOp found\";\n+  std::string not_found_message = \"No mlir::triton::xla::ExtractOp found\";\n   LOG(ERROR) << not_found_message;\n   return absl::InvalidArgumentError(not_found_message);\n }\n \n-LogicalResult SitofpInt4ToInt8Rewrite(ma::SIToFPOp op, PatternRewriter &r) {\n+LogicalResult SitofpInt4ToInt8Rewrite(ma::SIToFPOp op, PatternRewriter& r) {\n   if (!getElementTypeOrSelf(op.getIn().getType()).isInteger(4)) {\n     return r.notifyMatchFailure(op, \"not an i4 argument\");\n   }\n@@ -686,7 +531,7 @@ LogicalResult SitofpInt4ToInt8Rewrite(ma::SIToFPOp op, PatternRewriter &r) {\n }\n \n LogicalResult TruncfSitofpToSitofpRewrite(ma::TruncFOp trunc_op,\n-                                          PatternRewriter &rewriter) {\n+                                          PatternRewriter& rewriter) {\n   auto sitofp_op = trunc_op.getIn().getDefiningOp<ma::SIToFPOp>();\n   if (!sitofp_op) {\n     return rewriter.notifyMatchFailure(trunc_op, \"not preceded by sitofp\");\n@@ -712,7 +557,7 @@ bool opInputElementTypeIs(mlir::Value value, Type element_type) {\n \n // The pattern converts the Sitofp(i4): Fp32 to ExtFOp(Sitofp(i4): bf16): Fp32.\n LogicalResult SitofpToExtFpSitofpRewrite(ma::SIToFPOp sitofp_op,\n-                                         PatternRewriter &rewriter) {\n+                                         PatternRewriter& rewriter) {\n   auto input = sitofp_op.getIn();\n   if (!opInputElementTypeIs<ma::ExtSIOp>(input, rewriter.getIntegerType(4))) {\n     return rewriter.notifyMatchFailure(\n@@ -754,9 +599,9 @@ class LoadInt4RewritePass\n   // and converts it to the twice bigger i8 tensor where every i4 element uses\n   // i8 space. At the end the module accepts the tt.ptr<i8> to the packed i4\n   // tensor, and unpacks it to the i8 tensor for further processing. It gets the\n-  // packed dimension from MakeTensorPtrOp or mtx::ExtractOp.\n+  // packed dimension from mtx::ExtractOp.\n   void runOnOperation() override {\n-    auto *ctx = &getContext();\n+    auto* ctx = &getContext();\n     auto module = getOperation();\n \n     RewritePatternSet normalize_patterns(ctx);\n@@ -772,7 +617,7 @@ class LoadInt4RewritePass\n     int packed_dimension = 0;\n     // TODO(b/383255324): Support the case when both sides of the dot are packed\n     // differently.\n-    for (auto *op : ext_ops) {\n+    for (auto* op : ext_ops) {\n       VLOG(5) << \"ext_op: \" << DumpToString(op);\n       auto ops = TraverseUpwards(op);\n       auto packed_dimension_result = GetPackedDimension(ctx, ops);\n@@ -786,7 +631,7 @@ class LoadInt4RewritePass\n \n     ConversionTarget target(*ctx);\n     I4ToI8Converter converter(packed_dimension);\n-    target.markUnknownOpDynamicallyLegal([&](Operation *op) {\n+    target.markUnknownOpDynamicallyLegal([&](Operation* op) {\n       if (auto func_op = dyn_cast<mlir::FunctionOpInterface>(op)) {\n         VLOG(5) << \"check funcOp: \" << DumpToString(func_op);\n         if (func_op.getFunctionType() !=\n@@ -803,14 +648,6 @@ class LoadInt4RewritePass\n     scf::populateSCFStructuralTypeConversions(converter, patterns);\n     patterns.add<ExtSIInt4ToInt8Pattern>(converter, ctx, enable_bf16x2_);\n \n-    // TODO(b/393299275): LoadOp, AdvanceOp, AddPtrOp, and MakeTensorPtrOp will\n-    // not be emitted by the generic Triton emitter. Remove these once the\n-    // legacy Triton emitter is deprecated.\n-    patterns.add<OpTypeConversionPattern<LoadOp>>(converter, ctx);\n-    patterns.add<AdvanceOpConversionPattern>(converter, ctx);\n-    patterns.add<AddPtrOpConversionPattern>(converter, ctx);\n-    patterns.add<MakeTensorPtrOpConversionPattern>(converter, ctx);\n-\n     patterns.add<TritonXlaExtractOpConversionPattern>(converter, ctx);\n \n     // TODO(b/393299275): Remove mt::FuncOp once the legacy Triton emitter is"
        },
        {
            "sha": "f38b7473807bad2ade73f109ac46e885c3c31fe6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/int4_packed_dim.mlir",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70f6172dd675e513f50aa8c10adb1641ea9fb55f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70f6172dd675e513f50aa8c10adb1641ea9fb55f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir?ref=70f6172dd675e513f50aa8c10adb1641ea9fb55f",
            "patch": "@@ -1,118 +1,5 @@\n // RUN: xla-opt --split-input-file --int4-to-packed-int4-rewrite --canonicalize %s | FileCheck %s\n \n-// CHECK-LABEL: @major_1d\n-tt.func @major_1d(%arg0: !tt.ptr<i4>) -> (tensor<8x1xi8>) {\n-  %c0 = arith.constant 0 : i32\n-  %c1 = arith.constant 1 : i64\n-  %c8 = arith.constant 8 : i64\n-\n-  %0 = tt.make_tensor_ptr %arg0, [%c8, %c1], [%c1, %c1], [%c0, %c0] {order = array<i32: 1, 0>} : <tensor<8x1xi4>>\n-  // CHECK: %[[LOAD:.*]] = tt.load %{{.*}} : !tt.ptr<tensor<4x1xi8>>\n-  %1 = tt.load %0 : !tt.ptr<tensor<8x1xi4>>\n-  // CHECK-DAG: %[[SHLI:.*]] = arith.shli %[[LOAD]]\n-  // CHECK-DAG: %[[LO:.*]] = arith.shrsi %[[SHLI]]\n-  // CHECK-DAG: %[[HI:.*]] = arith.shrsi %[[LOAD]]\n-  // CHECK: %[[JOIN:.*]] = tt.join %[[LO]], %[[HI]]\n-  // CHECK: %[[TRANS:.*]] = tt.trans %[[JOIN]] {order = array<i32: 0, 2, 1>}\n-  // CHECK: %[[RESHAPE:.*]] = tt.reshape %[[TRANS]]\n-  // CHECK: tt.return %[[RESHAPE]] : tensor<8x1xi8>\n-  %2 = arith.extsi %1 : tensor<8x1xi4> to tensor<8x1xi8>\n-  tt.return %2 : tensor<8x1xi8>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: @minor_1d\n-tt.func @minor_1d(%arg0: !tt.ptr<i4>) -> (tensor<1x8xi8>) {\n-  %c0 = arith.constant 0 : i32\n-  %c1 = arith.constant 1 : i64\n-  %c8 = arith.constant 8 : i64\n-\n-  %0 = tt.make_tensor_ptr %arg0, [%c1, %c8], [%c1, %c1], [%c0, %c0] {order = array<i32: 1, 0>} : <tensor<1x8xi4>>\n-  // CHECK: %[[LOAD:.*]] = tt.load %{{.*}} : !tt.ptr<tensor<1x4xi8>>\n-  %1 = tt.load %0 : !tt.ptr<tensor<1x8xi4>>\n-  // CHECK-DAG: %[[SHLI:.*]] = arith.shli %[[LOAD]]\n-  // CHECK-DAG: %[[LO:.*]] = arith.shrsi %[[SHLI]]\n-  // CHECK-DAG: %[[HI:.*]] = arith.shrsi %[[LOAD]]\n-  // CHECK: %[[JOIN:.*]] = tt.join %[[LO]], %[[HI]]\n-  // CHECK-NOT: tt.trans\n-  // CHECK: %[[RESHAPE:.*]] = tt.reshape %[[JOIN]]\n-  // CHECK: tt.return %[[RESHAPE]] : tensor<1x8xi8>\n-  %2 = arith.extsi %1 : tensor<1x8xi4> to tensor<1x8xi8>\n-  tt.return %2 : tensor<1x8xi8>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: @major_2d\n-tt.func @major_2d(%arg0: !tt.ptr<i4>) -> (tensor<8x8xi8>) {\n-  %c0 = arith.constant 0 : i32\n-  %c1 = arith.constant 1 : i64\n-  %c8 = arith.constant 8 : i64\n-\n-  %0 = tt.make_tensor_ptr %arg0, [%c8, %c8], [%c1, %c8], [%c0, %c0] {order = array<i32: 1, 0>} : <tensor<8x8xi4>>\n-  // CHECK: %[[LOAD:.*]] = tt.load %{{.*}} : !tt.ptr<tensor<4x8xi8>>\n-  %1 = tt.load %0 : !tt.ptr<tensor<8x8xi4>>\n-  // CHECK-DAG: %[[SHLI:.*]] = arith.shli %[[LOAD]]\n-  // CHECK-DAG: %[[LO:.*]] = arith.shrsi %[[SHLI]]\n-  // CHECK-DAG: %[[HI:.*]] = arith.shrsi %[[LOAD]]\n-  // CHECK: %[[JOIN:.*]] = tt.join %[[LO]], %[[HI]]\n-  // CHECK: %[[TRANS:.*]] = tt.trans %[[JOIN]] {order = array<i32: 0, 2, 1>}\n-  // CHECK-SAME: tensor<4x8x2xi8> -> tensor<4x2x8xi8>\n-  // CHECK: %[[RESHAPE:.*]] = tt.reshape %[[TRANS]]\n-  // CHECK: tt.return %[[RESHAPE]] : tensor<8x8xi8>\n-  %2 = arith.extsi %1 : tensor<8x8xi4> to tensor<8x8xi8>\n-  tt.return %2 : tensor<8x8xi8>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: @minor_2d\n-tt.func @minor_2d(%arg0: !tt.ptr<i4>) -> (tensor<8x8xi8>) {\n-  %c0 = arith.constant 0 : i32\n-  %c1 = arith.constant 1 : i64\n-  %c8 = arith.constant 8 : i64\n-\n-  %0 = tt.make_tensor_ptr %arg0, [%c8, %c8], [%c8, %c1], [%c0, %c0] {order = array<i32: 1, 0>} : <tensor<8x8xi4>>\n-  // CHECK: %[[LOAD:.*]] = tt.load %{{.*}} : !tt.ptr<tensor<8x4xi8>>\n-  %1 = tt.load %0 : !tt.ptr<tensor<8x8xi4>>\n-  // CHECK-DAG: %[[SHLI:.*]] = arith.shli %[[LOAD]]\n-  // CHECK-DAG: %[[LO:.*]] = arith.shrsi %[[SHLI]]\n-  // CHECK-DAG: %[[HI:.*]] = arith.shrsi %[[LOAD]]\n-  // CHECK: %[[JOIN:.*]] = tt.join %[[LO]], %[[HI]]\n-  // CHECK-NOT: tt.trans\n-  // CHECK: %[[RESHAPE:.*]] = tt.reshape %[[JOIN]]\n-  // CHECK: tt.return %[[RESHAPE]] : tensor<8x8xi8>\n-  %2 = arith.extsi %1 : tensor<8x8xi4> to tensor<8x8xi8>\n-  tt.return %2 : tensor<8x8xi8>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: @major_3d\n-tt.func @major_3d(%arg0: !tt.ptr<i4>) -> (tensor<8x8x8xi8>) {\n-  %c0 = arith.constant 0 : i32\n-  %c1 = arith.constant 1 : i64\n-  %c8 = arith.constant 8 : i64\n-  %c64 = arith.constant 64 : i64\n-\n-  %0 = tt.make_tensor_ptr %arg0, [%c8, %c8, %c8], [%c1, %c8, %c64], [%c0, %c0, %c0] {order = array<i32: 2, 1, 0>} : <tensor<8x8x8xi4>>\n-  // CHECK: %[[LOAD:.*]] = tt.load %{{.*}} : !tt.ptr<tensor<4x8x8xi8>>\n-  %1 = tt.load %0 : !tt.ptr<tensor<8x8x8xi4>>\n-  // CHECK-DAG: %[[SHLI:.*]] = arith.shli %[[LOAD]]\n-  // CHECK-DAG: %[[LO:.*]] = arith.shrsi %[[SHLI]]\n-  // CHECK-DAG: %[[HI:.*]] = arith.shrsi %[[LOAD]]\n-  // CHECK: %[[JOIN:.*]] = tt.join %[[LO]], %[[HI]]\n-  // CHECK: %[[TRANS:.*]] = tt.trans %[[JOIN]] {order = array<i32: 0, 3, 1, 2>}\n-  // CHECK-SAME: tensor<4x8x8x2xi8> -> tensor<4x2x8x8xi8>\n-  // CHECK: %[[RESHAPE:.*]] = tt.reshape %[[TRANS]]\n-  // CHECK: tt.return %[[RESHAPE]] : tensor<8x8x8xi8>\n-  %2 = arith.extsi %1 : tensor<8x8x8xi4> to tensor<8x8x8xi8>\n-  tt.return %2 : tensor<8x8x8xi8>\n-}\n-\n-// -----\n-\n // CHECK-LABEL: @triton_xla_extract_2d\n func.func @triton_xla_extract_2d(%arg0: !tt.ptr<i4>) -> (tensor<16x16xi8>) {\n   // CHECK: %[[EXTRACT:.*]] = triton_xla.extract from %arg0"
        },
        {
            "sha": "54f504b415e55452408638014b8f961667cf1cd2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/int4_to_floats.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6422a9f8b634e34f3f8fa5609395d33d1fac6d9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_floats.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6422a9f8b634e34f3f8fa5609395d33d1fac6d9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_floats.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_floats.mlir?ref=6422a9f8b634e34f3f8fa5609395d33d1fac6d9e",
            "patch": "@@ -1,54 +0,0 @@\n-// RUN: xla-opt --split-input-file --int4-to-packed-int4-rewrite --canonicalize %s | FileCheck %s\n-\n-tt.func @gemm_fusion_dot_impl(%arg0: !tt.ptr<i4> {tt.divisibility = 16 : i32}) -> (tensor<16x32xf32>) {\n-  %c1_i64 = arith.constant 1 : i64\n-  %c0_i32 = arith.constant 0 : i32\n-  %c32_i64 = arith.constant 32 : i64\n-  %11 = tt.make_tensor_ptr %arg0, [%c32_i64, %c32_i64], [%c32_i64, %c1_i64], [%c0_i32, %c0_i32] {order = array<i32: 1, 0>} : <tensor<16x32xi4>>\n-  %12 = tt.advance %11, [%c0_i32, %c0_i32] : <tensor<16x32xi4>>\n-  %16 = tt.load %12 : !tt.ptr<tensor<16x32xi4>>\n-  // CHECK: tt.elementwise_inline_asm\n-  // CHECK: tt.join\n-  // CHECK: tt.reshape\n-  // CHECK: arith.extf\n-  %18 = arith.extsi %16 : tensor<16x32xi4> to tensor<16x32xi8>\n-  %19 = arith.sitofp %18 : tensor<16x32xi8> to tensor<16x32xf32>\n-  tt.return %19 : tensor<16x32xf32>\n-}\n-\n-// -----\n-\n-tt.func @gemm_fusion_dot_impl(%arg0: !tt.ptr<i4> {tt.divisibility = 16 : i32}) -> (tensor<16x32xbf16>) {\n-  %c1_i64 = arith.constant 1 : i64\n-  %c0_i32 = arith.constant 0 : i32\n-  %c32_i64 = arith.constant 32 : i64\n-  %11 = tt.make_tensor_ptr %arg0, [%c32_i64, %c32_i64], [%c32_i64, %c1_i64], [%c0_i32, %c0_i32] {order = array<i32: 1, 0>} : <tensor<16x32xi4>>\n-  %12 = tt.advance %11, [%c0_i32, %c0_i32] : <tensor<16x32xi4>>\n-  %16 = tt.load %12 : !tt.ptr<tensor<16x32xi4>>\n-  // CHECK: tt.elementwise_inline_asm\n-  // CHECK: tt.join\n-  // CHECK: tt.reshape\n-  // CHECK-NOT: arith.extf\n-  %18 = arith.extsi %16 : tensor<16x32xi4> to tensor<16x32xi8>\n-  %19 = arith.sitofp %18 : tensor<16x32xi8> to tensor<16x32xbf16>\n-  tt.return %19 : tensor<16x32xbf16>\n-}\n-\n-// -----\n-\n-tt.func @gemm_fusion_dot_impl(%arg0: !tt.ptr<i4> {tt.divisibility = 16 : i32}) -> (tensor<16x32xf16>) {\n-  %c1_i64 = arith.constant 1 : i64\n-  %c0_i32 = arith.constant 0 : i32\n-  %c32_i64 = arith.constant 32 : i64\n-  %11 = tt.make_tensor_ptr %arg0, [%c32_i64, %c32_i64], [%c32_i64, %c1_i64], [%c0_i32, %c0_i32] {order = array<i32: 1, 0>} : <tensor<16x32xi4>>\n-  %12 = tt.advance %11, [%c0_i32, %c0_i32] : <tensor<16x32xi4>>\n-  %16 = tt.load %12 : !tt.ptr<tensor<16x32xi4>>\n-  // CHECK: arith.shli\n-  // CHECK: arith.shrsi\n-  // CHECK: arith.shrsi\n-  // CHECK: tt.join\n-  %18 = arith.extsi %16 : tensor<16x32xi4> to tensor<16x32xi8>\n-  // CHECK: arith.sitofp\n-  %19 = arith.sitofp %18 : tensor<16x32xi8> to tensor<16x32xf16>\n-  tt.return %19 : tensor<16x32xf16>\n-}"
        },
        {
            "sha": "29cdd45524d57c218d78bfcf01166037f5789182",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/int4_to_packed_int4.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6422a9f8b634e34f3f8fa5609395d33d1fac6d9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_packed_int4.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6422a9f8b634e34f3f8fa5609395d33d1fac6d9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_packed_int4.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_packed_int4.mlir?ref=6422a9f8b634e34f3f8fa5609395d33d1fac6d9e",
            "patch": "@@ -1,110 +0,0 @@\n-// RUN: xla-opt --int4-to-packed-int4-rewrite %s --mlir-print-ir-after-all\n-\n-module {\n-  tt.func @gemm_fusion_dot_2_impl(%arg0: !tt.ptr<i4> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n-    %cst = arith.constant dense<0.000000e+00> : tensor<128x128xf32>\n-    %0 = tt.get_program_id x : i32\n-    %c16_i32 = arith.constant 16 : i32\n-    %1 = arith.divsi %0, %c16_i32 : i32\n-    %c8_i32 = arith.constant 8 : i32\n-    %2 = arith.muli %1, %c8_i32 : i32\n-    %c1_i32 = arith.constant 1 : i32\n-    %3 = arith.subi %c1_i32, %2 : i32\n-    %4 = arith.cmpi slt, %3, %c8_i32 : i32\n-    %5 = arith.select %4, %3, %c8_i32 : i32\n-    %6 = arith.remsi %0, %5 : i32\n-    %7 = arith.addi %2, %6 : i32\n-    %c16_i32_0 = arith.constant 16 : i32\n-    %8 = arith.remsi %0, %c16_i32_0 : i32\n-    %9 = arith.divsi %8, %5 : i32\n-    %c128_i32 = arith.constant 128 : i32\n-    %10 = arith.muli %7, %c128_i32 : i32\n-    %c1_i64 = arith.constant 1 : i64\n-    %c0_i32 = arith.constant 0 : i32\n-    %11 = arith.addi %10, %c0_i32 : i32\n-    %c128_i64 = arith.constant 128 : i64\n-    %c0_i32_1 = arith.constant 0 : i32\n-    %c128_i64_2 = arith.constant 128 : i64\n-    %c0_i32_3 = arith.constant 0 : i32\n-    %c128_i64_4 = arith.constant 128 : i64\n-    %c0_i32_5 = arith.constant 0 : i32\n-    %12 = arith.addi %c0_i32_3, %c0_i32_5 : i32\n-    %c64_i64 = arith.constant 64 : i64\n-    %c0_i32_6 = arith.constant 0 : i32\n-    %c64_i64_7 = arith.constant 64 : i64\n-    %c8192_i32 = arith.constant 8192 : i32\n-    %13 = tt.get_program_id y : i32\n-    %c0_i32_8 = arith.constant 0 : i32\n-    %14 = arith.addi %c0_i32_8, %13 : i32\n-    %15 = arith.muli %14, %c8192_i32 : i32\n-    %16 = tt.addptr %arg0, %15 : !tt.ptr<i4>, i32\n-    %17 = tt.make_tensor_ptr %16, [%c128_i64_2, %c64_i64_7], [%c1_i64, %c128_i64_4], [%c0_i32_1, %c0_i32_6] {order = array<i32: 1, 0>} : <tensor<128x32xi4>>\n-    %18 = tt.advance %17, [%10, %c0_i32_3] : <tensor<128x32xi4>>\n-    %c0_i32_9 = arith.constant 0 : i32\n-    %c256_i64 = arith.constant 256 : i64\n-    %c0_i32_10 = arith.constant 0 : i32\n-    %19 = arith.addi %c0_i32_9, %c0_i32_10 : i32\n-    %c64_i64_11 = arith.constant 64 : i64\n-    %c0_i32_12 = arith.constant 0 : i32\n-    %c64_i64_13 = arith.constant 64 : i64\n-    %c128_i32_14 = arith.constant 128 : i32\n-    %20 = arith.muli %9, %c128_i32_14 : i32\n-    %c1_i64_15 = arith.constant 1 : i64\n-    %c0_i32_16 = arith.constant 0 : i32\n-    %21 = arith.addi %20, %c0_i32_16 : i32\n-    %c256_i64_17 = arith.constant 256 : i64\n-    %c0_i32_18 = arith.constant 0 : i32\n-    %c256_i64_19 = arith.constant 256 : i64\n-    %c16384_i32 = arith.constant 16384 : i32\n-    %22 = tt.get_program_id y : i32\n-    %c0_i32_20 = arith.constant 0 : i32\n-    %23 = arith.addi %c0_i32_20, %22 : i32\n-    %24 = arith.muli %23, %c16384_i32 : i32\n-    %25 = tt.addptr %arg1, %24 : !tt.ptr<f32>, i32\n-    %26 = tt.make_tensor_ptr %25, [%c64_i64_13, %c256_i64_19], [%c256_i64, %c1_i64_15], [%c0_i32_12, %c0_i32_18] {order = array<i32: 1, 0>} : <tensor<32x128xf32>>\n-    %27 = tt.advance %26, [%c0_i32_9, %20] : <tensor<32x128xf32>>\n-    %c0_i32_21 = arith.constant 0 : i32\n-    %c64_i32 = arith.constant 64 : i32\n-    %c32_i32 = arith.constant 32 : i32\n-    %28:3 = scf.for %arg3 = %c0_i32_21 to %c64_i32 step %c32_i32 iter_args(%arg4 = %18, %arg5 = %27, %arg6 = %cst) -> (!tt.ptr<tensor<128x32xi4>>, !tt.ptr<tensor<32x128xf32>>, tensor<128x128xf32>)  : i32 {\n-      %39 = tt.load %arg4 : !tt.ptr<tensor<128x32xi4>>\n-      %c0_i32_35 = arith.constant 0 : i32\n-      %c32_i32_36 = arith.constant 32 : i32\n-      %40 = tt.advance %arg4, [%c0_i32_35, %c32_i32_36] : <tensor<128x32xi4>>\n-      %41 = tt.load %arg5 : !tt.ptr<tensor<32x128xf32>>\n-      %c32_i32_37 = arith.constant 32 : i32\n-      %c0_i32_38 = arith.constant 0 : i32\n-      %42 = tt.advance %arg5, [%c32_i32_37, %c0_i32_38] : <tensor<32x128xf32>>\n-      %43 = arith.extsi %39 : tensor<128x32xi4> to tensor<128x32xi8>\n-      %44 = arith.sitofp %43 : tensor<128x32xi8> to tensor<128x32xf32>\n-      %45 = tt.dot %44, %41, %arg6 : tensor<128x32xf32> * tensor<32x128xf32> -> tensor<128x128xf32>\n-      scf.yield %40, %42, %45 : !tt.ptr<tensor<128x32xi4>>, !tt.ptr<tensor<32x128xf32>>, tensor<128x128xf32>\n-    }\n-    %c128_i32_22 = arith.constant 128 : i32\n-    %29 = arith.muli %7, %c128_i32_22 : i32\n-    %c256_i64_23 = arith.constant 256 : i64\n-    %c0_i32_24 = arith.constant 0 : i32\n-    %30 = arith.addi %29, %c0_i32_24 : i32\n-    %c128_i64_25 = arith.constant 128 : i64\n-    %c0_i32_26 = arith.constant 0 : i32\n-    %c128_i64_27 = arith.constant 128 : i64\n-    %c128_i32_28 = arith.constant 128 : i32\n-    %31 = arith.muli %9, %c128_i32_28 : i32\n-    %c1_i64_29 = arith.constant 1 : i64\n-    %c0_i32_30 = arith.constant 0 : i32\n-    %32 = arith.addi %31, %c0_i32_30 : i32\n-    %c256_i64_31 = arith.constant 256 : i64\n-    %c0_i32_32 = arith.constant 0 : i32\n-    %c256_i64_33 = arith.constant 256 : i64\n-    %c32768_i32 = arith.constant 32768 : i32\n-    %33 = tt.get_program_id y : i32\n-    %c0_i32_34 = arith.constant 0 : i32\n-    %34 = arith.addi %c0_i32_34, %33 : i32\n-    %35 = arith.muli %34, %c32768_i32 : i32\n-    %36 = tt.addptr %arg2, %35 : !tt.ptr<f32>, i32\n-    %37 = tt.make_tensor_ptr %36, [%c128_i64_27, %c256_i64_33], [%c256_i64_23, %c1_i64_29], [%c0_i32_26, %c0_i32_32] {order = array<i32: 1, 0>} : <tensor<128x128xf32>>\n-    %38 = tt.advance %37, [%29, %31] : <tensor<128x128xf32>>\n-    tt.store %38, %28#2 : !tt.ptr<tensor<128x128xf32>>\n-    tt.return\n-  }\n-}"
        },
        {
            "sha": "f66b2f4a479d1d830be7a20d3e360351d4d8e13b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/int4_to_packed_int4_small.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6422a9f8b634e34f3f8fa5609395d33d1fac6d9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_packed_int4_small.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6422a9f8b634e34f3f8fa5609395d33d1fac6d9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_packed_int4_small.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_to_packed_int4_small.mlir?ref=6422a9f8b634e34f3f8fa5609395d33d1fac6d9e",
            "patch": "@@ -1,13 +0,0 @@\n-// RUN: xla-opt --int4-to-packed-int4-rewrite %s\n-\n-module {\n-  tt.func @dot_test(%arg0: !tt.ptr<i4> {tt.divisibility = 16 : i32}) -> tensor<16x16xi8> {\n-    %c0 = arith.constant 0 : i32\n-    %c1 = arith.constant 1: i64\n-    %c16 = arith.constant 16: i64\n-    %0 = tt.make_tensor_ptr %arg0, [%c16, %c16], [%c16, %c1], [%c0, %c0] {order = array<i32: 1, 0>} : <tensor<16x16xi4>>\n-    %1 = tt.load %0 : !tt.ptr<tensor<16x16xi4>>\n-    %2 = arith.extsi %1 : tensor<16x16xi4> to tensor<16x16xi8>\n-    tt.return %2 : tensor<16x16xi8>\n-  }\n-}"
        }
    ],
    "stats": {
        "total": 577,
        "additions": 62,
        "deletions": 515
    }
}