{
    "author": "tensorflower-gardener",
    "message": "Integrate LLVM at llvm/llvm-project@42a8ff877d47\n\nUpdates LLVM usage to match\n[42a8ff877d47](https://github.com/llvm/llvm-project/commit/42a8ff877d47)\n\nPiperOrigin-RevId: 826574010",
    "sha": "e0f6a6c7f32c91008430e7d6dce24154c7ba2665",
    "files": [
        {
            "sha": "32e6a7a1c0434f97697d067dfa2617b59902c4e5",
            "filename": "third_party/xla/third_party/llvm/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e0f6a6c7f32c91008430e7d6dce24154c7ba2665/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e0f6a6c7f32c91008430e7d6dce24154c7ba2665/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl?ref=e0f6a6c7f32c91008430e7d6dce24154c7ba2665",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n \n def repo(name):\n     \"\"\"Imports LLVM.\"\"\"\n-    LLVM_COMMIT = \"22079e3f3698d5c367c7b67f63de8c838791ae76\"\n-    LLVM_SHA256 = \"d5616e9c0f4b761f13da5535a0d9ec94acf4ae5226bbec3e47ac2929ea60cac2\"\n+    LLVM_COMMIT = \"42a8ff877d47131ecb1280a1cc7e5e3c3bca6952\"\n+    LLVM_SHA256 = \"f768c5c3b987f68318b8ab3dd4530e54988dfe7d6bfb9b7c9c96acf503367d50\"\n \n     tf_http_archive(\n         name = name,"
        },
        {
            "sha": "1cb2108d447123ddfd174c43fd6a2c741672355d",
            "filename": "third_party/xla/third_party/shardy/temporary.patch",
            "status": "modified",
            "additions": 5,
            "deletions": 709,
            "changes": 714,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e0f6a6c7f32c91008430e7d6dce24154c7ba2665/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e0f6a6c7f32c91008430e7d6dce24154c7ba2665/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch?ref=e0f6a6c7f32c91008430e7d6dce24154c7ba2665",
            "patch": "@@ -1,719 +1,15 @@\n-diff --git a/shardy/integrations/python/jax/mpmd/ops.py b/shardy/integrations/python/jax/mpmd/ops.py\n-index 60ae866..ea144d5 100644\n---- a/shardy/integrations/python/jax/mpmd/ops.py\n-+++ b/shardy/integrations/python/jax/mpmd/ops.py\n-@@ -29,13 +29,11 @@ from jax._src import util\n- from jax._src.interpreters import ad as internal_ad\n- from jax._src.interpreters import batching as internal_batching\n- from jax._src.interpreters import partial_eval as internal_pe\n-+import jax._src.lib.mlir.dialects as jax_mlir_dialects\n- import jax.extend as jex\n- from jax.extend import linear_util as lu\n- from jax.extend import source_info_util as siu\n- from jax.extend.core import primitives\n--from jax.extend.mlir import ir\n--from jax.extend.mlir.dialects import func as func_dialect\n--from jax.extend.mlir.dialects import mpmd\n- from jax.interpreters import ad\n- from jax.interpreters import batching\n- from jax.interpreters import mlir as jax_mlir\n-@@ -45,6 +43,10 @@ import jaxtyping\n- from shardy.integrations.python.jax.mpmd import utils\n- \n- \n-+ir = jax_mlir.ir\n-+mpmd = jax_mlir_dialects.mpmd\n-+func_dialect = jax_mlir_dialects.func\n-+\n- PyTree = jaxtyping.PyTree\n- X = TypeVar('X')\n- Y = TypeVar('Y')\n-diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch\n-index 4117b0a..509398d 100644\n---- a/third_party/llvm/generated.patch\n-+++ b/third_party/llvm/generated.patch\n-@@ -1,576 +1 @@\n- Auto generated patch. Do not edit or delete it, even if empty.\n--diff -ruN --strip-trailing-cr a/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/dap_server.py b/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/dap_server.py\n----- a/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/dap_server.py\n--+++ b/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/dap_server.py\n--@@ -10,8 +10,8 @@\n-- import subprocess\n-- import signal\n-- import sys\n--+import threading\n-- import warnings\n---import selectors\n-- import time\n-- from typing import (\n--     Any,\n--@@ -139,6 +139,35 @@\n--         outfile.write(\"\\n\")\n-- \n-- \n--+def read_packet(\n--+    f: IO[bytes], trace_file: Optional[IO[str]] = None\n--+) -> Optional[ProtocolMessage]:\n--+    \"\"\"Decode a JSON packet that starts with the content length and is\n--+    followed by the JSON bytes from a file 'f'. Returns None on EOF.\n--+    \"\"\"\n--+    line = f.readline().decode(\"utf-8\")\n--+    if len(line) == 0:\n--+        return None  # EOF.\n--+\n--+    # Watch for line that starts with the prefix\n--+    prefix = \"Content-Length: \"\n--+    if line.startswith(prefix):\n--+        # Decode length of JSON bytes\n--+        length = int(line[len(prefix) :])\n--+        # Skip empty line\n--+        separator = f.readline().decode()\n--+        if separator != \"\":\n--+            Exception(\"malformed DAP content header, unexpected line: \" + separator)\n--+        # Read JSON bytes\n--+        json_str = f.read(length).decode()\n--+        if trace_file:\n--+            trace_file.write(\"from adapter:\\n%s\\n\" % (json_str))\n--+        # Decode the JSON bytes into a python dictionary\n--+        return json.loads(json_str)\n--+\n--+    raise Exception(\"unexpected malformed message from lldb-dap: \" + line)\n--+\n--+\n-- def packet_type_is(packet, packet_type):\n--     return \"type\" in packet and packet[\"type\"] == packet_type\n-- \n--@@ -170,8 +199,16 @@\n--         self.log_file = log_file\n--         self.send = send\n--         self.recv = recv\n---        self.selector = selectors.DefaultSelector()\n---        self.selector.register(recv, selectors.EVENT_READ)\n--+\n--+        # Packets that have been received and processed but have not yet been\n--+        # requested by a test case.\n--+        self._pending_packets: List[Optional[ProtocolMessage]] = []\n--+        # Received packets that have not yet been processed.\n--+        self._recv_packets: List[Optional[ProtocolMessage]] = []\n--+        # Used as a mutex for _recv_packets and for notify when _recv_packets\n--+        # changes.\n--+        self._recv_condition = threading.Condition()\n--+        self._recv_thread = threading.Thread(target=self._read_packet_thread)\n-- \n--         # session state\n--         self.init_commands = init_commands\n--@@ -197,6 +234,9 @@\n--         # keyed by breakpoint id\n--         self.resolved_breakpoints: dict[str, Breakpoint] = {}\n-- \n--+        # trigger enqueue thread\n--+        self._recv_thread.start()\n--+\n--     @classmethod\n--     def encode_content(cls, s: str) -> bytes:\n--         return (\"Content-Length: %u\\r\\n\\r\\n%s\" % (len(s), s)).encode(\"utf-8\")\n--@@ -212,46 +252,17 @@\n--                 f\"seq mismatch in response {command['seq']} != {response['request_seq']}\"\n--             )\n-- \n---    def _read_packet(\n---        self,\n---        timeout: float = DEFAULT_TIMEOUT,\n---    ) -> Optional[ProtocolMessage]:\n---        \"\"\"Decode a JSON packet that starts with the content length and is\n---        followed by the JSON bytes from self.recv. Returns None on EOF.\n---        \"\"\"\n---\n---        ready = self.selector.select(timeout)\n---        if not ready:\n---            warnings.warn(\n---                \"timeout occurred waiting for a packet, check if the test has a\"\n---                \" negative assertion and see if it can be inverted.\",\n---                stacklevel=4,\n---            )\n---            return None  # timeout\n---\n---        line = self.recv.readline().decode(\"utf-8\")\n---        if len(line) == 0:\n---            return None  # EOF.\n---\n---        # Watch for line that starts with the prefix\n---        prefix = \"Content-Length: \"\n---        if line.startswith(prefix):\n---            # Decode length of JSON bytes\n---            length = int(line[len(prefix) :])\n---            # Skip empty line\n---            separator = self.recv.readline().decode()\n---            if separator != \"\":\n---                Exception(\"malformed DAP content header, unexpected line: \" + separator)\n---            # Read JSON bytes\n---            json_str = self.recv.read(length).decode()\n---            if self.trace_file:\n---                self.trace_file.write(\n---                    \"%s from adapter:\\n%s\\n\" % (time.time(), json_str)\n---                )\n---            # Decode the JSON bytes into a python dictionary\n---            return json.loads(json_str)\n---\n---        raise Exception(\"unexpected malformed message from lldb-dap: \" + line)\n--+    def _read_packet_thread(self):\n--+        try:\n--+            while True:\n--+                packet = read_packet(self.recv, trace_file=self.trace_file)\n--+                # `packet` will be `None` on EOF. We want to pass it down to\n--+                # handle_recv_packet anyway so the main thread can handle unexpected\n--+                # termination of lldb-dap and stop waiting for new packets.\n--+                if not self._handle_recv_packet(packet):\n--+                    break\n--+        finally:\n--+            dump_dap_log(self.log_file)\n-- \n--     def get_modules(\n--         self, start_module: Optional[int] = None, module_count: Optional[int] = None\n--@@ -299,6 +310,34 @@\n--             output += self.get_output(category, clear=clear)\n--         return output\n-- \n--+    def _enqueue_recv_packet(self, packet: Optional[ProtocolMessage]):\n--+        with self.recv_condition:\n--+            self.recv_packets.append(packet)\n--+            self.recv_condition.notify()\n--+\n--+    def _handle_recv_packet(self, packet: Optional[ProtocolMessage]) -> bool:\n--+        \"\"\"Handles an incoming packet.\n--+\n--+        Called by the read thread that is waiting for all incoming packets\n--+        to store the incoming packet in \"self._recv_packets\" in a thread safe\n--+        way. This function will then signal the \"self._recv_condition\" to\n--+        indicate a new packet is available.\n--+\n--+        Args:\n--+            packet: A new packet to store.\n--+\n--+        Returns:\n--+            True if the caller should keep calling this function for more\n--+            packets.\n--+        \"\"\"\n--+        with self._recv_condition:\n--+            self._recv_packets.append(packet)\n--+            self._recv_condition.notify()\n--+            # packet is None on EOF\n--+            return packet is not None and not (\n--+                packet[\"type\"] == \"response\" and packet[\"command\"] == \"disconnect\"\n--+            )\n--+\n--     def _recv_packet(\n--         self,\n--         *,\n--@@ -322,34 +361,46 @@\n--             The first matching packet for the given predicate, if specified,\n--             otherwise None.\n--         \"\"\"\n---        deadline = time.time() + timeout\n---\n---        while time.time() < deadline:\n---            packet = self._read_packet(timeout=deadline - time.time())\n---            if packet is None:\n---                return None\n---            self._process_recv_packet(packet)\n---            if not predicate or predicate(packet):\n---                return packet\n--+        assert (\n--+            threading.current_thread != self._recv_thread\n--+        ), \"Must not be called from the _recv_thread\"\n--+\n--+        def process_until_match():\n--+            self._process_recv_packets()\n--+            for i, packet in enumerate(self._pending_packets):\n--+                if packet is None:\n--+                    # We need to return a truthy value to break out of the\n--+                    # wait_for, use `EOFError` as an indicator of EOF.\n--+                    return EOFError()\n--+                if predicate and predicate(packet):\n--+                    self._pending_packets.pop(i)\n--+                    return packet\n--+\n--+        with self._recv_condition:\n--+            packet = self._recv_condition.wait_for(process_until_match, timeout)\n--+            return None if isinstance(packet, EOFError) else packet\n-- \n---    def _process_recv_packet(self, packet) -> None:\n--+    def _process_recv_packets(self) -> None:\n--         \"\"\"Process received packets, updating the session state.\"\"\"\n---        if packet and (\"seq\" not in packet or packet[\"seq\"] == 0):\n---            warnings.warn(\n---                f\"received a malformed packet, expected 'seq != 0' for {packet!r}\"\n---            )\n---        # Handle events that may modify any stateful properties of\n---        # the DAP session.\n---        if packet and packet[\"type\"] == \"event\":\n---            self._handle_event(packet)\n---        elif packet and packet[\"type\"] == \"request\":\n---            # Handle reverse requests and keep processing.\n---            self._handle_reverse_request(packet)\n--+        with self._recv_condition:\n--+            for packet in self._recv_packets:\n--+                if packet and (\"seq\" not in packet or packet[\"seq\"] == 0):\n--+                    warnings.warn(\n--+                        f\"received a malformed packet, expected 'seq != 0' for {packet!r}\"\n--+                    )\n--+                # Handle events that may modify any stateful properties of\n--+                # the DAP session.\n--+                if packet and packet[\"type\"] == \"event\":\n--+                    self._handle_event(packet)\n--+                elif packet and packet[\"type\"] == \"request\":\n--+                    # Handle reverse requests and keep processing.\n--+                    self._handle_reverse_request(packet)\n--+                # Move the packet to the pending queue.\n--+                self._pending_packets.append(packet)\n--+            self._recv_packets.clear()\n-- \n--     def _handle_event(self, packet: Event) -> None:\n--         \"\"\"Handle any events that modify debug session state we track.\"\"\"\n---        self.events.append(packet)\n---\n--         event = packet[\"event\"]\n--         body: Optional[Dict] = packet.get(\"body\", None)\n-- \n--@@ -402,8 +453,6 @@\n--             self.invalidated_event = packet\n--         elif event == \"memory\":\n--             self.memory_event = packet\n---        elif event == \"module\":\n---            self.module_events.append(packet)\n-- \n--     def _handle_reverse_request(self, request: Request) -> None:\n--         if request in self.reverse_requests:\n--@@ -472,14 +521,18 @@\n-- \n--         Returns the seq number of the request.\n--         \"\"\"\n---        packet[\"seq\"] = self.sequence\n---        self.sequence += 1\n--+        # Set the seq for requests.\n--+        if packet[\"type\"] == \"request\":\n--+            packet[\"seq\"] = self.sequence\n--+            self.sequence += 1\n--+        else:\n--+            packet[\"seq\"] = 0\n-- \n--         # Encode our command dictionary as a JSON string\n--         json_str = json.dumps(packet, separators=(\",\", \":\"))\n-- \n--         if self.trace_file:\n---            self.trace_file.write(\"%s to adapter:\\n%s\\n\" % (time.time(), json_str))\n--+            self.trace_file.write(\"to adapter:\\n%s\\n\" % (json_str))\n-- \n--         length = len(json_str)\n--         if length > 0:\n--@@ -860,8 +913,6 @@\n--         if restartArguments:\n--             command_dict[\"arguments\"] = restartArguments\n-- \n---        # Clear state, the process is about to restart...\n---        self._process_continued(True)\n--         response = self._send_recv(command_dict)\n--         # Caller must still call wait_for_stopped.\n--         return response\n--@@ -1428,10 +1479,8 @@\n-- \n--     def terminate(self):\n--         self.send.close()\n---        self.recv.close()\n---        self.selector.close()\n---        if self.log_file:\n---            dump_dap_log(self.log_file)\n--+        if self._recv_thread.is_alive():\n--+            self._recv_thread.join()\n-- \n--     def request_setInstructionBreakpoints(self, memory_reference=[]):\n--         breakpoints = []\n--@@ -1528,7 +1577,6 @@\n--             stdout=subprocess.PIPE,\n--             stderr=sys.stderr,\n--             env=adapter_env,\n---            bufsize=0,\n--         )\n-- \n--         if connection is None:\n--diff -ruN --strip-trailing-cr a/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/lldbdap_testcase.py b/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/lldbdap_testcase.py\n----- a/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/lldbdap_testcase.py\n--+++ b/lldb/packages/Python/lldbsuite/test/tools/lldb-dap/lldbdap_testcase.py\n--@@ -416,7 +416,7 @@\n--         return self.dap_server.wait_for_stopped()\n-- \n--     def continue_to_breakpoint(self, breakpoint_id: str):\n---        self.continue_to_breakpoints([breakpoint_id])\n--+        self.continue_to_breakpoints((breakpoint_id))\n-- \n--     def continue_to_breakpoints(self, breakpoint_ids):\n--         self.do_continue()\n--diff -ruN --strip-trailing-cr a/lldb/test/API/tools/lldb-dap/breakpoint-events/TestDAP_breakpointEvents.py b/lldb/test/API/tools/lldb-dap/breakpoint-events/TestDAP_breakpointEvents.py\n----- a/lldb/test/API/tools/lldb-dap/breakpoint-events/TestDAP_breakpointEvents.py\n--+++ b/lldb/test/API/tools/lldb-dap/breakpoint-events/TestDAP_breakpointEvents.py\n--@@ -81,20 +81,24 @@\n--                 breakpoint[\"verified\"], \"expect foo breakpoint to not be verified\"\n--             )\n-- \n--+        # Flush the breakpoint events.\n--+        self.dap_server.wait_for_breakpoint_events()\n--+\n--         # Continue to the breakpoint\n---        self.continue_to_breakpoint(foo_bp_id)\n---        self.continue_to_next_stop()  # foo_bp2\n---        self.continue_to_breakpoint(main_bp_id)\n---        self.continue_to_exit()\n--+        self.continue_to_breakpoints(dap_breakpoint_ids)\n-- \n---        bp_events = [e for e in self.dap_server.events if e[\"event\"] == \"breakpoint\"]\n--+        verified_breakpoint_ids = []\n--+        unverified_breakpoint_ids = []\n--+        for breakpoint_event in self.dap_server.wait_for_breakpoint_events():\n--+            breakpoint = breakpoint_event[\"body\"][\"breakpoint\"]\n--+            id = breakpoint[\"id\"]\n--+            if breakpoint[\"verified\"]:\n--+                verified_breakpoint_ids.append(id)\n--+            else:\n--+                unverified_breakpoint_ids.append(id)\n-- \n---        main_bp_events = [\n---            e for e in bp_events if e[\"body\"][\"breakpoint\"][\"id\"] == main_bp_id\n---        ]\n---        foo_bp_events = [\n---            e for e in bp_events if e[\"body\"][\"breakpoint\"][\"id\"] == foo_bp_id\n---        ]\n--+        self.assertIn(main_bp_id, unverified_breakpoint_ids)\n--+        self.assertIn(foo_bp_id, unverified_breakpoint_ids)\n-- \n---        self.assertTrue(main_bp_events)\n---        self.assertTrue(foo_bp_events)\n--+        self.assertIn(main_bp_id, verified_breakpoint_ids)\n--+        self.assertIn(foo_bp_id, verified_breakpoint_ids)\n--diff -ruN --strip-trailing-cr a/lldb/test/API/tools/lldb-dap/launch/TestDAP_launch.py b/lldb/test/API/tools/lldb-dap/launch/TestDAP_launch.py\n----- a/lldb/test/API/tools/lldb-dap/launch/TestDAP_launch.py\n--+++ b/lldb/test/API/tools/lldb-dap/launch/TestDAP_launch.py\n--@@ -156,7 +156,6 @@\n--         self.build_and_launch(\n--             program, debuggerRoot=program_parent_dir, initCommands=commands\n--         )\n---        self.continue_to_exit()\n--         output = self.get_console()\n--         self.assertTrue(output and len(output) > 0, \"expect console output\")\n--         lines = output.splitlines()\n--@@ -172,6 +171,7 @@\n--                     % (program_parent_dir, line[len(prefix) :]),\n--                 )\n--         self.assertTrue(found, \"verified lldb-dap working directory\")\n--+        self.continue_to_exit()\n-- \n--     def test_sourcePath(self):\n--         \"\"\"\n--diff -ruN --strip-trailing-cr a/lldb/test/API/tools/lldb-dap/module/TestDAP_module.py b/lldb/test/API/tools/lldb-dap/module/TestDAP_module.py\n----- a/lldb/test/API/tools/lldb-dap/module/TestDAP_module.py\n--+++ b/lldb/test/API/tools/lldb-dap/module/TestDAP_module.py\n--@@ -64,18 +64,19 @@\n--         self.assertEqual(program, program_module[\"path\"])\n--         self.assertIn(\"addressRange\", program_module)\n-- \n---        self.continue_to_exit()\n---\n--         # Collect all the module names we saw as events.\n--         module_new_names = []\n--         module_changed_names = []\n---        for module_event in self.dap_server.module_events:\n--+        module_event = self.dap_server.wait_for_event([\"module\"])\n--+        while module_event is not None:\n--             reason = module_event[\"body\"][\"reason\"]\n--             if reason == \"new\":\n--                 module_new_names.append(module_event[\"body\"][\"module\"][\"name\"])\n--             elif reason == \"changed\":\n--                 module_changed_names.append(module_event[\"body\"][\"module\"][\"name\"])\n-- \n--+            module_event = self.dap_server.wait_for_event([\"module\"])\n--+\n--         # Make sure we got an event for every active module.\n--         self.assertNotEqual(len(module_new_names), 0)\n--         for module in active_modules:\n--@@ -85,6 +86,7 @@\n--         # symbols got added.\n--         self.assertNotEqual(len(module_changed_names), 0)\n--         self.assertIn(program_module[\"name\"], module_changed_names)\n--+        self.continue_to_exit()\n-- \n--     @skipIfWindows\n--     def test_modules(self):\n--diff -ruN --strip-trailing-cr a/lldb/test/API/tools/lldb-dap/module-event/TestDAP_module_event.py b/lldb/test/API/tools/lldb-dap/module-event/TestDAP_module_event.py\n----- a/lldb/test/API/tools/lldb-dap/module-event/TestDAP_module_event.py\n--+++ b/lldb/test/API/tools/lldb-dap/module-event/TestDAP_module_event.py\n--@@ -1,58 +1,58 @@\n---\"\"\"\n---Test 'module' events for dynamically loaded libraries.\n---\"\"\"\n---\n--+import dap_server\n-- from lldbsuite.test.decorators import *\n-- from lldbsuite.test.lldbtest import *\n--+from lldbsuite.test import lldbutil\n-- import lldbdap_testcase\n--+import re\n-- \n-- \n-- class TestDAP_module_event(lldbdap_testcase.DAPTestCaseBase):\n---    def lookup_module_id(self, name):\n---        \"\"\"Returns the identifier for the first module event starting with the given name.\"\"\"\n---        for event in self.dap_server.module_events:\n---            if self.get_dict_value(event, [\"body\", \"module\", \"name\"]).startswith(name):\n---                return self.get_dict_value(event, [\"body\", \"module\", \"id\"])\n---        self.fail(f\"No module events matching name={name}\")\n---\n---    def module_events(self, id):\n---        \"\"\"Finds all module events by identifier.\"\"\"\n---        return [\n---            event\n---            for event in self.dap_server.module_events\n---            if self.get_dict_value(event, [\"body\", \"module\", \"id\"]) == id\n---        ]\n---\n---    def module_reasons(self, events):\n---        \"\"\"Returns the list of 'reason' values from the given events.\"\"\"\n---        return [event[\"body\"][\"reason\"] for event in events]\n---\n--     @skipIfWindows\n--     def test_module_event(self):\n---        \"\"\"\n---        Test that module events are fired on target load and when the list of\n---        dynamic libraries updates while running.\n---        \"\"\"\n--         program = self.getBuildArtifact(\"a.out\")\n--         self.build_and_launch(program)\n---        # We can analyze the order of events after the process exits.\n---        self.continue_to_exit()\n-- \n---        a_out_id = self.lookup_module_id(\"a.out\")\n---        a_out_events = self.module_events(id=a_out_id)\n--+        source = \"main.cpp\"\n--+        breakpoint1_line = line_number(source, \"// breakpoint 1\")\n--+        breakpoint2_line = line_number(source, \"// breakpoint 2\")\n--+        breakpoint3_line = line_number(source, \"// breakpoint 3\")\n-- \n---        self.assertIn(\n---            \"new\",\n---            self.module_reasons(a_out_events),\n---            \"Expected a.out to load during the debug session.\",\n--+        breakpoint_ids = self.set_source_breakpoints(\n--+            source, [breakpoint1_line, breakpoint2_line, breakpoint3_line]\n--         )\n--+        self.continue_to_breakpoints(breakpoint_ids)\n-- \n---        libother_id = self.lookup_module_id(\n---            \"libother.\"  # libother.so or libother.dylib based on OS.\n---        )\n---        libother_events = self.module_events(id=libother_id)\n---        self.assertEqual(\n---            self.module_reasons(libother_events),\n---            [\"new\", \"removed\"],\n---            \"Expected libother to be loaded then unloaded during the debug session.\",\n---        )\n--+        # We're now stopped at breakpoint 1 before the dlopen. Flush all the module events.\n--+        event = self.dap_server.wait_for_event([\"module\"])\n--+        while event is not None:\n--+            event = self.dap_server.wait_for_event([\"module\"])\n--+\n--+        # Continue to the second breakpoint, before the dlclose.\n--+        self.continue_to_breakpoints(breakpoint_ids)\n--+\n--+        # Make sure we got a module event for libother.\n--+        event = self.dap_server.wait_for_event([\"module\"])\n--+        self.assertIsNotNone(event, \"didn't get a module event\")\n--+        module_name = event[\"body\"][\"module\"][\"name\"]\n--+        module_id = event[\"body\"][\"module\"][\"id\"]\n--+        self.assertEqual(event[\"body\"][\"reason\"], \"new\")\n--+        self.assertIn(\"libother\", module_name)\n--+\n--+        # Continue to the third breakpoint, after the dlclose.\n--+        self.continue_to_breakpoints(breakpoint_ids)\n--+\n--+        # Make sure we got a module event for libother.\n--+        event = self.dap_server.wait_for_event([\"module\"])\n--+        self.assertIsNotNone(event, \"didn't get a module event\")\n--+        reason = event[\"body\"][\"reason\"]\n--+        self.assertEqual(reason, \"removed\")\n--+        self.assertEqual(event[\"body\"][\"module\"][\"id\"], module_id)\n--+\n--+        # The removed module event should omit everything but the module id and name\n--+        # as they are required fields.\n--+        module_data = event[\"body\"][\"module\"]\n--+        required_keys = [\"id\", \"name\"]\n--+        self.assertListEqual(list(module_data.keys()), required_keys)\n--+        self.assertEqual(module_data[\"name\"], \"\", \"expects empty name.\")\n--+\n--+        self.continue_to_exit()\n--diff -ruN --strip-trailing-cr a/lldb/test/API/tools/lldb-dap/restart/TestDAP_restart_console.py b/lldb/test/API/tools/lldb-dap/restart/TestDAP_restart_console.py\n----- a/lldb/test/API/tools/lldb-dap/restart/TestDAP_restart_console.py\n--+++ b/lldb/test/API/tools/lldb-dap/restart/TestDAP_restart_console.py\n--@@ -30,11 +30,7 @@\n--             if reason == \"entry\":\n--                 seen_stopped_event += 1\n-- \n---        self.assertEqual(\n---            seen_stopped_event,\n---            1,\n---            f\"expect only one stopped entry event in {stopped_events}\",\n---        )\n--+        self.assertEqual(seen_stopped_event, 1, \"expect only one stopped entry event.\")\n-- \n--     @skipIfAsan\n--     @skipIfWindows\n--@@ -96,13 +92,11 @@\n--         self.build_and_launch(program, console=\"integratedTerminal\", stopOnEntry=True)\n--         [bp_main] = self.set_function_breakpoints([\"main\"])\n-- \n---        self.dap_server.request_configurationDone()\n---        stopped_threads = list(self.dap_server.thread_stop_reasons.values())\n--+        self.dap_server.request_continue()  # sends configuration done\n--+        stopped_events = self.dap_server.wait_for_stopped()\n--         # We should be stopped at the entry point.\n---        self.assertEqual(\n---            len(stopped_threads), 1, \"Expected the main thread to be stopped on entry.\"\n---        )\n---        self.assertEqual(stopped_threads[0][\"reason\"], \"entry\")\n--+        self.assertGreaterEqual(len(stopped_events), 0, \"expect stopped events\")\n--+        self.verify_stopped_on_entry(stopped_events)\n-- \n--         # Then, if we continue, we should hit the breakpoint at main.\n--         self.dap_server.request_continue()\n--@@ -111,12 +105,8 @@\n--         # Restart and check that we still get a stopped event before reaching\n--         # main.\n--         self.dap_server.request_restart()\n---        stopped_threads = list(self.dap_server.thread_stop_reasons.values())\n---        # We should be stopped at the entry point.\n---        self.assertEqual(\n---            len(stopped_threads), 1, \"Expected the main thread to be stopped on entry.\"\n---        )\n---        self.assertEqual(stopped_threads[0][\"reason\"], \"entry\")\n--+        stopped_events = self.dap_server.wait_for_stopped()\n--+        self.verify_stopped_on_entry(stopped_events)\n-- \n--         # continue to main\n--         self.dap_server.request_continue()\n--diff -ruN --strip-trailing-cr a/lldb/test/API/tools/lldb-dap/send-event/TestDAP_sendEvent.py b/lldb/test/API/tools/lldb-dap/send-event/TestDAP_sendEvent.py\n----- a/lldb/test/API/tools/lldb-dap/send-event/TestDAP_sendEvent.py\n--+++ b/lldb/test/API/tools/lldb-dap/send-event/TestDAP_sendEvent.py\n--@@ -32,7 +32,7 @@\n--             ],\n--         )\n--         self.set_source_breakpoints(source, [breakpoint_line])\n---        self.do_continue()\n--+        self.continue_to_next_stop()\n-- \n--         custom_event = self.dap_server.wait_for_event(\n--             filter=[\"my-custom-event-no-body\"]\n diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl\n-index d012c7c..0859165 100644\n+index 0859165..32e6a7a 100644\n --- a/third_party/llvm/workspace.bzl\n +++ b/third_party/llvm/workspace.bzl\n @@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n  \n  def repo(name):\n      \"\"\"Imports LLVM.\"\"\"\n--    LLVM_COMMIT = \"4c46ae394841521914e0e8575e7619a1c0d1149d\"\n--    LLVM_SHA256 = \"55c824ce2e1a3afafa4e108532f4eff9f194d20d44d1c5ddc6107bb23d7c6c2a\"\n-+    LLVM_COMMIT = \"22079e3f3698d5c367c7b67f63de8c838791ae76\"\n-+    LLVM_SHA256 = \"d5616e9c0f4b761f13da5535a0d9ec94acf4ae5226bbec3e47ac2929ea60cac2\"\n+-    LLVM_COMMIT = \"22079e3f3698d5c367c7b67f63de8c838791ae76\"\n+-    LLVM_SHA256 = \"d5616e9c0f4b761f13da5535a0d9ec94acf4ae5226bbec3e47ac2929ea60cac2\"\n++    LLVM_COMMIT = \"42a8ff877d47131ecb1280a1cc7e5e3c3bca6952\"\n++    LLVM_SHA256 = \"f768c5c3b987f68318b8ab3dd4530e54988dfe7d6bfb9b7c9c96acf503367d50\"\n  \n      tf_http_archive(\n          name = name,\n-diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch\n-index c445b82..a42cbd7 100755\n---- a/third_party/stablehlo/temporary.patch\n-+++ b/third_party/stablehlo/temporary.patch\n-@@ -1,3 +1,88 @@\n-+diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n-+--- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n-++++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n-+@@ -768,7 +768,7 @@\n-+ // CHECK-PRIMITIVE: %[[MAP:.+]] = linalg.map\n-+ // CHECK-PRIMITIVE-SAME: ins(%[[ARG0]], %[[ARG1]]\n-+ // CHECK-PRIMITIVE-SAME: outs(%[[INIT]] : tensor<?xi1>)\n-+-// CHECK-PRIMITIVE-NEXT: (%[[A:.+]]: complex<f32>, %[[B:.+]]: complex<f32>) {\n-++// CHECK-PRIMITIVE-NEXT: (%[[A:.+]]: complex<f32>, %[[B:.+]]: complex<f32>, %{{.+}}: i1) {\n-+ // CHECK-PRIMITIVE: %[[RE1:.+]] = complex.re %[[A]] : complex<f32>\n-+ // CHECK-PRIMITIVE: %[[RE2:.+]] = complex.re %[[B]] : complex<f32>\n-+ // CHECK-PRIMITIVE: %[[CMP:.+]] = arith.cmpf oeq, %[[RE1]], %[[RE2]] : f32\n-+diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir b/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n-+--- stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n-++++ stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n-+@@ -714,7 +714,7 @@\n-+ // CHECK-PRIMITIVE: linalg.map\n-+ // CHECK-PRIMITIVE-SAME: ins(\n-+ // CHECK-PRIMITIVE-SAME: outs(\n-+-// CHECK-PRIMITIVE-NEXT: (%[[LHS_IN:[a-zA-Z0-9]*]]: bf16, %[[RHS_IN:.*]]: bf16) {\n-++// CHECK-PRIMITIVE-NEXT: (%[[LHS_IN:[a-zA-Z0-9]*]]: bf16, %[[RHS_IN:.*]]: bf16, %[[RESULT_OUT:.*]]: i1) {\n-+ // CHECK-PRIMITIVE-NEXT:   %[[LHS_INT:.*]] = arith.bitcast %[[LHS_IN]] : bf16 to i16\n-+ // CHECK-PRIMITIVE-NEXT:   %[[LHS_CMP:.*]] = arith.cmpi slt, %[[LHS_INT]], %[[C0]] : i16\n-+ // CHECK-PRIMITIVE-NEXT:   %[[LHS_SUB:.*]] = arith.subi %[[C32767]], %[[LHS_INT]] : i16\n-+@@ -937,7 +937,7 @@\n-+ // CHECK-PRIMITIVE-SAME:   ins(%[[LHS]], %[[RHS]] : tensor<2x?xf32>, tensor<2x?xf32>)\n-+ // CHECK-PRIMITIVE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)\n-+ // CHECK-PRIMITIVE-SAME:   {someattr}\n-+-// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {\n-++// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %[[RESULT_OUT:.*]]: f32) {\n-+ // CHECK-PRIMITIVE:        %[[RES:.*]] = arith.select %[[PRED_ELEM]], %[[LHS_]], %[[RHS_]] : f32\n-+ // CHECK-PRIMITIVE:        linalg.yield %[[RES]]\n-+ \n-+@@ -978,7 +978,7 @@\n-+ // CHECK-PRIMITIVE-SAME:   ins(%[[LHS]], %[[RHS]] : tensor<2x?xf32>, tensor<2x?xf32>)\n-+ // CHECK-PRIMITIVE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)\n-+ // CHECK-PRIMITIVE-SAME:   {someattr}\n-+-// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {\n-++// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %[[RESULT_OUT:.*]]: f32) {\n-+ // CHECK-PRIMITIVE:        linalg.yield %[[LHS_]]\n-+ \n-+ // -----\n-+@@ -1416,7 +1416,7 @@\n-+ \n-+ // CHECK-PRIMITIVE: %[[INIT:.*]] = tensor.empty\n-+ // CHECK-PRIMITIVE: %[[RESULT:.*]] = linalg.map ins(%[[LB]], %[[X]], %[[UB]] : tensor<4xf32>, tensor<4xf32>, tensor<4xf32>) outs(%[[INIT]] : tensor<4xf32>)\n-+-// CHECK-PRIMITIVE: (%[[SCALAR_LB:.*]]: f32, %[[SCALAR_X:.*]]: f32, %[[SCALAR_UB:.*]]: f32)\n-++// CHECK-PRIMITIVE: (%[[SCALAR_LB:.*]]: f32, %[[SCALAR_X:.*]]: f32, %[[SCALAR_UB:.*]]: f32, %[[RESULT_OUT:.*]]: f32)\n-+ // CHECK-PRIMITIVE:   %[[MAX:.*]] = arith.maximumf %[[SCALAR_LB]], %[[SCALAR_X]] : f32\n-+ // CHECK-PRIMITIVE:   %[[MIN:.*]] = arith.minimumf %[[MAX]], %[[SCALAR_UB]] : f32\n-+ // CHECK-PRIMITIVE:   linalg.yield %[[MIN]]\n-+@@ -1478,7 +1478,7 @@\n-+ // CHECK-PRIMITIVE-DAG: %[[SCALAR_LB:.*]] = tensor.extract %[[LB]]\n-+ // CHECK-PRIMITIVE-DAG: %[[SCALAR_UB:.*]] = tensor.extract %[[UB]]\n-+ // CHECK-PRIMITIVE: %[[RESULT:.*]] = linalg.map ins(%[[X]] : tensor<?xf32>) outs(%[[INIT]] : tensor<?xf32>)\n-+-// CHECK-PRIMITIVE: (%[[SCALAR_X:.*]]: f32)\n-++// CHECK-PRIMITIVE: (%[[SCALAR_X:.*]]: f32, %[[RESULT_OUT:.*]]: f32)\n-+ // CHECK-PRIMITIVE:   %[[MAX:.*]] = arith.maximumf %[[SCALAR_LB]], %[[SCALAR_X]] : f32\n-+ // CHECK-PRIMITIVE:   %[[MIN:.*]] = arith.minimumf %[[MAX]], %[[SCALAR_UB]] : f32\n-+ // CHECK-PRIMITIVE:   linalg.yield %[[MIN]]\n-+@@ -1554,7 +1554,7 @@\n-+   // CHECK:   linalg.yield %[[V_NOT]] : i32\n-+   // CHECK-PRIMITIVE: %[[CST_N1:.+]] = arith.constant -1 : i32\n-+   // CHECK-PRIMITIVE: linalg.map\n-+-  // CHECK-PRIMITIVE:   (%[[IN:.+]]: i32)\n-++  // CHECK-PRIMITIVE:   (%[[IN:.+]]: i32, %[[RESULT_OUT:.+]]: i32)\n-+   // CHECK-PRIMITIVE:   %[[V_NOT:.+]] = arith.xori %[[IN]], %[[CST_N1]] : i32\n-+   // CHECK-PRIMITIVE:   linalg.yield %[[V_NOT]] : i32\n-+   %0 = \"stablehlo.not\"(%arg) : (tensor<2x2xi32>) -> tensor<2x2xi32>\n-+diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-+--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-++++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-+@@ -1748,6 +1748,12 @@\n-+ \n-+     rewriter.applySignatureConversion(&region.front(), signatureConverter,\n-+                                       getTypeConverter());\n-++    auto& blocks = linalgOp.getMapper().getBlocks();\n-++    if (blocks.empty()) {\n-++      return rewriter.notifyMatchFailure(op, \"expected at least one block\");\n-++    }\n-++    blocks.front().addArgument(resultType.getElementType(), loc);\n-++\n-+     auto result = rewriter.createOrFold<tensor::CastOp>(loc, resultType,\n-+                                                         linalgOp.getResults());\n-+     rewriter.replaceOp(op, result);\n- diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n- --- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n- +++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp"
        },
        {
            "sha": "56ecbe39fb767170f7ac4e4137795efdd79744a6",
            "filename": "third_party/xla/third_party/shardy/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e0f6a6c7f32c91008430e7d6dce24154c7ba2665/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e0f6a6c7f32c91008430e7d6dce24154c7ba2665/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl?ref=e0f6a6c7f32c91008430e7d6dce24154c7ba2665",
            "patch": "@@ -3,8 +3,8 @@\n load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n-    SHARDY_COMMIT = \"f1c951d74cf5c67b9e0f776e21fe2316d5c69f37\"\n-    SHARDY_SHA256 = \"0b8b96710a2f2eec4581186e4e773aa4c4cfe6ae5e9681b7803e9b8336ead2f7\"\n+    SHARDY_COMMIT = \"e269b4c1968c930518c42c02bfdcdf0d921793de\"\n+    SHARDY_SHA256 = \"bdf22ae5d5a1ecacdca762da892e2291a7f82ddc42a23b1ca096dadb490d6068\"\n \n     tf_http_archive(\n         name = \"shardy\","
        }
    ],
    "stats": {
        "total": 722,
        "additions": 9,
        "deletions": 713
    }
}