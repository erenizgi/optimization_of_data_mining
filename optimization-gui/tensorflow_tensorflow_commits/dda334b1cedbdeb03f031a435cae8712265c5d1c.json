{
    "author": "vwbaker",
    "message": "Reverts 0fbb870fa35d31fc2a7ddb66b41a2e0e82707466\n\nPiperOrigin-RevId: 799591091",
    "sha": "dda334b1cedbdeb03f031a435cae8712265c5d1c",
    "files": [
        {
            "sha": "ba3197e1c1f08b6ba0f04be14ef81291715dcdab",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=dda334b1cedbdeb03f031a435cae8712265c5d1c",
            "patch": "@@ -91,7 +91,6 @@ xla_test(\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n-        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n@@ -612,21 +611,14 @@ xla_test(\n         \":native_emitter\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/ir:hlo_module_group\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/service:compiler\",\n-        \"//xla/service:executable\",\n-        \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:nvptx_compiler\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n-        \"//xla/stream_executor:platform\",\n-        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n-        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "4dbf510fbc9438964a4d85c5a452988ffa26b6e1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 21,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=dda334b1cedbdeb03f031a435cae8712265c5d1c",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n-#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/substitute.h\"\n #include \"xla/autotuning.pb.h\"\n@@ -64,21 +63,20 @@ int CountTmaAllowed(\n class TritonBlockLevelFusionEmitterBackendTest\n     : public HloHardwareIndependentTestBase {\n  protected:\n-  TritonBlockLevelFusionEmitterBackendTest() {\n+  TritonBlockLevelFusionEmitterBackendTest()\n+      : backend_(PlatformUtil::GetDefaultPlatform()\n+                     .value()\n+                     ->ExecutorForDevice(0)\n+                     .value(),\n+                 &debug_options_, &compiler_) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n-    backend_ = std::make_unique<BlockLevelEmitterBackend>(\n-        PlatformUtil::GetDefaultPlatform()\n-            .value()\n-            ->ExecutorForDevice(0)\n-            .value(),\n-        &debug_options_, &compiler_);\n   }\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n-  std::unique_ptr<BlockLevelEmitterBackend> backend_;\n+  BlockLevelEmitterBackend backend_;\n };\n \n // Verifies that GetDefaultConfig correctly parses and returns the\n@@ -118,7 +116,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -162,7 +160,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -209,7 +207,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -258,7 +256,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -303,7 +301,7 @@ ENTRY %main {\n   // Call GetSupportedConfigs on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::vector<std::unique_ptr<BackendConfig>> configs,\n-      backend_->GetSupportedConfigs(\n+      backend_.GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction())));\n \n   // If device supports TMA, the backend should generate 70 combinations:\n@@ -315,7 +313,7 @@ ENTRY %main {\n   // The middle dimension (d1 = 1) must always have tile size 1.\n   //\n   // If device doesn't support TMA, we currently expect half the number (35).\n-  bool is_tma_supported = backend_->target_config()\n+  bool is_tma_supported = backend_.target_config()\n                               .device_description.cuda_compute_capability()\n                               .IsAtLeastHopper();\n   if (is_tma_supported) {\n@@ -387,7 +385,7 @@ backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n   // Call GetSupportedConfigs on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::vector<std::unique_ptr<BackendConfig>> configs,\n-      backend_->GetSupportedConfigs(\n+      backend_.GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction())));\n \n   // If device supports TMA, expect 40 total configurations:\n@@ -397,7 +395,7 @@ backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n   // The middle dimension (d1 = 0) must always have tile size 0.\n   //\n   // If device doesn't support TMA, we currently expect half the number (20).\n-  bool is_tma_supported = backend_->target_config()\n+  bool is_tma_supported = backend_.target_config()\n                               .device_description.cuda_compute_capability()\n                               .IsAtLeastHopper();\n   if (is_tma_supported) {\n@@ -466,7 +464,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -482,7 +480,7 @@ ENTRY %main {\n               )pb\"));\n \n   // Apply the generated config to the fusion instruction.\n-  EXPECT_THAT(backend_->ApplyConfig(*instr, *config), absl_testing::IsOk());\n+  EXPECT_THAT(backend_.ApplyConfig(*instr, *config), absl_testing::IsOk());\n   TF_ASSERT_OK_AND_ASSIGN(GpuBackendConfig gpu_backend_config,\n                           instr->backend_config<GpuBackendConfig>());\n   // Ensure that the backend config on the instruction matches what was applied.\n@@ -524,10 +522,10 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Attempt to compile the root instruction using the retrieved backend config.\n-  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_->Compile(\n+  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_.Compile(\n       *(module->entry_computation()->root_instruction()), *config);\n   // Verify that compilation succeeded and returned a valid executable.\n   EXPECT_THAT(executable, absl_testing::IsOk());"
        },
        {
            "sha": "0f81ec1407266faba5d5dae62c60744b6528c61e",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=dda334b1cedbdeb03f031a435cae8712265c5d1c",
            "patch": "@@ -73,7 +73,6 @@ class GpuCodegenBackend : public CodegenBackend {\n         false);\n \n     Compiler::CompileOptions options;\n-    options.is_autotuning_compilation = true;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));\n     return compiler_->RunBackend(std::move(optimized_module), stream_executor_,\n@@ -96,7 +95,7 @@ class GpuCodegenBackend : public CodegenBackend {\n   std::string name_;\n   stream_executor::StreamExecutor* stream_executor_;\n   Compiler::TargetConfig target_config_;\n-  const DebugOptions debug_options_;\n+  const DebugOptions& debug_options_;\n   // TODO(b/407494653): remove compiler when we don't need to run any HLO passes\n   // and the codegen backend can directly produce an executable without a\n   // compiler instance."
        },
        {
            "sha": "79699e4aaab49f0e574d5a5f9938d752454208a7",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=dda334b1cedbdeb03f031a435cae8712265c5d1c",
            "patch": "@@ -22,19 +22,12 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n-#include \"absl/status/statusor.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/service/compiler.h\"\n-#include \"xla/service/executable.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n-#include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/platform_util.h\"\n-#include \"xla/stream_executor/platform.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -190,54 +183,6 @@ TEST_F(NativeEmitterBackendTest, CompileForDefaultConfig) {\n   EXPECT_THAT(maybe_executable, absl_testing::IsOk());\n }\n \n-class MockCompiler : public Compiler {\n- public:\n-  MOCK_METHOD(absl::StatusOr<std::unique_ptr<Executable>>, RunBackend,\n-              (std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n-               const CompileOptions& options),\n-              (override));\n-  MOCK_METHOD(se::Platform::Id, PlatformId, (), (const, override));\n-  MOCK_METHOD(absl::StatusOr<std::unique_ptr<HloModule>>, RunHloPasses,\n-              (std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n-               const CompileOptions& options),\n-              (override));\n-  MOCK_METHOD(absl::StatusOr<std::vector<std::unique_ptr<Executable>>>, Compile,\n-              (std::unique_ptr<HloModuleGroup> module_group,\n-               std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n-               const CompileOptions& options),\n-              (override));\n-  MOCK_METHOD(\n-      absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>,\n-      CompileAheadOfTime,\n-      (std::unique_ptr<HloModuleGroup> module_group,\n-       const AotCompilationOptions& options),\n-      (override));\n-  MOCK_METHOD(HloCostAnalysis::ShapeSizeFunction, ShapeSizeBytesFunction, (),\n-              (const, override));\n-};\n-\n-TEST_F(NativeEmitterBackendTest, CompileSetsIsAutotuningCompilationOption) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n-                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n-  auto fusion = reduction_module->entry_computation()->root_instruction();\n-  MockCompiler mock_compiler;\n-  NativeEmitterBackend backend(\n-      PlatformUtil::GetDefaultPlatform().value()->ExecutorForDevice(0).value(),\n-      &debug_options_, &mock_compiler);\n-  // Call GetDefaultConfig on the fusion instruction.\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n-                          backend.GetDefaultConfig(*(fusion)));\n-  EXPECT_CALL(\n-      mock_compiler,\n-      RunBackend(\n-          testing::_, testing::_,\n-          testing::Field(&Compiler::CompileOptions::is_autotuning_compilation,\n-                         true)))\n-      .WillOnce(testing::Return(std::unique_ptr<Executable>()));\n-  // Attempt to compile the fusion using the retrieved backend config.\n-  EXPECT_THAT(backend.Compile(*fusion, *config), absl_testing::IsOk());\n-}\n-\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "deb5f3a7f35905e36e0721127a00096b16a0acb9",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 17,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=dda334b1cedbdeb03f031a435cae8712265c5d1c",
            "patch": "@@ -69,34 +69,33 @@ const char kHlo[] = R\"(\n \n class TritonBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n-  TritonBackendTest() {\n+  TritonBackendTest()\n+      : backend_(PlatformUtil::GetDefaultPlatform()\n+                     .value()\n+                     ->ExecutorForDevice(0)\n+                     .value(),\n+                 &debug_options_, &compiler_) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n-    backend_ =\n-        std::make_unique<TritonBackend>(PlatformUtil::GetDefaultPlatform()\n-                                            .value()\n-                                            ->ExecutorForDevice(0)\n-                                            .value(),\n-                                        &debug_options_, &compiler_);\n   }\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n-  std::unique_ptr<TritonBackend> backend_;\n+  TritonBackend backend_;\n };\n \n TEST_F(TritonBackendTest, GetSupportedConfigs) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kHlo));\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n-      backend_->GetSupportedConfigs(\n+      backend_.GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction()));\n   EXPECT_THAT(configs, absl_testing::IsOk());\n   EXPECT_GT(configs.value().size(), 0);\n \n-  if (backend_->target_config()\n+  if (backend_.target_config()\n           .device_description.cuda_compute_capability()\n           .IsAtLeastHopper()) {\n     auto count_tma_allowed =\n@@ -120,11 +119,11 @@ TEST_F(TritonBackendTest, GetSupportedConfigsRestrictedDefaultSearch) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kHlo));\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> default_configs =\n-      backend_->GetSupportedConfigs(\n+      backend_.GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction()));\n   debug_options_.set_xla_gpu_exhaustive_tiling_search(true);\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n-      exhaustive_configs = backend_->GetSupportedConfigs(\n+      exhaustive_configs = backend_.GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction()));\n   EXPECT_THAT(default_configs, IsOk());\n   EXPECT_THAT(exhaustive_configs, IsOk());\n@@ -139,7 +138,7 @@ TEST_F(TritonBackendTest, GetSupportedConfigsForUnsupportedInstruction) {\n                                           ->called_computations()[0]\n                                           ->root_instruction();\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n-      backend_->GetSupportedConfigs(*unsupported_instr);\n+      backend_.GetSupportedConfigs(*unsupported_instr);\n   EXPECT_THAT(configs, absl_testing::IsOk());\n   EXPECT_THAT(configs.value(), testing::IsEmpty());\n }\n@@ -151,7 +150,7 @@ TEST_F(TritonBackendTest, GetDefaultConfig) {\n       TritonGemmConfig(64, 64, 64, 1, 1, 2, 1, false).ToProto();\n \n   absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction()));\n \n   EXPECT_THAT(config, absl_testing::IsOk());\n@@ -168,7 +167,7 @@ TEST_F(TritonBackendTest, GetDefaultConfigForUnsupportedInstruction) {\n                                           ->called_computations()[0]\n                                           ->root_instruction();\n   absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n-      backend_->GetDefaultConfig(*unsupported_instr);\n+      backend_.GetDefaultConfig(*unsupported_instr);\n   EXPECT_THAT(config.status(), StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n@@ -177,9 +176,9 @@ TEST_F(TritonBackendTest, Compile) {\n                           ParseAndReturnVerifiedModule(kHlo));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_->GetDefaultConfig(\n+      backend_.GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n-  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_->Compile(\n+  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_.Compile(\n       *(module->entry_computation()->root_instruction()), *config);\n   EXPECT_THAT(executable, absl_testing::IsOk());\n }"
        },
        {
            "sha": "571e4eb593a1c95813706065898270dcf511fb59",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dda334b1cedbdeb03f031a435cae8712265c5d1c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=dda334b1cedbdeb03f031a435cae8712265c5d1c",
            "patch": "@@ -2685,9 +2685,9 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n           /*alias_info=*/std::move(alias_info),\n           /*debug_options=*/std::move(debug_opts),\n           /*device_description=*/gpu_device_info,\n-          // TODO b/407494653: AutotunerPass requires module to compile.\n-          // Remove module on autotuning runs once this is fixed.\n-          /*debug_module=*/std::move(module),\n+          /*debug_module=*/options.is_autotuning_compilation\n+              ? std::unique_ptr<HloModule>()\n+              : std::move(module),\n           /*enable_debug_info_manager=*/!options.is_autotuning_compilation}));\n \n   if (embed_ir_in_executable) {"
        }
    ],
    "stats": {
        "total": 145,
        "additions": 39,
        "deletions": 106
    }
}