{
    "author": "derdrdirk",
    "message": "Enable deviceless cache lookups when autotuning gemms using the AutotunerPass.\n\nPiperOrigin-RevId: 811326638",
    "sha": "8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215",
    "files": [
        {
            "sha": "e8b0857c2d1c9f7ab406e6ee6590ae0bc6be8087",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215",
            "patch": "@@ -258,7 +258,7 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<AutotunerPass> autotuner_pass,\n       AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n-                            thread_pool, should_autotune,\n+                            thread_pool, should_autotune, target_config,\n                             options.device_allocator));\n   pipeline->AddPass(std::move(autotuner_pass));\n "
        },
        {
            "sha": "6054ac5e269d10b57c73013e6d5a6652627140c2",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215",
            "patch": "@@ -715,6 +715,7 @@ cc_library(\n         \"//xla/backends/gpu/autotuner:legacy_cache\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/service:compiler\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "946ff5851d756a2a43eea87f433f8002fe8e5b04",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 9,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/legacy_cache.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -78,24 +79,30 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     const DebugOptions& debug_options,\n     stream_executor::StreamExecutor* stream_executor,\n     tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n-    se::DeviceMemoryAllocator* allocator) {\n-  // At least one of stream_executor or allocator must be provided.\n-  CHECK(stream_executor != nullptr || allocator != nullptr);\n-\n-  std::unique_ptr<GpuProfiler> profiler = GpuProfiler::Create(\n-      stream_executor, GetProfileOptions(debug_options), allocator);\n+    const Compiler::TargetConfig* target_config,\n+    se::DeviceMemoryAllocator* allocator, bool cache_only) {\n+  std::unique_ptr<Profiler> profiler = nullptr;\n+  AutotuneConfig autotune_config = GetAutotuneConfig(debug_options);\n+  if (cache_only) {\n+    autotune_config.expect_all_instructions_in_cache = true;\n+  } else {\n+    // If not cache_only, at least one of stream_executor or allocator must be\n+    // provided.\n+    CHECK(stream_executor != nullptr || allocator != nullptr);\n+    profiler = GpuProfiler::Create(stream_executor,\n+                                   GetProfileOptions(debug_options), allocator);\n+  }\n \n   std::unique_ptr<AutotunerCacheInterface> cache =\n       std::make_unique<LegacyCache>(\n           debug_options.xla_gpu_experimental_autotuner_cache_dir(),\n           debug_options.xla_gpu_experimental_autotune_cache_mode(),\n-          stream_executor->GetDeviceDescription());\n+          target_config->device_description);\n \n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<Autotuner> autotuner,\n       Autotuner::Create(std::move(backends), std::move(profiler),\n-                        GetAutotuneConfig(debug_options), std::move(cache),\n-                        thread_pool));\n+                        autotune_config, std::move(cache), thread_pool));\n   return absl::WrapUnique(\n       new AutotunerPass(std::move(autotuner), should_autotune));\n }"
        },
        {
            "sha": "7c6caae1dcfa9fb2108f05a6bb4e790a34c8f6e6",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n@@ -37,11 +38,15 @@ namespace gpu {\n \n class AutotunerPass : public HloModulePass {\n  public:\n+  // If 'cache_only' is true, tuning is disabled and only cache lookups are\n+  // performed. In this mode, 'stream_executor' and 'allocator' can be null.\n+  // target_config must outlive the pass.\n   static absl::StatusOr<std::unique_ptr<AutotunerPass>> Create(\n       std::vector<std::unique_ptr<CodegenBackend>> backends,\n       const DebugOptions& debug_options, se::StreamExecutor* stream_executor,\n       tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n-      se::DeviceMemoryAllocator* allocator = nullptr);\n+      const Compiler::TargetConfig* target_config,\n+      se::DeviceMemoryAllocator* allocator = nullptr, bool cache_only = false);\n \n   absl::string_view name() const override { return \"autotuner\"; }\n "
        },
        {
            "sha": "d1598257be3516574df8fe6ef0301d94fbc8411f",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 13,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215",
            "patch": "@@ -116,7 +116,7 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n       AutotunerPass::Create(std::move(backends),\n                             module->config().debug_options(), stream_executor_,\n                             &thread_pool, IsCublasGemmInstruction,\n-                            allocator_.get()));\n+                            &target_config, allocator_.get()));\n   EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n               absl_testing::IsOkAndHolds(true));\n   // Verify that the backend config has been updated in the HLO.\n@@ -147,7 +147,8 @@ TEST_F(AutotunerPassTest, CublasGemmIsNotAutotunedWhenFilterReturnsFalse) {\n       std::unique_ptr<AutotunerPass> pass,\n       AutotunerPass::Create(std::move(backends),\n                             module->config().debug_options(), stream_executor_,\n-                            &thread_pool, should_autotune, allocator_.get()));\n+                            &thread_pool, should_autotune, &target_config,\n+                            allocator_.get()));\n   EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n               absl_testing::IsOkAndHolds(true));\n   // Verify that the backend config has *not* been updated in the HLO.\n@@ -182,10 +183,10 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n \n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass,\n-        AutotunerPass::Create(std::move(backends),\n-                              module->config().debug_options(),\n-                              stream_executor_, &thread_pool,\n-                              IsCublasGemmInstruction, allocator_.get()));\n+        AutotunerPass::Create(\n+            std::move(backends), module->config().debug_options(),\n+            stream_executor_, &thread_pool, IsCublasGemmInstruction,\n+            &target_config, allocator_.get()));\n     EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n                 absl_testing::IsOkAndHolds(true));\n   }\n@@ -216,22 +217,22 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n   {\n     std::vector<std::unique_ptr<CodegenBackend>> backends2;\n     backends2.push_back(std::make_unique<CublasBackend>(\n-        stream_executor_, &module->config().debug_options(), &compiler_,\n+        stream_executor_, &module_2->config().debug_options(), &compiler_,\n         &target_config));\n \n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass2,\n-        AutotunerPass::Create(std::move(backends2),\n-                              module->config().debug_options(),\n-                              stream_executor_, &thread_pool,\n-                              IsCublasGemmInstruction, allocator_.get()));\n-    EXPECT_THAT(pass2->Run(module.get(), /*execution_threads=*/{}),\n+        AutotunerPass::Create(\n+            std::move(backends2), module_2->config().debug_options(),\n+            stream_executor_, &thread_pool, IsCublasGemmInstruction,\n+            &target_config, allocator_.get()));\n+    EXPECT_THAT(pass2->Run(module_2.get(), /*execution_threads=*/{}),\n                 absl_testing::IsOkAndHolds(true));\n   }\n \n   // Verify that the backend config in the HLO matches the cache.\n   const HloInstruction* gemm =\n-      module->entry_computation()->GetInstructionWithName(\"custom-call.1\");\n+      module_2->entry_computation()->GetInstructionWithName(\"custom-call.1\");\n   TF_ASSERT_OK_AND_ASSIGN(auto hlo_gpu_backend_config,\n                           gemm->backend_config<GpuBackendConfig>());\n   const GemmBackendConfig& hlo_backend_config =\n@@ -243,6 +244,70 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n   // logged that the cache was hit, which is the main purpose of this test.\n }\n \n+TEST_F(AutotunerPassTest, CublasGemmIsAutotunedWithCacheOnly) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kCublasCustomCallHlo));\n+\n+  std::string cache_dir = ::testing::TempDir();\n+  module->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_experimental_autotuner_cache_dir(cache_dir);\n+\n+  tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n+                                      /*num_threads=*/4);\n+  GpuCompiler::TargetConfig target_config(stream_executor_);\n+\n+  // Run the pass for the first time, this should populate the cache.\n+  {\n+    std::vector<std::unique_ptr<CodegenBackend>> backends;\n+    backends.push_back(std::make_unique<CublasBackend>(\n+        stream_executor_, &module->config().debug_options(), &compiler_,\n+        &target_config));\n+\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::unique_ptr<AutotunerPass> pass,\n+        AutotunerPass::Create(\n+            std::move(backends), module->config().debug_options(),\n+            stream_executor_, &thread_pool, IsCublasGemmInstruction,\n+            &target_config, allocator_.get()));\n+    EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n+                absl_testing::IsOkAndHolds(true));\n+  }\n+\n+  // Run the pass on the same original HLO with cache_only=true.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module_2,\n+                          ParseAndReturnVerifiedModule(kCublasCustomCallHlo));\n+\n+  module_2->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_experimental_autotuner_cache_dir(cache_dir);\n+\n+  {\n+    std::vector<std::unique_ptr<CodegenBackend>> backends2;\n+    backends2.push_back(std::make_unique<CublasBackend>(\n+        stream_executor_, &module_2->config().debug_options(), &compiler_,\n+        &target_config));\n+\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::unique_ptr<AutotunerPass> pass2,\n+        AutotunerPass::Create(\n+            std::move(backends2), module_2->config().debug_options(),\n+            /*stream_executor=*/nullptr, &thread_pool, IsCublasGemmInstruction,\n+            &target_config, /*allocator=*/nullptr, /*cache_only=*/true));\n+    EXPECT_THAT(pass2->Run(module_2.get(), /*execution_threads=*/{}),\n+                absl_testing::IsOkAndHolds(true));\n+  }\n+\n+  // Verify that the backend config in the HLO matches the cache.\n+  const HloInstruction* gemm =\n+      module_2->entry_computation()->GetInstructionWithName(\"custom-call.1\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto hlo_gpu_backend_config,\n+                          gemm->backend_config<GpuBackendConfig>());\n+  const GemmBackendConfig& hlo_backend_config =\n+      hlo_gpu_backend_config.gemm_backend_config();\n+  EXPECT_TRUE(hlo_backend_config.has_selected_algorithm());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "55e112293cd8aaae28580fcffce9932356a5b4db",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=8f2bcf7a19e55fdc5ac5d20bb3525e95b12e0215",
            "patch": "@@ -365,11 +365,13 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n     return instruction.opcode() == HloOpcode::kCustomCall &&\n            IsCublasGemm(instruction);\n   };\n+\n+  bool cache_only = stream_exec == nullptr;\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<AutotunerPass> autotuner_pass,\n       AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n-                            thread_pool, should_autotune,\n-                            options.device_allocator));\n+                            thread_pool, should_autotune, target_config,\n+                            options.device_allocator, cache_only));\n   pipeline->AddPass(std::move(autotuner_pass));\n   return absl::OkStatus();\n }\n@@ -437,11 +439,12 @@ absl::Status NVPTXCompiler::AddFusionAutotuningPass(\n   backends.push_back(std::make_unique<NativeEmitterBackend>(\n       &debug_options, this, target_config));\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::unique_ptr<AutotunerPass> autotuner_pass,\n-      AutotunerPass::Create(std::move(backends), debug_options, stream_executor,\n-                            thread_pool, ShouldAutotuneBetweenFusionEmitters,\n-                            options.device_allocator));\n+  bool cache_only = stream_executor == nullptr;\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<AutotunerPass> autotuner_pass,\n+                      AutotunerPass::Create(\n+                          std::move(backends), debug_options, stream_executor,\n+                          thread_pool, ShouldAutotuneBetweenFusionEmitters,\n+                          target_config, options.device_allocator, cache_only));\n   pipeline->AddPass(std::move(autotuner_pass));\n   return absl::OkStatus();\n }"
        }
    ],
    "stats": {
        "total": 143,
        "additions": 112,
        "deletions": 31
    }
}