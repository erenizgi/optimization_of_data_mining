{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 814985597",
    "sha": "79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
    "files": [
        {
            "sha": "80f5d98d9f122f93830f59a8bd1ee7aa893f6267",
            "filename": "tensorflow/core/kernels/data/experimental/choose_fastest_branch_dataset_op.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fchoose_fastest_branch_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fchoose_fastest_branch_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fchoose_fastest_branch_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -77,7 +77,7 @@ class WrapperDataset : public DatasetBase {\n     bool error = iterator_created_;\n     iterator_created_ = true;\n     return std::make_unique<WrapperIterator>(\n-        WrapperIterator::Params{this, strings::StrCat(prefix, \"::Wrapper\")},\n+        WrapperIterator::Params{this, absl::StrCat(prefix, \"::Wrapper\")},\n         error);\n   }\n \n@@ -225,7 +225,7 @@ class ChooseFastestBranchDatasetOp : public UnaryDatasetOpKernel {\n         const string& prefix) const override {\n       return std::make_unique<ChooseFastestIterator>(\n           ChooseFastestIterator::Params{\n-              this, strings::StrCat(prefix, \"::ChooseFastestBranch\")});\n+              this, absl::StrCat(prefix, \"::ChooseFastestBranch\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -511,7 +511,7 @@ class ChooseFastestBranchDatasetOp : public UnaryDatasetOpKernel {\n \n         DatasetContext::Params params;\n         params.type_string = \"ChooseFastestBranch_Wrapper\";\n-        params.node_name = strings::StrCat(params.type_string, branch_index);\n+        params.node_name = absl::StrCat(params.type_string, branch_index);\n         DatasetBase* temp_dataset = new WrapperDataset(\n             std::move(params), &input_impl_->output_dtypes(),\n             &input_impl_->output_shapes(), input_impl_.get());\n@@ -524,7 +524,7 @@ class ChooseFastestBranchDatasetOp : public UnaryDatasetOpKernel {\n           DatasetContext::Params take_dataset_params;\n           take_dataset_params.type_string = \"ChooseFastestBranch_Take\";\n           take_dataset_params.node_name =\n-              strings::StrCat(take_dataset_params.type_string, branch_index);\n+              absl::StrCat(take_dataset_params.type_string, branch_index);\n           int64_t count = dataset()->num_elements_per_branch_ *\n                           dataset()->ratio_numerator_ /\n                           dataset()->ratio_denominator_;"
        },
        {
            "sha": "fde91e511cc653fd6c7911474b00a7c02a963bd3",
            "filename": "tensorflow/core/kernels/data/experimental/choose_fastest_dataset_op.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fchoose_fastest_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fchoose_fastest_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fchoose_fastest_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -142,7 +142,7 @@ class ChooseFastestDatasetOp : public DatasetOpKernel {\n         const string& prefix) const override {\n       return std::make_unique<ChooseFastestIterator>(\n           ChooseFastestIterator::Params{\n-              this, strings::StrCat(prefix, \"::ChooseFastest\")});\n+              this, absl::StrCat(prefix, \"::ChooseFastest\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -208,7 +208,7 @@ class ChooseFastestDatasetOp : public DatasetOpKernel {\n         for (size_t i = 0, num_inputs = dataset()->inputs_.size();\n              i < num_inputs; ++i) {\n           TF_RETURN_IF_ERROR(dataset()->inputs_[i]->MakeIterator(\n-              ctx, this, strings::StrCat(prefix(), \"[\", i, \"]\"),\n+              ctx, this, absl::StrCat(prefix(), \"[\", i, \"]\"),\n               &input_impls_[i]));\n         }\n         return absl::OkStatus();\n@@ -279,7 +279,7 @@ class ChooseFastestDatasetOp : public DatasetOpKernel {\n             reader->ReadScalar(full_name(\"fastest_index\"), &fastest_index_));\n         if (fastest_index_ != -1) {\n           TF_RETURN_IF_ERROR(dataset()->inputs_[fastest_index_]->MakeIterator(\n-              ctx, this, strings::StrCat(prefix(), \"[\", fastest_index_, \"]\"),\n+              ctx, this, absl::StrCat(prefix(), \"[\", fastest_index_, \"]\"),\n               &fastest_input_impl_));\n           TF_RETURN_IF_ERROR(RestoreInput(ctx, reader, fastest_input_impl_));\n         } else if (reader->Contains(full_name(\"input_impls_empty\"))) {\n@@ -322,7 +322,7 @@ class ChooseFastestDatasetOp : public DatasetOpKernel {\n              i < num_inputs; ++i) {\n           threads[i].result = std::make_unique<InvocationResult>();\n           threads[i].thread = ctx->StartThread(\n-              strings::StrCat(\"tf_data_merge_\", i),\n+              absl::StrCat(\"tf_data_merge_\", i),\n               std::bind(&ChooseFastestIterator::RunnerThread, this, ctx,\n                         threads[i].result.get(), i));\n         }"
        },
        {
            "sha": "e85605ecb5441b33c2cf464fc8757284b46433d5",
            "filename": "tensorflow/core/kernels/data/experimental/csv_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fcsv_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fcsv_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fcsv_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -189,7 +189,7 @@ class CSVDatasetOp : public DatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::CSV\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::CSV\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override { return out_type_; }"
        },
        {
            "sha": "8174d37b4c07b39f1c14f697e2d1bd0235217147",
            "filename": "tensorflow/core/kernels/data/experimental/data_service_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdata_service_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdata_service_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdata_service_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -193,7 +193,7 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {\n   absl::Status CheckExternalState() const override {\n     return absl::Status(\n         absl::StatusCode::kFailedPrecondition,\n-        strings::StrCat(DebugString(), \" does not yet support serialization.\"));\n+        absl::StrCat(DebugString(), \" does not yet support serialization.\"));\n   }\n \n   absl::Status InputDatasets("
        },
        {
            "sha": "c45905556dd2df1e8a81f50f3a0d9c0759547e5d",
            "filename": "tensorflow/core/kernels/data/experimental/data_service_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdata_service_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdata_service_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdata_service_ops.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -147,7 +147,7 @@ void RegisterDatasetOp::Compute(OpKernelContext* ctx) {\n                      dataset_def, metadata, requested_dataset_id, dataset_id);\n                },\n                /*description=*/\n-               strings::StrCat(\"register dataset with dispatcher at \", address),\n+               absl::StrCat(\"register dataset with dispatcher at \", address),\n                deadline_micros));\n \n   if (op_version_ >= 2) {"
        },
        {
            "sha": "a4508f864bd562afcf4fffa6b85e1bd6e9fb64f8",
            "filename": "tensorflow/core/kernels/data/experimental/dense_to_sparse_batch_dataset_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdense_to_sparse_batch_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdense_to_sparse_batch_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdense_to_sparse_batch_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -95,7 +95,7 @@ class DenseToSparseBatchDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(typename Iterator::Params{\n-          this, strings::StrCat(prefix, \"::DenseToSparseBatch\")});\n+          this, absl::StrCat(prefix, \"::DenseToSparseBatch\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -108,8 +108,8 @@ class DenseToSparseBatchDatasetOp : public UnaryDatasetOpKernel {\n     }\n \n     string DebugString() const override {\n-      return strings::StrCat(\"DenseToSparseBatchDatasetOp(\", batch_size_,\n-                             \")::Dataset\");\n+      return absl::StrCat(\"DenseToSparseBatchDatasetOp(\", batch_size_,\n+                          \")::Dataset\");\n     }\n \n     int64_t CardinalityInternal(CardinalityOptions options) const override {"
        },
        {
            "sha": "88db8cccef65bffddbb29764e1a67ff90d4253c4",
            "filename": "tensorflow/core/kernels/data/experimental/directed_interleave_dataset_op.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdirected_interleave_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdirected_interleave_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdirected_interleave_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -175,8 +175,8 @@ class DirectedInterleaveDatasetOp::Dataset : public DatasetBase {\n       for (size_t i = 0; i < data_input_impls_.size(); ++i) {\n         const DatasetBase* data_input = dataset()->data_inputs_[i];\n         TF_RETURN_IF_ERROR(data_input->MakeIterator(\n-            &input_contexts_[i + 1], this,\n-            strings::StrCat(prefix(), \"[\", i, \"]\"), &data_input_impls_[i]));\n+            &input_contexts_[i + 1], this, absl::StrCat(prefix(), \"[\", i, \"]\"),\n+            &data_input_impls_[i]));\n         ctx->MergeCheckpoint(input_contexts_[i + 1].checkpoint());\n       }\n       return absl::OkStatus();\n@@ -264,7 +264,7 @@ class DirectedInterleaveDatasetOp::Dataset : public DatasetBase {\n       for (size_t i = 0; i < data_input_impls_.size(); ++i) {\n         const auto& data_input_impl = data_input_impls_[i];\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            full_name(strings::StrCat(kDataInputImplEmpty, \"[\", i, \"]\")),\n+            full_name(absl::StrCat(kDataInputImplEmpty, \"[\", i, \"]\")),\n             static_cast<int64_t>(!data_input_impl)));\n         if (data_input_impl) {\n           TF_RETURN_IF_ERROR(SaveInput(ctx, writer, data_input_impl));\n@@ -286,7 +286,7 @@ class DirectedInterleaveDatasetOp::Dataset : public DatasetBase {\n       }\n       for (size_t i = 0; i < data_input_impls_.size(); ++i) {\n         TF_RETURN_IF_ERROR(reader->ReadScalar(\n-            full_name(strings::StrCat(kDataInputImplEmpty, \"[\", i, \"]\")),\n+            full_name(absl::StrCat(kDataInputImplEmpty, \"[\", i, \"]\")),\n             &input_empty));\n         if (!static_cast<bool>(input_empty)) {\n           TF_RETURN_IF_ERROR(RestoreInput(ctx, reader, data_input_impls_[i]));"
        },
        {
            "sha": "a85920c5d82b693daf3d3e12a3f791357c393930",
            "filename": "tensorflow/core/kernels/data/experimental/distributed_save_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdistributed_save_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdistributed_save_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fdistributed_save_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -110,7 +110,7 @@ void DistributedSaveOp::Compute(OpKernelContext* ctx) {\n       grpc_util::Retry(\n           [&]() { return client.Snapshot(dataset_def, directory, metadata); },\n           /*description=*/\n-          strings::StrCat(\"save with tf.data service dispatcher at \", address),\n+          absl::StrCat(\"save with tf.data service dispatcher at \", address),\n           deadline_micros));\n   metrics::RecordTFDataServiceSnapshotOp(directory, kDistributedSave);\n }"
        },
        {
            "sha": "2020a81080bfd29092df119d20cb2e35c55510fe",
            "filename": "tensorflow/core/kernels/data/experimental/group_by_reducer_dataset_op.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fgroup_by_reducer_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fgroup_by_reducer_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fgroup_by_reducer_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -99,7 +99,7 @@ class GroupByReducerDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::GroupByReducer\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::GroupByReducer\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -312,10 +312,10 @@ class GroupByReducerDatasetOp : public UnaryDatasetOpKernel {\n           for (auto it = states_.begin(); it != states_.end(); ++idx, ++it) {\n             int64_t key = it->first;\n             TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(\"states[\", idx, \"]->key\")), key));\n+                full_name(absl::StrCat(\"states[\", idx, \"]->key\")), key));\n             if (!it->second.empty()) {\n               TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                  full_name(strings::StrCat(\"states[\", idx, \"]->state_size\")),\n+                  full_name(absl::StrCat(\"states[\", idx, \"]->state_size\")),\n                   it->second.size()));\n               for (int j = 0; j < it->second.size(); ++j) {\n                 TF_RETURN_IF_ERROR(writer->WriteTensor(\n@@ -336,7 +336,7 @@ class GroupByReducerDatasetOp : public UnaryDatasetOpKernel {\n                 writer->WriteScalar(full_name(\"keys_size\"), keys_.size()));\n             for (int idx = 0; idx < keys_.size(); ++idx) {\n               TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                  full_name(strings::StrCat(\"keys[\", idx, \"]\")), keys_[idx]));\n+                  full_name(absl::StrCat(\"keys[\", idx, \"]\")), keys_[idx]));\n             }\n           }\n         }\n@@ -359,13 +359,13 @@ class GroupByReducerDatasetOp : public UnaryDatasetOpKernel {\n           for (int idx = 0; idx < size; ++idx) {\n             int64_t key;\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(\"states[\", idx, \"]->key\")), &key));\n+                full_name(absl::StrCat(\"states[\", idx, \"]->key\")), &key));\n             std::vector<Tensor> state;\n-            if (reader->Contains(full_name(\n-                    strings::StrCat(\"states[\", idx, \"]->state_size\")))) {\n+            if (reader->Contains(\n+                    full_name(absl::StrCat(\"states[\", idx, \"]->state_size\")))) {\n               int64_t state_size;\n               TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                  full_name(strings::StrCat(\"states[\", idx, \"]->state_size\")),\n+                  full_name(absl::StrCat(\"states[\", idx, \"]->state_size\")),\n                   &state_size));\n               state.resize(state_size);\n               for (int j = 0; j < state_size; ++j) {\n@@ -392,7 +392,7 @@ class GroupByReducerDatasetOp : public UnaryDatasetOpKernel {\n             for (int idx = 0; idx < size; ++idx) {\n               int64_t key;\n               TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                  full_name(strings::StrCat(\"keys[\", idx, \"]\")), &key));\n+                  full_name(absl::StrCat(\"keys[\", idx, \"]\")), &key));\n               keys_[idx] = key;\n             }\n           }"
        },
        {
            "sha": "057254a45c258f70a1522696e0e02ae0e425ec9b",
            "filename": "tensorflow/core/kernels/data/experimental/group_by_window_dataset_op.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fgroup_by_window_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fgroup_by_window_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fgroup_by_window_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -93,7 +93,7 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::GroupByWindow\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::GroupByWindow\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -334,10 +334,10 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n           for (auto it = groups_.begin(); it != groups_.end(); it++) {\n             int64_t key = it->first;\n             TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(\"groups_[\", idx, \"]->key\")), key));\n-            TF_RETURN_IF_ERROR(SaveGroup(\n-                writer, full_name(strings::StrCat(\"groups_[\", idx, \"]\")),\n-                it->second));\n+                full_name(absl::StrCat(\"groups_[\", idx, \"]->key\")), key));\n+            TF_RETURN_IF_ERROR(\n+                SaveGroup(writer, full_name(absl::StrCat(\"groups_[\", idx, \"]\")),\n+                          it->second));\n             idx++;\n           }\n         }\n@@ -350,10 +350,10 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n           for (auto it = window_sizes_.begin(); it != window_sizes_.end();\n                it++) {\n             TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(\"window_sizes_[\", idx, \"]->key\")),\n+                full_name(absl::StrCat(\"window_sizes_[\", idx, \"]->key\")),\n                 it->first));\n             TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(\"window_sizes_[\", idx, \"]->value\")),\n+                full_name(absl::StrCat(\"window_sizes_[\", idx, \"]->value\")),\n                 it->second));\n             idx++;\n           }\n@@ -389,10 +389,10 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n           for (int idx = 0; idx < size; idx++) {\n             int64_t key;\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(\"groups_[\", idx, \"]->key\")), &key));\n+                full_name(absl::StrCat(\"groups_[\", idx, \"]->key\")), &key));\n             std::vector<std::vector<Tensor>> group;\n             TF_RETURN_IF_ERROR(RestoreGroup(\n-                ctx, reader, full_name(strings::StrCat(\"groups_[\", idx, \"]\")),\n+                ctx, reader, full_name(absl::StrCat(\"groups_[\", idx, \"]\")),\n                 &group));\n             groups_[key] = group;\n           }\n@@ -406,10 +406,10 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n           for (int idx = 0; idx < size; idx++) {\n             int64_t key;\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(\"window_sizes_[\", idx, \"]->key\")),\n+                full_name(absl::StrCat(\"window_sizes_[\", idx, \"]->key\")),\n                 &key));\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(\"window_sizes_[\", idx, \"]->value\")),\n+                full_name(absl::StrCat(\"window_sizes_[\", idx, \"]->value\")),\n                 &window_sizes_[key]));\n           }\n         }\n@@ -439,10 +439,10 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n                              const std::vector<std::vector<Tensor>>& group)\n           TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n         TF_RETURN_IF_ERROR(\n-            writer->WriteScalar(strings::StrCat(name, \"_size\"), group.size()));\n+            writer->WriteScalar(absl::StrCat(name, \"_size\"), group.size()));\n         for (int i = 0; i < group.size(); i++) {\n           TF_RETURN_IF_ERROR(writer->WriteScalar(\n-              strings::StrCat(name, \"[\", i, \"]_size\"), group[i].size()));\n+              absl::StrCat(name, \"[\", i, \"]_size\"), group[i].size()));\n           for (int j = 0; j < group[i].size(); j++) {\n             TF_RETURN_IF_ERROR(writer->WriteTensor(\n                 strings::StrCat(name, \"[\", i, \"][\", j, \"]\"), group[i][j]));\n@@ -457,12 +457,12 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n           TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n         int64_t group_size;\n         TF_RETURN_IF_ERROR(\n-            reader->ReadScalar(strings::StrCat(name, \"_size\"), &group_size));\n+            reader->ReadScalar(absl::StrCat(name, \"_size\"), &group_size));\n         group->resize(group_size);\n         for (int i = 0; i < group_size; i++) {\n           int64_t vector_size;\n           TF_RETURN_IF_ERROR(reader->ReadScalar(\n-              strings::StrCat(name, \"[\", i, \"]_size\"), &vector_size));\n+              absl::StrCat(name, \"[\", i, \"]_size\"), &vector_size));\n           group->at(i).resize(vector_size);\n           for (int j = 0; j < vector_size; j++) {\n             TF_RETURN_IF_ERROR(reader->ReadTensor(\n@@ -523,7 +523,7 @@ class GroupByWindowDatasetOp : public UnaryDatasetOpKernel {\n         // Create an iterator for the dataset that was returned by `f`.\n         return returned_dataset->MakeIterator(\n             MakeNestedIteratorContext(ctx), this,\n-            strings::StrCat(prefix(), \"[\", group_counter_++, \"]\"),\n+            absl::StrCat(prefix(), \"[\", group_counter_++, \"]\"),\n             &current_group_iterator_);\n       }\n "
        },
        {
            "sha": "663bcfd3ef0eea40ac65b55ab4e4d7ab1a37d534",
            "filename": "tensorflow/core/kernels/data/experimental/ignore_errors_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fignore_errors_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fignore_errors_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fignore_errors_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -56,7 +56,7 @@ class IgnoreErrorsDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::IgnoreErrors\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::IgnoreErrors\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "7c100231238e940538b918d3931eb2328255f36e",
            "filename": "tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fmap_and_batch_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fmap_and_batch_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fmap_and_batch_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -628,23 +628,23 @@ class MapAndBatchDatasetOp::Dataset : public DatasetBase {\n       batch_results_.push_back(\n           std::make_shared<BatchResult>(dataset()->batch_size_, ctx));\n       std::shared_ptr<BatchResult> result = batch_results_.back();\n-      string batch_prefix = strings::StrCat(kBatchResults, \"_\", index);\n+      string batch_prefix = absl::StrCat(kBatchResults, \"_\", index);\n       mutex_lock l(result->mu);\n       result->end_of_input = reader->Contains(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kEndOfInput));\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kEndOfInput));\n       TF_RETURN_IF_ERROR(reader->ReadScalar(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kNumCalls),\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kNumCalls),\n           &result->num_calls));\n       TF_RETURN_IF_ERROR(reader->ReadScalar(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kNumElements),\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kNumElements),\n           &result->num_elements));\n       result->output_allocated = reader->Contains(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kOutputAllocated));\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kOutputAllocated));\n \n       TF_RETURN_IF_ERROR(ReadBatch(ctx, reader, dataset()->batch_size_,\n                                    prefix(), batch_prefix, &result->output));\n       TF_RETURN_IF_ERROR(ReadStatus(prefix(),\n-                                    strings::StrCat(batch_prefix, \"_\", kStatus),\n+                                    absl::StrCat(batch_prefix, \"_\", kStatus),\n                                     reader, &result->status));\n       if (result->output_allocated) {\n         RecordBufferEnqueue(ctx, result->output);\n@@ -655,30 +655,29 @@ class MapAndBatchDatasetOp::Dataset : public DatasetBase {\n     absl::Status WriteBatchResult(IteratorStateWriter* writer, size_t index)\n         TF_EXCLUSIVE_LOCKS_REQUIRED(*mu_) {\n       std::shared_ptr<BatchResult> result = batch_results_[index];\n-      string batch_prefix = strings::StrCat(kBatchResults, \"_\", index);\n+      string batch_prefix = absl::StrCat(kBatchResults, \"_\", index);\n       mutex_lock l(result->mu);\n       if (result->end_of_input) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            prefix(), strings::StrCat(batch_prefix, \"_\", kEndOfInput), \"\"));\n+            prefix(), absl::StrCat(batch_prefix, \"_\", kEndOfInput), \"\"));\n       }\n       TF_RETURN_IF_ERROR(writer->WriteScalar(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kNumCalls),\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kNumCalls),\n           result->num_calls));\n       TF_RETURN_IF_ERROR(writer->WriteScalar(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kNumElements),\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kNumElements),\n           result->num_elements));\n       if (result->output_allocated) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            prefix(), strings::StrCat(batch_prefix, \"_\", kOutputAllocated),\n-            \"\"));\n+            prefix(), absl::StrCat(batch_prefix, \"_\", kOutputAllocated), \"\"));\n       }\n \n       TF_RETURN_IF_ERROR(WriteBatch(dataset()->batch_size_,\n                                     result->num_elements, prefix(),\n                                     batch_prefix, writer, &result->output));\n-      TF_RETURN_IF_ERROR(\n-          WriteStatus(prefix(), strings::StrCat(batch_prefix, \"_\", kStatus),\n-                      result->status, writer));\n+      TF_RETURN_IF_ERROR(WriteStatus(prefix(),\n+                                     absl::StrCat(batch_prefix, \"_\", kStatus),\n+                                     result->status, writer));\n       return absl::OkStatus();\n     }\n "
        },
        {
            "sha": "246821e7af0beb55ee246e4c1e0e4988f91e7da4",
            "filename": "tensorflow/core/kernels/data/experimental/matching_files_dataset_op.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fmatching_files_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fmatching_files_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fmatching_files_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -64,7 +64,7 @@ class MatchingFilesDatasetOp : public DatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::MatchingFiles\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::MatchingFiles\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -219,11 +219,11 @@ class MatchingFilesDatasetOp : public DatasetOpKernel {\n           int i = 0;\n           while (!filepath_queue_.empty()) {\n             TF_RETURN_IF_ERROR(\n-                writer->WriteScalar(full_name(strings::StrCat(\"path_\", i)),\n+                writer->WriteScalar(full_name(absl::StrCat(\"path_\", i)),\n                                     filepath_queue_.top().first));\n-            TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(\"path_status_\", i)),\n-                filepath_queue_.top().second));\n+            TF_RETURN_IF_ERROR(\n+                writer->WriteScalar(full_name(absl::StrCat(\"path_status_\", i)),\n+                                    filepath_queue_.top().second));\n             filepath_queue_.pop();\n             i++;\n           }\n@@ -262,10 +262,10 @@ class MatchingFilesDatasetOp : public DatasetOpKernel {\n           for (int i = 0; i < queue_size; i++) {\n             tstring path;\n             int64_t path_status;\n+            TF_RETURN_IF_ERROR(\n+                reader->ReadScalar(full_name(absl::StrCat(\"path_\", i)), &path));\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(\"path_\", i)), &path));\n-            TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(\"path_status_\", i)), &path_status));\n+                full_name(absl::StrCat(\"path_status_\", i)), &path_status));\n             filepath_queue_.push(\n                 PathStatus(path, static_cast<bool>(path_status)));\n           }"
        },
        {
            "sha": "df86acee074d6a4e0a7b182d00331b83c786beb6",
            "filename": "tensorflow/core/kernels/data/experimental/non_serializable_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fnon_serializable_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fnon_serializable_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fnon_serializable_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -55,7 +55,7 @@ class NonSerializableDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::NonSerializable\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::NonSerializable\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "88e7f1528d4c8346efe39ce8d1d5bed2309d46e6",
            "filename": "tensorflow/core/kernels/data/experimental/parallel_interleave_dataset_op.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 27,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fparallel_interleave_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fparallel_interleave_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fparallel_interleave_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -449,15 +449,15 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n                                              interleave_indices_.size()));\n       for (int i = 0; i < interleave_indices_.size(); ++i) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            prefix(), strings::StrCat(kInterleaveIndices, \"_\", i),\n+            prefix(), absl::StrCat(kInterleaveIndices, \"_\", i),\n             interleave_indices_[i]));\n       }\n       TF_RETURN_IF_ERROR(\n           writer->WriteScalar(prefix(), kStagingSize, staging_indices_.size()));\n       for (int i = 0; i < staging_indices_.size(); ++i) {\n-        TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            prefix(), strings::StrCat(kStagingIndices, \"_\", i),\n-            staging_indices_[i]));\n+        TF_RETURN_IF_ERROR(\n+            writer->WriteScalar(prefix(), absl::StrCat(kStagingIndices, \"_\", i),\n+                                staging_indices_[i]));\n       }\n       if (!worker_threads_.empty()) {\n         TF_RETURN_IF_ERROR(\n@@ -530,7 +530,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n         for (int64_t i = 0; i < interleave_size; ++i) {\n           int64_t temp;\n           TF_RETURN_IF_ERROR(reader->ReadScalar(\n-              prefix(), strings::StrCat(kInterleaveIndices, \"_\", i), &temp));\n+              prefix(), absl::StrCat(kInterleaveIndices, \"_\", i), &temp));\n           if (temp >= 0 && all_indices.find(temp) != all_indices.end()) {\n             return errors::Internal(\n                 \"Duplicate entry for \", temp,\n@@ -551,7 +551,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n         for (int i = 0; i < staging_size; ++i) {\n           int64_t temp;\n           TF_RETURN_IF_ERROR(reader->ReadScalar(\n-              prefix(), strings::StrCat(kStagingIndices, \"_\", i), &temp));\n+              prefix(), absl::StrCat(kStagingIndices, \"_\", i), &temp));\n           if (all_indices.find(temp) != all_indices.end()) {\n             return errors::Internal(\n                 \"Duplicate entry for \", temp,\n@@ -570,7 +570,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n         for (size_t i = 0; i < dataset()->num_threads(); ++i) {\n           std::shared_ptr<IteratorContext> new_ctx(new IteratorContext(*ctx));\n           worker_threads_.emplace_back(ctx->StartThread(\n-              strings::StrCat(kDataParallelInterleaveWorker, \"_\", i),\n+              absl::StrCat(kDataParallelInterleaveWorker, \"_\", i),\n               [this, new_ctx, i]() { WorkerThread(new_ctx, i); }));\n         }\n       }\n@@ -690,7 +690,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n           workers_[i].SetInputs(s, std::move(args));\n           std::shared_ptr<IteratorContext> new_ctx(new IteratorContext(*ctx));\n           worker_threads_.push_back(ctx->StartThread(\n-              strings::StrCat(kDataParallelInterleaveWorker, \"_\", i),\n+              absl::StrCat(kDataParallelInterleaveWorker, \"_\", i),\n               [this, new_ctx, i]() { WorkerThread(new_ctx, i); }));\n         }\n         DCHECK(interleave_indices_.size() == dataset()->cycle_length_);\n@@ -955,15 +955,15 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n                                              workers_[index].input.size()));\n       for (int i = 0; i < workers_[index].input.size(); ++i) {\n         TF_RETURN_IF_ERROR(writer->WriteTensor(iterator_name,\n-                                               strings::StrCat(kInput, \"_\", i),\n+                                               absl::StrCat(kInput, \"_\", i),\n                                                workers_[index].input[i]));\n       }\n       TF_RETURN_IF_ERROR(writer->WriteScalar(iterator_name, kOutputsSize,\n                                              workers_[index].outputs.size()));\n       for (int i = 0; i < workers_[index].outputs.size(); ++i) {\n         TF_RETURN_IF_ERROR(WriteOutputElemLocked(\n             writer, workers_[index].outputs[i], iterator_name,\n-            strings::StrCat(kOutputs, \"_\", i)));\n+            absl::StrCat(kOutputs, \"_\", i)));\n       }\n       if (workers_[index].is_producing) {\n         TF_RETURN_IF_ERROR(\n@@ -985,7 +985,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n       for (int i = 0; i < input_size; ++i) {\n         workers_[index].input.emplace_back();\n         TF_RETURN_IF_ERROR(reader->ReadTensor(ctx->flr(), worker_prefix,\n-                                              strings::StrCat(kInput, \"_\", i),\n+                                              absl::StrCat(kInput, \"_\", i),\n                                               &workers_[index].input.back()));\n       }\n       int64_t outputs_size;\n@@ -995,7 +995,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n         workers_[index].outputs.emplace_back(absl::OkStatus());\n         TF_RETURN_IF_ERROR(ReadOutputElemLocked(\n             ctx, reader, &workers_[index].outputs.back(), worker_prefix,\n-            strings::StrCat(kOutputs, \"_\", i)));\n+            absl::StrCat(kOutputs, \"_\", i)));\n       }\n       if (reader->Contains(worker_prefix, kIsProducing)) {\n         workers_[index].is_producing = true;\n@@ -1023,7 +1023,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n                               worker_thread_states_[index].input.size()));\n       for (int i = 0; i < worker_thread_states_[index].input.size(); ++i) {\n         TF_RETURN_IF_ERROR(\n-            writer->WriteTensor(iterator_name, strings::StrCat(kInput, \"_\", i),\n+            writer->WriteTensor(iterator_name, absl::StrCat(kInput, \"_\", i),\n                                 worker_thread_states_[index].input[i]));\n       }\n       TF_RETURN_IF_ERROR(WriteStatusLocked(\n@@ -1053,7 +1053,7 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n       for (int i = 0; i < input_size; ++i) {\n         state->input.emplace_back();\n         TF_RETURN_IF_ERROR(reader->ReadTensor(ctx->flr(), worker_prefix,\n-                                              strings::StrCat(kInput, \"_\", i),\n+                                              absl::StrCat(kInput, \"_\", i),\n                                               &state->input.back()));\n       }\n       // Restore iterator\n@@ -1086,11 +1086,11 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n                                        const string& iterator_name,\n                                        const string& prefix)\n         TF_EXCLUSIVE_LOCKS_REQUIRED(mu_, ckpt_mu_) {\n-      TF_RETURN_IF_ERROR(WriteStatusLocked(\n-          writer, iterator_name, strings::StrCat(prefix, \"_\", kStatus),\n-          output_elem.status));\n+      TF_RETURN_IF_ERROR(WriteStatusLocked(writer, iterator_name,\n+                                           absl::StrCat(prefix, \"_\", kStatus),\n+                                           output_elem.status));\n       TF_RETURN_IF_ERROR(writer->WriteScalar(\n-          iterator_name, strings::StrCat(prefix, \"_\", kOutputSize),\n+          iterator_name, absl::StrCat(prefix, \"_\", kOutputSize),\n           output_elem.output.size()));\n       for (int i = 0; i < output_elem.output.size(); ++i) {\n         TF_RETURN_IF_ERROR(writer->WriteTensor(\n@@ -1106,12 +1106,11 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n                                       const string& iterator_name,\n                                       const string& prefix) {\n       TF_RETURN_IF_ERROR(ReadStatusLocked(reader, iterator_name,\n-                                          strings::StrCat(prefix, \"_\", kStatus),\n+                                          absl::StrCat(prefix, \"_\", kStatus),\n                                           &output_elem->status));\n       int64_t output_size;\n       TF_RETURN_IF_ERROR(reader->ReadScalar(\n-          iterator_name, strings::StrCat(prefix, \"_\", kOutputSize),\n-          &output_size));\n+          iterator_name, absl::StrCat(prefix, \"_\", kOutputSize), &output_size));\n       output_elem->output.reserve(output_size);\n       for (int i = 0; i < output_size; ++i) {\n         output_elem->output.emplace_back();\n@@ -1128,12 +1127,12 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n                                    const string& prefix,\n                                    const absl::Status& status)\n         TF_EXCLUSIVE_LOCKS_REQUIRED(mu_, ckpt_mu_) {\n-      TF_RETURN_IF_ERROR(writer->WriteScalar(\n-          iterator_name, strings::StrCat(prefix, \"_\", kCode),\n-          static_cast<int64_t>(status.code())));\n+      TF_RETURN_IF_ERROR(\n+          writer->WriteScalar(iterator_name, absl::StrCat(prefix, \"_\", kCode),\n+                              static_cast<int64_t>(status.code())));\n       if (!status.ok()) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            iterator_name, strings::StrCat(prefix, \"_\", KMessage),\n+            iterator_name, absl::StrCat(prefix, \"_\", KMessage),\n             std::string(status.message())));\n       }\n       return absl::OkStatus();\n@@ -1144,13 +1143,13 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n                                   const string& prefix, absl::Status* status) {\n       int64_t code_int;\n       TF_RETURN_IF_ERROR(reader->ReadScalar(\n-          iterator_name, strings::StrCat(prefix, \"_\", kCode), &code_int));\n+          iterator_name, absl::StrCat(prefix, \"_\", kCode), &code_int));\n       absl::StatusCode code = static_cast<absl::StatusCode>(code_int);\n \n       if (code != absl::StatusCode::kOk) {\n         tstring error_message;\n         TF_RETURN_IF_ERROR(reader->ReadScalar(\n-            iterator_name, strings::StrCat(prefix, \"_\", KMessage),\n+            iterator_name, absl::StrCat(prefix, \"_\", KMessage),\n             &error_message));\n         *status = absl::Status(code, error_message);\n       } else {"
        },
        {
            "sha": "54e72b7ac08df4387f3958e82c9449828a50e580",
            "filename": "tensorflow/core/kernels/data/experimental/parse_example_dataset_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fparse_example_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fparse_example_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fparse_example_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -448,7 +448,7 @@ class ParseExampleDatasetOp : public UnaryDatasetOpKernel {\n         }\n         TF_RETURN_IF_ERROR(SaveInput(ctx, writer, input_impl_));\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            full_name(strings::StrCat(kInvocationResults, kSizeSuffix)),\n+            full_name(absl::StrCat(kInvocationResults, kSizeSuffix)),\n             invocation_results_.size()));\n         for (size_t i = 0; i < invocation_results_.size(); i++) {\n           const auto& result = *(invocation_results_[i]);\n@@ -479,7 +479,7 @@ class ParseExampleDatasetOp : public UnaryDatasetOpKernel {\n         TF_RETURN_IF_ERROR(RestoreInput(ctx, reader, input_impl_));\n         int64_t invocation_results_size;\n         TF_RETURN_IF_ERROR(reader->ReadScalar(\n-            full_name(strings::StrCat(kInvocationResults, kSizeSuffix)),\n+            full_name(absl::StrCat(kInvocationResults, kSizeSuffix)),\n             &invocation_results_size));\n         if (!invocation_results_.empty()) invocation_results_.clear();\n         for (size_t i = 0; i < invocation_results_size; i++) {\n@@ -495,7 +495,7 @@ class ParseExampleDatasetOp : public UnaryDatasetOpKernel {\n                 &size));\n             num_return_values = static_cast<size_t>(size);\n             if (num_return_values != size) {\n-              return errors::InvalidArgument(strings::StrCat(\n+              return errors::InvalidArgument(absl::StrCat(\n                   full_name(strings::StrCat(kInvocationResults, \"[\", i, \"]\",\n                                             kSizeSuffix)),\n                   \": \", size, \" is not a valid value of type size_t.\"));"
        },
        {
            "sha": "01eba9c38c7455f609109ec506271d8daef47bb5",
            "filename": "tensorflow/core/kernels/data/experimental/random_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Frandom_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Frandom_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Frandom_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -91,7 +91,7 @@ class RandomDatasetOp::Dataset : public DatasetBase {\n   std::unique_ptr<IteratorBase> MakeIteratorInternal(\n       const string& prefix) const override {\n     return std::make_unique<Iterator>(\n-        Iterator::Params{this, strings::StrCat(prefix, \"::Random\")},\n+        Iterator::Params{this, absl::StrCat(prefix, \"::Random\")},\n         manager_->get().get());\n   }\n "
        },
        {
            "sha": "c3207ba4501d9f3fee322707ede5f3b5a0737bf7",
            "filename": "tensorflow/core/kernels/data/experimental/rebatch_dataset_op.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Frebatch_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Frebatch_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Frebatch_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -205,9 +205,9 @@ class RebatchDatasetOp : public UnaryDatasetOpKernel {\n         if (slice_number_ % dataset()->num_replicas_ != 0) {\n           // Save state of input tensors.\n           for (int i = 0; i < input_descriptors_.size(); ++i) {\n-            TF_RETURN_IF_ERROR(writer->WriteTensor(\n-                full_name(strings::StrCat(\"tensors[\", i, \"]\")),\n-                input_descriptors_[i].whole_tensor));\n+            TF_RETURN_IF_ERROR(\n+                writer->WriteTensor(full_name(absl::StrCat(\"tensors[\", i, \"]\")),\n+                                    input_descriptors_[i].whole_tensor));\n           }\n         }\n         return absl::OkStatus();\n@@ -229,7 +229,7 @@ class RebatchDatasetOp : public UnaryDatasetOpKernel {\n         if (slice_number_ % dataset()->num_replicas_ != 0) {\n           for (int i = 0; i < input_descriptors_.size(); ++i) {\n             TF_RETURN_IF_ERROR(reader->ReadTensor(\n-                ctx->flr(), full_name(strings::StrCat(\"tensors[\", i, \"]\")),\n+                ctx->flr(), full_name(absl::StrCat(\"tensors[\", i, \"]\")),\n                 &input_descriptors_[i].whole_tensor));\n             input_descriptors_[i].original_batch_dim =\n                 input_descriptors_[i].whole_tensor.dim_size(0);\n@@ -587,7 +587,7 @@ class RebatchDatasetV2Op : public UnaryDatasetOpKernel {\n         if (offset_ != -1) {\n           for (int i = 0; i < tensors_.size(); ++i) {\n             TF_RETURN_IF_ERROR(writer->WriteTensor(\n-                full_name(strings::StrCat(\"tensors[\", i, \"]\")), tensors_[i]));\n+                full_name(absl::StrCat(\"tensors[\", i, \"]\")), tensors_[i]));\n           }\n         }\n         return absl::OkStatus();\n@@ -610,7 +610,7 @@ class RebatchDatasetV2Op : public UnaryDatasetOpKernel {\n           tensors_.resize(dataset()->output_dtypes().size());\n           for (int i = 0; i < tensors_.size(); ++i) {\n             TF_RETURN_IF_ERROR(reader->ReadTensor(\n-                ctx->flr(), full_name(strings::StrCat(\"tensors[\", i, \"]\")),\n+                ctx->flr(), full_name(absl::StrCat(\"tensors[\", i, \"]\")),\n                 &tensors_[i]));\n           }\n         }"
        },
        {
            "sha": "d4769f559793bea07ebb53bdcaa250f89c9b765c",
            "filename": "tensorflow/core/kernels/data/experimental/scan_dataset_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fscan_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fscan_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fscan_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -94,7 +94,7 @@ class ScanDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::Scan\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::Scan\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -271,7 +271,7 @@ class ScanDatasetOp : public UnaryDatasetOpKernel {\n             writer->WriteScalar(full_name(\"state_size\"), state_.size()));\n         for (int idx = 0; idx < state_.size(); idx++) {\n           TF_RETURN_IF_ERROR(writer->WriteTensor(\n-              full_name(strings::StrCat(\"state[\", idx, \"]\")), state_[idx]));\n+              full_name(absl::StrCat(\"state[\", idx, \"]\")), state_[idx]));\n         }\n         return absl::OkStatus();\n       }\n@@ -285,7 +285,7 @@ class ScanDatasetOp : public UnaryDatasetOpKernel {\n         state_.resize(size);\n         for (int idx = 0; idx < size; idx++) {\n           TF_RETURN_IF_ERROR(reader->ReadTensor(\n-              ctx->flr(), full_name(strings::StrCat(\"state[\", idx, \"]\")),\n+              ctx->flr(), full_name(absl::StrCat(\"state[\", idx, \"]\")),\n               &state_[idx]));\n         }\n         return absl::OkStatus();"
        },
        {
            "sha": "96dc117fe5103aef5d8f50fddd36dc2a75e1028e",
            "filename": "tensorflow/core/kernels/data/experimental/set_stats_aggregator_dataset_op.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fset_stats_aggregator_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fset_stats_aggregator_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fset_stats_aggregator_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -51,11 +51,11 @@ class StatsAggregatorWithTagAndPrefix : public StatsAggregator {\n   void IncrementCounter(const string& name, const string& label,\n                         int64_t val) override {\n     if (!prefix_.empty()) {\n-      wrapped_->IncrementCounter(\n-          strings::StrCat(prefix_, \"/\", TaggedName(name)), label, val);\n+      wrapped_->IncrementCounter(absl::StrCat(prefix_, \"/\", TaggedName(name)),\n+                                 label, val);\n     } else {\n-      wrapped_->IncrementCounter(\n-          strings::StrCat(\"/tensorflow/\", TaggedName(name)), label, val);\n+      wrapped_->IncrementCounter(absl::StrCat(\"/tensorflow/\", TaggedName(name)),\n+                                 label, val);\n     }\n   }\n \n@@ -67,7 +67,7 @@ class StatsAggregatorWithTagAndPrefix : public StatsAggregator {\n  private:\n   string TaggedName(const string& name) const {\n     if (!tag_.empty()) {\n-      string tagged_name = strings::StrCat(tag_, stats_utils::kDelimiter, name);\n+      string tagged_name = absl::StrCat(tag_, stats_utils::kDelimiter, name);\n       return tagged_name;\n     }\n     return name;\n@@ -125,8 +125,8 @@ class SetStatsAggregatorDatasetOp : public UnaryDatasetOpKernel {\n \n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n-      return std::make_unique<Iterator>(Iterator::Params{\n-          this, strings::StrCat(prefix, \"::SetStatsAggregator\")});\n+      return std::make_unique<Iterator>(\n+          Iterator::Params{this, absl::StrCat(prefix, \"::SetStatsAggregator\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "b765f96d60e71c8211c3cffb3d4b0b1869490f88",
            "filename": "tensorflow/core/kernels/data/experimental/sleep_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsleep_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsleep_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsleep_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -62,7 +62,7 @@ class SleepDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::Sleep\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::Sleep\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "aec04aed85c11429adc03465cd9641ca27df8922",
            "filename": "tensorflow/core/kernels/data/experimental/sliding_window_dataset_op.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsliding_window_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsliding_window_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsliding_window_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -102,7 +102,7 @@ class SlidingWindowDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::Slide\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::Slide\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -277,11 +277,10 @@ class SlidingWindowDatasetOp : public UnaryDatasetOpKernel {\n           TF_RETURN_IF_ERROR(SaveInput(ctx, writer, input_impl_));\n         }\n         // Save buffer.\n-        TF_RETURN_IF_ERROR(writer->WriteScalar(strings::StrCat(\"buffer_size\"),\n-                                               buffer_.size()));\n+        TF_RETURN_IF_ERROR(writer->WriteScalar(\"buffer_size\", buffer_.size()));\n         for (int64_t i = 0; i < buffer_.size(); i++) {\n           TF_RETURN_IF_ERROR(writer->WriteScalar(\n-              strings::StrCat(\"buffer[\", i, \"]_size\"), buffer_[i].size()));\n+              absl::StrCat(\"buffer[\", i, \"]_size\"), buffer_[i].size()));\n           for (int64_t j = 0; j < buffer_[i].size(); j++) {\n             TF_RETURN_IF_ERROR(writer->WriteTensor(\n                 strings::StrCat(\"buffer[\", i, \"][\", j, \"]\"), buffer_[i][j]));\n@@ -300,13 +299,12 @@ class SlidingWindowDatasetOp : public UnaryDatasetOpKernel {\n         }\n         // Restore buffer.\n         int64_t buffer_size = 0;\n-        TF_RETURN_IF_ERROR(\n-            reader->ReadScalar(strings::StrCat(\"buffer_size\"), &buffer_size));\n+        TF_RETURN_IF_ERROR(reader->ReadScalar(\"buffer_size\", &buffer_size));\n         buffer_.resize(buffer_size);\n         for (int64_t i = 0; i < buffer_size; i++) {\n           int64_t vector_size;\n           TF_RETURN_IF_ERROR(reader->ReadScalar(\n-              strings::StrCat(\"buffer[\", i, \"]_size\"), &vector_size));\n+              absl::StrCat(\"buffer[\", i, \"]_size\"), &vector_size));\n           buffer_[i].resize(vector_size);\n           for (int64_t j = 0; j < vector_size; j++) {\n             TF_RETURN_IF_ERROR(reader->ReadTensor("
        },
        {
            "sha": "c3b2660c1ead91c62925c5efbf924870e3fd5d36",
            "filename": "tensorflow/core/kernels/data/experimental/snapshot_dataset_op.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsnapshot_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsnapshot_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsnapshot_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -366,8 +366,7 @@ class SnapshotDatasetV2Op::Dataset : public DatasetBase {\n           dataset()->shard_func_->Instantiate(ctx, &instantiated_shard_func_));\n \n       return dataset()->input_->MakeIterator(\n-          ctx, this, strings::StrCat(prefix(), \"::WriterIterator\"),\n-          &input_impl_);\n+          ctx, this, absl::StrCat(prefix(), \"::WriterIterator\"), &input_impl_);\n     }\n \n     absl::Status GetNextInternal(IteratorContext* ctx,\n@@ -513,8 +512,8 @@ class SnapshotDatasetV2Op::Dataset : public DatasetBase {\n \n       experimental::SnapshotMetadataRecord metadata;\n       metadata.set_creation_timestamp(EnvTime::NowMicros());\n-      metadata.set_graph_hash(strings::StrCat(dataset()->hash_));\n-      metadata.set_run_id(strings::StrCat(run_id_));\n+      metadata.set_graph_hash(absl::StrCat(dataset()->hash_));\n+      metadata.set_run_id(absl::StrCat(run_id_));\n       metadata.set_version(kFileFormatVersion);\n       for (const auto& output_dtype : dataset()->output_dtypes()) {\n         metadata.add_dtype(output_dtype);\n@@ -1108,9 +1107,9 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n         if (dataset()->snapshot_name_.empty()) {\n           hash_dir_ = io::JoinPath(dataset()->dir_, dataset()->graph_hash_);\n         } else {\n-          hash_dir_ = io::JoinPath(\n-              dataset()->dir_,\n-              strings::StrCat(\"custom-\", dataset()->snapshot_name_));\n+          hash_dir_ =\n+              io::JoinPath(dataset()->dir_,\n+                           absl::StrCat(\"custom-\", dataset()->snapshot_name_));\n         }\n       }\n \n@@ -1393,16 +1392,16 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n           TF_RETURN_IF_ERROR(\n               writer->WriteScalar(full_name(kVersionStr), version_));\n           TF_RETURN_IF_ERROR(writer->WriteScalar(\n-              full_name(strings::StrCat(kFilenames, kSizeSuffix)),\n+              full_name(absl::StrCat(kFilenames, kSizeSuffix)),\n               filenames_.size()));\n           for (size_t i = 0; i < filenames_.size(); ++i) {\n             TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(kFilenames, \"[\", i, \"]\")),\n+                full_name(absl::StrCat(kFilenames, \"[\", i, \"]\")),\n                 filenames_[i]));\n           }\n           for (auto i = 0; i < dataset()->num_reader_threads_; ++i) {\n             TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(kCurrentFilenames, \"[\", i, \"]\")),\n+                full_name(absl::StrCat(kCurrentFilenames, \"[\", i, \"]\")),\n                 curr_filenames_[i]));\n           }\n           TF_RETURN_IF_ERROR(writer->WriteScalar(full_name(kElementsProduced),\n@@ -1442,14 +1441,14 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n           for (auto i = 0; i < dataset()->num_reader_threads_; ++i) {\n             curr_filenames_.emplace_back();\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(kCurrentFilenames, \"[\", i, \"]\")),\n+                full_name(absl::StrCat(kCurrentFilenames, \"[\", i, \"]\")),\n                 &curr_filenames_.back()));\n           }\n           size_t filenames_size;\n           {\n             int64_t temp;\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(kFilenames, kSizeSuffix)), &temp));\n+                full_name(absl::StrCat(kFilenames, kSizeSuffix)), &temp));\n             filenames_size = static_cast<size_t>(temp);\n           }\n           if (filenames_.size() != filenames_size) {\n@@ -1461,7 +1460,7 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n           for (size_t i = 0; i < filenames_size; ++i) {\n             filenames_.emplace_back();\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(kFilenames, \"[\", i, \"]\")),\n+                full_name(absl::StrCat(kFilenames, \"[\", i, \"]\")),\n                 &filenames_.back()));\n           }\n           {\n@@ -1811,8 +1810,7 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n           TF_RETURN_IF_ERROR(writer->WriteScalar(full_name(kElementsProduced),\n                                                  elements_produced_));\n           TF_RETURN_IF_ERROR(writer->WriteScalar(\n-              full_name(strings::StrCat(kBuffer, kSizeSuffix)),\n-              buffer_.size()));\n+              full_name(absl::StrCat(kBuffer, kSizeSuffix)), buffer_.size()));\n           for (size_t i = 0; i < buffer_.size(); ++i) {\n             auto& buffer_element = buffer_[i];\n             if (buffer_element.end_of_sequence) {\n@@ -1834,15 +1832,14 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n                                                  num_elements_written_));\n           if (next_elem_.end_of_sequence) {\n             TF_RETURN_IF_ERROR(writer->WriteScalar(\n-                full_name(strings::StrCat(kNextElem, \".\", kEndOfSequence)),\n-                \"\"));\n+                full_name(absl::StrCat(kNextElem, \".\", kEndOfSequence)), \"\"));\n           }\n           TF_RETURN_IF_ERROR(writer->WriteScalar(\n-              full_name(strings::StrCat(kNextElem, kSizeSuffix)),\n+              full_name(absl::StrCat(kNextElem, kSizeSuffix)),\n               next_elem_.value.size()));\n           for (size_t i = 0; i < next_elem_.value.size(); i++) {\n             TF_RETURN_IF_ERROR(writer->WriteTensor(\n-                full_name(strings::StrCat(kNextElem, \"[\", i, \"]\")),\n+                full_name(absl::StrCat(kNextElem, \"[\", i, \"]\")),\n                 next_elem_.value[i]));\n           }\n           VLOG(2) << \"Saving SnapshotWriterIterator: \" << num_elements_written_\n@@ -1882,7 +1879,7 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n           {\n             int64_t temp;\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(kBuffer, kSizeSuffix)), &temp));\n+                full_name(absl::StrCat(kBuffer, kSizeSuffix)), &temp));\n             buffer_size = static_cast<size_t>(temp);\n           }\n           for (size_t i = 0; i < buffer_size; i++) {\n@@ -1936,11 +1933,11 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n           {\n             int64_t temp;\n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                full_name(strings::StrCat(kNextElem, kSizeSuffix)), &temp));\n+                full_name(absl::StrCat(kNextElem, kSizeSuffix)), &temp));\n             next_elem_size = static_cast<size_t>(temp);\n           }\n           if (reader->Contains(\n-                  full_name(strings::StrCat(kNextElem, \".\", kEndOfSequence)))) {\n+                  full_name(absl::StrCat(kNextElem, \".\", kEndOfSequence)))) {\n             next_elem_.end_of_sequence = true;\n           } else {\n             next_elem_.end_of_sequence = false;\n@@ -1949,7 +1946,7 @@ class SnapshotDatasetOp : public UnaryDatasetOpKernel {\n           for (size_t i = 0; i < next_elem_size; i++) {\n             next_elem_.value.emplace_back();\n             TF_RETURN_IF_ERROR(reader->ReadTensor(\n-                ctx->flr(), full_name(strings::StrCat(kNextElem, \"[\", i, \"]\")),\n+                ctx->flr(), full_name(absl::StrCat(kNextElem, \"[\", i, \"]\")),\n                 &next_elem_.value.back()));\n           }\n           VLOG(2) << \"Restoring SnapshotWriterIterator: \""
        },
        {
            "sha": "c2bc283e140ed828b0f3655e27db6490b5469d6c",
            "filename": "tensorflow/core/kernels/data/experimental/sql_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsql_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsql_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fsql_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -96,7 +96,7 @@ class SqlDatasetOp : public DatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::Sql\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::Sql\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "95d4c0169c52afa85bf1bdb85c56ff4ed8205ed4",
            "filename": "tensorflow/core/kernels/data/experimental/stats_aggregator_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_aggregator_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_aggregator_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_aggregator_ops.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -104,7 +104,7 @@ class StatsAggregatorImpl : public StatsAggregator {\n           monitoring::Counter<1>::New(\n               /*streamz name*/ name,\n               /*streamz description*/\n-              strings::StrCat(name, \" generated or consumed by the component.\"),\n+              absl::StrCat(name, \" generated or consumed by the component.\"),\n               /*streamz label name*/ \"component_descriptor\"));\n     }\n     counters_map->at(name)->GetCell(label)->IncrementBy(val);"
        },
        {
            "sha": "3fbf149172c562bda2eae1399e8e87eb9e2abbda",
            "filename": "tensorflow/core/kernels/data/experimental/stats_dataset_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_dataset_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_dataset_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fstats_dataset_ops.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -70,7 +70,7 @@ class LatencyStatsDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::LatencyStats\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::LatencyStats\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -195,8 +195,8 @@ class BytesProducedStatsDatasetOp : public UnaryDatasetOpKernel {\n \n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n-      return std::make_unique<Iterator>(Iterator::Params{\n-          this, strings::StrCat(prefix, \"::BytesProducedStats\")});\n+      return std::make_unique<Iterator>(\n+          Iterator::Params{this, absl::StrCat(prefix, \"::BytesProducedStats\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "02cea46c2ff1adef5353b99a9b98df45313a394d",
            "filename": "tensorflow/core/kernels/data/experimental/take_while_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Ftake_while_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Ftake_while_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Ftake_while_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -72,7 +72,7 @@ class TakeWhileDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::TakeWhile\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::TakeWhile\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "a3696281f18071a43915c573399fde5b2b3171df",
            "filename": "tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fthreadpool_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fthreadpool_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fthreadpool_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -196,7 +196,7 @@ class ThreadPoolDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, strings::StrCat(prefix, \"::ThreadPool\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::ThreadPool\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -318,7 +318,7 @@ class MaxIntraOpParallelismDatasetOp::Dataset : public DatasetBase {\n   std::unique_ptr<IteratorBase> MakeIteratorInternal(\n       const string& prefix) const override {\n     return std::make_unique<Iterator>(Iterator::Params{\n-        this, strings::StrCat(prefix, \"::MaxIntraOpParallelism\")});\n+        this, absl::StrCat(prefix, \"::MaxIntraOpParallelism\")});\n   }\n \n   const DataTypeVector& output_dtypes() const override {\n@@ -462,7 +462,7 @@ class PrivateThreadPoolDatasetOp::Dataset : public DatasetBase {\n   std::unique_ptr<IteratorBase> MakeIteratorInternal(\n       const string& prefix) const override {\n     return std::make_unique<Iterator>(\n-        Iterator::Params{this, strings::StrCat(prefix, \"::PrivateThreadPool\")});\n+        Iterator::Params{this, absl::StrCat(prefix, \"::PrivateThreadPool\")});\n   }\n \n   const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "9ca25943a023368dd84fe16439ac3549fb430c41",
            "filename": "tensorflow/core/kernels/data/experimental/unbatch_dataset_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funbatch_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funbatch_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funbatch_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -90,7 +90,7 @@ class UnbatchDatasetOp : public UnaryDatasetOpKernel {\n     std::unique_ptr<IteratorBase> MakeIteratorInternal(\n         const std::string& prefix) const override {\n       return std::make_unique<Iterator>(\n-          Iterator::Params{this, StrCat(prefix, \"::Unbatch\")});\n+          Iterator::Params{this, absl::StrCat(prefix, \"::Unbatch\")});\n     }\n \n     const DataTypeVector& output_dtypes() const override {\n@@ -269,7 +269,7 @@ class UnbatchDatasetOp : public UnaryDatasetOpKernel {\n             !ctx->symbolic_checkpoint()) {\n           for (size_t i = 0; i < tensors_.size(); ++i) {\n             TF_RETURN_IF_ERROR(writer->WriteTensor(\n-                full_name(StrCat(\"tensors[\", i, \"]\")), tensors_[i]));\n+                full_name(absl::StrCat(\"tensors[\", i, \"]\")), tensors_[i]));\n           }\n         }\n         return absl::OkStatus();\n@@ -318,7 +318,7 @@ class UnbatchDatasetOp : public UnaryDatasetOpKernel {\n         } else {\n           for (size_t i = 0; i < tensors_.size(); ++i) {\n             TF_RETURN_IF_ERROR(reader->ReadTensor(\n-                ctx->flr(), full_name(StrCat(\"tensors[\", i, \"]\")),\n+                ctx->flr(), full_name(absl::StrCat(\"tensors[\", i, \"]\")),\n                 &tensors_[i]));\n           }\n         }"
        },
        {
            "sha": "e245ddddce90a9378a50da6219f9ac2194972863",
            "filename": "tensorflow/core/kernels/data/experimental/unique_dataset_op.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/79183a612db2cefd1d8ca4fbfc7b73078cbc7867/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Funique_dataset_op.cc?ref=79183a612db2cefd1d8ca4fbfc7b73078cbc7867",
            "patch": "@@ -49,7 +49,7 @@ class UniqueDatasetOp::Dataset : public DatasetBase {\n   std::unique_ptr<IteratorBase> MakeIteratorInternal(\n       const string& prefix) const override {\n     return std::make_unique<Iterator>(\n-        Iterator::Params{this, strings::StrCat(prefix, \"::Unique\")});\n+        Iterator::Params{this, absl::StrCat(prefix, \"::Unique\")});\n   }\n \n   const DataTypeVector& output_dtypes() const override {\n@@ -61,7 +61,7 @@ class UniqueDatasetOp::Dataset : public DatasetBase {\n   }\n \n   string DebugString() const override {\n-    return strings::StrCat(\"UniqueDatasetOp::Dataset\");\n+    return absl::StrCat(\"UniqueDatasetOp::Dataset\");\n   }\n \n   absl::Status InputDatasets(\n@@ -133,7 +133,7 @@ class UniqueDatasetOp::Dataset : public DatasetBase {\n       size_t i = 0;\n       for (const Tensor& t : unique_elements_) {\n         TF_RETURN_IF_ERROR(writer->WriteTensor(\n-            full_name(strings::StrCat(\"unique_elements[\", i++, \"]\")), t));\n+            full_name(absl::StrCat(\"unique_elements[\", i++, \"]\")), t));\n       }\n       return absl::OkStatus();\n     }\n@@ -153,7 +153,7 @@ class UniqueDatasetOp::Dataset : public DatasetBase {\n       for (int64_t i = 0; i < num_unique_elements; ++i) {\n         Tensor unique_element;\n         TF_RETURN_IF_ERROR(reader->ReadTensor(\n-            ctx->flr(), full_name(strings::StrCat(\"unique_elements[\", i, \"]\")),\n+            ctx->flr(), full_name(absl::StrCat(\"unique_elements[\", i, \"]\")),\n             &unique_element));\n         auto insert_result = unique_elements_.insert(unique_element);\n         if (!insert_result.second) {"
        }
    ],
    "stats": {
        "total": 319,
        "additions": 156,
        "deletions": 163
    }
}