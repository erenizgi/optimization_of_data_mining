{
    "author": "nputikhin",
    "message": "[XLA:GPU] Add gemm cost model to gpu cost model stats collection\n\nThe new cost model adds `reification_cost` entries with the name `experimental-gemm-cost-model`.\n\nMoves the relevant code from matmul_ptable_stats_collection and updates it to support the new emitter.\n\nPiperOrigin-RevId: 833757464",
    "sha": "e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a",
    "files": [
        {
            "sha": "1e84d86fd1a04f8efba9400f932d3e7d8dc0d6c5",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a",
            "patch": "@@ -217,17 +217,27 @@ cc_library(\n     srcs = [\"gpu_cost_model_stats_collection.cc\"],\n     hdrs = [\"gpu_cost_model_stats_collection.h\"],\n     deps = [\n+        \":block_level_parameters\",\n+        \":gpu_dot_fusion_cost_model\",\n         \":gpu_hlo_cost_analysis\",\n         \":gpu_performance_model\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_graph_dumper\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/time\",\n     ],\n )\n \n@@ -244,9 +254,10 @@ xla_cc_test(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/tests:xla_internal_test_main\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//mlir:IR\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -1092,21 +1103,15 @@ cc_library(\n     srcs = [\"matmul_ptable_stats_collection.cc\"],\n     hdrs = [\"matmul_ptable_stats_collection.h\"],\n     deps = [\n-        \":block_level_parameters\",\n-        \":gpu_dot_fusion_cost_model\",\n         \":hlo_op_profile_proto_cc\",\n         \":hlo_op_profiles\",\n         \":matmul_interpolator\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/gpu/codegen/triton:support\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n@@ -1119,7 +1124,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/time\",\n-        \"@llvm-project//mlir:IR\",\n     ],\n )\n "
        },
        {
            "sha": "987b0188611294c56eb62465030d1272f93045f7",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 98,
            "deletions": 1,
            "changes": 99,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc?ref=e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a",
            "patch": "@@ -16,17 +16,110 @@ limitations under the License.\n #include \"xla/service/gpu/model/gpu_cost_model_stats_collection.h\"\n \n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/time/time.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/gpu_dot_fusion_cost_model.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n \n+namespace {\n+\n+absl::StatusOr<absl::Duration> MaybeGetGemmCostModelForGemmTritonFusion(\n+    const se::DeviceDescription& device_info,\n+    const HloInstruction& instruction) {\n+  const HloFusionInstruction* fusion =\n+      DynCast<HloFusionInstruction>(&instruction);\n+  if (fusion == nullptr ||\n+      fusion->fusion_kind() != HloInstruction::FusionKind::kCustom) {\n+    return absl::FailedPreconditionError(\"Not a custom fusion.\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(GpuBackendConfig config,\n+                      fusion->backend_config<GpuBackendConfig>());\n+  if (config.fusion_backend_config().kind() != kTritonNestedGemmFusionKind) {\n+    return absl::FailedPreconditionError(\"Not a Triton GeMM fusion.\");\n+  }\n+\n+  const HloInstruction* dot_instruction =\n+      hlo_query::GetFirstInstructionWithOpcode(\n+          *fusion->fused_instructions_computation(), HloOpcode::kDot);\n+  if (dot_instruction == nullptr) {\n+    return absl::FailedPreconditionError(\n+        \"No kDot instruction found in Triton fusion.\");\n+  }\n+\n+  const HloDotInstruction* dot =\n+      DynCast<const HloDotInstruction>(dot_instruction);\n+  if (dot == nullptr) {\n+    return absl::FailedPreconditionError(\n+        \"No kDot instruction found in Triton fusion.\");\n+  }\n+\n+  if (!config.fusion_backend_config().has_block_level_fusion_config()) {\n+    return absl::FailedPreconditionError(\n+        \"Fusion backend config does not have block level fusion config.\");\n+  }\n+  BlockLevelParameters block_params =\n+      BlockLevelParameters::FromBlockLevelFusionConfig(\n+          config.fusion_backend_config().block_level_fusion_config());\n+\n+  return GpuDotFusionCostModel::EstimateRunTimeForDotOpWithBlockParameters(\n+      dot, block_params, device_info);\n+}\n+\n+// If `instruction` is a Triton-fused GEMM, computes its runtime estimation\n+// using an analytical cost model and adds this as a reification cost.\n+// This cost model focuses on the dot operation within the fusion. Fusions\n+// with non-trivial operations on dot operands might not be fully accounted for.\n+void RecordGemmCostModelEstimateIfApplicable(\n+    const se::DeviceDescription& device_info, HloInstruction& instruction) {\n+  absl::StatusOr<absl::Duration> duration =\n+      MaybeGetGemmCostModelForGemmTritonFusion(device_info, instruction);\n+  if (!duration.ok()) {\n+    VLOG(3) << \"Skipping the GeMM fusion cost model: \"\n+            << duration.status().ToString(\n+                   absl::StatusToStringMode::kWithNoExtraData)\n+            << \"\\nInstruction: \" << instruction.ToShortString();\n+    return;\n+  }\n+\n+  absl::StatusOr<GpuBackendConfig> gpu_config =\n+      instruction.backend_config<GpuBackendConfig>();\n+\n+  ReificationCost* gemm_reification_cost = gpu_config->add_reification_cost();\n+  gemm_reification_cost->set_name(\"experimental-gemm-cost-model\");\n+  gemm_reification_cost->set_end_to_end_cycles(\n+      absl::ToDoubleNanoseconds(*duration) * device_info.clock_rate_ghz());\n+  gemm_reification_cost->set_exec_time_us(\n+      absl::ToDoubleMicroseconds(*duration));\n+\n+  VLOG(1) << \"Adding GeMM fusion cost model estimate: \"\n+          << gemm_reification_cost->DebugString()\n+          << \"\\nInstruction: \" << instruction.ToString();\n+\n+  TF_CHECK_OK(instruction.set_backend_config(*gpu_config));\n+}\n+\n+}  // namespace\n+\n absl::StatusOr<bool> GpuCostModelStatsCollection::RunImpl(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n@@ -38,10 +131,14 @@ absl::StatusOr<bool> GpuCostModelStatsCollection::RunImpl(\n     TF_CHECK_OK(computation->Accept(&cost_analysis_));\n \n     for (auto* fusion_instr : computation->instructions()) {\n-      if (fusion_instr->opcode() != HloOpcode::kFusion) continue;\n+      if (fusion_instr->opcode() != HloOpcode::kFusion) {\n+        continue;\n+      }\n \n       gpu_performance_model.Get().RecordEstimatedRunTime(fusion_instr,\n                                                          &cost_analysis_);\n+\n+      RecordGemmCostModelEstimateIfApplicable(device_info_, *fusion_instr);\n     }\n   }\n   return false;"
        },
        {
            "sha": "f016b361ea2a0843e7e8d6de8e4e5264b5231f8c",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 10,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc?ref=e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a",
            "patch": "@@ -19,7 +19,9 @@ limitations under the License.\n \n #include <memory>\n \n+#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -28,15 +30,20 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n-#include \"tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n+namespace {\n+\n+using ::absl_testing::IsOkAndHolds;\n+using ::testing::Contains;\n+using ::testing::Truly;\n \n class GpuCostModelStatsCollectionTest : public HloHardwareIndependentTestBase {\n  public:\n   GpuCostModelStatsCollection cost_model_stats_{\n-      TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo(),\n       GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true},\n       &symbolic_expr_context_};\n \n@@ -45,8 +52,8 @@ class GpuCostModelStatsCollectionTest : public HloHardwareIndependentTestBase {\n   SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n-TEST_F(GpuCostModelStatsCollectionTest, FusinInEntryComputation) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+TEST_F(GpuCostModelStatsCollectionTest, FusionInEntryComputation) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"hlo(\n     HloModule test_module\n \n     log {\n@@ -58,9 +65,9 @@ TEST_F(GpuCostModelStatsCollectionTest, FusinInEntryComputation) {\n       %p0 = f32[16384] parameter(0)\n       ROOT %res = f32[16384]{0} fusion(p0), kind=kInput, calls=log\n     }\n-    )\"));\n+    )hlo\"));\n \n-  EXPECT_FALSE(cost_model_stats_.Run(module.get()).value());\n+  EXPECT_THAT(cost_model_stats_.Run(module.get()), IsOkAndHolds(false));\n \n   HloInstruction* root = module->entry_computation()->root_instruction();\n   TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n@@ -70,8 +77,8 @@ TEST_F(GpuCostModelStatsCollectionTest, FusinInEntryComputation) {\n   EXPECT_GT(gpu_config.reification_cost()[0].end_to_end_cycles(), 0);\n }\n \n-TEST_F(GpuCostModelStatsCollectionTest, FusinInWhileComputation) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+TEST_F(GpuCostModelStatsCollectionTest, FusionInWhileComputation) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"hlo(\n     HloModule test_module\n \n     cond {\n@@ -92,9 +99,9 @@ TEST_F(GpuCostModelStatsCollectionTest, FusinInWhileComputation) {\n     ENTRY main {\n       %p0 = f32[16384] parameter(0)\n       ROOT %while = f32[16384] while(%p0), body=%loop, condition=%cond\n-    })\"));\n+    })hlo\"));\n \n-  EXPECT_FALSE(cost_model_stats_.Run(module.get()).value());\n+  EXPECT_THAT(cost_model_stats_.Run(module.get()), IsOkAndHolds(false));\n \n   HloInstruction* root = module->entry_computation()\n                              ->root_instruction()\n@@ -107,5 +114,49 @@ TEST_F(GpuCostModelStatsCollectionTest, FusinInWhileComputation) {\n   EXPECT_GT(gpu_config.reification_cost()[0].end_to_end_cycles(), 0);\n }\n \n+TEST_F(GpuCostModelStatsCollectionTest, GemmCostModelAddedToGemmFusion) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"hlo(\n+  HloModule test_module\n+\n+  gemm_fusion_dot_computation {\n+    p0 = f16[1024,512]{1,0} parameter(0)\n+    p1 = f16[512,2048]{1,0} parameter(1)\n+    ROOT %dot.1 = f16[1024,2048]{1,0} dot(p0, p1),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  }\n+\n+  ENTRY main {\n+    p0 = f16[1024,512]{1,0} parameter(0)\n+    p1 = f16[512,2048]{1,0} parameter(1)\n+    ROOT gemm_fusion_dot = f16[1024,2048]{1,0} fusion(p0, p1), kind=kCustom,\n+      calls=gemm_fusion_dot_computation,\n+      backend_config={\n+        \"fusion_backend_config\": {\n+          \"kind\":\"__triton_nested_gemm_fusion\",\n+          \"block_level_fusion_config\": {\n+            \"num_warps\":\"4\",\n+            \"output_tiles\":[{\"sizes\":[\"64\",\"128\"]}],\n+            \"num_ctas\":1,\n+            \"num_stages\":3\n+          }\n+        }\n+      }\n+    }\n+    )hlo\"));\n+\n+  EXPECT_THAT(cost_model_stats_.Run(module.get()), IsOkAndHolds(false));\n+\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n+                          root->backend_config<GpuBackendConfig>());\n+\n+  EXPECT_THAT(gpu_config.reification_cost(),\n+              Contains(Truly([](const ReificationCost& cost) {\n+                return cost.name() == \"experimental-gemm-cost-model\" &&\n+                       cost.end_to_end_cycles() > 0;\n+              })));\n+}\n+\n+}  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "de0972b7e06126c5d2919bd3873da17147fade73",
            "filename": "third_party/xla/xla/service/gpu/model/matmul_ptable_stats_collection.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection.cc?ref=e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a",
            "patch": "@@ -27,24 +27,15 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/time/time.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"xla/backends/gpu/codegen/triton/support.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/service/gpu/model/block_level_parameters.h\"\n-#include \"xla/service/gpu/model/gpu_dot_fusion_cost_model.h\"\n #include \"xla/service/gpu/model/hlo_op_profile.pb.h\"\n #include \"xla/service/gpu/model/hlo_op_profiles.h\"\n #include \"xla/service/gpu/model/matmul_interpolator.h\"\n-#include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -56,8 +47,6 @@ namespace xla::gpu {\n \n namespace {\n \n-constexpr absl::string_view kGemmCostModelName = \"gemm-cost-model\";\n-\n constexpr absl::string_view kPerfTablesModelName = \"perf-table-model\";\n \n absl::StatusOr<HloInstructionProfileList> CollectProfiles(\n@@ -76,29 +65,6 @@ absl::StatusOr<HloInstructionProfileList> CollectProfiles(\n   return profile.entries().at(key);\n }\n \n-HloDotInstruction* GetTritonGemmInstruction(const HloInstruction& dot_fusion) {\n-  if (!(HloPredicateIsOp<HloOpcode::kFusion>(&dot_fusion) &&\n-        IsTritonFusedComputation(\n-            *dot_fusion.fused_instructions_computation()))) {\n-    return nullptr;\n-  }\n-\n-  HloInstruction* dot = hlo_query::GetFirstInstructionWithOpcode(\n-      *dot_fusion.fused_instructions_computation(), HloOpcode::kDot);\n-  if (dot == nullptr) {\n-    return nullptr;\n-  }\n-  return DynCast<HloDotInstruction>(dot);\n-}\n-\n-absl::StatusOr<BlockLevelParameters> GetBlockLevelParams(\n-    HloDotInstruction& dot, TritonGemmConfig& config) {\n-  mlir::MLIRContext ctx;\n-  SymbolicExprContext symbolic_expr_context(&ctx);\n-  return ::xla::gpu::detail::FindBlockLevelParameters(\n-      &dot, config, &symbolic_expr_context, se::DeviceDescription());\n-}\n-\n absl::Status SetReificationCost(HloInstruction& instr, absl::Duration exec_time,\n                                 absl::string_view reification_name) {\n   TF_ASSIGN_OR_RETURN(GpuBackendConfig gpu_config,\n@@ -109,32 +75,6 @@ absl::Status SetReificationCost(HloInstruction& instr, absl::Duration exec_time,\n   return instr.set_backend_config(gpu_config);\n }\n \n-// Computes the runtime estimation via analytical GEMM cost model and adds a\n-// reification cost to `instr`. We do not make any constraints on what fusions\n-// do we add the cost to. In particular it can be the case there's a non trivial\n-// fusion on dot operands. As of now the analytical GEMM model does not support\n-// these cases so result interpretation has take this into consideration.\n-absl::Status MaybeRecordGemmCostModelForGemmTritonFusion(\n-    const se::DeviceDescription& device_info, HloInstruction& instr) {\n-  HloDotInstruction* dot = GetTritonGemmInstruction(instr);\n-  if (dot == nullptr) {\n-    VLOG(2) << \"Cannot get triton gemm: \" << instr.ToString();\n-    return absl::OkStatus();\n-  }\n-  auto triton_gemm_key = instr.backend_config<GpuBackendConfig>()\n-                             ->fusion_backend_config()\n-                             .triton_gemm_config();\n-  TF_ASSIGN_OR_RETURN(TritonGemmConfig triton_gemm_config,\n-                      TritonGemmConfig::FromProto(triton_gemm_key));\n-  TF_ASSIGN_OR_RETURN(BlockLevelParameters block_params,\n-                      GetBlockLevelParams(*dot, triton_gemm_config));\n-  TF_ASSIGN_OR_RETURN(\n-      absl::Duration exec_time,\n-      GpuDotFusionCostModel::EstimateRunTimeForDotOpWithBlockParameters(\n-          dot, block_params, device_info));\n-  return SetReificationCost(instr, exec_time, kGemmCostModelName);\n-}\n-\n absl::Status MaybeRecordPerfTablesForDotsAndCustomCalls(\n     const se::DeviceDescription& device_info, HloInstruction& instr,\n     MatmulInterpolator& interpolator) {\n@@ -172,12 +112,6 @@ absl::StatusOr<bool> MatmulPerfTableStatsCollection::RunImpl(\n           VLOG(1) << \"Cannot record perf tables stats data: \"\n                   << instr->ToString() << \". Status: \" << status;\n         }\n-        if (absl::Status status = MaybeRecordGemmCostModelForGemmTritonFusion(\n-                device_info_, *instr);\n-            !status.ok()) {\n-          VLOG(1) << \"Cannot record GEMM cost model stats data: \"\n-                  << instr->ToString() << \". Status: \" << status;\n-        }\n       });\n \n   return false;"
        },
        {
            "sha": "d155a1a5ae01b61a54e4596cb3e61a2e6e1c65d7",
            "filename": "third_party/xla/xla/service/gpu/model/matmul_ptable_stats_collection_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 53,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fmatmul_ptable_stats_collection_test.cc?ref=e245e2ce4d4eec1c75140ceaf5a0a50e9252ee8a",
            "patch": "@@ -212,58 +212,5 @@ TEST_F(MatmulStatsCollectionTest,\n             0);\n }\n \n-TEST_F(MatmulStatsCollectionTest,\n-       CollectsMatmulGEMMCostModelDataForTritonFusionConfig) {\n-  absl::string_view hlo = R\"(\n-    HloModule m\n-\n-    comp {\n-      p0 = bf16[1024,1024] parameter(0)\n-      p1 = bf16[1024,1024] parameter(1)\n-      ROOT _ = bf16[1024,1024] dot(p0,p1),\n-        lhs_contracting_dims={1},\n-        rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY e {\n-      p0 = bf16[1024,1024] parameter(0)\n-      p1 = bf16[1024,1024] parameter(1)\n-      ROOT triton_gemm =  bf16[1024,1024] fusion(p0,p1),\n-        kind=kCustom,\n-        calls=comp,\n-        backend_config={\n-          \"operation_queue_id\":\"0\",\n-          \"wait_on_operation_queues\":[],\n-          \"fusion_backend_config\": {\n-            \"kind\":\"__triton_gemm\",\n-            \"triton_gemm_config\":{\n-              \"block_m\":\"128\",\n-              \"block_n\":\"128\",\n-              \"block_k\":\"64\",\n-              \"split_k\":\"1\",\n-              \"num_stages\":\"1\",\n-              \"num_warps\":\"8\",\n-              \"num_ctas\":\"1\"\n-            }\n-          },\n-        }\n-    }\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(hlo));\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      bool changed, MatmulPerfTableStatsCollection(profiles_path_, device_info_)\n-                        .Run(module.get()));\n-\n-  VLOG(1) << module->ToString();\n-\n-  EXPECT_FALSE(changed);\n-  EXPECT_THAT(module->entry_computation()\n-                  ->root_instruction()\n-                  ->backend_config<GpuBackendConfig>()\n-                  ->reification_cost(),\n-              ElementsAre(Property(&ReificationCost::exec_time_us,\n-                                   DoubleNear(141, /*max_abs_error=*/1))));\n-}\n-\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 309,
        "additions": 171,
        "deletions": 138
    }
}