{
    "author": "derdrdirk",
    "message": "[Autotuner] Allow Autotuner backends to be used without a device by passing a TargetConfig, which can be loaded from a proto. Currently we depend on the stream_executor which depends on a device being present. This change is a preparation for loading cached autotuning results in deviceless mode.\n\nPiperOrigin-RevId: 811263418",
    "sha": "198f34cf1fe7f4c04f11f947d190d097fc7631ab",
    "files": [
        {
            "sha": "5d0dc5d6df2e032d147cc0a879ed068251138fcb",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -94,12 +94,14 @@ xla_test(\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n@@ -179,7 +181,6 @@ xla_test(\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:platform_util\",\n-        \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n@@ -246,7 +247,6 @@ xla_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service:compiler\",\n         \"//xla/service:platform_util\",\n-        \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n@@ -324,10 +324,12 @@ xla_test(\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n@@ -388,7 +390,6 @@ xla_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service:compiler\",\n         \"//xla/service:platform_util\",\n-        \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/stream_executor:stream_executor_h\",\n@@ -464,9 +465,11 @@ xla_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n@@ -543,11 +546,13 @@ xla_test(\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "15a05f3049cb1707fd8e52e9aed43c3cd8c36226",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -99,12 +99,14 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       xla::Compiler::GetForPlatform(platform));\n   se::StreamExecutor* stream_executor = platform->ExecutorForDevice(0).value();\n   DebugOptions debug_options = GetDebugOptionsFromFlags();\n+  Compiler::TargetConfig target_config(stream_executor);\n \n   auto& registry = stream_executor::PlatformObjectRegistry::GetGlobalRegistry();\n   TF_ASSIGN_OR_RETURN(const GetCodegenBackends::Type& get_codegen_backends,\n                       registry.FindObject<GetCodegenBackends>(platform->id()));\n-  std::vector<std::unique_ptr<CodegenBackend>> backends = get_codegen_backends(\n-      stream_executor, &debug_options, compiler.get(), mlir_context);\n+  std::vector<std::unique_ptr<CodegenBackend>> backends =\n+      get_codegen_backends(stream_executor, &debug_options, compiler.get(),\n+                           &target_config, mlir_context);\n \n   std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n       std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n@@ -131,8 +133,8 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n \n   std::unique_ptr<AutotunerCacheInterface> cache;\n   if (!cache_dir.empty()) {\n-    cache = std::make_unique<LegacyCache>(\n-        cache_dir, it->second, stream_executor->GetDeviceDescription());\n+    cache = std::make_unique<LegacyCache>(cache_dir, it->second,\n+                                          target_config.device_description);\n   }\n \n   AutotuneConfig autotune_config;"
        },
        {
            "sha": "e10f32f29574bb599f4b7d66ac7560ae0851991c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -43,13 +43,13 @@ namespace gpu {\n class BlockLevelEmitterBackend : public GpuCodegenBackend {\n  public:\n   explicit BlockLevelEmitterBackend(\n-      stream_executor::StreamExecutor* absl_nonnull stream_executor,\n       const DebugOptions* absl_nonnull debug_options,\n       Compiler* absl_nonnull compiler,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n+      const Compiler::TargetConfig* target_config,\n       bool use_default_config = false)\n-      : GpuCodegenBackend(\"BlockLevelEmitter\", stream_executor, debug_options,\n-                          compiler),\n+      : GpuCodegenBackend(\"BlockLevelEmitter\", debug_options, compiler,\n+                          target_config),\n         use_default_config_(use_default_config),\n         shape_size_fn_(std::move(shape_size_fn)) {}\n "
        },
        {
            "sha": "ad5d16c1eed2b85a999c5dd4098dea3429656914",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -28,12 +28,14 @@ limitations under the License.\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n@@ -65,19 +67,22 @@ class TritonBlockLevelFusionEmitterBackendTest\n     : public HloHardwareIndependentTestBase {\n  protected:\n   TritonBlockLevelFusionEmitterBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_,\n-                 compiler_.ShapeSizeBytesFunction()) {\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(&debug_options_, &compiler_,\n+                 compiler_.ShapeSizeBytesFunction(), &target_config_) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n   }\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   BlockLevelEmitterBackend backend_;\n };\n \n@@ -450,9 +455,8 @@ ENTRY %main {\n \n TEST_F(TritonBlockLevelFusionEmitterBackendTest, UseDefaultConfigFlag) {\n   auto backend = BlockLevelEmitterBackend(\n-      PlatformUtil::GetDefaultPlatform().value()->ExecutorForDevice(0).value(),\n       &debug_options_, &compiler_, compiler_.ShapeSizeBytesFunction(),\n-      /*use_default_config=*/true);\n+      &target_config_, /*use_default_config=*/true);\n   // Parse an HLO module containing a kCustom Triton fusion with a backend\n   // config that includes block-level tiling parameters.\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,"
        },
        {
            "sha": "281be4581232b69c3722ed3075725533a98522f7",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -47,8 +47,10 @@ namespace gpu {\n class CublasBackend : public GpuCodegenBackend {\n  public:\n   explicit CublasBackend(stream_executor::StreamExecutor* stream_executor,\n-                         const DebugOptions* debug_options, Compiler* compiler)\n-      : GpuCodegenBackend(\"Cublas\", stream_executor, debug_options, compiler) {}\n+                         const DebugOptions* debug_options, Compiler* compiler,\n+                         const Compiler::TargetConfig* target_config)\n+      : GpuCodegenBackend(\"Cublas\", debug_options, compiler, target_config,\n+                          stream_executor) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;"
        },
        {
            "sha": "a8463704e0e0dd6d229ded21cbf04ae63e8aca57",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n@@ -93,14 +94,18 @@ class CublasBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   CublasBackend backend_;\n \n   CublasBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_) {}\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(stream_executor_, &debug_options_, &compiler_,\n+                 &target_config_) {}\n \n   CublasBackendConfig ExpectedDefaultAlgorithm() {\n     auto config = AutotuneResult::GemmKey();"
        },
        {
            "sha": "88c5f8a6d7e4428e74b6edc50633883ad5e5752d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -46,9 +46,10 @@ class CublasLtBackend : public GpuCodegenBackend {\n  public:\n   explicit CublasLtBackend(stream_executor::StreamExecutor* stream_executor,\n                            const DebugOptions* debug_options,\n-                           Compiler* compiler)\n-      : GpuCodegenBackend(\"CublasLt\", stream_executor, debug_options,\n-                          compiler) {}\n+                           Compiler* compiler,\n+                           const Compiler::TargetConfig* target_config)\n+      : GpuCodegenBackend(\"CublasLt\", debug_options, compiler, target_config,\n+                          stream_executor) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;"
        },
        {
            "sha": "6c128a39e873ec5f759458ccfc3d95050ead7448",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/blas.h\"\n@@ -105,14 +106,18 @@ class CublasLtBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   CublasLtBackend backend_;\n \n   CublasLtBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_) {}\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(stream_executor_, &debug_options_, &compiler_,\n+                 &target_config_) {}\n \n   CublasLtBackendConfig ExpectedDefaultAlgorithm() {\n     auto config = AutotuneResult::GemmKey();"
        },
        {
            "sha": "5a16f3226c1e6e3a255d785f36d1b1e92f70ee61",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -57,8 +57,10 @@ namespace gpu {\n class CudnnBackend : public GpuCodegenBackend {\n  public:\n   explicit CudnnBackend(stream_executor::StreamExecutor* stream_executor,\n-                        const DebugOptions* debug_options, Compiler* compiler)\n-      : GpuCodegenBackend(\"Cudnn\", stream_executor, debug_options, compiler) {}\n+                        const DebugOptions* debug_options, Compiler* compiler,\n+                        const Compiler::TargetConfig* target_config)\n+      : GpuCodegenBackend(\"Cudnn\", debug_options, compiler, target_config,\n+                          stream_executor) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;"
        },
        {
            "sha": "a736d3bb484796c67f82948b1a64981a7a5bff5c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -27,10 +27,12 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -110,6 +112,8 @@ class CudnnBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   CudnnBackend backend_;\n \n   CudnnBackendTest()\n@@ -118,11 +122,13 @@ class CudnnBackendTest : public HloHardwareIndependentTestBase {\n           debug_options.set_xla_gpu_cudnn_gemm_fusion_level(2);\n           return debug_options;\n         }()),\n-        backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_) {}\n+        stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(stream_executor_, &debug_options_, &compiler_,\n+                 &target_config_) {}\n };\n \n TEST_F(CudnnBackendTest, CanCreateCublasBackend) {"
        },
        {
            "sha": "7d4cec2f7d8eb2cc8dd8d70009945a7d5f2a0374",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -36,8 +35,10 @@ class CustomKernelBackend : public GpuCodegenBackend {\n  public:\n   explicit CustomKernelBackend(stream_executor::StreamExecutor* stream_executor,\n                                const DebugOptions* debug_options,\n-                               Compiler* compiler)\n-      : GpuCodegenBackend(\"Cublas\", stream_executor, debug_options, compiler) {}\n+                               Compiler* compiler,\n+                               const Compiler::TargetConfig* target_config)\n+      : GpuCodegenBackend(\"CustomKernel\", debug_options, compiler,\n+                          target_config, stream_executor) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;"
        },
        {
            "sha": "b8cde0976c2d0a14182afc1ea65dfc3a215f1710",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n@@ -101,14 +102,18 @@ class CustomKernelBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   CustomKernelBackend backend_;\n \n   CustomKernelBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_) {}\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(stream_executor_, &debug_options_, &compiler_,\n+                 &target_config_) {}\n \n   CustomKernelBackendConfig ExpectedDefaultAlgorithm() {\n     auto config = AutotuneResult::CustomKernelFusionKey();"
        },
        {
            "sha": "42477542c277e4a5e1525f98a24f7bc585e87732",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -32,7 +32,7 @@ namespace gpu {\n struct GetCodegenBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      mlir::MLIRContext* mlir_context)>;\n+      const Compiler::TargetConfig*, mlir::MLIRContext* mlir_context)>;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "a7431bd2cd7b21a0605e4d4bed1543a546684e11",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -37,16 +37,17 @@ namespace gpu {\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n+    const Compiler::TargetConfig* target_config,\n     mlir::MLIRContext* mlir_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n-      stream_executor, debug_options, compiler, mlir_context));\n-  backends.push_back(std::make_unique<CublasBackend>(stream_executor,\n-                                                     debug_options, compiler));\n+      debug_options, compiler, target_config, mlir_context));\n+  backends.push_back(std::make_unique<CublasBackend>(\n+      stream_executor, debug_options, compiler, target_config));\n   backends.push_back(std::make_unique<CublasLtBackend>(\n-      stream_executor, debug_options, compiler));\n-  backends.push_back(\n-      std::make_unique<CudnnBackend>(stream_executor, debug_options, compiler));\n+      stream_executor, debug_options, compiler, target_config));\n+  backends.push_back(std::make_unique<CudnnBackend>(\n+      stream_executor, debug_options, compiler, target_config));\n   return backends;\n }\n "
        },
        {
            "sha": "635a07ffb83a2617c5cf248ce90b97a712f96e09",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_rocm.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -35,12 +35,13 @@ namespace gpu {\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n+    const Compiler::TargetConfig* target_config,\n     mlir::MLIRContext* mlir_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n-      stream_executor, debug_options, compiler, mlir_context));\n-  backends.push_back(std::make_unique<CublasBackend>(stream_executor,\n-                                                     debug_options, compiler));\n+      debug_options, compiler, target_config, mlir_context));\n+  backends.push_back(std::make_unique<CublasBackend>(\n+      stream_executor, debug_options, compiler, target_config));\n   return backends;\n }\n "
        },
        {
            "sha": "ce8daa7f7330d318e8588441b35b4e0f1d6e290a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -45,11 +45,15 @@ class FissionBackend : public GpuCodegenBackend {\n  public:\n   explicit FissionBackend(stream_executor::StreamExecutor* stream_executor,\n                           const DebugOptions* debug_options, Compiler* compiler,\n+                          const Compiler::TargetConfig* target_config,\n                           mlir::MLIRContext* mlir_context)\n-      : GpuCodegenBackend(\"Fission\", stream_executor, debug_options, compiler),\n-        cublas_backend_(stream_executor, debug_options, compiler),\n-        cublaslt_backend_(stream_executor, debug_options, compiler),\n-        custom_kernel_backend_(stream_executor, debug_options, compiler),\n+      : GpuCodegenBackend(\"Fission\", debug_options, compiler, target_config),\n+        cublas_backend_(stream_executor, debug_options, compiler,\n+                        target_config),\n+        cublaslt_backend_(stream_executor, debug_options, compiler,\n+                          target_config),\n+        custom_kernel_backend_(stream_executor, debug_options, compiler,\n+                               target_config),\n         mlir_context_(mlir_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>"
        },
        {
            "sha": "672cb439cc671943edc0b2483bdb202600b6bb06",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -31,9 +31,11 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n@@ -70,15 +72,19 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   FissionBackend backend_;\n   mlir::MLIRContext mlir_context_;\n \n   FissionBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_, &mlir_context_) {}\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(stream_executor_, &debug_options_, &compiler_, &target_config_,\n+                 &mlir_context_) {}\n };\n \n TEST_F(FissionBackendTest, CanCreateCublasBackend) {"
        },
        {
            "sha": "3b308ad2f490ae479846b893f912033ac5fb7d10",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -40,12 +40,14 @@ namespace  gpu {\n class GpuCodegenBackend : public CodegenBackend {\n  public:\n   // target_config, debug_options and compiler should outlive the backend.\n-  GpuCodegenBackend(absl::string_view name,\n-                    stream_executor::StreamExecutor* stream_executor,\n-                    const DebugOptions* debug_options, Compiler* compiler)\n+  // TODO(b/447096292): Remove stream_executor from GpuCodegenBackend.\n+  GpuCodegenBackend(absl::string_view name, const DebugOptions* debug_options,\n+                    Compiler* compiler,\n+                    const Compiler::TargetConfig* target_config,\n+                    stream_executor::StreamExecutor* stream_executor = nullptr)\n       : name_(name),\n         stream_executor_(stream_executor),\n-        target_config_(Compiler::TargetConfig(stream_executor)),\n+        target_config_(*target_config),\n         debug_options_(*debug_options),\n         compiler_(compiler) {}\n \n@@ -82,6 +84,7 @@ class GpuCodegenBackend : public CodegenBackend {\n     opts.set_xla_gpu_kernel_cache_file(\"\");\n \n     Compiler::CompileOptions options;\n+    options.target_config = target_config_;\n     options.is_autotuning_compilation = true;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));\n@@ -104,7 +107,7 @@ class GpuCodegenBackend : public CodegenBackend {\n \n   std::string name_;\n   stream_executor::StreamExecutor* stream_executor_;\n-  Compiler::TargetConfig target_config_;\n+  const Compiler::TargetConfig& target_config_;\n   const DebugOptions& debug_options_;\n   // TODO(b/407494653): remove compiler when we don't need to run any HLO passes\n   // and the codegen backend can directly produce an executable without a"
        },
        {
            "sha": "37eff23ea87e37aed67c75c6ca8f75f354ef7a13",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.h",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -38,12 +38,11 @@ namespace gpu {\n // backends.\n class NativeEmitterBackend : public GpuCodegenBackend {\n  public:\n-  explicit NativeEmitterBackend(\n-      stream_executor::StreamExecutor* absl_nonnull stream_executor,\n-      const DebugOptions* absl_nonnull debug_options,\n-      Compiler* absl_nonnull compiler)\n-      : GpuCodegenBackend(\"NativeEmitter\", stream_executor, debug_options,\n-                          compiler) {}\n+  explicit NativeEmitterBackend(const DebugOptions* absl_nonnull debug_options,\n+                                Compiler* absl_nonnull compiler,\n+                                const Compiler::TargetConfig* target_config)\n+      : GpuCodegenBackend(\"NativeEmitter\", debug_options, compiler,\n+                          target_config) {}\n \n   // Returns all supported configurations for the given instruction.\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>"
        },
        {
            "sha": "afb2e878e0f3d79efd77236f8182da65fba3ceb0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -91,14 +91,17 @@ ENTRY %entry_computation (p0: f32[32,16], p1: f32[32,16]) -> (f32[32,16], f32[32\n class NativeEmitterBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   NativeEmitterBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_) {}\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(&debug_options_, &compiler_, &target_config_) {}\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   NativeEmitterBackend backend_;\n };\n \n@@ -214,9 +217,8 @@ TEST_F(NativeEmitterBackendTest, CompileSetsIsAutotuningCompilationOption) {\n                           ParseAndReturnVerifiedModule(kReductionFusionHlo));\n   auto fusion = reduction_module->entry_computation()->root_instruction();\n   MockCompiler mock_compiler;\n-  NativeEmitterBackend backend(\n-      PlatformUtil::GetDefaultPlatform().value()->ExecutorForDevice(0).value(),\n-      &debug_options_, &mock_compiler);\n+  NativeEmitterBackend backend(&debug_options_, &mock_compiler,\n+                               &target_config_);\n   // Call GetDefaultConfig on the fusion instruction.\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n                           backend.GetDefaultConfig(*(fusion)));"
        },
        {
            "sha": "465436b0080c13fb939ac2d0b81581671e218788",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -36,10 +36,10 @@ namespace gpu {\n \n class TritonBackend : public GpuCodegenBackend {\n  public:\n-  explicit TritonBackend(stream_executor::StreamExecutor* stream_executor,\n-                         const DebugOptions* debug_options, Compiler* compiler,\n+  explicit TritonBackend(const DebugOptions* debug_options, Compiler* compiler,\n+                         const Compiler::TargetConfig* target_config,\n                          mlir::MLIRContext* mlir_context)\n-      : GpuCodegenBackend(\"Triton\", stream_executor, debug_options, compiler),\n+      : GpuCodegenBackend(\"Triton\", debug_options, compiler, target_config),\n         mlir_context_(mlir_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>"
        },
        {
            "sha": "300278ecbd49a24d2d92c76b253788b7e3414cc5",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -30,11 +30,13 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"xla/xla.pb.h\"\n@@ -71,18 +73,21 @@ const char kHlo[] = R\"(\n class TritonBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n   TritonBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_, &mlir_context_) {\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        backend_(&debug_options_, &compiler_, &target_config_, &mlir_context_) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n   }\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n   TritonBackend backend_;\n   mlir::MLIRContext mlir_context_;\n };"
        },
        {
            "sha": "cdafa03727ab5eb89edeef08054e04001dd63c59",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -1903,6 +1903,7 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:call_inliner\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:dump\",\n         \"//xla/service:float_support\",\n         \"//xla/service:hlo_cost_analysis\",\n@@ -2153,6 +2154,7 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:call_inliner\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:float_support\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_verifier\",\n@@ -2169,11 +2171,9 @@ cc_library(\n         \"//xla/service/gpu/transforms:gpusolver_rewriter\",\n         \"//xla/service/gpu/transforms:triangular_solve_rewriter\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/rocm:rocm_platform_id\",\n         \"//xla/stream_executor/rocm:rocm_solver_context\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "a36e3850b02e199dd6bb1fe15c1b1feb31674dfe",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -39,10 +39,10 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/dot_dimension_merger.h\"\n #include \"xla/hlo/transforms/simplifiers/float_normalization.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_folding.h\"\n-#include \"xla/hlo/transforms/simplifiers/reshape_mover.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/call_inliner.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/float_support.h\"\n #include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/autotuning/autotuner_pass.h\"\n@@ -228,7 +228,8 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n     HloPassPipeline* pipeline, const se::GpuComputeCapability& gpu_version,\n     const CompileOptions& options, HloModule* hlo_module,\n     AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n-    se::StreamExecutor* stream_exec) {\n+    se::StreamExecutor* stream_exec,\n+    const Compiler::TargetConfig* target_config) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n   if (hlo_module->config()\n           .debug_options()\n@@ -246,10 +247,10 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   // TODO: b/407494793 - Add proper support for ROCM. Currently the Cublas\n   // backend uses the same API as rocBLAS.\n-  backends.push_back(\n-      std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n-  backends.push_back(\n-      std::make_unique<CublasLtBackend>(stream_exec, &debug_options, this));\n+  backends.push_back(std::make_unique<CublasBackend>(\n+      stream_exec, &debug_options, this, target_config));\n+  backends.push_back(std::make_unique<CublasLtBackend>(\n+      stream_exec, &debug_options, this, target_config));\n   auto should_autotune = [](const HloInstruction& instruction) -> bool {\n     return instruction.opcode() == HloOpcode::kCustomCall &&\n            IsCublasGemm(instruction);"
        },
        {
            "sha": "5a4973801962cb92293f5bf9708a1226adb8de8c",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.h",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/gpu_compiler.h\"\n@@ -59,11 +60,13 @@ class AMDGPUCompiler : public GpuCompiler {\n   bool RequiresCollectiveScheduleLinearizer(\n       const HloModule* module, se::StreamExecutor* stream_exec) override;\n \n+  // target_config must outlive the pipeline.\n   absl::Status AddConvAndGemmAutotuningPasses(\n       HloPassPipeline* pipeline, const se::GpuComputeCapability& gpu_version,\n       const CompileOptions& options, HloModule* hlo_module,\n       AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n-      se::StreamExecutor* stream_exec) override;\n+      se::StreamExecutor* stream_exec,\n+      const Compiler::TargetConfig* target_config) override;\n \n   absl::StatusOr<BackendCompileResult> CompileTargetBinary(\n       const HloModuleConfig& module_config, llvm::Module* llvm_module,"
        },
        {
            "sha": "a7a6e15e1184bd56dc2fe9fc80b17f0679721962",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -339,6 +339,7 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:cublas_cudnn\",\n+        \"//xla/service/gpu:gpu_compiler\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform_manager\",\n@@ -350,6 +351,7 @@ xla_test(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:path\","
        },
        {
            "sha": "1b501dacea8eb3876bb0dac19a59d414b5074ce7",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/service/gpu/autotuning/autotuner_pass.h\"\n \n-#include <cstdint>\n #include <memory>\n #include <string>\n #include <utility>\n@@ -25,6 +24,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/ascii.h\"\n #include \"xla/backends/autotuner/autotuner.h\"\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n@@ -36,18 +36,16 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/service/gpu/gpu_compiler.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n-#include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n-#include \"tsl/platform/path.h\"\n \n namespace xla {\n namespace gpu {\n@@ -108,8 +106,10 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  GpuCompiler::TargetConfig target_config(stream_executor_);\n   backends.push_back(std::make_unique<CublasBackend>(\n-      stream_executor_, &module->config().debug_options(), &compiler_));\n+      stream_executor_, &module->config().debug_options(), &compiler_,\n+      &target_config));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<AutotunerPass> pass,\n@@ -134,9 +134,11 @@ TEST_F(AutotunerPassTest, CublasGemmIsNotAutotunedWhenFilterReturnsFalse) {\n \n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n+  GpuCompiler::TargetConfig target_config(stream_executor_);\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<CublasBackend>(\n-      stream_executor_, &module->config().debug_options(), &compiler_));\n+      stream_executor_, &module->config().debug_options(), &compiler_,\n+      &target_config));\n \n   auto should_autotune = [](const HloInstruction& instruction) {\n     return false;\n@@ -169,12 +171,14 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n \n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuning\",\n                                       /*num_threads=*/4);\n+  GpuCompiler::TargetConfig target_config(stream_executor_);\n \n   // Run the pass for the first time, this should populate the cache.\n   {\n     std::vector<std::unique_ptr<CodegenBackend>> backends;\n     backends.push_back(std::make_unique<CublasBackend>(\n-        stream_executor_, &module->config().debug_options(), &compiler_));\n+        stream_executor_, &module->config().debug_options(), &compiler_,\n+        &target_config));\n \n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass,\n@@ -212,7 +216,8 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n   {\n     std::vector<std::unique_ptr<CodegenBackend>> backends2;\n     backends2.push_back(std::make_unique<CublasBackend>(\n-        stream_executor_, &module->config().debug_options(), &compiler_));\n+        stream_executor_, &module->config().debug_options(), &compiler_,\n+        &target_config));\n \n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass2,"
        },
        {
            "sha": "ccba6f34c41c593aee5935140411605a3929fa4a",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -1594,7 +1594,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n     HloPassPipeline pipeline(\"autotune-fusion-emitters\");\n     TF_RETURN_IF_ERROR(AddFusionAutotuningPass(\n         &pipeline, hlo_module, options, thread_pool.get_mutable(), stream_exec,\n-        ShapeSizeBytesFunction()));\n+        &gpu_target_config, ShapeSizeBytesFunction()));\n     TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n   }\n \n@@ -1852,7 +1852,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n \n   TF_RETURN_IF_ERROR(AddConvAndGemmAutotuningPasses(\n       &pipeline, gpu_version, options, hlo_module, autotune_config, thread_pool,\n-      stream_exec));\n+      stream_exec, &gpu_target_config));\n \n   // The GEMM fusion autotuner can insert new bf16 reductions that need to be\n   // normalized again."
        },
        {
            "sha": "97d1e1715d720f1a2100d465fa476266fd0127e9",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -166,11 +166,13 @@ class GpuCompiler : public LLVMCompiler {\n   }\n \n   // Add autotuning passes for convolution and gemm (except triton).\n+  // target_config must outlive the pipeline.\n   virtual absl::Status AddConvAndGemmAutotuningPasses(\n       HloPassPipeline* pipeline, const se::GpuComputeCapability& gpu_version,\n       const CompileOptions& options, HloModule* hlo_module,\n       AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n-      se::StreamExecutor* stream_exec) {\n+      se::StreamExecutor* stream_exec,\n+      const Compiler::TargetConfig* target_config) {\n     return absl::OkStatus();\n   }\n \n@@ -184,10 +186,12 @@ class GpuCompiler : public LLVMCompiler {\n     return absl::OkStatus();\n   }\n \n+  // target_config must outlive the pipeline.\n   virtual absl::Status AddFusionAutotuningPass(\n       HloPassPipeline* pipeline, HloModule* hlo_module,\n       const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n       stream_executor::StreamExecutor* stream_executor,\n+      const Compiler::TargetConfig* target_config,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n     return absl::OkStatus();\n   }"
        },
        {
            "sha": "00a902da99cf0ff5022f8adea76fc9bb120b650b",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -61,6 +61,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/call_inliner.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/float_support.h\"\n #include \"xla/service/gpu/alias_info.h\"\n@@ -336,7 +337,8 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n     HloPassPipeline* pipeline, const se::GpuComputeCapability& gpu_version,\n     const CompileOptions& options, HloModule* hlo_module,\n     AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n-    se::StreamExecutor* stream_exec) {\n+    se::StreamExecutor* stream_exec,\n+    const Compiler::TargetConfig* target_config) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n   if (hlo_module->config()\n           .debug_options()\n@@ -355,10 +357,10 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n   }\n \n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n-  backends.push_back(\n-      std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n-  backends.push_back(\n-      std::make_unique<CublasLtBackend>(stream_exec, &debug_options, this));\n+  backends.push_back(std::make_unique<CublasBackend>(\n+      stream_exec, &debug_options, this, target_config));\n+  backends.push_back(std::make_unique<CublasLtBackend>(\n+      stream_exec, &debug_options, this, target_config));\n   auto should_autotune = [](const HloInstruction& instruction) -> bool {\n     return instruction.opcode() == HloOpcode::kCustomCall &&\n            IsCublasGemm(instruction);\n@@ -416,6 +418,7 @@ absl::Status NVPTXCompiler::AddFusionAutotuningPass(\n     HloPassPipeline* pipeline, HloModule* hlo_module,\n     const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n     stream_executor::StreamExecutor* stream_executor,\n+    const Compiler::TargetConfig* target_config,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n   if (stream_executor == nullptr) {\n     return absl::OkStatus();\n@@ -429,10 +432,10 @@ absl::Status NVPTXCompiler::AddFusionAutotuningPass(\n \n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<BlockLevelEmitterBackend>(\n-      stream_executor, &debug_options, this, shape_size_fn,\n+      &debug_options, this, shape_size_fn, target_config,\n       /*use_default_config=*/true));\n   backends.push_back(std::make_unique<NativeEmitterBackend>(\n-      stream_executor, &debug_options, this));\n+      &debug_options, this, target_config));\n \n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<AutotunerPass> autotuner_pass,"
        },
        {
            "sha": "873d03f6c5cd9111842ef9581121f71740b7ca36",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.h",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/198f34cf1fe7f4c04f11f947d190d097fc7631ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h?ref=198f34cf1fe7f4c04f11f947d190d097fc7631ab",
            "patch": "@@ -69,11 +69,13 @@ class NVPTXCompiler : public GpuCompiler {\n   bool RequiresCollectiveScheduleLinearizer(\n       const HloModule* module, se::StreamExecutor* stream_exec) override;\n \n+  // target_config must outlive the pipeline.\n   absl::Status AddConvAndGemmAutotuningPasses(\n       HloPassPipeline* pipeline, const se::GpuComputeCapability& gpu_version,\n       const CompileOptions& options, HloModule* hlo_module,\n       AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool,\n-      se::StreamExecutor* stream_exec) override;\n+      se::StreamExecutor* stream_exec,\n+      const Compiler::TargetConfig* target_config) override;\n \n   absl::Status AddGemmFusionAutotuningPasses(\n       HloPassPipeline* pipeline, HloModule* hlo_module,\n@@ -82,10 +84,12 @@ class NVPTXCompiler : public GpuCompiler {\n       const se::SemanticVersion& toolkit_version,\n       se::StreamExecutor* stream_executor) override;\n \n+  // target_config must outlive the pipeline.\n   absl::Status AddFusionAutotuningPass(\n       HloPassPipeline* pipeline, HloModule* hlo_module,\n       const CompileOptions& options, tsl::thread::ThreadPool* thread_pool,\n       stream_executor::StreamExecutor* stream_executor,\n+      const Compiler::TargetConfig* target_config,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn) override;\n \n   absl::Status RunCudnnCompilerPasses(HloModule* module,"
        }
    ],
    "stats": {
        "total": 325,
        "additions": 203,
        "deletions": 122
    }
}