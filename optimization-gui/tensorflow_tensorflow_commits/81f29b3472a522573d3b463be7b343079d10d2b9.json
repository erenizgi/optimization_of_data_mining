{
    "author": "dimvar",
    "message": "PR #32724: Disable only the test cases that are failing and enable 3 test targets on B200.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32724\n\nCopybara import of the project:\n\n--\nc3f4ff8ec6af27d24b61e2aa529585697b8aa77a by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nDisable only the test cases that are failing and enable 3 test targets on B200.\n\n--\n1f6e52218ec124bb52d4dba70aa7832311762465 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nDisable test case in cudnn_test that fails on Google's B200.\nKeep gpu_compiler_test off CI for now due to memory leak\nfound by ASAN, but don't revert the changes in the file,\nso it can be enabled more easily in the future.\n\n--\n42e501a41e43c174538ab186c659a072101b4ab2 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nDisable ConvWgradWithNHWCLayoutExecutesCorrectly only on Blackwell.\n\nMerging this change closes #32724\n\nPiperOrigin-RevId: 821992088",
    "sha": "81f29b3472a522573d3b463be7b343079d10d2b9",
    "files": [
        {
            "sha": "0e909c3952a9471c06cd619f0c8ec236546223ab",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=81f29b3472a522573d3b463be7b343079d10d2b9",
            "patch": "@@ -79,10 +79,6 @@ cc_library(\n xla_test(\n     name = \"cudnn_test\",\n     srcs = [\"cudnn_test.cc\"],\n-    backend_tags = {\n-        # TODO(b/445172709): Re-enable once fixed.\n-        \"b200\": [\"broken\"],\n-    },\n     backends = [\"gpu\"],\n     tags = [\"cuda-only\"],\n     deps = ["
        },
        {
            "sha": "573a6818b9dc25c859592cc5d3e527b020ef1994",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=81f29b3472a522573d3b463be7b343079d10d2b9",
            "patch": "@@ -71,11 +71,13 @@ class CuDnnFusionTest : public GpuCodegenTest {\n     debug_options.set_xla_gpu_cudnn_gemm_fusion_level(2);\n     return debug_options;\n   }\n+  se::CudaComputeCapability get_cuda_cc() const {\n+    se::StreamExecutor* executor = backend().default_stream_executor();\n+    return executor->GetDeviceDescription().cuda_compute_capability();\n+  }\n   bool IsAtLeastAmpereWithCuDnn9() {\n     se::StreamExecutor* executor = backend().default_stream_executor();\n-    return executor->GetDeviceDescription()\n-               .cuda_compute_capability()\n-               .IsAtLeastAmpere() &&\n+    return get_cuda_cc().IsAtLeastAmpere() &&\n            GetDnnVersionInfoOrDefault(executor).major_version() >= 9;\n   }\n   bool IsAtLeastCuDnn91() {\n@@ -232,6 +234,11 @@ ENTRY e {\n }\n \n TEST_F(CuDnnFusionExecutionTest, CompilerSupportsFusionsWithWorkspace) {\n+  if (get_cuda_cc().IsAtLeastBlackwell()) {\n+    // TODO(b/445172709): Re-enable once fixed.\n+    GTEST_SKIP();\n+  }\n+\n   const std::string kHloText = R\"(\n f {\n   a = f32[32,96] parameter(0)\n@@ -889,6 +896,10 @@ ENTRY Test {\n }\n \n TEST_F(CuDnnFusionExecutionTest, ConvWgradWithNHWCLayoutExecutesCorrectly) {\n+  if (get_cuda_cc().IsAtLeastBlackwell()) {\n+    // TODO(b/445172709): Re-enable once fixed.\n+    GTEST_SKIP();\n+  }\n   EXPECT_TRUE(RunAndCompare(R\"(\n fusion {\n   zero = f32[] constant(0)\n@@ -1179,10 +1190,7 @@ TEST_F(CuDnnFusionRewriteTest, AutotuningPicksCuDnnForS8BF16OnHopper) {\n   // The test case relies on measurements by the autotuner and current\n   // performance comparison of the backends. May need to be updated if\n   // the situation changes.\n-  if (backend()\n-          .default_stream_executor()\n-          ->GetDeviceDescription()\n-          .cuda_compute_capability() != se::CudaComputeCapability::Hopper()) {\n+  if (get_cuda_cc() != se::CudaComputeCapability::Hopper()) {\n     GTEST_SKIP() << \"The test is for Hopper.\";\n   }\n   MatchOptimizedHlo(R\"("
        },
        {
            "sha": "81aeed434075284fd0772be5caae03c1b3824688",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=81f29b3472a522573d3b463be7b343079d10d2b9",
            "patch": "@@ -1916,7 +1916,7 @@ xla_test(\n     name = \"gpu_compiler_test\",\n     srcs = [\"gpu_compiler_test.cc\"],\n     backend_tags = {\n-        # TODO(b/445172709): Re-enable once fixed.\n+        # TODO(b/445172709): Re-enable once nvbug 5552596 is fixed.\n         \"b200\": [\"broken\"],\n     },\n     backends = [\"gpu\"],\n@@ -3093,10 +3093,6 @@ xla_cc_test(\n xla_test(\n     name = \"determinism_test\",\n     srcs = [\"determinism_test.cc\"],\n-    backend_tags = {\n-        # TODO(b/445172709): Re-enable once fixed.\n-        \"b200\": [\"broken\"],\n-    },\n     backends = [\"gpu\"],\n     deps = [\n         \"//xla:literal\","
        },
        {
            "sha": "6d02dbe5ec28aedf0e7dd94c9e692470e7f554e7",
            "filename": "third_party/xla/xla/service/gpu/determinism_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc?ref=81f29b3472a522573d3b463be7b343079d10d2b9",
            "patch": "@@ -56,6 +56,11 @@ class DeterminismTest : public GpuCodegenTest {\n         DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n   }\n \n+  se::CudaComputeCapability get_cuda_cc() const {\n+    se::StreamExecutor* executor = backend().default_stream_executor();\n+    return executor->GetDeviceDescription().cuda_compute_capability();\n+  }\n+\n   // Runs the HLO several times with the same random inputs, and asserts the\n   // outputs are bitwise identical.\n   void AssertDeterminism(absl::string_view hlo_string, int num_runs = 10) {\n@@ -145,13 +150,7 @@ class DeterminismTest : public GpuCodegenTest {\n     EXPECT_TRUE(filecheck_result.value());\n   }\n \n-  bool IsAmpereOrLater() const {\n-    return backend()\n-        .default_stream_executor()\n-        ->GetDeviceDescription()\n-        .cuda_compute_capability()\n-        .IsAtLeastAmpere();\n-  }\n+  bool IsAmpereOrLater() const { return get_cuda_cc().IsAtLeastAmpere(); }\n \n   bool IsRocm() const {\n     return std::holds_alternative<stream_executor::RocmComputeCapability>(\n@@ -201,6 +200,10 @@ TEST_F(DeterminismTest, DeterministicTritonGemmUsesDefaultConfig) {\n     GTEST_SKIP() << \"Triton is not supported on non-NVIDIA and \"\n                     \"pre-Ampere NVIDIA GPUs.\";\n   }\n+  if (get_cuda_cc().IsAtLeastBlackwell()) {\n+    // TODO(b/445172709): Re-enable once fixed.\n+    GTEST_SKIP();\n+  }\n \n   constexpr absl::string_view kHloText = R\"(\n ENTRY e {"
        },
        {
            "sha": "9fd7a419c2f8f8a11c98e54dce2f11641f087c4a",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 57,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81f29b3472a522573d3b463be7b343079d10d2b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=81f29b3472a522573d3b463be7b343079d10d2b9",
            "patch": "@@ -145,6 +145,11 @@ class GpuCompilerTest : public HloTestBase {\n                         test_runner().HloModuleFromWrapped(executable.get()));\n     return {{optimized_module, std::move(executable)}};\n   }\n+\n+  se::CudaComputeCapability get_cuda_cc() const {\n+    se::StreamExecutor* executor = backend().default_stream_executor();\n+    return executor->GetDeviceDescription().cuda_compute_capability();\n+  }\n };\n \n TEST_F(GpuCompilerTest, CompiledProgramsCount) {\n@@ -645,14 +650,14 @@ class GpuCompilerTestWithAutotuneDb : public GpuCompilerTest {\n \n TEST_F(GpuCompilerTestWithAutotuneDb,\n        GemmFusionIsNoOpWhenGemmFusionAutotunerFallsBackToCublas) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-  if (!cc.IsAtLeastAmpere()) {\n+  if (!get_cuda_cc().IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"Autotuning results have only been generated for Ampere \"\n                  << \"and later GPUs\";\n   }\n+  if (get_cuda_cc().IsAtLeastBlackwell()) {\n+    // TODO(b/445172709): Re-enable once fixed.\n+    GTEST_SKIP();\n+  }\n   const absl::string_view hlo_string = R\"(\n HloModule test\n \n@@ -713,14 +718,14 @@ ENTRY main {\n \n TEST_F(GpuCompilerTestWithAutotuneDb,\n        CublasF8NumericallySameWithTritonFallbackAndWithoutTriton) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-  if (!cc.IsAtLeastHopper()) {\n+  if (!get_cuda_cc().IsAtLeastHopper()) {\n     GTEST_SKIP()\n         << \"Autotuning results have only been generated for Hopper GPUs\";\n   }\n+  if (get_cuda_cc().IsAtLeastBlackwell()) {\n+    // TODO(b/445172709): Re-enable once fixed.\n+    GTEST_SKIP();\n+  }\n   const absl::string_view hlo_string = R\"(\n HloModule test\n \n@@ -823,10 +828,7 @@ ENTRY main {\n                     .gpu_compute_capability();\n   bool is_cuda =\n       std::holds_alternative<stream_executor::CudaComputeCapability>(gpu_cc);\n-  auto cuda_cc = backend()\n-                     .default_stream_executor()\n-                     ->GetDeviceDescription()\n-                     .cuda_compute_capability();\n+  auto cuda_cc = get_cuda_cc();\n   auto rocm_cc = backend()\n                      .default_stream_executor()\n                      ->GetDeviceDescription()\n@@ -1119,11 +1121,6 @@ bool HasBlockLevelFusionConfig(const HloInstruction* fusion) {\n \n TEST_F(GpuCompilerTest,\n        LoopFusionRootedInTransposeIsRewrittenToBlockLevelByDefaultPostAmpere) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-\n   constexpr absl::string_view transpose_fusion_module = R\"(\n transpose {\n   p0 = f32[1024,1024,1024] parameter(0)\n@@ -1144,7 +1141,7 @@ ENTRY main {\n       GetOptimizedModuleForExecutable(transpose_fusion_module, config));\n   const HloModule* optimized_module = module_and_executable.first;\n \n-  if (cc.IsAtLeastAmpere()) {\n+  if (get_cuda_cc().IsAtLeastAmpere()) {\n     EXPECT_TRUE(HasBlockLevelFusionConfig(\n         optimized_module->entry_computation()->root_instruction()));\n   } else {\n@@ -1156,11 +1153,7 @@ ENTRY main {\n TEST_F(\n     GpuCompilerTest,\n     FusionBlockLevelRewriterRewritesKLoopTransposeWithBitcastIfTheSmallMinorDimIsAPowerOfTwo) {  // NOLINT(whitespace/line_length)\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-  if (!cc.IsAtLeastAmpere()) {\n+  if (!get_cuda_cc().IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"FusionBlockLevelRewriter requires Ampere+ to run.\";\n   }\n \n@@ -1395,18 +1388,14 @@ using GpuCompilerPassTest = GpuCompilerTest;\n \n TEST_F(GpuCompilerPassTest,\n        GpuCompilerRunsTritonGemmRewriterByDefaultFromAmpere) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-\n   bool is_rocm = std::holds_alternative<stream_executor::RocmComputeCapability>(\n       backend()\n           .default_stream_executor()\n           ->GetDeviceDescription()\n           .gpu_compute_capability());\n \n-  bool expect_triton_gemm_rewriter_has_run = cc.IsAtLeastAmpere() || is_rocm;\n+  bool expect_triton_gemm_rewriter_has_run =\n+      get_cuda_cc().IsAtLeastAmpere() || is_rocm;\n \n   constexpr absl::string_view constant_module = R\"(\n HloModule noop\n@@ -1433,13 +1422,8 @@ ENTRY main {\n \n TEST_F(GpuCompilerPassTest,\n        GpuCompilerRunsCustomKernelFusionByDefaultFromVolta) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-\n   bool expect_custom_kernel_fusion_rewriter_has_run =\n-      cc.major == se::CudaComputeCapability::kVolta;\n+      get_cuda_cc().major == se::CudaComputeCapability::kVolta;\n \n   constexpr absl::string_view constant_module = R\"(\n HloModule noop\n@@ -1617,11 +1601,7 @@ TEST_F(PassOrderTest, OffloadingPassesAreRunInCorrectOrder) {\n }\n \n TEST_F(PassOrderTest, FusionDispatchRunsAfterAllFusionPasses) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-  if (!cc.IsAtLeastAmpere()) {\n+  if (!get_cuda_cc().IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"fusion-dispatch requires Ampere+ to run.\";\n   }\n \n@@ -1707,11 +1687,7 @@ TEST_F(PassOrderTest, LHSRunsIfProfileDataIsAvailable) {\n }\n \n TEST_F(PassOrderTest, GemmFusionRunsAfterDotNormalizer) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-  if (!cc.IsAtLeastAmpere()) {\n+  if (!get_cuda_cc().IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"GemmFusion requires Ampere+ to run.\";\n   }\n   DebugOptions options = GetDebugOptionsForTest();\n@@ -1741,11 +1717,7 @@ TEST_F(PassOrderTest, NestGemmFusionRunsAfterGemmFusionAutotuner) {\n }\n \n TEST_F(PassOrderTest, TransposeDimensionGrouperRunsBeforeGemmRewriter) {\n-  auto cc = backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .cuda_compute_capability();\n-  if (!cc.IsAtLeastAmpere()) {\n+  if (!get_cuda_cc().IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"triton-gemm-rewriter requires at least Ampere to run.\";\n   }\n   if (!optimized_module_) {\n@@ -2103,11 +2075,7 @@ ENTRY main {\n }\n \n TEST_F(GpuCompilerTest, NoCudnnVectorizationOnHopperAndBeyond) {\n-  bool is_hopper_or_beyond = backend()\n-                                 .default_stream_executor()\n-                                 ->GetDeviceDescription()\n-                                 .cuda_compute_capability()\n-                                 .IsAtLeastHopper();\n+  bool is_hopper_or_beyond = get_cuda_cc().IsAtLeastHopper();\n \n   auto module = ParseAndReturnVerifiedModule(R\"(\n   HloModule TestModule"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 51,
        "deletions": 80
    }
}