{
    "author": "sergachev",
    "message": "PR #35098: [GPU] Wrap single instructions in fusions before autotuning.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/35098\n\nüìù Summary of Changes\nWrap single instructions into fusions before autotuning so that they get picked up.\n\nüéØ Justification\nCreating fusions out of single instructions earlier enables more autotuning.\n\nüöÄ Kind of Contribution\n‚ö°Ô∏è Performance Improvement ‚ôªÔ∏è Cleanup\n\nüìä Benchmark (for Performance Improvements)\n\nüß™ Unit Tests:\nyes\n\nüß™ Execution Tests:\nno\nCopybara import of the project:\n\n--\n9da7077726da5b5dc36f204ee004996313dd8102 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Wrap single instructions in fusions before autotuning.\n\nMerging this change closes #35098\n\nPiperOrigin-RevId: 843319000",
    "sha": "7940664e6b72631d8d6d5a78b83802539a7f1931",
    "files": [
        {
            "sha": "ad4de9bf2257acd9eff141c6bcab6f8890ad0462",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/triton_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 13,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc?ref=7940664e6b72631d8d6d5a78b83802539a7f1931",
            "patch": "@@ -685,7 +685,8 @@ ENTRY e {\n \n   MatchOptimizedHlo(kHloText, R\"(\n ; CHECK: ENTRY\n-; CHECK: transpose\n+; CHECK: fusion\n+; CHECK-SAME: kind=kLoop\n ; CHECK: fusion\n ; CHECK-SAME: kind=kCustom\n ; CHECK-SAME: \"__triton_nested_gemm_fusion\"\n@@ -710,7 +711,8 @@ ENTRY e {\n \n   MatchOptimizedHlo(kHloText, R\"(\n ; CHECK: ENTRY\n-; CHECK: transpose\n+; CHECK: fusion\n+; CHECK-SAME: kind=kLoop\n ; CHECK: fusion\n ; CHECK-SAME: kind=kCustom\n ; CHECK-SAME: \"__triton_nested_gemm_fusion\"\n@@ -1205,7 +1207,8 @@ ENTRY e {\n \n   MatchOptimizedHlo(kHloText, R\"(\n ; CHECK:      ENTRY\n-; CHECK:      concatenate\n+; CHECK:      fusion\n+; CHECK-SAME:   kind=kLoop\n ; CHECK:      fusion\n ; CHECK-SAME:   kind=kCustom\n ; CHECK-SAME:   \"__triton_nested_gemm_fusion\"\n@@ -1336,12 +1339,14 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           GetOptimizedModule(kHloText));\n \n-  EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              GmockMatch(m::Add(\n-                  m::Fusion(m::Parameter(), m::Parameter())\n-                      .WithFusionKind(HloInstruction::FusionKind::kCustom),\n-                  m::Fusion(m::Parameter(), m::Parameter())\n-                      .WithFusionKind(HloInstruction::FusionKind::kCustom))));\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(\n+          m::Fusion(m::Fusion(m::Parameter(), m::Parameter())\n+                        .WithFusionKind(HloInstruction::FusionKind::kCustom),\n+                    m::Fusion(m::Parameter(), m::Parameter())\n+                        .WithFusionKind(HloInstruction::FusionKind::kCustom))\n+              .WithFusionKind(HloInstruction::FusionKind::kLoop)));\n \n   EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n }\n@@ -1511,10 +1516,12 @@ ENTRY e {\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           GetOptimizedModule(kHloText));\n-  EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              GmockMatch(m::Sin(\n-                  m::Fusion(m::Parameter(), m::Parameter())\n-                      .WithFusionKind(HloInstruction::FusionKind::kCustom))));\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(\n+          m::Fusion(m::Fusion(m::Parameter(), m::Parameter())\n+                        .WithFusionKind(HloInstruction::FusionKind::kCustom))\n+              .WithFusionKind(HloInstruction::FusionKind::kLoop)));\n }\n \n // TODO(b/393299275): this should just be a fusion test and does not need to be"
        },
        {
            "sha": "d8b6a86f15528862a492e605784aa9055fbb97c8",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=7940664e6b72631d8d6d5a78b83802539a7f1931",
            "patch": "@@ -1487,6 +1487,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   TF_RETURN_IF_ERROR(RunAsyncDotPasses(hlo_module));\n   {\n     HloPassPipeline pipeline(\"autotune-fusion-emitters\");\n+    pipeline.AddPass<FusionWrapper>(gpu_target_config.device_description);\n     TF_RETURN_IF_ERROR(AddFusionAutotuningPass(\n         &pipeline, hlo_module, options, thread_pool.get_mutable(), stream_exec,\n         &gpu_target_config, ShapeSizeBytesFunction()));"
        },
        {
            "sha": "0e204cb3466ef4110d73b3a42c7a8791f6c47c36",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=7940664e6b72631d8d6d5a78b83802539a7f1931",
            "patch": "@@ -455,6 +455,24 @@ TEST_F(PersistedAutotuningTest, WriteResultsOnEachCompilation) {\n   }\n }\n \n+TEST_F(PersistedAutotuningTest, SingleOperationGetsAutotuned) {\n+  xla_gpu_dump_autotune_results_to_ = GetUniqueTempFilePath(\".txt\");\n+\n+  TF_EXPECT_OK(GetOptimizedModule(R\"(\n+e {\n+  a = f32[64,128] parameter(0)\n+  t = f32[128,64] transpose(a), dimensions={1,0}\n+})\")\n+                   .status());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::string autotune_results_str,\n+                          ReadNonEmptyFile(xla_gpu_dump_autotune_results_to_));\n+  AutotuneResults results;\n+  EXPECT_TRUE(tsl::protobuf::TextFormat::ParseFromString(autotune_results_str,\n+                                                         &results));\n+  EXPECT_THAT(results.results(), Not(IsEmpty()));\n+}\n+\n int64_t CountCopies(const HloComputation& computation) {\n   int64_t count = 0;\n   for (const auto& instruction : computation.instructions()) {"
        },
        {
            "sha": "982d34501f47301972864cb2fb6179d2811afae8",
            "filename": "third_party/xla/xla/service/gpu/tests/dot_bf16.hlo",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdot_bf16.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdot_bf16.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fdot_bf16.hlo?ref=7940664e6b72631d8d6d5a78b83802539a7f1931",
            "patch": "@@ -3,11 +3,11 @@\n // RUN: %if IS_ROCM %{ hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/mi200.txtpb --split-input-file --xla_gpu_autotune_level=0 --xla_gpu_enable_triton_gemm=false | FileCheck %s --check-prefixes=CHECK-SM80 %}\n \n \n-// CHECK-SM70: %[[convert1:.+]] = f32[1536,6144]{1,0} convert(%{{.+}})\n-// CHECK-SM70: %[[convert2:.+]] = f32[32,1536]{1,0} convert(%{{.+}})\n+// CHECK-SM70: %[[convert1:.+]] = f32[1536,6144]{1,0} fusion(%{{.+}})\n+// CHECK-SM70: %[[convert2:.+]] = f32[32,1536]{1,0} fusion(%{{.+}})\n // CHECK-SM70: custom-call(%[[convert1]], %[[convert2]]), custom_call_target=\"__cublas$gemm\"\n \n-// CHECK-SM80: %[[convert:.+]] = bf16[1536,6144]{1,0} convert(%{{.+}})\n+// CHECK-SM80: %[[convert:.+]] = bf16[1536,6144]{1,0} fusion(%{{.+}})\n // CHECK-SM80: %[[b:.+]] = bf16[32,1536]{1,0} parameter(1)\n // CHECK-SM80: custom-call(%[[convert]], %[[b]]), custom_call_target=\"__cublas$gemm\"\n \n@@ -22,11 +22,11 @@ ENTRY %computation1 {\n \n // -----\n \n-// CHECK-SM70: %[[convert1:.+]] = f32[1536,6144]{1,0} convert(%{{.+}})\n-// CHECK-SM70: %[[convert2:.+]] = f32[32,1536]{1,0} convert(%{{.+}})\n+// CHECK-SM70: %[[convert1:.+]] = f32[1536,6144]{1,0} fusion(%{{.+}})\n+// CHECK-SM70: %[[convert2:.+]] = f32[32,1536]{1,0} fusion(%{{.+}})\n // CHECK-SM70: (f32[6144,32]{1,0}, s8[4194304]{0}) custom-call(%[[convert1]], %[[convert2]]), custom_call_target=\"__cublas$gemm\"\n \n-// CHECK-SM80: %[[convert:.+]] = bf16[1536,6144]{1,0} convert(%{{.+}})\n+// CHECK-SM80: %[[convert:.+]] = bf16[1536,6144]{1,0} fusion(%{{.+}})\n // CHECK-SM80: %[[b:.+]] = bf16[32,1536]{1,0} parameter(1)\n // CHECK-SM80: (f32[6144,32]{1,0}, s8[4194304]{0}) custom-call(%[[convert]], %[[b]]), custom_call_target=\"__cublas$gemm\"\n "
        },
        {
            "sha": "fd858818c3dbcdde3b4140dc53bacfbb7d0b4043",
            "filename": "third_party/xla/xla/service/gpu/tests/sub_byte_collectives.hlo",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsub_byte_collectives.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsub_byte_collectives.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsub_byte_collectives.hlo?ref=7940664e6b72631d8d6d5a78b83802539a7f1931",
            "patch": "@@ -19,11 +19,11 @@ e {\n \n // CHECK-NOT: convert\n // CHECK:      s4[4,2]{1,0:E(4)} parameter\n-// CHECK-NEXT: s4[2,4]{1,0:E(4)} transpose\n+// CHECK:      s4[2,4]{1,0:E(4)} fusion(%{{.+}})\n // CHECK-NEXT: s8[2,2]{0,1} bitcast\n // CHECK:      s8[2,4]{0,1} all-gather-done\n // CHECK-NEXT: s4[4,4]{1,0:E(4)} bitcast\n-// CHECK-NEXT: s4[4,4]{1,0:E(4)} transpose\n+// CHECK:      s4[4,4]{1,0:E(4)} fusion(%{{.+}})\n \n // -----\n "
        },
        {
            "sha": "4c412e16ade5e0ad238a2b58a61a631db7fb5958",
            "filename": "third_party/xla/xla/service/gpu/transforms/cublas_gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_gemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_gemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_gemm_rewriter_test.cc?ref=7940664e6b72631d8d6d5a78b83802539a7f1931",
            "patch": "@@ -583,7 +583,7 @@ ENTRY test {\n     EXPECT_THAT(optimized_module->entry_computation()->root_instruction(),\n                 GmockMatch(m::GetTupleElement(\n                     m::CustomCall(m::Parameter(0), m::Parameter(1),\n-                                  m::Negate(m::Parameter(2))),\n+                                  m::Fusion(m::Parameter(2))),\n                     0)));\n   }\n }\n@@ -625,7 +625,7 @@ ENTRY test {\n     EXPECT_THAT(optimized_module->entry_computation()->root_instruction(),\n                 GmockMatch(m::GetTupleElement(\n                     m::CustomCall(m::Parameter(0), m::Parameter(1),\n-                                  m::Negate(m::Parameter(2))),\n+                                  m::Fusion(m::Parameter(2))),\n                     0)));\n   }\n }\n@@ -932,7 +932,7 @@ ENTRY AddDotsFunc {\n ; CHECK-DAG:         \"epilogue\":\"DEFAULT\"\n ; CHECK:           }\n ; CHECK-NEXT:  [[GEMM:%[^ ]+]] = f32[1024,1024]{1,0} get-tuple-element([[GEMM_TUPLE]]), index=0\n-; CHECK-NEXT:  ROOT [[OUT:%[^ ]+]] = f32[1024,1024]{1,0} add([[GEMM]], [[BIAS]])\n+; CHECK:  ROOT [[OUT:%[^ ]+]] = f32[1024,1024]{1,0} fusion([[GEMM]], [[BIAS]]), kind=kLoop\n )\");\n }\n \n@@ -1399,7 +1399,7 @@ ENTRY test {\n ; CHECK-DAG:         \"epilogue\":\"BIAS\"\n ; CHECK:           }\n ; CHECK-NEXT:    [[GETTUPLE:%[^ ]+]] = f32[4,4]{1,0} get-tuple-element([[MATMUL]]), index=0\n-; CHECK-NEXT:    ROOT [[OUT:%[^ ]+]] = f32[2,3]{1,0} slice([[GETTUPLE]]), slice={[0:2], [0:3]}\n+; CHECK:    ROOT [[OUT:%[^ ]+]] = f32[2,3]{1,0} fusion([[GETTUPLE]]), kind=kLoop\n       )\");\n }\n \n@@ -1775,7 +1775,7 @@ ENTRY test {\n ; CHECK-DAG:         \"epilogue\":\"RELU\"\n ; CHECK:           }\n ; CHECK:         [[MATMUL:%[^ ]+]] = f32[2,4]{1,0} get-tuple-element([[MATMUL_TUPLE]]), index=0\n-; CHECK-NEXT:    ROOT [[OUT:%[^ ]+]] = f32[2,2]{1,0} slice([[MATMUL]]), slice={[0:2], [0:2]}\n+; CHECK:    ROOT [[OUT:%[^ ]+]] = f32[2,2]{1,0} fusion([[MATMUL]]), kind=kLoop\n       )\");\n }\n \n@@ -3335,7 +3335,7 @@ ENTRY test {\n     EXPECT_THAT(optimized_module->entry_computation()->root_instruction(),\n                 GmockMatch(m::GetTupleElement(\n                     m::CustomCall(m::Parameter(0), m::Parameter(1),\n-                                  m::Negate(m::Parameter(2))),\n+                                  m::Fusion(m::Parameter(2))),\n                     0)));\n   }\n }"
        },
        {
            "sha": "449dfe481c19a6bea946444cd898a24e49a0d837",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7940664e6b72631d8d6d5a78b83802539a7f1931/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc?ref=7940664e6b72631d8d6d5a78b83802539a7f1931",
            "patch": "@@ -585,10 +585,9 @@ ENTRY AddDotsFunc {\n                     R\"(\n ; CHECK-LABEL: ENTRY %AddDotsFunc ({{.*}}: f32[3,2,5], {{.*}}: f32[5,3,4]) -> f32[5,2,4] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[3,2,5]{2,1,0} parameter(0)\n+; CHECK-DAG:     [[FUSION:%[^ ]+]] = f32[5,2,3]{2,1,0} fusion([[P0]])\n ; CHECK-DAG:     [[P1:%[^ ]+]] = f32[5,3,4]{2,1,0} parameter(1)\n-; CHECK-DAG:     [[FUSION:%[^ ]+]] = f32[5,2,3]{2,1,0} transpose([[P0]])\n-; CHECK-NEXT:    [[GEMM:%[^ ]+]] = {{.*}} custom-call([[FUSION]], [[P1]]),\n-; CHECK:           custom_call_target=\"<<CUBLAS_CUSTOM_CALL_TARGET_PLACEHOLDER>>\",\n+; CHECK:         {{[^ ]+}} = {{.*}} custom-call([[FUSION]], [[P1]]), custom_call_target=\"<<CUBLAS_CUSTOM_CALL_TARGET_PLACEHOLDER>>\",\n ; CHECK:           backend_config={\n ; CHECK-DAG:         \"alpha_real\":1\n ; CHECK-DAG:         \"alpha_imag\":0\n@@ -1545,8 +1544,7 @@ ENTRY DotFunc {\n ; CHECK-LABEL: ENTRY %DotFunc ({{.*}}: f32[3,3], {{.*}}: f32[3,3]) -> f32[3,3] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[3,3]{1,0} parameter(0)\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[3,3]{1,0} parameter(1)\n-; CHECK-NEXT:    [[GEMM:%[^ ]+]] = {{.*}} dot([[P0]], [[P1]]),\n-; CHECK:           lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+; CHECK-NEXT:    ROOT {{[^ ]+}} = f32[3,3]{1,0} fusion([[P0]], [[P1]]), kind=kLoop\n )\");\n }\n "
        }
    ],
    "stats": {
        "total": 88,
        "additions": 56,
        "deletions": 32
    }
}