{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Always pass tensors in the values map in the tiled emitter.\n\nPiperOrigin-RevId: 828426672",
    "sha": "22189ecacb3bb97d977f8f3270de9932a02185ae",
    "files": [
        {
            "sha": "f5237241b00f83f95eadb7cdea377e8539066e1f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 41,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/22189ecacb3bb97d977f8f3270de9932a02185ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/22189ecacb3bb97d977f8f3270de9932a02185ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=22189ecacb3bb97d977f8f3270de9932a02185ae",
            "patch": "@@ -372,13 +372,13 @@ absl::StatusOr<ScalarOrTensor> EmitScope(\n \n absl::StatusOr<ScalarOrTensor> EmitReduce(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_hlo_reduce,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values,\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n     const se::DeviceDescription& device_info) {\n   // At the moment, we should only emit a full reduction over a single\n   // dimension using a scalar as a neutral element.\n   const HloReduceInstruction& hlo_reduce =\n       *::xla::Cast<HloReduceInstruction>(tiled_hlo_reduce.hlo());\n-  TensorValue input = values[tiled_hlo_reduce.operand(0)].UnwrapTensor();\n+  TensorValue input = values[tiled_hlo_reduce.operand(0)];\n   llvm::ArrayRef<int64_t> input_shape = input.getType().getShape();\n   absl::Span<const int64_t> source_tensor_shape =\n       hlo_reduce.operand(0)->shape().dimensions();\n@@ -407,14 +407,12 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n                                          constant.UnwrapUnsafe());\n \n     TensorValue neutral = BroadcastInDims(\n-        b, MakeTensor(b, values[tiled_hlo_reduce.operand(1)].UnwrapUnsafe()),\n-        input_shape, /*dims=*/{});\n+        b, values[tiled_hlo_reduce.operand(1)], input_shape, /*dims=*/{});\n     input = mlir::cast<TensorValue>(\n         b.create<arith::SelectOp>(mask, input, neutral).getResult());\n   }\n \n-  Value init_value =\n-      MakeTensor(b, values[tiled_hlo_reduce.operand(1)].UnwrapScalar());\n+  Value init_value = values[tiled_hlo_reduce.operand(1)];\n \n   stablehlo::ReduceOp reduction =\n       b.create<stablehlo::ReduceOp>(input, init_value, reduction_dimension);\n@@ -511,21 +509,20 @@ ArrayRef<T> MakeArrayRef(const absl::Span<const T> span) {\n \n TensorValue EmitTiledBroadcast(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_broadcast,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   const SmallVector<int64_t>& input_tile_shape =\n       tiled_broadcast.operand(0)->tile_sizes();\n   const SmallVector<int64_t>& output_tile_shape = tiled_broadcast.tile_sizes();\n \n   if (input_tile_shape.empty() && output_tile_shape.empty()) {\n-    return MakeTensor(b, values[tiled_broadcast.operand(0)].UnwrapUnsafe());\n+    return values[tiled_broadcast.operand(0)];\n   }\n   CHECK(!output_tile_shape.empty());\n \n   SmallVector<int64_t> padded_output_tile_shape =\n       GetPaddedTileSizes(output_tile_shape);\n \n-  TensorValue input =\n-      MakeTensor(b, values[tiled_broadcast.operand(0)].UnwrapUnsafe());\n+  TensorValue input = values[tiled_broadcast.operand(0)];\n   return BroadcastInDims(b, input, padded_output_tile_shape,\n                          MakeArrayRef(tiled_broadcast.hlo()->dimensions()));\n }\n@@ -576,14 +573,18 @@ absl::StatusOr<TensorValue> EmitTiledIota(\n \n SmallVector<Value> GetRuntimeValues(\n     const TiledHloInstruction& tiled_hlo,\n-    const absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>&\n+    const absl::flat_hash_map<const TiledHloInstruction*, TensorValue>&\n         values) {\n   SmallVector<Value> runtime_values;\n   if (!tiled_hlo.runtime_variables().empty()) {\n     for (const TiledHloInstruction* rt : tiled_hlo.runtime_variables()) {\n       CHECK(values.contains(rt))\n           << absl::StrCat(\" runtime variable \", rt->ToString(), \" not found\");\n-      runtime_values.push_back(values.at(rt).UnwrapScalar());\n+      TensorValue value = values.at(rt);\n+      mlir::OpBuilder builder(value.getContext());\n+      builder.setInsertionPointAfterValue(value);\n+      runtime_values.push_back(\n+          xtile::ToScalarOp::create(builder, value.getLoc(), value));\n     }\n   }\n   return runtime_values;\n@@ -704,7 +705,7 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values);\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values);\n \n // Returns the number of iterations of the loop over the contracting\n // dimension of matrix multiplication.\n@@ -884,7 +885,7 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   // We expect to get a tiled HLO in form:\n   //\n   // left { ... }\n@@ -1052,7 +1053,7 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   VLOG(2) << \"EmitScaledDot: \" << tiled_hlo_dot.ToString();\n   const HloScaledDotInstruction& scaled_dot =\n       *::xla::Cast<HloScaledDotInstruction>(tiled_hlo_dot.hlo());\n@@ -1206,7 +1207,7 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n     const TiledHloInstruction& tiled_concatenate,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   const int64_t concatenate_dimension =\n       tiled_concatenate.hlo()->concatenate_dimension();\n \n@@ -1312,7 +1313,7 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n absl::StatusOr<ScalarOrTensor> EmitPad(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const TiledHloInstruction& tiled_pad,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values,\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n     Value pid) {\n   if (!IsTritonSupportedInstruction(*tiled_pad.hlo(),\n                                     device_info.gpu_compute_capability())) {\n@@ -1361,14 +1362,14 @@ absl::StatusOr<ScalarOrTensor> EmitPad(\n     mask = mask ? b.create<arith::AndIOp>(mask, cmp) : cmp;\n   }\n   if (!mask) {\n-    return values[tiled_operand];\n+    return MakeScalarOrTensor(b, values[tiled_operand]);\n   }\n   const TiledHloInstruction* padding_value = tiled_pad.operand(1);\n \n   TensorValue pad_value_splat =\n-      Splat(b, values[padding_value].UnwrapUnsafe(), padded_tile_sizes);\n-  auto result = ScalarOrTensor(b.create<arith::SelectOp>(\n-      mask, values[tiled_operand].UnwrapUnsafe(), pad_value_splat));\n+      Splat(b, values[padding_value], padded_tile_sizes);\n+  auto result = ScalarOrTensor(\n+      b.create<arith::SelectOp>(mask, values[tiled_operand], pad_value_splat));\n   return result;\n }\n \n@@ -1377,7 +1378,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n     const HloFusionInstruction* fusion, const TiledHloInstruction& tiled_hlo,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   const HloInstruction* hlo = tiled_hlo.hlo();\n   VLOG(4) << \"EmitTiledHloInstruction: \" << hlo->ToString();\n \n@@ -1469,51 +1470,46 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n     operands.reserve(hlo->operands().size());\n \n     for (const TiledHloInstruction* operand : tiled_hlo.operands()) {\n-      operands.push_back(MakeTensor(b, values[operand].UnwrapUnsafe()));\n+      operands.push_back(values[operand]);\n     }\n     TF_ASSIGN_OR_RETURN(Value result,\n                         EmitElementwise(b, device_info, *hlo, operands));\n     return MakeScalarOrTensor(b, result);\n   }\n \n   if (hlo->opcode() == HloOpcode::kReshape) {\n-    TF_ASSIGN_OR_RETURN(\n-        TensorValue reshaped_value,\n-        EmitTiledReshape(\n-            b, tiled_hlo.tile_sizes(),\n-            MakeTensor(b, values[tiled_hlo.operand(0)].UnwrapUnsafe())));\n+    TF_ASSIGN_OR_RETURN(TensorValue reshaped_value,\n+                        EmitTiledReshape(b, tiled_hlo.tile_sizes(),\n+                                         values[tiled_hlo.operand(0)]));\n     return MakeScalarOrTensor(b, reshaped_value);\n   }\n \n   if (hlo->opcode() == HloOpcode::kBitcast) {\n     TF_ASSIGN_OR_RETURN(\n         TensorValue bitcast_value,\n-        EmitTiledBitcast(\n-            b, tiled_hlo,\n-            MakeTensor(b, values[tiled_hlo.operand(0)].UnwrapUnsafe())));\n+        EmitTiledBitcast(b, tiled_hlo, values[tiled_hlo.operand(0)]));\n     return MakeScalarOrTensor(b, bitcast_value);\n   }\n \n   if (hlo->opcode() == HloOpcode::kTranspose) {\n     auto transpose =\n         ::xla::Cast<const HloTransposeInstruction>(tiled_hlo.hlo());\n     return MakeScalarOrTensor(\n-        b,\n-        EmitTiledTranspose(\n-            b, tiled_hlo.tile_sizes(), llvm::to_vector(transpose->dimensions()),\n-            MakeTensor(b, values[tiled_hlo.operand(0)].UnwrapUnsafe())));\n+        b, EmitTiledTranspose(b, tiled_hlo.tile_sizes(),\n+                              llvm::to_vector(transpose->dimensions()),\n+                              values[tiled_hlo.operand(0)]));\n   }\n \n   // Slice is currently supported only as an operation on indices\n   // which is pushed to loads and stores. We don't generate any further code.\n   if (hlo->opcode() == HloOpcode::kSlice) {\n-    return values[tiled_hlo.operand(0)];\n+    return MakeScalarOrTensor(b, values[tiled_hlo.operand(0)]);\n   }\n \n   if (hlo->opcode() == HloOpcode::kDynamicSlice) {\n     // Dynamic slice is implemented as a load and does not require any further\n     // processing.\n-    return values[tiled_hlo.operand(0)];\n+    return MakeScalarOrTensor(b, values[tiled_hlo.operand(0)]);\n   }\n \n   return absl::UnimplementedError(\n@@ -1529,7 +1525,7 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n-    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   VLOG(2) << \"EmitTiledComputation: \" << tiled_computation.ToString();\n   for (const TiledHloInstruction* tiled_hlo :\n        tiled_computation.instructions()) {\n@@ -1557,13 +1553,15 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n         ScalarOrTensor result,\n         EmitTiledHloInstruction(b, device_info, fusion, *tiled_hlo,\n                                 block_level_parameters, fn, pid, values));\n-    TF_RET_CHECK(values.insert({tiled_hlo, result}).second) << hlo->ToString();\n+    TF_RET_CHECK(\n+        values.insert({tiled_hlo, MakeTensor(b, result.UnwrapUnsafe())}).second)\n+        << hlo->ToString();\n     VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n   }\n   std::vector<ScalarOrTensor> results;\n   results.reserve(tiled_computation.GetRoots().size());\n   for (const auto* root : tiled_computation.GetRoots()) {\n-    results.push_back(values[root]);\n+    results.push_back(MakeScalarOrTensor(b, values[root]));\n   }\n   return std::move(results);\n }\n@@ -1785,7 +1783,7 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n           << tiled_hlo_computation.ToString();\n \n   Value tile_id = fn.getTileId();\n-  absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor> values;\n+  absl::flat_hash_map<const TiledHloInstruction*, TensorValue> values;\n   TF_ASSIGN_OR_RETURN(\n       auto results,\n       EmitTiledComputation(b, device_info, fusion, tiled_hlo_computation,"
        },
        {
            "sha": "8666f0fc2d24260f4786ff49dad0f73f337479f1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/22189ecacb3bb97d977f8f3270de9932a02185ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/22189ecacb3bb97d977f8f3270de9932a02185ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=22189ecacb3bb97d977f8f3270de9932a02185ae",
            "patch": "@@ -2033,6 +2033,7 @@ ENTRY main {\n \n // CHECK: %[[EXTRACT:.*]] = xtile.extract %[[IN]]{{.*}}\n // CHECK: %[[PAD_VALUE:.*]] = arith.constant 1.000000e+00 : f32\n+// CHECK: %[[TO_TENSOR_PAD_VALUE:.*]] = xtile.to_tensor %[[PAD_VALUE]]\n // CHECK: %[[TILE_OFFSET:.*]] = xla.apply_indexing\n // CHECK: %[[IOTA_VAL:.*]] = stablehlo.iota dim = 0 : tensor<32xi32>\n // CHECK: %[[IOTA:.*]] = stablehlo.broadcast_in_dim %[[IOTA_VAL]], dims = [0] : (tensor<32xi32>) -> tensor<32xi32>\n@@ -2042,7 +2043,6 @@ ENTRY main {\n // CHECK: %[[TO_TENSOR_THRESHOLD:.*]] = xtile.to_tensor %[[THRESHOLD]]\n // CHECK: %[[THRESHOLD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[TO_TENSOR_THRESHOLD]], dims = []\n // CHECK: %[[MASK:.*]] = arith.cmpi slt, %[[IOTA]], %[[THRESHOLD_SPLAT]]\n-// CHECK: %[[TO_TENSOR_PAD_VALUE:.*]] = xtile.to_tensor %[[PAD_VALUE]]\n // CHECK: %[[PAD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[TO_TENSOR_PAD_VALUE]], dims = []\n // CHECK: %[[SELECT:.*]] = arith.select %[[MASK]], %[[EXTRACT]], %[[PAD_SPLAT]]\n "
        },
        {
            "sha": "9e1ff8ffa237f4fb74d8732013dc355a1a7db9f7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/22189ecacb3bb97d977f8f3270de9932a02185ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/22189ecacb3bb97d977f8f3270de9932a02185ae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=22189ecacb3bb97d977f8f3270de9932a02185ae",
            "patch": "@@ -215,8 +215,8 @@ ENTRY e {\n       this, *module->GetComputationWithName(\"reduce_fusion\"),\n       block_level_parameters,\n       R\"(\n-CHECK: %[[REDUCE_INPUT:.*]] = arith.select {{.*}}\n CHECK: %[[INIT_VALUE_TO_TENSOR:.*]] = xtile.to_tensor %{{.*}} : f32\n+CHECK: %[[REDUCE_INPUT:.*]] = arith.select {{.*}}\n CHECK: %[[RES:.*]] = stablehlo.reduce(%[[REDUCE_INPUT]] init: %[[INIT_VALUE_TO_TENSOR]]) across dimensions = [0] : (tensor<256x16xf32>, tensor<f32>) -> tensor<16xf32>\n CHECK: reducer(%[[ARG_0:.*]]: tensor<f32>, %[[ARG_1:.*]]: tensor<f32>)  {\n CHECK:   %[[SUM:.*]] = arith.addf %[[ARG_0]], %[[ARG_1]] : tensor<f32>"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 41,
        "deletions": 43
    }
}