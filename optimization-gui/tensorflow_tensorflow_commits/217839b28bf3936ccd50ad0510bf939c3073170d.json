{
    "author": "sergachev",
    "message": "PR #30805: [GPU] Fix and re-enable layout assignment tests for convolutions.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30805\n\nCurrent 3 test cases are always skipped because xla_cc_test is deviceless.\nCopybara import of the project:\n\n--\nd65b26e2736ea77fd736e4fd4d1e685adf23ce7d by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Fix and re-enable layout assignment tests for convolutions.\n\nMerging this change closes #30805\n\nPiperOrigin-RevId: 802095412",
    "sha": "217839b28bf3936ccd50ad0510bf939c3073170d",
    "files": [
        {
            "sha": "7b85daad2430c2194b400cc776245fcce7b4135b",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=217839b28bf3936ccd50ad0510bf939c3073170d",
            "patch": "@@ -1,11 +1,16 @@\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"//xla:lit.bzl\", \"enforce_glob\", \"lit_test_suite\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\n     \"//xla/stream_executor:build_defs.bzl\",\n     \"if_gpu_is_configured\",\n )\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_google\", \"if_oss\")\n+load(\n+    \"//xla/tsl/platform:build_config_root.bzl\",\n+    \"tf_gpu_tests_tags\",\n+)\n load(\n     \"//xla/tsl/platform/default:cuda_build_defs.bzl\",\n     \"if_cuda_is_configured\",\n@@ -1912,6 +1917,31 @@ xla_cc_test(\n     ],\n )\n \n+lit_test_suite(\n+    name = \"hlo_lit_tests\",\n+    srcs = enforce_glob(\n+        [\n+            \"layout_assignment_v100.hlo\",\n+            \"layout_assignment_a100.hlo\",\n+            \"layout_assignment_h100.hlo\",\n+        ],\n+        include = [\n+            \"*.hlo\",\n+        ],\n+    ),\n+    cfg = \"//xla:lit.cfg.py\",\n+    data = [\n+        \"//xla/tools/hlo_opt:gpu_specs/a100_pcie_80.txtpb\",\n+        \"//xla/tools/hlo_opt:gpu_specs/h100_sxm.txtpb\",\n+        \"//xla/tools/hlo_opt:gpu_specs/v100.txtpb\",\n+    ],\n+    default_tags = tf_gpu_tests_tags(),\n+    tools = [\n+        \"//xla/tools:hlo-opt\",\n+        \"@llvm-project//llvm:FileCheck\",\n+    ],\n+)\n+\n cc_library(\n     name = \"move_copy_to_users\",\n     srcs = [\"move_copy_to_users.cc\"],"
        },
        {
            "sha": "f5f35fca937004f6b68b704e080b01054527d615",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_a100.hlo",
            "status": "added",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo?ref=217839b28bf3936ccd50ad0510bf939c3073170d",
            "patch": "@@ -0,0 +1,18 @@\n+// RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/a100_pcie_80.txtpb --split-input-file | FileCheck %s\n+\n+// CHECK: fused_transpose\n+// CHECK-NEXT: bf16[3,3,16,32]{3,2,1,0} parameter(0)\n+// CHECK-NEXT: bf16[144,32]{1,0} bitcast\n+// CHECK-NEXT: bf16[32,144]{1,0} transpose\n+// CHECK-SAME: dimensions={1,0}\n+// CHECK: (bf16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n+// CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n+\n+HloModule ConvCuDNN\n+\n+ENTRY main {\n+  Arg_0.1 = bf16[1,64,64,16]{3,2,1,0} parameter(0), sharding={replicated}\n+  Arg_1.2 = bf16[3,3,16,32]{3,2,1,0} parameter(1), sharding={replicated}\n+  ROOT convolution.3 = bf16[1,64,64,32]{3,2,1,0} convolution(Arg_0.1, Arg_1.2),\n+    window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n+}"
        },
        {
            "sha": "07b3a04afabf741606df6b62ee47a8bc190b2450",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_h100.hlo",
            "status": "added",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo?ref=217839b28bf3936ccd50ad0510bf939c3073170d",
            "patch": "@@ -0,0 +1,18 @@\n+// RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/h100_sxm.txtpb --split-input-file | FileCheck %s\n+\n+// CHECK: fused_transpose\n+// CHECK-NEXT: f8e4m3fn[3,3,16,32]{3,2,1,0} parameter(0)\n+// CHECK-NEXT: f8e4m3fn[144,32]{1,0} bitcast\n+// CHECK-NEXT: f8e4m3fn[32,144]{1,0} transpose\n+// CHECK-SAME: dimensions={1,0}\n+// CHECK: (f8e4m3fn[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n+// CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n+\n+HloModule ConvCuDNN\n+\n+ENTRY main {\n+  Arg_0.1 = f8e4m3fn[1,64,64,16]{3,2,1,0} parameter(0), sharding={replicated}\n+  Arg_1.2 = f8e4m3fn[3,3,16,32]{3,2,1,0} parameter(1), sharding={replicated}\n+  ROOT convolution.3 = f8e4m3fn[1,64,64,32]{3,2,1,0} convolution(Arg_0.1, Arg_1.2),\n+    window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n+}"
        },
        {
            "sha": "8b4a046c9c8d4c72a7584429ac6fe326ec6899a7",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 77,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc?ref=217839b28bf3936ccd50ad0510bf939c3073170d",
            "patch": "@@ -696,83 +696,6 @@ ENTRY entry {\n       absl_testing::IsOkAndHolds(true));\n }\n \n-TEST_F(LayoutAssignmentTest, ConvCuDNNF8) {\n-  if (!GetCudaComputeCapability().IsAtLeast(\n-          se::CudaComputeCapability::kHopper)) {\n-    GTEST_SKIP() << \"FP8 convolutions require HOPPER or newer archiecture.\";\n-  }\n-\n-  const char* hlo = R\"(\n-\n-  HloModule jit_conv_general_dilated\n-\n-  ENTRY main.4 {\n-    Arg_0 = f8e4m3fn[1,64,64,16]{3,2,1,0} parameter(0)\n-    Arg_1 = f8e4m3fn[3,3,16,32]{3,2,1,0} parameter(1)\n-    ROOT conv = f8e4m3fn[1,64,64,32]{3,2,1,0} convolution(Arg_0, Arg_1), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n-  }\n-)\";\n-\n-  MatchOptimizedHlo(hlo, R\"(\n-  // CHECK: [[P0:%[^ ]+]] = f8e4m3fn[1,64,64,16]{3,2,1,0} parameter(0)\n-  // CHECK: [[P1:%[^ ]+]] = f8e4m3fn[3,3,16,32]{3,2,1,0} parameter(1)\n-  // CHECK-NEXT: [[P2:%[^ ]+]] = f8e4m3fn[32,3,3,16]{3,2,1,0} transpose([[P1]]), dimensions={3,0,1,2}\n-  // CHECK-NEXT: [[CONV:%[^ ]+]] = (f8e4m3fn[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call([[P0]], [[P2]]), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForwardGraph\"\n-  )\");\n-}\n-\n-TEST_F(LayoutAssignmentTest, ConvCuDNNBF16) {\n-  if (!GetCudaComputeCapability().IsAtLeast(\n-          se::CudaComputeCapability::kAmpere)) {\n-    GTEST_SKIP() << \"Conv with Bfloat16 uses NHWC layout for \"\n-                    \"architectures with Tensor Cores.\";\n-  }\n-\n-  const char* hlo = R\"(\n-\n-  HloModule jit_conv_general_dilated\n-\n-  ENTRY main.4 {\n-    Arg_0.1 = bf16[1,64,64,16]{3,2,1,0} parameter(0), sharding={replicated}\n-    Arg_1.2 = bf16[3,3,16,32]{3,2,1,0} parameter(1), sharding={replicated}\n-    ROOT convolution.3 = bf16[1,64,64,32]{3,2,1,0} convolution(Arg_0.1, Arg_1.2), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, metadata={op_name=\"jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 64, 64, 16) rhs_shape=(3, 3, 16, 32) precision=None preferred_element_type=None]\" source_file=\"/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py\" source_line=438}\n-  }\n-)\";\n-\n-  MatchOptimizedHlo(hlo, R\"(\n-  // CHECK: [[P0:%[^ ]+]] = bf16[1,64,64,16]{3,2,1,0} parameter(0), sharding={replicated}\n-  // CHECK: [[P1:%[^ ]+]] = bf16[3,3,16,32]{3,2,1,0} parameter(1), sharding={replicated}\n-  // CHECK-NEXT: [[P2:%[^ ]+]] = bf16[32,3,3,16]{3,2,1,0} transpose([[P1]]), dimensions={3,0,1,2}\n-  // CHECK-NEXT: %cudnn-conv.1 = (bf16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call([[P0]], [[P2]]), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\"\n-  )\");\n-}\n-\n-TEST_F(LayoutAssignmentTest, ConvCuDNNFP16) {\n-  if (!GetCudaComputeCapability().IsAtLeast(\n-          se::CudaComputeCapability::kVolta)) {\n-    GTEST_SKIP() << \"Conv with FP16 uses NHWC layout for \"\n-                    \"architectures with Tensor Cores.\";\n-  }\n-\n-  const char* hlo = R\"(\n-\n-  HloModule jit_conv_general_dilated\n-\n-  ENTRY main.4 {\n-    Arg_0.1 = f16[1,64,64,16]{3,2,1,0} parameter(0), sharding={replicated}\n-    Arg_1.2 = f16[3,3,16,32]{3,2,1,0} parameter(1), sharding={replicated}\n-    ROOT convolution.3 = f16[1,64,64,32]{3,2,1,0} convolution(Arg_0.1, Arg_1.2), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n-  }\n-)\";\n-\n-  MatchOptimizedHlo(hlo, R\"(\n-  // CHECK: [[P0:%[^ ]+]] = f16[1,64,64,16]{3,2,1,0} parameter(0), sharding={replicated}\n-  // CHECK: [[P1:%[^ ]+]] = f16[3,3,16,32]{3,2,1,0} parameter(1), sharding={replicated}\n-  // CHECK-NEXT: [[P2:%[^ ]+]] = f16[32,3,3,16]{3,2,1,0} transpose([[P1]]), dimensions={3,0,1,2}\n-  // CHECK-NEXT: %cudnn-conv.1 = (f16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call([[P0]], [[P2]]), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\"\n-  )\");\n-}\n-\n TEST_F(LayoutAssignmentTest, ReduceOperandLayout) {\n   const char* module_str = R\"(\n scalar_add_computation {"
        },
        {
            "sha": "f91bea89a7cce5ca6659fd6c7d9713df4c754ee5",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_v100.hlo",
            "status": "added",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/217839b28bf3936ccd50ad0510bf939c3073170d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo?ref=217839b28bf3936ccd50ad0510bf939c3073170d",
            "patch": "@@ -0,0 +1,18 @@\n+// RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/v100.txtpb --split-input-file | FileCheck %s\n+\n+// CHECK: fused_transpose\n+// CHECK-NEXT: f16[3,3,16,32]{3,2,1,0} parameter(0)\n+// CHECK-NEXT: f16[144,32]{1,0} bitcast\n+// CHECK-NEXT: f16[32,144]{1,0} transpose\n+// CHECK-SAME: dimensions={1,0}\n+// CHECK: (f16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n+// CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n+\n+HloModule ConvCuDNN\n+\n+ENTRY main {\n+  Arg_0.1 = f16[1,64,64,16]{3,2,1,0} parameter(0), sharding={replicated}\n+  Arg_1.2 = f16[3,3,16,32]{3,2,1,0} parameter(1), sharding={replicated}\n+  ROOT convolution.3 = f16[1,64,64,32]{3,2,1,0} convolution(Arg_0.1, Arg_1.2),\n+    window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n+}"
        }
    ],
    "stats": {
        "total": 161,
        "additions": 84,
        "deletions": 77
    }
}