{
    "author": "metaflow",
    "message": "[XLA:GPU] accept dots with batch dimension in generic triton emitter\n\n.. as long as the order of contracting and free dimensions still match triton expectations and we don't need to add a transpose.\n\nPiperOrigin-RevId: 798984441",
    "sha": "44ade47caf66bf252c40bf61302501be36cca921",
    "files": [
        {
            "sha": "8b66a7eec546d710c644241a27b3916fbba74fee",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_int4_device_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc?ref=44ade47caf66bf252c40bf61302501be36cca921",
            "patch": "@@ -58,9 +58,10 @@ class TritonTest : public GpuCodegenTest {\n         .set_xla_gpu_experimental_enable_subchannel_dequantisation_fusion(true);\n     // TODO(b/393299275): remove this once flag is on by default and test is\n     // updated.\n-    // Note that we do NOT set\n-    // xla_gpu_unsupported_generic_triton_emitter_opts1 here as test\n-    // will run the pass forcefully later.\n+    // Note that we clear\n+    // xla_gpu_unsupported_generic_triton_emitter_opts here to disable\n+    // nest gemm fusion pass as test will run the pass manually.\n+    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n     return debug_options;\n   }\n "
        },
        {
            "sha": "f543963f090ed2b4d689a24d4891a26fe946e105",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_parametrized_legacy_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc?ref=44ade47caf66bf252c40bf61302501be36cca921",
            "patch": "@@ -75,6 +75,9 @@ class MixedTypeTest : public GpuCodegenTest,\n     debug_options.set_xla_gpu_cublas_fallback(false);\n     // Always rewrite Gemms with Triton regardless of size.\n     debug_options.set_xla_gpu_gemm_rewrite_size_threshold(0);\n+    // That is a test for legacy Triton emitter that is being replaced by the\n+    // generic Triton emitter.\n+    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n     return debug_options;\n   }\n };"
        },
        {
            "sha": "60336f5b3b778130e7f6f912208ed18c9a67b4de",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=44ade47caf66bf252c40bf61302501be36cca921",
            "patch": "@@ -69,7 +69,6 @@ limitations under the License.\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n@@ -554,10 +553,7 @@ ENTRY e {\n   MatchOptimizedHlo(kHloText, R\"(\n ; CHECK: reduce\n ; CHECK: ENTRY\n-; CHECK-NEXT: parameter\n-; CHECK-NEXT: parameter\n-; CHECK-NEXT: kCustom\n-; CHECK-NEXT: {{kLoop|kInput}}\n+; CHECK: ROOT {{.*}} fusion({{.*}}), kind=kLoop\n )\");\n \n   EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-3}));\n@@ -580,9 +576,14 @@ ENTRY e {\n })\";\n \n   MatchOptimizedHlo(kHloText, R\"(\n-; CHECK: f{{(16|32)}}[3,55,20]\n-; CHECK: {\"block_m\":16,\"block_n\":64,\"block_k\":32,\"split_k\":3,\"num_stages\":1,\"num_warps\":2,\"num_ctas\":1}\n-; CHECK: f16[55,20]{1,0} {{(reduce|fusion)}}\n+; CHECK: f16[55,3,40]{2,1,0} fusion\n+; CHECK-SAME: \"kind\":\"__triton_nested_gemm_fusion\"\n+; CHECK-SAME: \"sizes\":[\"16\",\"1\",\"32\"]\n+; CHECK: f16[3,40,20]{2,1,0} fusion\n+; CHECK-SAME: \"kind\":\"__triton_nested_gemm_fusion\"\n+; CHECK-SAME: \"sizes\":[\"1\",\"32\",\"64\"]\n+; CHECK: ENTRY\n+; CHECK: ROOT {{.*}} f16[55,20]{1,0} fusion({{.*}}), kind=kLoop\n )\");\n \n   EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));"
        },
        {
            "sha": "ab265ea2715a815da9ec702451d3c492d0bd7f9b",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 49,
            "deletions": 20,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/44ade47caf66bf252c40bf61302501be36cca921/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=44ade47caf66bf252c40bf61302501be36cca921",
            "patch": "@@ -1088,32 +1088,61 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n         compute_capability_(compute_capability) {}\n \n  private:\n-  absl::Status AcceptDotInstruction(const HloDotInstruction* dot) {\n-    auto dims = dot->dot_dimension_numbers();\n+  absl::Status AcceptDotOperand(const HloInstruction* operand,\n+                                absl::Span<const int64_t> batch_dims,\n+                                absl::Span<const int64_t> contracting_dims,\n+                                bool is_lhs) {\n+    if (contracting_dims.size() != 1) {\n+      return absl::InternalError(\n+          absl::StrCat(\"Expected \", is_lhs ? \"LHS\" : \"RHS\",\n+                       \" operand with exactly one contracting dimension, got \",\n+                       contracting_dims.size()));\n+    }\n+\n+    TF_ASSIGN_OR_RETURN(\n+        std::vector<int64_t> non_contracting_dimensions,\n+        GetNonContractingDims(operand->shape(), batch_dims, contracting_dims));\n+\n+    if (non_contracting_dimensions.size() != 1) {\n+      return absl::InternalError(absl::StrCat(\n+          \"Expected \", is_lhs ? \"LHS\" : \"RHS\",\n+          \" operand with exactly one non-contracting dimension, got \",\n+          non_contracting_dimensions.size()));\n+    }\n+\n+    if (is_lhs) {\n+      if (non_contracting_dimensions[0] >= contracting_dims[0]) {\n+        return absl::InternalError(absl::StrCat(\n+            \"Expected LHS non-contracting dimension to be before contracting \"\n+            \"dimension, got \",\n+            non_contracting_dimensions[0], \" >= \", contracting_dims[0]));\n+      }\n+    } else {\n+      if (non_contracting_dimensions[0] <= contracting_dims[0]) {\n+        return absl::InternalError(absl::StrCat(\n+            \"Expected RHS non-contracting dimension to be after contracting \"\n+            \"dimension, got \",\n+            non_contracting_dimensions[0], \" <= \", contracting_dims[0]));\n+      }\n+    }\n+    return absl::OkStatus();\n+  }\n \n+  absl::Status AcceptDotInstruction(const HloDotInstruction* dot) {\n     if (IsFeatureEnabled(\n             dot->GetModule(),\n             DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES)) {\n       return absl::OkStatus();\n     }\n-    if (dot->shape().dimensions().size() != 2 ||\n-        dot->operand(0)->shape().dimensions().size() != 2 ||\n-        dot->operand(1)->shape().dimensions().size() != 2) {\n-      return absl::InternalError(\n-          absl::StrCat(\"Only basic 2D dot shape is supported in nested GEMM \"\n-                       \"fusion, got \",\n-                       dot->shape().ToString()));\n-    }\n-    if (dims.lhs_contracting_dimensions().size() != 1 ||\n-        dims.lhs_contracting_dimensions(0) != 1 ||\n-        dims.rhs_contracting_dimensions().size() != 1 ||\n-        dims.rhs_contracting_dimensions(0) != 0) {\n-      return absl::InternalError(\n-          absl::StrCat(\"Expected dot with LHS contracting dimension 1 \"\n-                       \"and RHS contracting dimension 0, got \",\n-                       dims.SerializeAsString()));\n-    }\n-\n+    const HloInstruction* lhs = dot->operand(0);\n+    const HloInstruction* rhs = dot->operand(1);\n+    auto dims = dot->dot_dimension_numbers();\n+    TF_RETURN_IF_ERROR(AcceptDotOperand(lhs, dims.lhs_batch_dimensions(),\n+                                        dims.lhs_contracting_dimensions(),\n+                                        /*is_lhs=*/true));\n+    TF_RETURN_IF_ERROR(AcceptDotOperand(rhs, dims.rhs_batch_dimensions(),\n+                                        dims.rhs_contracting_dimensions(),\n+                                        /*is_lhs=*/false));\n     return absl::OkStatus();\n   }\n "
        }
    ],
    "stats": {
        "total": 96,
        "additions": 65,
        "deletions": 31
    }
}