{
    "author": "akuegel",
    "message": "Prefactoring to make MLIRContext available in LoopFusion class.\n\nWhile there, also make sure that the same MLIRContext instance is reused for\nall HLO passes.\n\nPiperOrigin-RevId: 810380946",
    "sha": "7b162bb889b92ea6dc7890943dfb087d446573aa",
    "files": [
        {
            "sha": "0e146fde8d8bcbe02e8ae220e7f04f5e6de5af95",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -440,6 +440,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -473,6 +474,7 @@ xla_test(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -514,6 +516,7 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -551,6 +554,7 @@ xla_test(\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -667,6 +671,7 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n     alwayslink = True,\n )\n@@ -687,6 +692,7 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n         \"//xla/stream_executor/rocm:rocm_platform_id\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n     alwayslink = True,\n )\n@@ -697,8 +703,8 @@ cc_library(\n     deps = [\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/service:compiler\",\n-        \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -820,6 +826,7 @@ xla_cc_binary(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:platform_port\",\n     ] + if_cuda_is_configured([\n         \":factory_cuda\","
        },
        {
            "sha": "d9da265a48c20f45d5ebfe8efeb606bd70945c51",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/strings/ascii.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/autotuner.h\"\n #include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n@@ -82,7 +83,8 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetModule(\n }\n \n absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n-                      const std::string& autotune_cache_mode_str) {\n+                      const std::string& autotune_cache_mode_str,\n+                      mlir::MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(std::string platform_name,\n                       PlatformUtil::CanonicalPlatformName(\"gpu\"));\n \n@@ -101,8 +103,8 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n   auto& registry = stream_executor::PlatformObjectRegistry::GetGlobalRegistry();\n   TF_ASSIGN_OR_RETURN(const GetCodegenBackends::Type& get_codegen_backends,\n                       registry.FindObject<GetCodegenBackends>(platform->id()));\n-  std::vector<std::unique_ptr<CodegenBackend>> backends =\n-      get_codegen_backends(stream_executor, &debug_options, compiler.get());\n+  std::vector<std::unique_ptr<CodegenBackend>> backends = get_codegen_backends(\n+      stream_executor, &debug_options, compiler.get(), mlir_context);\n \n   std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n       std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n@@ -176,7 +178,9 @@ int main(int argc, char* argv[]) {\n   tsl::port::InitMain(usage_string.c_str(), &argc, &argv);\n   auto module = xla::gpu::GetModule(hlo_file);\n   CHECK_OK(module.status());\n-  CHECK_OK(xla::gpu::Autotune(*module.value(), cache_dir, autotune_cache_mode));\n+  mlir::MLIRContext mlir_context;\n+  CHECK_OK(xla::gpu::Autotune(*module.value(), cache_dir, autotune_cache_mode,\n+                              &mlir_context));\n   std::cout << module.value()->ToString() << std::endl;\n   return 0;\n }"
        },
        {
            "sha": "c914a4d5dc37be330c7cf3cb6fc3d2094cbca2a6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -30,7 +31,8 @@ namespace gpu {\n \n struct GetCodegenBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n-      stream_executor::StreamExecutor*, const DebugOptions*, Compiler*)>;\n+      stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n+      mlir::MLIRContext* mlir_context)>;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "fd15f5173d7c4f8a1a757d76d7b983c6306cdc2f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n@@ -35,10 +36,11 @@ namespace gpu {\n \n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n-    const DebugOptions* debug_options, Compiler* compiler) {\n+    const DebugOptions* debug_options, Compiler* compiler,\n+    mlir::MLIRContext* mlir_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n-  backends.push_back(std::make_unique<TritonBackend>(stream_executor,\n-                                                     debug_options, compiler));\n+  backends.push_back(std::make_unique<TritonBackend>(\n+      stream_executor, debug_options, compiler, mlir_context));\n   backends.push_back(std::make_unique<CublasBackend>(stream_executor,\n                                                      debug_options, compiler));\n   backends.push_back(std::make_unique<CublasLtBackend>("
        },
        {
            "sha": "889aef45037cbaa386505ca699de6e2bab8a2f0d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_rocm.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/factory.h\"\n@@ -33,10 +34,11 @@ namespace gpu {\n \n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n-    const DebugOptions* debug_options, Compiler* compiler) {\n+    const DebugOptions* debug_options, Compiler* compiler,\n+    mlir::MLIRContext* mlir_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n-  backends.push_back(std::make_unique<TritonBackend>(stream_executor,\n-                                                     debug_options, compiler));\n+  backends.push_back(std::make_unique<TritonBackend>(\n+      stream_executor, debug_options, compiler, mlir_context));\n   backends.push_back(std::make_unique<CublasBackend>(stream_executor,\n                                                      debug_options, compiler));\n   return backends;"
        },
        {
            "sha": "1756ab5d47ee68004bf0226c73e66f7638ca1fa1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n@@ -77,7 +78,8 @@ HloCostAnalysis::Options PriorityFusionOptions() {\n // custom call, otherwise we will try to rewrite it to a cublas custom call.\n absl::Status FissionToCublas(HloModule* hlo_module,\n                              const se::DeviceDescription& device_description,\n-                             bool rewrite_to_cublaslt) {\n+                             bool rewrite_to_cublaslt,\n+                             mlir::MLIRContext* mlir_context) {\n   hlo_module->mutable_config()\n       .mutable_debug_options()\n       .set_xla_gpu_enable_cublaslt(rewrite_to_cublaslt);\n@@ -114,7 +116,8 @@ absl::Status FissionToCublas(HloModule* hlo_module,\n     is_rewritten_to_cublas_custom_call |= changed;\n \n     PriorityFusion fusion_pass(\n-        /*thread_pool=*/nullptr, device_description, PriorityFusionOptions());\n+        /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n+        mlir_context);\n     TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n   }\n \n@@ -126,10 +129,12 @@ absl::Status FissionToCublas(HloModule* hlo_module,\n }\n \n absl::Status FissionToCustomKernel(\n-    HloModule* hlo_module, const se::DeviceDescription& device_description) {\n+    HloModule* hlo_module, const se::DeviceDescription& device_description,\n+    mlir::MLIRContext* mlir_context) {\n   CustomKernelFusionRewriter custom_kernel_fusion_rewriter(&device_description);\n   PriorityFusion fusion_pass(\n-      /*thread_pool=*/nullptr, device_description, PriorityFusionOptions());\n+      /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n+      mlir_context);\n   TF_ASSIGN_OR_RETURN(bool is_rewritten_to_custom_kernel,\n                       custom_kernel_fusion_rewriter.Run(hlo_module));\n   TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n@@ -233,7 +238,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   std::unique_ptr<HloModule> cublas_hlo_module = hlo_module->Clone();\n   if (FissionToCublas(cublas_hlo_module.get(),\n                       target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/false)\n+                      /*rewrite_to_cublaslt=*/false, mlir_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> cublas_configs,\n@@ -248,7 +253,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   std::unique_ptr<HloModule> cublaslt_hlo_module = hlo_module->Clone();\n   if (FissionToCublas(cublaslt_hlo_module.get(),\n                       target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/true)\n+                      /*rewrite_to_cublaslt=*/true, mlir_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> cublaslt_configs,\n@@ -262,7 +267,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n \n   std::unique_ptr<HloModule> custom_kernel_hlo_module = hlo_module->Clone();\n   if (FissionToCustomKernel(custom_kernel_hlo_module.get(),\n-                            target_config().device_description)\n+                            target_config().device_description, mlir_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> custom_kernel_configs,\n@@ -308,9 +313,9 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n       computation->parent()->config().debug_options().xla_gpu_enable_cublaslt();\n \n   if (!use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(FissionToCublas(hlo_module,\n-                                       target_config().device_description,\n-                                       /*rewrite_to_cublaslt=*/false));\n+    TF_RETURN_IF_ERROR(\n+        FissionToCublas(hlo_module, target_config().device_description,\n+                        /*rewrite_to_cublaslt=*/false, mlir_context_));\n     for (HloComputation* computation :\n          hlo_module->MakeNonfusionComputations()) {\n       for (HloInstruction* instruction : computation->instructions()) {\n@@ -324,9 +329,9 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   }\n \n   if (use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(FissionToCublas(hlo_module,\n-                                       target_config().device_description,\n-                                       /*rewrite_to_cublaslt=*/true));\n+    TF_RETURN_IF_ERROR(\n+        FissionToCublas(hlo_module, target_config().device_description,\n+                        /*rewrite_to_cublaslt=*/true, mlir_context_));\n     for (HloComputation* computation :\n          hlo_module->MakeNonfusionComputations()) {\n       for (HloInstruction* instruction : computation->instructions()) {\n@@ -342,8 +347,8 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   }\n \n   if (config.Is<CustomKernelBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(\n-        FissionToCustomKernel(hlo_module, target_config().device_description));\n+    TF_RETURN_IF_ERROR(FissionToCustomKernel(\n+        hlo_module, target_config().device_description, mlir_context_));\n     for (HloComputation* computation : hlo_module->computations()) {\n       if (IsCustomKernel(computation)) {\n         TF_RETURN_IF_ERROR(custom_kernel_backend_.ApplyConfig("
        },
        {
            "sha": "fb142dd4b9931b823d6da100ecfc18b28b71f28b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -22,11 +22,12 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n #include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -43,11 +44,13 @@ namespace gpu {\n class FissionBackend : public GpuCodegenBackend {\n  public:\n   explicit FissionBackend(stream_executor::StreamExecutor* stream_executor,\n-                          const DebugOptions* debug_options, Compiler* compiler)\n+                          const DebugOptions* debug_options, Compiler* compiler,\n+                          mlir::MLIRContext* mlir_context)\n       : GpuCodegenBackend(\"Fission\", stream_executor, debug_options, compiler),\n         cublas_backend_(stream_executor, debug_options, compiler),\n         cublaslt_backend_(stream_executor, debug_options, compiler),\n-        custom_kernel_backend_(stream_executor, debug_options, compiler) {}\n+        custom_kernel_backend_(stream_executor, debug_options, compiler),\n+        mlir_context_(mlir_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;\n@@ -61,6 +64,7 @@ class FissionBackend : public GpuCodegenBackend {\n   CublasBackend cublas_backend_;\n   CublasLtBackend cublaslt_backend_;\n   CustomKernelBackend custom_kernel_backend_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "ccfda6457b1566205a832224bf7f936748a277a1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -70,13 +71,14 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   FissionBackend backend_;\n+  mlir::MLIRContext mlir_context_;\n \n   FissionBackendTest()\n       : backend_(PlatformUtil::GetDefaultPlatform()\n                      .value()\n                      ->ExecutorForDevice(0)\n                      .value(),\n-                 &debug_options_, &compiler_) {}\n+                 &debug_options_, &compiler_, &mlir_context_) {}\n };\n \n TEST_F(FissionBackendTest, CanCreateCublasBackend) {"
        },
        {
            "sha": "7978303a2375e646f34d312a2f2b67755ce20f85",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -201,15 +201,17 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonBackend::RunHloPasses(\n   HloCostAnalysis::Options priority_fusion_options;\n   priority_fusion_options.count_multiple_input_accesses = true;\n   PriorityFusion priority_fusion(\n-      /*thread_pool=*/nullptr, gpu_device_info, priority_fusion_options);\n+      /*thread_pool=*/nullptr, gpu_device_info, priority_fusion_options,\n+      mlir_context_);\n   TF_RETURN_IF_ERROR(priority_fusion.Run(hlo_module.get()).status());\n \n   // If the priority fusion pass above skipped some instructions, turn them\n   // into fusions.\n   FusionWrapper fusion_wrapper(gpu_device_info);\n   TF_RETURN_IF_ERROR(fusion_wrapper.Run(hlo_module.get()).status());\n \n-  NestGemmFusion nest_gemm_fusion(gpu_device_info.gpu_compute_capability());\n+  NestGemmFusion nest_gemm_fusion(gpu_device_info.gpu_compute_capability(),\n+                                  mlir_context_);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(hlo_module.get()).status());\n   return hlo_module;\n }"
        },
        {
            "sha": "21c961dfa0ac4bf449045603fce092c57245bf48",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -36,8 +37,10 @@ namespace gpu {\n class TritonBackend : public GpuCodegenBackend {\n  public:\n   explicit TritonBackend(stream_executor::StreamExecutor* stream_executor,\n-                         const DebugOptions* debug_options, Compiler* compiler)\n-      : GpuCodegenBackend(\"Triton\", stream_executor, debug_options, compiler) {}\n+                         const DebugOptions* debug_options, Compiler* compiler,\n+                         mlir::MLIRContext* mlir_context)\n+      : GpuCodegenBackend(\"Triton\", stream_executor, debug_options, compiler),\n+        mlir_context_(mlir_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;\n@@ -55,6 +58,7 @@ class TritonBackend : public GpuCodegenBackend {\n       const Compiler::CompileOptions& options) override;\n \n   bool IsSupported(const HloInstruction& instr);\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "f48c699a76555fa1cd1b9bcb561d3a871932fa9a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -74,7 +75,7 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n                      .value()\n                      ->ExecutorForDevice(0)\n                      .value(),\n-                 &debug_options_, &compiler_) {\n+                 &debug_options_, &compiler_, &mlir_context_) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n@@ -83,6 +84,7 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n   TritonBackend backend_;\n+  mlir::MLIRContext mlir_context_;\n };\n \n TEST_F(TritonBackendTest, GetSupportedConfigs) {"
        },
        {
            "sha": "187c2e3b07b68d90a0efcd2ca68a399aa8d7723d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -297,5 +297,6 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )"
        },
        {
            "sha": "f6004557a704437dcad986b2fabf42c39ec39e13",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -36,7 +36,7 @@ namespace gpu {\n // Generic loop fusion. Lowers to LLVM via MLIR.\n class LoopFusion final : public EmitterBase {\n  public:\n-  explicit LoopFusion(const HloFusionAnalysis& analysis)\n+  LoopFusion(const HloFusionAnalysis& analysis, mlir::MLIRContext* ctx)\n       : analysis_(analysis), config_(ComputeLoopFusionConfig(analysis)) {}\n   LaunchDimensions launch_dimensions() const override;\n "
        },
        {
            "sha": "bf6ce4b3ad05893e144beb4783ed28b2ff963c65",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/copy.h\"\n #include \"xla/backends/gpu/codegen/cudnn.h\"\n #include \"xla/backends/gpu/codegen/custom.h\"\n@@ -76,8 +77,8 @@ bool HloFusionInfo::CanEmitDynamicUpdateSliceInPlace() const {\n   return ret.ok() && *ret;\n }\n \n-std::unique_ptr<FusionInterface> GetFusionEmitter(\n-    const FusionInfo& fusion_info) {\n+std::unique_ptr<FusionInterface> GetFusionEmitter(const FusionInfo& fusion_info,\n+                                                  mlir::MLIRContext* ctx) {\n   const auto& analysis = fusion_info.analysis();\n   const FusionBackendConfig& backend_config = analysis.fusion_backend_config();\n \n@@ -108,7 +109,7 @@ std::unique_ptr<FusionInterface> GetFusionEmitter(\n           fusion_info.CanEmitDynamicUpdateSliceInPlace()) {\n         return std::make_unique<InPlaceDynamicUpdateSliceFusion>(analysis);\n       }\n-      return std::make_unique<LoopFusion>(analysis);\n+      return std::make_unique<LoopFusion>(analysis, ctx);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kReduction: {\n       return CreateReductionFusion(analysis);"
        },
        {
            "sha": "8c16653f2a4eb2759e88ebdb060f03f68b477296",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <optional>\n \n #include \"absl/status/statusor.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -94,8 +95,8 @@ class PreBufferAssignmentFusionInfo : public FusionInfo {\n };\n \n // Returns the emitter for the given fusion.\n-std::unique_ptr<FusionInterface> GetFusionEmitter(\n-    const FusionInfo& fusion_info);\n+std::unique_ptr<FusionInterface> GetFusionEmitter(const FusionInfo& fusion_info,\n+                                                  mlir::MLIRContext* ctx);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "4fb1389367a53e6c2c9f446b852ae83b2dd98919",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/test_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -41,7 +41,8 @@ absl::StatusOr<std::unique_ptr<EmitterData>> GetEmitter(\n   data->analysis.emplace(\n       HloFusionAnalysis::Create(*data->fusion, data->device.value()));\n   PreBufferAssignmentFusionInfo info(data->analysis.value());\n-  auto fusion_emitter = GetFusionEmitter(info);\n+  mlir::MLIRContext ctx = GetMlirContextForTest();\n+  auto fusion_emitter = GetFusionEmitter(info, &ctx);\n \n   auto emitter = dynamic_cast<EmitterBase*>(fusion_emitter.get());\n   TF_RET_CHECK(emitter != nullptr) << \"Expected emitter to be an EmitterBase\";"
        },
        {
            "sha": "f9dd89d443ad0e112a67aba23a9103a8ef02e4ef",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -596,6 +596,7 @@ xla_test(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:path\",\n     ],\n )"
        },
        {
            "sha": "f3047a458e381ecaf360025ab4a3195de6791e85",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -131,9 +131,9 @@ class TritonTest : public GpuCodegenTest {\n   GetModuleAndNestedFusionMetadata(absl::string_view hlo_text) {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<VerifiedHloModule> module,\n                         ParseAndReturnVerifiedModule(hlo_text));\n-    TF_ASSIGN_OR_RETURN(\n-        bool fusion_was_nested,\n-        NestGemmFusion(GpuComputeCapability()).Run(module.get()));\n+    TF_ASSIGN_OR_RETURN(bool fusion_was_nested,\n+                        NestGemmFusion(GpuComputeCapability(), &mlir_context_)\n+                            .Run(module.get()));\n     if (!fusion_was_nested) {\n       return absl::InternalError(\"Failed to nest the GEMM fusion.\");\n     }\n@@ -154,6 +154,8 @@ class TritonTest : public GpuCodegenTest {\n   const stream_executor::DeviceDescription& device_desc() {\n     return backend().default_stream_executor()->GetDeviceDescription();\n   }\n+\n+  mlir::MLIRContext mlir_context_;\n };\n \n class TritonGemmTest : public TritonTest {"
        },
        {
            "sha": "efe5baa39e6a0aac3788b513d5b59f0bb0acead8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_int4_device_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/strings/str_replace.h\"\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -110,7 +111,7 @@ class TritonTest : public GpuCodegenTest {\n     emitter_opts->Add(\n         DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n     absl::StatusOr<bool> nested_or =\n-        NestGemmFusion(device_desc().gpu_compute_capability())\n+        NestGemmFusion(device_desc().gpu_compute_capability(), &mlir_context_)\n             .Run(module.get());\n     if (!nested_or.ok()) {\n       return ::testing::AssertionFailure() << nested_or.status().message();\n@@ -151,6 +152,7 @@ class TritonTest : public GpuCodegenTest {\n   const stream_executor::DeviceDescription& device_desc() {\n     return backend().default_stream_executor()->GetDeviceDescription();\n   }\n+  mlir::MLIRContext mlir_context_;\n };\n \n // The following tests are for the channel and subchannel dequantization"
        },
        {
            "sha": "ecae17a4f6329fbf35684b5e18a518e622afa280",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -66,8 +66,9 @@ ENTRY entry_computation {\n   auto* root = module->entry_computation()->root_instruction();\n   HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n \n+  mlir::MLIRContext mlir_context;\n   std::unique_ptr<FusionInterface> emitter =\n-      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis});\n+      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis}, &mlir_context);\n   auto triton_fusion = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion, nullptr);\n   std::optional<TritonFusion::LaunchConfig> launch_config =\n@@ -103,14 +104,14 @@ ENTRY entry_computation {\n   auto* root = module->entry_computation()->root_instruction();\n   HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n \n+  mlir::MLIRContext mlir_context;\n   std::unique_ptr<FusionInterface> emitter =\n-      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis});\n+      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis}, &mlir_context);\n   auto triton_fusion_emitter = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion_emitter, nullptr);\n   EXPECT_EQ(triton_fusion_emitter->launch_config(), std::nullopt);\n \n   // Ensure that the emitter fails gracefully when the launch config is not set.\n-  mlir::MLIRContext mlir_context;\n   EXPECT_THAT(triton_fusion_emitter->GenerateTritonKernelAndWrapper(\n                   *::xla::Cast<HloFusionInstruction>(root), \"random_name\",\n                   device_info, /*llvm_module=*/nullptr, &mlir_context),"
        },
        {
            "sha": "e25d74fffdec18e947fac664f5247bbe356266e7",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -1382,6 +1382,7 @@ cc_library(\n         \"//xla/service/gpu/transforms:priority_fusion\",\n         \"//xla/service/gpu/transforms:variadic_op_splitter\",\n         \"//xla/stream_executor:device_description\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:env\",\n     ],\n )\n@@ -2001,6 +2002,7 @@ xla_test(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2265,6 +2267,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:protobuf\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n@@ -3010,6 +3013,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:logging\",\n         \"@local_tsl//tsl/platform:statusor\","
        },
        {
            "sha": "fa4b5039de765b2848b214001e28842feba17909",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -321,7 +321,8 @@ absl::Status AMDGPUCompiler::AddGemmFusionAutotuningPasses(\n     const se::SemanticVersion& toolkit_version,\n     se::StreamExecutor* stream_executor) {\n   pipeline->AddPass<GemmFusionAutotuner>(autotune_config, toolkit_version,\n-                                         thread_pool, key_value_store);\n+                                         thread_pool, key_value_store,\n+                                         mlir_context());\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "f1a18c38243b7428311de4f2059b665951dd169e",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -69,6 +69,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n     ],\n )\n@@ -106,6 +107,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n     ],\n )\n@@ -196,6 +198,7 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:protobuf\",\n         \"@local_tsl//tsl/profiler/lib:scoped_annotation\",\n@@ -255,7 +258,6 @@ xla_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -268,6 +270,7 @@ xla_test(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/time\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:platform_port\",\n     ],"
        },
        {
            "sha": "6b9203fdab9a3a397ddf15503d5c4b0d87cf87ab",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 16,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotune_results.pb.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n@@ -273,6 +274,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     const TritonGemmConfig& config,\n     const se::DeviceDescription& gpu_device_info,\n     const HloFusionInstruction* fusion, DebugOptions debug_opts,\n+    mlir::MLIRContext* mlir_context,\n     bool allow_filtering_kernels_spilling_registers) {\n   tsl::profiler::TraceMe traceme(\"TritonGemmAutotuneExtractor\");\n   std::unique_ptr<HloModule> new_module =\n@@ -305,7 +307,8 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     }\n \n     PriorityFusion priority_fusion(\n-        /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions());\n+        /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions(),\n+        mlir_context);\n     TF_RETURN_IF_ERROR(priority_fusion.Run(new_module.get()).status());\n \n     // If the priority fusion pass above skipped some instructions, turn them\n@@ -314,15 +317,17 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     TF_RETURN_IF_ERROR(fusion_wrapper.Run(new_module.get()).status());\n   }\n \n-  NestGemmFusion nest_gemm_fusion(gpu_device_info.gpu_compute_capability());\n+  NestGemmFusion nest_gemm_fusion(gpu_device_info.gpu_compute_capability(),\n+                                  mlir_context);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(new_module.get()).status());\n   return new_module;\n }\n \n absl::StatusOr<std::unique_ptr<HloModule>> CublasGemmAutotuneExtractor(\n     const AutotuneConfig& config, const se::DeviceDescription& gpu_device_info,\n     const se::SemanticVersion& toolkit_version,\n-    const HloFusionInstruction* fusion, const DebugOptions& debug_opts) {\n+    const HloFusionInstruction* fusion, const DebugOptions& debug_opts,\n+    mlir::MLIRContext* mlir_context) {\n   tsl::profiler::TraceMe traceme(\"CublasGemmAutotuneExtractor\");\n   const HloComputation* fusion_computation = fusion->called_computation();\n   std::unique_ptr<HloModule> new_module =\n@@ -347,7 +352,8 @@ absl::StatusOr<std::unique_ptr<HloModule>> CublasGemmAutotuneExtractor(\n                                toolkit_version, GemmRewriterOptions{dtype});\n     DotAlgorithmRewriter dot_algorithm_rewriter;\n     PriorityFusion fusion_pass(\n-        /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions());\n+        /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions(),\n+        mlir_context);\n     TF_RETURN_IF_ERROR(dot_algorithm_rewriter.Run(new_module.get()).status());\n     TF_RETURN_IF_ERROR(gemm_rewriter.Run(new_module.get()).status());\n     TF_RETURN_IF_ERROR(fusion_pass.Run(new_module.get()).status());\n@@ -370,7 +376,8 @@ absl::Status UpdateFusionInstructionKernelIndex(\n absl::StatusOr<std::unique_ptr<HloModule>> CustomFusionKernelAutotuneExtractor(\n     const GemmFusionAutotunerImpl::CustomKernelFusionConfig& cutlass_config,\n     const AutotuneConfig& config, const se::SemanticVersion& toolkit_version,\n-    const HloFusionInstruction* fusion, const DebugOptions& debug_opts) {\n+    const HloFusionInstruction* fusion, const DebugOptions& debug_opts,\n+    mlir::MLIRContext* mlir_context) {\n   tsl::profiler::TraceMe traceme(\"CustomFusionKernelAutotuneExtractor\");\n   const HloComputation* fusion_computation = fusion->called_computation();\n   std::unique_ptr<HloModule> new_module =\n@@ -380,7 +387,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CustomFusionKernelAutotuneExtractor(\n   CustomKernelFusionRewriter rewriter(&config.GetDeviceDescription());\n   PriorityFusion fusion_pass(\n       /*thread_pool=*/nullptr, config.GetDeviceDescription(),\n-      PriorityFusionOptions());\n+      PriorityFusionOptions(), mlir_context);\n   TF_RETURN_IF_ERROR(rewriter.Run(new_module.get()).status());\n   TF_RETURN_IF_ERROR(fusion_pass.Run(new_module.get()).status());\n \n@@ -479,7 +486,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetAutotunedModule(\n     const AutotuneConfig& autotune_config,\n     const se::SemanticVersion& toolkit_version, AutotunerCompileUtil& util,\n     const AutotuneResult result, const HloFusionInstruction* fusion,\n-    int fusion_id) {\n+    int fusion_id, mlir::MLIRContext* mlir_context) {\n   TritonGemmConfig triton_gemm_config;\n   if (result.has_triton()) {\n     TF_ASSIGN_OR_RETURN(triton_gemm_config,\n@@ -500,13 +507,13 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetAutotunedModule(\n         }\n         if (result.has_triton()) {\n           return TritonGemmAutotuneExtractor(\n-              triton_gemm_config, device_desc, fusion, debug_opts,\n+              triton_gemm_config, device_desc, fusion, debug_opts, mlir_context,\n               /*allow_filtering_kernels_spilling_registers=*/true);\n         }\n         if (result.has_gemm()) {\n           return CublasGemmAutotuneExtractor(autotune_config, device_desc,\n                                              toolkit_version, fusion,\n-                                             debug_opts);\n+                                             debug_opts, mlir_context);\n         }\n         LOG(FATAL) << \"Unknown result type: \" << result.DebugString();\n       }));\n@@ -1011,7 +1018,8 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n       auto executable_or = compile_util.Compile([&](const DebugOptions& opts) {\n         return TritonGemmAutotuneExtractor(\n             std::get<TritonGemmConfig>(config), config_.GetDeviceDescription(),\n-            fusion, opts, allow_filtering_kernels_spilling_registers);\n+            fusion, opts, mlir_context_,\n+            allow_filtering_kernels_spilling_registers);\n       });\n       return executable_or;\n     }\n@@ -1027,17 +1035,17 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n \n     if (std::holds_alternative<CuBlasConfig>(config)) {\n       return compile_util.Compile([&](const DebugOptions& opts) {\n-        return CublasGemmAutotuneExtractor(config_,\n-                                           config_.GetDeviceDescription(),\n-                                           toolkit_version_, fusion, opts);\n+        return CublasGemmAutotuneExtractor(\n+            config_, config_.GetDeviceDescription(), toolkit_version_, fusion,\n+            opts, mlir_context_);\n       });\n     }\n \n     if (std::holds_alternative<CustomKernelFusionConfig>(config)) {\n       return compile_util.Compile([&](const DebugOptions& opts) {\n         return CustomFusionKernelAutotuneExtractor(\n             std::get<CustomKernelFusionConfig>(config), config_,\n-            toolkit_version_, fusion, opts);\n+            toolkit_version_, fusion, opts, mlir_context_);\n       });\n     }\n \n@@ -1343,7 +1351,7 @@ absl::Status GemmFusionAutotunerImpl::Autotune(\n         !debug_options_.xla_gpu_dump_autotune_logs_to().empty()) {\n       TF_ASSIGN_OR_RETURN(\n           module, GetAutotunedModule(config_, toolkit_version_, compile_util,\n-                                     best, fusion, fusion_id));\n+                                     best, fusion, fusion_id, mlir_context_));\n     }\n \n     if (debug_options_.xla_gpu_dump_autotuned_gemm_fusions()) {\n@@ -1463,7 +1471,7 @@ absl::StatusOr<bool> GemmFusionAutotuner::Run(\n \n   const DebugOptions& debug_options = module->config().debug_options();\n   GemmFusionAutotunerImpl autotuner(config_, toolkit_version_, debug_options,\n-                                    thread_pool_);\n+                                    thread_pool_, mlir_context_);\n   GemmFusionCollector fusion_collector(&autotuner);\n   TF_ASSIGN_OR_RETURN(\n       GemmFusionCollectorResult fusions,"
        },
        {
            "sha": "bc937b6c8b7f9bb9d774721df23d6db8571c94e3",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -77,11 +78,13 @@ class GemmFusionAutotuner : public HloModulePass {\n   explicit GemmFusionAutotuner(const AutotuneConfig& config,\n                                const se::SemanticVersion& toolkit_version,\n                                tsl::thread::ThreadPool* thread_pool,\n-                               const MultiProcessKeyValueStore& key_value_store)\n+                               const MultiProcessKeyValueStore& key_value_store,\n+                               mlir::MLIRContext* mlir_context)\n       : config_(config),\n         toolkit_version_(toolkit_version),\n         thread_pool_(thread_pool),\n-        key_value_store_(key_value_store) {}\n+        key_value_store_(key_value_store),\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"gemm-fusion-autotuner\"; }\n \n@@ -95,18 +98,21 @@ class GemmFusionAutotuner : public HloModulePass {\n   se::SemanticVersion toolkit_version_;\n   tsl::thread::ThreadPool* thread_pool_;\n   MultiProcessKeyValueStore key_value_store_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n class GemmFusionAutotunerImpl {\n  public:\n   GemmFusionAutotunerImpl(\n       AutotuneConfig& config,\n       const stream_executor::SemanticVersion& toolkit_version,\n-      DebugOptions debug_options, tsl::thread::ThreadPool* thread_pool)\n+      DebugOptions debug_options, tsl::thread::ThreadPool* thread_pool,\n+      mlir::MLIRContext* mlir_context)\n       : config_(std::move(config)),\n         toolkit_version_(toolkit_version),\n         debug_options_(std::move(debug_options)),\n-        thread_pool_(thread_pool) {}\n+        thread_pool_(thread_pool),\n+        mlir_context_(mlir_context) {}\n \n   struct CuBlasConfig {\n     bool operator<(const CuBlasConfig& other) const;\n@@ -213,6 +219,7 @@ class GemmFusionAutotunerImpl {\n   DebugOptions debug_options_;\n   tsl::thread::ThreadPool* thread_pool_;\n   std::vector<TritonGemmConfig> triton_configs_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "7bfc757ae7e22e43430ee42309329e7c48ac1144",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 34,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -35,6 +35,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n #include \"absl/time/time.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotune_results.pb.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/error_spec.h\"\n@@ -191,7 +192,7 @@ class StatelessAutotunerTest : public HloTestBase {\n       const HloModule& module,\n       const se::GpuComputeCapability& compute_capability,\n       const se::SemanticVersion& toolkit_version,\n-      const DebugOptions& debug_options) {\n+      const DebugOptions& debug_options, mlir::MLIRContext* mlir_context) {\n     const HloFusionInstruction& fusion = *Cast<HloFusionInstruction>(\n         module.entry_computation()->root_instruction());\n     if (!isRocm()) {\n@@ -208,7 +209,7 @@ class StatelessAutotunerTest : public HloTestBase {\n     AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n         DeviceOrDevicelessConfig{test_config}, debug_options);\n     GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n-                                      debug_options, nullptr);\n+                                      debug_options, nullptr, mlir_context);\n     return autotuner.GenerateConfigs(fusion);\n   }\n \n@@ -245,7 +246,8 @@ class StatelessAutotunerTest : public HloTestBase {\n     AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n         DeviceOrDevicelessConfig{device_config}, GetDebugOptionsForTest());\n     GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n-                                      GetDebugOptionsForTest(), nullptr);\n+                                      GetDebugOptionsForTest(), nullptr,\n+                                      &mlir_context_);\n     const HloFusionInstruction& fusion = *Cast<HloFusionInstruction>(\n         module.entry_computation()->root_instruction());\n     return autotuner.GenerateConfigs(fusion);\n@@ -260,6 +262,9 @@ class StatelessAutotunerTest : public HloTestBase {\n               config);\n         });\n   }\n+\n+ protected:\n+  mlir::MLIRContext mlir_context_;\n };\n \n constexpr absl::string_view kHloDotFusionWithAlgorithm = R\"(\n@@ -377,7 +382,7 @@ class GemmFusionAutotunerTest : public StatelessAutotunerTest {\n                 DeviceConfig{backend().default_stream_executor(),\n                              backend().memory_allocator()}},\n             opts),\n-        GetToolkitVersion(), &thread_pool, key_value_store);\n+        GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n \n     RunAndFilecheckHloRewrite(\n         hlo, std::move(pipeline), expected, [](const HloModule* m) {\n@@ -411,7 +416,7 @@ absl::StatusOr<std::vector<TritonGemmConfig>>\n GetPossibleMatmulAutotuneTritonConfigs(\n     const D& dot, const se::CudaComputeCapability& compute_capability,\n     const se::SemanticVersion& toolkit_version,\n-    const DebugOptions& debug_options) {\n+    const DebugOptions& debug_options, mlir::MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(se::DeviceDescription device_description,\n                       se::DeviceDescription::FromProto(\n                           se::GpuDeviceInfoProto::default_instance()));\n@@ -428,7 +433,7 @@ GetPossibleMatmulAutotuneTritonConfigs(\n   AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n       DeviceOrDevicelessConfig{test_config}, debug_options);\n   GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n-                                    debug_options, nullptr);\n+                                    debug_options, nullptr, mlir_context);\n   return autotuner.GenerateTritonConfigs(dot);\n }\n \n@@ -451,7 +456,8 @@ ENTRY e {\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n-          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest()));\n+          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &mlir_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.num_stages > 2; }));\n@@ -473,7 +479,8 @@ ENTRY e {\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n-          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest()));\n+          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &mlir_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k >= 4; }));\n@@ -495,7 +502,8 @@ ENTRY e {\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n-          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest()));\n+          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &mlir_context_));\n   EXPECT_FALSE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k > 1; }));\n@@ -772,7 +780,8 @@ ENTRY main {\n                                       tsl::port::MaxParallelism());\n   MultiProcessKeyValueStore key_value_store;\n   pipeline.AddPass<GemmFusionAutotuner>(autotune_config, GetToolkitVersion(),\n-                                        &thread_pool, key_value_store);\n+                                        &thread_pool, key_value_store,\n+                                        &mlir_context_);\n   pipeline.AddPass<CallInliner>();\n   for (GemmRewriterOptions::DType dtype :\n        {GemmRewriterOptions::DType::kFp8Only,\n@@ -1001,7 +1010,7 @@ ENTRY e {\n           DeviceOrDevicelessConfig{DevicelessConfig{\n               backend().default_stream_executor()->GetDeviceDescription()}},\n           opts),\n-      GetToolkitVersion(), &thread_pool, key_value_store);\n+      GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(hlo));\n@@ -1049,7 +1058,8 @@ ENTRY e {\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n-          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest()));\n+          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &mlir_context_));\n \n   if (GetDebugOptionsForTest().xla_gpu_autotune_level() == 0) {\n     EXPECT_EQ(configs.size(), 1);\n@@ -1115,7 +1125,8 @@ ENTRY e {\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n-          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest()));\n+          compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &mlir_context_));\n   EXPECT_TRUE(std::all_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k == 1; }));\n@@ -1136,7 +1147,8 @@ TEST_F(GemmFusionAutotunerTest, SplitKFLoatNormalization) {\n   AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n       DeviceOrDevicelessConfig{test_config}, GetDebugOptionsForTest());\n   GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n-                                    GetDebugOptionsForTest(), nullptr);\n+                                    GetDebugOptionsForTest(), nullptr,\n+                                    &mlir_context_);\n   TF_ASSERT_OK_AND_ASSIGN(\n       AutotunerCompileUtil compile_util,\n       AutotunerCompileUtil::Create(autotune_config.DeviceConfig(),\n@@ -1201,9 +1213,9 @@ TEST_F(GemmFusionAutotunerTest, CreatesCustomKernelFusionConfigs) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n-      GetPossibleMatmulAutotuneConfigs(*module, compute_capability,\n-                                       GetToolkitVersion(),\n-                                       GetDebugOptionsForTest()));\n+      GetPossibleMatmulAutotuneConfigs(\n+          *module, compute_capability, GetToolkitVersion(),\n+          GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const GemmFusionAutotunerImpl::BackendConfig& config) {\n@@ -1244,9 +1256,9 @@ TEST_F(GemmFusionAutotunerTest, GeneratesTwoConfigsForUpcastGemmWithPrologue) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n-      GetPossibleMatmulAutotuneConfigs(*module, compute_capability,\n-                                       GetToolkitVersion(),\n-                                       GetDebugOptionsForTest()));\n+      GetPossibleMatmulAutotuneConfigs(\n+          *module, compute_capability, GetToolkitVersion(),\n+          GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1290,9 +1302,9 @@ TEST_F(GemmFusionAutotunerTest, GeneratesOneConfigForUpcastGemmWithPrologue) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n-      GetPossibleMatmulAutotuneConfigs(*module, compute_capability,\n-                                       GetToolkitVersion(),\n-                                       GetDebugOptionsForTest()));\n+      GetPossibleMatmulAutotuneConfigs(\n+          *module, compute_capability, GetToolkitVersion(),\n+          GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_EQ(\n       1, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1339,9 +1351,9 @@ TEST_F(GemmFusionAutotunerTest,\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n-      GetPossibleMatmulAutotuneConfigs(*module, compute_capability,\n-                                       GetToolkitVersion(),\n-                                       GetDebugOptionsForTest()));\n+      GetPossibleMatmulAutotuneConfigs(\n+          *module, compute_capability, GetToolkitVersion(),\n+          GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1447,10 +1459,10 @@ class GemmFusionShardedAutotunerTest : public GemmFusionAutotunerTest {\n   }\n \n   GemmFusionAutotuner GemmFusionAutotunerForKeyValueStore(\n-      MultiProcessKeyValueStore& multi_process_key_value_store) const {\n+      MultiProcessKeyValueStore& multi_process_key_value_store) {\n     return GemmFusionAutotuner(GetAutotuneConfigForTest(), GetToolkitVersion(),\n                                /*thread_pool=*/{},\n-                               multi_process_key_value_store);\n+                               multi_process_key_value_store, &mlir_context_);\n   }\n };\n \n@@ -1701,14 +1713,14 @@ TEST_F(GemmFusionAutotunerTest, VerifyHopperConfigsAreDifferentFromBlackwell) {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kBlackwell, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest()));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<TritonGemmConfig> hopper_configs,\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest()));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   std::set<TritonGemmConfig> blackwell_configs_set(blackwell_configs.begin(),\n                                                    blackwell_configs.end());\n@@ -1742,7 +1754,7 @@ TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsAreGenerated) {\n           *Cast<HloScaledDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kBlackwell, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest()));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n   std::set<TritonGemmConfig> blackwell_configs_set(blackwell_configs.begin(),\n                                                    blackwell_configs.end());\n   EXPECT_GT(blackwell_configs_set.size(), 0);\n@@ -1783,15 +1795,15 @@ TEST_F(GemmFusionAutotunerEnableTma,\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kAmpere, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest()));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<TritonGemmConfig> hopper_configs,\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest()));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   std::set<TritonGemmConfig> ampere_configs_set(ampere_configs.begin(),\n                                                 ampere_configs.end());\n@@ -1838,7 +1850,7 @@ TEST_F(GemmFusionAutotunerEnableTma,\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest()));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   auto is_disallowed_tma_config = [](const TritonGemmConfig& c) {\n     return c.num_stages > 2 && c.is_tma_allowed;"
        },
        {
            "sha": "e0b8d2b2ade589d090c05fe2f9c037c576cb6822",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <memory>\n #include <utility>\n \n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/pass/hlo_pass_fix.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n@@ -42,7 +43,8 @@ HloPassPipeline FusionPipeline(\n     const DebugOptions& debug_options,\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n     tsl::thread::ThreadPool* thread_pool,\n-    const se::DeviceDescription& gpu_device_info) {\n+    const se::DeviceDescription& gpu_device_info,\n+    mlir::MLIRContext* mlir_context) {\n   HloPassFix<HloPassPipeline> fusion(\"fusion\");\n   // We try to split variadic ops with many parameters into several such ops\n   // to avoid exceeding the parameter space.\n@@ -61,15 +63,17 @@ HloPassPipeline FusionPipeline(\n       /*min_latencies_seconds=*/{},\n       /*count_multiple_input_accesses=*/true};\n   fusion.AddPass<PriorityFusion>(thread_pool, gpu_device_info,\n-                                 std::move(cost_analysis_options));\n+                                 std::move(cost_analysis_options),\n+                                 mlir_context);\n \n   // Running CSE affects how many users an op has. This plays a role in what\n   // we detect as a tiled transpose fusion.\n   fusion.AddPass<HloCSE>(\n       /*is_layout_sensitive=*/true, /*ignore_control_dependencies=*/false,\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);\n   fusion.AddPass<HloDCE>();\n-  fusion.AddPass<MultiOutputFusion>(gpu_device_info, shape_size_bytes_function);\n+  fusion.AddPass<MultiOutputFusion>(gpu_device_info, shape_size_bytes_function,\n+                                    mlir_context);\n   fusion.AddPass<HloCSE>(\n       /*is_layout_sensitive=*/true, /*ignore_control_dependencies=*/false,\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);"
        },
        {
            "sha": "4a1daef5a4ba44383b823a9dfbf1f5050333980e",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_FUSION_PIPELINE_H_\n #define XLA_SERVICE_GPU_FUSION_PIPELINE_H_\n \n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -31,7 +32,8 @@ HloPassPipeline FusionPipeline(\n     const DebugOptions& debug_options,\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n     tsl::thread::ThreadPool* thread_pool,\n-    const se::DeviceDescription& gpu_device_info);\n+    const se::DeviceDescription& gpu_device_info,\n+    mlir::MLIRContext* mlir_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "9452052da8374d2f5344252e92fe4f909eb8dfab",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 20,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -468,8 +468,6 @@ GpuThunkAotCompilationResult::LoadExecutable(\n   std::string platform_name = platform->Name();\n   const se::DeviceDescription& gpu_device_info =\n       stream_exec->GetDeviceDescription();\n-  mlir::DialectRegistry registry;\n-  auto mlir_context = std::make_unique<mlir::MLIRContext>(registry);\n   llvm::LLVMContext llvm_context;\n   auto* gpu_compiler = dynamic_cast<GpuCompiler*>(compiler);\n   if (gpu_compiler == nullptr) {\n@@ -490,7 +488,8 @@ GpuThunkAotCompilationResult::LoadExecutable(\n \n   IrEmitterContext ir_emitter_context(\n       hlo_module.get(), buffer_assignment.get(), &execution_stream_assignment,\n-      platform_name, gpu_device_info, mlir_context.get(), llvm_module.get(),\n+      platform_name, gpu_device_info, gpu_compiler->mlir_context(),\n+      llvm_module.get(),\n       /*llvm_module_constants=*/nullptr,\n       /*emit_kernels=*/false);\n \n@@ -1155,7 +1154,8 @@ absl::Status RunLayoutAssignmentPasses(\n absl::Status RunFusionPasses(HloModule* hlo_module,\n                              const Compiler::TargetConfig& gpu_target_config,\n                              tsl::thread::ThreadPool* thread_pool,\n-                             HloCostAnalysis::ShapeSizeFunction shape_size_fn) {\n+                             HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n+                             mlir::MLIRContext* mlir_context) {\n   const se::DeviceDescription& gpu_device_info =\n       gpu_target_config.device_description;\n \n@@ -1165,7 +1165,7 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n \n   TF_RETURN_IF_ERROR(\n       FusionPipeline(hlo_module->config().debug_options(), shape_size_fn,\n-                     thread_pool, gpu_device_info)\n+                     thread_pool, gpu_device_info, mlir_context)\n           .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n           .status());\n \n@@ -1223,13 +1223,14 @@ void AddCollectiveCombinerPasses(\n     HloPassPipeline& pipeline, const HloModule& module,\n     const se::DeviceDescription& device_description,\n     const GpuAliasInfo* alias_info, int pointer_size,\n-    const GpuCompiler::CompileOptions& options) {\n+    const GpuCompiler::CompileOptions& options,\n+    mlir::MLIRContext* mlir_context) {\n   const DebugOptions& opts = module.config().debug_options();\n \n   if (EnableHeuristicCollectiveCombining(module.config(), device_description,\n                                          options.slice_size)) {\n-    pipeline.AddPass<CollectiveCombinerAnnotator>(device_description,\n-                                                  alias_info, pointer_size);\n+    pipeline.AddPass<CollectiveCombinerAnnotator>(\n+        device_description, alias_info, pointer_size, mlir_context);\n   }\n \n   pipeline.AddPass<GpuAllGatherCombiner>(\n@@ -1254,11 +1255,12 @@ void AddCollectiveCombinerPasses(\n absl::Status RunPostFusionPasses(\n     HloModule* hlo_module, const se::DeviceDescription& device_description,\n     const GpuAliasInfo* alias_info, int pointer_size,\n-    const GpuCompiler::CompileOptions& options) {\n+    const GpuCompiler::CompileOptions& options,\n+    mlir::MLIRContext* mlir_context) {\n   HloPassPipeline pipeline(\"post-fusion optimization\");\n   pipeline.AddPass<RenameFusions>();\n   AddCollectiveCombinerPasses(pipeline, *hlo_module, device_description,\n-                              alias_info, pointer_size, options);\n+                              alias_info, pointer_size, options, mlir_context);\n \n   pipeline.AddPass<AllReduceContiguous>();\n \n@@ -1309,7 +1311,8 @@ absl::Status RunPostFusionSimplificationPasses(\n absl::Status RunPostFusionVerificationPasses(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n     const GpuCompiler::CompileOptions& options,\n-    const Compiler::TargetConfig& gpu_target_config) {\n+    const Compiler::TargetConfig& gpu_target_config,\n+    mlir::MLIRContext* mlir_context) {\n   HloPassPipeline pipeline(\"post-fusion-verification-pipeline optimization\");\n \n   if (hlo_module->config()\n@@ -1318,7 +1321,8 @@ absl::Status RunPostFusionVerificationPasses(\n     DeviceOrDevicelessConfig device_config =\n         GetDeviceConfig(stream_exec, options, gpu_target_config);\n     if (!device_config.IsDeviceless()) {\n-      pipeline.AddPass<TritonFusionNumericsVerifier>(device_config);\n+      pipeline.AddPass<TritonFusionNumericsVerifier>(device_config,\n+                                                     mlir_context);\n     }\n   }\n \n@@ -1575,9 +1579,10 @@ absl::Status GpuCompiler::OptimizeHloModule(\n \n   TF_RETURN_IF_ERROR(RunFusionPasses(hlo_module, gpu_target_config,\n                                      thread_pool.get_mutable(),\n-                                     ShapeSizeBytesFunction()));\n+                                     ShapeSizeBytesFunction(), &mlir_context_));\n   TF_RETURN_IF_ERROR(RunPostFusionPasses(hlo_module, device_description,\n-                                         alias_info, pointer_size_, options));\n+                                         alias_info, pointer_size_, options,\n+                                         &mlir_context_));\n   TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(hlo_module));\n   TF_RETURN_IF_ERROR(RunPostFusionSimplificationPasses(\n       hlo_module,\n@@ -1588,7 +1593,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n       gpu_version, gpu_target_config));\n \n   TF_RETURN_IF_ERROR(RunPostFusionVerificationPasses(\n-      hlo_module, stream_exec, options, gpu_target_config));\n+      hlo_module, stream_exec, options, gpu_target_config, &mlir_context_));\n \n   TF_RETURN_IF_ERROR(\n       RunCollectiveScheduleLinearizerPasses(hlo_module, stream_exec));\n@@ -1788,6 +1793,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       pipeline.AddPass<HloDCE>();\n       pipeline.AddPass<SoftmaxRewriterTriton>(\n           gpu_target_config.device_description, ShapeSizeBytesFunction(),\n+          &mlir_context_,\n           /*only_fuse_if_profitable=*/true);\n     }\n \n@@ -1864,7 +1870,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n   // Match the location of this pass in `gemm_fusion_autotuner.cc` to make sure\n   // that there is no discrepancy.\n   pipeline.AddPass<NestGemmFusion>(\n-      gpu_target_config.device_description.gpu_compute_capability());\n+      gpu_target_config.device_description.gpu_compute_capability(),\n+      &mlir_context_);\n \n   // Clean up new_tuple described above.\n   pipeline.AddPass<TupleSimplifier>();\n@@ -2478,7 +2485,7 @@ GpuCompiler::CompileToBackendResult(\n                                             alias_info.get()));\n   TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n                       ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n-                                        alias_info.get()));\n+                                        &mlir_context_, alias_info.get()));\n   HloPassPipeline pipeline(\"scheduled-gpu-module\");\n   AddHloVerifier(&pipeline);\n   TF_RETURN_IF_ERROR(pipeline.Run(module).status());\n@@ -2802,14 +2809,15 @@ absl::Status GpuCompiler::RunPreSchedulingPasses(\n         /*min_latencies_seconds=*/{},\n         /*count_multiple_input_accesses=*/true};\n     // Cost model analysis for compute.\n-    pipeline.AddPass<GpuCostModelStatsCollection>(gpu_device_info,\n-                                                  cost_analysis_options);\n+    pipeline.AddPass<GpuCostModelStatsCollection>(\n+        gpu_device_info, cost_analysis_options, &mlir_context_);\n     // S-curve model analysis for collectives.\n     if (module->config()\n             .debug_options()\n             .xla_gpu_enable_analytical_sol_latency_estimator()) {\n       pipeline.AddPass<SolGpuCostModelStatsCollection>(\n-          gpu_device_info, ShapeSizeBytesFunction(), pointer_size_);\n+          gpu_device_info, ShapeSizeBytesFunction(), pointer_size_,\n+          &mlir_context_);\n     }\n \n     // Perf tables model analysis for collectives."
        },
        {
            "sha": "f527440f723dda69090e19f74a5411f8c53c4b57",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotune_results.pb.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -111,6 +112,8 @@ class GpuCompiler : public LLVMCompiler {\n       const Compiler::CompileOptions& options, const DebugOptions& debug_opts,\n       se::StreamExecutor* executor);\n \n+  mlir::MLIRContext* mlir_context() { return &mlir_context_; }\n+\n   virtual std::unique_ptr<GpuAliasInfo> GetAliasInfo(\n       const se::DeviceDescription& device_description) const {\n     return std::make_unique<GpuAliasInfo>(device_description);\n@@ -286,6 +289,10 @@ class GpuCompiler : public LLVMCompiler {\n   // THey need to be set globally whenever we call into LLVM.\n   virtual std::vector<std::string> GetLLVMCommandLineOptions(\n       const DebugOptions& debug_options) const = 0;\n+\n+  // A MLIR context that can be used by pre-codegen passes. For codegen, we will\n+  // need to have a context with more dialects registered.\n+  mlir::MLIRContext mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "551f2fa8f10a81586f7d6eb6e77bfd8a79ed7eba",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -120,9 +120,10 @@ class GpuCompilerTest : public HloTestBase {\n     GpuCompiler* gpu_compiler = tensorflow::down_cast<GpuCompiler*>(compiler);\n     std::unique_ptr<GpuAliasInfo> alias_info =\n         gpu_compiler->GetAliasInfo(gpu_device_info);\n-    TF_RETURN_IF_ERROR(\n-        ScheduleGpuModule(module, 4, gpu_device_info, alias_info.get())\n-            .status());\n+    TF_RETURN_IF_ERROR(ScheduleGpuModule(module, 4, gpu_device_info,\n+                                         gpu_compiler->mlir_context(),\n+                                         alias_info.get())\n+                           .status());\n     return gpu_compiler->RunPostSchedulingPipelines(\n         module, 4 * 1024 * 1024, gpu_device_info, alias_info.get());\n   }"
        },
        {
            "sha": "32331fa9f0bfeb0c1aab610e8505973b7a38b4e4",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n@@ -463,7 +464,7 @@ std::string TagWithFingerprint(HloModule* module) {\n std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     const HloModule& module, int pointer_size,\n     const se::DeviceDescription& gpu_device_info, absl::string_view fingerprint,\n-    const SchedulerConfig& config) {\n+    const SchedulerConfig& config, mlir::MLIRContext* mlir_context) {\n   const DebugOptions& options = module.config().debug_options();\n \n   auto gpu_latency_estimator =\n@@ -487,7 +488,8 @@ std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     VLOG(1) << \"Using analytical latency estimator\";\n     return std::make_unique<AnalyticalLatencyEstimator>(\n         config, std::move(gpu_latency_estimator), gpu_device_info,\n-        ShapeSizeBytesFunction(pointer_size), module.entry_computation());\n+        ShapeSizeBytesFunction(pointer_size), module.entry_computation(),\n+        mlir_context);\n   }\n \n   if (SolLatencyEstimator::IsSupportedForModule(module, gpu_device_info)) {\n@@ -510,7 +512,7 @@ std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     auto sol_latency_estimator = SolLatencyEstimator::Create(\n         config, std::move(gpu_latency_estimator), gpu_device_info,\n         ShapeSizeBytesFunction(pointer_size), module.entry_computation(),\n-        std::move(cost_analysis));\n+        mlir_context, std::move(cost_analysis));\n     if (sol_latency_estimator.ok()) {\n       return std::move(*sol_latency_estimator);\n     }\n@@ -564,7 +566,7 @@ LegalizeSchedulingAnnotations::Config SchedulingAnnotationsConfig() {\n absl::Status RunLatencyHidingSchedulerPasses(\n     HloModule* module, int pointer_size, absl::string_view fingerprint,\n     uint64_t memory_limit, const se::DeviceDescription& gpu_device_info,\n-    const GpuAliasInfo* alias_info) {\n+    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"RunLatencyHidingSchedulerPasses\");\n   HloPassPipeline pipeline(\"latency-hiding-scheduler\");\n   const DebugOptions& options = module->config().debug_options();\n@@ -577,8 +579,9 @@ absl::Status RunLatencyHidingSchedulerPasses(\n \n   auto shape_size_in_bytes = ShapeSizeBytesFunction(pointer_size);\n \n-  std::unique_ptr<LatencyEstimator> estimator = GetLatencyEstimator(\n-      *module, pointer_size, gpu_device_info, fingerprint, config);\n+  std::unique_ptr<LatencyEstimator> estimator =\n+      GetLatencyEstimator(*module, pointer_size, gpu_device_info, fingerprint,\n+                          config, mlir_context);\n \n   if (NeedAccuracyChecker(options, *estimator)) {\n     pipeline.AddPass<PGLEAccuracyChecker>(\n@@ -721,7 +724,7 @@ absl::Status RunAsyncCollectivesConversionPasses(HloModule* module) {\n absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n     HloModule* module, int64_t pointer_size,\n     const se::DeviceDescription& gpu_device_info,\n-    const GpuAliasInfo* alias_info) {\n+    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"ScheduleGpuModule\");\n \n   // Tag the module with its 128 bit fingerprint. The fingerprint should include\n@@ -755,7 +758,7 @@ absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n   if (enable_latency_hiding_scheduler) {\n     TF_RETURN_IF_ERROR(RunLatencyHidingSchedulerPasses(\n         module, pointer_size, fingerprint, memory_limit, gpu_device_info,\n-        alias_info));\n+        mlir_context, alias_info));\n   }\n \n   return ScheduleMetadata{memory_limit, peak_memory_bytes};"
        },
        {
            "sha": "1cf840623181319c141bb1ec72a7e4ba19fd7a4f",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/gpu/alias_info.h\"\n@@ -56,7 +57,7 @@ uint64_t GetSchedulerMemoryLimit(const HloModule& module,\n absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n     HloModule* module, int64_t pointer_size,\n     const se::DeviceDescription& gpu_device_info,\n-    const GpuAliasInfo* alias_info);\n+    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info);\n \n HloInstructionSequence PostProcessSchedule(const HloInstructionSequence& input);\n "
        },
        {
            "sha": "b30f0c21530914b547908bf996f3f978f6ace31a",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -81,6 +81,7 @@ class GpuHloScheduleTest : public HloTestBase {\n         gpu_compiler->GetAliasInfo(gpu_device_info);\n     int64_t pointer_size = gpu_compiler->GetPointerSize();\n     return xla::gpu::ScheduleGpuModule(module, pointer_size, gpu_device_info,\n+                                       gpu_compiler->mlir_context(),\n                                        alias_info.get());\n   }\n "
        },
        {
            "sha": "9b0e370a0ff0e51853401fd7d1ffdbccc776a118",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -77,7 +78,8 @@ class GpuLatencyHidingSchedulerBaseTest\n     options.set_xla_gpu_pgle_accuracy_checker(strictness);\n \n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, /*pointer_size=*/8,\n-                                         gpu_device_info, &alias_info)\n+                                         gpu_device_info, &mlir_context_,\n+                                         &alias_info)\n                            .status());\n     return module;\n   }\n@@ -95,6 +97,8 @@ class GpuLatencyHidingSchedulerBaseTest\n     config.set_fdo_profile(fdo_profile);\n     return config;\n   }\n+\n+  mlir::MLIRContext mlir_context_;\n };\n \n TEST_F(GpuLatencyHidingSchedulerBaseTest,"
        },
        {
            "sha": "ef120a30be92476ecc180bc2fb42a05eafc6eb75",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -1655,7 +1655,8 @@ absl::Status IrEmitterUnnested::EmitFusion(const HloFusionInstruction* instr) {\n           /*analysis=*/fusion_analysis, instr,\n           /*buffer_assignment=*/\n           &ir_emitter_context_->buffer_assignment(),\n-          /*call_graph=*/*call_graph_));\n+          /*call_graph=*/*call_graph_),\n+      ir_emitter_context_->mlir_context());\n   TF_ASSIGN_OR_RETURN(auto result, emitter->Emit(*ir_emitter_context_, *instr));\n \n   const ExecutionStreamAssignment& stream_assignment ="
        },
        {
            "sha": "e25356794d082ac1c71791e233499253ced8e271",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -31,7 +31,6 @@ cc_library(\n         \":gpu_collective_performance_model\",\n         \":gpu_hlo_cost_analysis\",\n         \":gpu_performance_model\",\n-        \":gpu_performance_model_base\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_query\",\n@@ -41,6 +40,7 @@ cc_library(\n         \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/time\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -77,6 +77,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/time\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -106,6 +107,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/time\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -223,7 +225,6 @@ cc_library(\n     deps = [\n         \":gpu_hlo_cost_analysis\",\n         \":gpu_performance_model\",\n-        \":gpu_performance_model_base\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:hlo_cost_analysis\",\n@@ -232,15 +233,14 @@ cc_library(\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n-        \"@local_tsl//tsl/platform:status\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n xla_cc_test(\n     name = \"gpu_cost_model_stats_collection_test\",\n     srcs = [\"gpu_cost_model_stats_collection_test.cc\"],\n     deps = [\n-        \":fusion_analysis_cache\",\n         \":gpu_cost_model_stats_collection\",\n         \":gpu_hlo_cost_analysis\",\n         \"//xla/hlo/ir:hlo\",\n@@ -250,6 +250,7 @@ xla_cc_test(\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n@@ -329,6 +330,7 @@ cc_library(\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -349,6 +351,7 @@ xla_cc_test(\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n@@ -375,6 +378,7 @@ cc_library(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -1126,6 +1130,7 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -1144,6 +1149,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n "
        },
        {
            "sha": "000e97ac1cd87f01bfa164ade178605b1d5ed1fa",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"absl/log/log.h\"\n #include \"absl/time/time.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n@@ -76,10 +77,10 @@ AnalyticalLatencyEstimator::AnalyticalLatencyEstimator(\n     std::unique_ptr<LatencyEstimator> latency_estimator,\n     const se::DeviceDescription& gpu_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-    HloComputation* computation)\n+    HloComputation* computation, mlir::MLIRContext* mlir_context)\n     : config_(config),\n       gpu_info_(gpu_info),\n-      gpu_performance_model_(gpu_info),\n+      gpu_performance_model_(gpu_info, mlir_context),\n       latency_estimator_(std::move(latency_estimator)),\n       shape_size_function_(shape_size_function) {\n   cost_analysis_.emplace("
        },
        {
            "sha": "2b1c51737dea1f325dba8d28898bd8aa488530d3",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <memory>\n #include <optional>\n \n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n@@ -39,7 +40,7 @@ class AnalyticalLatencyEstimator : public LatencyEstimator {\n       std::unique_ptr<LatencyEstimator> latency_estimator,\n       const se::DeviceDescription& gpu_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      HloComputation* computation);\n+      HloComputation* computation, mlir::MLIRContext* mlir_context);\n \n   TimeCost GetLatencyBetween(const HloGraphNode& from,\n                              const HloGraphNode& target) const override;"
        },
        {
            "sha": "0decac886408e6c92990fddd4f9d1834959a5ae3",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -236,8 +236,8 @@ std::optional<GroupedByOpIndexingMap> GetThreadIdToInputMemoryLayoutsMaps(\n     const HloFusionAnalysis& fusion_analysis,\n     absl::Span<const HloInstruction* const> operands,\n     MLIRContext* mlir_context) {\n-  auto emitter =\n-      GetFusionEmitter(PreBufferAssignmentFusionInfo{fusion_analysis});\n+  auto emitter = GetFusionEmitter(\n+      PreBufferAssignmentFusionInfo{fusion_analysis}, mlir_context);\n   const auto* fusion_interface =\n       dynamic_cast<const KernelFusionInterface*>(emitter.get());\n "
        },
        {
            "sha": "e371705ebddfbe0f7691e59093b59ae891f666fc",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -60,7 +60,8 @@ class CoalescingTest : public HloHardwareIndependentTestBase {\n   std::vector<bool> IsReadCoalescedPerOperand(const HloInstruction* root) {\n     auto fusion_adaptor = HloFusionAdaptor::ForInstruction(root);\n     auto analysis = HloFusionAnalysis::Create(*root, device_info_);\n-    auto emitter = GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis});\n+    auto emitter = GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis},\n+                                    &mlir_context_);\n     auto fusion = dynamic_cast<KernelFusionInterface*>(emitter.get());\n     EXPECT_NE(fusion, nullptr);\n "
        },
        {
            "sha": "3aadd7d6557c536563e29bad96a596bda8f486a3",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -32,7 +32,7 @@ absl::StatusOr<bool> GpuCostModelStatsCollection::Run(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   // Scan all computations for fusion instructions.\n \n-  GpuPerformanceModelOwning gpu_performance_model{device_info_};\n+  GpuPerformanceModelOwning gpu_performance_model{device_info_, mlir_context_};\n   for (auto* computation : module->MakeComputationPostOrder()) {\n     TF_CHECK_OK(computation->Accept(&cost_analysis_));\n "
        },
        {
            "sha": "2850657f0874ecbd21f8a3feb6c06333760b5c05",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.h",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n@@ -35,8 +36,11 @@ class GpuCostModelStatsCollection : public HloModulePass {\n  public:\n   explicit GpuCostModelStatsCollection(\n       const se::DeviceDescription& d,\n-      const GpuHloCostAnalysis::Options& cost_analysis_options)\n-      : device_info_(d), cost_analysis_(cost_analysis_options, device_info_) {}\n+      const GpuHloCostAnalysis::Options& cost_analysis_options,\n+      mlir::MLIRContext* mlir_context)\n+      : device_info_(d),\n+        cost_analysis_(cost_analysis_options, device_info_),\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override {\n     return \"gpu_cost_model_stats_collection\";\n@@ -50,6 +54,7 @@ class GpuCostModelStatsCollection : public HloModulePass {\n  private:\n   se::DeviceDescription device_info_;\n   GpuHloCostAnalysis cost_analysis_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "d3faefb50b829614fd7a427933054c2b6c56411a",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <memory>\n \n #include <gtest/gtest.h>\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n@@ -35,7 +36,11 @@ class GpuCostModelStatsCollectionTest : public HloHardwareIndependentTestBase {\n  public:\n   GpuCostModelStatsCollection cost_model_stats_{\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-      GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true}};\n+      GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true},\n+      &mlir_context_};\n+\n+ protected:\n+  mlir::MLIRContext mlir_context_;\n };\n \n TEST_F(GpuCostModelStatsCollectionTest, FusinInEntryComputation) {"
        },
        {
            "sha": "545abd3df851d21b8ea264a0eab76c508edd8d58",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -311,7 +311,7 @@ GpuPerformanceModelWithIndexingAnalysis::EstimateRunTimeForFusion(\n   auto root_shape = roots.front().shape();\n \n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis);\n+      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n \n   int64_t num_blocks = launch_dimensions.num_blocks();\n "
        },
        {
            "sha": "df0cf6b00f84af416604b6df7693357d86f5f2e2",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -46,10 +47,12 @@ namespace gpu {\n GpuPerformanceModel::GpuPerformanceModel(\n     const se::DeviceDescription& device_info,\n     HloFusionAnalysisCache& fusion_analysis_cache,\n-    GpuPerformanceModelCache& gpu_performance_model_cache)\n+    GpuPerformanceModelCache& gpu_performance_model_cache,\n+    mlir::MLIRContext* mlir_context)\n     : device_info_(device_info),\n       fusion_analysis_cache_(fusion_analysis_cache),\n-      gpu_performance_model_cache_(gpu_performance_model_cache) {};\n+      gpu_performance_model_cache_(gpu_performance_model_cache),\n+      mlir_context_(mlir_context) {};\n \n EstimateRunTimeData GpuPerformanceModel::EstimateRunTimeForInstructionImpl(\n     const HloInstruction* instr, const GpuHloCostAnalysis* cost_analysis) {\n@@ -60,7 +63,7 @@ EstimateRunTimeData GpuPerformanceModel::EstimateRunTimeForInstructionImpl(\n \n   const auto& fusion_analysis = fusion_analysis_cache_.Get(*instr);\n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis);\n+      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n   int64_t num_blocks = launch_dimensions.num_blocks();\n \n   absl::Duration compute_time =\n@@ -142,7 +145,7 @@ absl::Duration GpuPerformanceModel::EstimateRunTimeForFusionImpl(\n       fusion_analysis_cache_.Get(*producer, *consumer);\n \n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis);\n+      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n \n   int64_t flops = producer_runtime.flops * utilization_by_this_consumer +\n                   consumer_runtime.flops;"
        },
        {
            "sha": "c150db6dde043f57d345a6563140503478492ce8",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model.h",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n \n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n@@ -34,7 +35,8 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n   // Lifetime to all references to this constructor must live at least as long\n   GpuPerformanceModel(const se::DeviceDescription& device_info,\n                       HloFusionAnalysisCache& fusion_analysis_cache,\n-                      GpuPerformanceModelCache& gpu_performance_model_cache);\n+                      GpuPerformanceModelCache& gpu_performance_model_cache,\n+                      mlir::MLIRContext* mlir_context);\n \n   EstimateRunTimeData EstimateRunTimeForInstruction(\n       const HloInstruction* instr, const GpuHloCostAnalysis* cost_analysis);\n@@ -76,6 +78,7 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n   // this is not possible because the cache is used directly by\n   // xla::gpu::PriorityFusionQueue\n   GpuPerformanceModelCache& gpu_performance_model_cache_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n // An owning wrapper around GpuPerformanceModel that also owns the caches.\n@@ -85,11 +88,12 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n // owning model should be used.\n class GpuPerformanceModelOwning {\n  public:\n-  explicit GpuPerformanceModelOwning(const se::DeviceDescription& device_info)\n+  GpuPerformanceModelOwning(const se::DeviceDescription& device_info,\n+                            mlir::MLIRContext* mlir_context)\n       : fusion_analysis_cache_(device_info),\n         gpu_performance_model_(std::make_unique<GpuPerformanceModel>(\n-            device_info, fusion_analysis_cache_,\n-            gpu_performance_model_cache_)) {};\n+            device_info, fusion_analysis_cache_, gpu_performance_model_cache_,\n+            mlir_context)) {};\n \n   GpuPerformanceModel& Get() const { return *gpu_performance_model_; }\n "
        },
        {
            "sha": "b815ca9fab321899384d1aa98cb26e7523228d45",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/time.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion.h\"\n@@ -142,9 +143,9 @@ void GpuPerformanceModelCache::Invalidate(const HloInstruction& instruction) {\n \n /*static*/\n LaunchDimensions GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n-    const HloFusionAnalysis& fusion_analysis) {\n+    const HloFusionAnalysis& fusion_analysis, mlir::MLIRContext* ctx) {\n   auto emitter =\n-      GetFusionEmitter(PreBufferAssignmentFusionInfo{fusion_analysis});\n+      GetFusionEmitter(PreBufferAssignmentFusionInfo{fusion_analysis}, ctx);\n   if (const auto* kernel_emitter =\n           dynamic_cast<const KernelFusionInterface*>(emitter.get())) {\n     return kernel_emitter->launch_dimensions();"
        },
        {
            "sha": "a9d04ed197710e644326e85c663fd240df9c077b",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/time/time.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n@@ -154,7 +155,7 @@ class GpuPerformanceModelBase {\n   // Uses HloFusionAnalysis for computing the actual number of threads and\n   // blocks that the IR emitter will use.\n   static LaunchDimensions EstimateFusionLaunchDimensions(\n-      const HloFusionAnalysis& fusion_analysis);\n+      const HloFusionAnalysis& fusion_analysis, mlir::MLIRContext* ctx);\n \n   // Returns bytes accessed of operand output by instruction. Returns 0, if the\n   // operand is not used by the instruction."
        },
        {
            "sha": "f829fdfcc64f47d9682603151647125b5598ce63",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n \n #include <gtest/gtest.h>\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n@@ -42,6 +43,7 @@ class GpuPerformanceModelBaseTest : public HloHardwareIndependentTestBase {\n   // on A6000 by profiling the execution of the HLOs.\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   std::unique_ptr<GpuHloCostAnalysis> analysis_;\n+  mlir::MLIRContext mlir_context_;\n \n   GpuPerformanceModelBaseTest() {\n     options_.count_multiple_input_accesses = true;\n@@ -238,7 +240,8 @@ ENTRY entry_computation {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n+                                                              &mlir_context_);\n \n   EXPECT_EQ(launch_dimensions.num_blocks(), 128);\n   EXPECT_EQ(launch_dimensions.num_threads_per_block(), 128);\n@@ -274,7 +277,8 @@ ENTRY e {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n+                                                              &mlir_context_);\n \n   EXPECT_EQ(launch_dimensions.num_blocks(), 16);\n   EXPECT_EQ(launch_dimensions.num_threads_per_block(), 64);\n@@ -303,7 +307,8 @@ ENTRY e {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n+                                                              &mlir_context_);\n \n   // CuNnnFusion doesn't implement KernelLaunchInsterface, so\n   // EstimateFusionLaunchDimensions returns a default estimate."
        },
        {
            "sha": "b90889759aae90c3b6a7db2c43ab8860f55ff4d0",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -71,7 +71,8 @@ class GpuPerformanceModelTest : public HloHardwareIndependentTestBase {\n   GpuHloCostAnalysis analysis_{options_, device_info_};\n   GpuPerformanceModelCache gpu_performance_model_cache_;\n   GpuPerformanceModel gpu_performance_model_{\n-      device_info_, fusion_analysis_cache_, gpu_performance_model_cache_};\n+      device_info_, fusion_analysis_cache_, gpu_performance_model_cache_,\n+      &mlir_context_};\n \n   GpuPerformanceModelWithIndexingAnalysis indexing_cost_model_{\n       &device_info_, &fusion_analysis_cache_, HloCostAnalysis::DefaultShapeSize,"
        },
        {
            "sha": "21b4e6ad448948cac69812c22ab0febaf99ccce9",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -104,7 +104,7 @@ absl::StatusOr<bool> SolGpuCostModelStatsCollection::Run(\n       SolLatencyEstimator::Create(\n           scheduler_config,\n           std::make_unique<GpuLatencyEstimator>(pointer_size_), device_info_,\n-          shape_size_in_bytes_fn_, module->entry_computation(),\n+          shape_size_in_bytes_fn_, module->entry_computation(), mlir_context_,\n           std::move(cost_analysis)));\n \n   for (HloComputation* comp : module->MakeComputationPostOrder()) {"
        },
        {
            "sha": "829406f2bc7fd222e5ec388b12d0906168195152",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection.h",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/hlo_verifier.h\"\n@@ -30,10 +31,12 @@ class SolGpuCostModelStatsCollection : public HloModulePass {\n  public:\n   explicit SolGpuCostModelStatsCollection(\n       const se::DeviceDescription& device_description,\n-      ShapeSizeFn shape_size_in_bytes_fn, int pointer_size)\n+      ShapeSizeFn shape_size_in_bytes_fn, int pointer_size,\n+      mlir::MLIRContext* mlir_context)\n       : device_info_(device_description),\n         shape_size_in_bytes_fn_(shape_size_in_bytes_fn),\n-        pointer_size_(pointer_size) {}\n+        pointer_size_(pointer_size),\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override {\n     return \"sol-gpu-cost-model-stats-collection\";\n@@ -49,6 +52,7 @@ class SolGpuCostModelStatsCollection : public HloModulePass {\n   se::DeviceDescription device_info_;\n   ShapeSizeFn shape_size_in_bytes_fn_;\n   int pointer_size_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "7870330d0c1c4b41e603bee3776a36894582148b",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/log/log.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n@@ -62,6 +63,7 @@ class SolGpuCostModelStatsCollectionTest\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(se::CudaComputeCapability(9, 0));\n   ShapeSizeFn shape_size_fn_;\n   int pointer_size_ = 8;\n+  mlir::MLIRContext mlir_context_;\n };\n \n TEST_F(SolGpuCostModelStatsCollectionTest,\n@@ -85,10 +87,10 @@ TEST_F(SolGpuCostModelStatsCollectionTest,\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      bool changed, SolGpuCostModelStatsCollection(device_info_, shape_size_fn_,\n-                                                   pointer_size_)\n-                        .Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, SolGpuCostModelStatsCollection(\n+                                            device_info_, shape_size_fn_,\n+                                            pointer_size_, &mlir_context_)\n+                                            .Run(module.get()));\n \n   VLOG(1) << module->ToString();\n "
        },
        {
            "sha": "a4e708595f17b6a2a03e209d35bc1eacc50f9900",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 15,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/time/time.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -105,15 +106,16 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n     int num_participating_hosts, int num_communicators,\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     const SolGPUCostModel::Config& sol_flags,\n-    const GpuHloCostAnalysis& analysis) {\n+    const GpuHloCostAnalysis& analysis, mlir::MLIRContext* mlir_context) {\n   SolGPUCostModel sol_model(sol_flags);\n   const int64_t msg_size = analysis.BytesTransferred(instr);\n \n   // TODO(b/385111575): We should call just `.exec_time` but we need to better\n   // (more granularly) model bytes accessed (input + output) for collectives.\n   absl::Duration result = absl::Seconds(1.0f * analysis.bytes_accessed(instr) /\n                                         gpu_device_info.memory_bandwidth());\n-  GpuPerformanceModelOwning gpu_performance_model{gpu_device_info};\n+  GpuPerformanceModelOwning gpu_performance_model{gpu_device_info,\n+                                                  mlir_context};\n   switch (instr.opcode()) {\n     case HloOpcode::kAllGather:\n     case HloOpcode::kAllGatherStart: {\n@@ -192,7 +194,8 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n     const se::DeviceDescription& gpu_device_info,\n     const SolGPUCostModel::Config& sol_flags,\n     const GpuHloCostAnalysis& analysis,\n-    const CollectiveInterpolator* collective_interpolator) {\n+    const CollectiveInterpolator* collective_interpolator,\n+    mlir::MLIRContext* mlir_context) {\n   TF_RETURN_IF_ERROR(communication_type.status());\n \n   GPUCommunicationType comm = *communication_type;\n@@ -204,13 +207,13 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second / sol_flags.gpus_per_node,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n-          gpu_device_info, sol_flags, analysis);\n+          gpu_device_info, sol_flags, analysis, mlir_context);\n     }\n     case GPUCommunicationType::NON_RAIL_ALIGNED: {\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n-          gpu_device_info, sol_flags, analysis);\n+          gpu_device_info, sol_flags, analysis, mlir_context);\n     }\n     case GPUCommunicationType::SINGLE_HOST: {\n       if (collective_interpolator == nullptr) {\n@@ -262,7 +265,7 @@ absl::StatusOr<std::unique_ptr<MatmulInterpolator>> CreateMatmulInterpolator(\n SolLatencyEstimator::ComputeCollectiveTime(\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-    const SolGPUCostModel::Config& sol_flags,\n+    const SolGPUCostModel::Config& sol_flags, mlir::MLIRContext* mlir_context,\n     const CollectiveInterpolator* collective_interpolator) {\n   GpuHloCostAnalysis analysis(\n       GpuHloCostAnalysis::Options{shape_size_fn,\n@@ -278,7 +281,7 @@ SolLatencyEstimator::ComputeCollectiveTime(\n   }\n \n   return SolLatencyEstimator::ComputeCollectiveTime(\n-      instr, gpu_device_info, shape_size_fn, sol_flags, analysis,\n+      instr, gpu_device_info, shape_size_fn, sol_flags, analysis, mlir_context,\n       collective_interpolator);\n }\n \n@@ -287,7 +290,7 @@ SolLatencyEstimator::ComputeCollectiveTime(\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n     const SolGPUCostModel::Config& sol_flags,\n-    const GpuHloCostAnalysis& analysis,\n+    const GpuHloCostAnalysis& analysis, mlir::MLIRContext* mlir_context,\n     const CollectiveInterpolator* collective_interpolator) {\n   if (HloDataflowAnalysis::IsAsynchronousOperationDone(instr.opcode())) {\n     VLOG(8) << \"Returning 0 cost for async done op \" << instr.name();\n@@ -310,7 +313,8 @@ SolLatencyEstimator::ComputeCollectiveTime(\n   TF_ASSIGN_OR_RETURN(\n       absl::Duration result,\n       DispatchEstimation(communication_type, *collective_instr, gpu_device_info,\n-                         sol_flags, analysis, collective_interpolator));\n+                         sol_flags, analysis, collective_interpolator,\n+                         mlir_context));\n   return result;\n }\n \n@@ -320,7 +324,7 @@ SolLatencyEstimator::Create(\n     std::unique_ptr<LatencyEstimator> latency_estimator,\n     const se::DeviceDescription& gpu_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-    const HloComputation* computation,\n+    const HloComputation* computation, mlir::MLIRContext* mlir_context,\n     std::unique_ptr<GpuHloCostAnalysis> cost_analysis) {\n   if (cost_analysis == nullptr) {\n     cost_analysis =\n@@ -345,7 +349,7 @@ SolLatencyEstimator::Create(\n   return std::unique_ptr<SolLatencyEstimator>(new SolLatencyEstimator(\n       config, std::move(latency_estimator), gpu_info, std::move(cost_analysis),\n       shape_size_function, sol_config, std::move(collective_interpolator),\n-      std::move(matmul_interpolator)));\n+      std::move(matmul_interpolator), mlir_context));\n }\n \n /*static*/ bool SolLatencyEstimator::IsSupportedForModule(\n@@ -384,7 +388,7 @@ LatencyEstimator::TimeCost SolLatencyEstimator::GetLatencyBetween(\n \n   absl::StatusOr<absl::Duration> coll_time = ComputeCollectiveTime(\n       from.GetInstr(), gpu_info_, shape_size_function_, sol_flags_,\n-      *cost_analysis_, collective_interpolator_.get());\n+      *cost_analysis_, mlir_context_, collective_interpolator_.get());\n   if (!coll_time.ok()) {\n     VLOG(1) << \"Failed to compute collective time: \" << coll_time.status()\n             << \" for \" << from.GetInstr().name();\n@@ -432,16 +436,18 @@ SolLatencyEstimator::SolLatencyEstimator(\n     const HloCostAnalysis::ShapeSizeFunction shape_size_function,\n     const SolGPUCostModel::Config sol_flags,\n     std::unique_ptr<CollectiveInterpolator> collective_interpolator,\n-    std::unique_ptr<MatmulInterpolator> matmul_interpolator)\n+    std::unique_ptr<MatmulInterpolator> matmul_interpolator,\n+    mlir::MLIRContext* mlir_context)\n     : config_(config),\n       gpu_info_(gpu_info),\n-      gpu_performance_model_(gpu_info),\n+      gpu_performance_model_(gpu_info, mlir_context),\n       cost_analysis_(std::move(cost_analysis)),\n       latency_estimator_(std::move(latency_estimator)),\n       shape_size_function_(shape_size_function),\n       sol_flags_(sol_flags),\n       collective_interpolator_(std::move(collective_interpolator)),\n-      matmul_interpolator_(std::move(matmul_interpolator)) {}\n+      matmul_interpolator_(std::move(matmul_interpolator)),\n+      mlir_context_(mlir_context) {}\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "2dd1dfb0acccfa9ba41724e00c10f5dd20749cfd",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.h",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/time/time.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/model/collective_interpolator.h\"\n@@ -65,7 +66,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n   static absl::StatusOr<absl::Duration> ComputeCollectiveTime(\n       const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-      const SolGPUCostModel::Config& sol_flags,\n+      const SolGPUCostModel::Config& sol_flags, mlir::MLIRContext* mlir_context,\n       const CollectiveInterpolator* collective_interpolator = nullptr);\n \n   // Computes the time it takes to execute the given collective instruction.\n@@ -77,7 +78,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n       const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n       const SolGPUCostModel::Config& sol_flags,\n-      const GpuHloCostAnalysis& cost_analysis,\n+      const GpuHloCostAnalysis& cost_analysis, mlir::MLIRContext* mlir_context,\n       const CollectiveInterpolator* collective_interpolator = nullptr);\n \n   // Factory method to create a `SolLatencyEstimator`.\n@@ -86,7 +87,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n       std::unique_ptr<LatencyEstimator> latency_estimator,\n       const se::DeviceDescription& gpu_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      const HloComputation* computation,\n+      const HloComputation* computation, mlir::MLIRContext* mlir_context,\n       std::unique_ptr<GpuHloCostAnalysis> cost_analysis = nullptr);\n \n   // Returns true if the module is supported by the SoL latency estimator.\n@@ -107,7 +108,8 @@ class SolLatencyEstimator : public LatencyEstimator {\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n       SolGPUCostModel::Config sol_flags,\n       std::unique_ptr<CollectiveInterpolator> collective_interpolator,\n-      std::unique_ptr<MatmulInterpolator> matmul_interpolator);\n+      std::unique_ptr<MatmulInterpolator> matmul_interpolator,\n+      mlir::MLIRContext* mlir_context);\n \n   const SchedulerConfig config_;\n   const se::DeviceDescription& gpu_info_;\n@@ -118,6 +120,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n   const SolGPUCostModel::Config sol_flags_;\n   const std::unique_ptr<const CollectiveInterpolator> collective_interpolator_;\n   const std::unique_ptr<const MatmulInterpolator> matmul_interpolator_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "0355ab0f4fe6a302d33aa98cc2672181053afc97",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/time/time.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n@@ -95,7 +96,7 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n   absl::StatusOr<absl::Duration> ComputeCollectiveTime(\n       const HloInstruction& instr) {\n     return SolLatencyEstimator::ComputeCollectiveTime(\n-        instr, gpu_device_info_, shape_size_fn_, sol_flags_,\n+        instr, gpu_device_info_, shape_size_fn_, sol_flags_, &mlir_context_,\n         collective_interpolator_.get());\n   }\n \n@@ -104,7 +105,7 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n     std::unique_ptr<SolLatencyEstimator> estimator =\n         *SolLatencyEstimator::Create(\n             scheduler_config_, std::make_unique<DummyLatencyEstimator>(),\n-            gpu_device_info_, shape_size_fn_, computation);\n+            gpu_device_info_, shape_size_fn_, computation, &mlir_context_);\n     LatencyEstimator::TimeCost cost_val = estimator->NodeCost(&instr);\n     return absl::Microseconds(static_cast<int64_t>(cost_val));\n   }\n@@ -114,6 +115,7 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n   const SolGPUCostModel::Config sol_flags_;\n   SchedulerConfig scheduler_config_;\n   std::unique_ptr<CollectiveInterpolator> collective_interpolator_;\n+  mlir::MLIRContext mlir_context_;\n };\n \n TEST_P(SolLatencyEstimatorTest, TestLatencyEstimation) {"
        },
        {
            "sha": "8227155092c26cf8fe323fc4362abb508878f1ed",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -398,7 +398,8 @@ absl::Status NVPTXCompiler::AddGemmFusionAutotuningPasses(\n     const se::SemanticVersion& toolkit_version,\n     se::StreamExecutor* stream_executor) {\n   pipeline->AddPass<GemmFusionAutotuner>(autotune_config, toolkit_version,\n-                                         thread_pool, key_value_store);\n+                                         thread_pool, key_value_store,\n+                                         mlir_context());\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "ea17b578aa3c9c6bbe97938de72d04c8e7aa98ab",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -74,7 +75,7 @@ class NVPTXCompilerTest : public HloTestBase {\n     std::unique_ptr<GpuAliasInfo> alias_info =\n         compiler.GetAliasInfo(gpu_device_info);\n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, pointer_size, gpu_device_info,\n-                                         alias_info.get())\n+                                         &mlir_context_, alias_info.get())\n                            .status());\n \n     auto buffer_size_bytes_function =\n@@ -88,6 +89,9 @@ class NVPTXCompilerTest : public HloTestBase {\n         /*color_alignment=*/\n         [](LogicalBuffer::Color) { return kXlaAllocatedBufferAlignBytes; });\n   }\n+\n+ protected:\n+  mlir::MLIRContext mlir_context_;\n };\n \n class NVPTXCompilerTestTriton : public NVPTXCompilerTest {"
        },
        {
            "sha": "7e7c8ae38bbab873f18b3163ae4a890b2b4009d5",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -1982,6 +1982,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2003,6 +2004,7 @@ xla_cc_test(\n         \"//xla/service/gpu:gpu_fusible\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2071,6 +2073,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2151,6 +2154,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2615,6 +2619,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2999,6 +3004,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -3030,6 +3036,7 @@ xla_test(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n "
        },
        {
            "sha": "458d2d9e227e4555eca6e71873bec35207764b79",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -310,6 +310,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -329,6 +330,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n "
        },
        {
            "sha": "3bfc86beca29c283ab14dad4bde52556b4182ff3",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -96,13 +97,14 @@ struct Metadata {\n //   synchronous post scheduling.\n absl::StatusOr<Metadata> GetSchedulingMetadata(\n     const HloModule& module, int64_t pointer_size,\n-    const se::DeviceDescription& device_info, const GpuAliasInfo* alias_info) {\n+    const se::DeviceDescription& device_info, mlir::MLIRContext* mlir_context,\n+    const GpuAliasInfo* alias_info) {\n   std::unique_ptr<HloModule> cloned_module = module.Clone();\n   AnnotateCollectives(cloned_module.get());\n   TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(cloned_module.get()));\n   TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n                       ScheduleGpuModule(cloned_module.get(), pointer_size,\n-                                        device_info, alias_info));\n+                                        device_info, mlir_context, alias_info));\n   TF_RETURN_IF_ERROR(AnnotateSyncCollectives(cloned_module.get()));\n   return Metadata{schedule_metadata.peak_memory_usage,\n                   SyncCollectiveIds(*cloned_module)};\n@@ -125,7 +127,8 @@ absl::StatusOr<bool> CollectiveCombinerAnnotator::Run(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   TF_ASSIGN_OR_RETURN(\n       Metadata metadata,\n-      GetSchedulingMetadata(*module, pointer_size_, device_info_, alias_info_));\n+      GetSchedulingMetadata(*module, pointer_size_, device_info_, mlir_context_,\n+                            alias_info_));\n   int64_t combiner_threshold =\n       MaxAvailableMemory(*module, device_info_) - metadata.peak_memory_bytes;\n   if (combiner_threshold <= 0) {"
        },
        {
            "sha": "ec5d664fb180fe608d5402460f8a7b62f4c9aa59",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator.h",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n@@ -37,10 +38,12 @@ class CollectiveCombinerAnnotator : public HloModulePass {\n  public:\n   CollectiveCombinerAnnotator(se::DeviceDescription device_info,\n                               const GpuAliasInfo* alias_info,\n-                              int64_t pointer_size)\n+                              int64_t pointer_size,\n+                              mlir::MLIRContext* mlir_context)\n       : device_info_(std::move(device_info)),\n         alias_info_(alias_info),\n-        pointer_size_(pointer_size) {}\n+        pointer_size_(pointer_size),\n+        mlir_context_(mlir_context) {}\n \n   absl::StatusOr<bool> Run(\n       HloModule* module,\n@@ -54,6 +57,7 @@ class CollectiveCombinerAnnotator : public HloModulePass {\n   const se::DeviceDescription device_info_;\n   const GpuAliasInfo* alias_info_;\n   const int64_t pointer_size_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n // Returns true if `instr` is a combinable sync collective. False otherwise."
        },
        {
            "sha": "573db47cd4ff5137e55b19bb9e0cdbbbbf5ccf43",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/alias_info.h\"\n@@ -45,10 +46,12 @@ class CollectiveCombinerAnnotatorTest : public HloHardwareIndependentTestBase {\n     stream_executor::DeviceDescription device_info;\n     device_info.set_device_memory_size(device_memory_size);\n     GpuAliasInfo alias_info(device_info);\n-    return RunHloPass(CollectiveCombinerAnnotator(std::move(device_info),\n-                                                  &alias_info, pointer_size),\n-                      module);\n+    return RunHloPass(\n+        CollectiveCombinerAnnotator(std::move(device_info), &alias_info,\n+                                    pointer_size, &mlir_context_),\n+        module);\n   }\n+  mlir::MLIRContext mlir_context_;\n };\n \n TEST_F(CollectiveCombinerAnnotatorTest, SynchronousCollectivesNoOverlap) {"
        },
        {
            "sha": "f5dfb24f67d5b11524d41893fa5273724bf9847a",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -431,7 +431,7 @@ absl::StatusOr<bool> MultiOutputFusion::DoMultiOutputFusion() {\n       computation_->MakeInstructionPostOrder();\n \n   FusionInfoCache fusion_info_cache(device_info_);\n-  GpuPerformanceModelOwning gpu_performance_model(device_info_);\n+  GpuPerformanceModelOwning gpu_performance_model(device_info_, mlir_context_);\n   // Traverse the HLO in uses-before-defs order.\n   for (auto it = defs_before_uses.rbegin(); it != defs_before_uses.rend();\n        ++it) {"
        },
        {
            "sha": "65cc4a353c066f6b2fc43ee99c6dcff566bb6371",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.h",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/hlo_dfs_reachability.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -96,8 +97,11 @@ class MultiOutputFusion : public HloModulePass {\n  public:\n   explicit MultiOutputFusion(\n       const se::DeviceDescription& device_info,\n-      HloCostAnalysis::ShapeSizeFunction shape_size_function)\n-      : device_info_(device_info), shape_size_function_(shape_size_function) {}\n+      HloCostAnalysis::ShapeSizeFunction shape_size_function,\n+      mlir::MLIRContext* mlir_context)\n+      : device_info_(device_info),\n+        shape_size_function_(shape_size_function),\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"multi_output_fusion\"; }\n \n@@ -126,6 +130,7 @@ class MultiOutputFusion : public HloModulePass {\n \n   se::DeviceDescription device_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_function_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "fe8b1ba4a44b0c47150c0db4f3301f38a72b31f4",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -44,16 +45,19 @@ namespace m = ::xla::match;\n class MultiOutputFusionTest : public HloHardwareIndependentTestBase {\n  public:\n   MultiOutputFusion mof_{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         HloCostAnalysis::DefaultShapeSize};\n+                         HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n \n   void CheckMultiOutputFusion(absl::string_view hlo,\n                               std::optional<absl::string_view> expected) {\n     RunAndFilecheckHloRewrite(\n         hlo,\n         MultiOutputFusion{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                          HloCostAnalysis::DefaultShapeSize},\n+                          HloCostAnalysis::DefaultShapeSize, &mlir_context_},\n         expected);\n   }\n+\n+ protected:\n+  mlir::MLIRContext mlir_context_;\n };\n \n const char kModulePrefix[] = R\"("
        },
        {
            "sha": "3a7ea839e016eb3c5c64cad13c3c00dc493c13da",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -1281,10 +1281,10 @@ absl::StatusOr<bool> NestGemmFusion::RunOnModule(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   bool changed = false;\n   auto call_graph = CallGraph::Build(module, execution_threads);\n-  mlir::MLIRContext ctx;\n   for (HloComputation* computation :\n        module->MakeNonfusionComputations(execution_threads)) {\n-    NestGemmFusionVisitor visitor(&ctx, call_graph.get(), compute_capability_);\n+    NestGemmFusionVisitor visitor(mlir_context_, call_graph.get(),\n+                                  compute_capability_);\n     TF_RETURN_IF_ERROR(computation->Accept(&visitor));\n     changed |= visitor.changed();\n   }"
        },
        {
            "sha": "3f0fb6efebec4ffc47826de4de9e8f7876051ccd",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -46,8 +46,9 @@ namespace xla::gpu {\n // nested fusions, each with their own BlockLevelFusionConfig.\n class NestGemmFusion : public HloModulePass {\n  public:\n-  explicit NestGemmFusion(const se::GpuComputeCapability& compute_capability)\n-      : compute_capability_(compute_capability) {}\n+  explicit NestGemmFusion(const se::GpuComputeCapability& compute_capability,\n+                          mlir::MLIRContext* mlir_context)\n+      : compute_capability_(compute_capability), mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"nest_gemm_fusion\"; }\n \n@@ -58,6 +59,7 @@ class NestGemmFusion : public HloModulePass {\n \n  private:\n   const se::GpuComputeCapability compute_capability_;\n+  mlir::MLIRContext* mlir_context_;\n   absl::StatusOr<bool> RunOnModule(\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads);"
        },
        {
            "sha": "f5047f10eda7889280df36251bd0d7431c599085",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 100,
            "deletions": 63,
            "changes": 163,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n@@ -73,6 +74,7 @@ class NestGemmFusionTest : public HloHardwareIndependentTestBase {\n   const se::GpuComputeCapability compute_capability_{\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(se::CudaComputeCapability::Ampere())\n           .gpu_compute_capability()};\n+  mlir::MLIRContext mlir_context_;\n \n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions options =\n@@ -112,8 +114,9 @@ ENTRY entry {\n })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n-  ASSERT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -170,8 +173,9 @@ ENTRY e {\n                          \"num_ctas\":1}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   HloComputation* fusion_computation = module->entry_computation()\n                                            ->root_instruction()\n@@ -233,7 +237,8 @@ ENTRY e {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n   TF_ASSERT_OK_AND_ASSIGN(\n-      bool updated, NestGemmFusion(compute_capability_).Run(module.get()));\n+      bool updated,\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()));\n   EXPECT_TRUE(updated);\n   HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_EQ(root->opcode(), HloOpcode::kTuple);\n@@ -286,8 +291,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  ASSERT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -332,8 +338,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  ASSERT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -378,8 +385,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -412,8 +420,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -445,8 +454,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK: f16[3,11]{1,0} convert(\n@@ -490,8 +500,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -525,8 +536,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n   CHECK: ENTRY\n@@ -568,8 +580,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -606,8 +619,9 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -641,8 +655,9 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -675,8 +690,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -706,8 +722,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -751,7 +768,9 @@ ENTRY e {\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n   // We can nest the fusion including the broadcast.\n-  EXPECT_TRUE(NestGemmFusion(compute_capability_).Run(module.get()).ok());\n+  EXPECT_TRUE(NestGemmFusion(compute_capability_, &mlir_context_)\n+                  .Run(module.get())\n+                  .ok());\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   // Cos should not be rewritten as we cannot hoist bitcast.\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n@@ -791,7 +810,9 @@ ENTRY e {\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n   // We can nest the fusion including the broadcast.\n-  EXPECT_TRUE(NestGemmFusion(compute_capability_).Run(module.get()).ok());\n+  EXPECT_TRUE(NestGemmFusion(compute_capability_, &mlir_context_)\n+                  .Run(module.get())\n+                  .ok());\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   // Cos should not be rewritten as we cannot hoist bitcast.\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n@@ -832,8 +853,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -871,8 +893,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            R\"(\n@@ -921,8 +944,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -971,8 +995,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1006,8 +1031,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1042,8 +1068,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1078,8 +1105,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -1116,8 +1144,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   // Checks that transpose is on rank 3 tensor from hoisting bitcast1, not rank\n   // 4 tensor from hoisting bitcast0 first and then failing to hoist bitcast1.\n@@ -1154,8 +1183,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1189,8 +1219,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1225,8 +1256,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -1259,8 +1291,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1295,8 +1328,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1335,8 +1369,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast\n@@ -1385,8 +1420,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast\n@@ -1432,8 +1468,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(compute_capability_).Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(compute_capability_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast"
        },
        {
            "sha": "ca018f574718a49b7ae15e0dafa42fcb9a6e74d9",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -165,7 +165,7 @@ class PriorityFusionQueue {\n         thread_pool_(thread_pool),\n         fusion_analysis_cache_(fusion_analysis_cache),\n         gpu_performance_model_(*device_info, fusion_analysis_cache,\n-                               gpu_performance_model_cache_),\n+                               gpu_performance_model_cache_, mlir_context),\n         fusion_deduplication_cache_(fusion_deduplication_cache),\n         fusion_info_cache_(*device_info_),\n         reachability_(HloDfsReachability::Build(computation)),\n@@ -1161,7 +1161,7 @@ absl::StatusOr<bool> PriorityFusion::Run(\n \n     auto fusion_queue = std::make_unique<PriorityFusionQueue>(\n         computation, cost_analysis_options_, &device_info_,\n-        fusion_process_dump_.get(), thread_pool_, &mlir_context_,\n+        fusion_process_dump_.get(), thread_pool_, mlir_context_,\n         fusion_analysis_cache_, fusion_deduplication_cache,\n         triton_heroless_fusion_enabled);\n "
        },
        {
            "sha": "ebd629610941723a42fc5d07a22100a0472deafa",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.h",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -43,11 +43,13 @@ class PriorityFusion : public HloModulePass {\n  public:\n   PriorityFusion(tsl::thread::ThreadPool* thread_pool,\n                  const se::DeviceDescription& device,\n-                 GpuHloCostAnalysis::Options cost_analysis_options)\n+                 GpuHloCostAnalysis::Options cost_analysis_options,\n+                 mlir::MLIRContext* mlir_context)\n       : thread_pool_(thread_pool),\n         device_info_(device),\n         cost_analysis_options_(std::move(cost_analysis_options)),\n-        fusion_analysis_cache_(device_info_) {}\n+        fusion_analysis_cache_(device_info_),\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"priority-fusion\"; }\n \n@@ -84,7 +86,7 @@ class PriorityFusion : public HloModulePass {\n \n   HloFusionAnalysisCache fusion_analysis_cache_;\n \n-  mlir::MLIRContext mlir_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "3875f0bb17d035531a6fcfa54f4b0d730460d567",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n@@ -72,9 +73,11 @@ class PriorityFusionTest : public HloHardwareIndependentTestBase {\n   }\n \n   se::DeviceDescription device_info_ = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+  mlir::MLIRContext mlir_context_;\n   PriorityFusion priority_fusion_{\n       /*thread_pool=*/nullptr, device_info_,\n-      GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true}};\n+      GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true},\n+      &mlir_context_};\n };\n \n TEST_F(PriorityFusionTest, FuseWithSharedArgument) {\n@@ -1370,8 +1373,8 @@ TEST_F(PriorityFusionWithTritonEnabledTest,\n   tsl::thread::ThreadPool pool(tsl::Env::Default(), \"priority-fusion-test\", 8);\n   GpuHloCostAnalysis::Options options;\n   options.count_multiple_input_accesses = true;\n-  PriorityFusion priority_fusion_with_thread_pool{/*thread_pool=*/&pool,\n-                                                  device_info_, options};\n+  PriorityFusion priority_fusion_with_thread_pool{\n+      /*thread_pool=*/&pool, device_info_, options, &mlir_context_};\n   EXPECT_THAT(priority_fusion_with_thread_pool.Run(module.get()),\n               absl_testing::IsOkAndHolds(true));\n   HloInstruction* root = module->entry_computation()->root_instruction();"
        },
        {
            "sha": "fc7a1d9520414cd805b88473974a182c62b894ef",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 14,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -266,7 +266,8 @@ absl::StatusOr<HloFusionInstruction*> MakeFusionForDiamond(\n // passes that rewrite and split reductions.\n absl::Status RunFusionPipeline(\n     HloModule* module, const se::DeviceDescription& device_info,\n-    const HloCostAnalysis::ShapeSizeFunction& shape_size) {\n+    const HloCostAnalysis::ShapeSizeFunction& shape_size,\n+    mlir::MLIRContext* mlir_context) {\n   HloPassPipeline reduction_pipeline(\"reduction_pipeline\");\n   // Passes that run after SoftmaxRewriterTriton and before PriorityFusion and\n   // transform reductions.\n@@ -279,7 +280,7 @@ absl::Status RunFusionPipeline(\n   TF_RETURN_IF_ERROR(reduction_pipeline.Run(module).status());\n \n   return FusionPipeline(module->config().debug_options(), shape_size,\n-                        /*thread_pool=*/nullptr, device_info)\n+                        /*thread_pool=*/nullptr, device_info, mlir_context)\n       .Run(module)\n       .status();\n }\n@@ -297,14 +298,15 @@ absl::StatusOr<absl::Duration>\n EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n     const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n-    const HloCostAnalysis::ShapeSizeFunction& shape_size) {\n+    const HloCostAnalysis::ShapeSizeFunction& shape_size,\n+    mlir::MLIRContext* mlir_context) {\n   auto new_module = ExtractComputationIntoNewModule(\n       *fusion->fused_instructions_computation());\n \n   // After this call, the `new_module` will have instruction fused without\n   // SoftmaxRewriterTriton.\n-  TF_RETURN_IF_ERROR(\n-      RunFusionPipeline(new_module.get(), device_info, shape_size));\n+  TF_RETURN_IF_ERROR(RunFusionPipeline(new_module.get(), device_info,\n+                                       shape_size, mlir_context));\n \n   VLOG(3) << \"priority fusion module: \" << new_module->ToString();\n \n@@ -319,7 +321,7 @@ EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n \n   absl::Duration total_run_time = absl::ZeroDuration();\n \n-  GpuPerformanceModelOwning gpu_performance_model(device_info);\n+  GpuPerformanceModelOwning gpu_performance_model(device_info, mlir_context);\n   for (const HloInstruction* instr : entry_computation->instructions()) {\n     total_run_time += gpu_performance_model.Get()\n                           .EstimateRunTimeForInstruction(instr, &cost_analysis)\n@@ -344,7 +346,7 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    bool use_cost_model_to_evaluate_fusions) {\n+    mlir::MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(normalization_fusion);\n \n   TF_ASSIGN_OR_RETURN(\n@@ -361,9 +363,10 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n       std::get<TiledRunTimeData>(std::move(tiled_runtime_data_or));\n \n   if (use_cost_model_to_evaluate_fusions) {\n-    TF_ASSIGN_OR_RETURN(absl::Duration run_time_without_softmax_rewriter,\n-                        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n-                            normalization_fusion, device_info, shape_size));\n+    TF_ASSIGN_OR_RETURN(\n+        absl::Duration run_time_without_softmax_rewriter,\n+        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n+            normalization_fusion, device_info, shape_size, mlir_context));\n \n     VLOG(2) << \"run time estimate if normalization diamond fused together: \"\n             << tiled_runtime_data.runtime_data.exec_time;\n@@ -395,7 +398,7 @@ absl::StatusOr<bool> MaybeFuseDiamondImpl(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    bool use_cost_model_to_evaluate_fusions) {\n+    mlir::MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n   TF_ASSIGN_OR_RETURN(HloFusionInstruction * normalization_fusion,\n                       MakeFusionForDiamond(diamond));\n   HloInstruction* root = diamond.root;\n@@ -406,7 +409,7 @@ absl::StatusOr<bool> MaybeFuseDiamondImpl(\n       FusionDecision fusion_decision,\n       DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n           normalization_fusion, indexing_performance_model, device_info,\n-          shape_size, use_cost_model_to_evaluate_fusions));\n+          shape_size, mlir_context, use_cost_model_to_evaluate_fusions));\n \n   if (!fusion_decision.CanFuse()) {\n     VLOG(2) << \"Not fusing: \" << fusion_decision.Explain();\n@@ -631,10 +634,11 @@ absl::StatusOr<bool> SoftmaxRewriterTriton::MaybeFuseNormalizationDiamond(\n     const DiamondDescriptor& diamond) {\n   HloFusionAnalysisCache fusion_analysis_cache(device_info_);\n   GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n-      &device_info_, &fusion_analysis_cache, shape_size_, &mlir_context_);\n+      &device_info_, &fusion_analysis_cache, shape_size_, mlir_context_);\n \n   return MaybeFuseDiamondImpl(diamond, indexing_performance_model, device_info_,\n-                              shape_size_, use_cost_model_to_evaluate_fusions_);\n+                              shape_size_, mlir_context_,\n+                              use_cost_model_to_evaluate_fusions_);\n }\n \n absl::StatusOr<bool> SoftmaxRewriterTriton::Run("
        },
        {
            "sha": "72a31cd65fc8ca0d2d9b14b766581bd44f74fdcd",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -51,10 +51,12 @@ class SoftmaxRewriterTriton : public HloModulePass {\n  public:\n   explicit SoftmaxRewriterTriton(const se::DeviceDescription& device_info,\n                                  HloCostAnalysis::ShapeSizeFunction shape_size,\n+                                 mlir::MLIRContext* mlir_context,\n                                  bool only_fuse_if_profitable = false)\n       : device_info_(device_info),\n         shape_size_(shape_size),\n-        use_cost_model_to_evaluate_fusions_(only_fuse_if_profitable) {}\n+        use_cost_model_to_evaluate_fusions_(only_fuse_if_profitable),\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"triton-softmax-rewriter\"; }\n \n@@ -104,7 +106,7 @@ class SoftmaxRewriterTriton : public HloModulePass {\n   const se::DeviceDescription& device_info_;\n   const HloCostAnalysis::ShapeSizeFunction shape_size_;\n   bool use_cost_model_to_evaluate_fusions_;\n-  mlir::MLIRContext mlir_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "e40222034df15bee53fa9a19b2646ff4dbf472e9",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status_matchers.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -61,8 +62,9 @@ class SoftmaxRewriterTritonTest\n       public ::testing::WithParamInterface<PrimitiveType> {\n  protected:\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n-  SoftmaxRewriterTriton fusion_rewriter_{device_info_,\n-                                         HloCostAnalysis::DefaultShapeSize};\n+  mlir::MLIRContext mlir_context_;\n+  SoftmaxRewriterTriton fusion_rewriter_{\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n };\n \n TEST_F(SoftmaxRewriterTritonTest, CanFuseSingleNormalizationF32) {\n@@ -562,7 +564,7 @@ ENTRY main {\n       SoftmaxRewriterTriton(\n           TestGpuDeviceInfo::RTXA6000DeviceInfo(\n               se::CudaComputeCapability{se::CudaComputeCapability::kVolta, 0}),\n-          HloCostAnalysis::DefaultShapeSize)\n+          HloCostAnalysis::DefaultShapeSize, &mlir_context_)\n           .Run(module.get()),\n       absl_testing::StatusIs(\n           tsl::error::FAILED_PRECONDITION,\n@@ -590,7 +592,8 @@ ENTRY main {\n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n \n   EXPECT_TRUE(SoftmaxRewriterTriton(TestGpuDeviceInfo::AMDMI210DeviceInfo(),\n-                                    HloCostAnalysis::DefaultShapeSize)\n+                                    HloCostAnalysis::DefaultShapeSize,\n+                                    &mlir_context_)\n                   .Run(module.get())\n                   .ok());\n }\n@@ -675,8 +678,8 @@ ENTRY main {\n }\n )\";\n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n-  SoftmaxRewriterTriton fusion_rewriter(device_info_,\n-                                        HloCostAnalysis::DefaultShapeSize);\n+  SoftmaxRewriterTriton fusion_rewriter(\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n   EXPECT_FALSE(fusion_rewriter_.Run(module.get()).value());\n }\n \n@@ -824,7 +827,7 @@ ENTRY main {\n \n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n   SoftmaxRewriterTriton softmax_rewriter_triton(\n-      device_info_, HloCostAnalysis::DefaultShapeSize);\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n   int unmatched = 0, matched = 0;\n   for (HloInstruction* instruction :\n        module->entry_computation()->MakeInstructionPostOrder()) {\n@@ -1079,7 +1082,7 @@ ENTRY main {\n     // Verify that SoftmaxRewriterTriton without Cost Model will fuse the\n     // normalization diamond.\n     SoftmaxRewriterTriton fusion_rewriter_without_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize,\n+        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n         /*only_fuse_if_profitable=*/false};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n@@ -1094,7 +1097,7 @@ ENTRY main {\n     // SoftmaxRewriterTriton with Cost Model will discard the normalization\n     // diamond, because row size is too large.\n     SoftmaxRewriterTriton fusion_rewriter_with_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize,\n+        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n         /*only_fuse_if_profitable=*/true};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();"
        },
        {
            "sha": "a154b7780ae69c8e5397625f216cff4100225125",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 9,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -124,7 +125,8 @@ absl::Status InlineModuleFusions(HloModule* hlo_module) {\n // days instead of milliseconds).\n absl::StatusOr<std::unique_ptr<HloModule>> NewHloModuleFromFusionComputation(\n     const HloFusionInstruction& fusion, const DebugOptions& debug_opts,\n-    const se::DeviceDescription& gpu_device_info) {\n+    const se::DeviceDescription& gpu_device_info,\n+    mlir::MLIRContext* mlir_context) {\n   std::unique_ptr<HloModule> new_module =\n       ExtractComputationIntoNewModule(*fusion.fused_instructions_computation());\n   new_module->mutable_config().set_debug_options(debug_opts);\n@@ -147,7 +149,8 @@ absl::StatusOr<std::unique_ptr<HloModule>> NewHloModuleFromFusionComputation(\n           .Run(new_module.get())\n           .status());\n   PriorityFusion fusion_pass(\n-      /*thread_pool=*/nullptr, gpu_device_info, HloCostAnalysis::Options{});\n+      /*thread_pool=*/nullptr, gpu_device_info, HloCostAnalysis::Options{},\n+      mlir_context);\n   TF_RETURN_IF_ERROR(fusion_pass.Run(new_module.get()).status());\n \n   // If the priority fusion pass above skipped some instructions, turn them\n@@ -173,12 +176,13 @@ namespace triton_fusion_numerics_pass_internal {\n absl::StatusOr<ScopedShapedBuffer> CompileAndRunFusion(\n     AutotunerCompileUtil& util, const HloFusionInstruction& fusion,\n     const DeviceOrDevicelessConfig& config, const DebugOptions& debug_opts,\n-    bool disable_triton) {\n+    bool disable_triton, mlir::MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<Executable> executable,\n       util.Compile([&](const DebugOptions& opts) {\n         return disable_triton ? NewHloModuleFromFusionComputation(\n-                                    fusion, opts, config.GetDeviceDescription())\n+                                    fusion, opts, config.GetDeviceDescription(),\n+                                    mlir_context)\n                               : NewHloModuleWithTritonFromFusion(fusion, opts);\n       }));\n   if (executable == nullptr) {\n@@ -249,15 +253,16 @@ namespace {\n absl::Status VerifyTritonFusion(AutotunerCompileUtil& util,\n                                 const HloFusionInstruction& fusion,\n                                 const DeviceOrDevicelessConfig& config,\n-                                const DebugOptions& debug_opts) {\n+                                const DebugOptions& debug_opts,\n+                                mlir::MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(auto triton_result,\n                       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n                           util, fusion, config, debug_opts,\n-                          /*disable_triton=*/false));\n+                          /*disable_triton=*/false, mlir_context));\n   TF_ASSIGN_OR_RETURN(auto emitters_result,\n                       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n                           util, fusion, config, debug_opts,\n-                          /*disable_triton=*/true));\n+                          /*disable_triton=*/true, mlir_context));\n \n   TF_ASSIGN_OR_RETURN(auto stream, config.GetStream());\n   auto status = triton_fusion_numerics_pass_internal::CompareBuffers(\n@@ -319,8 +324,8 @@ absl::StatusOr<bool> TritonFusionNumericsVerifier::Run(\n           ++cache_hits_;\n           return it->second;\n         }\n-        auto result =\n-            VerifyTritonFusion(compile_util, fusion, config_, debug_options);\n+        auto result = VerifyTritonFusion(compile_util, fusion, config_,\n+                                         debug_options, mlir_context_);\n         fusion_result_cache_[key] = result;\n         return result;\n       }));"
        },
        {
            "sha": "ca04d8768df2bdfa5809fd51021304613205ed17",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier.h",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n@@ -41,8 +42,9 @@ namespace xla::gpu {\n // generated with the regular emitters.\n class TritonFusionNumericsVerifier : public HloModulePass {\n  public:\n-  explicit TritonFusionNumericsVerifier(const DeviceOrDevicelessConfig& config)\n-      : config_(config) {}\n+  TritonFusionNumericsVerifier(const DeviceOrDevicelessConfig& config,\n+                               mlir::MLIRContext* mlir_context)\n+      : config_(config), mlir_context_(mlir_context) {}\n \n   static absl::string_view Name() { return \"triton-numerics-verifier\"; }\n   absl::string_view name() const override { return Name(); }\n@@ -58,6 +60,7 @@ class TritonFusionNumericsVerifier : public HloModulePass {\n \n  private:\n   DeviceOrDevicelessConfig config_;\n+  mlir::MLIRContext* mlir_context_;\n \n   // In some models there are many identical fusions. These are cached to avoid\n   // expensive recomputations.\n@@ -70,7 +73,7 @@ namespace triton_fusion_numerics_pass_internal {\n absl::StatusOr<ScopedShapedBuffer> CompileAndRunFusion(\n     AutotunerCompileUtil& util, const HloFusionInstruction& fusion,\n     const DeviceOrDevicelessConfig& config, const DebugOptions& debug_opts,\n-    bool disable_triton);\n+    bool disable_triton, mlir::MLIRContext* mlir_context);\n absl::Status CompareBuffers(const ScopedShapedBuffer& current,\n                             const ScopedShapedBuffer& expected,\n                             const Shape& shape, const DebugOptions& debug_opts,"
        },
        {
            "sha": "53d088000bed2f3eb5196a1befd587c76b14f3f2",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier_test.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 17,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/primitive_util.h\"\n@@ -86,6 +87,8 @@ class TritonFusionNumericsVerifierTest\n     TF_EXPECT_OK(compile_util_or);\n     return std::move(compile_util_or).value();\n   }\n+\n+  mlir::MLIRContext mlir_context_;\n };\n \n constexpr absl::string_view kSoftmaxHlo = R\"(\n@@ -130,8 +133,8 @@ TEST_P(TritonFusionNumericsVerifierTest, VerifyExactSoftmaxFusionNumerics) {\n                        primitive_util::LowercasePrimitiveTypeName(GetParam()));\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n-  auto verifier =\n-      TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig());\n+  auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -185,8 +188,8 @@ ENTRY entry {\n                        primitive_util::LowercasePrimitiveTypeName(GetParam()));\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n-  auto verifier =\n-      TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig());\n+  auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -216,8 +219,8 @@ ENTRY main{\n                        primitive_util::LowercasePrimitiveTypeName(GetParam()));\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n-  auto verifier =\n-      TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig());\n+  auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -306,8 +309,8 @@ ENTRY main (p0: bf16[128,512], p1: bf16[256,512], p2: bf16[512,512]) -> bf16[384\n                        primitive_util::LowercasePrimitiveTypeName(GetParam()));\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n-  auto verifier =\n-      TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig());\n+  auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -339,12 +342,12 @@ TEST_F(TritonFusionNumericsVerifierTest, CheckMismatch) {\n \n   auto f64_result = triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n       compile_util, *fusion_f64, autotune_config, debug_options,\n-      /*disable_triton=*/false);\n+      /*disable_triton=*/false, &mlir_context_);\n   TF_EXPECT_OK(f64_result);\n \n   auto f32_result = triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n       compile_util, *fusion_f32, autotune_config, debug_options,\n-      /*disable_triton=*/false);\n+      /*disable_triton=*/false, &mlir_context_);\n   TF_EXPECT_OK(f32_result);\n \n   auto stream = autotune_config.GetStream();\n@@ -393,8 +396,8 @@ ENTRY main {\n })\",\n                        \"\");\n \n-  auto verifier =\n-      TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig());\n+  auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n   auto fusion = TritonFusion(*module);\n   EXPECT_NE(fusion, nullptr);\n@@ -405,7 +408,7 @@ ENTRY main {\n   auto compilation_result =\n       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n           compile_util, *fusion, autotune_config, GetDebugOptionsForTest(),\n-          /*disable_triton=*/false);\n+          /*disable_triton=*/false, &mlir_context_);\n \n   // Verify that the compilation with default flags fails. The compilation\n   // fails, because the kernel will spill registers, but the error is\n@@ -463,8 +466,8 @@ ENTRY main {\n   )\";\n \n   std::unique_ptr<HloModule> module = Module(hlo_text, \"\");\n-  auto verifier =\n-      TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig());\n+  auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n   EXPECT_EQ(verifier.CacheHitsForTestingOnly(), 1);\n }\n@@ -518,8 +521,8 @@ ENTRY main {\n   )\";\n   auto module = Module(hlo_text, \"\");\n   EXPECT_NE(TritonFusion(*module), nullptr);\n-  auto verifier =\n-      TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig());\n+  auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n "
        },
        {
            "sha": "87bf01a8ae38d356f669129ca611ce35d736e689",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_opt.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b162bb889b92ea6dc7890943dfb087d446573aa/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc?ref=7b162bb889b92ea6dc7890943dfb087d446573aa",
            "patch": "@@ -213,7 +213,8 @@ class GpuOptProvider : public CompiledOptProvider {\n       TF_ASSIGN_OR_RETURN(gpu::ScheduleMetadata schedule_metadata,\n                           gpu::ScheduleGpuModule(\n                               optimized_module, gpu_compiler->GetPointerSize(),\n-                              device_description, alias_info.get()));\n+                              device_description, gpu_compiler->mlir_context(),\n+                              alias_info.get()));\n       TF_RETURN_IF_ERROR(gpu_compiler->RunPostSchedulingPipelines(\n           optimized_module, schedule_metadata.scheduler_mem_limit,\n           device_description, alias_info.get()));"
        }
    ],
    "stats": {
        "total": 956,
        "additions": 609,
        "deletions": 347
    }
}