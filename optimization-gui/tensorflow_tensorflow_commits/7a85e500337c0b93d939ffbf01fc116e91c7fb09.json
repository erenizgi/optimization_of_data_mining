{
    "author": "unknown",
    "message": "Merge remote-tracking branch 'refs/remotes/origin/FixForCuda12.9.1Compilation' into FixForCuda12.9.1Compilation",
    "sha": "7a85e500337c0b93d939ffbf01fc116e91c7fb09",
    "files": [
        {
            "sha": "950439bfc14bcdfb3abdcab3d26bb4da70cc59a0",
            "filename": "tensorflow/compiler/mlir/lite/schema/conversion_metadata_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fschema%2Fconversion_metadata_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fschema%2Fconversion_metadata_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fschema%2Fconversion_metadata_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n namespace tflite {"
        },
        {
            "sha": "a8275936736d5e0a89cc2c0c4aa75600bb74097d",
            "filename": "tensorflow/compiler/mlir/lite/schema/schema_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fschema%2Fschema_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fschema%2Fschema_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fschema%2Fschema_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n namespace tflite {"
        },
        {
            "sha": "1f607290338ae3776fde616629c683468938ded5",
            "filename": "tensorflow/lite/acceleration/configuration/configuration_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Facceleration%2Fconfiguration%2Fconfiguration_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Facceleration%2Fconfiguration%2Fconfiguration_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Facceleration%2Fconfiguration%2Fconfiguration_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n namespace tflite {"
        },
        {
            "sha": "676670f1e8bd034c56ba4988ea52d77ffe2aab46",
            "filename": "tensorflow/lite/delegates/gpu/cl/compiled_program_cache_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcl%2Fcompiled_program_cache_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcl%2Fcompiled_program_cache_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcl%2Fcompiled_program_cache_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n namespace tflite {"
        },
        {
            "sha": "fa9356613dfe395e789efb80085086e41ce85234",
            "filename": "tensorflow/lite/delegates/gpu/cl/serialization_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcl%2Fserialization_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcl%2Fserialization_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcl%2Fserialization_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n #include \"gpu_model_generated.h\""
        },
        {
            "sha": "4ad3e77129873373b1959e8114ce04e0959d973f",
            "filename": "tensorflow/lite/delegates/gpu/common/gpu_model_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcommon%2Fgpu_model_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcommon%2Fgpu_model_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcommon%2Fgpu_model_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -21,9 +21,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n #include \"tflite_serialization_base_generated.h\""
        },
        {
            "sha": "3e50db9e38f6d08f61a8413b1749e55886214ca5",
            "filename": "tensorflow/lite/delegates/gpu/common/task/tflite_serialization_base_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcommon%2Ftask%2Ftflite_serialization_base_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcommon%2Ftask%2Ftflite_serialization_base_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fgpu%2Fcommon%2Ftask%2Ftflite_serialization_base_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n namespace tflite {"
        },
        {
            "sha": "ab2f789d4094592a21b9105a1576fc206bbebed8",
            "filename": "tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fconfiguration%2Fconfiguration_generated.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fconfiguration%2Fconfiguration_generated.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fconfiguration%2Fconfiguration_generated.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -22,9 +22,9 @@ limitations under the License.\n \n // Ensure the included flatbuffers.h is the same version as when this file was\n // generated, otherwise it may not be compatible.\n-static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&\n-              FLATBUFFERS_VERSION_MINOR == 3 &&\n-              FLATBUFFERS_VERSION_REVISION == 25,\n+static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&\n+              FLATBUFFERS_VERSION_MINOR == 2 &&\n+              FLATBUFFERS_VERSION_REVISION == 10,\n              \"Non-compatible flatbuffers version included\");\n \n namespace tflite {"
        },
        {
            "sha": "9e31bd33ac42ce58edcba3d53ba93954fe9cca32",
            "filename": "tensorflow/tools/ci_build/release/requirements_common.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Ftools%2Fci_build%2Frelease%2Frequirements_common.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Ftools%2Fci_build%2Frelease%2Frequirements_common.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fci_build%2Frelease%2Frequirements_common.txt?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -3,7 +3,7 @@\n # This will change in the future.\n absl-py ~= 1.0.0\n astunparse ~= 1.6.3\n-flatbuffers ~= 24.3.25\n+flatbuffers ~= 25.2.10\n google_pasta ~= 0.2\n h5py ~= 3.10.0  # Earliest version for Python 3.12\n ml_dtypes ~= 0.5.1"
        },
        {
            "sha": "457cb5a1463f59a7f9986e8ad2e975555f3f828f",
            "filename": "tensorflow/tools/pip_package/setup.py.tpl",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Ftools%2Fpip_package%2Fsetup.py.tpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Ftools%2Fpip_package%2Fsetup.py.tpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fpip_package%2Fsetup.py.tpl?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -81,7 +81,7 @@ def standard_or_nightly(standard, nightly):\n REQUIRED_PACKAGES = [\n     'absl-py >= 1.0.0',\n     'astunparse >= 1.6.0',\n-    'flatbuffers >= 24.3.25',\n+    'flatbuffers >= 25.2.10',\n     'gast >=0.2.1,!=0.5.0,!=0.5.1,!=0.5.2',\n     'google_pasta >= 0.1.1',\n     'libclang >= 13.0.0',"
        },
        {
            "sha": "deb23bf29bb2fbe24527d7c99d4a9eaeeec6e369",
            "filename": "tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Ftools%2Ftf_sig_build_dockerfiles%2Fdevel.requirements.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/tensorflow%2Ftools%2Ftf_sig_build_dockerfiles%2Fdevel.requirements.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftf_sig_build_dockerfiles%2Fdevel.requirements.txt?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -6,7 +6,7 @@\n # This will change in the future.\n absl-py ~= 1.0.0\n astunparse ~= 1.6.3\n-flatbuffers ~= 24.3.25\n+flatbuffers ~= 25.2.10\n google_pasta ~= 0.2\n h5py ~= 3.11.0 # Earliest version for NumPy 2.0\n ml_dtypes ~= 0.5.1 # Earliest version with mxfloat types"
        },
        {
            "sha": "16c70e587af19c9857868063c1b45dbf78f453bc",
            "filename": "third_party/flatbuffers/workspace.bzl",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fflatbuffers%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fflatbuffers%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fflatbuffers%2Fworkspace.bzl?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -2,20 +2,17 @@\n \n load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n-# _FLATBUFFERS_GIT_COMMIT / _FLATBUFFERS_SHA256 were added due to an urgent change being made to\n-# Flatbuffers that needed to be updated in order for Flatbuffers/TfLite be compatible with Android\n-# API level >= 23. They can be removed next flatbuffers offical release / update.\n-_FLATBUFFERS_GIT_COMMIT = \"e6463926479bd6b330cbcf673f7e917803fd5831\"\n+_FLATBUFFERS_VERSION = \"25.2.10\"\n \n-# curl -L https://github.com/google/flatbuffers/archive/<_FLATBUFFERS_GIT_COMMIT>.tar.gz | shasum -a 256\n-_FLATBUFFERS_SHA256 = \"c9c6b8653597ed7ee5c62243979010bd0f09b29a46be414505bc5b58a874bb17\"\n+# curl -L https://github.com/google/flatbuffers/archive/<_FLATBUFFERS_VERSION>.tar.gz | shasum -a 256\n+_FLATBUFFERS_SHA256 = \"b9c2df49707c57a48fc0923d52b8c73beb72d675f9d44b2211e4569be40a7421\"\n \n def repo():\n     tf_http_archive(\n         name = \"flatbuffers\",\n-        strip_prefix = \"flatbuffers-%s\" % _FLATBUFFERS_GIT_COMMIT,\n+        strip_prefix = \"flatbuffers-%s\" % _FLATBUFFERS_VERSION,\n         sha256 = _FLATBUFFERS_SHA256,\n-        urls = tf_mirror_urls(\"https://github.com/google/flatbuffers/archive/%s.tar.gz\" % _FLATBUFFERS_GIT_COMMIT),\n+        urls = tf_mirror_urls(\"https://github.com/google/flatbuffers/archive/v%s.tar.gz\" % _FLATBUFFERS_VERSION),\n         build_file = \"//third_party/flatbuffers:flatbuffers.BUILD\",\n         system_build_file = \"//third_party/flatbuffers:BUILD.system\",\n         link_files = {"
        },
        {
            "sha": "7ac815ada9d5e270758ae26fa8699dd6c0e3c00d",
            "filename": "third_party/xla/WORKSPACE",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2FWORKSPACE",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2FWORKSPACE",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2FWORKSPACE?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -9,10 +9,10 @@ load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n # Details: https://github.com/google-ml-infra/rules_ml_toolchain\n http_archive(\n     name = \"rules_ml_toolchain\",\n-    sha256 = \"9a3e9b3e1f5e8368ab5dfa7d4ec17688810ceeb521b637c975c53d8ade65d513\",\n-    strip_prefix = \"rules_ml_toolchain-087c24520fa94fcded738b8a3cd1113566629140\",\n+    sha256 = \"e7e44c4e349a1c1f31398bd2257c51432e73ea0e7e24cce67090b68b0b50007e\",\n+    strip_prefix = \"rules_ml_toolchain-55dcd0a52c7e0f9eec9927a32512229c09ac3b3e\",\n     urls = [\n-        \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/087c24520fa94fcded738b8a3cd1113566629140.tar.gz\",\n+        \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/55dcd0a52c7e0f9eec9927a32512229c09ac3b3e.tar.gz\",\n     ],\n )\n "
        },
        {
            "sha": "b40d86d97a60dd659899aa90e9ea6001b2d7aace",
            "filename": "third_party/xla/third_party/raft/raft.BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Fraft%2Fraft.BUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Fraft%2Fraft.BUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fraft%2Fraft.BUILD?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -18,28 +18,19 @@ BASE_COPTS = [\n \n cuda_library(\n     name = \"raft_matrix\",\n-    srcs = glob([\n-        \"cpp/include/raft/core/detail/*.hpp\",\n-        \"cpp/include/raft/core/resource/detail/*.hpp\",\n-        \"cpp/include/raft/linalg/detail/*.hpp\",\n-        \"cpp/include/raft/matrix/detail/*.hpp\",\n-        \"cpp/include/raft/util/detail/*.hpp\",\n-    ]),\n-    hdrs = glob([\n-        \"cpp/include/raft/core/*.hpp\",\n-        \"cpp/include/raft/core/resource/*.hpp\",\n-        \"cpp/include/raft/linalg/*.hpp\",\n-        \"cpp/include/raft/matrix/*.hpp\",\n-        \"cpp/include/raft/util/*.hpp\",\n-    ]),\n     copts = BASE_COPTS,\n-    includes = [\"cpp/include\"],\n+    includes = [\n+        \"cpp/include\",\n+        \"cpp/internal\",\n+    ],\n     textual_hdrs = glob([\n-        \"cpp/include/raft/core/**/*.cuh\",\n-        \"cpp/include/raft/linalg/**/*.cuh\",\n-        \"cpp/include/raft/matrix/**/*.cuh\",\n-        \"cpp/include/raft/util/**/*.cuh\",\n-    ]),\n+        \"cpp/include/**/*.cuh\",\n+        \"cpp/include/**/*.hpp\",\n+        \"cpp/internal/**/*.cuh\",\n+    ]) + [\n+        \"cpp/include/raft/thirdparty/mdspan/include/experimental/mdarray\",\n+        \"cpp/include/raft/thirdparty/mdspan/include/experimental/mdspan\",\n+    ],\n     visibility = [\"//visibility:public\"],\n     deps = [\n         \"@kokkos//:mdspan\","
        },
        {
            "sha": "23d9fccae09e97967aac7d32092912bbb66ad1ec",
            "filename": "third_party/xla/workspace0.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fworkspace0.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fworkspace0.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fworkspace0.bzl?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -140,10 +140,10 @@ def workspace():\n     if \"rules_ml_toolchain\" not in native.existing_rules():\n         http_archive(\n             name = \"rules_ml_toolchain\",\n-            sha256 = \"9a3e9b3e1f5e8368ab5dfa7d4ec17688810ceeb521b637c975c53d8ade65d513\",\n-            strip_prefix = \"rules_ml_toolchain-087c24520fa94fcded738b8a3cd1113566629140\",\n+            sha256 = \"e7e44c4e349a1c1f31398bd2257c51432e73ea0e7e24cce67090b68b0b50007e\",\n+            strip_prefix = \"rules_ml_toolchain-55dcd0a52c7e0f9eec9927a32512229c09ac3b3e\",\n             urls = [\n-                \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/087c24520fa94fcded738b8a3cd1113566629140.tar.gz\",\n+                \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/55dcd0a52c7e0f9eec9927a32512229c09ac3b3e.tar.gz\",\n             ],\n         )\n "
        },
        {
            "sha": "1818ad9ab1cee0a1061bd8362710a76287e90dee",
            "filename": "third_party/xla/xla/backends/cpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcollective_thunk.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -133,6 +133,10 @@ class CollectiveThunk : public Thunk {\n \n   const Shape& destination_shape(int64_t index) const;\n \n+  // Collective operations are typically completed asynchronously on the IO\n+  // thread pool, owned by the underlying collective implementation.\n+  bool ExecutesOnExternalThreadPool() const final { return true; }\n+\n  private:\n   OpParams op_params_;\n   OpBuffers op_buffers_;"
        },
        {
            "sha": "aae721bdfe710161276825f33bb7a83352ee7598",
            "filename": "third_party/xla/xla/backends/cpu/transforms/collectives/BUILD",
            "status": "added",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2FBUILD?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,52 @@\n+load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n+load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = [\":friends\"],\n+    licenses = [\"notice\"],\n+)\n+\n+package_group(\n+    name = \"friends\",\n+    includes = [\n+        \"//xla:friends\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"all_reduce_combiner\",\n+    srcs = [\"all_reduce_combiner.cc\"],\n+    hdrs = [\"all_reduce_combiner.h\"],\n+    deps = [\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/hlo/transforms/collectives:all_reduce_combiner\",\n+        \"//xla/service:hlo_domain_map\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"all_reduce_combiner_test\",\n+    srcs = [\"all_reduce_combiner_test.cc\"],\n+    deps = [\n+        \":all_reduce_combiner\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:filecheck\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/utils:hlo_matchers\",\n+        \"//xla/service:collective_utils\",\n+        \"//xla/tsl/platform:status_matchers\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)"
        },
        {
            "sha": "105ad861eff106faa556ddc17403059235d22fb3",
            "filename": "third_party/xla/xla/backends/cpu/transforms/collectives/all_reduce_combiner.cc",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,33 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/transforms/collectives/all_reduce_combiner.h\"\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/transforms/collectives/all_reduce_combiner.h\"\n+\n+namespace xla::cpu {\n+\n+absl::StatusOr<bool> CpuAllReduceCombiner::Run(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  return RunWithKeyCombiner(module, execution_threads,\n+                            AllReduceCombiner::CombineKey);\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "85d8560337e325962dc78a0ef42860009bbfe674",
            "filename": "third_party/xla/xla/backends/cpu/transforms/collectives/all_reduce_combiner.h",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,44 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_TRANSFORMS_COLLECTIVES_ALL_REDUCE_COMBINER_H_\n+#define XLA_BACKENDS_CPU_TRANSFORMS_COLLECTIVES_ALL_REDUCE_COMBINER_H_\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/hlo/transforms/collectives/all_reduce_combiner.h\"\n+\n+namespace xla::cpu {\n+\n+// Combines `AllReduce` ops into a single larger `AllReduce` op to maximize\n+// network bandwidth usage.\n+class CpuAllReduceCombiner : public AllReduceCombiner {\n+ public:\n+  using AllReduceCombiner::AllReduceCombiner;\n+\n+  absl::string_view name() const override { return \"cpu-all-reduce-combiner\"; }\n+\n+  using HloPassInterface::Run;\n+  absl::StatusOr<bool> Run(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+};\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_TRANSFORMS_COLLECTIVES_ALL_REDUCE_COMBINER_H_"
        },
        {
            "sha": "8f3422bc405940009fc0d24720be5f6e9f9bce2f",
            "filename": "third_party/xla/xla/backends/cpu/transforms/collectives/all_reduce_combiner_test.cc",
            "status": "added",
            "additions": 103,
            "deletions": 0,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fcollectives%2Fall_reduce_combiner_test.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,103 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+\n+You may obtain a copy of the License at\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/transforms/collectives/all_reduce_combiner.h\"\n+\n+#include <cstdint>\n+#include <limits>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n+#include \"xla/hlo/testlib/filecheck.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::cpu {\n+namespace {\n+\n+using CpuAllReduceCombinerTest = HloHardwareIndependentTestBase;\n+\n+absl::StatusOr<bool> RunCombiner(HloModule* module) {\n+  return CpuAllReduceCombiner(std::numeric_limits<int64_t>::max(),\n+                              std::numeric_limits<int64_t>::max())\n+      .Run(module);\n+}\n+\n+TEST_F(CpuAllReduceCombinerTest, CombinesCollectivesUpToSpecifiedThreshold) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule m\n+\n+add {\n+  a = f32[] parameter(0)\n+  b = f32[] parameter(1)\n+  ROOT add = f32[] add(a, b)\n+}\n+\n+ENTRY main {\n+  p0 = f32[16,256,8,128]{3,2,1,0} parameter(0)\n+  p1 = f32[32,256,2,128]{3,2,1,0} parameter(1)\n+  p2 = f32[16,8,128,256]{3,2,1,0} parameter(2)\n+  p3 = f32[32,256,3072]{2,1,0} parameter(3)\n+  p4 = f32[16,3072,256]{2,1,0} parameter(4)\n+  p5 = f32[66,1024]{1,0} parameter(5)\n+  p6 = f32[2,32768,256]{2,1,0} parameter(6)\n+  r0 = f32[16,256,8,128]{3,2,1,0} all-reduce(p0), channel_id=1, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=add\n+  r1 = f32[32,256,2,128]{3,2,1,0} all-reduce(p1), channel_id=2, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=add\n+  r2 = f32[16,8,128,256]{3,2,1,0} all-reduce(p2), channel_id=3, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=add\n+  r3 = f32[32,256,3072]{2,1,0} all-reduce(p3), channel_id=4, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=add\n+  r4 = f32[16,3072,256]{2,1,0} all-reduce(p4), channel_id=5, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=add\n+  r5 = f32[66,1024]{1,0} all-reduce(p5), channel_id=6, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=add\n+  r6 = f32[2,32768,256]{2,1,0} all-reduce(p6), channel_id=7, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=add\n+  ROOT tuple = tuple(r0, r1, r2, r3, r4, r5, r6)\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+  EXPECT_THAT(RunCombiner(module.get()), absl_testing::IsOkAndHolds(true));\n+\n+  constexpr absl::string_view kExpected = R\"('\n+    // CHECK:      ENTRY\n+    // CHECK-DAG:  %[[P0:.*]] = parameter(0)\n+    // CHECK-DAG:  %[[P1:.*]] = parameter(1)\n+    // CHECK-DAG:  %[[P2:.*]] = parameter(2)\n+    // CHECK-DAG:  %[[P3:.*]] = parameter(3)\n+    // CHECK-DAG:  %[[P4:.*]] = parameter(4)\n+    // CHECK-DAG:  %[[P5:.*]] = parameter(5)\n+    // CHECK-DAG:  %[[P6:.*]] = parameter(6)\n+    // CHECK:      all-reduce(%[[P0]], %[[P1]], %[[P2]], %[[P3]], %[[P4]], %[[P5]], %[[P6]])\n+    // CHECK-SAME: channel_id=1,\n+    // CHECK-SAME: replica_groups={\n+    // CHECK-SAME:   {0,1,2,3}\n+    // CHECK-SAME: },\n+    // CHECK-SAME: use_global_device_ids=true,\n+    // CHECK-SAME: to_apply=%add\n+  )\";\n+\n+  EXPECT_TRUE(*RunFileCheck(\n+      module->ToString(HloPrintOptions()\n+                           .set_print_operand_index_annotation_interval(false)\n+                           .set_print_operand_shape(false)\n+                           .set_print_result_shape(false)),\n+      kExpected));\n+}\n+\n+}  // namespace\n+}  // namespace xla::cpu"
        },
        {
            "sha": "aa1b49989cf32ba7eb8ab60280dd94c083127b65",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,3 +1,4 @@\n+load(\"@local_config_cuda//cuda:build_defs.bzl\", \"cuda_library\")\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\", \"xla_internal\")\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n@@ -861,6 +862,64 @@ xla_cc_test(\n     ],\n )\n \n+cuda_library(\n+    name = \"raft_select_k_exec\",\n+    srcs = [\"raft_select_k_exec.cc\"],\n+    hdrs = [\"raft_select_k_exec.h\"],\n+    copts = [\n+        \"-fexceptions\",\n+        \"-DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE\",\n+    ],\n+    tags = [\"cuda-only\"],\n+    textual_hdrs = [\"raft_vectorized_bf16.h\"],\n+    deps = [\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:scratch_allocator\",\n+        \"//xla/stream_executor:stream\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@raft//:raft_matrix\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"raft_select_k_exec_test\",\n+    srcs = [\"raft_select_k_exec_test.cc\"],\n+    backends = [\n+        \"gpu\",\n+    ],\n+    tags = [\n+        \"cuda-only\",\n+    ],\n+    deps = [\n+        \":raft_select_k_exec\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/random\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ],\n+)\n+\n cc_library(\n     name = \"memset_thunk\",\n     srcs = [\"memset_thunk.cc\"],"
        },
        {
            "sha": "101755fd9abbc3bdf6c0e6be90e0396cb3feb62e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -121,7 +121,9 @@ bool IsAtLeastCuda12300(const se::StreamExecutor* stream_executor) {\n       &device_description.gpu_compute_capability());\n   if (cuda_cc != nullptr) {\n     if (device_description.driver_version() >=\n-        stream_executor::SemanticVersion(12, 3, 0)) {\n+            stream_executor::SemanticVersion(12, 3, 0) &&\n+        device_description.runtime_version() >=\n+            stream_executor::SemanticVersion(12, 3, 0)) {\n       return true;\n     }\n   }\n@@ -135,7 +137,9 @@ bool IsAtLeastCuda12900(const se::StreamExecutor* stream_executor) {\n       &device_description.gpu_compute_capability());\n   if (cuda_cc != nullptr) {\n     if (device_description.driver_version() >=\n-        stream_executor::SemanticVersion(12, 9, 0)) {\n+            stream_executor::SemanticVersion(12, 9, 0) &&\n+        device_description.runtime_version() >=\n+            stream_executor::SemanticVersion(12, 9, 0)) {\n       return true;\n     }\n   }"
        },
        {
            "sha": "88f714ecd285a894739a037dbab7e3da48d32f4c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_select_k_exec.cc",
            "status": "added",
            "additions": 316,
            "deletions": 0,
            "changes": 316,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,316 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/raft_select_k_exec.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <exception>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+\n+#include \"absl/base/optimization.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"third_party/gpus/cuda/include/cuda_bf16.h\"\n+#include \"raft/core/device_mdspan.hpp\"\n+#include \"raft/core/mdspan_types.hpp\"\n+#include \"raft/core/resource/cuda_stream.hpp\"\n+#include \"raft/core/resource/device_memory_resource.hpp\"\n+#include \"raft/core/resources.hpp\"\n+#include \"raft/matrix/select_k.cuh\"\n+#include \"raft/matrix/select_k_types.hpp\"\n+// NOTE: This include is required for vectorized BF16 GPU runtime support.\n+// It will no longer be needed after upgrading to raft v25.10.00.\n+#include \"xla/backends/gpu/runtime/raft_vectorized_bf16.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/scratch_allocator.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+namespace se = ::stream_executor;\n+using raft::matrix::SelectAlgo;\n+\n+namespace {\n+\n+// Simple RAII wrapper to manage temporary device memory allocations\n+class OwningScratchAllocator {\n+ public:\n+  OwningScratchAllocator(int device_ordinal,\n+                         se::DeviceMemoryAllocator* allocator)\n+      : device_ordinal_(device_ordinal), allocator_(allocator) {}\n+\n+  OwningScratchAllocator(OwningScratchAllocator&&) = default;\n+  OwningScratchAllocator& operator=(OwningScratchAllocator&&) = default;\n+\n+  // Allocate memory and track ownership\n+  absl::StatusOr<se::DeviceMemory<uint8_t>> AllocateBytes(int64_t byte_size) {\n+    TF_ASSIGN_OR_RETURN(se::OwningDeviceMemory buffer,\n+                        allocator_->Allocate(device_ordinal_, byte_size,\n+                                             /*retry_on_failure=*/false));\n+\n+    se::DeviceMemory<uint8_t> res = *buffer;\n+    void* raw_ptr = res.opaque();\n+    buffers_.emplace(raw_ptr, std::move(buffer));\n+    return res;\n+  }\n+\n+  // Deallocate tracked memory; safe no-op if pointer not found\n+  absl::Status DeallocateBytes(void* ptr) {\n+    auto it = buffers_.find(ptr);\n+    if (it != buffers_.end()) {\n+      buffers_.erase(it);  // RAII frees memory\n+      return absl::OkStatus();\n+    }\n+    return absl::NotFoundError(\"Pointer not found\");\n+  }\n+\n+ private:\n+  int device_ordinal_;\n+  se::DeviceMemoryAllocator* allocator_;\n+  // key = raw device pointer, value = owning memory object\n+  absl::flat_hash_map<void*, se::OwningDeviceMemory> buffers_;\n+};\n+\n+// Custom RMM memory resource backed by StreamExecutor allocator\n+class XlaDeviceMemoryResource : public rmm::mr::device_memory_resource {\n+ public:\n+  XlaDeviceMemoryResource(int device_ordinal,\n+                          se::DeviceMemoryAllocator* allocator)\n+      : scratch_allocator_(device_ordinal, allocator) {}\n+\n+ protected:\n+  void* do_allocate(std::size_t bytes, rmm::cuda_stream_view stream) override {\n+    auto mem = scratch_allocator_.AllocateBytes(bytes);\n+    if (!mem.ok()) {\n+      // RMM expects exceptions\n+      throw rmm::bad_alloc(std::string(mem.status().ToString()));\n+    }\n+    return mem->opaque();\n+  }\n+\n+  void do_deallocate(void* ptr, std::size_t bytes,\n+                     rmm::cuda_stream_view stream) override {\n+    auto status = scratch_allocator_.DeallocateBytes(ptr);\n+    if (!status.ok()) {\n+      // do_deallocate should be noexcept. Donâ€™t throw; just log.\n+      LOG(ERROR) << \"Scratch Deallocation failed: \" << status;\n+    }\n+  }\n+\n+ private:\n+  OwningScratchAllocator scratch_allocator_;\n+};\n+\n+// RAII wrapper for RAFT resources bound to a CUDA stream\n+struct RaftStreamResource : public se::Stream::Resource {\n+  raft::resources res;\n+\n+  // Factory to create a RaftStreamResource tied to a CUDA stream.\n+  // Sets up `raft::resources` with a custom XlaDeviceMemoryResource\n+  // using the given allocator and binds it to the provided stream.\n+  //\n+  // Args:\n+  //   device_ordinal: Device index.\n+  //   allocator: StreamExecutor memory allocator.\n+  //   cuda_stream: CUDA stream to bind.\n+  // Returns:\n+  //   Unique pointer to an initialized RaftStreamResource.\n+  static std::unique_ptr<RaftStreamResource> Create(\n+      int device_ordinal, se::DeviceMemoryAllocator* allocator,\n+      cudaStream_t cuda_stream) {\n+    // Assign our custom AllocatorForRaft for this device\n+    auto handle = std::make_unique<RaftStreamResource>();\n+    raft::resource::set_workspace_resource(\n+        handle->res,\n+        std::make_shared<XlaDeviceMemoryResource>(device_ordinal, allocator));\n+    // Set Cuda Stream\n+    raft::resource::set_cuda_stream(handle->res,\n+                                    rmm::cuda_stream_view{cuda_stream});\n+    return handle;\n+  }\n+};\n+\n+// ============================================================================\n+// choose_select_k_algorithm\n+//\n+// Purpose:\n+//   Heuristic-based selection of the optimal \"select k\" algorithm depending on\n+//   problem shape (rows, cols, k). The decision is based on benchmark data.\n+//\n+// How the heuristic is generated:\n+//\n+//   1. Build the benchmark module:\n+//        raft/cpp/bench/prims/matrix\n+//\n+//   2. Collect performance data by running microbenchmarks:\n+//\n+//        From the RAFT project root:\n+//          ./cpp/build/bench/prims/MATRIX_BENCH \\\n+//            --benchmark_filter=Select \\\n+//            --benchmark_out_format=json \\\n+//            --benchmark_out=select_k_times.json\n+//\n+//        Output:\n+//          - Benchmark results are written to `select_k_times.json`\n+//\n+//   3. Generate the heuristic using the provided notebook:\n+//\n+//        ./cpp/scripts/heuristics/select_k/generate_heuristic.ipynb\n+//\n+//        The notebook consumes `select_k_times.json`, analyzes performance\n+//        trade-offs, and produces the decision tree implemented here.\n+//\n+// Notes:\n+//   - To generate performance data for BFloat16,\n+//     modify cpp/bench/prims/matrix/select_k.cu  and register nv_bfloat16 type\n+//     using SELECTION_REGISTER mactos.\n+// ============================================================================\n+\n+template <typename T>\n+SelectAlgo choose_select_k_algorithm(uint32_t rows, uint32_t cols, uint32_t k) {\n+  static_assert(sizeof(T) == 0,\n+                \"choose_select_k_algorithm<T>: Unsupported type\");\n+  ABSL_UNREACHABLE();\n+}\n+\n+template <>\n+SelectAlgo choose_select_k_algorithm<float>(uint32_t rows, uint32_t cols,\n+                                            uint32_t k) {\n+  if (k > 256) {\n+    return SelectAlgo::kRadix11bits;\n+  } else if (k > 3) {\n+    if (cols > 55000) {\n+      return SelectAlgo::kWarpDistributedShm;\n+    } else {\n+      if (cols > 5250) {\n+        if (k > 192) {\n+          return SelectAlgo::kRadix11bits;\n+        } else {\n+          return SelectAlgo::kWarpDistributedShm;\n+        }\n+      } else {\n+        return SelectAlgo::kWarpDistributedShm;\n+      }\n+    }\n+  } else {\n+    return SelectAlgo::kWarpImmediate;\n+  }\n+}\n+\n+template <>\n+SelectAlgo choose_select_k_algorithm<nv_bfloat16>(uint32_t rows, uint32_t cols,\n+                                                  uint32_t k) {\n+  if (k > 256) {\n+    return SelectAlgo::kRadix11bits;\n+  } else if (k > 3) {\n+    if (cols > 5250 && k > 192) {\n+      return SelectAlgo::kRadix11bits;\n+    } else {\n+      return SelectAlgo::kWarpDistributedShm;\n+    }\n+  } else {\n+    return SelectAlgo::kWarpImmediate;\n+  }\n+}\n+\n+}  // namespace\n+\n+// Host-side entry point for raft select_k\n+template <typename T>\n+absl::Status raft_select_k_exec(\n+    int device_ordinal, se::DeviceMemoryAllocator* allocator,\n+    se::Stream* stream, se::DeviceMemoryBase data_in,\n+    se::DeviceMemoryBase data_out, se::DeviceMemoryBase indices_out,\n+    std::uint32_t batch, std::uint32_t n, std::uint32_t k) {\n+  // Validate input sizes\n+  DCHECK_EQ(data_in.size(), static_cast<uint64_t>(batch) * n * sizeof(T));\n+  DCHECK_EQ(data_out.size(), static_cast<uint64_t>(batch) * k * sizeof(T));\n+  DCHECK_EQ(indices_out.size(),\n+            static_cast<uint64_t>(batch) * k * sizeof(uint32_t));\n+  DCHECK_GE(n, k);\n+\n+  // Pick the most suitable algorithm\n+  SelectAlgo algo = choose_select_k_algorithm<T>(batch, n, k);\n+  VLOG(3) << \"raft_select_k_exec: \"\n+          << \"device_ordinal: \" << device_ordinal << \", \"\n+          << \"data_in: \" << data_in.opaque() << \" (\" << data_in.size() << \"B)\"\n+          << \", data_out: \" << data_out.opaque() << \" (\" << data_out.size()\n+          << \"B)\"\n+          << \", indices_out: \" << indices_out.opaque() << \" (\"\n+          << indices_out.size() << \"B)\"\n+          << \", batch: \" << batch << \", n: \" << n << \", k: \" << k\n+          << \", algo: \" << algo;\n+\n+  // Retrieve or create RAFT resource for this stream\n+  cudaStream_t cuda_stream =\n+      reinterpret_cast<cudaStream_t>(stream->platform_specific_handle().stream);\n+  DCHECK(cuda_stream != nullptr);\n+  RaftStreamResource* resContainer =\n+      stream->GetOrCreateResource<RaftStreamResource>(\n+          [device_ordinal, allocator, cuda_stream] {\n+            return RaftStreamResource::Create(device_ordinal, allocator,\n+                                              cuda_stream);\n+          });\n+\n+  try {\n+    // Wrap raw device pointers in RAFT matrix views\n+    auto input_view =\n+        raft::make_device_matrix_view<const T, uint32_t, raft::row_major>(\n+            reinterpret_cast<const T*>(data_in.opaque()), batch, n);\n+\n+    auto output_values_view =\n+        raft::make_device_matrix_view<T, uint32_t, raft::row_major>(\n+            reinterpret_cast<T*>(data_out.opaque()), batch, k);\n+\n+    auto output_indices_view =\n+        raft::make_device_matrix_view<uint32_t, uint32_t, raft::row_major>(\n+            reinterpret_cast<uint32_t*>(indices_out.opaque()), batch, k);\n+\n+    // Call RAFT select_k kernel\n+    raft::matrix::select_k<T, uint32_t>(\n+        resContainer->res, input_view,\n+        std::nullopt,  // d_input_indices can be omitted\n+        output_values_view, output_indices_view,\n+        /*select_min=*/false,\n+        /*sorted=*/true,\n+        /*algo=*/algo);\n+\n+    return absl::OkStatus();\n+  } catch (const std::exception& e) {\n+    return absl::InternalError(absl::StrCat(\"select_k failed: \", e.what()));\n+  } catch (...) {\n+    return absl::InternalError(\"select_k failed with unknown exception\");\n+  }\n+}\n+\n+// Explicit instantiations for supported types\n+template absl::Status raft_select_k_exec<float>(\n+    int, se::DeviceMemoryAllocator*, se::Stream*, se::DeviceMemoryBase,\n+    se::DeviceMemoryBase, se::DeviceMemoryBase, std::uint32_t, std::uint32_t,\n+    std::uint32_t);\n+\n+template absl::Status raft_select_k_exec<nv_bfloat16>(\n+    int, se::DeviceMemoryAllocator*, se::Stream*, se::DeviceMemoryBase,\n+    se::DeviceMemoryBase, se::DeviceMemoryBase, std::uint32_t, std::uint32_t,\n+    std::uint32_t);\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "d0831ec3a5f5f0d28273e1ed582949bd37cc0b4a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_select_k_exec.h",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,54 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_RAFT_SELECT_K_EXEC_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_RAFT_SELECT_K_EXEC_H_\n+\n+#include <cstdint>\n+\n+#include \"absl/status/status.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/stream.h\"\n+\n+namespace xla::gpu {\n+\n+// Launches a RAFT Top-K selection on GPU for a batch of matrices.\n+//\n+// Args:\n+//   device_ordinal: GPU device index to run the operation on.\n+//   allocator: StreamExecutor memory allocator for device buffers.\n+//   stream: StreamExecutor stream for the GPU operations.\n+//   data_in: Device memory containing input matrices (batch Ã— n).\n+//   data_out: Device memory to store top-k values (batch Ã— k).\n+//   indices_out: Device memory to store top-k indices (batch Ã— k).\n+//   batch: Number of rows (matrices) in the batch.\n+//   n: Number of columns (elements per row) in input matrices.\n+//   k: Number of top elements to select per row.\n+//\n+// Returns:\n+//   absl::Status indicating success or failure of the operation.\n+template <typename T>\n+absl::Status raft_select_k_exec(\n+    int device_ordinal, ::stream_executor::DeviceMemoryAllocator* allocator,\n+    ::stream_executor::Stream* stream,\n+    ::stream_executor::DeviceMemoryBase data_in,\n+    ::stream_executor::DeviceMemoryBase data_out,\n+    ::stream_executor::DeviceMemoryBase indices_out, std::uint32_t batch,\n+    std::uint32_t n, std::uint32_t k);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_RAFT_SELECT_K_EXEC_H_"
        },
        {
            "sha": "2f86c42f18c105ebd49d6027e8b272fcaeef22ed",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_select_k_exec_test.cc",
            "status": "added",
            "additions": 155,
            "deletions": 0,
            "changes": 155,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec_test.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,155 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/raft_select_k_exec.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <cstring>\n+#include <functional>\n+#include <memory>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/base/casts.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/random/random.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/ascii.h\"\n+#include \"absl/types/span.h\"\n+#include \"third_party/gpus/cuda/include/cuda_bf16.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+// Returns the first GPU StreamExecutor\n+se::StreamExecutor* GpuExecutor() {\n+  auto name =\n+      absl::AsciiStrToUpper(PlatformUtil::CanonicalPlatformName(\"gpu\").value());\n+  auto* platform = se::PlatformManager::PlatformWithName(name).value();\n+  CHECK(platform != nullptr);\n+  CHECK_OK(platform->ExecutorForDevice(0));\n+  return platform->ExecutorForDevice(0).value();\n+}\n+\n+// Trait: map Data type to Mask type and a default kStartBits\n+template <typename T>\n+struct MaskFor;\n+\n+template <>\n+struct MaskFor<float> {\n+  using type = uint32_t;\n+  static constexpr type kStartBits = 0x3C000000;  // float32: 1/128\n+};\n+\n+template <>\n+struct MaskFor<nv_bfloat16> {\n+  using type = uint16_t;\n+  static constexpr type kStartBits = 0x3C00;  // bfloat16: 1/128\n+};\n+\n+// Fills vector with unique values using bit patterns starting from kStartBits\n+template <typename T>\n+void append_unique_numbers(size_t count, std::vector<T>& arr) {\n+  using Traits = MaskFor<T>;\n+  using MaskT = typename Traits::type;\n+  MaskT bits = Traits::kStartBits;\n+\n+  for (size_t i = 0; i < count; ++i, ++bits) {\n+    T val = absl::bit_cast<T>(bits);\n+    arr.push_back(val);\n+  }\n+}\n+\n+}  // namespace\n+\n+// Template test function for raft select_k\n+template <typename T>\n+void RunSelectKTest() {\n+  se::StreamExecutor* stream_executor = GpuExecutor();\n+  TF_ASSERT_OK_AND_ASSIGN(auto stream, stream_executor->CreateStream());\n+  int device_ordinal = stream_executor->device_ordinal();\n+  se::StreamExecutorMemoryAllocator allocator(stream_executor);\n+\n+  std::uint32_t batch = 4;\n+  std::uint32_t n = 4096;\n+  std::uint32_t k = 32;\n+  absl::BitGen gen;\n+\n+  // Prepare unique values for Top-K testing\n+  std::vector<T> topk;\n+  topk.reserve(n);\n+  append_unique_numbers<T>(n, topk);\n+\n+  // Populate input matrix (batch x n) with shuffled topk values\n+  std::vector<T> h_data_in(batch * n);\n+  for (int j = 0; j < batch; ++j) {\n+    std::shuffle(topk.begin(), topk.end(), gen);\n+    std::copy(topk.begin(), topk.end(), h_data_in.begin() + j * n);\n+  }\n+\n+  // Compute golden Top-K values for verification\n+  std::sort(topk.begin(), topk.end(), std::greater<T>());\n+  topk.resize(k);\n+\n+  // Allocate device memory for input and outputs\n+  se::DeviceMemory<T> d_data_in =\n+      stream_executor->AllocateArray<T>(batch * n, 0);\n+  se::DeviceMemory<T> d_data_out =\n+      stream_executor->AllocateArray<T>(batch * k, 0);\n+  se::DeviceMemory<uint32_t> d_indices_out =\n+      stream_executor->AllocateArray<uint32_t>(batch * k, 0);\n+\n+  // Copy host to device\n+  TF_ASSERT_OK(stream->MemcpyH2D(absl::Span<const T>(h_data_in), &d_data_in));\n+\n+  // Run raft select_k\n+  TF_ASSERT_OK(raft_select_k_exec<T>(device_ordinal, &allocator, stream.get(),\n+                                     d_data_in, d_data_out, d_indices_out,\n+                                     batch, n, k));\n+\n+  // Copy results back to host\n+  std::vector<T> h_data_out(batch * k);\n+  std::vector<uint32_t> h_indices_out(batch * k);\n+  TF_ASSERT_OK(stream->MemcpyD2H(d_data_out, absl::Span<T>(h_data_out)));\n+  TF_ASSERT_OK(\n+      stream->MemcpyD2H(d_indices_out, absl::Span<uint32_t>(h_indices_out)));\n+  TF_ASSERT_OK(stream->BlockHostUntilDone());\n+\n+  // Verify Top-K values and corresponding indices\n+  for (int j = 0; j < batch; ++j) {\n+    for (int i = 0; i < k; ++i) {\n+      EXPECT_EQ(h_data_out[j * k + i], topk[i]) << \"batch=\" << j << \" i=\" << i;\n+      auto idx = h_indices_out[j * k + i];\n+      EXPECT_EQ(h_data_in[j * n + idx], topk[i]) << \"batch=\" << j << \" i=\" << i;\n+    }\n+  }\n+}\n+\n+TEST(RaftSelectKExecTest, SelectKFloat) { RunSelectKTest<float>(); }\n+\n+TEST(RaftSelectKExecTest, SelectKBFloat16) { RunSelectKTest<nv_bfloat16>(); }\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "13920c71beef38d9a18ebc82ac4c8267ed515188",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_vectorized_bf16.h",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_vectorized_bf16.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_vectorized_bf16.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_vectorized_bf16.h?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -0,0 +1,56 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_RAFT_VECTORIZED_BF16_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_RAFT_VECTORIZED_BF16_H_\n+\n+#pragma once\n+#include \"third_party/gpus/cuda/include/cuda_bf16.h\"\n+#include \"raft/util/vectorized.cuh\"\n+\n+namespace raft {\n+\n+template <>\n+struct IOType<__nv_bfloat16, 1> {\n+  typedef __nv_bfloat16 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat16, 2> {\n+  typedef __nv_bfloat162 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat16, 4> {\n+  typedef uint2 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat16, 8> {\n+  typedef uint4 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat162, 1> {\n+  typedef __nv_bfloat162 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat162, 2> {\n+  typedef uint2 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat162, 4> {\n+  typedef uint4 Type;\n+};\n+\n+}  // namespace raft\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_RAFT_VECTORIZED_BF16_H_"
        },
        {
            "sha": "57144b9ceb5448add49034d8d6e30780e3a31db1",
            "filename": "third_party/xla/xla/pjrt/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -163,7 +163,6 @@ cc_library(\n         \"//xla/hlo/builder:xla_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/ir:hlo_module_group\",\n-        \"//xla/pjrt:abstract_tracked_device_buffer\",\n         \"//xla/pjrt:async_work_runner\",\n         \"//xla/pjrt:common_pjrt_client\",\n         \"//xla/pjrt:device_event\",\n@@ -182,6 +181,7 @@ cc_library(\n         \"//xla/pjrt:thread_pool_async_work_runner\",\n         \"//xla/pjrt:transpose\",\n         \"//xla/pjrt:utils\",\n+        \"//xla/pjrt/dump\",\n         \"//xla/pjrt/plugin/xla_cpu:cpu_client_options\",\n         \"//xla/pjrt/plugin/xla_cpu:cpu_execute_options\",\n         \"//xla/pjrt/plugin/xla_cpu:cpu_topology\","
        },
        {
            "sha": "e0a019040437acc53ece9a98489b1f046e24ee58",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -74,6 +74,7 @@ limitations under the License.\n #include \"xla/pjrt/cpu/raw_buffer.h\"\n #include \"xla/pjrt/cpu/tracked_cpu_device_buffer.h\"\n #include \"xla/pjrt/device_event.h\"\n+#include \"xla/pjrt/dump/dump.h\"\n #include \"xla/pjrt/host_callback.h\"\n #include \"xla/pjrt/host_memory_spaces.h\"\n #include \"xla/pjrt/host_to_device_transfer_manager.h\"\n@@ -589,6 +590,20 @@ static absl::StatusOr<std::unique_ptr<xla::Executable>> CompileAheadOfTime(\n \n absl::StatusOr<std::unique_ptr<PjRtLoadedExecutable>>\n PjRtCpuClient::CompileAndLoad(mlir::ModuleOp module, CompileOptions options) {\n+  // Dump compile inputs to the specified path if populated.\n+  if (options.executable_build_options.has_debug_options()) {\n+    std::string dump_path =\n+        options.executable_build_options.debug_options().xla_dump_to();\n+    if (!dump_path.empty()) {\n+      LOG(INFO) << \"Dumping compile inputs to \" << dump_path;\n+      auto dump_status =\n+          pjrt::DumpCompileInputs(dump_path, options, module, topology_);\n+      if (!dump_status.ok()) {\n+        LOG(WARNING) << \"Failed to dump compile inputs: \" << dump_status;\n+      }\n+    }\n+  }\n+\n   XlaComputation xla_computation;\n   ExecutableBuildOptions& exec_build_options = options.executable_build_options;\n   TF_RETURN_IF_ERROR(MlirToXlaComputation("
        },
        {
            "sha": "c7b2b097a61da88ff3f2260ec42831f4405bf897",
            "filename": "third_party/xla/xla/pjrt/dump/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fpjrt%2Fdump%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fpjrt%2Fdump%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdump%2FBUILD?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,6 +1,6 @@\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n-load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_libtpu_portable\")\n+load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_libtpu_portable\", \"get_compatible_with_portable\")\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -12,7 +12,7 @@ cc_library(\n     name = \"dump\",\n     srcs = [\"dump.cc\"],\n     hdrs = [\"dump.h\"],\n-    compatible_with = get_compatible_with_libtpu_portable(),\n+    compatible_with = (get_compatible_with_libtpu_portable() + get_compatible_with_portable()),\n     deps = [\n         \":mlir\",\n         \"//xla/pjrt:pjrt_compiler\",\n@@ -62,7 +62,7 @@ cc_library(\n     name = \"mlir\",\n     srcs = [\"mlir.cc\"],\n     hdrs = [\"mlir.h\"],\n-    compatible_with = get_compatible_with_libtpu_portable(),\n+    compatible_with = (get_compatible_with_libtpu_portable() + get_compatible_with_portable()),\n     deps = [\n         \"//xla/tsl/platform:env\",\n         \"@com_google_absl//absl/status\",\n@@ -74,7 +74,7 @@ cc_library(\n xla_cc_test(\n     name = \"mlir_test\",\n     srcs = [\"mlir_test.cc\"],\n-    compatible_with = get_compatible_with_libtpu_portable(),\n+    compatible_with = (get_compatible_with_libtpu_portable() + get_compatible_with_portable()),\n     deps = [\n         \":mlir\",\n         \"//xla/mlir_hlo\","
        },
        {
            "sha": "3afcf1b01624f0fa4ad5780c23c96e7c9a319c11",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -238,6 +238,7 @@ cc_library(\n         \"//xla/backends/cpu/runtime:thunk_proto_serdes\",\n         \"//xla/backends/cpu/transforms:dot_library_rewriter\",\n         \"//xla/backends/cpu/transforms:xnn_graph_fusion\",\n+        \"//xla/backends/cpu/transforms/collectives:all_reduce_combiner\",\n         \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_ordering\",\n         \"//xla/hlo/analysis:indexed_array_analysis\","
        },
        {
            "sha": "a5a862f14c0ee9be6335b7054d540a1f29a38f8f",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n #include <cstring>\n #include <functional>\n+#include <limits>\n #include <memory>\n #include <optional>\n #include <stack>\n@@ -95,6 +96,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/backends/cpu/runtime/thunk.pb.h\"\n #include \"xla/backends/cpu/runtime/thunk_proto_serdes.h\"\n+#include \"xla/backends/cpu/transforms/collectives/all_reduce_combiner.h\"\n #include \"xla/backends/cpu/transforms/dot_library_rewriter.h\"\n #include \"xla/backends/cpu/transforms/xnn_graph_fusion.h\"\n #include \"xla/backends/cpu/xnn_support.h\"\n@@ -927,6 +929,12 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n     pipeline.AddPass<CallInliner>(/*single_call_site=*/true);\n   }\n \n+  // Combine collective operations to maximize network bandwidth usage.\n+  constexpr int64_t kCombineBytes = std::numeric_limits<int64_t>::max();\n+  constexpr int64_t kCombineCount = 256;\n+  pipeline.AddPass<CpuAllReduceCombiner>(kCombineBytes, kCombineCount);\n+  pipeline.AddPass<TupleSimplifier>();\n+\n   // The LayoutAssignment pass may leave behind kCopy instructions which are\n   // duplicate or NOPs, so remove them with algebraic simplification and CSE.\n   // Run this to a fixed point."
        }
    ],
    "stats": {
        "total": 1028,
        "additions": 960,
        "deletions": 68
    }
}