{
    "author": "tensorflower-gardener",
    "message": "Merge pull request #99852 from Intel-tensorflow:nhatle/fix_fused_bn_bug_tf_serving\n\nPiperOrigin-RevId: 817533866",
    "sha": "f71e821dce70b9b26bc72e695f5fbce775b7fc42",
    "files": [
        {
            "sha": "fe4378d1e1984e0a6d2c7736ef37deb9db365327",
            "filename": "tensorflow/core/kernels/mkl/mkl_fused_batch_norm_op.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f71e821dce70b9b26bc72e695f5fbce775b7fc42/tensorflow%2Fcore%2Fkernels%2Fmkl%2Fmkl_fused_batch_norm_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f71e821dce70b9b26bc72e695f5fbce775b7fc42/tensorflow%2Fcore%2Fkernels%2Fmkl%2Fmkl_fused_batch_norm_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmkl%2Fmkl_fused_batch_norm_op.cc?ref=f71e821dce70b9b26bc72e695f5fbce775b7fc42",
            "patch": "@@ -844,9 +844,6 @@ class MklFusedBatchNormOp : public OpKernel {\n     OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                 absl::InvalidArgumentError(\"Invalid data format\"));\n     OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n-    depth_ = 0;\n-    mean_values_ = nullptr;\n-    variance_values_ = nullptr;\n \n     if (!is_batch_norm_ex) {\n       activation_mode_ = FusedBNActivationMode::kIdentity;\n@@ -868,6 +865,9 @@ class MklFusedBatchNormOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     try {\n+      U* mean_values = nullptr;\n+      U* variance_values = nullptr;\n+      size_t depth = 0;\n       const size_t kSrcIndex = 0;       // index of src input tensor\n       const size_t kScaleIndex = 1;     // index of scale tensor\n       const size_t kShiftIndex = 2;     // index of shift tensor\n@@ -963,9 +963,9 @@ class MklFusedBatchNormOp : public OpKernel {\n       }\n \n       if (dnn_shape_src.IsMklTensor())\n-        depth_ = dnn_shape_src.DimSize(MklDnnDims::Dim_C);\n+        depth = dnn_shape_src.DimSize(MklDnnDims::Dim_C);\n       else\n-        ExtractParams(context);\n+        ExtractParams(context, depth);\n \n       // Index of output tensor(diff_src).\n       const size_t kDstIndex = 0;\n@@ -1014,7 +1014,7 @@ class MklFusedBatchNormOp : public OpKernel {\n       auto dst_md = memory::desc(src_dims, MklDnnType<T>(), dnn_fmt);\n #endif  // ENABLE_ONEDNN_V3\n \n-      MklBatchNormFwdParams fwdParams(src_dims, depth_, epsilon_, is_training_,\n+      MklBatchNormFwdParams fwdParams(src_dims, depth, epsilon_, is_training_,\n #ifndef ENABLE_ONEDNN_V3\n                                       tensor_format_, src_md, activation_mode_);\n #else\n@@ -1059,43 +1059,44 @@ class MklFusedBatchNormOp : public OpKernel {\n       }\n \n       if (is_training_)\n-        SetMeanVariance(*batch_mean_tensor, *batch_variance_tensor);\n+        SetMeanVariance(*batch_mean_tensor, *batch_variance_tensor,\n+                        &mean_values, &variance_values);\n       else\n-        SetMeanVariance(est_mean_tensor, est_variance_tensor);\n+        SetMeanVariance(est_mean_tensor, est_variance_tensor, &mean_values,\n+                        &variance_values);\n \n #ifndef ENABLE_ONEDNN_V3\n       // oneDNN packs scale & shift as a combined array in float32 type\n       // <scale>...<scale><shift>...<shift>\n-      scale_shift.AllocateBuffer(2 * depth_ * sizeof(U));\n+      scale_shift.AllocateBuffer(2 * depth * sizeof(U));\n       U* scale_shift_data =\n           reinterpret_cast<U*>(scale_shift.GetAllocatedBuffer());\n       const U* scale_tf = scale_tensor.flat<U>().data();\n       const U* shift_tf = shift_tensor.flat<U>().data();\n \n-      std::memcpy(scale_shift_data, scale_tf, depth_ * sizeof(U));\n-      std::memcpy(scale_shift_data + depth_, shift_tf, depth_ * sizeof(U));\n+      std::memcpy(scale_shift_data, scale_tf, depth * sizeof(U));\n+      std::memcpy(scale_shift_data + depth, shift_tf, depth * sizeof(U));\n #else\n       // oneDNN v3.x requires scale and shift as separate float32 arrays\n-      scale.AllocateBuffer(depth_ * sizeof(U));\n+      scale.AllocateBuffer(depth * sizeof(U));\n       U* scale_data = reinterpret_cast<U*>(scale.GetAllocatedBuffer());\n-      shift.AllocateBuffer(depth_ * sizeof(U));\n+      shift.AllocateBuffer(depth * sizeof(U));\n       U* shift_data = reinterpret_cast<U*>(shift.GetAllocatedBuffer());\n       const U* scale_tf = scale_tensor.flat<U>().data();\n       const U* shift_tf = shift_tensor.flat<U>().data();\n-      std::memcpy(scale_data, scale_tf, depth_ * sizeof(U));\n-      std::memcpy(shift_data, shift_tf, depth_ * sizeof(U));\n+      std::memcpy(scale_data, scale_tf, depth * sizeof(U));\n+      std::memcpy(shift_data, shift_tf, depth * sizeof(U));\n #endif  // !ENABLE_ONEDNN_V3\n \n       char* saved_mean_data_tf =\n           reinterpret_cast<char*>(saved_mean_tensor->flat<U>().data());\n-      std::memcpy(saved_mean_data_tf, reinterpret_cast<char*>(mean_values_),\n-                  depth_ * sizeof(U));\n+      std::memcpy(saved_mean_data_tf, reinterpret_cast<char*>(mean_values),\n+                  depth * sizeof(U));\n \n       char* saved_variance_data_tf =\n           reinterpret_cast<char*>(saved_variance_tensor->flat<U>().data());\n       std::memcpy(saved_variance_data_tf,\n-                  reinterpret_cast<char*>(variance_values_),\n-                  depth_ * sizeof(U));\n+                  reinterpret_cast<char*>(variance_values), depth * sizeof(U));\n \n       // Check if reorder is needed for src.\n       const T* src_data = nullptr;\n@@ -1161,14 +1162,14 @@ class MklFusedBatchNormOp : public OpKernel {\n       auto est_variance_data = est_variance_tensor.flat<U>().data();\n       if (is_training_) {\n         if (exponential_avg_factor_ == U(1.0)) {\n-          for (int k = 0; k < depth_; k++) {\n+          for (int k = 0; k < depth; k++) {\n             batch_mean_data[k] = mean_data[k];\n             batch_variance_data[k] =\n                 static_cast<U>(adjust_factor) * variance_data[k];\n           }\n         } else {\n           U one_minus_factor = U(1.0) - exponential_avg_factor_;\n-          for (int k = 0; k < depth_; k++) {\n+          for (int k = 0; k < depth; k++) {\n             batch_mean_data[k] = one_minus_factor * est_mean_data[k] +\n                                  exponential_avg_factor_ * mean_data[k];\n             batch_variance_data[k] = one_minus_factor * est_variance_data[k] +\n@@ -1178,8 +1179,8 @@ class MklFusedBatchNormOp : public OpKernel {\n           }\n         }\n       } else {\n-        std::memcpy(batch_mean_data, mean_data, depth_ * sizeof(U));\n-        std::memcpy(batch_variance_data, variance_data, depth_ * sizeof(U));\n+        std::memcpy(batch_mean_data, mean_data, depth * sizeof(U));\n+        std::memcpy(batch_variance_data, variance_data, depth * sizeof(U));\n       }\n     } catch (dnnl::error& e) {\n       string error_msg = \"Status: \" + std::to_string(e.status) +\n@@ -1196,20 +1197,18 @@ class MklFusedBatchNormOp : public OpKernel {\n   U exponential_avg_factor_;\n   TensorFormat tensor_format_;\n   bool is_training_;\n-  U* mean_values_;\n-  U* variance_values_;\n-  size_t depth_;  // Batch normalization is performed for per channel.\n   FusedBNActivationMode activation_mode_;\n   engine cpu_engine_ = engine(engine::kind::cpu, 0);\n \n-  void ExtractParams(OpKernelContext* context) {\n+  void ExtractParams(OpKernelContext* context, size_t& depth) {\n     const Tensor& input = MklGetInput(context, 0);\n-    depth_ = static_cast<int>(GetTensorDim(input, tensor_format_, 'C'));\n+    depth = static_cast<int>(GetTensorDim(input, tensor_format_, 'C'));\n   }\n \n-  void SetMeanVariance(const Tensor& mean, const Tensor& variance) {\n-    mean_values_ = reinterpret_cast<U*>(const_cast<U*>(mean.flat<U>().data()));\n-    variance_values_ =\n+  void SetMeanVariance(const Tensor& mean, const Tensor& variance,\n+                       U** mean_values, U** variance_values) {\n+    *mean_values = reinterpret_cast<U*>(const_cast<U*>(mean.flat<U>().data()));\n+    *variance_values =\n         reinterpret_cast<U*>(const_cast<U*>(variance.flat<U>().data()));\n   }\n "
        }
    ],
    "stats": {
        "total": 61,
        "additions": 30,
        "deletions": 31
    }
}