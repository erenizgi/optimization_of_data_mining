{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Explicitly tile & vectorize elemental loops.\n\nPiperOrigin-RevId: 840245588",
    "sha": "6ff96032210ee64793439869c7726f7a7ad2360a",
    "files": [
        {
            "sha": "6194b8b5b77669bac13729491cf2eb90f57014d4",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -337,6 +337,7 @@ static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n \n   pm.addPass(CreateLinalgElementwiseToVectorPass());\n \n+  pm.addPass(mlir::memref::createFoldMemRefAliasOpsPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(mlir::createCSEPass());\n }"
        },
        {
            "sha": "fed1a4ab20bf927e66b52e9d81887278d553217a",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -65,6 +65,7 @@ cc_library(\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/numeric:bits\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//llvm:Support\",\n@@ -80,15 +81,16 @@ cc_library(\n         \"@llvm-project//mlir:LinalgUtils\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MemRefDialect\",\n+        \"@llvm-project//mlir:MemRefTransforms\",\n         \"@llvm-project//mlir:MemRefUtils\",\n         \"@llvm-project//mlir:Pass\",\n         \"@llvm-project//mlir:SCFDialect\",\n         \"@llvm-project//mlir:SCFUtils\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:TransformUtils\",\n-        \"@llvm-project//mlir:UBDialect\",\n         \"@llvm-project//mlir:VectorDialect\",\n+        \"@llvm-project//mlir:VectorTransforms\",\n         \"@stablehlo//:stablehlo_ops\",\n     ],\n )"
        },
        {
            "sha": "52d38e1d95f3e8053af18b9245bb31a1c40315b9",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/linalg_elementwise_to_vector_pass.cc",
            "status": "modified",
            "additions": 103,
            "deletions": 10,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flinalg_elementwise_to_vector_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flinalg_elementwise_to_vector_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flinalg_elementwise_to_vector_pass.cc?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -14,14 +14,25 @@ limitations under the License.\n ==============================================================================*/\n \n #include <cassert>\n+#include <cstdint>\n #include <memory>\n+#include <optional>\n #include <utility>\n \n+#include \"absl/algorithm/container.h\"\n+#include \"absl/numeric/bits.h\"\n+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/SmallVectorExtras.h\"\n+#include \"llvm/Support/LogicalResult.h\"\n #include \"mlir/Dialect/Func/Transforms/FuncConversions.h\"\n #include \"mlir/Dialect/Linalg/IR/Linalg.h\"\n #include \"mlir/Dialect/Linalg/Transforms/Transforms.h\"\n #include \"mlir/Dialect/Linalg/Utils/Utils.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Dialect/MemRef/Transforms/Transforms.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/Dialect/Vector/Transforms/VectorRewritePatterns.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/PatternMatch.h\"\n@@ -42,6 +53,84 @@ namespace xla::cpu {\n \n namespace {\n \n+// TODO(willfroom): Make this dependent on the largest element type / target\n+// platform.\n+static constexpr int64_t kMaxVectorDim = 8;\n+\n+// Check that the given shape is possible to be vectorized with with a single\n+// dimension vector (or one with which will collapse to a single dimension), the\n+// minor dimension size is of a natural length (one that fits in registers).\n+bool IsVectorizable(llvm::ArrayRef<int64_t> shape) {\n+  if (shape.empty()) {\n+    return true;\n+  }\n+\n+  bool leading_dims_unit =\n+      absl::c_all_of(shape.drop_back(), [](int64_t size) { return size == 1; });\n+  if (!leading_dims_unit) {\n+    return false;\n+  }\n+  int64_t minor_dim = shape.back();\n+  if (mlir::ShapedType::isDynamic(minor_dim) || minor_dim > kMaxVectorDim) {\n+    return false;\n+  }\n+\n+  bool minor_dim_power_of_two = absl::has_single_bit<uint64_t>(minor_dim);\n+  return minor_dim_power_of_two;\n+}\n+\n+// We need to tile the op so that it has a size small enough to fit into vector\n+// registers. We peel any that would otherwise need masking into smaller vector\n+// sizes.\n+class TileElementwiseOp\n+    : public mlir::OpInterfaceRewritePattern<mlir::linalg::LinalgOp> {\n+  using OpInterfaceRewritePattern::OpInterfaceRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::linalg::LinalgOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    if (!mlir::linalg::isElementwise(op)) {\n+      return rewriter.notifyMatchFailure(op, \"Op is not elementwise\");\n+    }\n+\n+    auto result_shape = op.getShape(op.getDpsInitOperand(0));\n+\n+    if (IsVectorizable(result_shape)) {\n+      return rewriter.notifyMatchFailure(op, \"Op is already tiled\");\n+    }\n+\n+    if (mlir::ShapedType::isDynamic(result_shape.back())) {\n+      return rewriter.notifyMatchFailure(op,\n+                                         \"Op has a zero-size minor dimension\");\n+    }\n+\n+    mlir::linalg::LinalgTilingOptions options;\n+    llvm::SmallVector<int64_t> tile_sizes(result_shape.size() - 1, 1);\n+    auto vector_size = kMaxVectorDim;\n+    while (vector_size > result_shape.back()) {\n+      vector_size /= 2;\n+    }\n+    tile_sizes.push_back(vector_size);\n+\n+    options.setTileSizes(tile_sizes);\n+    mlir::FailureOr<mlir::linalg::TiledLinalgOp> tile_result =\n+        mlir::linalg::tileLinalgOp(rewriter, op, options);\n+    if (failed(tile_result)) {\n+      return rewriter.notifyMatchFailure(op, \"failed to tile\");\n+    }\n+\n+    rewriter.replaceOp(op, tile_result->tensorResults);\n+\n+    auto loops =\n+        llvm::map_to_vector(tile_result->loops, [](mlir::Operation* loop) {\n+          return mlir::cast<mlir::scf::ForOp>(loop);\n+        });\n+    mlir::linalg::peelLoops(rewriter, loops);\n+    return mlir::success();\n+  }\n+};\n+\n+// Once the op is tiled, we can vectorize it directly.\n class ElementwiseToVectorPattern\n     : public mlir::OpInterfaceRewritePattern<mlir::linalg::LinalgOp> {\n  public:\n@@ -69,18 +158,15 @@ class ElementwiseToVectorPattern\n                                          \"only static shapes are supported\");\n     }\n \n-    // The default linalg vectorization is very naive and just replaces the\n-    // elementwise op with a transfer_read -> super_vector -> transfer_write,\n-    // but this works as a first pass.\n-    // TODO(willfroom): replace this with explicit loops on natural vector\n-    // sizes.\n+    if (!IsVectorizable(result_type.getShape())) {\n+      return rewriter.notifyMatchFailure(op, \"Op is not yet tiled\");\n+    }\n+\n     mlir::FailureOr<mlir::linalg::VectorizationResult> result =\n         mlir::linalg::vectorize(rewriter, op);\n-\n-    if (mlir::failed(result)) {\n-      return rewriter.notifyMatchFailure(op, \"could not vectorize\");\n+    if (failed(result)) {\n+      return rewriter.notifyMatchFailure(op, \"failed to vectorize\");\n     }\n-\n     rewriter.replaceOp(op, result->replacements);\n     return mlir::success();\n   }\n@@ -92,7 +178,14 @@ struct LinalgElementwiseToVectorPass\n   void runOnOperation() override {\n     mlir::MLIRContext* context = &getContext();\n     mlir::RewritePatternSet patterns(context);\n-    patterns.add<ElementwiseToVectorPattern>(context);\n+    patterns.add<TileElementwiseOp, ElementwiseToVectorPattern>(context);\n+    mlir::scf::ForOp::getCanonicalizationPatterns(patterns, context);\n+    mlir::memref::SubViewOp::getCanonicalizationPatterns(patterns, context);\n+    mlir::vector::TransferReadOp::getCanonicalizationPatterns(patterns,\n+                                                              context);\n+    mlir::vector::TransferWriteOp::getCanonicalizationPatterns(patterns,\n+                                                               context);\n+    mlir::vector::populateCastAwayVectorLeadingOneDimPatterns(patterns);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       signalPassFailure();"
        },
        {
            "sha": "a3926151949e079443af6ef56b82f24a5b1bc256",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -52,6 +52,8 @@ def LinalgElementwiseToVectorPass : Pass<\"xtile-cpu-linalg-elementwise-to-vector\n   let constructor = \"CreateLinalgElementwiseToVectorPass()\";\n \n   let dependentDialects = [\n+    \"mlir::arith::ArithDialect\",\n+    \"mlir::scf::SCFDialect\",\n     \"mlir::vector::VectorDialect\",\n   ];\n }"
        },
        {
            "sha": "d2021d539d56ca834f00c34fcda342a60005e260",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/linalg_elementwise_to_vector_pass.mlir",
            "status": "modified",
            "additions": 96,
            "deletions": 11,
            "changes": 107,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flinalg_elementwise_to_vector_pass.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flinalg_elementwise_to_vector_pass.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flinalg_elementwise_to_vector_pass.mlir?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -1,16 +1,101 @@\n-// RUN: fusion_compiler_opt %s -xtile-cpu-linalg-elementwise-to-vector -split-input-file | FileCheck %s\n+// RUN: fusion_compiler_opt %s -split-input-file \\\n+// RUN: -xtile-cpu-linalg-elementwise-to-vector -fold-memref-alias-ops \\\n+// RUN: | FileCheck %s\n \n func.func @elementwise_add_to_vector(\n-    %lhs : memref<8x1024xf32>,\n-    %rhs : memref<8x1024xf32>,\n-    %out : memref<8x1024xf32>) {\n-  // CHECK: %1 = vector.transfer_read %arg0\n-  // CHECK: %2 = vector.transfer_read %arg1\n-  // CHECK: %3 = arith.addf {{.*}} : vector<8x1024xf32>\n-  // CHECK: vector.transfer_write %{{.*}}, %arg2{{.*}} :\n-  // CHECK-SAME: vector<8x1024xf32>, memref<8x1024xf32>\n+    %arg0 : memref<8x1024xf32>,\n+    %arg1 : memref<8x1024xf32>,\n+    %arg2 : memref<8x1024xf32>) {\n+  // CHECK-DAG: %[[MASK:.*]] = ub.poison : f32\n+  // CHECK-DAG: %[[C0:.*]] = arith.constant 0 : index\n+  // CHECK-DAG: %[[C1:.*]] = arith.constant 1 : index\n+  // CHECK-DAG: %[[C8:.*]] = arith.constant 8 : index\n+  // CHECK-DAG: %[[C1024:.*]] = arith.constant 1024 : index\n+  // CHECK: scf.for %[[IV0:.*]] = %[[C0]] to %[[C8]] step %[[C1]] {\n+  // CHECK:   scf.for %[[IV1:.*]] = %[[C0]] to %[[C1024]] step %[[C8]] {\n+  // CHECK:     %[[LHS:.*]] = vector.transfer_read %arg0[%[[IV0]], %[[IV1]]],\n+  // CHECK-SAME:  %[[MASK]] {in_bounds = [true]} : memref<8x1024xf32>, vector<8xf32>\n+  // CHECK:     %[[RHS:.*]] = vector.transfer_read %arg1[%[[IV0]], %[[IV1]]],\n+  // CHECK-SAME:  %[[MASK]] {in_bounds = [true]} : memref<8x1024xf32>, vector<8xf32>\n+  // CHECK:     %[[OUT:.*]] = arith.addf %[[LHS]], %[[RHS]] : vector<8xf32>\n+  // CHECK:     vector.transfer_write %[[OUT]], %arg2[%[[IV0]], %[[IV1]]]\n+  // CHECK-SAME:  {in_bounds = [true]} : vector<8xf32>, memref<8x1024xf32>\n+  // CHECK:   }\n+  // CHECK: }\n   linalg.elementwise kind=#linalg.elementwise_kind<add>\n-    ins(%lhs, %rhs : memref<8x1024xf32>, memref<8x1024xf32>)\n-    outs(%out : memref<8x1024xf32>)\n+    ins(%arg0, %arg1 : memref<8x1024xf32>, memref<8x1024xf32>)\n+    outs(%arg2 : memref<8x1024xf32>)\n+  return\n+}\n+\n+//------\n+\n+func.func @elementwise_add_to_vector_non_multiple_of_8(\n+    %arg0 : memref<8x100xf32>,\n+    %arg1 : memref<8x100xf32>,\n+    %arg2 : memref<8x100xf32>) {\n+  // CHECK-DAG: %[[MASK:.*]] = ub.poison : f32\n+  // CHECK-DAG: %[[C0:.*]] = arith.constant 0 : index\n+  // CHECK-DAG: %[[C1:.*]] = arith.constant 1 : index\n+  // CHECK-DAG: %[[C8:.*]] = arith.constant 8 : index\n+  // CHECK-DAG: %[[C96:.*]] = arith.constant 96 : index\n+  // CHECK: scf.for %[[IV0:.*]] = %[[C0]] to %[[C8]] step %[[C1]] {\n+  // CHECK:   scf.for %[[IV1:.*]] = %[[C0]] to %[[C96]] step %[[C8]] {\n+  // CHECK:     %[[LHS:.*]] = vector.transfer_read %arg0[%[[IV0]], %[[IV1]]],\n+  // CHECK-SAME:  %[[MASK]] {in_bounds = [true]} : memref<8x100xf32>, vector<8xf32>\n+  // CHECK:     %[[RHS:.*]] = vector.transfer_read %arg1[%[[IV0]], %[[IV1]]],\n+  // CHECK-SAME:  %[[MASK]] {in_bounds = [true]} : memref<8x100xf32>, vector<8xf32>\n+  // CHECK:     %[[OUT:.*]] = arith.addf %[[LHS]], %[[RHS]] : vector<8xf32>\n+  // CHECK:     vector.transfer_write %[[OUT]], %arg2[%[[IV0]], %[[IV1]]]\n+  // CHECK-SAME:  {in_bounds = [true]} : vector<8xf32>, memref<8x100xf32>\n+  // CHECK:   }\n+  // CHECK: %[[UNROLL_LHS:.*]] = vector.transfer_read %arg0[%[[IV0]], %[[C96]]], %[[MASK]]\n+  // CHECK-SAME: {in_bounds = [true]} : memref<8x100xf32>, vector<4xf32>\n+  // CHECK: %[[UNROLL_RHS:.*]] = vector.transfer_read %arg1[%[[IV0]], %[[C96]]], %[[MASK]]\n+  // CHECK-SAME: {in_bounds = [true]} : memref<8x100xf32>, vector<4xf32>\n+  // CHECK: %[[UNROLL_OUT:.*]] = arith.addf %[[UNROLL_LHS]], %[[UNROLL_RHS]] : vector<4xf32>\n+  // CHECK: vector.transfer_write %[[UNROLL_OUT]], %arg2[%[[IV0]], %[[C96]]]\n+  // CHECK-SAME: {in_bounds = [true]} : vector<4xf32>, memref<8x100xf32>\n+  // CHECK: }\n+  linalg.elementwise kind=#linalg.elementwise_kind<add>\n+    ins(%arg0, %arg1 : memref<8x100xf32>, memref<8x100xf32>)\n+    outs(%arg2 : memref<8x100xf32>)\n+  return\n+}\n+\n+//------\n+\n+#map = affine_map<(d0, d1) -> (d0, d1)>\n+func.func @fused(%arg0: memref<8x1024xf32>,\n+                 %arg1: memref<8x1024xf32>,\n+                 %arg2: memref<8x1024xf32>) {\n+  // CHECK: scf.for\n+  // CHECK: scf.for\n+  // CHECK-NEXT: vector.transfer_read\n+  // CHECK-NEXT: vector.transfer_read\n+  // CHECK-NEXT: arith.mulf\n+  // CHECK-NEXT: arith.addf\n+  // CHECK-NEXT: vector.transfer_write\n+  linalg.generic\n+    {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]}\n+    ins(%arg0, %arg1 : memref<8x1024xf32>, memref<8x1024xf32>)\n+    outs(%arg2 : memref<8x1024xf32>) {\n+  ^bb0(%lhs: f32, %rhs: f32, %out: f32):\n+    %mul = arith.mulf %lhs, %rhs : f32\n+    %res = arith.addf %mul, %rhs : f32\n+    linalg.yield %res : f32\n+  }\n+  return\n+}\n+\n+// -----\n+\n+func.func @elementwise_add_to_vector_small_minor(\n+    %arg0 : memref<8x3xf32>,\n+    %arg1 : memref<8x3xf32>,\n+    %arg2 : memref<8x3xf32>) {\n+  linalg.elementwise kind=#linalg.elementwise_kind<add>\n+    ins(%arg0, %arg1 : memref<8x3xf32>, memref<8x3xf32>)\n+    outs(%arg2 : memref<8x3xf32>)\n   return\n }"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 205,
        "deletions": 22
    }
}