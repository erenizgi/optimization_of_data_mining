{
    "author": "tensorflower-gardener",
    "message": "Support sinking all-reduce feeding an all-gather.\n\nPiperOrigin-RevId: 818471462",
    "sha": "9cefa031f1ce90b7dba56b113a187bd0a4fca946",
    "files": [
        {
            "sha": "954d39c6c1b7750ccd56ee888b236454edf0ba94",
            "filename": "third_party/xla/xla/service/collective_pipeliner.cc",
            "status": "modified",
            "additions": 196,
            "deletions": 175,
            "changes": 371,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9cefa031f1ce90b7dba56b113a187bd0a4fca946/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9cefa031f1ce90b7dba56b113a187bd0a4fca946/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.cc?ref=9cefa031f1ce90b7dba56b113a187bd0a4fca946",
            "patch": "@@ -2270,6 +2270,199 @@ absl::Status TransformLoopForward(\n   return absl::OkStatus();\n }\n \n+absl::Status TransformFormattingOp(\n+    HloInstruction* formatting_op, const WhileMoveInfo& to_move,\n+    HloComputation* loop_computation, InstructionMap& pipelined_map,\n+    const absl::flat_hash_set<HloInstruction*>& to_add_batch_set,\n+    int64_t& next_channel_id) {\n+  auto collect_operands = [&pipelined_map, &to_add_batch_set, loop_computation,\n+                           &to_move](HloInstruction* instr) {\n+    std::vector<HloInstruction*> operands;\n+    for (auto* operand : instr->mutable_operands()) {\n+      if (operand->opcode() == HloOpcode::kConstant) {\n+        if (instr->opcode() == HloOpcode::kPad &&\n+            instr->operand_index(operand) == 1) {\n+          // No need to broadcast the padding value.\n+          operands.push_back(loop_computation->AddInstruction(\n+              operand->CloneWithNewOperands(operand->shape(), {})));\n+          continue;\n+        }\n+\n+        // Broadcast constant into full shape.\n+        HloInstruction* cloned_constant = loop_computation->AddInstruction(\n+            operand->CloneWithNewOperands(operand->shape(), {}));\n+        if (!to_add_batch_set.contains(instr)) {\n+          operands.push_back(cloned_constant);\n+          continue;\n+        }\n+        Shape full_shape =\n+            ComputeFullOutputShape(to_move, cloned_constant->shape());\n+        absl::InlinedVector<int64_t, 4> operand_dims;\n+        operand_dims.resize(cloned_constant->shape().dimensions().size());\n+        absl::c_iota(operand_dims, 1);\n+        HloInstruction* broadcasted =\n+            loop_computation->AddInstruction(HloInstruction::CreateBroadcast(\n+                full_shape, cloned_constant, operand_dims));\n+        operands.push_back(broadcasted);\n+        continue;\n+      }\n+      auto it = pipelined_map.find(operand);\n+      CHECK(it != pipelined_map.end());\n+      operands.push_back(it->second);\n+    }\n+    return operands;\n+  };\n+  const int64_t new_dim_limit =\n+      to_move.dynamic_update_slices[0]->shape().dimensions(0);\n+  if (pipelined_map.contains(formatting_op)) {\n+    return absl::OkStatus();\n+  }\n+  if (!to_add_batch_set.contains(formatting_op) &&\n+      formatting_op->opcode() != HloOpcode::kBroadcast) {\n+    HloInstruction* cloned_not_to_batch =\n+        loop_computation->AddInstruction(formatting_op->CloneWithNewOperands(\n+            formatting_op->shape(), collect_operands(formatting_op)));\n+    UpdateInstructionChannelId(cloned_not_to_batch, next_channel_id);\n+    pipelined_map[formatting_op] = cloned_not_to_batch;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->IsElementwise() ||\n+      formatting_op->opcode() == HloOpcode::kReshape ||\n+      formatting_op->opcode() == HloOpcode::kAllReduce ||\n+      formatting_op->opcode() == HloOpcode::kConvert ||\n+      formatting_op->opcode() == HloOpcode::kCollectivePermute) {\n+    HloInstruction* cloned_elementwise =\n+        loop_computation->AddInstruction(formatting_op->CloneWithNewOperands(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()),\n+            collect_operands(formatting_op)));\n+    pipelined_map[formatting_op] = cloned_elementwise;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->opcode() == HloOpcode::kAllGather) {\n+    auto* all_gather_instruction = Cast<HloAllGatherInstruction>(formatting_op);\n+    auto operands = collect_operands(formatting_op);\n+    HloInstruction* expanded_all_gather =\n+        loop_computation->AddInstruction(HloInstruction::CreateAllGather(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()), operands,\n+            all_gather_instruction->all_gather_dimension() + 1,\n+            all_gather_instruction->replica_groups(),\n+            all_gather_instruction->constrain_layout(),\n+            all_gather_instruction->channel_id(),\n+            all_gather_instruction->use_global_device_ids()));\n+    pipelined_map[formatting_op] = expanded_all_gather;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->opcode() == HloOpcode::kReduce) {\n+    auto operands = collect_operands(formatting_op);\n+    std::vector<int64_t> dimensions(formatting_op->dimensions().begin(),\n+                                    formatting_op->dimensions().end());\n+    for (auto& dim : dimensions) {\n+      ++dim;\n+    }\n+    // Look through broadcast for reduce init value.\n+    if (operands[1]->opcode() == HloOpcode::kBroadcast) {\n+      CHECK(operands[1]->operand(0)->opcode() == HloOpcode::kConstant);\n+      operands[1] = operands[1]->mutable_operand(0);\n+    }\n+    HloInstruction* expanded_reduce =\n+        loop_computation->AddInstruction(HloInstruction::CreateReduce(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()),\n+            operands[0], operands[1], dimensions, formatting_op->to_apply()));\n+    pipelined_map[formatting_op] = expanded_reduce;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->opcode() == HloOpcode::kBroadcast) {\n+    auto operands = collect_operands(formatting_op);\n+    std::vector<int64_t> dimensions(1, 0);\n+    for (const int64_t dim : formatting_op->dimensions()) {\n+      dimensions.push_back(dim + 1);\n+    }\n+    // Constant scalars don't get expanded ahead of time and are kept\n+    // scalar.\n+    if (operands[0]->shape().dimensions().empty()) {\n+      dimensions.clear();\n+    }\n+    HloInstruction* expanded_broadcast =\n+        loop_computation->AddInstruction(HloInstruction::CreateBroadcast(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()),\n+            operands[0], dimensions));\n+    pipelined_map[formatting_op] = expanded_broadcast;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->opcode() == HloOpcode::kSlice) {\n+    std::vector<int64_t> slice_start = formatting_op->slice_starts();\n+    std::vector<int64_t> slice_limits = formatting_op->slice_limits();\n+    std::vector<int64_t> slice_strides = formatting_op->slice_strides();\n+    slice_start.insert(slice_start.begin(), 0);\n+    slice_limits.insert(slice_limits.begin(), new_dim_limit);\n+    slice_strides.insert(slice_strides.begin(), 1);\n+    HloInstruction* expanded_slice =\n+        loop_computation->AddInstruction(HloInstruction::CreateSlice(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()),\n+            collect_operands(formatting_op)[0], slice_start, slice_limits,\n+            slice_strides));\n+    pipelined_map[formatting_op] = expanded_slice;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->opcode() == HloOpcode::kDynamicSlice) {\n+    std::vector<int64_t> dynamic_slice_sizes =\n+        formatting_op->dynamic_slice_sizes();\n+    dynamic_slice_sizes.insert(dynamic_slice_sizes.begin(), new_dim_limit);\n+    HloDynamicSliceInstruction* dynslice =\n+        Cast<HloDynamicSliceInstruction>(formatting_op);\n+    HloInstruction* zero = loop_computation->AddInstruction(\n+        HloInstruction::CreateConstant(LiteralUtil::Zero(\n+            formatting_op->operand(dynslice->first_index_operand_number())\n+                ->shape()\n+                .element_type())));\n+    std::vector<HloInstruction*> indices(1, zero);\n+    auto collected_operands = collect_operands(formatting_op);\n+    indices.insert(indices.end(), std::next(collected_operands.begin()),\n+                   collected_operands.end());\n+    HloInstruction* expanded_dynslice =\n+        loop_computation->AddInstruction(HloInstruction::CreateDynamicSlice(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()),\n+            collected_operands[0], indices, dynamic_slice_sizes));\n+    pipelined_map[formatting_op] = expanded_dynslice;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->opcode() == HloOpcode::kPad) {\n+    HloPadInstruction* pad_instruction = Cast<HloPadInstruction>(formatting_op);\n+    PaddingConfig p_config = pad_instruction->padding_config();\n+    PaddingConfig new_p_config;\n+    new_p_config.add_dimensions();\n+    for (auto& dim : p_config.dimensions()) {\n+      auto* new_dim = new_p_config.add_dimensions();\n+      *new_dim = dim;\n+    }\n+    auto new_operands = collect_operands(formatting_op);\n+    HloInstruction* expanded_pad =\n+        loop_computation->AddInstruction(HloInstruction::CreatePad(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()),\n+            new_operands[0], new_operands[1], new_p_config));\n+    pipelined_map[formatting_op] = expanded_pad;\n+    return absl::OkStatus();\n+  }\n+  if (formatting_op->opcode() == HloOpcode::kTranspose) {\n+    HloTransposeInstruction* transpose_instruction =\n+        Cast<HloTransposeInstruction>(formatting_op);\n+    std::vector<int64_t> new_dims(transpose_instruction->dimensions().begin(),\n+                                  transpose_instruction->dimensions().end());\n+    new_dims.insert(new_dims.begin(), 0);\n+    for (int64_t i = 1; i < new_dims.size(); ++i) {\n+      ++new_dims[i];\n+    }\n+    HloInstruction* expanded_transpose =\n+        loop_computation->AddInstruction(HloInstruction::CreateTranspose(\n+            ComputeFullOutputShape(to_move, formatting_op->shape()),\n+            collect_operands(formatting_op)[0], new_dims));\n+    pipelined_map[formatting_op] = expanded_transpose;\n+    return absl::OkStatus();\n+  }\n+  return absl::InvalidArgumentError(\n+      absl::StrCat(\"Unsupported instruction \", formatting_op->ToString()));\n+}\n+\n // Function that does the work of sinking all-reduces the output of which are\n // concatenated after the loop. Rough transformation: while (i < LAYERS) {\n //   p0 = param(0)\n@@ -2569,8 +2762,6 @@ absl::Status TransformLoopForwardSink(const WhileLoopAnalysis& loop_analysis,\n           HloInstruction::CreateGetTupleElement(new_while, gte_index));\n       pipelined_map[collective->mutable_operand(0)] = to_sink;\n     }\n-    const int64_t new_dim_limit =\n-        to_move.dynamic_update_slices[0]->shape().dimensions(0);\n     auto pipelined_instrs = CollectDependenciesToPipeline(\n         absl::MakeSpan(to_move.collectives_to_move),\n         absl::MakeSpan(to_move.formatting_ops));\n@@ -2608,44 +2799,6 @@ absl::Status TransformLoopForwardSink(const WhileLoopAnalysis& loop_analysis,\n       pipelined_map[collective] = pipelined_instr_cloned;\n     }\n     absl::flat_hash_set<HloInstruction*> to_add_batch_set;\n-    auto collect_operands = [&pipelined_map, &to_add_batch_set,\n-                             loop_computation,\n-                             &to_move](HloInstruction* instr) {\n-      std::vector<HloInstruction*> operands;\n-      for (auto* operand : instr->mutable_operands()) {\n-        if (operand->opcode() == HloOpcode::kConstant) {\n-          if (instr->opcode() == HloOpcode::kPad &&\n-              instr->operand_index(operand) == 1) {\n-            // No need to broadcast the padding value.\n-            operands.push_back(loop_computation->AddInstruction(\n-                operand->CloneWithNewOperands(operand->shape(), {})));\n-            continue;\n-          }\n-\n-          // Broadcast constant into full shape.\n-          HloInstruction* cloned_constant = loop_computation->AddInstruction(\n-              operand->CloneWithNewOperands(operand->shape(), {}));\n-          if (!to_add_batch_set.contains(instr)) {\n-            operands.push_back(cloned_constant);\n-            continue;\n-          }\n-          Shape full_shape =\n-              ComputeFullOutputShape(to_move, cloned_constant->shape());\n-          absl::InlinedVector<int64_t, 4> operand_dims;\n-          operand_dims.resize(cloned_constant->shape().dimensions().size());\n-          absl::c_iota(operand_dims, 1);\n-          HloInstruction* broadcasted =\n-              loop_computation->AddInstruction(HloInstruction::CreateBroadcast(\n-                  full_shape, cloned_constant, operand_dims));\n-          operands.push_back(broadcasted);\n-          continue;\n-        }\n-        auto it = pipelined_map.find(operand);\n-        CHECK(it != pipelined_map.end());\n-        operands.push_back(it->second);\n-      }\n-      return operands;\n-    };\n     for (auto* current : to_move.formatting_ops) {\n       if (IsLoopInvariant(current, invariant_cache)) {\n         continue;\n@@ -2657,141 +2810,9 @@ absl::Status TransformLoopForwardSink(const WhileLoopAnalysis& loop_analysis,\n     //  an effect on the instruction itself (like say broadcast, slices ...\n     //  etc).\n     for (HloInstruction* formatting_op : to_move.formatting_ops) {\n-      if (pipelined_map.contains(formatting_op)) {\n-        continue;\n-      }\n-      if (!to_add_batch_set.contains(formatting_op) &&\n-          formatting_op->opcode() != HloOpcode::kBroadcast) {\n-        HloInstruction* cloned_not_to_batch = loop_computation->AddInstruction(\n-            formatting_op->CloneWithNewOperands(\n-                formatting_op->shape(), collect_operands(formatting_op)));\n-        UpdateInstructionChannelId(cloned_not_to_batch, next_channel_id);\n-        pipelined_map[formatting_op] = cloned_not_to_batch;\n-        continue;\n-      }\n-      if (formatting_op->IsElementwise() ||\n-          formatting_op->opcode() == HloOpcode::kReshape ||\n-          formatting_op->opcode() == HloOpcode::kAllReduce ||\n-          formatting_op->opcode() == HloOpcode::kConvert ||\n-          formatting_op->opcode() == HloOpcode::kCollectivePermute) {\n-        HloInstruction* cloned_elementwise = loop_computation->AddInstruction(\n-            formatting_op->CloneWithNewOperands(\n-                ComputeFullOutputShape(to_move, formatting_op->shape()),\n-                collect_operands(formatting_op)));\n-        pipelined_map[formatting_op] = cloned_elementwise;\n-        continue;\n-      }\n-      if (formatting_op->opcode() == HloOpcode::kReduce) {\n-        auto operands = collect_operands(formatting_op);\n-        std::vector<int64_t> dimensions(formatting_op->dimensions().begin(),\n-                                        formatting_op->dimensions().end());\n-        for (auto& dim : dimensions) {\n-          ++dim;\n-        }\n-        // Look through broadcast for reduce init value.\n-        if (operands[1]->opcode() == HloOpcode::kBroadcast) {\n-          CHECK(operands[1]->operand(0)->opcode() == HloOpcode::kConstant);\n-          operands[1] = operands[1]->mutable_operand(0);\n-        }\n-        HloInstruction* expanded_reduce =\n-            loop_computation->AddInstruction(HloInstruction::CreateReduce(\n-                ComputeFullOutputShape(to_move, formatting_op->shape()),\n-                operands[0], operands[1], dimensions,\n-                formatting_op->to_apply()));\n-        pipelined_map[formatting_op] = expanded_reduce;\n-        continue;\n-      }\n-      if (formatting_op->opcode() == HloOpcode::kBroadcast) {\n-        auto operands = collect_operands(formatting_op);\n-        std::vector<int64_t> dimensions(1, 0);\n-        for (const int64_t dim : formatting_op->dimensions()) {\n-          dimensions.push_back(dim + 1);\n-        }\n-        // Constant scalars don't get expanded ahead of time and are kept\n-        // scalar.\n-        if (operands[0]->shape().dimensions().empty()) {\n-          dimensions.clear();\n-        }\n-        HloInstruction* expanded_broadcast =\n-            loop_computation->AddInstruction(HloInstruction::CreateBroadcast(\n-                ComputeFullOutputShape(to_move, formatting_op->shape()),\n-                operands[0], dimensions));\n-        pipelined_map[formatting_op] = expanded_broadcast;\n-        continue;\n-      }\n-      if (formatting_op->opcode() == HloOpcode::kSlice) {\n-        std::vector<int64_t> slice_start = formatting_op->slice_starts();\n-        std::vector<int64_t> slice_limits = formatting_op->slice_limits();\n-        std::vector<int64_t> slice_strides = formatting_op->slice_strides();\n-        slice_start.insert(slice_start.begin(), 0);\n-        slice_limits.insert(slice_limits.begin(), new_dim_limit);\n-        slice_strides.insert(slice_strides.begin(), 1);\n-        HloInstruction* expanded_slice =\n-            loop_computation->AddInstruction(HloInstruction::CreateSlice(\n-                ComputeFullOutputShape(to_move, formatting_op->shape()),\n-                collect_operands(formatting_op)[0], slice_start, slice_limits,\n-                slice_strides));\n-        pipelined_map[formatting_op] = expanded_slice;\n-        continue;\n-      }\n-      if (formatting_op->opcode() == HloOpcode::kDynamicSlice) {\n-        std::vector<int64_t> dynamic_slice_sizes =\n-            formatting_op->dynamic_slice_sizes();\n-        dynamic_slice_sizes.insert(dynamic_slice_sizes.begin(), new_dim_limit);\n-        HloDynamicSliceInstruction* dynslice =\n-            Cast<HloDynamicSliceInstruction>(formatting_op);\n-        HloInstruction* zero = loop_computation->AddInstruction(\n-            HloInstruction::CreateConstant(LiteralUtil::Zero(\n-                formatting_op->operand(dynslice->first_index_operand_number())\n-                    ->shape()\n-                    .element_type())));\n-        std::vector<HloInstruction*> indices(1, zero);\n-        auto collected_operands = collect_operands(formatting_op);\n-        indices.insert(indices.end(), std::next(collected_operands.begin()),\n-                       collected_operands.end());\n-        HloInstruction* expanded_dynslice =\n-            loop_computation->AddInstruction(HloInstruction::CreateDynamicSlice(\n-                ComputeFullOutputShape(to_move, formatting_op->shape()),\n-                collected_operands[0], indices, dynamic_slice_sizes));\n-        pipelined_map[formatting_op] = expanded_dynslice;\n-        continue;\n-      }\n-      if (formatting_op->opcode() == HloOpcode::kPad) {\n-        HloPadInstruction* pad_instruction =\n-            Cast<HloPadInstruction>(formatting_op);\n-        PaddingConfig p_config = pad_instruction->padding_config();\n-        PaddingConfig new_p_config;\n-        new_p_config.add_dimensions();\n-        for (auto& dim : p_config.dimensions()) {\n-          auto* new_dim = new_p_config.add_dimensions();\n-          *new_dim = dim;\n-        }\n-        auto new_operands = collect_operands(formatting_op);\n-        HloInstruction* expanded_pad =\n-            loop_computation->AddInstruction(HloInstruction::CreatePad(\n-                ComputeFullOutputShape(to_move, formatting_op->shape()),\n-                new_operands[0], new_operands[1], new_p_config));\n-        pipelined_map[formatting_op] = expanded_pad;\n-        continue;\n-      }\n-      if (formatting_op->opcode() == HloOpcode::kTranspose) {\n-        HloTransposeInstruction* transpose_instruction =\n-            Cast<HloTransposeInstruction>(formatting_op);\n-        std::vector<int64_t> new_dims(\n-            transpose_instruction->dimensions().begin(),\n-            transpose_instruction->dimensions().end());\n-        new_dims.insert(new_dims.begin(), 0);\n-        for (int64_t i = 1; i < new_dims.size(); ++i) {\n-          ++new_dims[i];\n-        }\n-        HloInstruction* expanded_transpose =\n-            loop_computation->AddInstruction(HloInstruction::CreateTranspose(\n-                ComputeFullOutputShape(to_move, formatting_op->shape()),\n-                collect_operands(formatting_op)[0], new_dims));\n-        pipelined_map[formatting_op] = expanded_transpose;\n-        continue;\n-      }\n-      CHECK(false) << \"Unsupported instruction \" << formatting_op->ToString();\n+      TF_RETURN_IF_ERROR(TransformFormattingOp(\n+          formatting_op, to_move, loop_computation, pipelined_map,\n+          to_add_batch_set, next_channel_id));\n     }\n     for (int64_t i = 0; i < to_move.output_indices.size(); ++i) {\n       HloDynamicUpdateSliceInstruction* d_update ="
        },
        {
            "sha": "d85b6c80461c4db2581014fe58aa4ab9cf6a09ad",
            "filename": "third_party/xla/xla/service/collective_pipeliner_test.cc",
            "status": "modified",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9cefa031f1ce90b7dba56b113a187bd0a4fca946/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9cefa031f1ce90b7dba56b113a187bd0a4fca946/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner_test.cc?ref=9cefa031f1ce90b7dba56b113a187bd0a4fca946",
            "patch": "@@ -2447,6 +2447,89 @@ ENTRY entry {\n   EXPECT_EQ(select_instr_loop->opcode(), HloOpcode::kSelect);\n }\n \n+TEST_F(CollectivePipelinerTest, ForwardSinkAllReduceFeedingAllGather) {\n+  constexpr absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+add {\n+  lhs = bf16[] parameter(0)\n+  rhs = bf16[] parameter(1)\n+  ROOT add = bf16[] add(lhs, rhs)\n+}\n+\n+while_cond {\n+  param = (s32[], bf16[3,8,128], bf16[3,8,128]) parameter(0)\n+  gte = s32[] get-tuple-element(param), index=0\n+  constant.1 = s32[] constant(3)\n+  ROOT cmp = pred[] compare(gte, constant.1), direction=LT\n+}\n+\n+while_body {\n+  param = (s32[], bf16[3,8,128], bf16[3,8,128]) parameter(0)\n+  get-tuple-element.394 = s32[] get-tuple-element(param), index=0\n+  get-tuple-element.395 = bf16[3,8,128] get-tuple-element(param), index=1\n+  get-tuple-element.35 = bf16[3,8,128] get-tuple-element(param), index=2\n+  constant.2557 = s32[] constant(1)\n+  add.230 = s32[] add(get-tuple-element.394, constant.2557)\n+  constant.2559 = s32[] constant(3)\n+  subtract.139 = s32[] subtract(constant.2559, get-tuple-element.394)\n+  constant.2560 = s32[] constant(-1)\n+  add.231 = s32[] add(subtract.139, constant.2560)\n+  constant.2561 = s32[] constant(0)\n+  compare.747 = pred[] compare(add.231, constant.2561), direction=LT\n+  constant.2562 = s32[] constant(2)\n+  add.232 = s32[] add(subtract.139, constant.2562)\n+  select.1348 = s32[] select(compare.747, add.232, add.231)\n+  dynamic-slice.99 = bf16[1,8,128] dynamic-slice(get-tuple-element.35, select.1348, constant.2561, constant.2561), dynamic_slice_sizes={1,8,128}\n+  mul = bf16[1,8,128] multiply(dynamic-slice.99, dynamic-slice.99)\n+  t2 = bf16[1,128,8] transpose(mul), dimensions={0,2,1}\n+  rs.1 = bf16[1,128,2] reduce-scatter(t2), replica_groups={{0,1,2,3}}, dimensions={2}, to_apply=add\n+  ar.1 = bf16[1,128,2] all-reduce(rs.1), replica_groups={{0,1,2,3}}, to_apply=add\n+  ag.1 = bf16[1,128,8] all-gather(ar.1), replica_groups={{0,1,2,3}}, dimensions={2}\n+  %c = bf16[] custom-call(), custom_call_target=\"Boh\"\n+  %b = bf16[1,128,8] broadcast(c), dimensions={}\n+  %a = bf16[1,128,8] add(ag.1, b)\n+  %t = bf16[1,8,128] transpose(a), dimensions={0,2,1}\n+  dynamic-update-slice.35 = bf16[3,8,128] dynamic-update-slice(get-tuple-element.395, t, select.1348, constant.2561, constant.2561)\n+  ROOT tuple = (s32[], bf16[3,8,128], bf16[3,8,128]) tuple(add.230, dynamic-update-slice.35, get-tuple-element.35), control-predecessors={select.1348}\n+}\n+\n+ENTRY entry {\n+  c0 = s32[] constant(0)\n+  p0 = bf16[3,8,128] parameter(0)\n+  tuple = (s32[], bf16[3,8,128], bf16[3,8,128]) tuple(c0, p0, p0)\n+  while = (s32[], bf16[3,8,128], bf16[3,8,128]) while(tuple), condition=while_cond, body=while_body\n+  ROOT gte1 = bf16[3,8,128] get-tuple-element(while), index=1\n+}\n+)\";\n+  auto module = ParseAndReturnUnverifiedModule(hlo_string, config_).value();\n+  EXPECT_TRUE(RunOptimizer(\n+                  module.get(), /*last_run=*/true,\n+                  /*level_to_operate_on=*/0,\n+                  /*pipeline_use_tree=*/true,\n+                  /*process_different_sized_ops=*/true,\n+                  collective_pipeliner_utils::PipeliningDirection::kForwardSink)\n+                  .value());\n+  XLA_VLOG_LINES(1, module->ToString());\n+  const HloInstruction* while_instr =\n+      FindInstruction(module.get(), HloOpcode::kWhile);\n+  const HloComputation* comp = while_instr->while_body();\n+  const HloInstruction* root_loop = comp->root_instruction();\n+  EXPECT_TRUE(root_loop->HasControlDependencies());\n+  EXPECT_EQ(root_loop->control_predecessors().size(), 1);\n+  const HloInstruction* select_instr_loop =\n+      root_loop->control_predecessors()[0];\n+  const HloInstruction* transpose_instr =\n+      FindInstruction(module.get(), HloOpcode::kTranspose);\n+  EXPECT_EQ(transpose_instr->dimensions(), std::vector<int64_t>({0, 1, 3, 2}));\n+  EXPECT_EQ(select_instr_loop->opcode(), HloOpcode::kSelect);\n+  // All-reduce is sunk outside the loop with all-gather consumer.\n+  const HloInstruction* all_reduce =\n+      FindInstruction(module.get(), HloOpcode::kAllReduce);\n+  EXPECT_EQ(all_reduce->users().size(), 1);\n+  EXPECT_EQ(all_reduce->users()[0]->opcode(), HloOpcode::kAllGather);\n+}\n+\n TEST_F(CollectivePipelinerTest, ForwardSinkLinearShape4097) {\n   constexpr absl::string_view hlo_string = R\"(\n HloModule module"
        }
    ],
    "stats": {
        "total": 454,
        "additions": 279,
        "deletions": 175
    }
}