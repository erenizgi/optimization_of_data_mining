{
    "author": "WillFroom",
    "message": "[XLA:CPU/GPU][XTile] Split out lowering functionality from emitter_helpers.\n\nThis is is a stepping stone to allow us to split out the emitting-only part of the tiled emitter such that it doesn't depend explicitly on triton.\n\nNote: this is purely code movement - there is no functional change.\nPiperOrigin-RevId: 837061032",
    "sha": "3a4a157d5b0fad310458438b29a52f4b689f4244",
    "files": [
        {
            "sha": "9e0de28e65d3c533ec52f11421c3e103a36492b2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 28,
            "deletions": 17,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -102,16 +102,15 @@ cc_library(\n     hdrs = [\n         \"emitter_helpers.h\",\n     ],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n-        \":tma_utils\",\n         \"//xla:comparison_util\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/emitters:elemental_hlo_to_mlir\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n@@ -121,12 +120,7 @@ cc_library(\n         \"//xla/mlir_hlo\",\n         \"//xla/mlir_hlo:map_mhlo_to_scalar_op\",\n         \"//xla/mlir_hlo:transformation_helpers\",\n-        \"//xla/service/gpu:target_util\",\n-        \"//xla/service/gpu:triton_fusion_analysis\",\n         \"//xla/service/llvm_ir:llvm_util\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:launch_dim\",\n-        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -137,24 +131,42 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//llvm:TargetParser\",\n-        \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:IR\",\n-        \"@llvm-project//mlir:LLVMDialect\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TensorDialect\",\n         \"@stablehlo//:stablehlo_ops\",\n-        \"@triton//:TritonDialects\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"lowering_util\",\n+    srcs = [\"lowering_util.cc\"],\n+    hdrs = [\"lowering_util.h\"],\n+    deps = [\n+        \":tma_utils\",\n+        \"//xla:util\",\n+        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n+        \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:LLVMDialect\",\n     ],\n )\n \n xla_cc_test(\n-    name = \"emitter_helpers_test\",\n-    srcs = [\"emitter_helpers_test.cc\"],\n+    name = \"lowering_util_test\",\n+    srcs = [\"lowering_util_test.cc\"],\n     deps = [\n-        \":emitter_helpers\",\n+        \":lowering_util\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n@@ -271,12 +283,12 @@ cc_library(\n         \"@stablehlo//:stablehlo_ops\",\n         \"@triton//:TritonDialects\",\n     ] + if_cuda_or_rocm_is_configured([\n+        \":lowering_util\",\n         \":compilation_pipeline\",\n         \":collective_emitter\",\n         \":dot_algorithms\",\n         \":emitter_helpers\",\n         \":support\",\n-        \":tma_utils\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n@@ -926,14 +938,13 @@ cc_library(\n     deps = [\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:LLVMDialect\",\n     ],\n )\n "
        },
        {
            "sha": "9e7b7e6425705477ee9a087657b17e4fa3dbc38c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -258,11 +258,10 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n     region_values[reduction_computation->parameter_instruction(0)] =\n         accumulator;\n     region_values[reduction_computation->parameter_instruction(1)] = next_tile;\n-    TF_ASSIGN_OR_RETURN(\n-        accumulator,\n-        triton::EmitScope(b,\n-                          /*analysis=*/nullptr, /*instructions=*/to_emit,\n-                          /*values=*/region_values));\n+    TF_ASSIGN_OR_RETURN(accumulator,\n+                        triton::EmitScope(b,\n+                                          /*instructions=*/to_emit,\n+                                          /*values=*/region_values));\n   }\n   return accumulator;\n }"
        },
        {
            "sha": "1dccc05b02b9adfcff57ac86610e3f068f785b20",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 163,
            "changes": 165,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -28,18 +28,13 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/IR/Metadata.h\"\n-#include \"llvm/IR/Module.h\"\n #include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/MathExtras.h\"\n-#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n@@ -48,8 +43,6 @@ limitations under the License.\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n-#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n-#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/elemental_hlo_to_mlir.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n@@ -66,20 +59,13 @@ limitations under the License.\n #include \"xla/mlir_hlo/mhlo/transforms/map_mhlo_to_scalar_op.h\"\n #include \"xla/mlir_hlo/mhlo/transforms/transformation_helpers.h\"\n #include \"xla/primitive_util.h\"\n-#include \"xla/service/gpu/target_util.h\"\n-#include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/gpu/tma_metadata.h\"\n-#include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace xla::gpu::triton {\n \n@@ -93,7 +79,6 @@ using ::mlir::ValueRange;\n namespace ma = ::mlir::arith;\n namespace mh = ::mlir::mhlo;\n namespace mm = ::mlir::math;\n-namespace mt = ::mlir::triton;\n \n namespace {\n using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n@@ -169,7 +154,7 @@ absl::StatusOr<TensorValue> EmitNestedFusion(\n \n   TF_RET_CHECK(to_emit.back() == fusion_computation->root_instruction());\n \n-  return EmitScope(b, /*analysis=*/nullptr, to_emit, region_values);\n+  return EmitScope(b, to_emit, region_values);\n }\n }  // namespace\n \n@@ -422,59 +407,6 @@ Value Minimum(EmitterLocOpBuilder& b, ValueRange values) {\n   return ma::MinSIOp::create(b, values);\n }\n \n-bool IsSupportedElementwiseLibdeviceFunction(const HloInstruction& hlo) {\n-  auto dev_fn_id = GetTargetDeviceFunctionID(hlo.opcode());\n-  if (!dev_fn_id.has_value()) {\n-    return false;\n-  }\n-  PrimitiveType output_type = hlo.shape().element_type();\n-  return output_type == PrimitiveType::BF16 ||\n-         output_type == PrimitiveType::F16 ||\n-         output_type == PrimitiveType::F32 || output_type == PrimitiveType::F64;\n-}\n-\n-// TODO(willfroom): Remove this (and associated functions) once the legacy\n-// matmul is removed.\n-absl::StatusOr<Value> EmitElementwiseLibdeviceFunction(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info, const HloInstruction& hlo,\n-    ValueRange inputs) {\n-  auto dev_fn_id = GetTargetDeviceFunctionID(hlo.opcode());\n-  if (!dev_fn_id.has_value()) {\n-    return absl::InvalidArgumentError(\n-        absl::StrCat(\"No libdevice function for operation \", hlo.ToString()));\n-  }\n-  PrimitiveType output_type = hlo.shape().element_type();\n-  if (output_type != PrimitiveType::BF16 && output_type != PrimitiveType::F16 &&\n-      output_type != PrimitiveType::F32 && output_type != PrimitiveType::F64) {\n-    return absl::InvalidArgumentError(\n-        absl::StrCat(\"Unsupported elementwise operation \", hlo.ToString()));\n-  }\n-  llvm::Triple triple(\"nvptx64-unknown-unknown\");\n-  if (device_info.gpu_compute_capability().IsRocm()) {\n-    triple.setTriple(\"amdgcn-unknown-unknown\");\n-  }\n-  llvm::SmallVector<Value, 2> casted_inputs;\n-  if (output_type == PrimitiveType::BF16 || output_type == PrimitiveType::F16) {\n-    // Upcast the inputs to F32.\n-    for (int64_t i = 0; i < inputs.size(); ++i) {\n-      casted_inputs.push_back(Cast(b, inputs[i], b.getF32Type()));\n-    }\n-  } else {\n-    casted_inputs.assign(inputs.begin(), inputs.end());\n-  }\n-  Value res = mt::ExternElementwiseOp::create(\n-      b, casted_inputs[0].getType(), casted_inputs, \"libdevice\", libdevice_path,\n-      ObtainDeviceFunctionName(dev_fn_id.value(), output_type, triple),\n-      /*pure=*/true);\n-  if (output_type == PrimitiveType::BF16 || output_type == PrimitiveType::F16) {\n-    // Downcast back to the original output type.\n-    TF_ASSIGN_OR_RETURN(auto dst_ty, TritonType(b, output_type));\n-    res = Cast(b, res, dst_ty);\n-  }\n-  return res;\n-}\n-\n absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n                                       const HloInstruction& hlo,\n                                       ValueRange inputs) {\n@@ -628,98 +560,6 @@ Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n   return mlir::arith::BitcastOp::create(b, value_type, value);\n }\n \n-std::vector<llvm::Metadata*> ExtractNvvmAnnotations(\n-    llvm::Module* ll_triton_module) {\n-  std::vector<llvm::Metadata*> captured_nvvm_annotations;\n-  llvm::NamedMDNode* nvvm_annotations =\n-      ll_triton_module->getNamedMetadata(\"nvvm.annotations\");\n-  if (nvvm_annotations) {\n-    for (llvm::MDNode* operand : nvvm_annotations->operands()) {\n-      captured_nvvm_annotations.push_back(operand);\n-    }\n-    ll_triton_module->eraseNamedMetadata(nvvm_annotations);\n-  }\n-  return captured_nvvm_annotations;\n-}\n-\n-absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n-    mlir::ModuleOp triton_module, mlir::LLVM::LLVMFuncOp func_op) {\n-  // Extract the launch information from the Triton module.\n-  auto threads_per_warp_attr =\n-      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.threads-per-warp\");\n-  if (!threads_per_warp_attr) {\n-    return absl::InternalError(\"ttg.threads-per-warp attribute not found.\");\n-  }\n-  auto num_warps_attr =\n-      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.num-warps\");\n-  if (!num_warps_attr) {\n-    return absl::InternalError(\"ttg.num-warps attribute not found.\");\n-  }\n-  auto total_num_warps_attr =\n-      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.total-num-warps\");\n-  if (!total_num_warps_attr) {\n-    return absl::InternalError(\"ttg.total-num-warps attribute not found.\");\n-  }\n-  auto reqntid_attr =\n-      func_op->getAttrOfType<mlir::DenseI32ArrayAttr>(\"nvvm.reqntid\");\n-  if (!reqntid_attr) {\n-    return absl::InternalError(\"nvvm.reqntid attribute not found.\");\n-  }\n-  auto reqntids = reqntid_attr.asArrayRef();\n-  if (reqntids.empty()) {\n-    return absl::InternalError(\"nvvm.reqntid attribute is empty.\");\n-  }\n-  if (reqntids.size() > 3) {\n-    return absl::InternalError(\n-        \"nvvm.reqntid attribute has more than 3 dimensions.\");\n-  }\n-\n-  // Validate the launch information.\n-  if (num_warps_attr.getInt() != total_num_warps_attr.getInt()) {\n-    VLOG(6)\n-        << \"num_warps and total_num_warps are different! This can happen if \"\n-           \"Triton compilation decides to use a different number of warps than \"\n-           \"configured. e.g. auto warp specialization can do that.\";\n-  }\n-  int64_t expected_total_threads = xla::Product<int32_t>(reqntids);\n-  int64_t actual_total_threads =\n-      total_num_warps_attr.getInt() * threads_per_warp_attr.getInt();\n-  if (actual_total_threads != expected_total_threads) {\n-    return absl::InternalError(absl::StrCat(\n-        \"Expected total threads as per reqntid attribute to be \",\n-        expected_total_threads, \" but got \", actual_total_threads,\n-        \" as per ttg.total-num-warps and tt.threads-per-warp attributes.\"));\n-  }\n-\n-  stream_executor::ThreadDim thread_dims(reqntids[0],\n-                                         reqntids.size() > 1 ? reqntids[1] : 1,\n-                                         reqntids.size() > 2 ? reqntids[2] : 1);\n-  return thread_dims;\n-}\n-\n-absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n-    mlir::LLVM::LLVMFuncOp func_op) {\n-  stream_executor::gpu::TmaMetadata tma_metadata;\n-  for (auto [idx, arg] : llvm::enumerate(func_op.getArguments())) {\n-    if (auto attr =\n-            func_op.getArgAttrOfType<mlir::triton::xla::TmaDescriptorAttr>(\n-                idx, \"tt.tma_descriptor\")) {\n-      TF_ASSIGN_OR_RETURN(\n-          auto tma_desc,\n-          CreateTmaDescriptor(attr.getGlobalShape(), attr.getTileShape(),\n-                              attr.getTileStrides(), attr.getLayout(),\n-                              attr.getElementByteSize(),\n-                              attr.getSwizzleMode().getValue()));\n-      tma_metadata.arg_index_to_tma_info.insert({idx, tma_desc});\n-    }\n-  }\n-  return tma_metadata;\n-}\n-\n-mt::PointerType GetGlobalPointerType(mlir::Type element_type) {\n-  return mlir::cast<mt::PointerType>(mt::getPointerTypeToElement(element_type));\n-}\n-\n /*static */ absl::StatusOr<TileInfo> TileInfo::Construct(\n     EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n     const TiledHloInstruction& tiled_hlo) {\n@@ -755,8 +595,7 @@ TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n }\n \n absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n-    absl::Span<const HloInstruction* const> instructions,\n+    EmitterLocOpBuilder b, absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n   for (const HloInstruction* hlo : instructions) {\n     TensorValue result;"
        },
        {
            "sha": "6bef628efddb201aaeaa4c4b975c5eb63dfaa074",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 1,
            "deletions": 39,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -19,23 +19,17 @@ limitations under the License.\n #include <cstdint>\n #include <string>\n #include <utility>\n-#include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/IR/Metadata.h\"\n-#include \"llvm/IR/Module.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n@@ -46,16 +40,11 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/literal.h\"\n-#include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/gpu/tma_metadata.h\"\n-#include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/status.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace xla::gpu::triton {\n \n@@ -219,38 +208,12 @@ mlir::Value Cast(EmitterLocOpBuilder& b, mlir::Value value,\n absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n     EmitterLocOpBuilder& b, const HloInstruction& constant);\n \n-bool IsSupportedElementwiseLibdeviceFunction(const HloInstruction& hlo);\n-\n-// Should only be called if IsSupportedElementwiseLibdeviceFunction() returns\n-// true for `hlo`, otherwise an error is returned.\n-absl::StatusOr<mlir::Value> EmitElementwiseLibdeviceFunction(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info, const HloInstruction& hlo,\n-    mlir::ValueRange inputs);\n-\n absl::StatusOr<mlir::Value> EmitElementwise(EmitterLocOpBuilder& b,\n                                             const HloInstruction& hlo,\n                                             mlir::ValueRange inputs);\n \n mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n \n-// Extracts NVVM annotations from the Triton module.\n-std::vector<llvm::Metadata*> ExtractNvvmAnnotations(\n-    llvm::Module* ll_triton_module);\n-\n-// Extracts TMA metadata information from LLVM generated by the Triton\n-// compilation. The underlying map will be empty if TMA is not used.\n-absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n-    mlir::LLVM::LLVMFuncOp func_op);\n-\n-// Extracts thread dimensions from Triton module attributes.\n-absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n-    mlir::ModuleOp triton_module, mlir::LLVM::LLVMFuncOp func_op);\n-\n-// Returns the triton pointer type with global memory space and the given\n-// element type.\n-::mlir::triton::PointerType GetGlobalPointerType(mlir::Type element_type);\n-\n // Emits an xtile::ExtractTileOp for the given tile info and argument.\n TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n                                  const TileInfo& tile_info, mlir::Value arg);\n@@ -266,8 +229,7 @@ TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n // Example usage within [EmitReduce] includes using it to emit the body of the\n // `HloInstruction::to_apply` computation.\n absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n-    absl::Span<const HloInstruction* const> instructions,\n+    EmitterLocOpBuilder b, absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values);\n \n // Same as HLO BroadcastInDims. The sorted indices in `dims` specify the"
        },
        {
            "sha": "61b83a1df527487ac08ce90f09f995db2f3c87cc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -99,6 +99,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/backends/gpu/codegen/triton/lowering_util.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n@@ -115,7 +116,6 @@ limitations under the License.\n #include \"xla/codegen/xtile/ir/xtile_dialect.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -270,8 +270,8 @@ absl::StatusOr<TensorValue> EmitReduce(\n \n     TF_RET_CHECK(!to_emit.empty());\n \n-    TF_ASSIGN_OR_RETURN(TensorValue result, EmitScope(b, /*analysis=*/nullptr,\n-                                                      to_emit, region_values));\n+    TF_ASSIGN_OR_RETURN(TensorValue result,\n+                        EmitScope(b, to_emit, region_values));\n     stablehlo::ReturnOp::create(b, SmallVector<Value>({result}));\n     b.setInsertionPointAfter(reduction);\n   }"
        },
        {
            "sha": "834d8aefa12833376b538949551878cedb12c97d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/lowering_util.cc",
            "status": "added",
            "additions": 128,
            "deletions": 0,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util.cc?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -0,0 +1,128 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/codegen/triton/lowering_util.h\"\n+\n+#include <cstdint>\n+#include <vector>\n+\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/IR/Metadata.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::gpu::triton {\n+\n+absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n+    mlir::ModuleOp triton_module, mlir::LLVM::LLVMFuncOp func_op) {\n+  // Extract the launch information from the Triton module.\n+  auto threads_per_warp_attr =\n+      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.threads-per-warp\");\n+  if (!threads_per_warp_attr) {\n+    return absl::InternalError(\"ttg.threads-per-warp attribute not found.\");\n+  }\n+  auto num_warps_attr =\n+      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.num-warps\");\n+  if (!num_warps_attr) {\n+    return absl::InternalError(\"ttg.num-warps attribute not found.\");\n+  }\n+  auto total_num_warps_attr =\n+      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.total-num-warps\");\n+  if (!total_num_warps_attr) {\n+    return absl::InternalError(\"ttg.total-num-warps attribute not found.\");\n+  }\n+  auto reqntid_attr =\n+      func_op->getAttrOfType<mlir::DenseI32ArrayAttr>(\"nvvm.reqntid\");\n+  if (!reqntid_attr) {\n+    return absl::InternalError(\"nvvm.reqntid attribute not found.\");\n+  }\n+  auto reqntids = reqntid_attr.asArrayRef();\n+  if (reqntids.empty()) {\n+    return absl::InternalError(\"nvvm.reqntid attribute is empty.\");\n+  }\n+  if (reqntids.size() > 3) {\n+    return absl::InternalError(\n+        \"nvvm.reqntid attribute has more than 3 dimensions.\");\n+  }\n+\n+  // Validate the launch information.\n+  if (num_warps_attr.getInt() != total_num_warps_attr.getInt()) {\n+    VLOG(6)\n+        << \"num_warps and total_num_warps are different! This can happen if \"\n+           \"Triton compilation decides to use a different number of warps than \"\n+           \"configured. e.g. auto warp specialization can do that.\";\n+  }\n+  int64_t expected_total_threads = xla::Product<int32_t>(reqntids);\n+  int64_t actual_total_threads =\n+      total_num_warps_attr.getInt() * threads_per_warp_attr.getInt();\n+  if (actual_total_threads != expected_total_threads) {\n+    return absl::InternalError(absl::StrCat(\n+        \"Expected total threads as per reqntid attribute to be \",\n+        expected_total_threads, \" but got \", actual_total_threads,\n+        \" as per ttg.total-num-warps and tt.threads-per-warp attributes.\"));\n+  }\n+\n+  stream_executor::ThreadDim thread_dims(reqntids[0],\n+                                         reqntids.size() > 1 ? reqntids[1] : 1,\n+                                         reqntids.size() > 2 ? reqntids[2] : 1);\n+  return thread_dims;\n+}\n+\n+absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n+    mlir::LLVM::LLVMFuncOp func_op) {\n+  stream_executor::gpu::TmaMetadata tma_metadata;\n+  for (auto [idx, arg] : llvm::enumerate(func_op.getArguments())) {\n+    if (auto attr =\n+            func_op.getArgAttrOfType<mlir::triton::xla::TmaDescriptorAttr>(\n+                idx, \"tt.tma_descriptor\")) {\n+      TF_ASSIGN_OR_RETURN(\n+          auto tma_desc,\n+          CreateTmaDescriptor(attr.getGlobalShape(), attr.getTileShape(),\n+                              attr.getTileStrides(), attr.getLayout(),\n+                              attr.getElementByteSize(),\n+                              attr.getSwizzleMode().getValue()));\n+      tma_metadata.arg_index_to_tma_info.insert({idx, tma_desc});\n+    }\n+  }\n+  return tma_metadata;\n+}\n+\n+std::vector<llvm::Metadata*> ExtractNvvmAnnotations(\n+    llvm::Module* ll_triton_module) {\n+  std::vector<llvm::Metadata*> captured_nvvm_annotations;\n+  llvm::NamedMDNode* nvvm_annotations =\n+      ll_triton_module->getNamedMetadata(\"nvvm.annotations\");\n+  if (nvvm_annotations) {\n+    for (llvm::MDNode* operand : nvvm_annotations->operands()) {\n+      captured_nvvm_annotations.push_back(operand);\n+    }\n+    ll_triton_module->eraseNamedMetadata(nvvm_annotations);\n+  }\n+  return captured_nvvm_annotations;\n+}\n+\n+}  // namespace xla::gpu::triton"
        },
        {
            "sha": "3e20093dd63bb99346daf0bb9a79cd6777757b2b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/lowering_util.h",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util.h?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -0,0 +1,46 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_CODEGEN_TRITON_LOWERING_UTIL_H_\n+#define XLA_BACKENDS_GPU_CODEGEN_TRITON_LOWERING_UTIL_H_\n+\n+#include <vector>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/IR/Metadata.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+\n+namespace xla::gpu::triton {\n+\n+// Extracts thread dimensions from Triton module attributes.\n+absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n+    mlir::ModuleOp triton_module, mlir::LLVM::LLVMFuncOp func_op);\n+\n+// Extracts TMA metadata information from LLVM generated by the Triton\n+// compilation. The underlying map will be empty if TMA is not used.\n+absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n+    mlir::LLVM::LLVMFuncOp func_op);\n+\n+// Extracts NVVM annotations from the Triton module.\n+std::vector<llvm::Metadata*> ExtractNvvmAnnotations(\n+    llvm::Module* ll_triton_module);\n+\n+}  // namespace xla::gpu::triton\n+\n+#endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_LOWERING_UTIL_H_"
        },
        {
            "sha": "50e2d57c3970d7097fdb983d828aa9f69de6cb11",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/lowering_util_test.cc",
            "status": "renamed",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Flowering_util_test.cc?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -12,7 +12,8 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n-#include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+\n+#include \"xla/backends/gpu/codegen/triton/lowering_util.h\"\n \n #include <string>\n ",
            "previous_filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers_test.cc"
        },
        {
            "sha": "552724f1cff6e3608f7f6cf7da94ae542b4f477a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_xtile_pass.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a4a157d5b0fad310458438b29a52f4b689f4244/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc?ref=3a4a157d5b0fad310458438b29a52f4b689f4244",
            "patch": "@@ -72,7 +72,7 @@ llvm::SmallVector<mlir::Type> GetTransformedArgTypes(\n   for (const auto& arg : entry_op.getBufferArgs()) {\n     mlir::MemRefType memref_type = mlir::cast<mlir::MemRefType>(arg.getType());\n     arg_types.push_back(\n-        ::xla::gpu::triton::GetGlobalPointerType(memref_type.getElementType()));\n+        ttir::getPointerTypeToElement(memref_type.getElementType()));\n   }\n   mlir::TypeRange opaque_args(entry_op.getOpaqueArgs());\n   arg_types.append(opaque_args.begin(), opaque_args.end());\n@@ -131,8 +131,8 @@ absl::StatusOr<llvm::SmallVector<int64_t>> getPermutationMinorToMajor(\n \n MemrefToPtrOp CreateMemrefToPtr(mlir::OpBuilder& builder,\n                                 mlir::TypedValue<mlir::MemRefType> memref) {\n-  PointerType ptr_type = ::xla::gpu::triton::GetGlobalPointerType(\n-      memref.getType().getElementType());\n+  mlir::Type ptr_type =\n+      ttir::getPointerTypeToElement(memref.getType().getElementType());\n   return MemrefToPtrOp::create(builder, memref.getLoc(), ptr_type, memref);\n }\n "
        }
    ],
    "stats": {
        "total": 448,
        "additions": 217,
        "deletions": 231
    }
}