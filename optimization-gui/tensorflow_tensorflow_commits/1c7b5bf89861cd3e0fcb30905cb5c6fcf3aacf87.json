{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 828854549",
    "sha": "1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
    "files": [
        {
            "sha": "689e6ca3f7bf41a0eacfd91830d1edce1fde094f",
            "filename": "tensorflow/compiler/tf2xla/kernels/stateless_random_ops_v2.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fstateless_random_ops_v2.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fstateless_random_ops_v2.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fstateless_random_ops_v2.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -128,7 +128,7 @@ class StatelessRandomUniformOp : public XlaOpKernel {\n \n  private:\n   DataType dtype_;\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   StatelessRandomUniformOp(const StatelessRandomUniformOp&) = delete;\n   void operator=(const StatelessRandomUniformOp&) = delete;\n@@ -177,7 +177,7 @@ class StatelessRandomUniformIntOp : public XlaOpKernel {\n \n  private:\n   DataType dtype_;\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   StatelessRandomUniformIntOp(const StatelessRandomUniformIntOp&) = delete;\n   void operator=(const StatelessRandomUniformIntOp&) = delete;\n@@ -225,7 +225,7 @@ class StatelessRandomUniformFullIntOp : public XlaOpKernel {\n \n  private:\n   DataType dtype_;\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   StatelessRandomUniformFullIntOp(const StatelessRandomUniformFullIntOp&) =\n       delete;\n@@ -295,7 +295,7 @@ class StatelessRandomNormalOp : public XlaOpKernel {\n \n  private:\n   DataType dtype_;\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   StatelessRandomNormalOp(const StatelessRandomNormalOp&) = delete;\n   void operator=(const StatelessRandomNormalOp&) = delete;\n@@ -330,7 +330,7 @@ class StatelessTruncatedNormalOp : public XlaOpKernel {\n \n  private:\n   DataType dtype_;\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   StatelessTruncatedNormalOp(const StatelessTruncatedNormalOp&) = delete;\n   void operator=(const StatelessTruncatedNormalOp&) = delete;\n@@ -369,7 +369,7 @@ class GetKeyCounterOp : public XlaOpKernel {\n   }\n \n  private:\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   GetKeyCounterOp(const GetKeyCounterOp&) = delete;\n   void operator=(const GetKeyCounterOp&) = delete;\n@@ -392,7 +392,7 @@ class GetAlgOp : public XlaOpKernel {\n   }\n \n  private:\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   GetAlgOp(const GetAlgOp&) = delete;\n   void operator=(const GetAlgOp&) = delete;\n@@ -430,7 +430,7 @@ class GetKeyCounterAlgOp : public XlaOpKernel {\n   }\n \n  private:\n-  string device_type_string_;\n+  std::string device_type_string_;\n \n   GetKeyCounterAlgOp(const GetKeyCounterAlgOp&) = delete;\n   void operator=(const GetKeyCounterAlgOp&) = delete;"
        },
        {
            "sha": "1b44d1e07c4bd829a1da77fac38b67b0e14d48d2",
            "filename": "tensorflow/compiler/tf2xla/kernels/strided_slice_op.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fstrided_slice_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fstrided_slice_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fstrided_slice_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -142,7 +142,7 @@ class StridedSliceOp : public XlaOpKernel {\n       // Pad input to 2x to avoid OOB access.\n       slice = xla::Pad(slice, xla::Zero(ctx->builder(), ctx->input_xla_type(0)),\n                        padding_config);\n-      for (int64 i = 0; i < result_dims_are_dynamic.size(); ++i) {\n+      for (int64_t i = 0; i < result_dims_are_dynamic.size(); ++i) {\n         if (result_dims_are_dynamic[i]) {\n           slice = xla::RemoveDynamicDimension(slice, i);\n         }\n@@ -178,7 +178,7 @@ class StridedSliceOp : public XlaOpKernel {\n           // Can't infer a lower bound.\n           return false;\n         }\n-        return lower_bound->Get<int32>({}) >= 0;\n+        return lower_bound->Get<int32_t>({}) >= 0;\n       };\n       if (begin_mask) {\n         begin_index = zero;\n@@ -220,7 +220,7 @@ class StridedSliceOp : public XlaOpKernel {\n     // size 1 dims of a shape.\n     slice = xla::Reshape(slice, final_shape.dim_sizes());\n     for (int64_t i = 0; i < final_shape.dims(); ++i) {\n-      int64 processing_shape_dim = shape_spec.output_to_processing_mapping[i];\n+      int64_t processing_shape_dim = shape_spec.output_to_processing_mapping[i];\n       // If processing_shape_dim is -1, it means the output dimension was newly\n       // added by new_axis_mask_, which doesn't show up in input.\n       if (processing_shape_dim != -1) {\n@@ -341,9 +341,9 @@ class StridedSliceOp : public XlaOpKernel {\n         int64_t sparse_index = shape_spec.output_to_sparse_mapping[i];\n         bool end_is_dynamic =\n             sparse_index == -1 ? false : ends_are_dynamic[sparse_index];\n-        bool backward_slice = sparse_index == -1\n-                                  ? false\n-                                  : end_literal.Get<int32>({sparse_index}) < 0;\n+        bool backward_slice =\n+            sparse_index == -1 ? false\n+                               : end_literal.Get<int32_t>({sparse_index}) < 0;\n         if (input_is_dynamic || end_is_dynamic) {\n           OP_REQUIRES(\n               ctx, strides[input_index] == 1,\n@@ -363,8 +363,8 @@ class StridedSliceOp : public XlaOpKernel {\n                             \"sized slice with dynamic negative index %lld. \"));\n             operand_size = xla::Add(\n                 operand_size,\n-                xla::ConstantR0<int32>(ctx->builder(),\n-                                       end_literal.Get<int32>({sparse_index})));\n+                xla::ConstantR0<int32_t>(\n+                    ctx->builder(), end_literal.Get<int32_t>({sparse_index})));\n           } else {\n             // The end of slice with dynamic slice size is the min of operand\n             // shape and slice size. E.g., t[:end_size], result size is\n@@ -376,13 +376,13 @@ class StridedSliceOp : public XlaOpKernel {\n                                       {});\n             } else {\n               end_size =\n-                  xla::ConstantR0<int32>(ctx->builder(), end[input_index]);\n+                  xla::ConstantR0<int32_t>(ctx->builder(), end[input_index]);\n             }\n             operand_size = xla::Min(operand_size, end_size);\n           }\n           slice = xla::SetDimensionSize(\n               slice,\n-              xla::Sub(operand_size, xla::ConstantR0<int32>(\n+              xla::Sub(operand_size, xla::ConstantR0<int32_t>(\n                                          ctx->builder(), begin[input_index])),\n               i);\n         }\n@@ -397,8 +397,8 @@ class StridedSliceOp : public XlaOpKernel {\n   }\n \n  private:\n-  int32 begin_mask_, end_mask_;\n-  int32 ellipsis_mask_, new_axis_mask_, shrink_axis_mask_;\n+  int32_t begin_mask_, end_mask_;\n+  int32_t ellipsis_mask_, new_axis_mask_, shrink_axis_mask_;\n   DataType index_type_;\n };\n \n@@ -634,8 +634,8 @@ class StridedSliceGradOp : public XlaOpKernel {\n   }\n \n  private:\n-  int32 begin_mask_, end_mask_;\n-  int32 ellipsis_mask_, new_axis_mask_, shrink_axis_mask_;\n+  int32_t begin_mask_, end_mask_;\n+  int32_t ellipsis_mask_, new_axis_mask_, shrink_axis_mask_;\n   DataType index_type_;\n };\n \n@@ -751,8 +751,8 @@ class StridedSliceAssignOp : public XlaOpKernel {\n   }\n \n  private:\n-  int32 begin_mask_, end_mask_;\n-  int32 ellipsis_mask_, new_axis_mask_, shrink_axis_mask_;\n+  int32_t begin_mask_, end_mask_;\n+  int32_t ellipsis_mask_, new_axis_mask_, shrink_axis_mask_;\n   DataType index_type_;\n   DataType dtype_;\n };"
        },
        {
            "sha": "e89c3e3b4f837be9c0a16a81231bcfa7036199c9",
            "filename": "tensorflow/compiler/tf2xla/kernels/tensor_array_ops.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_array_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_array_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_array_ops.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -94,7 +94,7 @@ absl::Status MaybeInitializeTensorArray(xla::XlaBuilder* builder,\n \n // Checks that the TensorArray 'resource' has been initialized, and has type\n // 'dtype'. Sets 'shape' to the shape\n-absl::Status CheckTensorArrayIsInitialized(const string& op_name,\n+absl::Status CheckTensorArrayIsInitialized(const std::string& op_name,\n                                            const XlaResource* resource,\n                                            DataType dtype) {\n   if (resource->kind() != XlaResource::kTensorArray) {\n@@ -184,7 +184,7 @@ class TensorArrayOp : public XlaOpKernel {\n  private:\n   PartialTensorShape element_shape_;\n   DataType dtype_;\n-  string tensor_array_name_;\n+  std::string tensor_array_name_;\n \n   TensorArrayOp(const TensorArrayOp&) = delete;\n   void operator=(const TensorArrayOp&) = delete;\n@@ -218,7 +218,7 @@ class TensorArrayWriteOp : public XlaOpKernel {\n \n     // start_indices of the DynamicUpdateSlice are [index, 0, 0, ..., 0].\n     std::vector<xla::XlaOp> start_indices(elem_shape.dims() + 1,\n-                                          xla::ConstantR0<int32>(b, 0));\n+                                          xla::ConstantR0<int32_t>(b, 0));\n     start_indices[0] = index;\n \n     TensorShape slice_shape = elem_shape;\n@@ -270,7 +270,7 @@ class TensorArrayReadOp : public XlaOpKernel {\n \n     // start_indices of the DynamicSlice are [index, 0, 0, ..., 0].\n     std::vector<xla::XlaOp> start_indices(ta_shape.dims(),\n-                                          xla::ConstantR0<int32>(b, 0));\n+                                          xla::ConstantR0<int32_t>(b, 0));\n     start_indices[0] = index;\n \n     auto slice_shape = ta_shape.dim_sizes();\n@@ -430,7 +430,7 @@ class TensorArrayScatterOp : public XlaOpKernel {\n         // start_indices of the DynamicUpdateSlice are [index, 0, 0, ..., 0].\n         auto index = xla::Reshape(xla::Slice(indices, {i}, {i + 1}, {1}), {});\n         std::vector<xla::XlaOp> start_indices(elem_shape.dims() + 1,\n-                                              xla::ConstantR0<int32>(b, 0));\n+                                              xla::ConstantR0<int32_t>(b, 0));\n         start_indices[0] = index;\n         ta = DynamicAddSlice(b, ta, slice, slice_dims, start_indices, dtype_);\n       }\n@@ -570,7 +570,8 @@ class TensorArraySizeOp : public XlaOpKernel {\n     XlaResource* var;\n     OP_REQUIRES_OK(ctx, ctx->GetResourceInput(0, &var));\n     Tensor size_tensor(DT_INT32, {});\n-    size_tensor.scalar<int32>()() = static_cast<int32>(var->max_array_size());\n+    size_tensor.scalar<int32_t>()() =\n+        static_cast<int32_t>(var->max_array_size());\n     ctx->SetConstantOutput(0, size_tensor);\n   }\n \n@@ -609,7 +610,7 @@ class TensorArrayGradOp : public XlaOpKernel {\n   }\n \n  private:\n-  string source_;\n+  std::string source_;\n \n   TensorArrayGradOp(const TensorArrayGradOp&) = delete;\n   void operator=(const TensorArrayGradOp&) = delete;"
        },
        {
            "sha": "f128c96c570e6c6223fe67a47f23a0add75ae01f",
            "filename": "tensorflow/compiler/tf2xla/kernels/tensor_list_ops.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_list_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_list_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_list_ops.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -70,7 +70,7 @@ absl::StatusOr<std::vector<std::vector<xla::XlaOp>>> GetTensorListDynamicDims(\n     dynamic_dims.push_back(ctx->Input(1));\n   } else {\n     dynamic_dims.push_back(\n-        xla::ConstantR0<int32>(ctx->builder(), num_elements));\n+        xla::ConstantR0<int32_t>(ctx->builder(), num_elements));\n   }\n   for (int64_t dim = 0; dim < element_shape.dimensions().size(); ++dim) {\n     if (dims_are_dynamic[dim]) {\n@@ -80,7 +80,7 @@ absl::StatusOr<std::vector<std::vector<xla::XlaOp>>> GetTensorListDynamicDims(\n       dynamic_dims.push_back(dynamic_dim_size);\n     } else {\n       dynamic_dims.push_back(\n-          xla::ConstantR0<int32>(ctx->builder(), dynamic_sizes[dim]));\n+          xla::ConstantR0<int32_t>(ctx->builder(), dynamic_sizes[dim]));\n     }\n   }\n   list_dynamic_dims.push_back(std::move(dynamic_dims));\n@@ -191,7 +191,7 @@ class TensorListReserveOp : public XlaOpKernel {\n       OP_REQUIRES_OK(\n           ctx,\n           SetTensorListPushIndex(\n-              new_list, xla::ConstantR0<int32>(ctx->builder(), num_elements),\n+              new_list, xla::ConstantR0<int32_t>(ctx->builder(), num_elements),\n               &result));\n       ctx->SetTensorListOutput(0, result);\n       return;\n@@ -324,13 +324,13 @@ class TensorListElementShapeOp : public XlaOpKernel {\n         ctx->SetOutput(0, xla::ConstantR1<int64_t>(b, list_shape.dimensions()));\n         break;\n       case DT_INT32: {\n-        std::vector<int32> size;\n+        std::vector<int32_t> size;\n         const auto& dimensions = list_shape.dimensions();\n         size.reserve(dimensions.size());\n         for (int64_t s : dimensions) {\n           size.push_back(s);\n         }\n-        ctx->SetOutput(0, xla::ConstantR1<int32>(b, size));\n+        ctx->SetOutput(0, xla::ConstantR1<int32_t>(b, size));\n         break;\n       }\n       default:"
        },
        {
            "sha": "0a7297456fce8d99f777568a8c709394b0f93904",
            "filename": "tensorflow/compiler/tf2xla/kernels/tensor_list_utils.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_list_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_list_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftensor_list_utils.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -393,7 +393,7 @@ absl::Status ExecuteTensorListPushBack(xla::XlaOp list, xla::XlaOp element,\n \n       std::vector<xla::XlaOp> start_indices(\n           element_part_shape.dimensions().size() + 1,\n-          xla::ConstantR0<int32>(b, 0));\n+          xla::ConstantR0<int32_t>(b, 0));\n       start_indices[0] = push_index;\n \n       xla::XlaOp list_part = xla::GetTupleElement(list, i);\n@@ -409,7 +409,7 @@ absl::Status ExecuteTensorListPushBack(xla::XlaOp list, xla::XlaOp element,\n     xla::XlaOp update = xla::Reshape(element, element_dims);\n \n     std::vector<xla::XlaOp> start_indices(element_shape.dimensions().size() + 1,\n-                                          xla::ConstantR0<int32>(b, 0));\n+                                          xla::ConstantR0<int32_t>(b, 0));\n     start_indices[0] = push_index;\n \n     xla::XlaOp list_part = xla::GetTupleElement(list, 0);\n@@ -418,7 +418,7 @@ absl::Status ExecuteTensorListPushBack(xla::XlaOp list, xla::XlaOp element,\n     result_parts.push_back(updated_list_part);\n   }\n \n-  xla::XlaOp updated_push_index = push_index + xla::ConstantR0<int32>(b, 1);\n+  xla::XlaOp updated_push_index = push_index + xla::ConstantR0<int32_t>(b, 1);\n   result_parts.push_back(updated_push_index);\n \n   *result = xla::Tuple(b, result_parts);\n@@ -441,14 +441,14 @@ absl::Status ExecuteTensorListPopBack(xla::XlaOp list, xla::XlaOp* list_result,\n   TF_ASSIGN_OR_RETURN(xla::Shape list_shape, b->GetShape(list));\n   int list_tuple_size = xla::ShapeUtil::TupleElementCount(list_shape);\n   xla::XlaOp push_index = xla::GetTupleElement(list, list_tuple_size - 1);\n-  push_index = push_index - xla::ConstantR0<int32>(b, 1);\n+  push_index = push_index - xla::ConstantR0<int32_t>(b, 1);\n \n   std::vector<xla::XlaOp> list_result_parts, element_result_parts;\n   for (int i = 0; i < list_tuple_size - 1; i++) {\n     const xla::Shape& list_part_shape =\n         xla::ShapeUtil::GetTupleElementShape(list_shape, i);\n     std::vector<xla::XlaOp> start_indices(list_part_shape.dimensions().size(),\n-                                          xla::ConstantR0<int32>(b, 0));\n+                                          xla::ConstantR0<int32_t>(b, 0));\n     start_indices[0] = push_index;\n \n     std::vector<int64_t> slice_shape =\n@@ -496,7 +496,7 @@ absl::Status ExecuteTensorListSetItem(xla::XlaOp list, xla::XlaOp index,\n   xla::XlaOp update = xla::Reshape(element, element_dims);\n \n   std::vector<xla::XlaOp> start_indices(element_shape.dimensions().size() + 1,\n-                                        xla::ConstantR0<int32>(b, 0));\n+                                        xla::ConstantR0<int32_t>(b, 0));\n   start_indices[0] = index;\n \n   xla::XlaOp list_part = xla::GetTupleElement(list, 0);\n@@ -550,7 +550,7 @@ absl::Status ExecuteTensorListGetItem(xla::XlaOp list, xla::XlaOp index,\n   const xla::Shape& buffer_shape =\n       xla::ShapeUtil::GetTupleElementShape(list_shape, 0);\n   std::vector<xla::XlaOp> start_indices(buffer_shape.dimensions().size(),\n-                                        xla::ConstantR0<int32>(b, 0));\n+                                        xla::ConstantR0<int32_t>(b, 0));\n   start_indices[0] = index;\n \n   std::vector<int64_t> slice_shape =\n@@ -585,7 +585,7 @@ absl::Status ExecuteTensorListFromTensor(int push_index, xla::XlaOp tensor,\n   }\n \n   std::vector<xla::XlaOp> result_parts{tensor,\n-                                       xla::ConstantR0<int32>(b, push_index)};\n+                                       xla::ConstantR0<int32_t>(b, push_index)};\n   *result = xla::Tuple(b, result_parts);\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "9c4e0b63490205e8774da29b7317f89e43605ac2",
            "filename": "tensorflow/compiler/tf2xla/kernels/transpose_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftranspose_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftranspose_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Ftranspose_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -137,7 +137,7 @@ class InvertPermutationOp : public XlaOpKernel {\n     absl::Status status;\n     switch (dtype) {\n       case DT_INT32:\n-        InvertPermutation<int32>(ctx);\n+        InvertPermutation<int32_t>(ctx);\n         break;\n       case DT_INT64:\n         InvertPermutation<int64_t>(ctx);"
        },
        {
            "sha": "1d487f70d09d212b426f547ca5f0e38cb762a447",
            "filename": "tensorflow/compiler/tf2xla/kernels/unary_ops_composition.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Funary_ops_composition.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Funary_ops_composition.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Funary_ops_composition.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -36,7 +36,7 @@ namespace tensorflow {\n namespace {\n \n using XlaUnaryOpGenerator = std::function<xla::XlaOp(xla::XlaOp)>;\n-using XlaOpGeneratorMap = absl::flat_hash_map<string, XlaUnaryOpGenerator>;\n+using XlaOpGeneratorMap = absl::flat_hash_map<std::string, XlaUnaryOpGenerator>;\n \n void PopulateXlaOpGeneratorMap(XlaOpGeneratorMap* op_generator_map) {\n   auto add_xla_op_generator = [&](std::string name,\n@@ -120,7 +120,7 @@ class UnaryOpsCompositionOp : public XlaOpKernel {\n   }\n \n  private:\n-  std::vector<string> op_names_;\n+  std::vector<std::string> op_names_;\n };\n \n REGISTER_XLA_OP(Name(\"_UnaryOpsComposition\"), UnaryOpsCompositionOp);"
        },
        {
            "sha": "c9ddab9efb6e22c1edac9a9797785d6869e8bb95",
            "filename": "tensorflow/compiler/tf2xla/kernels/variable_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fvariable_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fvariable_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fvariable_ops.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -165,7 +165,7 @@ class ResourceGatherOp : public XlaOpKernel {\n   }\n \n  private:\n-  int32 batch_dims_;\n+  int32_t batch_dims_;\n };\n REGISTER_XLA_OP(Name(\"ResourceGather\"), ResourceGatherOp);\n "
        },
        {
            "sha": "57821f74e97024cac5f6ca61eea9d0badc264932",
            "filename": "tensorflow/compiler/tf2xla/kernels/while_op.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fwhile_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fwhile_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fwhile_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -449,7 +449,8 @@ void XlaWhileOp::Compile(XlaOpKernelContext* ctx) {\n \n       // Add any TensorArray gradients touched by the body to the enclosing\n       // graph.\n-      for (const string& grad_source : update.tensor_array_gradients_accessed) {\n+      for (const std::string& grad_source :\n+           update.tensor_array_gradients_accessed) {\n         VLOG(4) << \"TensorArray \" << resource->name() << \" accessed gradient \"\n                 << grad_source;\n         XlaResource* gradient;\n@@ -553,7 +554,7 @@ void XlaWhileOp::Compile(XlaOpKernelContext* ctx) {\n       // Set token input for this \"while\" op.\n       std::vector<xla::XlaOp> token_inputs;\n       token_inputs.reserve(token_input_nodes_.size());\n-      for (const string& node_name : token_input_nodes_) {\n+      for (const std::string& node_name : token_input_nodes_) {\n         auto token_or = compiler->GetNodeToken(node_name);\n         OP_REQUIRES_OK(ctx, token_or.status());\n         token_inputs.push_back(token_or.value());\n@@ -590,7 +591,7 @@ void XlaWhileOp::Compile(XlaOpKernelContext* ctx) {\n           } else {\n             int32_t dim_size = shape.dimensions(0);\n             dynamic_dims.push_back(\n-                xla::ConstantR0<int32>(ctx->builder(), dim_size));\n+                xla::ConstantR0<int32_t>(ctx->builder(), dim_size));\n           }\n \n           // Set dynamic dimension size to 0 for element value. Inside the while"
        },
        {
            "sha": "b1937c14f0bebcbff5a4af02920a1fff6bfb0342",
            "filename": "tensorflow/compiler/tf2xla/kernels/while_op.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fwhile_op.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fwhile_op.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fwhile_op.h?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -61,8 +61,8 @@ class XlaWhileOp : public XlaOpKernel {\n   NameAttrList cond_name_attr_;\n   NameAttrList body_name_attr_;\n   bool has_token_input_output_;\n-  std::vector<string> token_input_nodes_;\n-  string original_node_name_;\n+  std::vector<std::string> token_input_nodes_;\n+  std::string original_node_name_;\n   // Whether to propagate compile time consts into the loop body.\n   // This is not supported by default now since it may cause HBM memory\n   // overheads."
        },
        {
            "sha": "e06c0b09ba9938f5e99df76498bca0ef6fdf7784",
            "filename": "tensorflow/compiler/tf2xla/kernels/xla_call_module_op.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_call_module_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_call_module_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_call_module_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -166,13 +166,13 @@ class XlaCallModuleOp : public XlaOpKernel {\n   explicit XlaCallModuleOp(OpKernelConstruction *ctx) : XlaOpKernel(ctx) {\n     int version;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"version\", &version));\n-    string module_str;\n+    std::string module_str;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"module\", &module_str));\n     std::vector<PartialTensorShape> expected_output_shapes;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"Sout\", &expected_output_shapes));\n     std::vector<DataType> expected_output_dtypes;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"Tout\", &expected_output_dtypes));\n-    std::vector<string> dim_args_spec;\n+    std::vector<std::string> dim_args_spec;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"dim_args_spec\", &dim_args_spec));\n     OP_REQUIRES(ctx, dim_args_spec.empty(),\n                 absl::UnimplementedError(\n@@ -183,9 +183,9 @@ class XlaCallModuleOp : public XlaOpKernel {\n                     \"The size of Sout (\", expected_output_shapes.size(),\n                     \") must match the size of Tout (\",\n                     expected_output_dtypes.size(), \")\")));\n-    std::vector<string> disabled_checks;\n+    std::vector<std::string> disabled_checks;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"disabled_checks\", &disabled_checks));\n-    std::vector<string> platforms;\n+    std::vector<std::string> platforms;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"platforms\", &platforms));\n     // TODO(necula): change this to OP_REQUIRES_OK when 6 months have passed\n     // since we added the function_list and has_token_input_output\n@@ -222,7 +222,7 @@ class XlaCallModuleOp : public XlaOpKernel {\n                                })\n               << \"])\";\n     }\n-    string compilation_device_type = ctx->device_type().type_string();\n+    std::string compilation_device_type = ctx->device_type().type_string();\n     compilation_platform_ = \"\";\n     if (compilation_device_type == DEVICE_CPU_XLA_JIT) {\n       compilation_platform_ = \"CPU\";\n@@ -293,7 +293,7 @@ class XlaCallModuleOp : public XlaOpKernel {\n     xla::XlaOp token_input;\n     if (!op_token_input_nodes_.empty()) {\n       std::vector<xla::XlaOp> token_inputs;\n-      for (const string &node_name : op_token_input_nodes_) {\n+      for (const std::string& node_name : op_token_input_nodes_) {\n         auto token = compiler->GetNodeToken(node_name);\n         OP_REQUIRES_OK(ctx, token.status());\n         token_inputs.push_back(token.value());"
        },
        {
            "sha": "99a0ec6d9e38ddbc40aad3ccf22a7e8501d5f7e7",
            "filename": "tensorflow/compiler/tf2xla/kernels/xla_custom_call_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_custom_call_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_custom_call_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_custom_call_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -55,8 +55,8 @@ class XlaCustomCallOp : public XlaOpKernel {\n   }\n \n  private:\n-  string target_name_;\n-  string backend_config_;\n+  std::string target_name_;\n+  std::string backend_config_;\n   DataType output_type_;\n   TensorShape output_shape_;\n };"
        },
        {
            "sha": "6889c093a112012eb8ebc343b2e66532916bb056",
            "filename": "tensorflow/compiler/tf2xla/kernels/xla_dequantize_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_dequantize_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_dequantize_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_dequantize_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -42,15 +42,15 @@ class XlaDequantizeOp : public XlaOpKernel {\n     xla::QuantizedRange range(min_range_, max_range_);\n \n     xla::XlaOp output =\n-        xla::Dequantize<uint8>(input, range, mode_, transpose_output_);\n+        xla::Dequantize<uint8_t>(input, range, mode_, transpose_output_);\n     context->SetOutput(0, output);\n   }\n \n  private:\n   float min_range_;\n   float max_range_;\n   bool transpose_output_;\n-  string mode_;\n+  std::string mode_;\n   XlaDequantizeOp(const XlaDequantizeOp&) = delete;\n   void operator=(const XlaDequantizeOp&) = delete;\n };"
        },
        {
            "sha": "f77cb46c44de8cc567c63db3f89eb8d81cfe2b9f",
            "filename": "tensorflow/compiler/tf2xla/kernels/xla_dot_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_dot_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_dot_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_dot_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -34,12 +34,12 @@ namespace {\n class XlaDotOp : public XlaOpKernel {\n  public:\n   explicit XlaDotOp(OpKernelConstruction* context) : XlaOpKernel(context) {\n-    string dnums_attr;\n+    std::string dnums_attr;\n     OP_REQUIRES_OK(context, context->GetAttr(\"dimension_numbers\", &dnums_attr));\n     OP_REQUIRES(\n         context, dnums_.ParsePartialFromString(dnums_attr),\n         errors::InvalidArgument(\"Error parsing convolution dimension numbers\"));\n-    string precision_config_attr;\n+    std::string precision_config_attr;\n     OP_REQUIRES_OK(\n         context, context->GetAttr(\"precision_config\", &precision_config_attr));\n     OP_REQUIRES("
        },
        {
            "sha": "7765de131e865c2789c816b5658f2da3f506ea53",
            "filename": "tensorflow/compiler/tf2xla/kernels/xla_self_adjoint_eig_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_self_adjoint_eig_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_self_adjoint_eig_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_self_adjoint_eig_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -41,7 +41,7 @@ class XlaSelfAdjointEigOp : public XlaOpKernel {\n \n  private:\n   bool lower_;\n-  int32 max_iter_;\n+  int32_t max_iter_;\n   float epsilon_;\n };\n "
        },
        {
            "sha": "6639c8003e1a15cc00005071ff3babfb4a298030",
            "filename": "tensorflow/compiler/tf2xla/kernels/xla_svd_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_svd_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_svd_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_svd_op.cc?ref=1c7b5bf89861cd3e0fcb30905cb5c6fcf3aacf87",
            "patch": "@@ -37,7 +37,7 @@ class XlaSvdOp : public XlaOpKernel {\n   explicit XlaSvdOp(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_iter\", &max_iter_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"epsilon\", &epsilon_));\n-    string precision_config_attr;\n+    std::string precision_config_attr;\n     OP_REQUIRES_OK(ctx,\n                    ctx->GetAttr(\"precision_config\", &precision_config_attr));\n     OP_REQUIRES(ctx,\n@@ -57,7 +57,7 @@ class XlaSvdOp : public XlaOpKernel {\n   }\n \n  private:\n-  int32 max_iter_;\n+  int32_t max_iter_;\n   float epsilon_;\n   xla::PrecisionConfig precision_config_;\n };"
        }
    ],
    "stats": {
        "total": 138,
        "additions": 70,
        "deletions": 68
    }
}