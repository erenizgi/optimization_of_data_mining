{
    "author": "sfvaroglu",
    "message": "PR #32297: [XLA:GPU] Add host offloading support to collective pipeliner\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32297\n\nüìù Summary of Changes\n1. Add host offloading support to collective pipeliner to enable memory optimization for collective operations by offloading intermediate results to host memory.\n\n2. Add dynamic variable detection for CollectivePipeliner-transformed loops to enable FusionDynamicMemcpyRewriter optimization, allowing better memory management for dynamically shaped computations.\n\nüéØ Justification\nWhen host memory offloading is enabled, activations are copied asynchronously; device to host in the forward pass and back for the backward pass. These copies often happen at loop boundaries, limiting overlap with compute. This PR adds host offloading support to collective pipeliner so that we can move these copies to nearby iterations to reduce dependencies and improve overlap. This feature is enabled via `xla_gpu_enable_pipelined_host_offloading` flag.\n\nüöÄ Kind of Contribution\n‚ö°Ô∏è Performance Improvement, ‚ú® New Feature\n\nüìä Benchmark (for Performance Improvements)\nThe changes in this PR are gated by a flag. The HLOs under `xla/tools/benchmarks/hlo` don't affect offloading behavior, so we shouldn't expect any impact.\n\nüß™ Unit Tests:\nAdded unit tests.\n\nüß™ Execution Tests:\nSaw ~12% speedup on GB200 with llama3-8b, fsdp=8 using MLP offloading plus collective pipelining, compared to just offloading.\nCopybara import of the project:\n\n--\n996c7cfc0aa847fd49b72d839a69195d4e88f3b4 by Sevin Varoglu <svaroglu@nvidia.com>:\n\n[XLA:GPU] Add host offloading support to collective pipeliner\n\n1. Add host offloading support to collective pipeliner to enable memory\n   optimization for collective operations by offloading intermediate results\n   to host memory.\n\n2. Add dynamic variable detection for CollectivePipeliner-transformed loops\n   to enable FusionDynamicMemcpyRewriter optimization, allowing better\n   memory management for dynamically shaped computations.\n\n--\n2f402d6af17e0590620fdc74bbe5af55ab1be0ec by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd HLO file\n\n--\nb98ed4c0194c4c387629053bd5d8608fd5846442 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd fix to preserve dynamic_variable_tuple_indices after loop unrolling\n\n--\nf599c596b4a53e6deb9adc7a01822c59092ec2aa by Sevin Varoglu <svaroglu@nvidia.com>:\n\nFix format\n\n--\n5e6f58087ad0561aff1a53bbad8267b1865447db by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd review feedback\n\n--\n0b602e197b835dfc6e3520fcf5a0212f26e8cb37 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nFix clang format error\n\n--\n506f3b29e70af57c0862bdefaca2f544fe44a333 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nIncorporate review feedback\n\n--\n9bdb3d1d60b38e9e0fd46d729a422f850e5ca32e by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAddress review feedback\n\n--\n599ebe31aea7de80f7b647b9d0d5e7fe8b8beb69 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nFix format\n\n--\ne0b67de9064f331a02640d9a8f78125c2ad35f1c by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAddress feedback\n\n--\n190ae1ce1c0e69e6116f1b53dd5cdee83bf4e7f4 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd tests\n\n--\nd9a68771aa14ec880ef4065018c2cb949a192b00 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nFix compile error\n\n--\nd4c033184fc95701154422853e3a4f4abf6a2cfb by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd find_dynamic_slice_operand\n\n--\nf53d220213028673304a44027dfa96594546ab3d by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd postprocess_transformed_while_loop\n\n--\n02b44c60bf6a91eb1938e43a6de7e7d9127fadaa by Sevin Varoglu <svaroglu@nvidia.com>:\n\nRename as additional_chain_start_op_finder, fix tests\n\n--\n10c716382cc95fff0bb60dfe4c6a5ba6083bf130 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nRemove visited_set\n\n--\n504af7c277bec4cc88e7f2d5940bc7fe3746369c by Sevin Varoglu <svaroglu@nvidia.com>:\n\nFix format\n\n--\ne2f0c909d098400a22466de56952476610b9d08c by Sevin Varoglu <svaroglu@nvidia.com>:\n\nUpdate BUILD\n\n--\n73da8624fff1bda792e2f1c0360fb6fa50e9ec14 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nMove flag\n\n--\nd1e3ddbc9cb196685b3a3182db191ee6d11cce18 by Sevin Varoglu <svaroglu@nvidia.com>:\n\nIncrement\n\n--\nc410305c267203896d85aff9be2e08adf28916ac by Sevin Varoglu <svaroglu@nvidia.com>:\n\nRemove headers\n\n--\n38e1d58aa53b96fc9e000bbbe3bb0b8c20adb9ec by Sevin Varoglu <svaroglu@nvidia.com>:\n\nAdd header\n\nMerging this change closes #32297\n\nPiperOrigin-RevId: 845155965",
    "sha": "96ed126a594735c8106699b4f9c2ac8f74798242",
    "files": [
        {
            "sha": "e2fff15a4cdcb1f64d88df053a3256acaf5c52ae",
            "filename": "third_party/xla/docs/flags_guidance.md",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fflags_guidance.md?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -81,19 +81,20 @@ data-parallel collectives (`xla_gpu_enable_pipelined_all_gather`,\n Hopper/Blackwell (`xla_gpu_enable_analytical_sol_latency_estimator`). See\n [GPU Effort Levels](https://openxla.org/xla/effort_levels) for details.\n \n-| Flag | Type | Notes |\n-| :---- | :---- | :----- |\n-| `xla_gpu_enable_latency_hiding_scheduler` | Boolean (true/false) |This flag enables latency hiding schedulers to overlap asynchronous communication with computation efficiently. The default value is False. |\n-| `xla_gpu_enable_analytical_sol_latency_estimator` | Boolean (true/false) | Enables platform specific scheduling decisions, which in turn improve compute-communication overlap. The default value is true. |\n-| `xla_gpu_analytical_latency_estimator_options` | Structured string | Configures parameters for the `xla_gpu_enable_analytical_sol_latency_estimator`. Adjust by setting `nic_speed_gbps=$NIC_SPEED,nccl_op_launch_us=$LAUNCH_OVERHEAD,chunk_prep_us=$CHUNK_PREP,rtt_us=$RTT,chunk_size_bytes=$CHUNK_SIZE,gpus_per_node=$GPUS_PER_NODE`. The default value depends on a detected platform. |\n-| `xla_gpu_enable_triton_gemm` | Boolean (true/false) | Use Triton-based matrix multiplication. |\n-| `xla_gpu_enable_command_buffer` | List of CommandBufferCmdType | Which kind of commands should be captured in command buffers. |\n-| `xla_gpu_all_reduce_combine_threshold_bytes` | Integer (bytes) | These flags tune when to combine multiple small AllGather / ReduceScatter / AllReduce into one big AllGather / ReduceScatter / AllReduce to reduce time spent on cross-device communication. For example, for the AllGather / ReduceScatter thresholds on a Transformer-based workload, consider tuning them high enough so as to combine at least a Transformer Layer‚Äôs weight AllGather / ReduceScatter. By default, the combine_threshold_bytes is set to 256. |\n-| `xla_gpu_all_gather_combine_threshold_bytes` | Integer (bytes) | See xla_gpu_all_reduce_combine_threshold_bytes above. |\n-| `xla_gpu_reduce_scatter_combine_threshold_bytes` | Integer (bytes) | See xla_gpu_all_reduce_combine_threshold_bytes above. |\n-| `xla_gpu_enable_pipelined_all_gather` | Boolean (true/false) | Enable pipelinling of all-gather instructions. |\n-| `xla_gpu_enable_pipelined_reduce_scatter` | Boolean (true/false) | Enable pipelinling of reduce-scatter instructions. |\n-| `xla_gpu_enable_pipelined_all_reduce` | Boolean (true/false) | Enable pipelinling of all-reduce instructions. |\n-| `xla_gpu_enable_while_loop_double_buffering` | Boolean (true/false) | Enable double-buffering for while loop. |\n-| `xla_gpu_enable_all_gather_combine_by_dim` | Boolean (true/false) | Combine all-gather ops with the same gather dimension or irrespective of their dimension. |\n-| `xla_gpu_enable_reduce_scatter_combine_by_dim` | Boolean (true/false) | Combine reduce-scatter ops with the same dimension or irrespective of their dimension. |\n+Flag                                              | Type                         | Notes\n+:------------------------------------------------ | :--------------------------- | :----\n+`xla_gpu_enable_latency_hiding_scheduler`         | Boolean (true/false)         | This flag enables latency hiding schedulers to overlap asynchronous communication with computation efficiently. The default value is False.\n+`xla_gpu_enable_analytical_sol_latency_estimator` | Boolean (true/false)         | Enables platform specific scheduling decisions, which in turn improve compute-communication overlap. The default value is true.\n+`xla_gpu_analytical_latency_estimator_options`    | Structured string            | Configures parameters for the `xla_gpu_enable_analytical_sol_latency_estimator`. Adjust by setting `nic_speed_gbps=$NIC_SPEED,nccl_op_launch_us=$LAUNCH_OVERHEAD,chunk_prep_us=$CHUNK_PREP,rtt_us=$RTT,chunk_size_bytes=$CHUNK_SIZE,gpus_per_node=$GPUS_PER_NODE`. The default value depends on a detected platform.\n+`xla_gpu_enable_triton_gemm`                      | Boolean (true/false)         | Use Triton-based matrix multiplication.\n+`xla_gpu_enable_command_buffer`                   | List of CommandBufferCmdType | Which kind of commands should be captured in command buffers.\n+`xla_gpu_all_reduce_combine_threshold_bytes`      | Integer (bytes)              | These flags tune when to combine multiple small AllGather / ReduceScatter / AllReduce into one big AllGather / ReduceScatter / AllReduce to reduce time spent on cross-device communication. For example, for the AllGather / ReduceScatter thresholds on a Transformer-based workload, consider tuning them high enough so as to combine at least a Transformer Layer‚Äôs weight AllGather / ReduceScatter. By default, the combine_threshold_bytes is set to 256.\n+`xla_gpu_all_gather_combine_threshold_bytes`      | Integer (bytes)              | See xla_gpu_all_reduce_combine_threshold_bytes above.\n+`xla_gpu_reduce_scatter_combine_threshold_bytes`  | Integer (bytes)              | See xla_gpu_all_reduce_combine_threshold_bytes above.\n+`xla_gpu_enable_pipelined_all_gather`             | Boolean (true/false)         | Enable pipelinling of all-gather instructions.\n+`xla_gpu_enable_pipelined_reduce_scatter`         | Boolean (true/false)         | Enable pipelinling of reduce-scatter instructions.\n+`xla_gpu_enable_pipelined_all_reduce`             | Boolean (true/false)         | Enable pipelinling of all-reduce instructions.\n+`xla_gpu_enable_pipelined_host_offloading`        | Boolean (true/false)         | Enable pipelining of host offloading instructions.\n+`xla_gpu_enable_while_loop_double_buffering`      | Boolean (true/false)         | Enable double-buffering for while loop.\n+`xla_gpu_enable_all_gather_combine_by_dim`        | Boolean (true/false)         | Combine all-gather ops with the same gather dimension or irrespective of their dimension.\n+`xla_gpu_enable_reduce_scatter_combine_by_dim`    | Boolean (true/false)         | Combine reduce-scatter ops with the same dimension or irrespective of their dimension."
        },
        {
            "sha": "01e47f38e0ab1d3522c500d59fb9f375974ccc78",
            "filename": "third_party/xla/xla/backends/gpu/codegen/copy_test.cc",
            "status": "modified",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -315,6 +315,73 @@ TEST_F(CopyFusionTest, BuildUpdateSliceDescriptor) {\n   EXPECT_EQ(offset.byte_stride, 8 * 8 * sizeof(float));\n }\n \n+TEST_F(CopyFusionTest, BuildDescriptorWithDynamicVariable) {\n+  constexpr char kModuleWithDynamicVariable[] = R\"(\n+    dynamic_slice {\n+      p0 = s32[4,8,8] parameter(0)\n+      p1 = s32[1,1,8] parameter(1)\n+      p2 = s32[] parameter(2)\n+      c1 = s32[] constant(1)\n+\n+      ROOT update-slice = s32[4,8,8] dynamic-update-slice(p0, p1, p2, c1, c1)\n+    }\n+\n+    body {\n+      p0 = (s32[], s32[4,8,8], s32[]) parameter(0)\n+      ivar = s32[] get-tuple-element(p0), index=0\n+      input = s32[4,8,8] get-tuple-element(p0), index=1\n+      dynamic_idx = s32[] get-tuple-element(p0), index=2\n+      val = s32[1,1,8] constant({{{1,2,3,4,5,6,7,8}}})\n+\n+      updated = s32[4,8,8] fusion(input, val, dynamic_idx), kind=kLoop, calls=dynamic_slice,\n+          backend_config={\"fusion_backend_config\":{\"kind\":\"__dynamic_memcpy\"}}\n+      c1 = s32[] constant(1)\n+      next_ivar = s32[] add(ivar, c1)\n+      next_dynamic_idx = s32[] add(dynamic_idx, c1)\n+\n+      ROOT result = (s32[], s32[4,8,8], s32[])\n+          tuple(next_ivar, updated, next_dynamic_idx)\n+    }\n+\n+    condition {\n+      p0 = (s32[], s32[4,8,8], s32[]) parameter(0)\n+      ivar = s32[] get-tuple-element(p0), index=0\n+      c6 = s32[] constant(6)\n+      ROOT cmp = pred[] compare(ivar, c6), direction=LT\n+    }\n+\n+    ENTRY main {\n+      input = s32[4,8,8] parameter(0)\n+      c0 = s32[] constant(0)\n+      tuple = (s32[], s32[4,8,8], s32[]) tuple(c0, input, c0)\n+      ROOT while = (s32[], s32[4,8,8], s32[]) while(tuple),\n+          condition=condition, body=body,\n+          backend_config={\"known_trip_count\":{\"n\":\"6\"},\n+                          \"known_init_step\":{\"init\":\"0\",\"step\":\"1\"},\n+                          \"known_induction_variable\":{\"tuple_index\":\"0\"},\n+                          \"dynamic_variable_tuple_indices\":[\"2\"]}\n+    })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleWithDynamicVariable));\n+\n+  auto descriptor = DynamicMemcpyFusion::GetMemcpyDescriptorForFusion(\n+      GetFusion(module.get()));\n+\n+  ASSERT_TRUE(descriptor.has_value());\n+  EXPECT_THAT(descriptor->src_dynamic_offsets, ::testing::IsEmpty());\n+  EXPECT_EQ(descriptor->src_byte_static_offset, 0);\n+\n+  ASSERT_THAT(descriptor->dst_dynamic_offsets, ::testing::SizeIs(1));\n+  const auto& offset = descriptor->dst_dynamic_offsets[0];\n+  EXPECT_EQ(descriptor->dst_byte_static_offset, 32);\n+  EXPECT_EQ(offset.while_loop->name(), \"while\");\n+  EXPECT_EQ(offset.induction_variable->name(), \"dynamic_idx\");\n+  EXPECT_EQ(offset.offset->name(), \"p2\");\n+  EXPECT_EQ(offset.dimension_size, 4);\n+  EXPECT_EQ(offset.byte_stride, 8 * 8 * sizeof(float));\n+}\n+\n TEST_F(CopyFusionTest, PackedSubByteTypesAreNotSupported) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n     dynamic_slice {"
        },
        {
            "sha": "e83d912282ac0573de8c31c292c3027183753af4",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -312,6 +312,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_enable_pipelined_all_reduce(false);\n   opts.set_xla_gpu_enable_pipelined_all_gather(false);\n   opts.set_xla_gpu_enable_pipelined_reduce_scatter(true);\n+  opts.set_xla_gpu_enable_pipelined_host_offloading(false);\n   opts.set_xla_gpu_enable_pipelined_p2p(false);\n \n   opts.set_xla_gpu_collective_permute_decomposer_threshold(\n@@ -1932,6 +1933,12 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n                     &DebugOptions::set_xla_gpu_enable_pipelined_reduce_scatter),\n                 debug_options->xla_gpu_enable_pipelined_reduce_scatter(),\n                 \"[Stable] Enable pipelinling of reduce-scatter instructions.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_enable_pipelined_host_offloading\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_gpu_enable_pipelined_host_offloading),\n+      debug_options->xla_gpu_enable_pipelined_host_offloading(),\n+      \"Enable pipelining of host offloading instructions.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_enable_pipelined_p2p\",\n       bool_setter_for(&DebugOptions::set_xla_gpu_enable_pipelined_p2p),"
        },
        {
            "sha": "f8821201e34dc6555d3475dad1a9b86b864def1e",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -504,6 +504,7 @@ xla_cc_test(\n         \":collective_pipeliner_utils\",\n         \":hlo_module_config\",\n         \":hlo_verifier\",\n+        \":host_offload_utils\",\n         \":legalize_scheduling_annotations\",\n         \":memory_annotations_hdr\",\n         \":scheduling_annotations_util\","
        },
        {
            "sha": "eec1f85305f70b4fb6327eca8beeae7e4a68a6e1",
            "filename": "third_party/xla/xla/service/collective_pipeliner.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 30,
            "changes": 94,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -514,7 +514,9 @@ std::optional<std::vector<HloInstruction*>> CollectIndependentOperandChain(\n     HloPredicate should_allow_loop_variant_parameter_in_chain,\n     const absl::flat_hash_set<const HloInstruction*>&\n         loop_invariant_instructions,\n-    bool should_add_loop_invariant_op_in_chain) {\n+    bool should_add_loop_invariant_op_in_chain,\n+    CollectivePipeliner::AdditionalChainStartOpFinder\n+        additional_chain_start_op_finder) {\n   std::vector<HloInstruction*> chain;\n   absl::flat_hash_set<const HloInstruction*> visited_set({instr});\n   std::vector<std::pair<HloInstruction*, int>> stack(1, {instr, 0});\n@@ -527,6 +529,16 @@ std::optional<std::vector<HloInstruction*>> CollectIndependentOperandChain(\n         return !IsLoopIterator(instr, loop_iter) &&\n                !loop_invariant_params.count(instr);\n       };\n+\n+  if (additional_chain_start_op_finder) {\n+    auto maybe_additional_op = additional_chain_start_op_finder(instr);\n+    if (maybe_additional_op.has_value()) {\n+      if (visited_set.insert(maybe_additional_op.value()).second) {\n+        stack.emplace_back(maybe_additional_op.value(), 0);\n+      }\n+    }\n+  }\n+\n   while (!stack.empty()) {\n     auto& curr = stack.back();\n     if (curr.second == curr.first->operand_count()) {\n@@ -600,14 +612,16 @@ std::optional<std::vector<HloInstruction*>> CollectChainsToPushBackwards(\n     bool should_allow_control_dependencies,\n     const absl::flat_hash_set<const HloInstruction*>&\n         loop_invariant_instructions,\n-    bool should_add_loop_invariant_op_in_chain) {\n+    bool should_add_loop_invariant_op_in_chain,\n+    CollectivePipeliner::AdditionalChainStartOpFinder\n+        additional_chain_start_op_finder) {\n   if (instr->HasControlDependencies() && !should_allow_control_dependencies) {\n     return std::nullopt;\n   }\n   return CollectIndependentOperandChain(\n       instr, loop_iter, loop_invariant_params,\n       should_allow_loop_variant_parameter_in_chain, loop_invariant_instructions,\n-      should_add_loop_invariant_op_in_chain);\n+      should_add_loop_invariant_op_in_chain, additional_chain_start_op_finder);\n }\n \n // Given a dynamic-update-slice find the output index of the loop we feed into.\n@@ -910,7 +924,9 @@ class WhileLoopAnalysis {\n       HloPredicate should_allow_loop_variant_parameter_in_chain =\n           HloPredicateFalse,\n       bool should_allow_control_dependencies = false,\n-      bool should_add_loop_invariant_op_in_chain = false);\n+      bool should_add_loop_invariant_op_in_chain = false,\n+      CollectivePipeliner::AdditionalChainStartOpFinder\n+          additional_chain_start_op_finder = nullptr);\n   HloInstruction* while_loop_instruction() const { return while_; }\n   void ExtractLoopInvariantOps();\n \n@@ -1321,7 +1337,9 @@ void WhileLoopAnalysis::CollectCollectivesToMove(\n     HloPredicate should_process, HloPredicate acceptable_formatting,\n     HloPredicate should_allow_loop_variant_parameter_in_chain,\n     bool should_allow_control_dependencies,\n-    bool should_add_loop_invariant_op_in_chain) {\n+    bool should_add_loop_invariant_op_in_chain,\n+    CollectivePipeliner::AdditionalChainStartOpFinder\n+        additional_chain_start_op_finder) {\n   move_infos_.clear();\n   HloComputation* while_body = while_->while_body();\n   const HloInstruction* loop_parameter =\n@@ -1498,7 +1516,8 @@ void WhileLoopAnalysis::CollectCollectivesToMove(\n           invariant_loop_parameters_,\n           should_allow_loop_variant_parameter_in_chain,\n           should_allow_control_dependencies, invariant_loop_instructions_,\n-          should_add_loop_invariant_op_in_chain);\n+          should_add_loop_invariant_op_in_chain,\n+          additional_chain_start_op_finder);\n       if (!chain_collected.has_value()) {\n         VLOG(5) << \"Skipping \" << instr->name()\n                 << \" because didn't find compatible slice of parameter\";\n@@ -1691,7 +1710,7 @@ HloInstruction* CreateZero(HloComputation* comp, const Shape& shape,\n // }\n // xg_last = all-reduce(x)\n // yg_last = all-reduce(y)\n-absl::Status TransformLoopForward(\n+absl::StatusOr<HloInstruction*> TransformLoopForward(\n     const WhileLoopAnalysis& loop_analysis, bool insert_non_alias_custom_call,\n     int64_t level_to_operate_on, bool pipeline_use_tree,\n     bool process_different_sized_ops, HloPredicate should_process,\n@@ -2179,7 +2198,7 @@ absl::Status TransformLoopForward(\n         absl::MakeSpan(loop_output_to_replace), output_stacked_data));\n   }\n   TF_RETURN_IF_ERROR(loop_computation->parent()->RemoveUnusedComputations());\n-  return absl::OkStatus();\n+  return new_while_loop;\n }\n \n absl::Status TransformFormattingOp(\n@@ -2412,7 +2431,7 @@ absl::Status TransformFormattingOp(\n // }\n // xg_all = all-reduce(x_all)\n // yg_all = all-reduce(y_all)\n-absl::Status TransformLoopForwardSink(\n+absl::StatusOr<HloInstruction*> TransformLoopForwardSink(\n     const WhileLoopAnalysis& loop_analysis, bool insert_non_alias_custom_call,\n     int64_t level_to_operate_on, bool pipeline_use_tree,\n     bool process_different_sized_ops, HloPredicate should_process,\n@@ -2767,7 +2786,7 @@ absl::Status TransformLoopForwardSink(\n   TF_RETURN_IF_ERROR(\n       loop_computation->RemoveInstructionAndUnusedOperands(while_loop));\n   TF_RETURN_IF_ERROR(loop_computation->parent()->RemoveUnusedComputations());\n-  return absl::OkStatus();\n+  return new_while;\n }\n \n // Function that does the work of pushing backward instructions that have been\n@@ -2793,7 +2812,7 @@ absl::Status TransformLoopForwardSink(\n //   x_ag = p0_ag_next\n // }\n // x_last = computation(p0_ag_next)\n-static absl::Status TransformLoopBackward(\n+static absl::StatusOr<HloInstruction*> TransformLoopBackward(\n     const WhileLoopAnalysis& loop_analysis, bool insert_non_alias_custom_call,\n     int64_t level_to_operate_on, bool process_different_sized_ops,\n     HloPredicate acceptable_formatting,\n@@ -3139,7 +3158,7 @@ static absl::Status TransformLoopBackward(\n   TF_RETURN_IF_ERROR(\n       loop_computation->RemoveInstructionAndUnusedOperands(while_loop));\n   TF_RETURN_IF_ERROR(loop_computation->parent()->RemoveUnusedComputations());\n-  return absl::OkStatus();\n+  return new_while_loop;\n }\n \n absl::StatusOr<bool> CollectivePipeliner::RunPipeliner(\n@@ -3162,6 +3181,7 @@ absl::StatusOr<bool> CollectivePipeliner::RunPipeliner(\n       if (instruction->opcode() != HloOpcode::kWhile) {\n         continue;\n       }\n+\n       if (std::none_of(instruction->while_body()->instructions().begin(),\n                        instruction->while_body()->instructions().end(),\n                        config_.should_process)) {\n@@ -3195,7 +3215,8 @@ absl::StatusOr<bool> CollectivePipeliner::RunPipeliner(\n         config_.should_process, config_.acceptable_formatting,\n         config_.should_allow_loop_variant_parameter_in_chain,\n         config_.should_allow_control_dependencies,\n-        config_.should_add_loop_invariant_op_in_chain);\n+        config_.should_add_loop_invariant_op_in_chain,\n+        config_.additional_chain_start_op_finder);\n     if (loop_analysis->GetMoveInfos().empty()) {\n       continue;\n     }\n@@ -3207,31 +3228,44 @@ absl::StatusOr<bool> CollectivePipeliner::RunPipeliner(\n         VLOG(1) << \"MoveInfo #\" << id++ << \"\\n\" << ToString(to_move);\n       }\n     }\n+    HloInstruction* transformed_while_loop;\n     if (config_.pipelining_direction ==\n         collective_pipeliner_utils::PipeliningDirection::kForward) {\n       CHECK(config_.reuse_pipelined_op_buffer);\n-      TF_RETURN_IF_ERROR(TransformLoopForward(\n-          *loop_analysis, !config_.last_run, config_.level_to_operate_on,\n-          config_.pipeline_use_tree, config_.process_different_sized_ops,\n-          config_.should_process, config_.acceptable_formatting,\n-          config_.reuse_pipelined_op_buffer, next_channel_id,\n-          config_.unique_channel_id, config_.postprocess_pipelined_ops));\n+      TF_ASSIGN_OR_RETURN(\n+          transformed_while_loop,\n+          TransformLoopForward(\n+              *loop_analysis, !config_.last_run, config_.level_to_operate_on,\n+              config_.pipeline_use_tree, config_.process_different_sized_ops,\n+              config_.should_process, config_.acceptable_formatting,\n+              config_.reuse_pipelined_op_buffer, next_channel_id,\n+              config_.unique_channel_id, config_.postprocess_pipelined_ops));\n     } else if (config_.pipelining_direction ==\n                collective_pipeliner_utils::PipeliningDirection::kForwardSink) {\n-      TF_RETURN_IF_ERROR(TransformLoopForwardSink(\n-          *loop_analysis, !config_.last_run, config_.level_to_operate_on,\n-          config_.pipeline_use_tree, config_.process_different_sized_ops,\n-          config_.should_process, next_channel_id, config_.unique_channel_id));\n+      TF_ASSIGN_OR_RETURN(\n+          transformed_while_loop,\n+          TransformLoopForwardSink(\n+              *loop_analysis, !config_.last_run, config_.level_to_operate_on,\n+              config_.pipeline_use_tree, config_.process_different_sized_ops,\n+              config_.should_process, next_channel_id,\n+              config_.unique_channel_id));\n     } else {\n       CHECK_EQ(config_.pipelining_direction,\n                collective_pipeliner_utils::PipeliningDirection::kBackward);\n-      TF_RETURN_IF_ERROR(TransformLoopBackward(\n-          *loop_analysis, !config_.last_run, config_.level_to_operate_on,\n-          config_.process_different_sized_ops, config_.acceptable_formatting,\n-          config_.postprocess_backward_peeled_op,\n-          config_.postprocess_backward_rotated_op,\n-          config_.postprocess_backward_peeled_trailing_op, next_channel_id,\n-          config_.unique_channel_id, config_.postprocess_pipelined_ops));\n+      TF_ASSIGN_OR_RETURN(\n+          transformed_while_loop,\n+          TransformLoopBackward(\n+              *loop_analysis, !config_.last_run, config_.level_to_operate_on,\n+              config_.process_different_sized_ops,\n+              config_.acceptable_formatting,\n+              config_.postprocess_backward_peeled_op,\n+              config_.postprocess_backward_rotated_op,\n+              config_.postprocess_backward_peeled_trailing_op, next_channel_id,\n+              config_.unique_channel_id, config_.postprocess_pipelined_ops));\n+    }\n+    if (config_.postprocess_transformed_while_loop) {\n+      TF_RETURN_IF_ERROR(\n+          config_.postprocess_transformed_while_loop(transformed_while_loop));\n     }\n     ++transformed_loops;\n     changed = true;"
        },
        {
            "sha": "427702e3fe72872cc08ace653efc2d98d14c87eb",
            "filename": "third_party/xla/xla/service/collective_pipeliner.h",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner.h?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -67,6 +67,10 @@ class CollectivePipeliner : public HloModulePass {\n   // created.\n   using HloPostprocessor = std::function<absl::Status(\n       HloInstruction* instr, HloInstruction* new_while_instr)>;\n+  using WhileLoopPostprocessor =\n+      std::function<absl::Status(HloInstruction* while_loop)>;\n+  using AdditionalChainStartOpFinder =\n+      std::function<std::optional<HloInstruction*>(HloInstruction*)>;\n \n   struct Config {\n     int64_t level_to_operate_on = 0;\n@@ -99,6 +103,11 @@ class CollectivePipeliner : public HloModulePass {\n     // pipelined. The control dependencies will be dropped when the operation is\n     // pipelined. This is currently only used to support kBackward pipelining.\n     bool should_allow_control_dependencies = false;\n+    // Function to find an additional operation to start the operand chain from.\n+    // If set, this function will be called to discover additional starting\n+    // points for the operand chain (e.g., DynamicSlice operations through\n+    // formatting ops).\n+    AdditionalChainStartOpFinder additional_chain_start_op_finder = nullptr;\n     // TODO(b/399476667): Consolidate these postprocessing functions.\n     HloPostprocessor postprocess_backward_peeled_op;\n     HloPostprocessor postprocess_backward_rotated_op;\n@@ -112,6 +121,9 @@ class CollectivePipeliner : public HloModulePass {\n     bool delay_sinking_large_collectives = true;\n     // When cloning collectives, use a unique channel id for each clone.\n     bool unique_channel_id = true;\n+    // Postprocessing hook which runs for every successfully transformed while\n+    // loop.\n+    WhileLoopPostprocessor postprocess_transformed_while_loop;\n   };\n   static const char* const kInsertedByPreviousStep;\n   static const char* const kSunkByPreviousStep;"
        },
        {
            "sha": "a8fb19690f159249fc8bb257ae6f829e51ff809a",
            "filename": "third_party/xla/xla/service/collective_pipeliner_test.cc",
            "status": "modified",
            "additions": 213,
            "deletions": 3,
            "changes": 216,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_pipeliner_test.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <queue>\n+#include <set>\n #include <string>\n #include <vector>\n \n@@ -52,6 +53,7 @@ limitations under the License.\n #include \"xla/service/collective_pipeliner_utils.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_verifier.h\"\n+#include \"xla/service/host_offload_utils.h\"\n #include \"xla/service/legalize_scheduling_annotations.h\"\n #include \"xla/service/memory_annotations.h\"\n #include \"xla/service/scheduling_annotations_util.h\"\n@@ -107,7 +109,9 @@ absl::StatusOr<bool> RunOptimizer(\n         {},\n     bool should_add_loop_invariant_op_in_chain = false,\n     int64_t collective_size_threshold_to_delay_sinking = INT64_MAX,\n-    bool unique_channel_id = true) {\n+    bool unique_channel_id = true,\n+    CollectivePipeliner::WhileLoopPostprocessor\n+        postprocess_transformed_while_loop = {}) {\n   CollectivePipeliner::Config config = {\n       /*level_to_operate_on=*/level_to_operate_on,\n       /*max_pipelining_per_loop=*/INT64_MAX,\n@@ -120,12 +124,14 @@ absl::StatusOr<bool> RunOptimizer(\n       /*acceptable_formatting=*/acceptable_formatting,\n       /*reuse_pipelined_op_buffer=*/reuse_pipelined_op_buffer,\n       should_allow_loop_variant_parameter_in_chain,\n-      /*should_allow_control_dependencies=*/false, postprocess_backward_peeled,\n+      /*should_allow_control_dependencies=*/false,\n+      /*additional_chain_start_op_finder=*/nullptr, postprocess_backward_peeled,\n       postprocess_backward_rotated, postprocess_backward_peeled_trailing,\n       should_add_loop_invariant_op_in_chain,\n       /*postprocess_pipelined_ops=*/{},\n       collective_size_threshold_to_delay_sinking,\n-      /*delay_sinking_large_collectives=*/true, unique_channel_id};\n+      /*delay_sinking_large_collectives=*/true, unique_channel_id,\n+      postprocess_transformed_while_loop};\n   HloPassPipeline pass(\"optimizer\");\n   pass.AddPass<HloVerifier>(/*layout_sensitive=*/false,\n                             /*allow_mixed_precision=*/false);\n@@ -5625,5 +5631,209 @@ ENTRY entry {\n   EXPECT_EQ(fusion_count, 4);\n }\n \n+TEST_F(CollectivePipelinerTest, HostOffloadingForward) {\n+  constexpr absl::string_view hlo_string = R\"(\n+HloModule jit_scanned\n+\n+%region_0.40 (arg_tuple.13: (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000])) -> (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000]) {\n+  %arg_tuple.13 = (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000]) parameter(0)\n+  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.13), index=0\n+  %constant.19 = s32[] constant(1)\n+  %add.38 = s32[] add(%get-tuple-element.14, %constant.19)\n+  %get-tuple-element.15 = f32[1000,1000] get-tuple-element(%arg_tuple.13), index=1\n+  %get-tuple-element.17 = f32[10,1000,8000] get-tuple-element(%arg_tuple.13), index=3\n+  %constant.20 = s32[] constant(0)\n+  %dynamic-slice.21 = f32[1,1000,8000] dynamic-slice(%get-tuple-element.17, %get-tuple-element.14, %constant.20, %constant.20), dynamic_slice_sizes={1,1000,8000}\n+  %reshape.22 = f32[1000,8000] reshape(%dynamic-slice.21)\n+  %dot.0 = f32[1000,8000] dot(%get-tuple-element.15, %reshape.22), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  %get-tuple-element.18 = f32[10,8000,1000] get-tuple-element(%arg_tuple.13), index=4\n+  %dynamic-slice.23 = f32[1,8000,1000] dynamic-slice(%get-tuple-element.18, %get-tuple-element.14, %constant.20, %constant.20), dynamic_slice_sizes={1,8000,1000}\n+  %reshape.24 = f32[8000,1000] reshape(%dynamic-slice.23)\n+  %dot.1 = f32[1000,1000] dot(%dot.0, %reshape.24), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  %get-tuple-element.16 = f32[10,1000,1000] get-tuple-element(%arg_tuple.13), index=2\n+  %custom-call.2 = f32[1000,1000] custom-call(%dot.1), custom_call_target=\"MoveToHost\"\n+  %reshape.36 = f32[1,1000,1000] reshape(%custom-call.2)\n+  %dynamic-update-slice.37 = f32[10,1000,1000] dynamic-update-slice(%get-tuple-element.16, %reshape.36, %get-tuple-element.14, %constant.20, %constant.20)\n+  ROOT %tuple.39 = (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000]) tuple(%add.38, %dot.1, %dynamic-update-slice.37, %get-tuple-element.17, %get-tuple-element.18)\n+}\n+\n+%region_1.49 (arg_tuple.41: (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000])) -> pred[] {\n+  %arg_tuple.41 = (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000]) parameter(0)\n+  %get-tuple-element.42 = s32[] get-tuple-element(%arg_tuple.41), index=0\n+  %constant.47 = s32[] constant(10)\n+  ROOT %compare.48 = pred[] compare(%get-tuple-element.42, %constant.47), direction=LT\n+}\n+\n+ENTRY %main.117 (Arg_0.1: f32[10,1000,8000], Arg_1.2: f32[10,8000,1000], Arg_2.3: f32[1000,1000]) -> (f32[10,1000,8000], f32[10,8000,1000]) {\n+  %constant.10 = s32[] constant(0)\n+  %constant.4 = f32[] constant(0)\n+  %Arg_2.3 = f32[1000,1000] parameter(2)\n+  %broadcast.12 = f32[10,1000,1000] broadcast(%constant.4), dimensions={}\n+  %Arg_0.1 = f32[10,1000,8000] parameter(0)\n+  %Arg_1.2 = f32[10,8000,1000] parameter(1)\n+  %tuple.50 = (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000]) tuple(%constant.10, %Arg_2.3, %broadcast.12, %Arg_0.1, %Arg_1.2)\n+  %while.51 = (s32[], f32[1000,1000], f32[10,1000,1000], f32[10,1000,8000], f32[10,8000,1000]) while(%tuple.50), condition=%region_1.49, body=%region_0.40\n+  %get-tuple-element.52 = f32[10,1000,8000] get-tuple-element(%while.51), index=3\n+  %get-tuple-element.53 = f32[10,8000,1000] get-tuple-element(%while.51), index=4\n+  ROOT %tuple.116 = (f32[10,1000,8000], f32[10,8000,1000]) tuple(%get-tuple-element.52, %get-tuple-element.53)\n+}\n+)\";\n+\n+  auto module = ParseAndReturnUnverifiedModule(hlo_string, config_).value();\n+\n+  EXPECT_TRUE(\n+      RunOptimizer(\n+          module.get(), /*last_run=*/true, 0,\n+          /*pipeline_use_tree=*/true,\n+          /*process_different_sized_ops=*/true,\n+          /*direction=*/\n+          collective_pipeliner_utils::PipeliningDirection::kForward,\n+          /*should_process=*/\n+          host_offload_utils::IsMoveToHostWithDynamicUpdateSlice,\n+          /*acceptable_formatting=*/HloPredicateTrue,\n+          /*reuse_pipelined_op_buffer=*/HloPredicateTrue,\n+          /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n+          /*postprocess_backward_peeled=*/{},\n+          /*postprocess_backward_rotated=*/{},\n+          /*postprocess_backward_peeled_trailing=*/{},\n+          /*should_add_loop_invariant_op_in_chain=*/false,\n+          /*collective_size_threshold_to_delay_sinking=*/INT64_MAX,\n+          /*unique_channel_id=*/true,\n+          /*postprocess_transformed_while_loop=*/\n+          host_offload_utils::MarkDynamicVariables)\n+          .value());\n+\n+  std::vector<HloInstruction*> while_loops;\n+  for (auto* instr : module->entry_computation()->instructions()) {\n+    if (instr->opcode() == HloOpcode::kWhile) {\n+      while_loops.push_back(instr);\n+    }\n+  }\n+  ASSERT_EQ(while_loops.size(), 1) << \"Expected 1 while loop in the module\";\n+\n+  XLA_VLOG_LINES(1, \"Transformed while body:\\n\" +\n+                        while_loops[0]->while_body()->ToString());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      WhileLoopBackendConfig config,\n+      while_loops[0]->backend_config<WhileLoopBackendConfig>());\n+\n+  std::set<int64_t> dynamic_indices(\n+      config.dynamic_variable_tuple_indices().begin(),\n+      config.dynamic_variable_tuple_indices().end());\n+\n+  std::set<int64_t> expected_indices = {0, 5};\n+  EXPECT_EQ(dynamic_indices, expected_indices);\n+}\n+\n+TEST_F(CollectivePipelinerTest, HostOffloadingBackward) {\n+  constexpr absl::string_view hlo_string = R\"(\n+HloModule jit_scanned\n+\n+%region_2.98 (arg_tuple.55: (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000])) -> (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000]) {\n+  %arg_tuple.55 = (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000]) parameter(0)\n+  %get-tuple-element.56 = s32[] get-tuple-element(%arg_tuple.55), index=0\n+  %constant.64 = s32[] constant(1)\n+  %add.96 = s32[] add(%get-tuple-element.56, %constant.64)\n+  %get-tuple-element.57 = f32[1000,1000] get-tuple-element(%arg_tuple.55), index=1\n+  %get-tuple-element.62 = f32[10,8000,1000] get-tuple-element(%arg_tuple.55), index=6\n+  %constant.1 = s32[] constant(9)\n+  %subtract = s32[] subtract(%constant.1, %get-tuple-element.56)\n+  %constant.63 = s32[] constant(0)\n+  %dynamic-slice.72 = f32[1,8000,1000] dynamic-slice(%get-tuple-element.62, %subtract, %constant.63, %constant.63), dynamic_slice_sizes={1,8000,1000}\n+  %reshape.73 = f32[8000,1000] reshape(%dynamic-slice.72)\n+  %dot.2 = f32[1000,8000] dot(%get-tuple-element.57, %reshape.73), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+  %get-tuple-element.61 = f32[10,1000,8000] get-tuple-element(%arg_tuple.55), index=5\n+  %dynamic-slice.70 = f32[1,1000,8000] dynamic-slice(%get-tuple-element.61, %subtract, %constant.63, %constant.63), dynamic_slice_sizes={1,1000,8000}\n+  %reshape.71 = f32[1000,8000] reshape(%dynamic-slice.70)\n+  %dot.3 = f32[1000,1000] dot(%dot.2, %reshape.71), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+  %get-tuple-element.58 = f32[10,1000,8000] get-tuple-element(%arg_tuple.55), index=2\n+  %get-tuple-element.60 = f32[10,1000,1000] get-tuple-element(%arg_tuple.55), index=4\n+  %dynamic-slice.68 = f32[1,1000,1000] dynamic-slice(%get-tuple-element.60, %subtract, %constant.63, %constant.63), dynamic_slice_sizes={1,1000,1000}\n+  %reshape.69 = f32[1000,1000] reshape(%dynamic-slice.68)\n+  %custom-call.3 = f32[1000,1000] custom-call(%reshape.69), custom_call_target=\"MoveToDevice\"\n+  %dot.7 = f32[1000,8000] dot(%custom-call.3, %dot.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+  %reshape.92 = f32[1,1000,8000] reshape(%dot.7)\n+  %dynamic-update-slice.93 = f32[10,1000,8000] dynamic-update-slice(%get-tuple-element.58, %reshape.92, %subtract, %constant.63, %constant.63)\n+  %get-tuple-element.59 = f32[10,8000,1000] get-tuple-element(%arg_tuple.55), index=3\n+  %dot.5 = f32[1000,8000] dot(%custom-call.3, %reshape.71), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  %dot.8 = f32[8000,1000] dot(%dot.5, %get-tuple-element.57), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+  %reshape.94 = f32[1,8000,1000] reshape(%dot.8)\n+  %dynamic-update-slice.95 = f32[10,8000,1000] dynamic-update-slice(%get-tuple-element.59, %reshape.94, %subtract, %constant.63, %constant.63)\n+  ROOT %tuple.97 = (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000]) tuple(%add.96, %dot.3, %dynamic-update-slice.93, %dynamic-update-slice.95, %get-tuple-element.60, /*index=5*/%get-tuple-element.61, %get-tuple-element.62)\n+}\n+\n+%region_3.109 (arg_tuple.99: (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000])) -> pred[] {\n+  %arg_tuple.99 = (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000]) parameter(0)\n+  %get-tuple-element.100 = s32[] get-tuple-element(%arg_tuple.99), index=0\n+  %constant.107 = s32[] constant(10)\n+  ROOT %compare.108 = pred[] compare(%get-tuple-element.100, %constant.107), direction=LT\n+}\n+\n+ENTRY %main.117 (Arg_0.1: f32[10,1000,8000], Arg_1.2: f32[10,8000,1000], Arg_2.3: f32[10,1000,1000]) -> (f32[10,1000,8000], f32[10,8000,1000]) {\n+  %constant.10 = s32[] constant(0)\n+  %constant.8 = f32[] constant(1)\n+  %broadcast.9 = f32[1000,1000] broadcast(%constant.8), dimensions={}\n+  %constant.4 = f32[] constant(0)\n+  %broadcast.7 = f32[10,1000,8000] broadcast(%constant.4), dimensions={}\n+  %broadcast.5 = f32[10,8000,1000] broadcast(%constant.4), dimensions={}\n+  %Arg_2.3 = f32[10,1000,1000] parameter(2)\n+  %Arg_0.1 = f32[10,1000,8000] parameter(0)\n+  %Arg_1.2 = f32[10,8000,1000] parameter(1)\n+  %tuple.110 = (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000]) tuple(%constant.10, %broadcast.9, %broadcast.7, %broadcast.5, %Arg_2.3, /*index=5*/%Arg_0.1, %Arg_1.2)\n+  %while.111 = (s32[], f32[1000,1000], f32[10,1000,8000], f32[10,8000,1000], f32[10,1000,1000], /*index=5*/f32[10,1000,8000], f32[10,8000,1000]) while(%tuple.110), condition=%region_3.109, body=%region_2.98\n+  %get-tuple-element.114 = f32[10,1000,8000] get-tuple-element(%while.111), index=2\n+  %get-tuple-element.115 = f32[10,8000,1000] get-tuple-element(%while.111), index=3\n+  ROOT %tuple.116 = (f32[10,1000,8000], f32[10,8000,1000]) tuple(%get-tuple-element.114, %get-tuple-element.115)\n+}\n+)\";\n+\n+  auto module = ParseAndReturnUnverifiedModule(hlo_string, config_).value();\n+\n+  EXPECT_TRUE(\n+      RunOptimizer(\n+          module.get(), /*last_run=*/true, 0,\n+          /*pipeline_use_tree=*/true,\n+          /*process_different_sized_ops=*/true,\n+          /*direction=*/\n+          collective_pipeliner_utils::PipeliningDirection::kBackward,\n+          /*should_process=*/\n+          host_offload_utils::IsMoveToDeviceWithDynamicSlice,\n+          /*acceptable_formatting=*/HloPredicateTrue,\n+          /*reuse_pipelined_op_buffer=*/HloPredicateTrue,\n+          /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n+          /*postprocess_backward_peeled=*/{},\n+          /*postprocess_backward_rotated=*/{},\n+          /*postprocess_backward_peeled_trailing=*/{},\n+          /*should_add_loop_invariant_op_in_chain=*/false,\n+          /*collective_size_threshold_to_delay_sinking=*/INT64_MAX,\n+          /*unique_channel_id=*/true,\n+          /*postprocess_transformed_while_loop=*/\n+          host_offload_utils::MarkDynamicVariables)\n+          .value());\n+\n+  std::vector<HloInstruction*> while_loops;\n+  for (auto* instr : module->entry_computation()->instructions()) {\n+    if (instr->opcode() == HloOpcode::kWhile) {\n+      while_loops.push_back(instr);\n+    }\n+  }\n+  ASSERT_EQ(while_loops.size(), 1) << \"Expected 1 while loop in the module\";\n+\n+  XLA_VLOG_LINES(1, \"Transformed while body:\\n\" +\n+                        while_loops[0]->while_body()->ToString());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      WhileLoopBackendConfig config,\n+      while_loops[0]->backend_config<WhileLoopBackendConfig>());\n+\n+  std::set<int64_t> dynamic_indices(\n+      config.dynamic_variable_tuple_indices().begin(),\n+      config.dynamic_variable_tuple_indices().end());\n+\n+  std::set<int64_t> expected_indices = {0, 8};\n+  EXPECT_EQ(dynamic_indices, expected_indices);\n+}\n+\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "fa1687089d29d24e270b7ab2688e87379784afbd",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -1728,11 +1728,13 @@ cc_library(\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service:hlo_value\",\n         \"//xla/service:hlo_verifier\",\n+        \"//xla/service:host_offload_utils\",\n         \"//xla/service:layout_assignment\",\n         \"//xla/service:layout_normalization\",\n         \"//xla/service:llvm_compiler\",\n         \"//xla/service:logical_buffer\",\n         \"//xla/service:loop_schedule_linearizer\",\n+        \"//xla/service:memory_annotations_hdr\",\n         \"//xla/service:reduce_scatter_combiner\",\n         \"//xla/service:reduce_scatter_reassociate\",\n         \"//xla/service:scatter_determinism_expander\","
        },
        {
            "sha": "dd90b654eace237c104d1ac3f28263e27127a03a",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <functional>\n #include <memory>\n #include <optional>\n+#include <set>\n #include <string>\n #include <utility>\n #include <variant>\n@@ -274,10 +275,12 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_value.h\"\n #include \"xla/service/hlo_verifier.h\"\n+#include \"xla/service/host_offload_utils.h\"\n #include \"xla/service/layout_assignment.h\"\n #include \"xla/service/layout_normalization.h\"\n #include \"xla/service/llvm_ir/llvm_command_line_options.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n+#include \"xla/service/memory_annotations.h\"\n #include \"xla/service/reduce_scatter_reassociate.h\"\n #include \"xla/service/scatter_determinism_expander.h\"\n #include \"xla/service/scatter_expander.h\"\n@@ -882,6 +885,7 @@ absl::Status RunCollectiveOptimizationPasses(\n         /*reuse_pipelined_op_buffer=*/HloPredicateFalse,\n         /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n         /*should_allow_control_dependencies=*/false,\n+        /*additional_chain_start_op_finder=*/nullptr,\n         /*postprocess_backward_peeled_op=*/{},\n         /*postprocess_backward_rotated_op=*/{},\n         /*postprocess_backward_peeled_trailing_op=*/{},\n@@ -905,6 +909,7 @@ absl::Status RunCollectiveOptimizationPasses(\n         /*reuse_pipelined_op_buffer=*/HloPredicateFalse,\n         /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n         /*should_allow_control_dependencies=*/false,\n+        /*additional_chain_start_op_finder=*/nullptr,\n         /*postprocess_backward_peeled_op=*/{},\n         /*postprocess_backward_rotated_op=*/{},\n         /*postprocess_backward_peeled_trailing_op=*/{},\n@@ -928,6 +933,7 @@ absl::Status RunCollectiveOptimizationPasses(\n         /*reuse_pipelined_op_buffer=*/HloPredicateFalse,\n         /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n         /*should_allow_control_dependencies=*/false,\n+        /*additional_chain_start_op_finder=*/nullptr,\n         /*postprocess_backward_peeled_op=*/{},\n         /*postprocess_backward_rotated_op=*/{},\n         /*postprocess_backward_peeled_trailing_op=*/{},\n@@ -937,6 +943,105 @@ absl::Status RunCollectiveOptimizationPasses(\n     collectives_pipeline.AddPass<CollectivePipeliner>(config);\n   }\n \n+  if (debug_options.xla_gpu_enable_pipelined_host_offloading() ||\n+      IsPassEnabledAtOptimizationEffort<CollectivePipeliner>(*hlo_module)) {\n+    // Forward pass host offloading pipelining\n+    CollectivePipeliner::Config config{\n+        /*level_to_operate_on=*/0,\n+        /*max_pipelining_per_loop=*/INT64_MAX,\n+        /*last_run=*/true,\n+        /*pipeline_use_tree=*/true,\n+        /*process_different_sized_ops=*/true,\n+        /*pipelining_direction=*/\n+        collective_pipeliner_utils::PipeliningDirection::kForward,\n+        /*should_process=*/\n+        host_offload_utils::IsMoveToHostWithDynamicUpdateSlice,\n+        /*acceptable_formatting=*/HloPredicateTrue,\n+        /*reuse_pipelined_op_buffer=*/HloPredicateFalse,\n+        /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n+        /*should_allow_control_dependencies=*/false,\n+        /*additional_chain_start_op_finder=*/nullptr,\n+        /*postprocess_backward_peeled_op=*/{},\n+        /*postprocess_backward_rotated_op=*/{},\n+        /*postprocess_backward_peeled_trailing_op=*/{},\n+        /*should_add_loop_invariant_op_in_chain=*/false,\n+        /*postprocess_pipelined_ops=*/AppendPipelinedInstruction,\n+        /*collective_size_threshold_to_delay_sinking=*/INT64_MAX,\n+        /*delay_sinking_large_collectives=*/true,\n+        /*unique_channel_id=*/true,\n+        /*postprocess_transformed_while_loop=*/\n+        host_offload_utils::MarkDynamicVariables,\n+    };\n+    collectives_pipeline.AddPass<CollectivePipeliner>(config);\n+  }\n+\n+  if (debug_options.xla_gpu_enable_pipelined_host_offloading() ||\n+      IsPassEnabledAtOptimizationEffort<CollectivePipeliner>(*hlo_module)) {\n+    // Backward pass host offloading pipelining\n+    auto acceptable_formatting = [](const HloInstruction* instr) {\n+      return instr->opcode() == HloOpcode::kReshape ||\n+             instr->opcode() == HloOpcode::kBroadcast ||\n+             instr->opcode() == HloOpcode::kTranspose;\n+    };\n+    CollectivePipeliner::Config config_backward{\n+        /*level_to_operate_on=*/0,\n+        /*max_pipelining_per_loop=*/INT64_MAX,\n+        /*last_run=*/true,\n+        /*pipeline_use_tree=*/true,\n+        /*process_different_sized_ops=*/true,\n+        /*pipelining_direction=*/\n+        collective_pipeliner_utils::PipeliningDirection::kBackward,\n+        /*should_process=*/host_offload_utils::IsMoveToDeviceWithDynamicSlice,\n+        /*acceptable_formatting=*/acceptable_formatting,\n+        /*reuse_pipelined_op_buffer=*/HloPredicateFalse,\n+        /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n+        /*should_allow_control_dependencies=*/false,\n+        /*additional_chain_start_op_finder=*/\n+        [acceptable_formatting](\n+            HloInstruction* instr) -> std::optional<HloInstruction*> {\n+          if (!instr->IsCustomCall(\n+                  memory_annotations::kMoveToDeviceCustomCallTarget)) {\n+            return std::nullopt;\n+          }\n+          if (instr->operand_count() == 0) {\n+            return std::nullopt;\n+          }\n+\n+          std::vector<HloInstruction*> to_check = {instr->mutable_operand(0)};\n+          std::set<HloInstruction*> visited;\n+\n+          while (!to_check.empty()) {\n+            HloInstruction* current = to_check.back();\n+            to_check.pop_back();\n+\n+            if (visited.insert(current).second) {\n+              if (current->opcode() == HloOpcode::kDynamicSlice) {\n+                return current;\n+              }\n+              if (acceptable_formatting(current)) {\n+                for (HloInstruction* operand : current->operands()) {\n+                  to_check.push_back(operand);\n+                }\n+              }\n+            }\n+          }\n+          return std::nullopt;\n+        },\n+        /*postprocess_backward_peeled_op=*/{},\n+        /*postprocess_backward_rotated_op=*/{},\n+        /*postprocess_backward_peeled_trailing_op=*/{},\n+        /*should_add_loop_invariant_op_in_chain=*/true,\n+        /*postprocess_pipelined_ops=*/AppendPipelinedInstruction,\n+        /*collective_size_threshold_to_delay_sinking=*/INT64_MAX,\n+        /*delay_sinking_large_collectives=*/true,\n+        /*unique_channel_id=*/true,\n+        /*postprocess_transformed_while_loop=*/\n+        host_offload_utils::MarkDynamicVariables,\n+    };\n+\n+    collectives_pipeline.AddPass<CollectivePipeliner>(config_backward);\n+  }\n+\n   DebugOptions::PipelineParallelismOptLevel pipeline_parallelism_opt_level =\n       debug_options.xla_gpu_experimental_pipeline_parallelism_opt_level();\n   if (debug_options.xla_gpu_enable_pipelined_p2p()) {"
        },
        {
            "sha": "022edaee8653c393a4d73d114b4f34d77fe5d6da",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 11,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -668,32 +668,62 @@ bool IsInductionVariable(const HloInstruction* maybe_variable,\n          maybe_variable->tuple_index() == loop.induction_variable_index;\n }\n \n+// Returns true if `variable` is marked as a dynamic variable.\n+bool IsDynamicVariable(const HloInstruction* variable,\n+                       const VerifiedLoop& loop) {\n+  auto config = loop.loop->backend_config<xla::WhileLoopBackendConfig>();\n+  if (!config.ok()) {\n+    return false;\n+  }\n+\n+  int64_t tuple_idx = variable->tuple_index();\n+  for (int64_t dynamic_idx : config->dynamic_variable_tuple_indices()) {\n+    if (dynamic_idx == tuple_idx) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n // Attempts to find the induction variable of `loop` in `dependencies`. If there\n // are any dependencies on non-induction variable loop-carried variables,\n // returns nullopt.\n std::optional<const HloInstruction*> VerifyInductionVariable(\n     const Dependencies& dependencies, const VerifiedLoop& loop) {\n   const HloInstruction* induction_var = nullptr;\n   for (const HloInstruction* gte : dependencies.get_tuple_elements) {\n-    if (IsInductionVariable(gte, loop)) {\n-      if (induction_var) {\n-        // This should never happen.\n-        VLOG(5) << \"Found non-unique GTEs for the induction variable. Did \"\n-                   \"HloCSE run?\";\n+    if (IsLoopCarriedVariable(gte, loop)) {\n+      if (IsInductionVariable(gte, loop)) {\n+        if (induction_var) {\n+          // This should never happen.\n+          VLOG(5) << \"Found non-unique GTEs for the induction variable. Did \"\n+                     \"HloCSE run?\";\n+          return std::nullopt;\n+        }\n+        induction_var = gte;\n+      } else if (IsDynamicVariable(gte, loop)) {\n+        // Dynamic variables are also acceptable because they represent tuple\n+        // indices used in DS/DUS that can be optimized by\n+        // FusionDynamicMemcpyRewriter.\n+        if (induction_var) {\n+          // This should never happen.\n+          VLOG(5) << \"Found non-unique GTEs for the dynamic variable. Did \"\n+                     \"HloCSE run?\";\n+          return std::nullopt;\n+        }\n+        induction_var = gte;\n+      } else {\n+        // Other dependencies on loop-carried variables are not allowed.\n+        VLOG(5) << \"Found illegal dependency on loop-carried variable.\";\n         return std::nullopt;\n       }\n-      induction_var = gte;\n-    } else if (IsLoopCarriedVariable(gte, loop)) {\n-      // Other dependencies on loop-carried variables are not allowed.\n-      VLOG(5) << \"Found illegal dependency on loop-carried variable.\";\n-      return std::nullopt;\n     }\n     // Other GTEs are OK, as long as their tuples are ultimately just derived\n     // from the loop's induction variable. We already verified that there are no\n     // side-effecting dependencies in GetLeafDependencies.\n   }\n   if (!induction_var) {\n-    VLOG(5) << \"Did not find an induction variable.\";\n+    VLOG(5) << \"Did not find an induction variable or dynamic variable.\";\n     return std::nullopt;\n   }\n   return induction_var;"
        },
        {
            "sha": "14762eb4a410fc7fd7f087fcee4088e1220b3f8c",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils_test.cc",
            "status": "modified",
            "additions": 159,
            "deletions": 0,
            "changes": 159,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -957,6 +957,165 @@ TEST_F(IrEmissionUtilsTest, NonInductionVariableLoopCarriedVariable) {\n                    .has_value());\n }\n \n+TEST_F(IrEmissionUtilsTest, DynamicVariableLoopCarriedVariable) {\n+  constexpr absl::string_view kHlo = R\"(\n+      while_body {\n+        p0 = (s32[], s32[], s32[]) parameter(0)\n+        ivar = s32[] get-tuple-element(p0), index=0\n+        dynamic_var = s32[] get-tuple-element(p0), index=1\n+        other_var = s32[] get-tuple-element(p0), index=2\n+\n+        c1 = s32[] constant(1)\n+        next_ivar = s32[] add(ivar, c1)\n+        next_dynamic_var = s32[] add(dynamic_var, c1)\n+        next_other = s32[] add(other_var, c1)\n+\n+        ROOT result = (s32[], s32[], s32[]) tuple(next_ivar, next_dynamic_var, next_other)\n+      }\n+\n+      condition {\n+        p0 = (s32[], s32[], s32[]) parameter(0)\n+        ivar = s32[] get-tuple-element(p0), index=0\n+        c5 = s32[] constant(5)\n+        ROOT cmp = pred[] compare(ivar, c5), direction=LT\n+      }\n+\n+      ENTRY main {\n+        c0 = s32[] constant(0)\n+        tuple = (s32[], s32[], s32[]) tuple(c0, c0, c0)\n+        ROOT while = (s32[], s32[], s32[]) while(tuple),\n+            condition=condition, body=while_body,\n+            backend_config={\"known_induction_variable\":{\"tuple_index\":\"0\"},\"dynamic_variable_tuple_indices\":[1]}\n+      }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHlo));\n+  HloComputation* while_body = module->GetComputationWithName(\"while_body\");\n+\n+  ASSERT_TRUE(ResolveFunctionalDependencyOnInductionVariable(\n+                  while_body->GetInstructionWithName(\"next_ivar\"))\n+                  .has_value());\n+\n+  ASSERT_TRUE(ResolveFunctionalDependencyOnInductionVariable(\n+                  while_body->GetInstructionWithName(\"next_dynamic_var\"))\n+                  .has_value());\n+\n+  ASSERT_FALSE(ResolveFunctionalDependencyOnInductionVariable(\n+                   while_body->GetInstructionWithName(\"next_other\"))\n+                   .has_value());\n+}\n+\n+TEST_F(IrEmissionUtilsTest, DynamicVariableWithIrrelevantGTE) {\n+  constexpr absl::string_view kHlo = R\"(\n+      while_body {\n+        p0 = (s32[], s32[], s32[], s32[]) parameter(0)\n+        ivar = s32[] get-tuple-element(p0), index=0\n+        dynamic_var = s32[] get-tuple-element(p0), index=1\n+        irrelevant_var = s32[] get-tuple-element(p0), index=2\n+        other_var = s32[] get-tuple-element(p0), index=3\n+\n+        c1 = s32[] constant(1)\n+        next_ivar = s32[] add(ivar, c1)\n+        \n+        dynamic_computation = s32[] add(ivar, c1)\n+        \n+        irrelevant_computation = s32[] add(irrelevant_var, c1)\n+        \n+        next_other = s32[] add(other_var, c1)\n+\n+        ROOT result = (s32[], s32[], s32[], s32[]) tuple(next_ivar, dynamic_computation, irrelevant_computation, next_other)\n+      }\n+\n+      condition {\n+        p0 = (s32[], s32[], s32[], s32[]) parameter(0)\n+        ivar = s32[] get-tuple-element(p0), index=0\n+        c5 = s32[] constant(5)\n+        ROOT cmp = pred[] compare(ivar, c5), direction=LT\n+      }\n+\n+      ENTRY main {\n+        c0 = s32[] constant(0)\n+        tuple = (s32[], s32[], s32[], s32[]) tuple(c0, c0, c0, c0)\n+        ROOT while = (s32[], s32[], s32[], s32[]) while(tuple),\n+            condition=condition, body=while_body,\n+            backend_config={\"known_induction_variable\":{\"tuple_index\":\"0\"},\"dynamic_variable_tuple_indices\":[1, 2]}\n+      }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHlo));\n+  HloComputation* while_body = module->GetComputationWithName(\"while_body\");\n+\n+  ASSERT_TRUE(ResolveFunctionalDependencyOnInductionVariable(\n+                  while_body->GetInstructionWithName(\"next_ivar\"))\n+                  .has_value());\n+\n+  ASSERT_TRUE(ResolveFunctionalDependencyOnInductionVariable(\n+                  while_body->GetInstructionWithName(\"dynamic_computation\"))\n+                  .has_value());\n+\n+  ASSERT_TRUE(ResolveFunctionalDependencyOnInductionVariable(\n+                  while_body->GetInstructionWithName(\"irrelevant_computation\"))\n+                  .has_value());\n+\n+  ASSERT_FALSE(ResolveFunctionalDependencyOnInductionVariable(\n+                   while_body->GetInstructionWithName(\"next_other\"))\n+                   .has_value());\n+}\n+\n+TEST_F(IrEmissionUtilsTest, MultipleDynamicVariables) {\n+  constexpr absl::string_view kHlo = R\"(\n+      while_body {\n+        p0 = (s32[], s32[], s32[], s32[]) parameter(0)\n+        ivar = s32[] get-tuple-element(p0), index=0\n+        dynamic_var1 = s32[] get-tuple-element(p0), index=1\n+        dynamic_var2 = s32[] get-tuple-element(p0), index=2\n+        regular_var = s32[] get-tuple-element(p0), index=3\n+\n+        c1 = s32[] constant(1)\n+        next_ivar = s32[] add(ivar, c1)\n+        \n+        compute1 = s32[] add(dynamic_var1, c1)\n+        compute2 = s32[] add(dynamic_var2, c1)\n+        compute_regular = s32[] add(regular_var, c1)\n+\n+        ROOT result = (s32[], s32[], s32[], s32[]) tuple(next_ivar, compute1, compute2, compute_regular)\n+      }\n+\n+      condition {\n+        p0 = (s32[], s32[], s32[], s32[]) parameter(0)\n+        ivar = s32[] get-tuple-element(p0), index=0\n+        c5 = s32[] constant(5)\n+        ROOT cmp = pred[] compare(ivar, c5), direction=LT\n+      }\n+\n+      ENTRY main {\n+        c0 = s32[] constant(0)\n+        tuple = (s32[], s32[], s32[], s32[]) tuple(c0, c0, c0, c0)\n+        ROOT while = (s32[], s32[], s32[], s32[]) while(tuple),\n+            condition=condition, body=while_body,\n+            backend_config={\"known_induction_variable\":{\"tuple_index\":\"0\"},\"dynamic_variable_tuple_indices\":[1, 2]}\n+      }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHlo));\n+  HloComputation* while_body = module->GetComputationWithName(\"while_body\");\n+\n+  ASSERT_TRUE(ResolveFunctionalDependencyOnInductionVariable(\n+                  while_body->GetInstructionWithName(\"compute1\"))\n+                  .has_value());\n+\n+  ASSERT_TRUE(ResolveFunctionalDependencyOnInductionVariable(\n+                  while_body->GetInstructionWithName(\"compute2\"))\n+                  .has_value());\n+\n+  ASSERT_FALSE(ResolveFunctionalDependencyOnInductionVariable(\n+                   while_body->GetInstructionWithName(\"compute_regular\"))\n+                   .has_value());\n+}\n+\n TEST_F(IrEmissionUtilsTest, Transpose_10) {\n   auto spec = GetTransposeSpecFromRoot(R\"(ENTRY entry {\n     p0 = f32[8, 32] parameter(0)"
        },
        {
            "sha": "cae46d2980bfbb4209c1d5014644b582b200f7fa",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/gpu_collective_combiner_utils_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils_test.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -275,6 +275,7 @@ TEST_F(CollectiveCombinerUtilsTest,\n       /*reuse_pipelined_op_buffer=*/HloPredicateFalse,\n       /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n       /*should_allow_control_dependencies=*/false,\n+      /*additional_chain_start_op_finder=*/nullptr,\n       /*postprocess_backward_peeled_op=*/{},\n       /*postprocess_backward_rotated_op=*/{},\n       /*postprocess_backward_peeled_trailing_op=*/{},\n@@ -370,6 +371,7 @@ TEST_F(CollectiveCombinerUtilsTest,\n       /*reuse_pipelined_op_buffer=*/HloPredicateFalse,\n       /*should_allow_loop_variant_parameter_in_chain=*/HloPredicateFalse,\n       /*should_allow_control_dependencies=*/false,\n+      /*additional_chain_start_op_finder=*/nullptr,\n       /*postprocess_backward_peeled_op=*/{},\n       /*postprocess_backward_rotated_op=*/{},\n       /*postprocess_backward_peeled_trailing_op=*/{},"
        },
        {
            "sha": "fa58739643633ab454acfdf33156d9edd7814e1a",
            "filename": "third_party/xla/xla/service/gpu/transforms/double_buffer_loop_unrolling.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -256,8 +256,13 @@ absl::StatusOr<bool> FullyUnroll(HloInstruction* while_instr,\n     changed = true;\n   }\n \n-  WhileLoopBackendConfig new_config;\n+  WhileLoopBackendConfig old_config;\n+  TF_ASSIGN_OR_RETURN(old_config,\n+                      while_instr->backend_config<WhileLoopBackendConfig>());\n+\n+  WhileLoopBackendConfig new_config = old_config;\n   new_config.mutable_known_trip_count()->set_n(1);\n+\n   TF_RETURN_IF_ERROR(while_instr->set_backend_config(new_config));\n \n   return changed;\n@@ -394,15 +399,9 @@ absl::StatusOr<bool> DoubleBufferingUnroll(HloInstruction* while_instr,\n                                                &old_loop_roots, input_parameter,\n                                                skip_control_dep_injection));\n \n-  WhileLoopBackendConfig new_config;\n+  WhileLoopBackendConfig new_config = config;\n   new_config.mutable_known_trip_count()->set_n(exact_trip_count / 2);\n \n-  // Keep known induction variable metadata if it was present before.\n-  if (config.has_known_induction_variable()) {\n-    *new_config.mutable_known_induction_variable() =\n-        config.known_induction_variable();\n-  }\n-\n   // Update the init/step metadata if it was present before.\n   if (config.has_known_init_step()) {\n     int64_t step = config.known_init_step().step();"
        },
        {
            "sha": "dbe914ce6037d3adfba501e21de914702106fa35",
            "filename": "third_party/xla/xla/service/gpu/transforms/double_buffer_loop_unrolling_test.cc",
            "status": "modified",
            "additions": 90,
            "deletions": 1,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdouble_buffer_loop_unrolling_test.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -18,6 +18,8 @@ limitations under the License.\n #include <cstdint>\n #include <memory>\n #include <optional>\n+#include <set>\n+#include <vector>\n \n #include <gtest/gtest.h>\n #include \"absl/container/flat_hash_set.h\"\n@@ -42,7 +44,6 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-\n int64_t CountInstructions(HloComputation& computation, HloOpcode opcode) {\n   int64_t count = 0;\n   hlo_query::ForEachInstructionWithOpcode(\n@@ -1498,6 +1499,94 @@ TEST_F(GpuLoopDoubleBufferTransformerTest, UpdateInitStepEvenTripCount) {\n   EXPECT_EQ(config.known_init_step().step(), 4);\n }\n \n+TEST_F(GpuLoopDoubleBufferTransformerTest,\n+       PreserveDynamicVariableIndicesAfterDoubleBuffering) {\n+  absl::string_view kModuleString = R\"(\n+HloModule test\n+\n+condition {\n+  input_tuple = (s32[], f32[2,8]{1,0:S(5)}, f32[1,8]{1,0}, s32[]) parameter(0)\n+  cond = s32[] get-tuple-element(input_tuple), index=0\n+  trip_count = s32[] constant(10)\n+  ROOT done = pred[] compare(cond, trip_count), direction=LT\n+}\n+\n+body {\n+  input_tuple = (s32[], f32[2,8]{1,0:S(5)}, f32[1,8]{1,0}, s32[]) parameter(0)\n+  idx = s32[] get-tuple-element(input_tuple), index=0\n+  buffer = f32[2,8]{1,0:S(5)} get-tuple-element(input_tuple), index=1\n+  update = f32[1,8]{1,0} get-tuple-element(input_tuple), index=2\n+  counter = s32[] get-tuple-element(input_tuple), index=3\n+\n+  c0 = s32[] constant(0)\n+  dus = f32[2,8]{1,0:S(5)} dynamic-update-slice(buffer, update, idx, c0)\n+\n+  c1 = s32[] constant(1)\n+  idx_plus_1 = s32[] add(idx, c1)\n+  counter_plus_1 = s32[] add(counter, c1)\n+  ROOT output = (s32[], f32[2,8]{1,0:S(5)}, f32[1,8]{1,0}, s32[]) tuple(idx_plus_1, dus, update, counter_plus_1)\n+}\n+\n+ENTRY main {\n+  c0 = s32[] constant(0)\n+  buffer_init = f32[2,8]{1,0:S(5)} parameter(0)\n+  update_init = f32[1,8]{1,0} parameter(1)\n+  input_tuple = (s32[], f32[2,8]{1,0:S(5)}, f32[1,8]{1,0}, s32[]) tuple(c0, buffer_init, update_init, c0)\n+  ROOT while = (s32[], f32[2,8]{1,0:S(5)}, f32[1,8]{1,0}, s32[]) while(input_tuple), condition=condition, body=body, backend_config={\"known_trip_count\":{\"n\":\"10\"},\"known_induction_variable\":{\"tuple_index\":\"0\"},\"dynamic_variable_tuple_indices\":[\"3\",\"0\"]}\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kModuleString));\n+\n+  DoubleBufferLoopUnrolling double_buffer(\n+      DoubleBufferLoopUnrolling::UnrollStrategy::kDoubleBuffer);\n+  TupleSimplifier tuple_simplifier;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, double_buffer.Run(module.get()));\n+  ASSERT_TRUE(changed);\n+  TF_ASSERT_OK_AND_ASSIGN(changed, tuple_simplifier.Run(module.get()));\n+\n+  std::vector<HloInstruction*> while_loops;\n+  for (HloComputation* comp : module->computations()) {\n+    for (HloInstruction* instr : comp->instructions()) {\n+      if (instr->opcode() == HloOpcode::kWhile) {\n+        while_loops.push_back(instr);\n+      }\n+    }\n+  }\n+\n+  ASSERT_FALSE(while_loops.empty())\n+      << \"Expected at least one while loop after double buffering\";\n+\n+  for (HloInstruction* while_loop : while_loops) {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        WhileLoopBackendConfig config,\n+        while_loop->backend_config<WhileLoopBackendConfig>());\n+\n+    std::set<int64_t> dynamic_indices(\n+        config.dynamic_variable_tuple_indices().begin(),\n+        config.dynamic_variable_tuple_indices().end());\n+\n+    EXPECT_FALSE(dynamic_indices.empty())\n+        << \"Expected dynamic_variable_tuple_indices to be preserved for while \"\n+           \"loop: \"\n+        << while_loop->name()\n+        << \". Double buffering should not erase indices set by \"\n+           \"CollectivePipeliner.\";\n+\n+    EXPECT_NE(dynamic_indices.find(0), dynamic_indices.end())\n+        << \"Expected tuple index 0 (induction variable) to be preserved as \"\n+           \"dynamic for while loop: \"\n+        << while_loop->name();\n+\n+    EXPECT_NE(dynamic_indices.find(3), dynamic_indices.end())\n+        << \"Expected tuple index 3 (additional counter) to be preserved as \"\n+           \"dynamic for while loop: \"\n+        << while_loop->name();\n+  }\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "1473db05c250f9440416322f47cd8f987f75ceb0",
            "filename": "third_party/xla/xla/service/host_offload_utils.cc",
            "status": "modified",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fhost_offload_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fhost_offload_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhost_offload_utils.cc?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -353,5 +353,88 @@ bool IsMoveToDeviceWithDynamicSlice(const HloInstruction* instr) {\n   return false;\n }\n \n+namespace {\n+\n+// Recursively finds GTE indices used in DS/DUS index operands.\n+absl::flat_hash_set<int64_t> FindTupleIndicesInOperand(\n+    const HloInstruction* operand) {\n+  absl::flat_hash_set<int64_t> indices;\n+\n+  if (operand->opcode() == HloOpcode::kGetTupleElement) {\n+    indices.insert(operand->tuple_index());\n+  } else if (operand->opcode() == HloOpcode::kCopy &&\n+             operand->operand_count() == 1) {\n+    auto copy_indices = FindTupleIndicesInOperand(operand->operand(0));\n+    indices.insert(copy_indices.begin(), copy_indices.end());\n+  } else if (operand->opcode() == HloOpcode::kAdd ||\n+             operand->opcode() == HloOpcode::kSubtract ||\n+             operand->opcode() == HloOpcode::kMultiply ||\n+             operand->opcode() == HloOpcode::kDivide) {\n+    for (int i = 0; i < operand->operand_count(); ++i) {\n+      auto op_indices = FindTupleIndicesInOperand(operand->operand(i));\n+      indices.insert(op_indices.begin(), op_indices.end());\n+    }\n+  }\n+\n+  return indices;\n+}\n+\n+}  // namespace\n+\n+absl::Status MarkDynamicVariables(HloInstruction* while_loop) {\n+  if (while_loop->opcode() != HloOpcode::kWhile) {\n+    return absl::OkStatus();\n+  }\n+\n+  if (!while_loop->while_body()) {\n+    return absl::OkStatus();\n+  }\n+\n+  bool has_host_offloading = false;\n+  for (const HloInstruction* instr : while_loop->while_body()->instructions()) {\n+    if (IsMoveToHostWithDynamicUpdateSlice(instr) ||\n+        IsMoveToDeviceWithDynamicSlice(instr)) {\n+      has_host_offloading = true;\n+      break;\n+    }\n+  }\n+  if (!has_host_offloading) {\n+    return absl::OkStatus();\n+  }\n+\n+  WhileLoopBackendConfig config;\n+  TF_ASSIGN_OR_RETURN(config,\n+                      while_loop->backend_config<WhileLoopBackendConfig>());\n+\n+  config.clear_dynamic_variable_tuple_indices();\n+\n+  std::set<int64_t> dynamic_slice_indices;\n+\n+  for (auto* instr : while_loop->while_body()->instructions()) {\n+    if (instr->opcode() == HloOpcode::kDynamicUpdateSlice ||\n+        instr->opcode() == HloOpcode::kDynamicSlice) {\n+      int first_index_operand =\n+          (instr->opcode() == HloOpcode::kDynamicUpdateSlice)\n+              ? Cast<HloDynamicUpdateSliceInstruction>(instr)\n+                    ->first_index_operand_number()\n+              : Cast<HloDynamicSliceInstruction>(instr)\n+                    ->first_index_operand_number();\n+\n+      for (int i = first_index_operand; i < instr->operand_count(); ++i) {\n+        auto* index_op = instr->operand(i);\n+        auto op_indices = FindTupleIndicesInOperand(index_op);\n+        dynamic_slice_indices.insert(op_indices.begin(), op_indices.end());\n+      }\n+    }\n+  }\n+\n+  for (int64_t tuple_idx : dynamic_slice_indices) {\n+    config.add_dynamic_variable_tuple_indices(tuple_idx);\n+  }\n+\n+  TF_RETURN_IF_ERROR(while_loop->set_backend_config(config));\n+  return absl::OkStatus();\n+}\n+\n }  // namespace host_offload_utils\n }  // namespace xla"
        },
        {
            "sha": "e49b9b64bd423bfbcf24ea537dcaf5cd4c75679b",
            "filename": "third_party/xla/xla/service/host_offload_utils.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fhost_offload_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fservice%2Fhost_offload_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhost_offload_utils.h?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -111,6 +111,10 @@ bool IsMoveToHostWithDynamicUpdateSlice(const HloInstruction* instr);\n \n bool IsMoveToDeviceWithDynamicSlice(const HloInstruction* instr);\n \n+// Scans while loop body for DS/DUS, traces their index operands back to GTEs\n+// and marks corresponding tuple indices as dynamic variables.\n+absl::Status MarkDynamicVariables(HloInstruction* while_loop);\n+\n }  // namespace host_offload_utils\n }  // namespace xla\n "
        },
        {
            "sha": "054a2de5446d649b79a2021a9cf1c60f318d088f",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -550,6 +550,8 @@ message DebugOptions {\n \n   optional bool xla_gpu_enable_pipelined_collectives = 239 [deprecated = true];\n \n+  optional bool xla_gpu_enable_pipelined_host_offloading = 440;\n+\n   optional bool xla_gpu_enable_pipelined_p2p = 246;\n \n   optional bool xla_gpu_enable_pipelined_reduce_scatter = 231;\n@@ -1328,7 +1330,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 440\n+  // Next id: 441\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        },
        {
            "sha": "92add3f8d46b573e7f827617fb10dabe50f1c732",
            "filename": "third_party/xla/xla/xla_data.proto",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fxla_data.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/96ed126a594735c8106699b4f9c2ac8f74798242/third_party%2Fxla%2Fxla%2Fxla_data.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla_data.proto?ref=96ed126a594735c8106699b4f9c2ac8f74798242",
            "patch": "@@ -1383,6 +1383,10 @@ message WhileLoopBackendConfig {\n   // This lets us distinguish between an unknown induction variable (or none)\n   // and tuple index 0.\n   KnownInductionVariable known_induction_variable = 3;\n+\n+  // Variables that should be treated as induction variables for dynamic memcpy\n+  // analysis, even though they are not the primary induction variable.\n+  repeated int64 dynamic_variable_tuple_indices = 4;\n }\n \n // Specifies a pair of output/operand buffers that alias each other for"
        }
    ],
    "stats": {
        "total": 951,
        "additions": 881,
        "deletions": 70
    }
}