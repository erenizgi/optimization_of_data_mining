{
    "author": "derdrdirk",
    "message": "[Autotuner]Noop: Reuse the primary stream from the allocator instead of creating a new stream when profiling autotuning results. WeRunning cuBLAS on multiple streams concurrently is currently not supported.\n\nPiperOrigin-RevId: 802978813",
    "sha": "bb4e7f5f1a6e390944ac15afee3a6c6c98461da9",
    "files": [
        {
            "sha": "b3fe67edbca4a48a6b2010cc806ed404f8248cd6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bb4e7f5f1a6e390944ac15afee3a6c6c98461da9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bb4e7f5f1a6e390944ac15afee3a6c6c98461da9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc?ref=bb4e7f5f1a6e390944ac15afee3a6c6c98461da9",
            "patch": "@@ -100,14 +100,17 @@ std::unique_ptr<GpuProfiler> GpuProfiler::Create(\n     active_allocator = owned_allocator.get();\n   }\n \n-  auto stream = stream_executor->CreateStream();\n+  // TODO(b/442997461): Create a new stream using\n+  // `stream_executor->CreateStream()` instead of reusing the allocator stream\n+  // once we can handle cuBLAS using multiple streams.\n+  auto stream = active_allocator->GetStream(stream_executor->device_ordinal());\n   if (!stream.ok()) {\n     LOG(ERROR) << \"Failed to create stream: \" << stream.status();\n     return nullptr;\n   }\n   return absl::WrapUnique(new GpuProfiler(stream_executor, active_allocator,\n                                           std::move(owned_allocator),\n-                                          std::move(stream.value()), options));\n+                                          stream.value(), options));\n }\n \n absl::StatusOr<std::unique_ptr<InputBuffers>> GpuProfiler::CreateInputBuffers(\n@@ -119,7 +122,7 @@ absl::StatusOr<std::unique_ptr<InputBuffers>> GpuProfiler::CreateInputBuffers(\n           RedzoneBuffers::BuffersToCreate::kAllInputs,\n           options_.should_init_buffers,\n           /*should_check_correctness=*/true, options_.redzone_padding_bytes,\n-          allocator_, stream_.get()));\n+          allocator_, stream_));\n   auto gpu_buffers = std::make_unique<GpuInputBuffers>();\n   gpu_buffers->redzone_buffers = std::move(buffers);\n   return gpu_buffers;\n@@ -175,7 +178,7 @@ absl::StatusOr<ExecutionOutput> GpuProfiler::Execute(\n \n   ExecutableRunOptions run_options;\n   run_options.set_device_ordinal(stream_executor_->device_ordinal());\n-  run_options.set_stream(stream_.get());\n+  run_options.set_stream(stream_);\n   run_options.set_allocator(allocator_);\n   run_options.set_gpu_executable_run_options(&gpu_opts);\n   run_options.set_execution_profile(profile);\n@@ -205,10 +208,9 @@ absl::Status GpuProfiler::CheckOutputBuffer(ScopedShapedBuffer& output,\n                                             float rtol) {\n   BufferComparator comparator(output.on_device_shape(), rtol);\n \n-  TF_ASSIGN_OR_RETURN(\n-      bool outputs_match,\n-      comparator.CompareEqual(stream_.get(), output.root_buffer(),\n-                              reference.root_buffer()));\n+  TF_ASSIGN_OR_RETURN(bool outputs_match,\n+                      comparator.CompareEqual(stream_, output.root_buffer(),\n+                                              reference.root_buffer()));\n   if (outputs_match) {\n     return absl::OkStatus();\n   }"
        },
        {
            "sha": "7ff255465de85b5f7ea82bf66ff45f4737a893b9",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bb4e7f5f1a6e390944ac15afee3a6c6c98461da9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bb4e7f5f1a6e390944ac15afee3a6c6c98461da9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h?ref=bb4e7f5f1a6e390944ac15afee3a6c6c98461da9",
            "patch": "@@ -63,11 +63,11 @@ class GpuProfiler : public Profiler {\n   explicit GpuProfiler(\n       se::StreamExecutor* stream_executor, se::DeviceMemoryAllocator* allocator,\n       std::unique_ptr<se::DeviceMemoryAllocator> owned_allocator,\n-      std::unique_ptr<se::Stream> stream, ProfileOptions options)\n+      se::Stream* stream, ProfileOptions options)\n       : stream_executor_(stream_executor),\n         allocator_(allocator),\n         owned_allocator_(std::move(owned_allocator)),\n-        stream_(std::move(stream)),\n+        stream_(stream),\n         options_(options) {}\n \n   absl::StatusOr<ExecutionOutput> Execute(Executable* executable,\n@@ -77,7 +77,7 @@ class GpuProfiler : public Profiler {\n   se::StreamExecutor* stream_executor_;\n   se::DeviceMemoryAllocator* allocator_;\n   std::unique_ptr<se::DeviceMemoryAllocator> owned_allocator_;\n-  std::unique_ptr<se::Stream> stream_;\n+  se::Stream* stream_;\n   ProfileOptions options_;\n };\n "
        }
    ],
    "stats": {
        "total": 24,
        "additions": 13,
        "deletions": 11
    }
}