{
    "author": "pschuh",
    "message": "Make converting PjRtBuffers to literals just a little bit easier.\n\nPiperOrigin-RevId: 828115160",
    "sha": "b4a6aabe28d482cc066815fdaf02a739c90ec12b",
    "files": [
        {
            "sha": "6e70b5de3bc9dc3a46f0f53a687a4ae0a95f8df5",
            "filename": "third_party/xla/xla/pjrt/pjrt_client.cc",
            "status": "modified",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a6aabe28d482cc066815fdaf02a739c90ec12b/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a6aabe28d482cc066815fdaf02a739c90ec12b/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client.cc?ref=b4a6aabe28d482cc066815fdaf02a739c90ec12b",
            "patch": "@@ -17,21 +17,26 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n+#include <optional>\n #include <string>\n #include <utility>\n+#include <vector>\n \n #include \"absl/base/casts.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/substitute.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/future.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/literal.h\"\n #include \"xla/pjrt/pjrt_common.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/pjrt/utils.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/errors.h\"\n@@ -99,4 +104,65 @@ PjRtExecutable* PjRtLoadedExecutable::GetExecutable() const {\n   return executable_forwarder_.get();\n }\n \n+absl::StatusOr<Shape> PjRtBuffer::HostShape() {\n+  Shape device_shape;\n+  if (!IsTuple()) {\n+    absl::Span<const int64_t> literal_dims;\n+    std::optional<std::vector<int64_t>> logical_dims_storage;\n+    if (has_dynamic_dimensions()) {\n+      TF_ASSIGN_OR_RETURN(std::vector<int64_t> logical_dims,\n+                          logical_dimensions());\n+      logical_dims_storage.emplace(std::move(logical_dims));\n+      literal_dims = *logical_dims_storage;\n+    } else {\n+      literal_dims = dimensions();\n+    }\n+    if (element_type() == TOKEN) {\n+      device_shape = ShapeUtil::MakeTokenShape();\n+    } else {\n+      device_shape = ShapeUtil::MakeShape(element_type(), literal_dims);\n+      // TODO(b/327524065): use PjRtLayout directly instead of xla::Layout\n+      *device_shape.mutable_layout() = layout()->xla_layout();\n+    }\n+  } else {\n+    // TODO(skyewm): does anything need to create tuple literals? The PJRT C\n+    // API doesn't support tuples or {logical_}on_device_shape(), so we prefer\n+    // to use the above non-tuple code path where possible.\n+    device_shape = on_device_shape();\n+    if (device_shape.is_dynamic()) {\n+      TF_ASSIGN_OR_RETURN(device_shape, logical_on_device_shape());\n+    }\n+  }\n+  return ShapeUtil::DeviceShapeToHostShape(device_shape);\n+}\n+\n+xla::Future<std::shared_ptr<Literal>> PjRtBuffer::ToLiteral() {\n+  absl::StatusOr<Shape> host_shape = HostShape();\n+  if (!host_shape.ok()) {\n+    return xla::Future<std::shared_ptr<Literal>>(host_shape.status());\n+  }\n+  auto [promise, future] = xla::Future<std::shared_ptr<Literal>>::MakePromise();\n+  auto shared_literal = std::make_shared<Literal>();\n+  Literal* literal = shared_literal.get();\n+  LazyToLiteral([literal, host_shape = *std::move(\n+                              host_shape)]() -> Future<MutableLiteralBase*> {\n+    auto literal_or = Literal::Make(host_shape);\n+    if (!literal_or.ok()) {\n+      return Future<MutableLiteralBase*>(literal_or.status());\n+    }\n+    *literal = *std::move(literal_or);\n+    return Future<MutableLiteralBase*>(literal);\n+  })\n+      .OnReady(\n+          [promise = std::move(promise),\n+           shared_literal = std::move(shared_literal)](absl::Status s) mutable {\n+            if (!s.ok()) {\n+              std::move(promise).Set(s);\n+            } else {\n+              std::move(promise).Set(std::move(shared_literal));\n+            }\n+          });\n+  return future;\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "41b1fba22eca5e60b0f514abd45ac5dc690b9858",
            "filename": "third_party/xla/xla/pjrt/pjrt_client.h",
            "status": "modified",
            "additions": 9,
            "deletions": 45,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a6aabe28d482cc066815fdaf02a739c90ec12b/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a6aabe28d482cc066815fdaf02a739c90ec12b/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client.h?ref=b4a6aabe28d482cc066815fdaf02a739c90ec12b",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/attributes.h\"\n+#include \"absl/base/macros.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/inlined_vector.h\"\n@@ -1136,58 +1137,21 @@ class PjRtBuffer {\n \n   // Synchronous overload of ToLiteral, as a convenience.\n   absl::Status ToLiteralSync(MutableLiteralBase* literal) {\n-    absl::Notification done;\n-    absl::Status status;\n-    ToLiteral(literal).OnReady([&](absl::Status s) {\n-      status = std::move(s);\n-      done.Notify();\n-    });\n-    done.WaitForNotification();\n-    return status;\n-  }\n-\n-  absl::StatusOr<Shape> HostShape() {\n-    Shape device_shape;\n-    if (!IsTuple()) {\n-      absl::Span<const int64_t> literal_dims;\n-      std::optional<std::vector<int64_t>> logical_dims_storage;\n-      if (has_dynamic_dimensions()) {\n-        TF_ASSIGN_OR_RETURN(std::vector<int64_t> logical_dims,\n-                            logical_dimensions());\n-        logical_dims_storage.emplace(std::move(logical_dims));\n-        literal_dims = *logical_dims_storage;\n-      } else {\n-        literal_dims = dimensions();\n-      }\n-      if (element_type() == TOKEN) {\n-        device_shape = ShapeUtil::MakeTokenShape();\n-      } else {\n-        device_shape = ShapeUtil::MakeShape(element_type(), literal_dims);\n-        // TODO(b/327524065): use PjRtLayout directly instead of xla::Layout\n-        *device_shape.mutable_layout() = layout()->xla_layout();\n-      }\n-    } else {\n-      // TODO(skyewm): does anything need to create tuple literals? The PJRT C\n-      // API doesn't support tuples or {logical_}on_device_shape(), so we prefer\n-      // to use the above non-tuple code path where possible.\n-      device_shape = on_device_shape();\n-      if (device_shape.is_dynamic()) {\n-        TF_ASSIGN_OR_RETURN(device_shape, logical_on_device_shape());\n-      }\n-    }\n-    return ShapeUtil::DeviceShapeToHostShape(device_shape);\n+    return ToLiteral(literal).Await();\n   }\n \n+  absl::StatusOr<Shape> HostShape();\n+\n   // Convenience synchronous overload that allocates a literal with a default\n   // layout.\n+  ABSL_DEPRECATE_AND_INLINE()\n   absl::StatusOr<std::shared_ptr<Literal>> ToLiteralSync() {\n-    TF_ASSIGN_OR_RETURN(Shape host_shape, HostShape());\n-    TF_ASSIGN_OR_RETURN(auto literal, Literal::Make(host_shape));\n-    auto shared_literal = std::make_shared<Literal>(std::move(literal));\n-    TF_RETURN_IF_ERROR(ToLiteralSync(shared_literal.get()));\n-    return shared_literal;\n+    return ToLiteral().Await();\n   }\n \n+  // ToLiteral overload which async allocates a literal with default layout.\n+  xla::Future<std::shared_ptr<Literal>> ToLiteral();\n+\n   // Returns the number of bytes of the buffer storage on the device.\n   virtual absl::StatusOr<size_t> GetOnDeviceSizeInBytes() const = 0;\n "
        },
        {
            "sha": "28b44ee0534d520b5b8efe3973032390eeaa229d",
            "filename": "third_party/xla/xla/pjrt/pjrt_client_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b4a6aabe28d482cc066815fdaf02a739c90ec12b/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b4a6aabe28d482cc066815fdaf02a739c90ec12b/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client_test.cc?ref=b4a6aabe28d482cc066815fdaf02a739c90ec12b",
            "patch": "@@ -544,11 +544,7 @@ TEST(PjRtClientTest, FulfillAliasBuffer) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto alias_buffer,\n       client->CreateAliasBuffer(shape, client->memory_spaces()[0]));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(Shape host_shape, alias_buffer.first->HostShape());\n-  TF_ASSERT_OK_AND_ASSIGN(auto literal, Literal::Make(host_shape));\n-  auto shared_literal = std::make_shared<Literal>(std::move(literal));\n-  auto future = alias_buffer.first->ToLiteral(shared_literal.get());\n+  auto future = alias_buffer.first->ToLiteral();\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto param,\n@@ -560,7 +556,7 @@ TEST(PjRtClientTest, FulfillAliasBuffer) {\n \n   ASSERT_NE(alias_buffer.second, nullptr);\n   TF_ASSERT_OK(std::move(alias_buffer.second)(param.get()));\n-  TF_ASSERT_OK(future.Await());\n+  TF_ASSERT_OK_AND_ASSIGN(auto shared_literal, future.Await());\n \n   std::vector<int32_t> expected = {1, 2, 3, 4, 5, 6};\n   EXPECT_EQ(shared_literal->data<int32_t>(), expected);"
        }
    ],
    "stats": {
        "total": 128,
        "additions": 77,
        "deletions": 51
    }
}