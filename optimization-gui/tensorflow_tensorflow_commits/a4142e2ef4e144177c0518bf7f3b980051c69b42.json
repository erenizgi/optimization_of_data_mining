{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 850667610",
    "sha": "a4142e2ef4e144177c0518bf7f3b980051c69b42",
    "files": [
        {
            "sha": "438abcea76f9392a3c0190bfb51d4532701bded9",
            "filename": "tensorflow/dtensor/mlir/utils/collective_lowering.cc",
            "status": "modified",
            "additions": 79,
            "deletions": 77,
            "changes": 156,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a4142e2ef4e144177c0518bf7f3b980051c69b42/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fcollective_lowering.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a4142e2ef4e144177c0518bf7f3b980051c69b42/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fcollective_lowering.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fcollective_lowering.cc?ref=a4142e2ef4e144177c0518bf7f3b980051c69b42",
            "patch": "@@ -84,11 +84,11 @@ namespace {\n namespace internal {\n \n namespace ops_util = ::mlir::TF::collection_ops_util;\n-constexpr int32 kUninitializedGroupKey = 0;\n+constexpr int32_t kUninitializedGroupKey = 0;\n constexpr char kCpuDevice[] = \"/device:CPU:0\";\n constexpr char kDeviceAttr[] = \"device\";\n \n-std::atomic<int32> tf_collective_instance_key_base{0};\n+std::atomic<int32_t> tf_collective_instance_key_base{0};\n \n bool HasEnableReuseGroupKey() {\n   // FIXME(b/258703996): use tsl::ReadBoolFromEnvVar()\n@@ -117,7 +117,7 @@ bool UseNcclCommunicationOnGpu() {\n mlir::LogicalResult EmitAllReduceForXla(\n     mlir::MLIRContext& context, mlir::OpBuilder& builder,\n     mlir::TF::DTensorAllReduceOp all_reduce,\n-    mlir::DenseIntElementsAttr group_assignment_attr, int32 key_base,\n+    mlir::DenseIntElementsAttr group_assignment_attr, int32_t key_base,\n     mlir::Operation** final_op) {\n   constexpr char kCrossReplica[] = \"CrossReplica\";\n \n@@ -137,18 +137,18 @@ llvm::SmallVector<int32_t, 4> GetGroupKeyOffsets(\n   *group_size = shape[1];\n   const int32_t num_devices = num_groups * *group_size;\n \n-  llvm::SmallVector<int32, 4> device_id_to_group_key(num_devices);\n+  llvm::SmallVector<int32_t, 4> device_id_to_group_key(num_devices);\n   device_id_to_group_key.resize(num_devices, kUninitializedGroupKey);\n   // 21 bits + 11 bits allow roughly 2M all-reduces in one program and up to a\n   // full DF pod.\n   DCHECK_LE(num_devices, 1L << 11) << \"Exceeding 2048 groups.\";\n   for (const auto& it :\n        llvm::enumerate(group_assignment.getValues<llvm::APInt>())) {\n-    int32 device_id = it.value().getSExtValue();\n+    int32_t device_id = it.value().getSExtValue();\n     DCHECK_LE(0, device_id);\n     DCHECK_LT(device_id, num_devices);\n     DCHECK_EQ(device_id_to_group_key[device_id], kUninitializedGroupKey);\n-    const int32 group_offset = static_cast<int32>(it.index()) / *group_size;\n+    const int32_t group_offset = static_cast<int32_t>(it.index()) / *group_size;\n     device_id_to_group_key[device_id] = group_offset;\n   }\n   return device_id_to_group_key;\n@@ -163,18 +163,18 @@ int32_t GetCollectiveKeyBase(\n   // same MLIR logic and therefore iterate over AllReduce ops in the same order\n   // (even in the presence of control flow), so they should indenpendently\n   // generate the same counter value for matching AllReduce ops across hosts.\n-  static std::atomic<int32> tf_collective_key_base{0};\n+  static std::atomic<int32_t> tf_collective_key_base{0};\n \n   if (!HasEnableReuseGroupKey()) {\n     return tf_collective_key_base++;\n   }\n   // Use an atomic counter to generate bases for group and instance keys.\n   static tensorflow::mutex* mtx = new tensorflow::mutex();\n   static auto* mesh_to_key_base =\n-      new std::map<std::tuple<std::string, llvm::SmallVector<int32, 4>>,\n+      new std::map<std::tuple<std::string, llvm::SmallVector<int32_t, 4>>,\n                    int32_t>();\n   int32_t group_size;\n-  const llvm::SmallVector<int32, 4> group_key_offsets =\n+  const llvm::SmallVector<int32_t, 4> group_key_offsets =\n       GetGroupKeyOffsets(group_assignment, &group_size);\n \n   const auto iter =\n@@ -203,11 +203,11 @@ mlir::Value GetRelativeDeviceId(mlir::Operation* op,\n \n void CreateGroupAndInstanceKey(\n     mlir::OpBuilder& builder, const mlir::Location& loc,\n-    const mlir::DenseIntElementsAttr& group_assignment, int32 key_base,\n+    const mlir::DenseIntElementsAttr& group_assignment, int32_t key_base,\n     mlir::Value device_id, mlir::Value* group_key_scalar,\n     mlir::Value* instance_key_scalar) {\n   int32_t group_size;\n-  llvm::SmallVector<int32, 4> device_id_to_group_key =\n+  llvm::SmallVector<int32_t, 4> device_id_to_group_key =\n       GetGroupKeyOffsets(group_assignment, &group_size);\n   // 21 bits + 11 bits allow roughly 2M all-reduces in one program and up to a\n   // full DF pod.\n@@ -232,7 +232,7 @@ void CreateGroupAndInstanceKey(\n \n   // Generate a unique instance key for this collective.\n   *instance_key_scalar = ops_util::CreateScalarConst(\n-      static_cast<int32>(tf_collective_instance_key_base++), builder,\n+      static_cast<int32_t>(tf_collective_instance_key_base++), builder,\n       DT_LOC2(loc, \"instance_key\"));\n }\n \n@@ -247,8 +247,8 @@ void CreateGroupAndInstanceKey(\n mlir::Operation* EmitCollectiveReduce(\n     mlir::OpBuilder& builder, const mlir::Location& loc, mlir::Value input,\n     const std::string& reduce_op_str,\n-    const mlir::DenseIntElementsAttr& group_assignment, int32 key_base,\n-    mlir::Value device_id, int32 host_group_size,\n+    const mlir::DenseIntElementsAttr& group_assignment, int32_t key_base,\n+    mlir::Value device_id, int32_t host_group_size,\n     const mlir::StringRef device_type) {\n   mlir::Value group_key_scalar;\n   mlir::Value instance_key_scalar;\n@@ -275,14 +275,14 @@ mlir::Operation* EmitCollectiveReduce(\n mlir::Operation* EmitCollectiveReduceScatter(\n     mlir::OpBuilder& builder, const mlir::Location& loc, mlir::Value input,\n     mlir::Type output_type, const std::string& reduce_op_str,\n-    const mlir::DenseIntElementsAttr& group_assignment, int32 scatter_dimension,\n-    int32 key_base, mlir::Value device_id, int32 host_group_size,\n-    const mlir::StringRef device_type) {\n+    const mlir::DenseIntElementsAttr& group_assignment,\n+    int32_t scatter_dimension, int32_t key_base, mlir::Value device_id,\n+    int32_t host_group_size, const mlir::StringRef device_type) {\n   mlir::TensorType input_type =\n       mlir::dyn_cast<mlir::TensorType>(input.getType());\n \n   const bool need_transpose = scatter_dimension != 0;\n-  std::vector<int64> perm_for_transpose;\n+  std::vector<int64_t> perm_for_transpose;\n   if (need_transpose) {\n     perm_for_transpose.reserve(input_type.getRank());\n     for (int i = 0; i < input_type.getRank(); i++) {\n@@ -296,8 +296,8 @@ mlir::Operation* EmitCollectiveReduceScatter(\n     // Compute transposed output type for CollectiveReduceScatter\n     auto output_shape =\n         mlir::dyn_cast<mlir::TensorType>(output_type).getShape();\n-    std::vector<int64> transposed_shape(output_shape.begin(),\n-                                        output_shape.end());\n+    std::vector<int64_t> transposed_shape(output_shape.begin(),\n+                                          output_shape.end());\n     for (int i = 0; i < output_shape.size(); i++) {\n       transposed_shape[i] = output_shape[perm_for_transpose[i]];\n     }\n@@ -339,9 +339,10 @@ mlir::Operation* EmitCollectiveReduceScatter(\n \n mlir::Operation* EmitCollectiveAllToAll(\n     mlir::OpBuilder& builder, const mlir::Location& loc, mlir::Value input,\n-    const mlir::DenseIntElementsAttr& group_assignment, int32 concat_dimension,\n-    int32 split_dimension, int32 key_base, mlir::Value device_id,\n-    int32 host_group_size, const mlir::StringRef device_type) {\n+    const mlir::DenseIntElementsAttr& group_assignment,\n+    int32_t concat_dimension, int32_t split_dimension, int32_t key_base,\n+    mlir::Value device_id, int32_t host_group_size,\n+    const mlir::StringRef device_type) {\n   // This function implements an all-to-all with variable split and concat\n   // dimensions using the CollectiveAllToAllV2 which treats the input as a flat\n   // buffer. This requires permuting the data before or after the all-to-all\n@@ -359,11 +360,11 @@ mlir::Operation* EmitCollectiveAllToAll(\n   // the same side of all-to-all.\n   const bool permute_before = concat_dimension < split_dimension;\n   const bool requires_transpose = concat_dimension != 0 && split_dimension != 0;\n-  std::vector<int64> transposed_shape(input_shape.begin(), input_shape.end());\n-  std::vector<int64> original_shape(input_shape);\n+  std::vector<int64_t> transposed_shape(input_shape.begin(), input_shape.end());\n+  std::vector<int64_t> original_shape(input_shape);\n   int move_dims = std::min(concat_dimension, split_dimension);\n   if (requires_transpose) {\n-    std::vector<int64> perm_for_transpose;\n+    std::vector<int64_t> perm_for_transpose;\n     perm_for_transpose.reserve(input_shape.size());\n     // Move all dims before concat/split to end. This will be undone after the\n     // all-to-all.\n@@ -387,7 +388,7 @@ mlir::Operation* EmitCollectiveAllToAll(\n \n   auto permute_data = [&](mlir::Value data) {\n     // Reshape\n-    std::vector<int64> new_shape;\n+    std::vector<int64_t> new_shape;\n     new_shape.reserve(input_shape.size() + 1);\n     for (int i = 0; i < input_shape.size(); ++i) {\n       if (i == split_dimension) {\n@@ -400,7 +401,7 @@ mlir::Operation* EmitCollectiveAllToAll(\n     auto reshape_op = mlir::TF::ReshapeOp::create(\n         builder, loc, data, ops_util::GetR1Const(new_shape, builder, loc));\n \n-    std::vector<int64> perm_for_permute_transpose;\n+    std::vector<int64_t> perm_for_permute_transpose;\n     perm_for_permute_transpose.reserve(input_shape.size() + 1);\n     for (int i = 0; i < input_shape.size(); ++i) {\n       if (i == concat_dimension) {\n@@ -419,10 +420,10 @@ mlir::Operation* EmitCollectiveAllToAll(\n \n   // Flatten input. CPU implementation requires first dim to equal the group\n   // size.\n-  int64 num_elements = std::accumulate(input_shape.begin(), input_shape.end(),\n-                                       1LL, std::multiplies<int64>());\n-  std::vector<int64> flatten_shape = {host_group_size,\n-                                      num_elements / host_group_size};\n+  int64_t num_elements = std::accumulate(input_shape.begin(), input_shape.end(),\n+                                         1LL, std::multiplies<int64_t>());\n+  std::vector<int64_t> flatten_shape = {host_group_size,\n+                                        num_elements / host_group_size};\n   auto flatten_reshape_op = mlir::TF::ReshapeOp::create(\n       builder, loc, input, ops_util::GetR1Const(flatten_shape, builder, loc));\n   mlir::TensorType output_type =\n@@ -452,7 +453,7 @@ mlir::Operation* EmitCollectiveAllToAll(\n         builder, loc, prev_op,\n         ops_util::GetR1Const(transposed_shape, builder, loc));\n     // Undo earlier transpose which moved split or concat dim to rank 0.\n-    std::vector<int64> perm_for_transpose;\n+    std::vector<int64_t> perm_for_transpose;\n     perm_for_transpose.reserve(input_shape.size());\n     for (int i = move_dims + 1; i < input_shape.size(); ++i) {\n       perm_for_transpose.push_back(i);\n@@ -475,7 +476,7 @@ mlir::Operation* EmitCollectiveAllToAll(\n   }\n \n   // Reshape\n-  std::vector<int64> output_shape(input_shape.begin(), input_shape.end());\n+  std::vector<int64_t> output_shape(input_shape.begin(), input_shape.end());\n   output_shape[concat_dimension] *= host_group_size;\n   output_shape[split_dimension] /= host_group_size;\n   auto post_reshape_op = mlir::TF::ReshapeOp::create(\n@@ -486,17 +487,17 @@ mlir::Operation* EmitCollectiveAllToAll(\n \n mlir::Operation* EmitCollectiveGather(\n     mlir::OpBuilder& builder, const mlir::Location& loc, mlir::Value input,\n-    const mlir::DenseIntElementsAttr& group_assignment, int32 key_base,\n-    mlir::Value device_id, int32 host_group_size,\n+    const mlir::DenseIntElementsAttr& group_assignment, int32_t key_base,\n+    mlir::Value device_id, int32_t host_group_size,\n     const mlir::StringRef device_type) {\n   DCHECK_EQ(group_assignment.getType().getRank(), 2);\n   auto shape = group_assignment.getType().getShape();\n-  const int32 group_size = shape[1];\n+  const int32_t group_size = shape[1];\n   const mlir::TensorType input_type =\n       mlir::dyn_cast<mlir::TensorType>(input.getType());\n   auto input_shape = input_type.getShape();\n   auto dim_0_shape = input_shape[0];\n-  std::vector<int64> output_shape = {input_shape.begin(), input_shape.end()};\n+  std::vector<int64_t> output_shape = {input_shape.begin(), input_shape.end()};\n   output_shape[0] = dim_0_shape * group_size;\n   auto output_type =\n       mlir::RankedTensorType::get(output_shape, input_type.getElementType());\n@@ -536,7 +537,7 @@ mlir::LogicalResult LowerAllReduceOpImpl(\n     return mlir::emitError(loc, \"group_assigment must be a constant.\");\n   if (group_assignment_attr.getType().getRank() != 2)\n     return mlir::emitError(loc, \"group_assignment should have two dimensions.\");\n-  int32 group_size = group_assignment_attr.getType().getShape()[1];\n+  int32_t group_size = group_assignment_attr.getType().getShape()[1];\n \n   Mesh mesh = output_layout->mesh();\n   // This will become more general when Topology is properly defined.\n@@ -606,7 +607,7 @@ mlir::LogicalResult LowerReduceScatterOp(\n     return reduce_scatter.emitOpError(\n         \"Scatter dimension not constant integer array.\");\n   }\n-  int32 scatter_dim = (*scatter_attr.begin()).getSExtValue();\n+  int32_t scatter_dim = (*scatter_attr.begin()).getSExtValue();\n \n   mlir::OpBuilder builder(reduce_scatter);\n   if (reduce_scatter.getDeviceType().ends_with(\"TPU\")) {\n@@ -623,7 +624,7 @@ mlir::LogicalResult LowerReduceScatterOp(\n     mlir::Value relative_device_id =\n         GetRelativeDeviceId(reduce_scatter, *output_layout, builder, loc);\n \n-    int32 group_size = group_assignment_attr.getType().getShape()[1];\n+    int32_t group_size = group_assignment_attr.getType().getShape()[1];\n     const int32_t key_base =\n         GetCollectiveKeyBase((*output_layout).mesh(), group_assignment_attr);\n \n@@ -645,7 +646,7 @@ mlir::LogicalResult LowerReduceScatterOp(\n     if (!input_layout.ok()) {\n       // If input layout is not defined, modify the output_layout based on the\n       // scattered dimension.\n-      std::vector<string> input_sharding_spec =\n+      std::vector<std::string> input_sharding_spec =\n           output_layout->sharding_spec_strs();\n       input_sharding_spec[scatter_dim] = Layout::kUnshardedDim;\n       input_layout =\n@@ -689,8 +690,8 @@ mlir::Value CreateZeroScalar(mlir::OpBuilder& builder, mlir::Location loc,\n // id).\n mlir::Value SelectElementsBasedOnId(\n     mlir::OpBuilder& builder, mlir::Location loc, mlir::Value device_id,\n-    const llvm::SmallVectorImpl<int64>& candidates_flat, int64 num_devices,\n-    int64 output_shape_size) {\n+    const llvm::SmallVectorImpl<int64_t>& candidates_flat, int64_t num_devices,\n+    int64_t output_shape_size) {\n   // Reshape the flat list to a matrix of shape num_devices * output_shape_size.\n   const mlir::Value candidates_flat_const =\n       ops_util::GetR1Const(candidates_flat, builder, loc);\n@@ -726,12 +727,12 @@ mlir::Value SelectElementsBasedOnId(\n StatusOr<const mlir::DenseIntElementsAttr> GetGroupAssignment(\n     mlir::OpBuilder builder, const Layout src_layout,\n     absl::flat_hash_set<std::string> reduced_dims) {\n-  std::vector<int32> partitions_flat;\n+  std::vector<int32_t> partitions_flat;\n   TF_ASSIGN_OR_RETURN(\n       auto all_partitions,\n       GetAllReducePartitionsFromReducedDims(src_layout, reduced_dims));\n \n-  const int32 num_partitions = all_partitions.size();\n+  const int32_t num_partitions = all_partitions.size();\n   for (auto& p : all_partitions) {\n     if (p.second.size() != all_partitions.begin()->second.size()) {\n       return errors::InvalidArgument(\n@@ -742,7 +743,7 @@ StatusOr<const mlir::DenseIntElementsAttr> GetGroupAssignment(\n                            p.second.end());\n   }\n \n-  const int32 partition_size = all_partitions.begin()->second.size();\n+  const int32_t partition_size = all_partitions.begin()->second.size();\n \n   const mlir::RankedTensorType shaped_type = mlir::RankedTensorType::get(\n       {num_partitions, partition_size},\n@@ -791,12 +792,12 @@ mlir::LogicalResult LowerAllGatherOpToCollective(\n \n   absl::flat_hash_set<std::string> dims_to_gather;\n \n-  std::vector<int32> num_shards_per_dim;\n-  absl::flat_hash_map<int32, int32> previous_sharded_dim;\n-  int32 last_sharded_dim = 0;\n+  std::vector<int32_t> num_shards_per_dim;\n+  absl::flat_hash_map<int32_t, int32_t> previous_sharded_dim;\n+  int32_t last_sharded_dim = 0;\n   std::vector<int64_t> input_shape_after_tr;\n \n-  std::vector<int64> perm_for_transpose;\n+  std::vector<int64_t> perm_for_transpose;\n   perm_for_transpose.reserve(src_layout.rank());\n   for (int i = 0; i < src_layout.rank(); i++) {\n     perm_for_transpose.push_back(i);\n@@ -808,7 +809,7 @@ mlir::LogicalResult LowerAllGatherOpToCollective(\n       continue;\n     }\n \n-    int64 temp = perm_for_transpose[0];\n+    int64_t temp = perm_for_transpose[0];\n     perm_for_transpose[0] = perm_for_transpose[i];\n     perm_for_transpose[i] = temp;\n \n@@ -829,16 +830,16 @@ mlir::LogicalResult LowerAllGatherOpToCollective(\n     return all_gather.emitOpError() << group_assignment_or.status().message();\n   }\n   auto group_assignment = group_assignment_or.value();\n-  int32 group_size = group_assignment.getType().getShape()[1];\n-  int32 key_base = GetCollectiveKeyBase(tgt_layout.mesh(), group_assignment);\n+  int32_t group_size = group_assignment.getType().getShape()[1];\n+  int32_t key_base = GetCollectiveKeyBase(tgt_layout.mesh(), group_assignment);\n   auto collective_op =\n       EmitCollectiveGather(builder, loc, prev_op_result, group_assignment,\n                            key_base, relative_device_id,\n                            /*host_group_size=*/group_size, device_type);\n \n   prev_op_result = collective_op->getResult(0);\n   if (num_shards_per_dim.size() > 1) {\n-    std::vector<int64> new_shape;\n+    std::vector<int64_t> new_shape;\n     new_shape.reserve(input_shape.size() + num_shards_per_dim.size());\n     for (int j = 0; j < num_shards_per_dim.size(); j++) {\n       new_shape.push_back(num_shards_per_dim[j]);\n@@ -862,7 +863,7 @@ mlir::LogicalResult LowerAllGatherOpToCollective(\n \n       // Transpose based on sharding. Sharded dims are updated in the front\n       // before calling collective.\n-      std::vector<int64> perm_arr = {};\n+      std::vector<int64_t> perm_arr = {};\n       // for (int j = 0; j <= src_layout.rank(); j++) {\n       perm_arr.reserve(new_shape.size());\n       for (int j = 0; j < new_shape.size(); j++) {\n@@ -897,8 +898,8 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n   const Layout src_layout = all_gather.getInputLayout();\n   const Layout tgt_layout = all_gather.getOutputLayout();\n \n-  llvm::SmallVector<int64, 4> concat_dims;\n-  for (int64 i = 0; i < src_layout.rank(); ++i)\n+  llvm::SmallVector<int64_t, 4> concat_dims;\n+  for (int64_t i = 0; i < src_layout.rank(); ++i)\n     if (src_layout.num_shards_for_dim(i) > 1 &&\n         Layout::IsUnshardedDimension(tgt_layout.sharding_spec(i)))\n       concat_dims.push_back(i);\n@@ -955,13 +956,13 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n   // ranges---num_devices * output_shape_size * (begin, end, stride)---as three\n   // flat lists.\n   // Consider making this a generalized N-dimensional helper on Layout.\n-  const int64 num_devices = src_layout.num_devices();\n-  const int64 output_shape_size = output_shape.size();\n-  llvm::SmallVector<int64, 4> device_id_to_begin_flat;\n-  llvm::SmallVector<int64, 4> device_id_to_end_flat;\n-  llvm::SmallVector<int64, 4> device_id_to_strides_flat;\n-  for (int64 device_id = 0; device_id < num_devices; ++device_id) {\n-    for (int64 i = 0; i < output_shape_size; ++i) {\n+  const int64_t num_devices = src_layout.num_devices();\n+  const int64_t output_shape_size = output_shape.size();\n+  llvm::SmallVector<int64_t, 4> device_id_to_begin_flat;\n+  llvm::SmallVector<int64_t, 4> device_id_to_end_flat;\n+  llvm::SmallVector<int64_t, 4> device_id_to_strides_flat;\n+  for (int64_t device_id = 0; device_id < num_devices; ++device_id) {\n+    for (int64_t i = 0; i < output_shape_size; ++i) {\n       if (llvm::find(concat_dims, i) == std::end(concat_dims)) {\n         // For unsharded dimensions, the slice range is [0, dim_size).\n         device_id_to_begin_flat.push_back(0);\n@@ -975,10 +976,10 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n           return all_gather.emitOpError()\n                  << device_loc_or_status.status().message();\n         const DeviceLocation device_loc = device_loc_or_status.value();\n-        const int32 mesh_idx =\n+        const int32_t mesh_idx =\n             src_layout.mesh().idx_for_dim(src_layout.sharding_spec(i)).value();\n-        const int64 device_offset = device_loc[mesh_idx];\n-        const int64 step = output_shape[i] / src_layout.num_shards()[i];\n+        const int64_t device_offset = device_loc[mesh_idx];\n+        const int64_t step = output_shape[i] / src_layout.num_shards()[i];\n         device_id_to_begin_flat.push_back(step * device_offset);\n         device_id_to_end_flat.push_back(step * device_offset + step);\n       }\n@@ -1027,7 +1028,7 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n   if (!partitions_or_status.ok())\n     return all_gather.emitOpError() << partitions_or_status.status().message();\n   auto partitions = partitions_or_status.value();\n-  const int32 num_partitions = partitions.size();\n+  const int32_t num_partitions = partitions.size();\n   assert(num_partitions <= num_devices);\n   if (num_partitions == num_devices) {\n     // TODO(unknown): Is this check needed? Since we check that num_shards for\n@@ -1041,15 +1042,15 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n     return mlir::success();\n   }\n \n-  std::vector<int32> partitions_flat;\n+  std::vector<int32_t> partitions_flat;\n   for (auto& p : partitions) {\n     if (p.second.size() != partitions.begin()->second.size())\n       return all_gather.emitOpError() << \"partitions had different sizes -- \"\n                                          \"this is not supported in MLIR.\";\n     partitions_flat.insert(partitions_flat.end(), p.second.begin(),\n                            p.second.end());\n   }\n-  const int32 partition_size = partitions.begin()->second.size();\n+  const int32_t partition_size = partitions.begin()->second.size();\n   const mlir::RankedTensorType shaped_type = mlir::RankedTensorType::get(\n       {num_partitions, partition_size},\n       mlir::IntegerType::get(builder.getContext(), 32));\n@@ -1117,8 +1118,8 @@ mlir::LogicalResult LowerAllScatterOp(\n   // [original_layout.mesh().rank(), original_layout.rank()]\n   // so the 2D index [i, j] corresponds to the 1D index of\n   // [i * original_layout.rank() + j].\n-  std::vector<int32> matrix(original_layout.mesh().rank() *\n-                            original_layout.rank());\n+  std::vector<int32_t> matrix(original_layout.mesh().rank() *\n+                              original_layout.rank());\n   for (int i = 0; i < original_layout.rank(); ++i) {\n     if (original_layout.sharding_spec(i) != desired_layout.sharding_spec(i)) {\n       if (mlir::ShapedType::isDynamic(output_shape[i])) {\n@@ -1210,7 +1211,7 @@ mlir::LogicalResult LowerAllToAllOp(mlir::TF::DTensorAllToAllOp all_to_all) {\n     return all_to_all.emitOpError() << group_assignment_or.status().message();\n   }\n   auto group_assignment = group_assignment_or.value();\n-  int32 group_size = group_assignment.getType().getShape()[1];\n+  int32_t group_size = group_assignment.getType().getShape()[1];\n \n   StatusOr<std::string> device_type_or_status =\n       DeviceTypeFromMesh(src_layout.mesh());\n@@ -1219,8 +1220,8 @@ mlir::LogicalResult LowerAllToAllOp(mlir::TF::DTensorAllToAllOp all_to_all) {\n   const std::string device_type = device_type_or_status.value();\n \n   // Find concat and split dimensions\n-  int32 split_dimension = -1;\n-  int32 concat_dimension = -1;\n+  int32_t split_dimension = -1;\n+  int32_t concat_dimension = -1;\n   for (int i = 0; i < src_layout.rank(); ++i) {\n     if (src_layout.sharding_spec(i) != tgt_layout.sharding_spec(i)) {\n       if (Layout::IsUnshardedDimension(src_layout.sharding_spec(i)) &&\n@@ -1248,7 +1249,8 @@ mlir::LogicalResult LowerAllToAllOp(mlir::TF::DTensorAllToAllOp all_to_all) {\n     // Use CollectiveAllToAllV2\n     mlir::Value relative_device_id =\n         GetRelativeDeviceId(all_to_all, tgt_layout, builder, loc);\n-    int32 key_base = GetCollectiveKeyBase(tgt_layout.mesh(), group_assignment);\n+    int32_t key_base =\n+        GetCollectiveKeyBase(tgt_layout.mesh(), group_assignment);\n \n     mlir::Operation* collective_op = EmitCollectiveAllToAll(\n         builder, loc, all_to_all.getInput(), group_assignment, concat_dimension,"
        }
    ],
    "stats": {
        "total": 156,
        "additions": 79,
        "deletions": 77
    }
}