{
    "author": "basioli-k",
    "message": "[XLA][codegen] Move memref layout attribute implementation from triton_xla to xtile\n\nAllows us to remove the triton_xla dependency from the sharede fusion emitter.\n\nPiperOrigin-RevId: 839231084",
    "sha": "502b997973f9361bdddf2aa97e742b0df162ad84",
    "files": [
        {
            "sha": "9144a99d88cb7d42f5dc4e4f04931872b1eec071",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -70,7 +70,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/collective_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n-#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/emitters/elemental_hlo_to_mlir.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n@@ -79,6 +78,7 @@ limitations under the License.\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/codegen/tiling/tiled_hlo_schedule.h\"\n #include \"xla/codegen/tiling/tiling_specification.h\"\n+#include \"xla/codegen/xtile/ir/xtile_attrs.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n@@ -114,7 +114,6 @@ namespace xla {\n namespace gpu {\n \n namespace arith = ::mlir::arith;\n-namespace mtx = ::mlir::triton::xla;\n namespace stablehlo = ::mlir::stablehlo;\n \n using ::llvm::SmallVector;\n@@ -1354,7 +1353,7 @@ absl::Status EmitGeneric(\n     const BlockLevelParameters& block_level_parameters,\n     MLIRContext* mlir_context) {\n   if (VLOG_IS_ON(6)) {\n-    VLOG(6) << \"Emitting Triton IR for fusion\\n\"\n+    VLOG(6) << \"Emitting XTile IR for fusion\\n\"\n             << ExtractInstructionIntoNewModule(*fusion)->ToString();\n   }\n   const HloComputation* computation = fusion->fused_instructions_computation();\n@@ -1480,7 +1479,7 @@ mlir::MemRefType GetMemRefType(const Shape& shape, mlir::Type element_type) {\n \n   auto minor_to_major_attr =\n       mlir::DenseI64ArrayAttr::get(context, shape.layout().minor_to_major());\n-  auto layout = mtx::LayoutAttr::get(context, minor_to_major_attr);\n+  auto layout = xtile::LayoutAttr::get(context, minor_to_major_attr);\n \n   return mlir::MemRefType::get(shape.dimensions(), storage_type, layout);\n }"
        },
        {
            "sha": "b593ed75df3cff4ee932c527a03faeee7e17d191",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -2480,7 +2480,7 @@ ENTRY entry_computation {\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n CHECK:      xtile.entry_func @xtile_dialect_fn(\n-CHECK-SAME: memref<48x16xi32, #triton_xla.layout<[0, 1]>>\n+CHECK-SAME: memref<48x16xi32, #xtile.layout<[0, 1]>>\n CHECK-SAME: memref<16x16x3xi32>,\n CHECK:      xtile.extract\n CHECK:      stablehlo.transpose"
        },
        {
            "sha": "cbff5db8d65846b5b793cc107b66b525985c9273",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2FBUILD?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -95,6 +95,7 @@ cc_library(\n         \":triton_xla_attrs_inc_gen\",\n         \":triton_xla_dialect_inc_gen\",\n         \":triton_xla_ops_inc_gen\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:DialectUtils\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "5f8bda65a87aa911351e98b0f133cfbc573dadfc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/tests/canonicalize.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fcanonicalize.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fcanonicalize.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fcanonicalize.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -7,13 +7,13 @@ tt.func @xla_triton_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: index) {\n   // CHECK-SAME:    [%arg1, 0] [16, 64] [128, 1]\n   // CHECK-SAME:    {noinline = false}\n   %tile = triton_xla.extract from %arg0\n-      as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<512x128xbf16, #xtile.layout<[1, 0]>>\n       [%arg1, %c0] [16, 64] [128, 1] {noinline = false} : tensor<16x64xbf16>\n   // CHECK:       triton_xla.insert\n   // CHECK-SAME:    [0, %arg1] [16, 64] [1, 1]\n   // CHECK-SAME:    {noinline = false}\n   triton_xla.insert %tile into %arg0\n-      as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<512x128xbf16, #xtile.layout<[1, 0]>>\n       [%c0, %arg1][16, 64][1, 1] {noinline = false} : tensor<16x64xbf16>\n   tt.return\n }"
        },
        {
            "sha": "f286deb2d042b794f4ce57d5028af65c8f0e236b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/tests/invalid.mlir",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Finvalid.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -2,62 +2,62 @@\n \n func.func @extract_0d(%arg0: !tt.ptr<bf16>) {\n   // expected-error @+1 {{unsupported 0-d tensor}}\n-  %0 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [][][] : tensor<bf16>\n+  %0 = triton_xla.extract from %arg0 as memref<bf16, #xtile.layout<[]>> [][][] : tensor<bf16>\n   return\n }\n \n // -----\n \n func.func @insert_0d(%arg0: tensor<bf16>, %arg1: !tt.ptr<bf16>) {\n   // expected-error @+1 {{unsupported 0-d tensor}}\n-  triton_xla.insert %arg0 into %arg1 as memref<bf16, #triton_xla.layout<[]>> [][][] : tensor<bf16>\n+  triton_xla.insert %arg0 into %arg1 as memref<bf16, #xtile.layout<[]>> [][][] : tensor<bf16>\n   return\n }\n \n // -----\n \n func.func @extract_wrong_layout(%arg0: !tt.ptr<bf16>) {\n   // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n-  %0 = triton_xla.extract from %arg0 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+  %0 = triton_xla.extract from %arg0 as memref<8xbf16, #xtile.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n func.func @insert_wrong_layout(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n   // expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n-  triton_xla.insert %arg0 into %arg1 as memref<8xbf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+  triton_xla.insert %arg0 into %arg1 as memref<8xbf16, #xtile.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n func.func @extract_wrong_rank(%arg0: !tt.ptr<bf16>) {\n   // expected-error @+1 {{expected 0 offset values, got 1}}\n-  %0 = triton_xla.extract from %arg0 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+  %0 = triton_xla.extract from %arg0 as memref<bf16, #xtile.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n func.func @insert_wrong_rank(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n   // expected-error @+1 {{expected 0 offset values, got 1}}\n-  triton_xla.insert %arg0 into %arg1 as memref<bf16, #triton_xla.layout<[]>> [0][8][1] : tensor<8xbf16>\n+  triton_xla.insert %arg0 into %arg1 as memref<bf16, #xtile.layout<[]>> [0][8][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n func.func @extract_wrong_shape(%arg0: !tt.ptr<bf16>) {\n   // expected-error @+1 {{expected type to be 'tensor<16xbf16>'}}\n-  %0 = triton_xla.extract from %arg0 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n+  %0 = triton_xla.extract from %arg0 as memref<16xbf16, #xtile.layout<[0]>> [0][16][1] : tensor<8xbf16>\n   return\n }\n \n // -----\n \n func.func @insert_wrong_shape(%arg0: tensor<8xbf16>, %arg1: !tt.ptr<bf16>) {\n   // expected-error @+1 {{expected type to be 'tensor<16xbf16>'}}\n-  triton_xla.insert %arg0 into %arg1 as memref<16xbf16, #triton_xla.layout<[0]>> [0][16][1] : tensor<8xbf16>\n+  triton_xla.insert %arg0 into %arg1 as memref<16xbf16, #xtile.layout<[0]>> [0][16][1] : tensor<8xbf16>\n   return\n }"
        },
        {
            "sha": "a8a8a05f22b31b2240ba67821b22acd11ccc120a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/tests/ops.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftests%2Fops.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -10,7 +10,7 @@\n tt.func @xla_triton_extract(%src: !tt.ptr<bf16>, %i : index) -> tensor<16x64xbf16> {\n   // CHECK: triton_xla.extract\n   %extracted_tensor = triton_xla.extract from %src\n-    as memref<512x1x128xbf16, #triton_xla.layout<[2, 1, 0]>>\n+    as memref<512x1x128xbf16, #xtile.layout<[2, 1, 0]>>\n     [0, 0, %i] [16, 1, 64] [128, 1, 1] : tensor<16x64xbf16>\n   tt.return %extracted_tensor : tensor<16x64xbf16>\n }\n@@ -19,7 +19,7 @@ tt.func @xla_triton_extract(%src: !tt.ptr<bf16>, %i : index) -> tensor<16x64xbf1\n tt.func @xla_triton_insert(%src: tensor<16x64xbf16>, %dst: !tt.ptr<bf16>, %j: index) {\n   // CHECK: triton_xla.insert\n   triton_xla.insert %src into %dst\n-    as memref<512x128xbf16, #triton_xla.layout<[0, 1]>>\n+    as memref<512x128xbf16, #xtile.layout<[0, 1]>>\n     [%j, 0][16, 64][1, 1] : tensor<16x64xbf16>\n   tt.return\n }"
        },
        {
            "sha": "b64d94cd87becbbcd02e114368564585871025f7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_attrs.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.cc?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -100,36 +100,4 @@ void TmaDescriptorAttr::print(mlir::AsmPrinter& printer) const {\n   printer << \">\";\n }\n \n-AffineMap LayoutAttr::getAffineMap() const {\n-  return AffineMap::getPermutationMap(getMinorToMajor(), getContext());\n-}\n-\n-LogicalResult LayoutAttr::verifyLayout(\n-    ArrayRef<int64_t> shape,\n-    function_ref<InFlightDiagnostic()> emit_error) const {\n-  if (getMinorToMajor().size() != shape.size()) {\n-    emit_error() << \"layout has \" << getMinorToMajor().size()\n-                 << \" dimensions, but shape has \" << shape.size();\n-    return failure();\n-  }\n-  if (!isPermutationVector(getMinorToMajor().asArrayRef())) {\n-    emit_error() << \"layout is not a permutation\";\n-    return failure();\n-  }\n-  return success();\n-}\n-\n-LogicalResult LayoutAttr::getStridesAndOffset(ArrayRef<int64_t> shape,\n-                                              SmallVectorImpl<int64_t>& strides,\n-                                              int64_t& offset) const {\n-  strides.resize(shape.size());\n-  int64_t size_product = 1;\n-  for (auto dim : getMinorToMajor().asArrayRef()) {\n-    strides[dim] = size_product;\n-    size_product *= shape[dim];\n-  }\n-  offset = 0;\n-  return success();\n-}\n-\n }  // namespace mlir::triton::xla"
        },
        {
            "sha": "2753a9c8b21c9210730afa5093ec8372378cd7f2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_attrs.td",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.td?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -70,19 +70,6 @@ def XLA_TmaDescriptorAttr : TTXLA_Attr<\"TmaDescriptor\"> {\n   ];\n }\n \n-def TTXLA_LayoutAttr : TTXLA_Attr<\"Layout\", [\n-    DeclareAttrInterfaceMethods<MemRefLayoutAttrInterface>]> {\n-  let mnemonic = \"layout\";\n-  let parameters = (ins \"::mlir::DenseI64ArrayAttr\":$minor_to_major);\n-  let assemblyFormat = \"`<` $minor_to_major `>`\";\n-  let extraClassDeclaration = [{\n-    LogicalResult verifyLayout(\n-        ArrayRef<int64_t> shape, function_ref<InFlightDiagnostic()> emit_error) const;\n-    LogicalResult getStridesAndOffset(\n-        ArrayRef<int64_t> shape, SmallVectorImpl<int64_t>& strides, int64_t& offset) const;\n-  }];\n-}\n-\n def TTXLA_ComparatorEnum : I32Enum<\"Comparator\",\n     \"A comparison operator for instructions.\",\n     ["
        },
        {
            "sha": "e978703b7ccdf5e8c6b9f56c0706a3819a6f680e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_dialect.td",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_dialect.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_dialect.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_dialect.td?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -30,7 +30,8 @@ def XlaTritonDialect : Dialect {\n   let useDefaultAttributePrinterParser = 1;\n   let dependentDialects = [\n     \"mlir::triton::TritonDialect\",\n-    \"mlir::triton::gpu::TritonGPUDialect\"\n+    \"mlir::triton::gpu::TritonGPUDialect\",\n+    \"::xla::xtile::XTileDialect\",\n   ];\n }\n "
        },
        {
            "sha": "4e0cd37bd520d8f0d3db4e55608eb756f4b77bca",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.cc?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -35,12 +35,15 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_dialect.cc.inc\"\n+#include \"xla/codegen/xtile/ir/xtile_attrs.h\"  // IWYU pragma: keep\n \n using mlir::LogicalResult;\n using mlir::Type;\n \n namespace mlir::triton::xla {\n \n+using ::xla::xtile::LayoutAttr;\n+\n // Parser hook for triton_xla.extract/insert ops assembly format.\n ParseResult parseAsMemRefType(OpAsmParser& parser, Type& type,\n                               DenseI64ArrayAttr& shape,\n@@ -175,7 +178,7 @@ class ExtractOpOffsetsSizesStridesFolder final\n   using OpRewritePattern<ExtractOp>::OpRewritePattern;\n \n   LogicalResult matchAndRewrite(ExtractOp op,\n-                                PatternRewriter &rewriter) const override {\n+                                PatternRewriter& rewriter) const override {\n     SmallVector<OpFoldResult> mixed_offsets(op.getMixedOffsets());\n     if (failed(foldDynamicIndexList(mixed_offsets, /*onlyNonNegative=*/true))) {\n       // No constant operands were folded, just return;\n@@ -191,8 +194,8 @@ class ExtractOpOffsetsSizesStridesFolder final\n   }\n };\n \n-void ExtractOp::getCanonicalizationPatterns(RewritePatternSet &results,\n-                                            MLIRContext *context) {\n+void ExtractOp::getCanonicalizationPatterns(RewritePatternSet& results,\n+                                            MLIRContext* context) {\n   results.add<ExtractOpOffsetsSizesStridesFolder>(context);\n }\n \n@@ -233,7 +236,7 @@ class InsertOpOffsetsSizesStridesFolder final\n   using OpRewritePattern<InsertOp>::OpRewritePattern;\n \n   LogicalResult matchAndRewrite(InsertOp op,\n-                                PatternRewriter &rewriter) const override {\n+                                PatternRewriter& rewriter) const override {\n     SmallVector<OpFoldResult> mixed_offsets(op.getMixedOffsets());\n     // No constant operands were folded, just return;\n     if (failed(foldDynamicIndexList(mixed_offsets, /*onlyNonNegative=*/true))) {\n@@ -249,8 +252,8 @@ class InsertOpOffsetsSizesStridesFolder final\n   }\n };\n \n-void InsertOp::getCanonicalizationPatterns(RewritePatternSet &results,\n-                                           MLIRContext *context) {\n+void InsertOp::getCanonicalizationPatterns(RewritePatternSet& results,\n+                                           MLIRContext* context) {\n   results.add<InsertOpOffsetsSizesStridesFolder>(context);\n }\n "
        },
        {
            "sha": "17ba8138e4a7f3fe65a6f1ea182fa8bf3a54c73f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.h?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"mlir/Interfaces/SideEffectInterfaces.h\"  // IWYU pragma: keep\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_dialect.h.inc\"  // IWYU pragma: keep\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_enums.h.inc\"\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"  // IWYU pragma: keep\n #include \"triton/Dialect/Triton/IR/Dialect.h\"       // IWYU pragma: keep\n #include \"triton/Dialect/Triton/IR/OpInterfaces.h\"  // IWYU pragma: keep\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"    // IWYU pragma: keep"
        },
        {
            "sha": "ad4f95d3fc5808ab4545437ee3b6aed48aab0153",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.td",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -87,7 +87,7 @@ def TTXLA_ExtractOp : TTXLA_OpWithOffsetSizesAndStrides<\"extract\", [\n     Example:\n       ```\n       %extracted_tensor = triton_xla.extract from %src\n-        as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+        as memref<512x128xbf16, #xtile.layout<[1, 0]>>\n         [0, 0] [16, 64] [128, 1] : tensor<16x64xbf16>\n       ```\n   }];\n@@ -148,7 +148,7 @@ def TTXLA_InsertOp : TTXLA_OpWithOffsetSizesAndStrides<\"insert\"> {\n     Example:\n       ```\n       triton_xla.insert %src into %dst\n-        as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+        as memref<512x128xbf16, #xtile.layout<[1, 0]>>\n         [0, 0] [8, 8] [1, 1] : tensor<8x8xbf16>\n       ```\n   }];"
        },
        {
            "sha": "d732d6b646ce779c9435e6109670c6190d5bc82d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/int4_packed_dim.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fint4_packed_dim.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -3,11 +3,11 @@\n // CHECK-LABEL: @triton_xla_extract_2d\n func.func @triton_xla_extract_2d(%arg0: !tt.ptr<i4>) -> (tensor<16x16xi8>) {\n   // CHECK: %[[EXTRACT:.*]] = triton_xla.extract from %arg0\n-  // CHECK-SAME: as memref<128x8x64xi8, #triton_xla.layout<[2, 1, 0]>>\n+  // CHECK-SAME: as memref<128x8x64xi8, #xtile.layout<[2, 1, 0]>>\n   // CHECK-SAME: [0, 0, 0] [16, 1, 8] [1, 1, 1] : tensor<16x8xi8>\n   %c0 = arith.constant 0 : index\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<128x8x128xi4, #triton_xla.layout<[2, 1, 0]>>\n+      as memref<128x8x128xi4, #xtile.layout<[2, 1, 0]>>\n       [0, 0, %c0] [16, 1, 16] [1, 1, 1] : tensor<16x16xi4>\n   %ext = arith.extsi %extracted_tensor : tensor<16x16xi4> to tensor<16x16xi8>\n   // CHECK: %[[SHLI:.*]] = arith.shli %[[EXTRACT]]"
        },
        {
            "sha": "787c8b61ee64aaad63e290e7d4960f6083b09ffe",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_convert_unsupported_types.mlir",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_convert_unsupported_types.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -2,27 +2,27 @@\n \n module {\n   // CHECK:   xtile.entry_func @triton_fn(\n-  // CHECK-SAME:   %arg0: memref<64x512xf8E4M3FN, #triton_xla.layout<[1, 0]>>,\n-  // CHECK-SAME:   %arg1: memref<64x16xi8, #triton_xla.layout<[1, 0]>>,\n-  // CHECK-SAME:   %arg2: memref<512x64xf8E4M3FN, #triton_xla.layout<[1, 0]>>,\n-  // CHECK-SAME:   %arg3: memref<16x64xi8, #triton_xla.layout<[1, 0]>>,\n+  // CHECK-SAME:   %arg0: memref<64x512xf8E4M3FN, #xtile.layout<[1, 0]>>,\n+  // CHECK-SAME:   %arg1: memref<64x16xi8, #xtile.layout<[1, 0]>>,\n+  // CHECK-SAME:   %arg2: memref<512x64xf8E4M3FN, #xtile.layout<[1, 0]>>,\n+  // CHECK-SAME:   %arg3: memref<16x64xi8, #xtile.layout<[1, 0]>>,\n   xtile.entry_func @triton_fn(\n-      %arg0: memref<64x512xf8E4M3FN, #triton_xla.layout<[1, 0]>>,\n-      %arg1: memref<64x16xf8E8M0FNU, #triton_xla.layout<[1, 0]>>,\n-      %arg2: memref<512x64xf8E4M3FN, #triton_xla.layout<[1, 0]>>,\n-      %arg3: memref<16x64xf8E8M0FNU, #triton_xla.layout<[1, 0]>>,\n+      %arg0: memref<64x512xf8E4M3FN, #xtile.layout<[1, 0]>>,\n+      %arg1: memref<64x16xf8E8M0FNU, #xtile.layout<[1, 0]>>,\n+      %arg2: memref<512x64xf8E4M3FN, #xtile.layout<[1, 0]>>,\n+      %arg3: memref<16x64xf8E8M0FNU, #xtile.layout<[1, 0]>>,\n       %tile_id: index) {\n     // CHECK-DAG: %[[C_0:.*]] = arith.constant 0 : index\n     %c_0 = arith.constant 0 : index\n     %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32>\n-    %extracted_tile = xtile.extract %arg0[%c_0, %c_0] [16, 32] [1, 1] : memref<64x512xf8E4M3FN, #triton_xla.layout<[1, 0]>> -> tensor<16x32xf8E4M3FN>\n-    // CHECK: %[[arg_0:.*]] = xtile.extract %arg0[%[[C_0]], %[[C_0]]] [16, 32] [1, 1] : memref<64x512xf8E4M3FN, #triton_xla.layout<[1, 0]>> -> tensor<16x32xf8E4M3FN>\n-    %extracted_tile_0 = xtile.extract %arg1[%c_0, %c_0] [16, 1] [1, 1] : memref<64x16xf8E8M0FNU, #triton_xla.layout<[1, 0]>> -> tensor<16x1xf8E8M0FNU>\n-    // CHECK: %[[arg_1:.*]] = xtile.extract %arg1[%[[C_0]], %[[C_0]]] [16, 1] [1, 1] : memref<64x16xi8, #triton_xla.layout<[1, 0]>> -> tensor<16x1xi8>\n-    %extracted_tile_1 = xtile.extract %arg2[%c_0, %c_0] [32, 16] [1, 1] : memref<512x64xf8E4M3FN, #triton_xla.layout<[1, 0]>> -> tensor<32x16xf8E4M3FN>\n-    // CHECK: %[[arg_2:.*]] = xtile.extract %arg2[%[[C_0]], %[[C_0]]] [32, 16] [1, 1] : memref<512x64xf8E4M3FN, #triton_xla.layout<[1, 0]>> -> tensor<32x16xf8E4M3FN>\n-    %extracted_tile_2 = xtile.extract %arg3[%c_0, %c_0] [1, 16] [1, 1] : memref<16x64xf8E8M0FNU, #triton_xla.layout<[1, 0]>> -> tensor<1x16xf8E8M0FNU>\n-    // CHECK: %[[arg_3:.*]] = xtile.extract %arg3[%[[C_0]], %[[C_0]]] [1, 16] [1, 1] : memref<16x64xi8, #triton_xla.layout<[1, 0]>> -> tensor<1x16xi8>\n+    %extracted_tile = xtile.extract %arg0[%c_0, %c_0] [16, 32] [1, 1] : memref<64x512xf8E4M3FN, #xtile.layout<[1, 0]>> -> tensor<16x32xf8E4M3FN>\n+    // CHECK: %[[arg_0:.*]] = xtile.extract %arg0[%[[C_0]], %[[C_0]]] [16, 32] [1, 1] : memref<64x512xf8E4M3FN, #xtile.layout<[1, 0]>> -> tensor<16x32xf8E4M3FN>\n+    %extracted_tile_0 = xtile.extract %arg1[%c_0, %c_0] [16, 1] [1, 1] : memref<64x16xf8E8M0FNU, #xtile.layout<[1, 0]>> -> tensor<16x1xf8E8M0FNU>\n+    // CHECK: %[[arg_1:.*]] = xtile.extract %arg1[%[[C_0]], %[[C_0]]] [16, 1] [1, 1] : memref<64x16xi8, #xtile.layout<[1, 0]>> -> tensor<16x1xi8>\n+    %extracted_tile_1 = xtile.extract %arg2[%c_0, %c_0] [32, 16] [1, 1] : memref<512x64xf8E4M3FN, #xtile.layout<[1, 0]>> -> tensor<32x16xf8E4M3FN>\n+    // CHECK: %[[arg_2:.*]] = xtile.extract %arg2[%[[C_0]], %[[C_0]]] [32, 16] [1, 1] : memref<512x64xf8E4M3FN, #xtile.layout<[1, 0]>> -> tensor<32x16xf8E4M3FN>\n+    %extracted_tile_2 = xtile.extract %arg3[%c_0, %c_0] [1, 16] [1, 1] : memref<16x64xf8E8M0FNU, #xtile.layout<[1, 0]>> -> tensor<1x16xf8E8M0FNU>\n+    // CHECK: %[[arg_3:.*]] = xtile.extract %arg3[%[[C_0]], %[[C_0]]] [1, 16] [1, 1] : memref<16x64xi8, #xtile.layout<[1, 0]>> -> tensor<1x16xi8>\n     %16 = arith.bitcast %extracted_tile_0 : tensor<16x1xf8E8M0FNU> to tensor<16x1xi8>\n     %17 = arith.bitcast %extracted_tile_2 : tensor<1x16xf8E8M0FNU> to tensor<1x16xi8>\n     %18 = tt.trans %17 {order = array<i32: 1, 0>} : tensor<1x16xi8> -> tensor<16x1xi8>"
        },
        {
            "sha": "87af74b095e54a092dc2c97d3deeeed7c3744b15",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_extract_insert_to_triton.mlir",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -8,10 +8,10 @@\n \n func.func @lower_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<512x8x128xbf16, #triton_xla.layout<[2, 1, 0]>>\n+      as memref<512x8x128xbf16, #xtile.layout<[2, 1, 0]>>\n       [0, 3, 0] [16, 1, 64] [1, 1, 1] : tensor<16x64xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<256x16x256xbf16, #triton_xla.layout<[2, 1, 0]>>\n+      as memref<256x16x256xbf16, #xtile.layout<[2, 1, 0]>>\n       [0, 5, 0] [16, 1, 64] [1, 1, 1] : tensor<16x64xbf16>\n   func.return\n }\n@@ -34,10 +34,10 @@ func.func @lower_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n \n func.func @non_perfect_tile_shape(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-    as memref<300x300xbf16, #triton_xla.layout<[1, 0]>>\n+    as memref<300x300xbf16, #xtile.layout<[1, 0]>>\n     [0, 0] [8, 8] [1, 1] : tensor<8x8xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-    as memref<300x300xbf16, #triton_xla.layout<[1, 0]>>\n+    as memref<300x300xbf16, #xtile.layout<[1, 0]>>\n     [0, 0] [8, 8] [1, 1] : tensor<8x8xbf16>\n   func.return\n }\n@@ -50,10 +50,10 @@ func.func @non_perfect_tile_shape(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n \n func.func @incompatible_tma_global_strides(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<234x234xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<234x234xbf16, #xtile.layout<[1, 0]>>\n       [0, 0] [16, 64] [128, 1] : tensor<16x64xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<123x123xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<123x123xbf16, #xtile.layout<[1, 0]>>\n       [0, 0] [16, 64] [128, 1] : tensor<16x64xbf16>\n   func.return\n }\n@@ -74,13 +74,13 @@ module {\n     %2 = arith.index_cast %1 : i64 to index\n     %3 = xla.apply_indexing #indexing_map(%2)\n     %extracted_tile = triton_xla.extract from %arg0\n-        as memref<64xf32, #triton_xla.layout<[0]>>\n+        as memref<64xf32, #xtile.layout<[0]>>\n         [%3][32][1] : tensor<32xf32>\n     %4 = math.absf %extracted_tile : tensor<32xf32>\n     %5 = arith.subf %cst, %4 : tensor<32xf32>\n-    triton_xla.insert %5 into %arg1 as memref<63xf32, #triton_xla.layout<[0]>>\n+    triton_xla.insert %5 into %arg1 as memref<63xf32, #xtile.layout<[0]>>\n         [%3][32][1] : tensor<32xf32>\n-    triton_xla.insert %4 into %arg2 as memref<63xf32, #triton_xla.layout<[0]>>\n+    triton_xla.insert %4 into %arg2 as memref<63xf32, #xtile.layout<[0]>>\n         [%3][32][1] : tensor<32xf32>\n     func.return\n   }\n@@ -103,13 +103,13 @@ module {\n     %2 = arith.index_cast %1 : i64 to index\n     %3 = xla.apply_indexing #indexing_map(%2)\n     %extracted_tile = triton_xla.extract from %arg0\n-        as memref<64xf32, #triton_xla.layout<[0]>>\n+        as memref<64xf32, #xtile.layout<[0]>>\n         [%3][32][1] : tensor<32xf32>\n     %4 = math.absf %extracted_tile : tensor<32xf32>\n     %5 = arith.subf %cst, %4 : tensor<32xf32>\n-    triton_xla.insert %5 into %arg1 as memref<63xf32, #triton_xla.layout<[0]>>\n+    triton_xla.insert %5 into %arg1 as memref<63xf32, #xtile.layout<[0]>>\n         [%3][32][1] : tensor<32xf32>\n-    triton_xla.insert %4 into %arg2 as memref<64xf32, #triton_xla.layout<[0]>>\n+    triton_xla.insert %4 into %arg2 as memref<64xf32, #xtile.layout<[0]>>\n         [%3][32][1] : tensor<32xf32>\n     func.return\n   }\n@@ -125,10 +125,10 @@ module {\n func.func @extract_with_non_unit_minor_dim_stride(%arg0: !tt.ptr<bf16>,\n                           %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<1024x1024xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<1024x1024xbf16, #xtile.layout<[1, 0]>>\n       [0, 0] [16, 64] [2, 2] : tensor<16x64xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<256x256xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<256x256xbf16, #xtile.layout<[1, 0]>>\n       [0, 0] [16, 64] [1, 1] : tensor<16x64xbf16>\n   func.return\n }\n@@ -141,10 +141,10 @@ func.func @extract_with_non_unit_minor_dim_stride(%arg0: !tt.ptr<bf16>,\n \n func.func @lower_extract_insert_1d(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<128xbf16, #triton_xla.layout<[0]>>\n+      as memref<128xbf16, #xtile.layout<[0]>>\n       [0] [16] [1] : tensor<16xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<256xbf16, #triton_xla.layout<[0]>>\n+      as memref<256xbf16, #xtile.layout<[0]>>\n       [0] [16] [1] : tensor<16xbf16>\n   func.return\n }\n@@ -167,10 +167,10 @@ func.func @lower_extract_insert_1d(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n \n func.func @lower_extract_insert_5d(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<16x16x16x16x16xbf16, #triton_xla.layout<[4, 3, 2, 1, 0]>>\n+      as memref<16x16x16x16x16xbf16, #xtile.layout<[4, 3, 2, 1, 0]>>\n       [0, 0, 0, 0, 0] [8, 8, 8, 8, 8] [1, 1, 1, 1, 1] : tensor<8x8x8x8x8xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<32x32x32x32x32xbf16, #triton_xla.layout<[4, 3, 2, 1, 0]>>\n+      as memref<32x32x32x32x32xbf16, #xtile.layout<[4, 3, 2, 1, 0]>>\n       [0, 0, 0, 0, 0] [8, 8, 8, 8, 8] [1, 1, 1, 1, 1] : tensor<8x8x8x8x8xbf16>\n   func.return\n }\n@@ -193,10 +193,10 @@ func.func @lower_extract_insert_5d(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n \n func.func @extract_insert_with_zero_stride(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<512x128xbf16, #xtile.layout<[1, 0]>>\n       [0, 0] [1, 64] [0, 1] : tensor<1x64xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<256x256xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<256x256xbf16, #xtile.layout<[1, 0]>>\n       [0, 0] [1, 64] [0, 1] : tensor<1x64xbf16>\n   func.return\n }\n@@ -210,10 +210,10 @@ func.func @extract_insert_with_zero_stride(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<\n func.func @incompatible_tma_const_offset_not_divisible_by_16_bytes(\n           %arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {\n   %extracted_tensor = triton_xla.extract from %arg0\n-      as memref<512x128xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<512x128xbf16, #xtile.layout<[1, 0]>>\n       [0, 15] [1, 64] [1, 1] : tensor<1x64xbf16>\n   triton_xla.insert %extracted_tensor into %arg1\n-      as memref<256x256xbf16, #triton_xla.layout<[1, 0]>>\n+      as memref<256x256xbf16, #xtile.layout<[1, 0]>>\n       [0, 0] [1, 64] [0, 1] : tensor<1x64xbf16>\n   func.return\n }\n@@ -235,13 +235,13 @@ module {\n     %2 = arith.index_cast %1 : i64 to index\n     %3 = xla.apply_indexing #indexing_map(%2)\n     %extracted_tile = triton_xla.extract from %arg0\n-        as memref<16x16xbf16, #triton_xla.layout<[1, 0]>>\n+        as memref<16x16xbf16, #xtile.layout<[1, 0]>>\n         [0, %3] [16, 16] [1, 1] : tensor<16x16xbf16>\n     %4 = tt.reshape %extracted_tile : tensor<16x16xbf16> -> tensor<16x1x16xbf16>\n     %5 = xla.apply_indexing #indexing_map1(%2)\n     %6 = xla.apply_indexing #indexing_map2(%2)\n     triton_xla.insert %4 into %arg1\n-        as memref<16x1x16xbf16, #triton_xla.layout<[2, 1, 0]>>\n+        as memref<16x1x16xbf16, #xtile.layout<[2, 1, 0]>>\n         [0, %5, %6] [16, 1, 16] [1, 1, 1] : tensor<16x1x16xbf16>\n     func.return\n   }\n@@ -257,17 +257,17 @@ func.func @parameter_into_broadcast_with_3_or_more_stages_does_not_use_tma(\n           %arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>) {\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32>\n   %extracted_tile = triton_xla.extract from %arg0 as\n-      memref<64xf32, #triton_xla.layout<[0]>> [0] [64] [1] : tensor<64xf32>\n+      memref<64xf32, #xtile.layout<[0]>> [0] [64] [1] : tensor<64xf32>\n   %0 = tt.expand_dims %extracted_tile {axis = 1 : i32}\n       : tensor<64xf32> -> tensor<64x1xf32>\n   %1 = tt.broadcast %0 : tensor<64x1xf32> -> tensor<64x64xf32>\n   %extracted_tile_0 = triton_xla.extract from %arg1 as\n-      memref<64x64xf32, #triton_xla.layout<[1, 0]>> [0, 0] [64, 64] [1, 1]\n+      memref<64x64xf32, #xtile.layout<[1, 0]>> [0, 0] [64, 64] [1, 1]\n       : tensor<64x64xf32>\n   %2 = tt.dot %1, %extracted_tile_0, %cst, inputPrecision = tf32\n       : tensor<64x64xf32> * tensor<64x64xf32> -> tensor<64x64xf32>\n   triton_xla.insert %2 into %arg2 as\n-      memref<64x64xf32, #triton_xla.layout<[1, 0]>> [0, 0] [64, 64] [1, 1]\n+      memref<64x64xf32, #xtile.layout<[1, 0]>> [0, 0] [64, 64] [1, 1]\n       : tensor<64x64xf32>\n   return\n }"
        },
        {
            "sha": "cb3a5bcaa7733dad222e5fc5fd8f24e4673caf89",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_fold_transpose.mlir",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_fold_transpose.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -4,19 +4,19 @@\n // CHECK-SAME: (%[[INPUT:.*]]: memref\n // CHECK-SAME: , %[[OFFSET0:.*]]: index, %[[OFFSET1:.*]]: index, %[[OFFSET2:.*]]: index)\n func.func @push_transpose_of_extract_tile_to_memref(\n-  %input: memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>>,\n+  %input: memref<4x8x16xf32, #xtile.layout<[2, 0, 1]>>,\n   %offset0: index, %offset1: index, %offset2: index)  ->  tensor<8x4xf32>\n {\n   // CHECK: %[[TRANSPOSE:.*]] = memref.transpose %[[INPUT]]\n \n   // CHECK-SAME: (d0, d1, d2) -> (d2, d1, d0)\n-  // CHECK-SAME: : memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>>\n+  // CHECK-SAME: : memref<4x8x16xf32, #xtile.layout<[2, 0, 1]>>\n   // CHECK-SAME: to memref<16x8x4xf32, strided<[1, 64, 16]>>\n   // CHECK: %[[EXTRACT:.*]] = xtile.extract %[[TRANSPOSE]]\n   // CHECK-SAME: [%[[OFFSET2]], %[[OFFSET1]], %[[OFFSET0]]] [8, 1, 4] [1, 1, 1]\n   // CHECK-SAME: : memref<16x8x4xf32, strided<[1, 64, 16]>> -> tensor<8x4xf32>\n   %tile = xtile.extract %input[%offset0, %offset1, %offset2][4, 1, 8][1, 1, 1]\n-    : memref<4x8x16xf32, #triton_xla.layout<[2, 0, 1]>> -> tensor<4x8xf32>\n+    : memref<4x8x16xf32, #xtile.layout<[2, 0, 1]>> -> tensor<4x8xf32>\n   %transposed = tt.trans %tile {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n   // CHECK: return %[[EXTRACT]] : tensor<8x4xf32>\n   return %transposed : tensor<8x4xf32>"
        },
        {
            "sha": "76884fc75a4f02f4fcec5feb971019189d07847f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_lower_xtile.mlir",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_lower_xtile.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -11,14 +11,14 @@ xtile.entry_func @extract_insert_no_layout(%input: memref<1024xf32, #nvvm.memory\n // CHECK: func.func @extract_insert_no_layout(%[[ARG0:.*]]: !tt.ptr<f32>, %[[ARG1:.*]]: !tt.ptr<f32>) {\n // CHECK:   %[[PID:.*]] = tt.get_program_id x : i32\n // CHECK:   %[[PID_IDX:.*]] = arith.index_cast %[[PID]] : i32 to index\n-// CHECK:   %[[TILE:.*]] = triton_xla.extract from %[[ARG0]] as memref<1024xf32, #triton_xla.layout<[0]>> [%[[PID_IDX]]] [1] [1] : tensor<1xf32>\n-// CHECK:   triton_xla.insert %[[TILE]] into %[[ARG1]] as memref<32xf32, #triton_xla.layout<[0]>> [%[[PID_IDX]]] [1] [1] : tensor<1xf32>\n+// CHECK:   %[[TILE:.*]] = triton_xla.extract from %[[ARG0]] as memref<1024xf32, #xtile.layout<[0]>> [%[[PID_IDX]]] [1] [1] : tensor<1xf32>\n+// CHECK:   triton_xla.insert %[[TILE]] into %[[ARG1]] as memref<32xf32, #xtile.layout<[0]>> [%[[PID_IDX]]] [1] [1] : tensor<1xf32>\n // CHECK:   return\n // CHECK: }\n \n // -----\n \n-!arg_type = memref<1024x32x1x1xbf16, #triton_xla.layout<[2, 3, 0, 1]>, #nvvm.memory_space<global>>\n+!arg_type = memref<1024x32x1x1xbf16, #xtile.layout<[2, 3, 0, 1]>, #nvvm.memory_space<global>>\n xtile.entry_func @layout_preserved(%input: !arg_type,\n                                    %tile_id: index) {\n   %c_0 = arith.constant 0 : index\n@@ -30,7 +30,7 @@ xtile.entry_func @layout_preserved(%input: !arg_type,\n // CHECK:   %[[PID:.*]] = tt.get_program_id x : i32\n // CHECK:   %[[PID_IDX:.*]] = arith.index_cast %[[PID]] : i32 to index\n // CHECK:   %[[TILE:.*]] = triton_xla.extract from %[[ARG0]]\n-// CHECK-SAME: as memref<1024x32x1x1xbf16, #triton_xla.layout<[3, 2, 0, 1]>>\n+// CHECK-SAME: as memref<1024x32x1x1xbf16, #xtile.layout<[3, 2, 0, 1]>>\n // CHECK-SAME: [%[[PID_IDX]], 0, 0, 0]\n // CHECK-SAME: [1, 1, 1, 1] [1, 1, 1, 1] : tensor<1x1x1x1xbf16>\n // CHECK:   return\n@@ -71,12 +71,12 @@ xtile.entry_func @insert_extract_with_opaque_arg(%input: !memref_type,\n // -----\n \n // CHECK-LABEL: func.func @fold_transpose_into_ptr\n-// CHECK-SAME: (%[[ARG0:.*]]: memref<32x16xf64, #triton_xla.layout<[0, 1]>>)\n+// CHECK-SAME: (%[[ARG0:.*]]: memref<32x16xf64, #xtile.layout<[0, 1]>>)\n func.func @fold_transpose_into_ptr(\n-    %arg0: memref<32x16xf64, #triton_xla.layout<[0, 1]>>) -> !tt.ptr<f64> {\n+    %arg0: memref<32x16xf64, #xtile.layout<[0, 1]>>) -> !tt.ptr<f64> {\n   %transposed = memref.transpose %arg0 (d0, d1) -> (d1, d0)\n-    : memref<32x16xf64, #triton_xla.layout<[0, 1]>> to memref<16x32xf64>\n-  // CHECK: %[[PTR:.*]] = triton_xla.memref_to_ptr %[[ARG0]] from memref<32x16xf64, #triton_xla.layout<[0, 1]>> to <f64>\n+    : memref<32x16xf64, #xtile.layout<[0, 1]>> to memref<16x32xf64>\n+  // CHECK: %[[PTR:.*]] = triton_xla.memref_to_ptr %[[ARG0]] from memref<32x16xf64, #xtile.layout<[0, 1]>> to <f64>\n   %ptr = triton_xla.memref_to_ptr %transposed from memref<16x32xf64> to !tt.ptr<f64>\n   // CHECK: return %[[PTR]] : !tt.ptr<f64>\n   return %ptr : !tt.ptr<f64>"
        },
        {
            "sha": "ad17b4fea736ef02322617504bd756aeb2b06cca",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_squeeze_dims.mlir",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -137,14 +137,14 @@ func.func @reshape_with_encoding(%arg0: tensor<1x32xf32, #arg_enc>) -> tensor<32\n // -----\n \n // CHECK-LABEL: func @fold_squeeze_dims_of_extract\n-// CHECK-SAME: (%[[INPUT:.*]]: memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>,\n+// CHECK-SAME: (%[[INPUT:.*]]: memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>>,\n func.func @fold_squeeze_dims_of_extract(\n-  %input: memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>, %offset: index)  -> tensor<4x8xf32>\n+  %input: memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>>, %offset: index)  -> tensor<4x8xf32>\n {\n   // CHECK: %[[EXTRACT:.*]] = xtile.extract %[[INPUT]]\n-  // CHECK-SAME: memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>> -> tensor<4x8xf32>\n+  // CHECK-SAME: memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>> -> tensor<4x8xf32>\n   %tile = xtile.extract %input[%offset, %offset, %offset][4, 1, 8][1, 1, 1]\n-    : memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>> -> tensor<4x1x8xf32>\n+    : memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>> -> tensor<4x1x8xf32>\n   // CHECK-NOT: triton_xla.squeeze_dims\n   %squeezed = triton_xla.squeeze_dims %tile {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n   // CHECK: return %[[EXTRACT]]\n@@ -155,32 +155,32 @@ func.func @fold_squeeze_dims_of_extract(\n // -----\n \n // CHECK-LABEL: func @squeeze_insert(\n-// CHECK-SAME: %[[BUFFER:.*]]: memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>,\n+// CHECK-SAME: %[[BUFFER:.*]]: memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>>,\n // CHECK-SAME: %[[TILE:.*]]: tensor<4x1x8xf32>\n func.func @squeeze_insert(\n-  %arg0: memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>,\n+  %arg0: memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>>,\n   %arg1: tensor<4x1x8xf32>,\n   %offset: index) {\n   // CHECK: %[[REDUCED:.*]] = triton_xla.squeeze_dims %[[TILE]]\n   // CHECK-SAME: {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n   // CHECK: xtile.insert %[[REDUCED]] into %[[BUFFER]]\n-  // CHECK-SAME: tensor<4x8xf32> -> memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>\n+  // CHECK-SAME: tensor<4x8xf32> -> memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>>\n   xtile.insert %arg1 into %arg0[%offset, %offset, %offset][4, 1, 8][1, 1, 1]\n-    : tensor<4x1x8xf32> -> memref<4x16x8xf32, #triton_xla.layout<[2, 1, 0]>>\n+    : tensor<4x1x8xf32> -> memref<4x16x8xf32, #xtile.layout<[2, 1, 0]>>\n   return\n }\n \n // -----\n \n // CHECK-LABEL: func @squeeze_insert_unit_tensor\n func.func @squeeze_insert_unit_tensor(\n-  %arg0: memref<1x1xf32,#triton_xla.layout<[0, 1]>>,\n+  %arg0: memref<1x1xf32,#xtile.layout<[0, 1]>>,\n   %arg1: tensor<1x1xf32>,\n   %offset: index) {\n   // CHECK: triton_xla.squeeze_dims\n   // CHECK: xtile.insert {{.*}} : tensor<1xf32>\n   xtile.insert %arg1 into %arg0[%offset, %offset] [1, 1] [1, 1]\n-    : tensor<1x1xf32> -> memref<1x1xf32,#triton_xla.layout<[0, 1]>>\n+    : tensor<1x1xf32> -> memref<1x1xf32,#xtile.layout<[0, 1]>>\n   return\n }\n "
        },
        {
            "sha": "ef3d0a5c415cf5f5b37d4f9b8b38929cf3339b00",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_xtile_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -81,7 +81,7 @@ llvm::SmallVector<mlir::Type> GetTransformedArgTypes(\n \n // Function to get the permutation vector from a MemRefType.\n // The motivation for extracting it from getStridesAndOffset vs directly from\n-// triton_xla.layout is that when we fold memrefs (such as in a transpose) it\n+// xtile.layout is that when we fold memrefs (such as in a transpose) it\n // will have a generic strided layout that does not directly encode the\n // permutation.\n absl::StatusOr<llvm::SmallVector<int64_t>> getPermutationMinorToMajor("
        },
        {
            "sha": "58f8c97439da40d2f9b184bab0292685d223ccf8",
            "filename": "third_party/xla/xla/codegen/xtile/ir/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2FBUILD?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -99,6 +99,7 @@ cc_library(\n         \"@llvm-project//mlir:BufferizationDialect\",\n         \"@llvm-project//mlir:BufferizationInterfaces\",\n         \"@llvm-project//mlir:BytecodeOpInterface\",\n+        \"@llvm-project//mlir:DialectUtils\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FunctionInterfaces\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "5584c18e8ac5263cdd1190de3c6d0d8d53c30196",
            "filename": "third_party/xla/xla/codegen/xtile/ir/tests/ops.mlir",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Ftests%2Fops.mlir?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -149,3 +149,16 @@ func.func @illegal_mask_out_of_bounds(%src: tensor<32xf64>, %mask: f64) -> tenso\n   return %masked : tensor<32xf64>\n }\n \n+// -----\n+\n+// expected-error @+1 {{layout has 0 dimensions, but shape has 1}}\n+func.func @memref_layout_shape_size_mismatch(%arg0: memref<1024xf32, #xtile.layout<[]>>) {\n+  return\n+}\n+\n+// -----\n+\n+// expected-error @+1 {{layout is not a permutation}}\n+func.func @memref_layout_is_not_a_permutation(%arg0: memref<1024xf32, #xtile.layout<[1]>>) {\n+  return\n+}"
        },
        {
            "sha": "796811d84e19d6567353d6e724496413d0b90a8d",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_attrs.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.cc?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -14,3 +14,47 @@ limitations under the License.\n ==============================================================================*/\n \n #include \"xla/codegen/xtile/ir/xtile_attrs.h\"\n+\n+#include <cstdint>\n+\n+#include \"mlir/Dialect/Utils/IndexingUtils.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/OpDefinition.h\"  // IWYU pragma: keep\n+#include \"mlir/Support/LLVM.h\"\n+\n+namespace xla::xtile {\n+\n+mlir::AffineMap LayoutAttr::getAffineMap() const {\n+  return mlir::AffineMap::getPermutationMap(getMinorToMajor(), getContext());\n+}\n+\n+mlir::LogicalResult LayoutAttr::verifyLayout(\n+    mlir::ArrayRef<int64_t> shape,\n+    mlir::function_ref<mlir::InFlightDiagnostic()> emit_error) const {\n+  if (getMinorToMajor().size() != shape.size()) {\n+    emit_error() << \"layout has \" << getMinorToMajor().size()\n+                 << \" dimensions, but shape has \" << shape.size();\n+    return mlir::failure();\n+  }\n+  if (!mlir::isPermutationVector(getMinorToMajor().asArrayRef())) {\n+    emit_error() << \"layout is not a permutation\";\n+    return mlir::failure();\n+  }\n+  return mlir::success();\n+}\n+\n+mlir::LogicalResult LayoutAttr::getStridesAndOffset(\n+    mlir::ArrayRef<int64_t> shape, mlir::SmallVectorImpl<int64_t>& strides,\n+    int64_t& offset) const {\n+  strides.resize(shape.size());\n+  int64_t size_product = 1;\n+  for (auto dim : getMinorToMajor().asArrayRef()) {\n+    strides[dim] = size_product;\n+    size_product *= shape[dim];\n+  }\n+  offset = 0;\n+  return mlir::success();\n+}\n+\n+}  // namespace xla::xtile"
        },
        {
            "sha": "5deb964741273d22c4a87e599b7a03f802837a87",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_attrs.td",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/502b997973f9361bdddf2aa97e742b0df162ad84/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_attrs.td?ref=502b997973f9361bdddf2aa97e742b0df162ad84",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_CODEGEN_XTILE_IR_XTILE_ATTRS\n \n include \"mlir/IR/AttrTypeBase.td\"\n+include \"mlir/IR/BuiltinAttributeInterfaces.td\"\n include \"mlir/IR/EnumAttr.td\"\n include \"xla/codegen/xtile/ir/xtile_dialect.td\"\n \n@@ -35,4 +36,17 @@ def XTile_TilingInfoAttr : XTile_Attr<\"TilingInfo\"> {\n   }];\n }\n \n+def XTile_LayoutAttr : XTile_Attr<\"Layout\", [\n+    DeclareAttrInterfaceMethods<MemRefLayoutAttrInterface>]> {\n+  let mnemonic = \"layout\";\n+  let parameters = (ins \"::mlir::DenseI64ArrayAttr\":$minor_to_major);\n+  let assemblyFormat = \"`<` $minor_to_major `>`\";\n+  let extraClassDeclaration = [{\n+    mlir::LogicalResult verifyLayout(\n+        mlir::ArrayRef<int64_t> shape, mlir::function_ref<mlir::InFlightDiagnostic()> emit_error) const;\n+    mlir::LogicalResult getStridesAndOffset(\n+        mlir::ArrayRef<int64_t> shape, mlir::SmallVectorImpl<int64_t>& strides, int64_t& offset) const;\n+  }];\n+}\n+\n #endif // XLA_CODEGEN_XTILE_IR_XTILE_ATTRS"
        }
    ],
    "stats": {
        "total": 308,
        "additions": 170,
        "deletions": 138
    }
}