{
    "author": "tensorflower-gardener",
    "message": "Apply llvm-use-new-mlir-op-builder fixes\n\nThis migrates `builder.create<Op>()` => `Op::create()`\n\nPiperOrigin-RevId: 846862419",
    "sha": "84ad581652911f91120f53bb07bcaeed812f34a3",
    "files": [
        {
            "sha": "c2aee328a9cd23bdf23eab450b420a04b4638707",
            "filename": "tensorflow/compiler/tf2xla/kernels/xla_call_module_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_call_module_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_call_module_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fkernels%2Fxla_call_module_op.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -519,7 +519,7 @@ class XlaCallModuleOp : public XlaOpKernel {\n         } else if (options.add_token_input_output) {\n           // Add a dummy token if the inner computation takes a token but the\n           // custom call doesn't have a token argument.\n-          args.push_back(builder.create<mlir::stablehlo::CreateTokenOp>(loc));\n+          args.push_back(mlir::stablehlo::CreateTokenOp::create(builder, loc));\n         }\n \n         input_args.reserve(result.input_mapping.size());\n@@ -530,7 +530,7 @@ class XlaCallModuleOp : public XlaOpKernel {\n \n       // Call the lowered function.\n       auto call =\n-          builder.create<mlir::func::CallOp>(loc, main_func, input_args);\n+          mlir::func::CallOp::create(builder, loc, main_func, input_args);\n \n       // Unpack the result tuple (`options.always_return_tuple` is true). If\n       // `has_tuple_input_output` is true, the first result is a token type.\n@@ -548,7 +548,7 @@ class XlaCallModuleOp : public XlaOpKernel {\n             mlir::Value token = results.back();\n             if (!token.use_empty()) {\n               token.replaceAllUsesWith(\n-                  builder.create<mlir::stablehlo::CreateTokenOp>(loc));\n+                  mlir::stablehlo::CreateTokenOp::create(builder, loc));\n             }\n             results.pop_back();\n           }"
        },
        {
            "sha": "c3bee77403884c1b3f5f42b4f9dc43199fab679f",
            "filename": "tensorflow/core/function/testing/test_pass.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ffunction%2Ftesting%2Ftest_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ffunction%2Ftesting%2Ftest_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ffunction%2Ftesting%2Ftest_pass.h?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -101,7 +101,8 @@ struct TestPassTfDialect\n     DCHECK(target != nullptr);\n \n     builder.setInsertionPoint(target);\n-    auto replacement = builder.create<mlir::TF::AddV2Op>(\n+    auto replacement = mlir::TF::AddV2Op::create(\n+        builder,\n         mlir::NameLoc::get(\n             mlir::StringAttr::get(builder.getContext(), \"x_plus_y\")),\n         target->getResultTypes(), target->getOperand(0), target->getOperand(1));"
        },
        {
            "sha": "e58e58aec7f9c02710dda759800a55ed5c00fd16",
            "filename": "tensorflow/core/transforms/eliminate_passthrough_iter_args/pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Feliminate_passthrough_iter_args%2Fpass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Feliminate_passthrough_iter_args%2Fpass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftransforms%2Feliminate_passthrough_iter_args%2Fpass.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -133,8 +133,8 @@ struct EliminateForPassthroughIterArgs\n   static ForRegionOp RebuildOp(const llvm::BitVector &indices, ForRegionOp op,\n                                IRRewriter &rewriter) {\n     rewriter.setInsertionPoint(op);\n-    auto new_op = rewriter.create<ForRegionOp>(\n-        op.getLoc(), FilterByIndex(op.getOuts().getTypes(), indices),\n+    auto new_op = ForRegionOp::create(\n+        rewriter, op.getLoc(), FilterByIndex(op.getOuts().getTypes(), indices),\n         op.getCtl().getType(), op.getStart(), op.getLimit(), op.getDelta(),\n         FilterByIndex(op.getInit(), indices), op.getCtls(),\n         op.getBodyAttrsAttr(), op.getRegionAttrsAttr());\n@@ -163,8 +163,8 @@ struct EliminateWhileLikePassthroughIterArgs\n                                      WhileLikeRegionOp op,\n                                      IRRewriter &rewriter) {\n     rewriter.setInsertionPoint(op);\n-    auto new_op = rewriter.create<WhileLikeRegionOp>(\n-        op.getLoc(), FilterByIndex(op.getOuts().getTypes(), indices),\n+    auto new_op = WhileLikeRegionOp::create(\n+        rewriter, op.getLoc(), FilterByIndex(op.getOuts().getTypes(), indices),\n         op.getCtl().getType(), FilterByIndex(op.getInit(), indices),\n         op.getCtls(), op.getParallelIterationsAttr(), op.getCondAttrsAttr(),\n         op.getBodyAttrsAttr(), op.getCondRegionAttrsAttr(),"
        },
        {
            "sha": "5be91dbb286d92bff68e6824ff93317a31b55e0c",
            "filename": "tensorflow/core/transforms/func_to_graph/func_to_graph.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Ffunc_to_graph%2Ffunc_to_graph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Ffunc_to_graph%2Ffunc_to_graph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftransforms%2Ffunc_to_graph%2Ffunc_to_graph.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -102,7 +102,7 @@ absl::Status FuncToGraph(GraphFuncOp func) {\n   }\n \n   OpBuilder builder(func);\n-  auto graph = builder.create<GraphOp>(func.getLoc(), version);\n+  auto graph = GraphOp::create(builder, func.getLoc(), version);\n \n   // Remove the terminator.\n   func.SingleBlock::getBody()->getTerminator()->erase();"
        },
        {
            "sha": "aaf67332ae2d48ca823c984e6205ed06d533dd76",
            "filename": "tensorflow/core/transforms/functional_to_region/impl.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Ffunctional_to_region%2Fimpl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Ffunctional_to_region%2Fimpl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftransforms%2Ffunctional_to_region%2Fimpl.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -322,8 +322,8 @@ LogicalResult ConvertIfLikeOp<IfLikeOp, IfLikeRegionOp>::matchAndRewrite(\n   // Create the region-based op, passing in the required attributes.\n   ValueRange args, ctls;\n   std::tie(args, ctls) = this->SplitControl(op.getArgs());\n-  auto region_op = rewriter.create<IfLikeRegionOp>(\n-      op.getLoc(), op.getResultTypes(), op.getCond(), ctls,\n+  auto region_op = IfLikeRegionOp::create(\n+      rewriter, op.getLoc(), op.getResultTypes(), op.getCond(), ctls,\n       op.getThenBranch().getAttrs(), op.getElseBranch().getAttrs(),\n       PreserveAttributes(then_func, /*drop_args=*/true),\n       PreserveAttributes(else_func, /*drop_args=*/true));\n@@ -390,8 +390,8 @@ LogicalResult ConvertCaseLikeOp<CaseLikeOp, CaseLikeRegionOp>::matchAndRewrite(\n   // Create the region-based op, passing in the required attributes.\n   ValueRange args, ctls;\n   std::tie(args, ctls) = this->SplitControl(op.getArgs());\n-  auto region_op = rewriter.create<CaseLikeRegionOp>(\n-      op.getLoc(), op.getResultTypes(), op.getBranchIndex(), ctls,\n+  auto region_op = CaseLikeRegionOp::create(\n+      rewriter, op.getLoc(), op.getResultTypes(), op.getBranchIndex(), ctls,\n       rewriter.getArrayAttr(branch_attrs), region_attrs,\n       op.getBranches().size());\n   util::ForwardNonIntrinsicAttributes(op, region_op);\n@@ -440,8 +440,8 @@ ConvertWhileLikeOp<WhileLikeOp, WhileLikeRegionOp>::matchAndRewrite(\n   // TODO(jeffniu): Change this to call the infer return types builder.\n   ValueRange init, ctls;\n   std::tie(init, ctls) = this->SplitControl(op.getArgs());\n-  auto region_op = rewriter.create<WhileLikeRegionOp>(\n-      op.getLoc(), op.getResultTypes(), init, ctls,\n+  auto region_op = WhileLikeRegionOp::create(\n+      rewriter, op.getLoc(), op.getResultTypes(), init, ctls,\n       op.getParallelIterationsAttr(), op.getCond().getAttrs(),\n       op.getBody().getAttrs(), PreserveAttributes(cond_func),\n       PreserveAttributes(body_func));\n@@ -482,8 +482,8 @@ LogicalResult ConvertForOp::matchAndRewrite(tfg::ForOp op,\n   // `ForRegion` does. We will need to insert casts.\n   ValueRange init, ctls;\n   std::tie(init, ctls) = SplitControl(op.getArgs());\n-  auto region_op = rewriter.create<ForRegionOp>(\n-      op.getLoc(), op.getResultTypes(), op.getStart(), op.getLimit(),\n+  auto region_op = ForRegionOp::create(\n+      rewriter, op.getLoc(), op.getResultTypes(), op.getStart(), op.getLimit(),\n       op.getDelta(), init, ctls, op.getBody().getAttrs(),\n       PreserveAttributes(body_func));\n   util::ForwardNonIntrinsicAttributes(op, region_op);"
        },
        {
            "sha": "ae9e8d48c6a17a4839efe56a7eaab1f90da20e62",
            "filename": "tensorflow/core/transforms/graph_to_func/graph_to_func.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Fgraph_to_func%2Fgraph_to_func.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Fgraph_to_func%2Fgraph_to_func.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftransforms%2Fgraph_to_func%2Fgraph_to_func.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -66,16 +66,16 @@ absl::Status GraphToFunc(GraphOp graph, ArrayRef<Value> feeds,\n \n   FunctionType func_type = builder.getFunctionType(arg_types, ret_types);\n   auto loc = graph.getLoc();\n-  auto func_op = builder.create<GraphFuncOp>(loc, func_name, func_type,\n-                                             /*generic=*/false);\n+  auto func_op = GraphFuncOp::create(builder, loc, func_name, func_type,\n+                                     /*generic=*/false);\n   func_op->setAttr(\"tfg.lifted_graph_version\", graph.getVersion());\n   func_op.getRegion().takeBody(graph.getRegion());\n \n   // Create the returnOp first so that if there are nodes in both feeds and\n   // fetches, the fetch value will be replaced with feed argument.\n   OpBuilder body_builder =\n       OpBuilder::atBlockEnd(func_op.SingleBlock::getBody());\n-  body_builder.create<ReturnOp>(loc, fetches, control_rets);\n+  ReturnOp::create(body_builder, loc, fetches, control_rets);\n \n   StringAttr tfg_name = dialect->getTfgNameAttrIdentifier();\n   StringAttr lifted_value_name = builder.getStringAttr(\"tfg.lifted_value_attr\");"
        },
        {
            "sha": "9fce62a74a117375afd8f868889fdb6208e705b7",
            "filename": "tensorflow/core/transforms/region_to_functional/impl.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Fregion_to_functional%2Fimpl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fcore%2Ftransforms%2Fregion_to_functional%2Fimpl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftransforms%2Fregion_to_functional%2Fimpl.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -232,8 +232,8 @@ struct ConvertIfLikeRegionOpToExplicitCapture\n \n   IfLikeRegionOp RebuildWith(IfLikeRegionOp op, ValueRange added,\n                              PatternRewriter &rewriter) const override {\n-    return rewriter.create<IfLikeRegionOp>(\n-        op.getLoc(), op.getResultTypes(), op.getCond(), op.getCtls(),\n+    return IfLikeRegionOp::create(\n+        rewriter, op.getLoc(), op.getResultTypes(), op.getCond(), op.getCtls(),\n         op.getThenAttrsAttr(), op.getElseAttrsAttr(),\n         op.getThenRegionAttrsAttr(), op.getElseRegionAttrsAttr());\n   }\n@@ -246,9 +246,9 @@ struct ConvertCaseLikeRegionOpToExplicitCapture\n \n   CaseLikeRegionOp RebuildWith(CaseLikeRegionOp op, ValueRange added,\n                                PatternRewriter &rewriter) const override {\n-    return rewriter.create<CaseLikeRegionOp>(\n-        op.getLoc(), op.getResultTypes(), op.getBranchIndex(), op.getCtls(),\n-        op.getBranchAttrsAttr(), op.getRegionAttrsAttr(),\n+    return CaseLikeRegionOp::create(\n+        rewriter, op.getLoc(), op.getResultTypes(), op.getBranchIndex(),\n+        op.getCtls(), op.getBranchAttrsAttr(), op.getRegionAttrsAttr(),\n         op.getBranches().size());\n   }\n };\n@@ -294,9 +294,9 @@ struct ConvertWhileLikeRegionOpToExplicitCapture\n     util::LoopRegionResultAdded(op.getBodyRegion(), added.size());\n \n     rewriter.setInsertionPoint(op);\n-    return rewriter.create<WhileLikeRegionOp>(\n-        op.getLoc(), results, op.getCtl().getType(), operands, op.getCtls(),\n-        op.getParallelIterationsAttr(), op.getCondAttrsAttr(),\n+    return WhileLikeRegionOp::create(\n+        rewriter, op.getLoc(), results, op.getCtl().getType(), operands,\n+        op.getCtls(), op.getParallelIterationsAttr(), op.getCondAttrsAttr(),\n         op.getBodyAttrsAttr(), op.getCondRegionAttrsAttr(),\n         op.getBodyRegionAttrsAttr());\n   }\n@@ -323,8 +323,8 @@ struct ConvertForRegionOpToExplicitCapture\n     util::LoopRegionResultAdded(op.getBodyRegion(), added.size());\n \n     rewriter.setInsertionPoint(op);\n-    return rewriter.create<ForRegionOp>(\n-        op.getLoc(), results, op.getCtl().getType(), op.getStart(),\n+    return ForRegionOp::create(\n+        rewriter, op.getLoc(), results, op.getCtl().getType(), op.getStart(),\n         op.getLimit(), op.getDelta(), operands, op.getCtls(),\n         op.getBodyAttrsAttr(), op.getRegionAttrsAttr());\n   }\n@@ -869,8 +869,8 @@ LogicalResult ConvertIfLikeOp<IfLikeRegionOp, IfLikeOp>::matchAndRewrite(\n \n   rewriter.setInsertionPoint(op);\n   auto func_op =\n-      rewriter.create<IfLikeOp>(op.getLoc(), op.getResultTypes(), op.getCond(),\n-                                operands, branches[0], branches[1]);\n+      IfLikeOp::create(rewriter, op.getLoc(), op.getResultTypes(), op.getCond(),\n+                       operands, branches[0], branches[1]);\n   util::ForwardNonIntrinsicAttributes(op, func_op);\n   rewriter.replaceOp(op, func_op.getResults());\n   return success();\n@@ -923,9 +923,9 @@ LogicalResult ConvertCaseLikeOp<CaseLikeRegionOp, CaseLikeOp>::matchAndRewrite(\n   llvm::append_range(operands, op.getCtls());\n \n   rewriter.setInsertionPoint(op);\n-  auto func_op = rewriter.create<CaseLikeOp>(op.getLoc(), op.getResultTypes(),\n-                                             op.getBranchIndex(), operands,\n-                                             rewriter.getArrayAttr(branches));\n+  auto func_op = CaseLikeOp::create(rewriter, op.getLoc(), op.getResultTypes(),\n+                                    op.getBranchIndex(), operands,\n+                                    rewriter.getArrayAttr(branches));\n   util::ForwardNonIntrinsicAttributes(op, func_op);\n   rewriter.replaceOp(op, func_op.getResults());\n   return success();\n@@ -999,9 +999,9 @@ ConvertWhileLikeOp<WhileLikeRegionOp, WhileLikeOp>::matchAndRewrite(\n   llvm::append_range(operands, op.getCtls());\n \n   rewriter.setInsertionPoint(op);\n-  auto func_op = rewriter.create<WhileLikeOp>(op.getLoc(), op.getResultTypes(),\n-                                              operands, cond_ref, body_ref,\n-                                              op.getParallelIterationsAttr());\n+  auto func_op =\n+      WhileLikeOp::create(rewriter, op.getLoc(), op.getResultTypes(), operands,\n+                          cond_ref, body_ref, op.getParallelIterationsAttr());\n   util::ForwardNonIntrinsicAttributes(op, func_op);\n   rewriter.replaceOp(op, func_op.getResults());\n   return success();\n@@ -1037,9 +1037,9 @@ LogicalResult ConvertForOp::matchAndRewrite(ForRegionOp op,\n   llvm::append_range(operands, op.getCtls());\n \n   rewriter.setInsertionPoint(op);\n-  auto func_op = rewriter.create<tfg::ForOp>(\n-      op.getLoc(), op.getResultTypes(), op.getStart(), op.getLimit(),\n-      op.getDelta(), operands, body_ref[0]);\n+  auto func_op = tfg::ForOp::create(rewriter, op.getLoc(), op.getResultTypes(),\n+                                    op.getStart(), op.getLimit(), op.getDelta(),\n+                                    operands, body_ref[0]);\n   util::ForwardNonIntrinsicAttributes(op, func_op);\n   rewriter.replaceOp(op, func_op.getResults());\n   return success();"
        },
        {
            "sha": "2f8d75cca43fa9edf182719ddd67cf876a2a820a",
            "filename": "tensorflow/dtensor/cc/save_restore_util.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fcc%2Fsave_restore_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fcc%2Fsave_restore_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fcc%2Fsave_restore_util.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -156,13 +156,12 @@ SaveOpSpecs BuildPerDeviceSave(\n         shape_and_slice_specs.push_back({});\n \n         mlir::Value new_prefix =\n-            builder\n-                .create<mlir::TF::AddOp>(\n-                    prefix.getLoc(),\n-                    mlir::dyn_cast<mlir::RankedTensorType>(prefix.getType()),\n-                    prefix,\n-                    StringScalarConst(builder, prefix.getLoc(),\n-                                      DeviceSuffix(device_id, total_devices)))\n+            mlir::TF::AddOp::create(\n+                builder, prefix.getLoc(),\n+                mlir::dyn_cast<mlir::RankedTensorType>(prefix.getType()),\n+                prefix,\n+                StringScalarConst(builder, prefix.getLoc(),\n+                                  DeviceSuffix(device_id, total_devices)))\n                 .getZ();\n         // Generate new prefix based on device_id and save op index, only when\n         // we need a new save_op."
        },
        {
            "sha": "51107b7adf544c6256ff1ecddaa1e7c9a4f9bc3d",
            "filename": "tensorflow/dtensor/mlir/cluster_function_conversion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fcluster_function_conversion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fcluster_function_conversion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fcluster_function_conversion.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -142,8 +142,8 @@ mlir::LogicalResult ReplaceClusterWithPartitionCallOp(\n   llvm::StringRef function_name = cluster_func.getFunc();\n \n   builder->setInsertionPoint(cluster_func);\n-  auto call_op = builder->create<mlir::TF::StatefulPartitionedCallOp>(\n-      cluster_func.getLoc(), output_types, cluster_func.getOperands(),\n+  auto call_op = mlir::TF::StatefulPartitionedCallOp::create(\n+      *builder, cluster_func.getLoc(), output_types, cluster_func.getOperands(),\n       /*args_attrs=*/nullptr, /*res_attrs=*/nullptr, function_name, mesh_attr,\n       /*config_proto=*/builder->getStringAttr(\"\"),\n       /*executor_type=*/builder->getStringAttr(\"\"));"
        },
        {
            "sha": "ca4f5b6e8febdac9ef5eb948f6ecaec9eb8a212e",
            "filename": "tensorflow/dtensor/mlir/collectives.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 52,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fcollectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fcollectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fcollectives.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -105,8 +105,8 @@ StatusOr<mlir::Value> EmitAllGather(\n \n   mlir::Location loc = DT_LOC2(input.getLoc(), \"DTensorAllGatherOp\");\n   mlir::TF::DTensorAllGatherOp all_gather =\n-      builder.create<mlir::TF::DTensorAllGatherOp>(\n-          loc, output_type, input,\n+      mlir::TF::DTensorAllGatherOp::create(\n+          builder, loc, output_type, input,\n           mlir::dtensor::LayoutAttr::get(builder.getContext(), src_layout),\n           mlir::dtensor::LayoutAttr::get(builder.getContext(), tgt_layout));\n   SetSingleLayoutOnOp(all_gather, tgt_layout);\n@@ -153,8 +153,8 @@ StatusOr<const mlir::Value> EmitAllScatter(\n \n   mlir::Location loc = DT_LOC2(original_value.getLoc(), \"DTensorAllScatterOp\");\n   mlir::TF::DTensorAllScatterOp all_scatter =\n-      builder.create<mlir::TF::DTensorAllScatterOp>(\n-          loc, output_type, original_value,\n+      mlir::TF::DTensorAllScatterOp::create(\n+          builder, loc, output_type, original_value,\n           mlir::dtensor::LayoutAttr::get(builder.getContext(), original_layout),\n           mlir::dtensor::LayoutAttr::get(builder.getContext(), desired_layout));\n   SetSingleLayoutOnOp(all_scatter, desired_layout);\n@@ -224,11 +224,10 @@ StatusOr<mlir::Value> EmitAllToAll(\n                       LocalTypeFromGlobalType(tgt_layout, global_type));\n \n   mlir::Location loc = DT_LOC2(input.getLoc(), \"DTensorAllToAllOp\");\n-  mlir::TF::DTensorAllToAllOp all_to_all =\n-      builder.create<mlir::TF::DTensorAllToAllOp>(\n-          loc, output_type, input,\n-          mlir::dtensor::LayoutAttr::get(builder.getContext(), src_layout),\n-          mlir::dtensor::LayoutAttr::get(builder.getContext(), tgt_layout));\n+  mlir::TF::DTensorAllToAllOp all_to_all = mlir::TF::DTensorAllToAllOp::create(\n+      builder, loc, output_type, input,\n+      mlir::dtensor::LayoutAttr::get(builder.getContext(), src_layout),\n+      mlir::dtensor::LayoutAttr::get(builder.getContext(), tgt_layout));\n   SetSingleLayoutOnOp(all_to_all, tgt_layout);\n \n   if (newly_created_ops != nullptr) newly_created_ops->insert(all_to_all);\n@@ -247,20 +246,21 @@ StatusOr<mlir::Value> EmitDenseToSparseToDense(\n   // values tensor = tf.gather_nd(input, indices)\n   // shape tensor = tf.shape(input)\n   mlir::TF::ZerosLikeOp zeros_like =\n-      builder.create<mlir::TF::ZerosLikeOp>(input.getLoc(), input);\n-  mlir::TF::NotEqualOp not_equal = builder.create<mlir::TF::NotEqualOp>(\n-      zeros_like.getLoc(), input, zeros_like, builder.getBoolAttr(false));\n+      mlir::TF::ZerosLikeOp::create(builder, input.getLoc(), input);\n+  mlir::TF::NotEqualOp not_equal =\n+      mlir::TF::NotEqualOp::create(builder, zeros_like.getLoc(), input,\n+                                   zeros_like, builder.getBoolAttr(false));\n \n-  mlir::TF::WhereOp indices = builder.create<mlir::TF::WhereOp>(\n-      not_equal.getLoc(),\n+  mlir::TF::WhereOp indices = mlir::TF::WhereOp::create(\n+      builder, not_equal.getLoc(),\n       mlir::RankedTensorType::get(GetShapeOfValue(not_equal).value(),\n                                   builder.getI64Type()),\n       not_equal);\n \n-  mlir::TF::GatherNdOp values = builder.create<mlir::TF::GatherNdOp>(\n-      input.getLoc(), input.getType(), input, indices);\n-  auto shape = builder.create<mlir::TF::ShapeOp>(input.getLoc(), input,\n-                                                 builder.getBoolAttr(false));\n+  mlir::TF::GatherNdOp values = mlir::TF::GatherNdOp::create(\n+      builder, input.getLoc(), input.getType(), input, indices);\n+  auto shape = mlir::TF::ShapeOp::create(builder, input.getLoc(), input,\n+                                         builder.getBoolAttr(false));\n \n   // Emit a SparseToDenseOp and replace the SparseTensor with the result of\n   // this new op.\n@@ -270,8 +270,8 @@ StatusOr<mlir::Value> EmitDenseToSparseToDense(\n           builder, input.getLoc(),\n           mlir::cast<mlir::TensorType>(input.getType()).getElementType()));\n \n-  auto dense = builder.create<mlir::TF::SparseToDenseOp>(\n-      input.getLoc(), input.getType(),\n+  auto dense = mlir::TF::SparseToDenseOp::create(\n+      builder, input.getLoc(), input.getType(),\n       mlir::ValueRange({indices, shape, values, zero_scalar}));\n \n   if (newly_created_ops != nullptr) {\n@@ -310,8 +310,8 @@ StatusOr<mlir::Value> EmitRelayout(\n   // If two layouts are the same, or the only difference is layout type, then\n   // there is no need to actually relayout data.\n   if (src_layout.IsEquivalentIgnoringType(tgt_layout)) {\n-    mlir::TF::IdentityOp op = builder.create<mlir::TF::IdentityOp>(\n-        input.getLoc(), input.getType(), input);\n+    mlir::TF::IdentityOp op = mlir::TF::IdentityOp::create(\n+        builder, input.getLoc(), input.getType(), input);\n     if (newly_created_ops != nullptr) newly_created_ops->insert(op);\n     return op.getOutput();\n   }\n@@ -405,7 +405,7 @@ mlir::Operation* EmitTransposeOp(mlir::OpBuilder& builder,\n \n   auto constant_attr = builder.getI64TensorAttr(perm_arr);\n   auto perm_op =\n-      builder.create<mlir::TF::ConstOp>(loc, perm_type, constant_attr);\n+      mlir::TF::ConstOp::create(builder, loc, perm_type, constant_attr);\n \n   std::vector<int64_t> transposed_shape(shape.begin(), shape.end());\n   for (int i = 0; i < shape.size(); i++) {\n@@ -414,8 +414,8 @@ mlir::Operation* EmitTransposeOp(mlir::OpBuilder& builder,\n   auto transposed_type = mlir::RankedTensorType::get(\n       transposed_shape, tr_input_type.getElementType());\n \n-  return builder.create<mlir::TF::TransposeOp>(loc, transposed_type, input,\n-                                               perm_op);\n+  return mlir::TF::TransposeOp::create(builder, loc, transposed_type, input,\n+                                       perm_op);\n }\n \n StatusOr<mlir::Operation*> EmitBarrierWithConstValue(mlir::OpBuilder& builder,\n@@ -470,10 +470,10 @@ StatusOr<mlir::Operation*> EmitAllReduce(\n                       DeviceTypeFromMesh(output_layout.mesh()));\n \n   mlir::Location loc = DT_LOC2(input->getLoc(), \"DTensorAllReduceOp\");\n-  auto all_reduce = builder.create<mlir::TF::DTensorAllReduceOp>(\n-      loc, input->getResultTypes()[0], input->getOpResult(0),\n-      builder.create<mlir::TF::ConstOp>(DT_LOC2(loc, \"group_assignment\"),\n-                                        group_assignment),\n+  auto all_reduce = mlir::TF::DTensorAllReduceOp::create(\n+      builder, loc, input->getResultTypes()[0], input->getOpResult(0),\n+      mlir::TF::ConstOp::create(builder, DT_LOC2(loc, \"group_assignment\"),\n+                                group_assignment),\n       builder.getStringAttr(std::string(reduce_op)),\n       builder.getStringAttr(device_type));\n   SetSingleLayoutOnOp(all_reduce, output_layout);\n@@ -575,7 +575,7 @@ StatusOr<mlir::Value> CreateConstSrcTargetPair(const Mesh& mesh,\n   auto src_target_attr =\n       mlir::DenseIntElementsAttr::get(shaped_type, src_target_pair_flat);\n   mlir::Value src_target_pair_tensor =\n-      builder.create<mlir::TF::ConstOp>(location, src_target_attr);\n+      mlir::TF::ConstOp::create(builder, location, src_target_attr);\n   return src_target_pair_tensor;\n }\n \n@@ -636,13 +636,14 @@ StatusOr<mlir::Value> EmitHaloExchange(mlir::OpBuilder& builder, int halo_size,\n   //\n   // For example, if mesh dimension splits the input tensor by its height\n   // dimension, then `left` actually means tensor to pad on the top side.\n-  mlir::Value is_on_left_edge = builder.create<mlir::TF::EqualOp>(\n-      location, CreateIntScalarConst(0, builder, location, /*use_int64=*/false),\n+  mlir::Value is_on_left_edge = mlir::TF::EqualOp::create(\n+      builder, location,\n+      CreateIntScalarConst(0, builder, location, /*use_int64=*/false),\n       scalar_mesh_coordinate, builder.getBoolAttr(true));\n \n   TF_ASSIGN_OR_RETURN(const int mesh_dim_size, mesh.dim_size(mesh_dim));\n-  mlir::Value is_on_right_edge = builder.create<mlir::TF::EqualOp>(\n-      location,\n+  mlir::Value is_on_right_edge = mlir::TF::EqualOp::create(\n+      builder, location,\n       CreateIntScalarConst(mesh_dim_size - 1, builder, location,\n                            /*use_int64=*/false),\n       scalar_mesh_coordinate, builder.getBoolAttr(true));\n@@ -663,7 +664,7 @@ StatusOr<mlir::Value> EmitHaloExchange(mlir::OpBuilder& builder, int halo_size,\n   }\n \n   mlir::Value ghost_tensor_left =\n-      builder.create<mlir::TF::ConstOp>(location, const_attr).getResult();\n+      mlir::TF::ConstOp::create(builder, location, const_attr).getResult();\n \n   // Get the right side slice of the input tensor to pad on left side.\n   llvm::SmallVector<int64_t, 4> begin_left(layout.rank(), 0);\n@@ -676,11 +677,13 @@ StatusOr<mlir::Value> EmitHaloExchange(mlir::OpBuilder& builder, int halo_size,\n   size[split_dim_index] = halo_size;\n \n   mlir::Value size_tensor_left = ops_util::GetR1Const(size, builder, location);\n-  mlir::Value sliced_tensor_left = builder.create<mlir::TF::SliceOp>(\n-      location, halo_type, tensor, begin_tensor_left, size_tensor_left);\n+  mlir::Value sliced_tensor_left =\n+      mlir::TF::SliceOp::create(builder, location, halo_type, tensor,\n+                                begin_tensor_left, size_tensor_left);\n \n-  mlir::Value halo_tensor_left = builder.create<mlir::TF::SelectV2Op>(\n-      location, is_on_right_edge, ghost_tensor_left, sliced_tensor_left);\n+  mlir::Value halo_tensor_left =\n+      mlir::TF::SelectV2Op::create(builder, location, is_on_right_edge,\n+                                   ghost_tensor_left, sliced_tensor_left);\n \n   // Invoke collective permute to receive the tensor from neighboring processor.\n   // Halo slices from the left neighbor are received on each processor (they\n@@ -690,12 +693,12 @@ StatusOr<mlir::Value> EmitHaloExchange(mlir::OpBuilder& builder, int halo_size,\n       CreateConstSrcTargetPair(mesh, mesh_dim, /*shift_left=*/false, location,\n                                builder));\n \n-  mlir::Value left_concat_value = builder.create<mlir::TF::CollectivePermuteOp>(\n-      location, sliced_tensor_left.getType(), halo_tensor_left,\n+  mlir::Value left_concat_value = mlir::TF::CollectivePermuteOp::create(\n+      builder, location, sliced_tensor_left.getType(), halo_tensor_left,\n       src_target_pair_left);\n \n   mlir::Value ghost_tensor_right =\n-      builder.create<mlir::TF::ConstOp>(location, const_attr).getResult();\n+      mlir::TF::ConstOp::create(builder, location, const_attr).getResult();\n \n   // Else, values to pad is tensor from different processor. We use collective\n   // permute to access tensor slice from another device.\n@@ -704,13 +707,15 @@ StatusOr<mlir::Value> EmitHaloExchange(mlir::OpBuilder& builder, int halo_size,\n   mlir::Value begin_tensor_right =\n       ops_util::GetR1Const(begin_right, builder, location);\n   mlir::Value size_tensor_right = ops_util::GetR1Const(size, builder, location);\n-  mlir::Value sliced_tensor_right = builder.create<mlir::TF::SliceOp>(\n-      location, halo_type, tensor, begin_tensor_right, size_tensor_right);\n+  mlir::Value sliced_tensor_right =\n+      mlir::TF::SliceOp::create(builder, location, halo_type, tensor,\n+                                begin_tensor_right, size_tensor_right);\n \n   // Find the halo tensor value to pad on the `right` side.\n   // If input block is on the right edge, we use zero ghost tensor instead.\n-  mlir::Value halo_tensor_right = builder.create<mlir::TF::SelectV2Op>(\n-      location, is_on_left_edge, ghost_tensor_right, sliced_tensor_right);\n+  mlir::Value halo_tensor_right =\n+      mlir::TF::SelectV2Op::create(builder, location, is_on_left_edge,\n+                                   ghost_tensor_right, sliced_tensor_right);\n \n   // Invoke collective permute to receive the tensor from neighboring processor.\n   // Halo slices from the right neighbor are received on each processor (they\n@@ -719,10 +724,9 @@ StatusOr<mlir::Value> EmitHaloExchange(mlir::OpBuilder& builder, int halo_size,\n       mlir::Value src_target_pair_right,\n       CreateConstSrcTargetPair(mesh, mesh_dim, /*shift_left=*/true, location,\n                                builder));\n-  mlir::Value right_concat_value =\n-      builder.create<mlir::TF::CollectivePermuteOp>(\n-          location, sliced_tensor_right.getType(), halo_tensor_right,\n-          src_target_pair_right);\n+  mlir::Value right_concat_value = mlir::TF::CollectivePermuteOp::create(\n+      builder, location, sliced_tensor_right.getType(), halo_tensor_right,\n+      src_target_pair_right);\n \n   // Final halo exchanged value is concatenated value of left_concat_value,\n   // tensor, and right_concat_value in the mesh_dimension.\n@@ -734,8 +738,8 @@ StatusOr<mlir::Value> EmitHaloExchange(mlir::OpBuilder& builder, int halo_size,\n       final_shape, input_tensor_type.getElementType());\n   mlir::Value concat_axis =\n       CreateIntScalarConst(split_dim_index, builder, location);\n-  mlir::Value final_value = builder.create<mlir::TF::ConcatV2Op>(\n-      location, final_type,\n+  mlir::Value final_value = mlir::TF::ConcatV2Op::create(\n+      builder, location, final_type,\n       llvm::SmallVector<mlir::Value, 4>{left_concat_value, tensor,\n                                         right_concat_value},\n       concat_axis);"
        },
        {
            "sha": "3b0e959ee329792fafb9beb8a44edddadf82c8f4",
            "filename": "tensorflow/dtensor/mlir/device_mesh_cluster_coarsening.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdevice_mesh_cluster_coarsening.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdevice_mesh_cluster_coarsening.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdevice_mesh_cluster_coarsening.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -194,8 +194,8 @@ mlir::LogicalResult CreateMergedMeshCluster(\n     output_values_to_replace.emplace_back(std::get<1>(cluster_return_value));\n   }\n \n-  *merged_cluster = builder->create<mlir::tf_device::ClusterOp>(\n-      current_cluster.getLoc(), merged_cluster_output_types);\n+  *merged_cluster = mlir::tf_device::ClusterOp::create(\n+      *builder, current_cluster.getLoc(), merged_cluster_output_types);\n   auto mesh_attr = current_cluster->getAttrOfType<mlir::StringAttr>(kMeshAttr);\n   if (!mesh_attr)\n     return current_cluster.emitOpError(kMissingMeshAttributeErrorMessage);\n@@ -206,8 +206,8 @@ mlir::LogicalResult CreateMergedMeshCluster(\n   // `current_cluster` and `merging_cluster`.\n   merged_cluster->getBody().push_back(new mlir::Block);\n   builder->setInsertionPointToEnd(&merged_cluster->GetBody());\n-  builder->create<mlir::tf_device::ReturnOp>(merged_cluster->getLoc(),\n-                                             merged_cluster_output_values);\n+  mlir::tf_device::ReturnOp::create(*builder, merged_cluster->getLoc(),\n+                                    merged_cluster_output_values);\n \n   // Make sure to replace usages of tf_device.cluster ops to be merged-away with\n   // newly created tf_device.cluster op."
        },
        {
            "sha": "09b53ae4b72895d5f3b3027033d2ac365e34f876",
            "filename": "tensorflow/dtensor/mlir/dtensor_allreduce_combine_optimization.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_combine_optimization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_combine_optimization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_combine_optimization.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -151,10 +151,10 @@ mlir::LogicalResult MergeAllReduceGroup(\n   mlir::Location loc = all_reduce_group[0].getLoc();\n   mlir::Type elem_type = all_reduce_group[0].getType().getElementType();\n   auto zero_scalar = ops_util::CreateScalarConst(0, builder, loc);\n-  auto zero_scalar_elem_type = builder.create<mlir::TF::CastOp>(\n-      loc, mlir::RankedTensorType::get({}, elem_type), zero_scalar);\n-  auto merged = builder.create<mlir::TF::FillOp>(\n-      loc, ops_util::GetR1Const({total_num_elements}, builder, loc),\n+  auto zero_scalar_elem_type = mlir::TF::CastOp::create(\n+      builder, loc, mlir::RankedTensorType::get({}, elem_type), zero_scalar);\n+  auto merged = mlir::TF::FillOp::create(\n+      builder, loc, ops_util::GetR1Const({total_num_elements}, builder, loc),\n       zero_scalar_elem_type);\n \n   // Store every all-reduce's input at an offset location in the merged tensor,\n@@ -175,23 +175,23 @@ mlir::LogicalResult MergeAllReduceGroup(\n     }\n \n     int num_elements = all_reduce_ranked_type.getNumElements();\n-    auto flattened = builder.create<mlir::TF::ReshapeOp>(\n-        DT_LOC2(loc, \"CombinedReduceFlatten\"), all_reduce.getInput(),\n+    auto flattened = mlir::TF::ReshapeOp::create(\n+        builder, DT_LOC2(loc, \"CombinedReduceFlatten\"), all_reduce.getInput(),\n         ops_util::GetR1Const({num_elements}, builder, loc));\n     flattened_types.push_back(flattened.getType());\n     auto indices = ops_util::GetR1Const({offset_num_elements}, builder, loc);\n \n     if (all_reduce.getDeviceType().contains(\"TPU\")) {\n-      updated = builder.create<mlir::TF::XlaDynamicUpdateSliceOp>(\n-          DT_LOC2(loc, \"CombinedReduceUpdateSlice\"), merged.getType(),\n+      updated = mlir::TF::XlaDynamicUpdateSliceOp::create(\n+          builder, DT_LOC2(loc, \"CombinedReduceUpdateSlice\"), merged.getType(),\n           /*input=*/i == 0 ? merged.getResult() : updated,\n           /*update=*/flattened, indices);\n     } else {\n       auto end = ops_util::GetR1Const({offset_num_elements + num_elements},\n                                       builder, loc);\n       auto strides = ops_util::GetR1Const({1}, builder, loc);\n-      updated = builder.create<mlir::TF::TensorStridedSliceUpdateOp>(\n-          DT_LOC2(loc, \"CombinedReduceUpdateSlice\"), merged.getType(),\n+      updated = mlir::TF::TensorStridedSliceUpdateOp::create(\n+          builder, DT_LOC2(loc, \"CombinedReduceUpdateSlice\"), merged.getType(),\n           /*input=*/i == 0 ? merged.getResult() : updated, indices, end,\n           strides,\n           /*value=*/flattened);\n@@ -200,8 +200,8 @@ mlir::LogicalResult MergeAllReduceGroup(\n   }\n \n   // All-reduce the updated merged tensor.\n-  auto merged_all_reduce = builder.create<mlir::TF::DTensorAllReduceOp>(\n-      all_reduce_group[0].getLoc(), updated.getType(), updated,\n+  auto merged_all_reduce = mlir::TF::DTensorAllReduceOp::create(\n+      builder, all_reduce_group[0].getLoc(), updated.getType(), updated,\n       all_reduce_group[0].getGroupAssignment(),\n       all_reduce_group[0].getReduceOp(), all_reduce_group[0].getDeviceType());\n   SetSingleLayoutOnOp(\n@@ -223,13 +223,13 @@ mlir::LogicalResult MergeAllReduceGroup(\n           all_reduce_ranked_type));\n     }\n     int num_elements = all_reduce_ranked_type.getNumElements();\n-    auto slice = builder.create<mlir::TF::SliceOp>(\n-        DT_LOC2(loc, \"PostCombinedReduceSlice\"), flattened_types[i],\n+    auto slice = mlir::TF::SliceOp::create(\n+        builder, DT_LOC2(loc, \"PostCombinedReduceSlice\"), flattened_types[i],\n         /*input=*/merged_all_reduce,\n         /*begin=*/ops_util::GetR1Const({offset_num_elements}, builder, loc),\n         /*size=*/ops_util::GetR1Const({num_elements}, builder, loc));\n-    auto replacement = builder.create<mlir::TF::ReshapeOp>(\n-        DT_LOC2(loc, \"PostCombinedReduceReshape\"), slice.getResult(),\n+    auto replacement = mlir::TF::ReshapeOp::create(\n+        builder, DT_LOC2(loc, \"PostCombinedReduceReshape\"), slice.getResult(),\n         ops_util::GetR1Const(all_reduce_shapes[i], builder, loc));\n     replacements.push_back(replacement);\n     offset_num_elements += num_elements;"
        },
        {
            "sha": "5721d03ce2c343c9c568479251bf347dcb7ff9fb",
            "filename": "tensorflow/dtensor/mlir/dtensor_allreduce_scatter_optimization.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_scatter_optimization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_scatter_optimization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_scatter_optimization.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -137,14 +137,14 @@ mlir::LogicalResult ApplyOptimization(mlir::func::FuncOp function) {\n         VLOG(2) << \"Fuse reduce scatter with scatter_dim: \" << scatter_dim;\n \n         mlir::OpBuilder builder(all_reduce);\n-        auto scatter_dim_const_op = builder.create<mlir::TF::ConstOp>(\n-            all_reduce.getLoc(),\n+        auto scatter_dim_const_op = mlir::TF::ConstOp::create(\n+            builder, all_reduce.getLoc(),\n             mlir::DenseIntElementsAttr::get(\n                 mlir::RankedTensorType::get({}, builder.getI32Type()),\n                 {scatter_dim}));\n \n-        auto reduce_scatter = builder.create<mlir::TF::DTensorReduceScatterOp>(\n-            all_reduce.getLoc(), all_scatter->getResultTypes(),\n+        auto reduce_scatter = mlir::TF::DTensorReduceScatterOp::create(\n+            builder, all_reduce.getLoc(), all_scatter->getResultTypes(),\n             all_reduce.getOperand(0), all_reduce.getGroupAssignment(),\n             scatter_dim_const_op, all_reduce.getReduceOp(),\n             all_reduce.getDeviceType());"
        },
        {
            "sha": "e8a2fde042ae62d953eeaa007eb98a5d31a29efd",
            "filename": "tensorflow/dtensor/mlir/dtensor_allreduce_sum_optimization.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_sum_optimization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_sum_optimization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_allreduce_sum_optimization.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -160,8 +160,8 @@ mlir::LogicalResult OptimizeAllReduceAndSum(mlir::Operation* op,\n   mlir::OpBuilder builder(op);\n   builder.setInsertionPointAfterValue(op->getResult(0));\n   mlir::TF::DTensorAllReduceOp all_reduce =\n-      builder.create<mlir::TF::DTensorAllReduceOp>(\n-          op->getLoc(), op->getResult(0).getType(), op->getResult(0),\n+      mlir::TF::DTensorAllReduceOp::create(\n+          builder, op->getLoc(), op->getResult(0).getType(), op->getResult(0),\n           group_assignment, builder.getStringAttr(std::string(kReduceOpAdd)),\n           builder.getStringAttr(first_reduction_op.getDeviceType()));\n \n@@ -394,8 +394,8 @@ mlir::LogicalResult ExtractAllReduceFromWhileOp(\n \n   // Create a singe reduction operation that reduces the result of the locally\n   // added tensor.\n-  auto new_all_reduce = builder.create<mlir::TF::DTensorAllReduceOp>(\n-      all_reduce.getLoc(), while_output.getType(), while_output,\n+  auto new_all_reduce = mlir::TF::DTensorAllReduceOp::create(\n+      builder, all_reduce.getLoc(), while_output.getType(), while_output,\n       cloned_group_assignment->getResult(0),\n       builder.getStringAttr(std::string(kReduceOpAdd)),\n       builder.getStringAttr(all_reduce.getDeviceType()));"
        },
        {
            "sha": "457cec03a0e1ca47df71bcdfae9cfadcc8fc9a2c",
            "filename": "tensorflow/dtensor/mlir/dtensor_layout_to_xla_sharding_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_layout_to_xla_sharding_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_layout_to_xla_sharding_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_layout_to_xla_sharding_op.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -112,8 +112,8 @@ void DTensorLayoutToXlaShardingOpPass::runOnOperation() {\n         // the V1 sharding attr, so set V2 sharding to \"\" here. It may be better\n         // to set the V2 sharding attr here and then removed it when V1 is\n         // removed.\n-        auto sharding_op = builder.create<mlir::TF::XlaShardingOp>(\n-            layout_op.getLoc(), layout_op.getOutput().getType(),\n+        auto sharding_op = mlir::TF::XlaShardingOp::create(\n+            builder, layout_op.getLoc(), layout_op.getOutput().getType(),\n             layout_op.getInput(),\n             /*sharding=*/builder.getStringAttr(\"\"),  // Not used by tf2xla.\n             /*_xlaSharding=*/sharding_attr,"
        },
        {
            "sha": "c0f066483451fe38c1bf06eebb273f724c524a6f",
            "filename": "tensorflow/dtensor/mlir/dtensor_mixed_precision_reduce.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_mixed_precision_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_mixed_precision_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_mixed_precision_reduce.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -98,16 +98,16 @@ mlir::LogicalResult MaybeUpcastForReduction(ReduceOpType reduce_op,\n   const mlir::RankedTensorType& output_type =\n       mlir::dyn_cast<mlir::RankedTensorType>(reduce_op.getOutput().getType());\n \n-  mlir::TF::CastOp upcast = builder.create<mlir::TF::CastOp>(\n-      loc,\n+  mlir::TF::CastOp upcast = mlir::TF::CastOp::create(\n+      builder, loc,\n       mlir::RankedTensorType::get(input_type.getShape(), builder.getF32Type()),\n       reduce_op.getInput());\n   reduce_op->setOperand(0, upcast.getY());\n   reduce_op.getOutput().setType(upcast.getY().getType());\n \n   builder.setInsertionPointAfter(reduce_op);\n-  mlir::TF::CastOp downcast = builder.create<mlir::TF::CastOp>(\n-      loc,\n+  mlir::TF::CastOp downcast = mlir::TF::CastOp::create(\n+      builder, loc,\n       mlir::RankedTensorType::get(output_type.getShape(),\n                                   output_type.getElementType()),\n       reduce_op);"
        },
        {
            "sha": "1b320bcfc100ab2c10eb315eb86b72b06f534514",
            "filename": "tensorflow/dtensor/mlir/dtensor_replace_relayout_with_identity.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_replace_relayout_with_identity.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_replace_relayout_with_identity.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_replace_relayout_with_identity.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -37,9 +37,9 @@ class DTensorReplaceRelayoutWithIdentityPass\n       mlir::OpBuilder builder(relayout_op);\n       // Inserts an IdentityOp at the position of the relayout_op with the same\n       // attributes as the relayout_op.\n-      auto new_identity = builder.create<mlir::TF::IdentityOp>(\n-          relayout_op->getLoc(), relayout_op.getType(), relayout_op.getInput(),\n-          relayout_op->getAttrs());\n+      auto new_identity = mlir::TF::IdentityOp::create(\n+          builder, relayout_op->getLoc(), relayout_op.getType(),\n+          relayout_op.getInput(), relayout_op->getAttrs());\n       relayout_op.getOutput().replaceAllUsesWith(new_identity.getOutput());\n       relayout_op.erase();\n     });"
        },
        {
            "sha": "fa6d2bd041189f876f5ebf00b57be3ba102702bd",
            "filename": "tensorflow/dtensor/mlir/dtensor_send_recv.cc",
            "status": "modified",
            "additions": 66,
            "deletions": 66,
            "changes": 132,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_send_recv.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_send_recv.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fdtensor_send_recv.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -85,8 +85,8 @@ mlir::Value GetOrCreateCompilationKey(mlir::Operation* op) {\n   auto result_type =\n       mlir::RankedTensorType::get({3}, builder.getType<mlir::TF::StringType>());\n   auto new_compilation_key =\n-      builder.create<mlir::TF::_XlaCompileMlirPlaceholderProgramKeyOp>(\n-          cluster.getLoc(), /*program=*/result_type,\n+      mlir::TF::_XlaCompileMlirPlaceholderProgramKeyOp::create(\n+          builder, cluster.getLoc(), /*program=*/result_type,\n           llvm::ArrayRef<mlir::Value>{});\n   return new_compilation_key.getProgram();\n }\n@@ -107,8 +107,8 @@ StatusOr<mlir::Value> GetDeviceOrdinal(const Mesh& mesh,\n   }\n   // Slice out the device ordinal using the device ID as index.\n   TF_ASSIGN_OR_RETURN(mlir::Value device_id, DeviceId(function));\n-  mlir::TF::SliceOp device_ordinal = builder->create<mlir::TF::SliceOp>(\n-      loc,\n+  mlir::TF::SliceOp device_ordinal = mlir::TF::SliceOp::create(\n+      *builder, loc,\n       /*output=*/EffectivelyScalarR1Type(builder->getIntegerType(32)),\n       /*input=*/IntConst(*builder, loc, device_id_to_ordinal),\n       /*begin=*/\n@@ -118,8 +118,8 @@ StatusOr<mlir::Value> GetDeviceOrdinal(const Mesh& mesh,\n   mlir::Value device_ordinal_scalar =\n       ReshapeSizeTypeToScalar(*builder, loc, device_ordinal);\n   if (return_int64_type) {\n-    device_ordinal_scalar = builder->create<mlir::TF::CastOp>(\n-        loc, mlir::RankedTensorType::get({}, builder->getI64Type()),\n+    device_ordinal_scalar = mlir::TF::CastOp::create(\n+        *builder, loc, mlir::RankedTensorType::get({}, builder->getI64Type()),\n         device_ordinal_scalar);\n   }\n   return device_ordinal_scalar;\n@@ -138,8 +138,8 @@ StatusOr<mlir::Operation*> LowerDTensorSendToTFOp(\n   absl::Span<const std::string> receiving_devices = target_mesh.local_devices();\n \n   mlir::Operation* lowered_send_op;\n-  lowered_send_op = builder.create<mlir::TF::_HostSendOp>(\n-      send_input.getLoc(), send_input, tensor_name, sending_devices[0],\n+  lowered_send_op = mlir::TF::_HostSendOp::create(\n+      builder, send_input.getLoc(), send_input, tensor_name, sending_devices[0],\n       /*send_device_incarnation=*/0, receiving_devices[0],\n       /*client_terminated=*/false);\n \n@@ -184,12 +184,13 @@ StatusOr<mlir::Operation*> LowerDTensorSendToXlaOp(\n           GetDeviceOrdinal(send_input_layout.mesh(), loc, send_func, &builder));\n     }\n     // Create XlaSendFromHostV2 op\n-    lowered_send_op = builder.create<mlir::TF::_XlaSendFromHostV2Op>(\n-        loc, value_to_send, program_key, device_ordinal, dtensor_send.getKey());\n+    lowered_send_op = mlir::TF::_XlaSendFromHostV2Op::create(\n+        builder, loc, value_to_send, program_key, device_ordinal,\n+        dtensor_send.getKey());\n   } else {\n     // Note that for ops running in XLA/TPU, device ordinal input is not needed.\n-    lowered_send_op = builder.create<mlir::TF::XlaSendToHostOp>(\n-        loc, send_input, dtensor_send.getKey());\n+    lowered_send_op = mlir::TF::XlaSendToHostOp::create(\n+        builder, loc, send_input, dtensor_send.getKey());\n   }\n \n   dtensor_send.erase();\n@@ -246,16 +247,16 @@ StatusOr<mlir::Operation*> LowerDTensorRecvToXlaOp(\n \n     auto program_key = GetOrCreateCompilationKey(dtensor_recv);\n     builder.setInsertionPoint(dtensor_recv);\n-    recv_xla_op = builder.create<mlir::TF::_XlaRecvAtHostV2Op>(\n-        dtensor_recv.getLoc(), output_types,\n+    recv_xla_op = mlir::TF::_XlaRecvAtHostV2Op::create(\n+        builder, dtensor_recv.getLoc(), output_types,\n         /*dynamic_key=*/program_key, device_ordinal, dtensor_recv.getKeyAttr());\n   } else {\n     TF_ASSIGN_OR_RETURN(auto local_shape_attr,\n                         GetDTensorRecvLocalShapeAttr(dtensor_recv));\n \n     // Create XlaRecvFromHost op.\n-    recv_xla_op = builder.create<mlir::TF::XlaRecvFromHostOp>(\n-        dtensor_recv.getLoc(), output_type, local_shape_attr,\n+    recv_xla_op = mlir::TF::XlaRecvFromHostOp::create(\n+        builder, dtensor_recv.getLoc(), output_type, local_shape_attr,\n         dtensor_recv.getKeyAttr());\n   }\n \n@@ -299,8 +300,8 @@ StatusOr<mlir::Operation*> LowerDTensorSendFromCPUToTFOp(\n \n   mlir::Operation* lowered_send_op;\n   for (size_t i = 0; i < receiving_devices.size(); ++i)\n-    lowered_send_op = builder.create<mlir::TF::_HostSendOp>(\n-        send_input.getLoc(), dtensor_send.getInput(), tensor_name,\n+    lowered_send_op = mlir::TF::_HostSendOp::create(\n+        builder, send_input.getLoc(), dtensor_send.getInput(), tensor_name,\n         sending_devices[0],\n         /*send_device_incarnation=*/0, receiving_devices[i]);\n \n@@ -326,8 +327,8 @@ StatusOr<mlir::Operation*> LowerDTensorRecvFromCPUToTFOp(\n   mlir::Operation* lowered_recv_op;\n   mlir::Location loc = dtensor_recv.getLoc();\n   for (size_t i = 0; i < receiving_devices.size(); ++i)\n-    lowered_recv_op = builder.create<mlir::TF::_HostRecvOp>(\n-        loc, dtensor_recv.getType(), tensor_name, sending_devices[0],\n+    lowered_recv_op = mlir::TF::_HostRecvOp::create(\n+        builder, loc, dtensor_recv.getType(), tensor_name, sending_devices[0],\n         /*send_device_incarnation=*/0, receiving_devices[i]);\n \n   // Replace dtensor_recv with newly created recv op and remove DTensorRecv op.\n@@ -351,8 +352,8 @@ StatusOr<mlir::Operation*> LowerDTensorRecvToTFOp(\n   absl::Span<const std::string> receiving_devices = recv_mesh.local_devices();\n \n   mlir::Location loc = dtensor_recv.getLoc();\n-  mlir::Operation* lowered_recv_op = builder.create<mlir::TF::_HostRecvOp>(\n-      loc, output_type, tensor_name, sending_devices[0],\n+  mlir::Operation* lowered_recv_op = mlir::TF::_HostRecvOp::create(\n+      builder, loc, output_type, tensor_name, sending_devices[0],\n       /*send_device_incarnation=*/0, receiving_devices[0]);\n \n   return lowered_recv_op;\n@@ -385,7 +386,7 @@ llvm::SmallVector<mlir::Attribute, 4> GenerateBranches(\n                                   ? func_op.getArgument(0)\n                                   : mlir::BlockArgument{};\n     auto branch_op = fn(fn_builder, location, arg, it.value());\n-    fn_builder.create<mlir::func::ReturnOp>(location, branch_op->getResults());\n+    mlir::func::ReturnOp::create(fn_builder, location, branch_op->getResults());\n \n     branches.push_back(mlir::SymbolRefAttr::get(func_op));\n   }\n@@ -429,25 +430,24 @@ StatusOr<mlir::Operation*> LowerOneToOneDTensorSendToTFHostSend(\n         mlir::Value val = arg;\n         if (i32_copy) {\n           auto val_type = mlir::cast<mlir::TensorType>(val.getType());\n-          val = op_builder\n-                    .create<mlir::TF::CastOp>(\n-                        loc,\n-                        mlir::RankedTensorType::get(\n-                            val_type.getShape(), op_builder.getIntegerType(64)),\n-                        val)\n+          val = mlir::TF::CastOp::create(\n+                    op_builder, loc,\n+                    mlir::RankedTensorType::get(val_type.getShape(),\n+                                                op_builder.getIntegerType(64)),\n+                    val)\n                     ->getResult(0);\n         }\n-        return op_builder.create<mlir::TF::_HostSendOp>(\n-            loc, val, tensor_name, std::get<0>(device_pair),\n+        return mlir::TF::_HostSendOp::create(\n+            op_builder, loc, val, tensor_name, std::get<0>(device_pair),\n             /*send_device_incarnation=*/0, std::get<1>(device_pair));\n       });\n-  mlir::Operation* case_op = builder.create<mlir::TF::CaseOp>(\n-      dtensor_send.getLoc(),\n-      /*output=*/llvm::ArrayRef<mlir::Type>{},\n-      /*branch_index=*/device_ordinal,\n-      /*input=*/dtensor_send->getOperands(),\n-      /*branches=*/builder.getArrayAttr(branches),\n-      /*is_stateless=*/builder.getBoolAttr(false));\n+  mlir::Operation* case_op =\n+      mlir::TF::CaseOp::create(builder, dtensor_send.getLoc(),\n+                               /*output=*/llvm::ArrayRef<mlir::Type>{},\n+                               /*branch_index=*/device_ordinal,\n+                               /*input=*/dtensor_send->getOperands(),\n+                               /*branches=*/builder.getArrayAttr(branches),\n+                               /*is_stateless=*/builder.getBoolAttr(false));\n \n   // erase the send op here iff targeting a gpu\n   // otherwise there will be 'op not within cluster' error(s)\n@@ -494,14 +494,15 @@ StatusOr<mlir::Operation*> LowerOneToOneDTensorRecvToTFHostRecv(\n       \"{0}_receive_{1}_{2}\", device_pairs,\n       [&](mlir::OpBuilder& op_builder, auto& loc, auto _,\n           auto device_pair) -> mlir::Operation* {\n-        auto recv_op = op_builder.create<mlir::TF::_HostRecvOp>(\n-            loc, local_output_type, tensor_name, std::get<0>(device_pair),\n+        auto recv_op = mlir::TF::_HostRecvOp::create(\n+            op_builder, loc, local_output_type, tensor_name,\n+            std::get<0>(device_pair),\n             /*send_device_incarnation=*/0, std::get<1>(device_pair));\n         SetSingleLayoutOnOp(recv_op, recv_layout);\n         return recv_op;\n       });\n-  mlir::Operation* case_op = builder.create<mlir::TF::CaseOp>(\n-      dtensor_recv.getLoc(),\n+  mlir::Operation* case_op = mlir::TF::CaseOp::create(\n+      builder, dtensor_recv.getLoc(),\n       /*output=*/llvm::ArrayRef<mlir::Type>{local_output_type},\n       /*branch_index=*/device_ordinal,\n       /*input=*/dtensor_recv->getOperands(),\n@@ -510,8 +511,8 @@ StatusOr<mlir::Operation*> LowerOneToOneDTensorRecvToTFHostRecv(\n \n   mlir::Operation* lowered_recv;\n   if (i32_copy) {\n-    lowered_recv = builder.create<mlir::TF::CastOp>(\n-        dtensor_recv.getLoc(), local_recv_type, case_op->getResult(0));\n+    lowered_recv = mlir::TF::CastOp::create(\n+        builder, dtensor_recv.getLoc(), local_recv_type, case_op->getResult(0));\n   } else {\n     lowered_recv = case_op;\n   }\n@@ -639,12 +640,12 @@ StatusOr<mlir::Operation*> LowerDTensorSend(mlir::Operation* send_op,\n         GetDeviceOrdinal(*mesh, loc,\n                          send_cluster->getParentOfType<mlir::func::FuncOp>(),\n                          &builder));\n-    mlir::Value predicate = builder.create<mlir::TF::EqualOp>(\n-        loc, device_ordinal, CreateIntScalarConst(0, builder, loc),\n+    mlir::Value predicate = mlir::TF::EqualOp::create(\n+        builder, loc, device_ordinal, CreateIntScalarConst(0, builder, loc),\n         /*incompatible_shape_error=*/builder.getBoolAttr(true));\n \n-    auto send_if = builder.create<mlir::TF::IfRegionOp>(\n-        loc, llvm::SmallVector<mlir::Type, 4>{}, predicate,\n+    auto send_if = mlir::TF::IfRegionOp::create(\n+        builder, loc, llvm::SmallVector<mlir::Type, 4>{}, predicate,\n         /*is_stateless=*/builder.getBoolAttr(true),\n         GetUniqueControlflowFnName(\"copy_to_mesh_send_if_then\", builder),\n         GetUniqueControlflowFnName(\"copy_to_mesh_send_if_else\", builder));\n@@ -653,16 +654,15 @@ StatusOr<mlir::Operation*> LowerDTensorSend(mlir::Operation* send_op,\n     auto& else_branch = send_if.getElseBranch();\n     else_branch.push_back(new mlir::Block);\n     builder.setInsertionPointToEnd(&else_branch.front());\n-    builder.create<mlir::TF::YieldOp>(\n-        loc,\n-        /*operands=*/llvm::ArrayRef<mlir::Value>{});\n+    mlir::TF::YieldOp::create(builder, loc,\n+                              /*operands=*/llvm::ArrayRef<mlir::Value>{});\n \n     // Create then branch region with DTensorSend op.\n     auto& then_branch = send_if.getThenBranch();\n     then_branch.push_back(new mlir::Block);\n     builder.setInsertionPointToEnd(&then_branch.front());\n-    auto yield = builder.create<mlir::TF::YieldOp>(\n-        loc, /*operands=*/llvm::ArrayRef<mlir::Value>{});\n+    auto yield = mlir::TF::YieldOp::create(\n+        builder, loc, /*operands=*/llvm::ArrayRef<mlir::Value>{});\n     dtensor_send->moveBefore(yield);\n \n     // Lower DTensorSend op to actual TF op.\n@@ -684,8 +684,8 @@ StatusOr<mlir::Operation*> LowerDTensorSend(mlir::Operation* send_op,\n       if (!recv_mesh.is_cpu_mesh() &&\n           send_type.getElementType().isInteger(32)) {\n         builder.setInsertionPointAfter(send_input.getDefiningOp());\n-        auto cast_to_int64 = builder.create<mlir::TF::CastOp>(\n-            send_input.getLoc(),\n+        auto cast_to_int64 = mlir::TF::CastOp::create(\n+            builder, send_input.getLoc(),\n             mlir::RankedTensorType::get(send_type.getShape(),\n                                         builder.getIntegerType(64)),\n             send_input);\n@@ -781,8 +781,8 @@ StatusOr<mlir::Operation*> LowerDTensorRecv(mlir::Operation* send_op,\n         GetDeviceOrdinal(recv_mesh, loc,\n                          recv_cluster->getParentOfType<mlir::func::FuncOp>(),\n                          &builder));\n-    mlir::Value predicate = builder.create<mlir::TF::EqualOp>(\n-        loc, device_ordinal, CreateIntScalarConst(0, builder, loc),\n+    mlir::Value predicate = mlir::TF::EqualOp::create(\n+        builder, loc, device_ordinal, CreateIntScalarConst(0, builder, loc),\n         /*incompatible_shape_error=*/builder.getBoolAttr(true));\n \n     mlir::TensorType recv_type = dtensor_recv.getType();\n@@ -795,8 +795,8 @@ StatusOr<mlir::Operation*> LowerDTensorRecv(mlir::Operation* send_op,\n                                           builder.getIntegerType(64))\n             : recv_type;\n \n-    auto recv_if = builder.create<mlir::TF::IfRegionOp>(\n-        loc, llvm::SmallVector<mlir::Type, 4>{output_type}, predicate,\n+    auto recv_if = mlir::TF::IfRegionOp::create(\n+        builder, loc, llvm::SmallVector<mlir::Type, 4>{output_type}, predicate,\n         /*is_stateless=*/builder.getBoolAttr(true),\n         GetUniqueControlflowFnName(\"copy_to_mesh_recv_if_then\", builder),\n         GetUniqueControlflowFnName(\"copy_to_mesh_recv_if_else\", builder));\n@@ -831,9 +831,9 @@ StatusOr<mlir::Operation*> LowerDTensorRecv(mlir::Operation* send_op,\n       return absl::InvalidArgumentError(\"unsupported output type\");\n     }\n \n-    mlir::Value zeros = builder.create<mlir::TF::ConstOp>(loc, const_attr);\n-    builder.create<mlir::TF::YieldOp>(\n-        loc, /*operands=*/llvm::ArrayRef<mlir::Value>{zeros});\n+    mlir::Value zeros = mlir::TF::ConstOp::create(builder, loc, const_attr);\n+    mlir::TF::YieldOp::create(builder, loc,\n+                              /*operands=*/llvm::ArrayRef<mlir::Value>{zeros});\n \n     // Create then branch region with DTensorRecv op.\n     auto& then_branch = recv_if.getThenBranch();\n@@ -843,8 +843,8 @@ StatusOr<mlir::Operation*> LowerDTensorRecv(mlir::Operation* send_op,\n \n     TF_ASSIGN_OR_RETURN(mlir::Operation * xla_recv,\n                         lower_fn(send_mesh, dtensor_recv, output_type));\n-    builder.create<mlir::TF::YieldOp>(\n-        loc,\n+    mlir::TF::YieldOp::create(\n+        builder, loc,\n         /*operands=*/llvm::ArrayRef<mlir::Value>{xla_recv->getResult(0)});\n \n     // Broadcast the received output to all GPU/TPU devices.\n@@ -859,8 +859,8 @@ StatusOr<mlir::Operation*> LowerDTensorRecv(mlir::Operation* send_op,\n                                     kReduceOpAdd));\n \n     if (need_i32_to_i64_upcast) {\n-      lowered_recv = builder.create<mlir::TF::CastOp>(\n-          loc, recv_type, lowered_recv->getResult(0));\n+      lowered_recv = mlir::TF::CastOp::create(builder, loc, recv_type,\n+                                              lowered_recv->getResult(0));\n     }\n \n     // Replaces usages of DTensorRecv op with the broadcasted value."
        },
        {
            "sha": "10b6296d5638b6cd3e0deffe2e4edc86cbf1395d",
            "filename": "tensorflow/dtensor/mlir/expansions/argmax_spmd_expander.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fargmax_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fargmax_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fargmax_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -106,9 +106,9 @@ StatusOr<mlir::Operation*> ArgMaxSPMDExpander::ExpandOp(mlir::Operation* op) {\n     }\n   }\n \n-  auto new_argmax = builder.create<mlir::TF::ArgMaxOp>(\n-      argmax_op.getLoc(), argmax_op.getResult().getType(), input,\n-      argmax_op.getDimension());\n+  auto new_argmax = mlir::TF::ArgMaxOp::create(builder, argmax_op.getLoc(),\n+                                               argmax_op.getResult().getType(),\n+                                               input, argmax_op.getDimension());\n   op->getResult(0).replaceAllUsesWith(new_argmax.getOutput());\n   op->erase();\n "
        },
        {
            "sha": "6fb9cb790910ed49f30810070d4015cb97917352",
            "filename": "tensorflow/dtensor/mlir/expansions/dataparallel_spmd_expander.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fdataparallel_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fdataparallel_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fdataparallel_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -257,8 +257,8 @@ StatusOr<mlir::Operation*> DataparallelSPMDExpander::RelayoutOperandsAndOutputs(\n   builder.setInsertionPointAfter(last_op_after_splitting);\n \n   // Tie all outputs together with identity_n\n-  auto identity_op = builder.create<mlir::TF::IdentityNOp>(\n-      op->getLoc(), generated_types, generated_outputs);\n+  auto identity_op = mlir::TF::IdentityNOp::create(\n+      builder, op->getLoc(), generated_types, generated_outputs);\n   newly_created_ops.insert(identity_op);\n   for (int i = 0; i < output_layouts.size(); ++i) {\n     op->getOpResult(i).replaceAllUsesExcept(identity_op.getResult(i),"
        },
        {
            "sha": "7de31a8bb7e5f1ed228b0150cb5ea7138282a88f",
            "filename": "tensorflow/dtensor/mlir/expansions/io_op_spmd_expander.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fio_op_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fio_op_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fio_op_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -83,8 +83,8 @@ StatusOr<mlir::Operation*> Expand(mlir::Operation* op) {\n   mlir::Block* then_fn_block = then_func.addEntryBlock();\n   mlir::OpBuilder then_fn_builder =\n       mlir::OpBuilder::atBlockBegin(then_fn_block);\n-  then_fn_builder.create<mlir::TF::NoOp>(location);\n-  then_fn_builder.create<mlir::func::ReturnOp>(location);\n+  mlir::TF::NoOp::create(then_fn_builder, location);\n+  mlir::func::ReturnOp::create(then_fn_builder, location);\n \n   // Build else_func that is the branch of device_id == 0.\n   // The else func is just the original op.\n@@ -100,9 +100,9 @@ StatusOr<mlir::Operation*> Expand(mlir::Operation* op) {\n   mlir::OpBuilder else_fn_builder =\n       mlir::OpBuilder::atBlockBegin(else_fn_block);\n \n-  else_fn_builder.create<T>(location, op->getResultTypes(),\n-                            else_fn_block->getArguments());\n-  else_fn_builder.create<mlir::func::ReturnOp>(location);\n+  T::create(else_fn_builder, location, op->getResultTypes(),\n+            else_fn_block->getArguments());\n+  mlir::func::ReturnOp::create(else_fn_builder, location);\n \n   symbol_table.insert(then_func);\n   symbol_table.insert(else_func);\n@@ -115,12 +115,12 @@ StatusOr<mlir::Operation*> Expand(mlir::Operation* op) {\n           builder, location,\n           mlir::cast<mlir::TensorType>(device_id.getType()).getElementType()));\n \n-  mlir::TF::NotEqualOp not_equal = builder.create<mlir::TF::NotEqualOp>(\n-      location, device_id, zero_scalar,\n+  mlir::TF::NotEqualOp not_equal = mlir::TF::NotEqualOp::create(\n+      builder, location, device_id, zero_scalar,\n       /*incompatible_shape_error=*/builder.getBoolAttr(false));\n \n-  mlir::Operation* if_op = builder.create<mlir::TF::IfOp>(\n-      location, then_func.getFunctionType().getResults(),\n+  mlir::Operation* if_op = mlir::TF::IfOp::create(\n+      builder, location, then_func.getFunctionType().getResults(),\n       /*cond=*/not_equal.getResult(),\n       /*input=*/op->getOperands(),\n       /*then_branch=*/then_func.getSymName(),"
        },
        {
            "sha": "0bd4da477d2205cd233f6bb1d6d4e75ae7de2f77",
            "filename": "tensorflow/dtensor/mlir/expansions/iterator_spmd_expander.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fiterator_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fiterator_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fiterator_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -62,8 +62,8 @@ StatusOr<mlir::Operation*> IteratorGetNextSPMDExpander::ExpandOp(\n         local_shape, global_output_type.getElementType());\n   }\n \n-  auto new_op = builder.create<mlir::TF::IteratorGetNextOp>(\n-      DT_LOC(op->getLoc()), local_types, original_op->getOperand(0));\n+  auto new_op = mlir::TF::IteratorGetNextOp::create(\n+      builder, DT_LOC(op->getLoc()), local_types, original_op->getOperand(0));\n \n   for (int i = 0; i < original_op->getNumResults(); ++i) {\n     original_op.getResult(i).replaceAllUsesWith(new_op.getResult(i));"
        },
        {
            "sha": "b2d6ca37777281e1e890bef834cafbe988406b30",
            "filename": "tensorflow/dtensor/mlir/expansions/meta_spmd_expander.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fmeta_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fmeta_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fmeta_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -791,7 +791,7 @@ StatusOr<mlir::Operation*> ReshapeSPMDExpander::ExpandOp(mlir::Operation* op) {\n   auto const_attr =\n       mlir::DenseIntElementsAttr::get(new_shape, local_reshape_const);\n   auto new_reshape_const_op =\n-      builder.create<mlir::TF::ConstOp>(DT_LOC(op), const_attr);\n+      mlir::TF::ConstOp::create(builder, DT_LOC(op), const_attr);\n   mlir::TF::ReshapeOp new_reshape_op = mlir::TF::ReshapeOp::create(\n       builder, op->getLoc(), new_input, new_reshape_const_op);\n "
        },
        {
            "sha": "a45a2df40a32e4c3e414c8c8196a0f048515eeb9",
            "filename": "tensorflow/dtensor/mlir/expansions/optional_spmd_expander.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Foptional_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Foptional_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Foptional_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -54,8 +54,8 @@ StatusOr<mlir::Operation*> OptionalGetValueSPMDExpander::ExpandOp(\n     local_types[i] = local_type;\n   }\n \n-  auto new_op = builder.create<mlir::TF::OptionalGetValueOp>(\n-      DT_LOC(op->getLoc()), local_types, original_op->getOperand(0));\n+  auto new_op = mlir::TF::OptionalGetValueOp::create(\n+      builder, DT_LOC(op->getLoc()), local_types, original_op->getOperand(0));\n \n   for (int i = 0; i < original_op->getNumResults(); ++i) {\n     original_op.getResult(i).replaceAllUsesWith(new_op.getResult(i));"
        },
        {
            "sha": "6175e133710f7a2091dc12567d2a2d13e5e020bd",
            "filename": "tensorflow/dtensor/mlir/expansions/random_op_spmd_expander.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 20,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Frandom_op_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Frandom_op_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Frandom_op_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -151,26 +151,26 @@ StatusOr<mlir::Value> GetDeviceSeed(const Layout& layout, mlir::Operation* op) {\n   mlir::Attribute const_attr =\n       mlir::DenseIntElementsAttr::get(const_type, multipliers);\n   mlir::Value multiplier =\n-      builder.create<mlir::TF::ConstOp>(cluster.getLoc(), const_attr)\n+      mlir::TF::ConstOp::create(builder, cluster.getLoc(), const_attr)\n           .getOutput();\n \n   const mlir::RankedTensorType one_by_one =\n       mlir::RankedTensorType::get({1, 1}, builder.getIntegerType(32));\n \n-  mlir::Value seed = builder.create<mlir::TF::MatMulOp>(\n-      cluster.getLoc(), one_by_one, mesh_coordinates, multiplier);\n+  mlir::Value seed = mlir::TF::MatMulOp::create(\n+      builder, cluster.getLoc(), one_by_one, mesh_coordinates, multiplier);\n \n   // Largest prime in 16 bits.\n   mlir::Value prime = CreateIntScalarConst(\n       /*value=*/65521, builder, cluster.getLoc(), /*use_int64=*/false);\n \n   mlir::Value seed_plus_prime =\n-      builder\n-          .create<mlir::TF::AddV2Op>(cluster.getLoc(), one_by_one, seed, prime)\n+      mlir::TF::AddV2Op::create(builder, cluster.getLoc(), one_by_one, seed,\n+                                prime)\n           .getZ();\n \n-  mlir::TF::SqueezeOp squeeze = builder.create<mlir::TF::SqueezeOp>(\n-      cluster.getLoc(),\n+  mlir::TF::SqueezeOp squeeze = mlir::TF::SqueezeOp::create(\n+      builder, cluster.getLoc(),\n       mlir::RankedTensorType::get({}, builder.getIntegerType(32)),\n       seed_plus_prime, builder.getI64ArrayAttr({0, 1}));\n \n@@ -207,11 +207,12 @@ StatusOr<mlir::Value> ComputeNewSeed(mlir::OpBuilder& builder,\n   mlir::Type seed_type =\n       mlir::cast<mlir::TensorType>(op_seed.getType()).getElementType();\n \n-  device_id_seed = builder.create<mlir::TF::CastOp>(\n-      location, mlir::RankedTensorType::get({}, seed_type), device_id_seed);\n+  device_id_seed = mlir::TF::CastOp::create(\n+      builder, location, mlir::RankedTensorType::get({}, seed_type),\n+      device_id_seed);\n \n-  mlir::Value seed_xor =\n-      builder.create<mlir::TF::BitwiseXorOp>(location, op_seed, device_id_seed);\n+  mlir::Value seed_xor = mlir::TF::BitwiseXorOp::create(\n+      builder, location, op_seed, device_id_seed);\n   return seed_xor;\n }\n \n@@ -240,8 +241,8 @@ StatusOr<mlir::Operation*> CreatedShardedLocalRandomOpV1(const Layout& layout,\n \n   auto new_shape_value = Int64Const(builder, location, new_random_shape);\n   // TODO(zhonglinhan) : check different input for StatelessRandomUniformInt\n-  auto local_random = builder.create<RandomOp>(location, new_random_type,\n-                                               new_shape_value, seed_xor);\n+  auto local_random = RandomOp::create(builder, location, new_random_type,\n+                                       new_shape_value, seed_xor);\n   op->getResult(0).replaceAllUsesWith(local_random.getOutput());\n   op->erase();\n   return local_random.getOperation();\n@@ -272,9 +273,9 @@ StatusOr<mlir::Operation*> CreatedShardedLocalRandomOpV2(const Layout& layout,\n \n   auto new_shape_value = Int64Const(builder, location, new_random_shape);\n \n-  auto local_random = builder.create<RandomOp>(\n-      location, new_random_type, new_shape_value, seed_xor,\n-      random_op.getCounter(), random_op.getAlg());\n+  auto local_random =\n+      RandomOp::create(builder, location, new_random_type, new_shape_value,\n+                       seed_xor, random_op.getCounter(), random_op.getAlg());\n   op->getResult(0).replaceAllUsesWith(local_random.getOutput());\n   op->erase();\n   return local_random.getOperation();\n@@ -305,10 +306,10 @@ StatusOr<mlir::Operation*> CreatedShardedLocalRandomOpV2Range(\n \n   auto new_shape_value = Int64Const(builder, location, new_random_shape);\n \n-  auto local_random = builder.create<RandomOp>(\n-      location, new_random_type, new_shape_value, seed_xor,\n-      random_op.getCounter(), random_op.getAlg(), random_op.getMinval(),\n-      random_op.getMaxval());\n+  auto local_random =\n+      RandomOp::create(builder, location, new_random_type, new_shape_value,\n+                       seed_xor, random_op.getCounter(), random_op.getAlg(),\n+                       random_op.getMinval(), random_op.getMaxval());\n   op->getResult(0).replaceAllUsesWith(local_random.getOutput());\n   op->erase();\n   return local_random.getOperation();"
        },
        {
            "sha": "f55d62efa815018faba0758eb89e7a82f1733c5b",
            "filename": "tensorflow/dtensor/mlir/expansions/replicated_spmd_expander.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Freplicated_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Freplicated_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Freplicated_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -82,8 +82,8 @@ ReplicatedOpSPMDExpander::ReplicatedRelayoutOperandsAndOutputs(\n   builder.setInsertionPointAfter(last_op_after_splitting);\n \n   // Tie all outputs together with identity_n\n-  auto identity_op = builder.create<mlir::TF::IdentityNOp>(\n-      op->getLoc(), generated_types, generated_outputs);\n+  auto identity_op = mlir::TF::IdentityNOp::create(\n+      builder, op->getLoc(), generated_types, generated_outputs);\n   newly_created_ops.insert(identity_op);\n   for (int i = 0; i < output_layouts.size(); ++i) {\n     op->getOpResult(i).replaceAllUsesExcept(identity_op.getResult(i),"
        },
        {
            "sha": "c2fc958965ec33b2c21b4350fb5eb778d002938a",
            "filename": "tensorflow/dtensor/mlir/expansions/segmentation_spmd_expander.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fsegmentation_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fsegmentation_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fsegmentation_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -122,9 +122,9 @@ StatusOr<mlir::Operation*> UnsortedSegmentSumSPMDExpander::ExpandOp(\n       EmitRelayout(segment_ids, segment_ids_layout, new_segment_ids_layout));\n \n   mlir::OpBuilder builder(op);\n-  mlir::Operation* new_sum_op = builder.create<mlir::TF::UnsortedSegmentSumOp>(\n-      op->getLoc(), sum_op.getOutput().getType(), data, new_segment_ids,\n-      sum_op.getNumSegments());\n+  mlir::Operation* new_sum_op = mlir::TF::UnsortedSegmentSumOp::create(\n+      builder, op->getLoc(), sum_op.getOutput().getType(), data,\n+      new_segment_ids, sum_op.getNumSegments());\n \n   InferSPMDExpandedLocalShape(new_sum_op);\n "
        },
        {
            "sha": "4cf10413879cbf9895567b418daccd82e9dbf48b",
            "filename": "tensorflow/dtensor/mlir/expansions/slice_spmd_expander.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fslice_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fslice_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fslice_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -187,11 +187,10 @@ StatusOr<mlir::Operation*> SliceSPMDExpander::ExpandOp(mlir::Operation* op) {\n   else\n     new_size = Int64Const(builder, loc, sizes);\n \n-  auto new_op = builder\n-                    .create<mlir::TF::SliceOp>(\n-                        loc, slice_op.getOutput().getType(), relayout_input,\n-                        slice_op.getBegin(), new_size)\n-                    .getOperation();\n+  auto new_op =\n+      mlir::TF::SliceOp::create(builder, loc, slice_op.getOutput().getType(),\n+                                relayout_input, slice_op.getBegin(), new_size)\n+          .getOperation();\n   new_op = InferSPMDExpandedLocalShape(new_op);\n \n   TF_ASSIGN_OR_RETURN(auto relayout_output,"
        },
        {
            "sha": "62fc9413e783072fba5caa9cbf6f5d8f060eea05",
            "filename": "tensorflow/dtensor/mlir/expansions/softmax_spmd_expander.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 52,
            "changes": 100,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fsoftmax_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fsoftmax_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Fsoftmax_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -70,12 +70,12 @@ StatusOr<mlir::Value> ComputeGlobalReduce(\n \n   // First compute a local reduce\n   if (reduce_op == kReduceOpAdd) {\n-    local_reduce = builder.create<mlir::TF::SumOp>(\n-        input.getLoc(), input, reduction_indices,\n+    local_reduce = mlir::TF::SumOp::create(\n+        builder, input.getLoc(), input, reduction_indices,\n         /*keep_dims=*/builder.getBoolAttr(true));\n   } else if (reduce_op == kReduceOpMax) {\n-    local_reduce = builder.create<mlir::TF::MaxOp>(\n-        input.getLoc(), input, reduction_indices,\n+    local_reduce = mlir::TF::MaxOp::create(\n+        builder, input.getLoc(), input, reduction_indices,\n         /*keep_dims=*/builder.getBoolAttr(true));\n   } else {\n     return errors::Unimplemented(\"reduction \", reduce_op, \" not implemented\");\n@@ -107,8 +107,8 @@ StatusOr<mlir::Value> ComputeGlobalReduce(\n     // dimension attribute type. Everything else is OK with int32_t dimensions.\n     std::vector<int64_t> reduce_dim_array_64(reduced_dims.begin(),\n                                              reduced_dims.end());\n-    global_reduce = builder.create<mlir::TF::SqueezeOp>(\n-        input.getLoc(), new_type, global_reduce->getResult(0),\n+    global_reduce = mlir::TF::SqueezeOp::create(\n+        builder, input.getLoc(), new_type, global_reduce->getResult(0),\n         builder.getI64ArrayAttr(reduce_dim_array_64));\n   }\n   return global_reduce->getResult(0);\n@@ -143,9 +143,9 @@ absl::Status ComputeExpAndSum(mlir::OpBuilder& builder,\n \n   // Subtract max from local copy of logits.\n   shifted_logits =\n-      builder.create<mlir::TF::SubOp>(loc, logits, max_logits).getResult();\n+      mlir::TF::SubOp::create(builder, loc, logits, max_logits).getResult();\n   exp_of_shifted_logits =\n-      builder.create<mlir::TF::ExpOp>(loc, shifted_logits).getResult();\n+      mlir::TF::ExpOp::create(builder, loc, shifted_logits).getResult();\n \n   // Sum the exponential.\n   TF_ASSIGN_OR_RETURN(\n@@ -162,8 +162,9 @@ mlir::Value ComputeSoftmax(mlir::OpBuilder& builder,\n                            const mlir::Value& exp_of_shifted_logits,\n                            const mlir::Value& sum_of_exp) {\n   // For Softmax, we compute exp(shifted_logits)/sum(exp(shifted_logits))\n-  auto softmax = builder.create<mlir::TF::DivOp>(\n-      exp_of_shifted_logits.getLoc(), exp_of_shifted_logits, sum_of_exp);\n+  auto softmax =\n+      mlir::TF::DivOp::create(builder, exp_of_shifted_logits.getLoc(),\n+                              exp_of_shifted_logits, sum_of_exp);\n   return softmax.getResult();\n }\n \n@@ -174,9 +175,9 @@ mlir::Value ComputeLogSoftmax(mlir::OpBuilder& builder,\n                               const mlir::Value& sum_of_exp) {\n   // For LogSoftmax, we compute shifted_logs - log(sum(exp(shifted_logits)))\n   auto log_of_sum =\n-      builder.create<mlir::TF::LogOp>(shifted_logits.getLoc(), sum_of_exp);\n-  auto log_softmax = builder.create<mlir::TF::SubOp>(\n-      shifted_logits.getLoc(), shifted_logits, log_of_sum.getResult());\n+      mlir::TF::LogOp::create(builder, shifted_logits.getLoc(), sum_of_exp);\n+  auto log_softmax = mlir::TF::SubOp::create(\n+      builder, shifted_logits.getLoc(), shifted_logits, log_of_sum.getResult());\n   return log_softmax.getResult();\n }\n \n@@ -223,12 +224,11 @@ StatusOr<mlir::Value> GetFPConstOfType(mlir::OpBuilder& builder,\n                                        const mlir::Value& input, float value) {\n   if (mlir::TensorType type =\n           mlir::dyn_cast<mlir::TensorType>(input.getType())) {\n-    return builder\n-        .create<mlir::TF::ConstOp>(\n-            input.getLoc(),\n-            mlir::DenseFPElementsAttr::get<float>(\n-                mlir::RankedTensorType::get({}, type.getElementType()),\n-                {value}))\n+    return mlir::TF::ConstOp::create(\n+               builder, input.getLoc(),\n+               mlir::DenseFPElementsAttr::get<float>(\n+                   mlir::RankedTensorType::get({}, type.getElementType()),\n+                   {value}))\n         .getOutput();\n   } else {\n     return errors::Unimplemented(\"non tensor type for labels is not supported\");\n@@ -290,49 +290,47 @@ StatusOr<mlir::Value> ComputeOneHot(mlir::OpBuilder& builder,\n \n   // Slice out the [1,1] for mesh_dim_index.\n   mlir::Value shard_id =\n-      builder\n-          .create<mlir::TF::SliceOp>(\n-              loc, mlir::RankedTensorType::get({1, 1}, builder.getI32Type()),\n-              mesh_coordinates,\n-              IntConst(builder, input.getLoc(), {0, mesh_dim_index}),\n-              IntConst(builder, input.getLoc(), {1, 1}))\n+      mlir::TF::SliceOp::create(\n+          builder, loc,\n+          mlir::RankedTensorType::get({1, 1}, builder.getI32Type()),\n+          mesh_coordinates,\n+          IntConst(builder, input.getLoc(), {0, mesh_dim_index}),\n+          IntConst(builder, input.getLoc(), {1, 1}))\n           .getOutput();\n \n-  shard_id = builder\n-                 .create<mlir::TF::SqueezeOp>(\n-                     loc, mlir::RankedTensorType::get({}, builder.getI32Type()),\n-                     shard_id, builder.getI64ArrayAttr({0, 1}))\n-                 .getOutput();\n+  shard_id =\n+      mlir::TF::SqueezeOp::create(\n+          builder, loc, mlir::RankedTensorType::get({}, builder.getI32Type()),\n+          shard_id, builder.getI64ArrayAttr({0, 1}))\n+          .getOutput();\n \n   // `new_indices` = `input` - `shard_id` * (classes/num_shards)\n   mlir::Value id_offset =\n-      builder.create<mlir::TF::MulOp>(loc, shard_id, depth).getZ();\n+      mlir::TF::MulOp::create(builder, loc, shard_id, depth).getZ();\n \n   // Note that the type of id_offset (int32) may not match the type of input.\n   // So we insert a cast in this case.\n   mlir::TensorType input_type =\n       mlir::dyn_cast<mlir::TensorType>(input.getType());\n   if (!input_type) return errors::InvalidArgument(\"input is not a TensorType\");\n   if (!input_type.getElementType().isInteger(32))\n-    id_offset =\n-        builder\n-            .create<mlir::TF::CastOp>(\n-                loc,\n-                mlir::RankedTensorType::get({}, input_type.getElementType()),\n-                id_offset)\n-            .getY();\n+    id_offset = mlir::TF::CastOp::create(builder, loc,\n+                                         mlir::RankedTensorType::get(\n+                                             {}, input_type.getElementType()),\n+                                         id_offset)\n+                    .getY();\n \n   mlir::Value indices =\n-      builder.create<mlir::TF::SubOp>(loc, input, id_offset).getZ();\n+      mlir::TF::SubOp::create(builder, loc, input, id_offset).getZ();\n \n   TF_ASSIGN_OR_RETURN(mlir::Value on_value,\n                       GetFPConstOfType(builder, features, 1.0));\n   TF_ASSIGN_OR_RETURN(mlir::Value off_value,\n                       GetFPConstOfType(builder, features, 0.0));\n \n-  return builder\n-      .create<mlir::TF::OneHotOp>(input.getLoc(), indices, depth, on_value,\n-                                  off_value, builder.getI64IntegerAttr(1))\n+  return mlir::TF::OneHotOp::create(builder, input.getLoc(), indices, depth,\n+                                    on_value, off_value,\n+                                    builder.getI64IntegerAttr(1))\n       .getOutput();\n }\n \n@@ -530,7 +528,7 @@ StatusOr<mlir::Operation*> SoftmaxLossOpSPMDExpander::MaybeRelayoutOutputs(\n   llvm::SmallVector<mlir::Value, 4> values = {new_loss, new_backprop};\n \n   mlir::TF::IdentityNOp identity_op =\n-      builder.create<mlir::TF::IdentityNOp>(loss.getLoc(), types, values);\n+      mlir::TF::IdentityNOp::create(builder, loss.getLoc(), types, values);\n \n   newly_created_ops.insert(identity_op);\n \n@@ -627,17 +625,15 @@ StatusOr<mlir::Operation*> SoftmaxLossOpSPMDExpander::ExpandOp(\n                       GetFPConstOfType(builder, labels, 0.0));\n \n   const mlir::Value is_labels_zero =\n-      builder\n-          .create<mlir::TF::EqualOp>(op->getLoc(), labels, labels_zero,\n-                                     builder.getBoolAttr(true))\n+      mlir::TF::EqualOp::create(builder, op->getLoc(), labels, labels_zero,\n+                                builder.getBoolAttr(true))\n           .getZ();\n   const mlir::Value safe_softmax =\n-      builder\n-          .create<mlir::TF::SelectV2Op>(op->getLoc(), is_labels_zero,\n-                                        features_zero, log_softmax)\n+      mlir::TF::SelectV2Op::create(builder, op->getLoc(), is_labels_zero,\n+                                   features_zero, log_softmax)\n           .getOutput();\n   const mlir::Value prod =\n-      builder.create<mlir::TF::MulOp>(op->getLoc(), labels, safe_softmax)\n+      mlir::TF::MulOp::create(builder, op->getLoc(), labels, safe_softmax)\n           .getZ();\n \n   // Compute the reduce sum\n@@ -648,10 +644,10 @@ StatusOr<mlir::Operation*> SoftmaxLossOpSPMDExpander::ExpandOp(\n \n   builder.setInsertionPointAfterValue(positive_loss);\n   mlir::Value loss =\n-      builder.create<mlir::TF::NegOp>(op->getLoc(), positive_loss).getY();\n+      mlir::TF::NegOp::create(builder, op->getLoc(), positive_loss).getY();\n \n   mlir::Value backprop =\n-      builder.create<mlir::TF::SubOp>(op->getLoc(), softmax, labels);\n+      mlir::TF::SubOp::create(builder, op->getLoc(), softmax, labels);\n \n   return MaybeRelayoutOutputs(op, loss, backprop, internal_layout,\n                               output_layouts[0], output_layouts[1]);"
        },
        {
            "sha": "8e2fa02dcc9f446e054d67eee13c82c1a1e9547b",
            "filename": "tensorflow/dtensor/mlir/expansions/tensorlist_reserve_spmd_expander.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Ftensorlist_reserve_spmd_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Ftensorlist_reserve_spmd_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fexpansions%2Ftensorlist_reserve_spmd_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -58,10 +58,9 @@ StatusOr<mlir::Operation*> TensorListReserveSPMDExpander::ExpandOp(\n               mlir::RankedTensorType::get(local_shape, element_type),\n               builder.getContext()));\n   mlir::Value new_shape_value = Int64Const(builder, DT_LOC(op), local_shape);\n-  mlir::TF::TensorListReserveOp new_op =\n-      builder.create<mlir::TF::TensorListReserveOp>(\n-          DT_LOC(op), new_output_type, new_shape_value,\n-          tensorlist_op.getNumElements());\n+  mlir::TF::TensorListReserveOp new_op = mlir::TF::TensorListReserveOp::create(\n+      builder, DT_LOC(op), new_output_type, new_shape_value,\n+      tensorlist_op.getNumElements());\n \n   op->getResult(0).replaceAllUsesWith(new_op.getResult());\n   op->erase();"
        },
        {
            "sha": "f1e3a60f8a2d2198192f30b690c082aaf01ad6cf",
            "filename": "tensorflow/dtensor/mlir/handle_cross_cluster_dependencies.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fhandle_cross_cluster_dependencies.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fhandle_cross_cluster_dependencies.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fhandle_cross_cluster_dependencies.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -206,8 +206,8 @@ mlir::LogicalResult HandleCopyToMeshWithinCluster(\n       }\n     }\n     mlir::OpBuilder builder(op);\n-    auto identity_op = builder.create<mlir::TF::IdentityOp>(\n-        op.getLoc(), input.getType(), input);\n+    auto identity_op = mlir::TF::IdentityOp::create(builder, op.getLoc(),\n+                                                    input.getType(), input);\n     op->getResult(0).replaceAllUsesWith(identity_op.getOutput());\n     op->erase();\n     return mlir::WalkResult::advance();\n@@ -246,8 +246,9 @@ mlir::LogicalResult LowerToSendRecv(mlir::TF::CopyToMeshOp copy_to_mesh,\n \n   // Create send op that sends data from input cluster to target cluster.\n   const Mesh& target_mesh = mesh_or_status.value();\n-  builder.create<mlir::TF::DTensorSend>(\n-      copy_to_mesh.getLoc(), value_to_send, builder.getStringAttr(op_key),\n+  mlir::TF::DTensorSend::create(\n+      builder, copy_to_mesh.getLoc(), value_to_send,\n+      builder.getStringAttr(op_key),\n       mlir::dtensor::MeshAttr::get(context, target_mesh));\n \n   // Create recv op that recvs data from send op.\n@@ -258,8 +259,8 @@ mlir::LogicalResult LowerToSendRecv(mlir::TF::CopyToMeshOp copy_to_mesh,\n         \"CopyToMesh op must have static shape.\");\n \n   builder.setInsertionPoint(copy_to_mesh);\n-  auto recv_op = builder.create<mlir::TF::DTensorRecv>(\n-      copy_to_mesh.getLoc(), value_to_send.getType(),\n+  auto recv_op = mlir::TF::DTensorRecv::create(\n+      builder, copy_to_mesh.getLoc(), value_to_send.getType(),\n       builder.getStringAttr(op_key),\n       mlir::TF::ShapeAttr::get(context, tensor_type),\n       mlir::dtensor::MeshAttr::get(context, target_mesh));\n@@ -396,8 +397,9 @@ mlir::LogicalResult InsertCopyToMesh(mlir::tf_device::ClusterOp cluster) {\n     if (input_mesh == mesh) continue;\n     mlir::OpBuilder builder(op);\n \n-    auto new_op = builder.create<mlir::TF::CopyToMeshOp>(\n-        op->getLoc(), op->getResult(0).getType(), input, mesh.ToString());\n+    auto new_op = mlir::TF::CopyToMeshOp::create(builder, op->getLoc(),\n+                                                 op->getResult(0).getType(),\n+                                                 input, mesh.ToString());\n     op->replaceUsesOfWith(input, new_op.getResult());\n   }\n   return mlir::success();"
        },
        {
            "sha": "49ede9025b43109c925c34363ed14bfc59f86f7d",
            "filename": "tensorflow/dtensor/mlir/layout_propagation_v2.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 19,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -733,9 +733,9 @@ mlir::LogicalResult InsertDTensorLayoutOps(\n     mlir::Type value_type = GetSubtypeOrSelf(merged_layout.first);\n \n     if (auto type = mlir::dyn_cast<mlir::TensorType>(value_type)) {\n-      auto layout_op = builder.create<mlir::TF::DTensorLayout>(\n-          merged_layout.first.getLoc(), merged_layout.first, layout_attr,\n-          mlir::TF::ShapeAttr::get(builder.getContext(), type));\n+      auto layout_op = mlir::TF::DTensorLayout::create(\n+          builder, merged_layout.first.getLoc(), merged_layout.first,\n+          layout_attr, mlir::TF::ShapeAttr::get(builder.getContext(), type));\n       llvm::SmallPtrSet<mlir::Operation*, 4> exception{layout_op};\n       merged_layout.first.replaceAllUsesExcept(layout_op.getOutput(),\n                                                exception);\n@@ -1234,30 +1234,26 @@ mlir::LogicalResult InsertRelayoutForWhileLoops(\n       mlir::TF::ShapeAttr global_shape = mlir::TF::ShapeAttr::get(\n           builder.getContext(),\n           mlir::cast<mlir::TensorType>(yield_op->getOperand(i).getType()));\n-      mlir::TF::RelayoutOp first_relayout =\n-          builder.create<mlir::TF::RelayoutOp>(\n-              op.getLoc(), yield_op->getOperand(i).getType(),\n-              yield_op->getOperand(i), input_layout.ToString());\n-      mlir::TF::DTensorLayout first_layout_op =\n-          builder.create<mlir::TF::DTensorLayout>(\n-              op.getLoc(), first_relayout.getOutput(),\n-              mlir::dtensor::LayoutAttr::get(builder.getContext(),\n-                                             input_layout),\n-              global_shape);\n+      mlir::TF::RelayoutOp first_relayout = mlir::TF::RelayoutOp::create(\n+          builder, op.getLoc(), yield_op->getOperand(i).getType(),\n+          yield_op->getOperand(i), input_layout.ToString());\n+      mlir::TF::DTensorLayout first_layout_op = mlir::TF::DTensorLayout::create(\n+          builder, op.getLoc(), first_relayout.getOutput(),\n+          mlir::dtensor::LayoutAttr::get(builder.getContext(), input_layout),\n+          global_shape);\n       yield_op->setOperand(i, first_layout_op.getOutput());\n \n       // Insert the second relayout op after the loop itself.\n       builder.setInsertionPointAfter(op);\n       mlir::TF::DTensorLayout second_layout_op =\n-          builder.create<mlir::TF::DTensorLayout>(\n-              op.getLoc(), op->getResult(i),\n+          mlir::TF::DTensorLayout::create(\n+              builder, op.getLoc(), op->getResult(i),\n               mlir::dtensor::LayoutAttr::get(builder.getContext(),\n                                              input_layout),\n               global_shape);\n-      mlir::TF::RelayoutOp second_relayout =\n-          builder.create<mlir::TF::RelayoutOp>(\n-              op.getLoc(), second_layout_op.getOutput().getType(),\n-              second_layout_op.getOutput(), output_layout.ToString());\n+      mlir::TF::RelayoutOp second_relayout = mlir::TF::RelayoutOp::create(\n+          builder, op.getLoc(), second_layout_op.getOutput().getType(),\n+          second_layout_op.getOutput(), output_layout.ToString());\n       op->getResult(i).replaceAllUsesExcept(\n           second_relayout.getOutput(), llvm::SmallPtrSet<mlir::Operation*, 1>{\n                                            second_layout_op.getOperation()});"
        },
        {
            "sha": "142932afbee7da6f7e82c049bd02bd3416ee6c55",
            "filename": "tensorflow/dtensor/mlir/lower_send_recv.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Flower_send_recv.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Flower_send_recv.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Flower_send_recv.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -90,8 +90,8 @@ void PropagateDeviceIdToClusters(mlir::ModuleOp module) {\n \n   module.walk([&](mlir::tf_device::ClusterOp op) {\n     mlir::OpBuilder builder(&op.GetBody().front());\n-    builder.create<mlir::TF::IdentityOp>(main_func.getLoc(),\n-                                         device_id->getType(), *device_id);\n+    mlir::TF::IdentityOp::create(builder, main_func.getLoc(),\n+                                 device_id->getType(), *device_id);\n   });\n }\n "
        },
        {
            "sha": "81a856aa1a0c9ce8c9ad45a8a042c7b5433bdd7d",
            "filename": "tensorflow/dtensor/mlir/merge_clusters.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fmerge_clusters.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fmerge_clusters.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fmerge_clusters.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -288,31 +288,31 @@ void CloneEmptyIfWithPredicate(mlir::TF::IfRegionOp if_region, const Mesh& mesh,\n       absl::StrCat(kSendRecvKeyPrefix, *num_send_recvs);\n   *num_send_recvs += 1;\n \n-  builder.create<mlir::TF::DTensorSend>(\n-      if_region.getLoc(), if_region.getCond(),\n-      builder.getStringAttr(send_recv_key),\n-      mlir::dtensor::MeshAttr::get(context, mesh));\n+  mlir::TF::DTensorSend::create(builder, if_region.getLoc(),\n+                                if_region.getCond(),\n+                                builder.getStringAttr(send_recv_key),\n+                                mlir::dtensor::MeshAttr::get(context, mesh));\n \n   // Create new cluster op that contains cloned if operation.\n-  auto new_cluster = builder.create<mlir::tf_device::ClusterOp>(\n-      if_region.getLoc(), llvm::SmallVector<mlir::Type, 4>{});\n+  auto new_cluster = mlir::tf_device::ClusterOp::create(\n+      builder, if_region.getLoc(), llvm::SmallVector<mlir::Type, 4>{});\n   new_cluster.getBody().push_back(new mlir::Block);\n   builder.setInsertionPointToEnd(&new_cluster.GetBody());\n-  auto return_op = builder.create<mlir::tf_device::ReturnOp>(\n-      if_region.getLoc(), llvm::SmallVector<mlir::Value, 4>{});\n+  auto return_op = mlir::tf_device::ReturnOp::create(\n+      builder, if_region.getLoc(), llvm::SmallVector<mlir::Value, 4>{});\n \n   // Add DTensorRecv op inside new cluster that receives the cluster.\n   builder.setInsertionPoint(return_op);\n-  auto recv_op = builder.create<mlir::TF::DTensorRecv>(\n-      if_region.getLoc(), predicate_tensor_type,\n+  auto recv_op = mlir::TF::DTensorRecv::create(\n+      builder, if_region.getLoc(), predicate_tensor_type,\n       builder.getStringAttr(send_recv_key),\n       mlir::TF::ShapeAttr::get(context, predicate_tensor_type),\n       mlir::dtensor::MeshAttr::get(context, mesh));\n \n   // Clone tf.IfRegion op inside newly created cluster and make sure\n   // that the predicate tensor is from DTensorRecv op created above.\n-  auto host_side_if = builder.create<mlir::TF::IfRegionOp>(\n-      if_region.getLoc(), llvm::SmallVector<mlir::Type, 4>{},\n+  auto host_side_if = mlir::TF::IfRegionOp::create(\n+      builder, if_region.getLoc(), llvm::SmallVector<mlir::Type, 4>{},\n       recv_op.getOutput(), if_region.getIsStateless(),\n       GetUniqueControlflowFnName(\"cloned_if_then\", builder),\n       GetUniqueControlflowFnName(\"cloned_if_else\", builder));\n@@ -322,15 +322,15 @@ void CloneEmptyIfWithPredicate(mlir::TF::IfRegionOp if_region, const Mesh& mesh,\n   auto& then_branch = host_side_if.getThenBranch();\n   then_branch.push_back(new mlir::Block);\n   builder.setInsertionPointToEnd(&then_branch.front());\n-  builder.create<mlir::TF::YieldOp>(if_region.getLoc(),\n-                                    /*operands=*/llvm::ArrayRef<mlir::Value>{});\n+  mlir::TF::YieldOp::create(builder, if_region.getLoc(),\n+                            /*operands=*/llvm::ArrayRef<mlir::Value>{});\n \n   // Create empty else branch region.\n   auto& else_branch = host_side_if.getElseBranch();\n   else_branch.push_back(new mlir::Block);\n   builder.setInsertionPointToEnd(&else_branch.front());\n-  builder.create<mlir::TF::YieldOp>(if_region.getLoc(),\n-                                    /*operands=*/llvm::ArrayRef<mlir::Value>{});\n+  mlir::TF::YieldOp::create(builder, if_region.getLoc(),\n+                            /*operands=*/llvm::ArrayRef<mlir::Value>{});\n   new_cluster->setAttr(kMeshAttr, builder.getStringAttr(mesh.ToString()));\n }\n \n@@ -550,8 +550,8 @@ mlir::LogicalResult MergeClusters(mlir::ModuleOp module) {\n \n     // Create a single cluster op contains merged computations for `mesh`.\n     builder.setInsertionPoint(&func_block.front());\n-    auto new_cluster = builder.create<mlir::tf_device::ClusterOp>(\n-        module.getLoc(), merged_return_types);\n+    auto new_cluster = mlir::tf_device::ClusterOp::create(\n+        builder, module.getLoc(), merged_return_types);\n     new_cluster.getBody().push_back(new mlir::Block);\n     new_cluster->setAttr(kMeshAttr, builder.getStringAttr(mesh.ToString()));\n \n@@ -578,8 +578,8 @@ mlir::LogicalResult MergeClusters(mlir::ModuleOp module) {\n     }\n \n     builder.setInsertionPointToEnd(&new_cluster.GetBody());\n-    builder.create<mlir::tf_device::ReturnOp>(new_cluster.getLoc(),\n-                                              merged_return_values);\n+    mlir::tf_device::ReturnOp::create(builder, new_cluster.getLoc(),\n+                                      merged_return_values);\n \n     // Replace return value usages.\n     for (auto it :"
        },
        {
            "sha": "894b1bacbe72eebe76de51d8ee9038e1554e2b40",
            "filename": "tensorflow/dtensor/mlir/move_compilation_to_host.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fmove_compilation_to_host.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fmove_compilation_to_host.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fmove_compilation_to_host.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -117,8 +117,8 @@ mlir::LogicalResult CreateSendRecvOpsToTransferProgramKey(\n   builder.setInsertionPointAfter(compile_op);\n   for (int i = 0; i < num_tpu_devices; ++i) {\n     const std::string& tensor_name = device_key_map[i];\n-    auto send = builder.create<mlir::TF::_HostSendOp>(\n-        compile_op->getLoc(), compilation_key, tensor_name,\n+    auto send = mlir::TF::_HostSendOp::create(\n+        builder, compile_op->getLoc(), compilation_key, tensor_name,\n         compile_op_launch.getDevice(),\n         /*send_device_incarnation=*/0, local_devices[i]);\n     send->setAttr(\"device\", compile_op_launch.getDeviceAttr());\n@@ -148,15 +148,15 @@ mlir::LogicalResult CreateSendRecvOpsToTransferProgramKey(\n \n     mlir::Block* fn_block = recv_select_fn.addEntryBlock();\n     mlir::OpBuilder fn_builder = mlir::OpBuilder::atBlockEnd(fn_block);\n-    auto recv = fn_builder.create<mlir::TF::_HostRecvOp>(\n-        compile_op->getLoc(),\n+    auto recv = mlir::TF::_HostRecvOp::create(\n+        fn_builder, compile_op->getLoc(),\n         mlir::cast<mlir::TensorType>(compilation_key.getType()),\n         device_key_map[i], compile_op_launch.getDevice(),\n         /*send_device_incarnation=*/0, local_devices[i]);\n     recv->setAttr(\"device\", builder.getStringAttr(local_devices[i]));\n \n-    fn_builder.create<mlir::func::ReturnOp>(recv_select_fn.getLoc(),\n-                                            recv.getTensor());\n+    mlir::func::ReturnOp::create(fn_builder, recv_select_fn.getLoc(),\n+                                 recv.getTensor());\n \n     compilation_key_functions.emplace_back(recv_select_fn);\n   }\n@@ -172,8 +172,8 @@ mlir::LogicalResult CreateSendRecvOpsToTransferProgramKey(\n     symbols.push_back(mlir::SymbolRefAttr::get(func));\n \n   // Create a TF::Case op that selects `values` based on `id`.\n-  auto program_key = builder.create<mlir::TF::CaseOp>(\n-      compile_op.getLoc(),\n+  auto program_key = mlir::TF::CaseOp::create(\n+      builder, compile_op.getLoc(),\n       /*output=*/llvm::SmallVector<mlir::Type, 4>{compilation_key.getType()},\n       /*branch_index=*/*device_id,\n       /*input=*/llvm::ArrayRef<mlir::Value>{},\n@@ -288,15 +288,16 @@ mlir::LogicalResult HandleCompilationOps(\n           llvm::formatv(\"error while creating TPU compilation logic. {0}\",\n                         device_ordinal_host.status().message()));\n \n-    mlir::Value predicate_host = builder.create<mlir::TF::EqualOp>(\n-        compile_op.getLoc(), *device_ordinal_host,\n+    mlir::Value predicate_host = mlir::TF::EqualOp::create(\n+        builder, compile_op.getLoc(), *device_ordinal_host,\n         CreateIntScalarConst(0, builder, compile_op.getLoc()),\n         /*incompatible_shape_error=*/builder.getBoolAttr(true));\n \n     // If op here contains send/recv and TPUCompile op that should not be pruned\n     // away. Therefore, we explicitly set the op to be stateful.\n-    auto if_host = builder.create<mlir::TF::IfRegionOp>(\n-        compile_op.getLoc(), llvm::SmallVector<mlir::Type, 4>{}, predicate_host,\n+    auto if_host = mlir::TF::IfRegionOp::create(\n+        builder, compile_op.getLoc(), llvm::SmallVector<mlir::Type, 4>{},\n+        predicate_host,\n         /*is_stateless=*/builder.getBoolAttr(false),\n         GetUniqueControlflowFnName(\"compilation_host_then\", builder),\n         GetUniqueControlflowFnName(\"compilation_host_else\", builder));\n@@ -305,18 +306,17 @@ mlir::LogicalResult HandleCompilationOps(\n     auto& host_else_branch = if_host.getElseBranch();\n     host_else_branch.push_back(new mlir::Block);\n     builder.setInsertionPointToEnd(&host_else_branch.front());\n-    builder.create<mlir::TF::YieldOp>(\n-        compile_op.getLoc(),\n-        /*operands=*/llvm::ArrayRef<mlir::Value>{});\n+    mlir::TF::YieldOp::create(builder, compile_op.getLoc(),\n+                              /*operands=*/llvm::ArrayRef<mlir::Value>{});\n \n     // Create then branch region with logic to compile TPU program and send\n     // program key to all TPU devices.\n     auto& host_then_branch = if_host.getThenBranch();\n     host_then_branch.push_back(new mlir::Block);\n     builder.setInsertionPointToEnd(&host_then_branch.front());\n-    auto yield = builder.create<mlir::TF::YieldOp>(\n-        compile_op.getLoc(),\n-        /*operands=*/llvm::ArrayRef<mlir::Value>{});\n+    auto yield =\n+        mlir::TF::YieldOp::create(builder, compile_op.getLoc(),\n+                                  /*operands=*/llvm::ArrayRef<mlir::Value>{});\n     compilation_move_before = yield;\n \n     builder.setInsertionPointAfter(if_host);"
        },
        {
            "sha": "89c351b0f71ccc0cfa09fedbe494cf3bbb437989",
            "filename": "tensorflow/dtensor/mlir/op_to_device_cluster.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fop_to_device_cluster.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fop_to_device_cluster.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fop_to_device_cluster.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -51,8 +51,8 @@ mlir::LogicalResult WrapDeviceCluster(mlir::OpBuilder *builder,\n                                       mlir::Operation *op) {\n   // Create new tf_device.cluster op wrapping a single operation.\n   builder->setInsertionPoint(op);\n-  auto cluster = builder->create<mlir::tf_device::ClusterOp>(\n-      op->getLoc(), op->getResultTypes());\n+  auto cluster = mlir::tf_device::ClusterOp::create(*builder, op->getLoc(),\n+                                                    op->getResultTypes());\n   if (auto layout_op = llvm::dyn_cast<mlir::TF::DTensorLayout>(op)) {\n     cluster->setAttr(kMeshAttr, builder->getStringAttr(\n                                     layout_op.getLayout().mesh().ToString()));\n@@ -89,7 +89,7 @@ mlir::LogicalResult WrapDeviceCluster(mlir::OpBuilder *builder,\n   cluster.getBody().push_back(new mlir::Block);\n \n   builder->setInsertionPointToEnd(&cluster.GetBody());\n-  builder->create<mlir::tf_device::ReturnOp>(op->getLoc(), op->getResults());\n+  mlir::tf_device::ReturnOp::create(*builder, op->getLoc(), op->getResults());\n \n   // Move `op` inside newly created `ClusterOp`.\n   op->moveBefore(cluster.GetBody().getTerminator());"
        },
        {
            "sha": "4b7a776ea2cd2cc2e249ada7c3bfedd0649097f6",
            "filename": "tensorflow/dtensor/mlir/op_utils.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fop_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fop_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fop_utils.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -116,8 +116,8 @@ mlir::LogicalResult ReplaceAuxiliaryDTensorLayoutOpsWithIdentity(\n \n       // Replace DTensorLayout op with identity op.\n       mlir::OpBuilder builder(input_layout_op);\n-      auto new_identity = builder.create<mlir::TF::IdentityOp>(\n-          input_layout_op->getLoc(), input_layout_op.getType(),\n+      auto new_identity = mlir::TF::IdentityOp::create(\n+          builder, input_layout_op->getLoc(), input_layout_op.getType(),\n           input_layout_op.getInput());\n       input_layout_op.getOutput().replaceAllUsesWith(new_identity.getOutput());\n       input_layout_op.erase();"
        },
        {
            "sha": "7be77a3f624ff42d20555e2bc0272d2f2ed08ae4",
            "filename": "tensorflow/dtensor/mlir/propagate_default_layout.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fpropagate_default_layout.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fpropagate_default_layout.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fpropagate_default_layout.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -53,8 +53,8 @@ void CreateDTensorLayoutOp(const Layout& layout, mlir::Value input,\n                            mlir::MLIRContext* context) {\n   if (layout.IsEmpty()) return;\n \n-  auto layout_op = builder->create<mlir::TF::DTensorLayout>(\n-      loc, input, mlir::dtensor::LayoutAttr::get(context, layout),\n+  auto layout_op = mlir::TF::DTensorLayout::create(\n+      *builder, loc, input, mlir::dtensor::LayoutAttr::get(context, layout),\n       mlir::TF::ShapeAttr::get(context, type));\n   if (arg_index != nullptr) {\n     layout_op->setAttr(kFromArgIndex, arg_index);"
        },
        {
            "sha": "7381e3628e25d96d45c8cdba483a31a479378892",
            "filename": "tensorflow/dtensor/mlir/propagate_device_id_to_function_args.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fpropagate_device_id_to_function_args.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fpropagate_device_id_to_function_args.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fpropagate_device_id_to_function_args.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -125,17 +125,17 @@ mlir::LogicalResult PrependDeviceIdToCallsites(mlir::OpBuilder* builder,\n   mlir::Operation* new_call = nullptr;\n   if (auto stateful_partitioned_call =\n           llvm::dyn_cast<mlir::TF::StatefulPartitionedCallOp>(op)) {\n-    new_call = builder->create<mlir::TF::StatefulPartitionedCallOp>(\n-        op->getLoc(), op->getResultTypes(), new_operands,\n+    new_call = mlir::TF::StatefulPartitionedCallOp::create(\n+        *builder, op->getLoc(), op->getResultTypes(), new_operands,\n         /*args_attrs=*/nullptr,\n         /*res_attrs=*/nullptr, stateful_partitioned_call.getF(),\n         stateful_partitioned_call.getConfig(),\n         stateful_partitioned_call.getConfigProto(),\n         stateful_partitioned_call.getExecutorType());\n   } else {\n     auto partitioned_call = llvm::cast<mlir::TF::PartitionedCallOp>(op);\n-    new_call = builder->create<mlir::TF::PartitionedCallOp>(\n-        op->getLoc(), op->getResultTypes(), new_operands,\n+    new_call = mlir::TF::PartitionedCallOp::create(\n+        *builder, op->getLoc(), op->getResultTypes(), new_operands,\n         /*args_attrs=*/nullptr,\n         /*res_attrs=*/nullptr, partitioned_call.getF(),\n         partitioned_call.getConfig(), partitioned_call.getConfigProto(),"
        },
        {
            "sha": "3be8637314be9720d79d0382ce2fd35e37771b7e",
            "filename": "tensorflow/dtensor/mlir/restore_shape_inference.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Frestore_shape_inference.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Frestore_shape_inference.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Frestore_shape_inference.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -85,8 +85,8 @@ mlir::LogicalResult BackwardShapeInferenceToRestoreOp(mlir::ModuleOp module,\n     // O(N).\n     value.setType(type);\n   } else if (auto cast_op = llvm::dyn_cast_or_null<mlir::TF::CastOp>(op)) {\n-    auto new_cast_op = builder->create<mlir::TF::CastOp>(cast_op.getLoc(), type,\n-                                                         cast_op.getOperand());\n+    auto new_cast_op = mlir::TF::CastOp::create(*builder, cast_op.getLoc(),\n+                                                type, cast_op.getOperand());\n     cast_op.replaceAllUsesWith(new_cast_op.getResult());\n     cast_op.erase();\n \n@@ -103,8 +103,8 @@ mlir::LogicalResult BackwardShapeInferenceToRestoreOp(mlir::ModuleOp module,\n         module, builder, new_cast_op.getOperand(), new_type);\n   } else if (auto identity_op =\n                  llvm::dyn_cast_or_null<mlir::TF::IdentityOp>(op)) {\n-    auto new_identity_op = builder->create<mlir::TF::IdentityOp>(\n-        identity_op.getLoc(), type, identity_op.getInput());\n+    auto new_identity_op = mlir::TF::IdentityOp::create(\n+        *builder, identity_op.getLoc(), type, identity_op.getInput());\n     identity_op.getOutput().replaceAllUsesWith(new_identity_op.getOutput());\n     identity_op.erase();\n \n@@ -128,8 +128,9 @@ mlir::LogicalResult BackwardShapeInferenceToRestoreOp(mlir::ModuleOp module,\n     // RestoreV2Op we want to fix is on the mesh of the corresponding\n     // DTensorSend. Set shape of this DTensorRecv first and go to the\n     // corresponding DTensorSend.\n-    auto new_recv_op = builder->create<mlir::TF::DTensorRecv>(\n-        recv_op.getLoc(), type, builder->getStringAttr(recv_op.getKey()),\n+    auto new_recv_op = mlir::TF::DTensorRecv::create(\n+        *builder, recv_op.getLoc(), type,\n+        builder->getStringAttr(recv_op.getKey()),\n         mlir::TF::ShapeAttr::get(builder->getContext(),\n                                  mlir::dyn_cast<mlir::TensorType>(type)),\n         mlir::dtensor::MeshAttr::get(builder->getContext(), recv_op.getMesh()));"
        },
        {
            "sha": "e695320769ecc4e10868e2dffa0cbb55acea8dc6",
            "filename": "tensorflow/dtensor/mlir/sparse_expansions/dynamic_enqueue_sparse_expander.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fsparse_expansions%2Fdynamic_enqueue_sparse_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fsparse_expansions%2Fdynamic_enqueue_sparse_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fsparse_expansions%2Fdynamic_enqueue_sparse_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -53,14 +53,14 @@ StatusOr<mlir::Value> ExpandIndices(mlir::OpBuilder& builder,\n           .getElementType());\n   // Little trick to make a rank-2 tensor of [[0,0], [0,1]] using rank 1\n   // constants.\n-  mlir::Value indices_padding = builder.create<mlir::TF::ReshapeOp>(\n-      loc,\n+  mlir::Value indices_padding = mlir::TF::ReshapeOp::create(\n+      builder, loc,\n       mlir::TF::collection_ops_util::GetR1Const({0, 0, 0, 1}, builder, loc),\n       mlir::TF::collection_ops_util::GetR1Const({2, 2}, builder, loc));\n   mlir::Value indices_padded =\n-      builder.create<mlir::TF::PadOp>(loc, indices_padded_type,\n-                                      /*input=*/indices,\n-                                      /*paddings=*/indices_padding);\n+      mlir::TF::PadOp::create(builder, loc, indices_padded_type,\n+                              /*input=*/indices,\n+                              /*paddings=*/indices_padding);\n   return indices_padded;\n }\n \n@@ -98,16 +98,15 @@ StatusOr<mlir::Operation*> DynamicEnqueueSparseExpander::ExpandOp(\n   // This op does not have a return value so we do not need to replace any\n   // consumers.\n   mlir::Operation* sparse_enqueue_op =\n-      builder\n-          .create<mlir::TF::DynamicEnqueueTPUEmbeddingArbitraryTensorBatchOp>(\n-              location,\n-              /*sample_indices_or_row_splits_list=*/indices,\n-              /*embedding_indices=*/values,\n-              /*aggregation_weights=*/dense_enqueue_op.getAggregationWeights(),\n-              /*mode_override=*/\n-              dense_enqueue_op.getModeOverride(),\n-              /*device_ordinal=*/dense_enqueue_op.getDeviceOrdinal(),\n-              /*combiners=*/dense_enqueue_op.getCombiners());\n+      mlir::TF::DynamicEnqueueTPUEmbeddingArbitraryTensorBatchOp::create(\n+          builder, location,\n+          /*sample_indices_or_row_splits_list=*/indices,\n+          /*embedding_indices=*/values,\n+          /*aggregation_weights=*/dense_enqueue_op.getAggregationWeights(),\n+          /*mode_override=*/\n+          dense_enqueue_op.getModeOverride(),\n+          /*device_ordinal=*/dense_enqueue_op.getDeviceOrdinal(),\n+          /*combiners=*/dense_enqueue_op.getCombiners());\n   dense_enqueue_op.erase();\n   return sparse_enqueue_op;\n }"
        },
        {
            "sha": "5056b89ca9ae3210ef0317cdd8c040217b457774",
            "filename": "tensorflow/dtensor/mlir/sparse_expansions/matmul_sparse_expander.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fsparse_expansions%2Fmatmul_sparse_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fsparse_expansions%2Fmatmul_sparse_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fsparse_expansions%2Fmatmul_sparse_expander.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -38,8 +38,8 @@ StatusOr<mlir::Operation*> MatMulSparseExpander::ExpandOp(mlir::Operation* op) {\n     // Since operand 0 is a SparseValue, we don't need to check that\n     // the indices, values, and dense_shapes exist.\n     mlir::TF::SparseTensorDenseMatMulOp new_op =\n-        builder.create<mlir::TF::SparseTensorDenseMatMulOp>(\n-            op->getLoc(), op->getResultTypes(),\n+        mlir::TF::SparseTensorDenseMatMulOp::create(\n+            builder, op->getLoc(), op->getResultTypes(),\n             mlir::ValueRange{\n                 GetIndicesFromSparseTensor(op->getOperand(0)).value(),\n                 GetValuesFromSparseTensor(op->getOperand(0)).value(),"
        },
        {
            "sha": "9fd3af1af33c07de7e49242b57c3e5d5bdc74aa0",
            "filename": "tensorflow/dtensor/mlir/spmd_expander_common.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fspmd_expander_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fspmd_expander_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fspmd_expander_common.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -125,8 +125,8 @@ absl::Status CreateSplitOp(const int num_split, const int split_dimension,\n       mlir::RankedTensorType::get({}, builder->getIntegerType(32));\n   auto split_dimension_attr =\n       mlir::DenseElementsAttr::get(split_dim_type, split_dimension);\n-  auto split_dimension_op = builder->create<mlir::TF::ConstOp>(\n-      location, split_dim_type, split_dimension_attr);\n+  auto split_dimension_op = mlir::TF::ConstOp::create(\n+      *builder, location, split_dim_type, split_dimension_attr);\n \n   // Correctly set output shapes of split op output if input shape is statically\n   // known.\n@@ -157,8 +157,9 @@ absl::Status CreateSplitOp(const int num_split, const int split_dimension,\n \n   // Creates a split op that splits |src_input| along |split_dimension|.\n   llvm::SmallVector<mlir::Type, 4> output_types(num_split, output_type);\n-  *split_op = builder->create<mlir::TF::SplitOp>(\n-      location, output_types, split_dimension_op.getOutput(), src_input);\n+  *split_op =\n+      mlir::TF::SplitOp::create(*builder, location, output_types,\n+                                split_dimension_op.getOutput(), src_input);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "e7fffe44a1f52099fbbe0091c9daf7cd5188c79c",
            "filename": "tensorflow/dtensor/mlir/tpu_integration.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Ftpu_integration.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Ftpu_integration.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Ftpu_integration.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -110,8 +110,8 @@ mlir::LogicalResult CreateTPUCluster(\n   auto& function_block = function->getCallableRegion()->front();\n   builder->setInsertionPointToStart(&function_block);\n \n-  auto cluster = builder->create<mlir::tf_device::ClusterOp>(\n-      tpu_call.getLoc(), function->getResultTypes());\n+  auto cluster = mlir::tf_device::ClusterOp::create(*builder, tpu_call.getLoc(),\n+                                                    function->getResultTypes());\n   cluster.getBody().push_back(new mlir::Block);\n \n   auto& function_body = function_block.getOperations();\n@@ -121,8 +121,8 @@ mlir::LogicalResult CreateTPUCluster(\n \n   builder->setInsertionPointToEnd(&cluster.GetBody());\n   mlir::Operation* function_block_terminator = function_block.getTerminator();\n-  builder->create<mlir::tf_device::ReturnOp>(\n-      tpu_call.getLoc(), function_block_terminator->getOperands());\n+  mlir::tf_device::ReturnOp::create(*builder, tpu_call.getLoc(),\n+                                    function_block_terminator->getOperands());\n \n   function_block_terminator->setOperands(cluster.getResults());\n "
        },
        {
            "sha": "7858b3430d33efa548379fa472af63e477fe9677",
            "filename": "tensorflow/dtensor/mlir/utils/collective_lowering.cc",
            "status": "modified",
            "additions": 91,
            "deletions": 84,
            "changes": 175,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fcollective_lowering.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fcollective_lowering.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fcollective_lowering.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -122,8 +122,8 @@ mlir::LogicalResult EmitAllReduceForXla(\n   constexpr char kCrossReplica[] = \"CrossReplica\";\n \n   // For TPUs, lower to XlaAllReduce straightforwardly.\n-  *final_op = builder.create<mlir::TF::XlaAllReduceOp>(\n-      all_reduce.getLoc(), all_reduce.getResult().getType(),\n+  *final_op = mlir::TF::XlaAllReduceOp::create(\n+      builder, all_reduce.getLoc(), all_reduce.getResult().getType(),\n       all_reduce.getInput(), all_reduce.getGroupAssignment(),\n       all_reduce.getReduceOpAttr(), builder.getStringAttr(kCrossReplica));\n   return mlir::success();\n@@ -198,7 +198,7 @@ mlir::Value GetRelativeDeviceId(mlir::Operation* op,\n       ops_util::ReshapeScalarToSizeType(builder, DeviceId(op).value(), loc);\n   mlir::Value start_device_id = ops_util::GetR1Const(\n       {output_layout.mesh().min_global_device_id()}, builder, loc);\n-  return builder.create<mlir::TF::SubOp>(loc, device_id, start_device_id);\n+  return mlir::TF::SubOp::create(builder, loc, device_id, start_device_id);\n }\n \n void CreateGroupAndInstanceKey(\n@@ -219,13 +219,14 @@ void CreateGroupAndInstanceKey(\n   // Create a scalar group key by slicing device_id_to_group_key with\n   // device_id.\n   auto group_key_loc = DT_LOC2(loc, \"group_key\");\n-  auto group_key_slice = builder.create<mlir::TF::SliceOp>(\n-      group_key_loc, EffectivelyScalarR1Type(builder.getIntegerType(32)),\n+  auto group_key_slice = mlir::TF::SliceOp::create(\n+      builder, group_key_loc,\n+      EffectivelyScalarR1Type(builder.getIntegerType(32)),\n       /*input=*/IntConst(builder, loc, device_id_to_group_key),\n       /*begin=*/device_id,\n       /*size=*/IntConst(builder, loc, {1}));\n-  auto group_key_reshape = builder.create<mlir::TF::ReshapeOp>(\n-      group_key_loc, /*tensor=*/group_key_slice.getResult(),\n+  auto group_key_reshape = mlir::TF::ReshapeOp::create(\n+      builder, group_key_loc, /*tensor=*/group_key_slice.getResult(),\n       /*shape=*/ops_util::GetR1Const({}, builder, loc));\n   *group_key_scalar = group_key_reshape.getResult();\n \n@@ -257,8 +258,8 @@ mlir::Operation* EmitCollectiveReduce(\n   const bool is_mean_op = reduce_op_str == kReduceOpMean;\n   mlir::Value group_size_scalar = ops_util::CreateScalarConst(\n       host_group_size, builder, DT_LOC2(loc, \"group_size\"));\n-  auto collective_reduce = builder.create<mlir::TF::CollectiveReduceV2Op>(\n-      loc, /*output_type=*/input.getType(), input, group_size_scalar,\n+  auto collective_reduce = mlir::TF::CollectiveReduceV2Op::create(\n+      builder, loc, /*output_type=*/input.getType(), input, group_size_scalar,\n       group_key_scalar, instance_key_scalar,\n       /*ordering_token=*/mlir::ValueRange({}),\n       /*merge_op=*/builder.getStringAttr(is_mean_op ? \"Add\" : reduce_op_str),\n@@ -312,19 +313,21 @@ mlir::Operation* EmitCollectiveReduceScatter(\n   const bool is_mean_op = reduce_op_str == kReduceOpMean;\n   mlir::Value group_size_scalar = ops_util::CreateScalarConst(\n       host_group_size, builder, DT_LOC2(loc, \"group_size\"));\n-  auto collective_reduce_scatter = builder.create<\n-      mlir::TF::CollectiveReduceScatterV2Op>(\n-      loc, output_type, input, group_size_scalar, group_key_scalar,\n-      instance_key_scalar,\n-      /*ordering_token=*/mlir::ValueRange({}),\n-      /*merge_op=*/builder.getStringAttr(is_mean_op ? \"Add\" : reduce_op_str),\n-      /*final_op=*/builder.getStringAttr(is_mean_op ? \"Div\" : \"Id\"),\n-      /*communication_hint=*/builder.getStringAttr(\"nccl\"),  // TODO(tmorris):\n-                                                             // this shouldn't\n-                                                             // be needed\n-      /*timeout_seconds=*/builder.getF32FloatAttr(0.),\n-      /*is_stateless=*/builder.getBoolAttr(false),\n-      /*max_subdivs_per_device=*/builder.getI64IntegerAttr(16));\n+  auto collective_reduce_scatter =\n+      mlir::TF::CollectiveReduceScatterV2Op::create(\n+          builder, loc, output_type, input, group_size_scalar, group_key_scalar,\n+          instance_key_scalar,\n+          /*ordering_token=*/mlir::ValueRange({}),\n+          /*merge_op=*/\n+          builder.getStringAttr(is_mean_op ? \"Add\" : reduce_op_str),\n+          /*final_op=*/builder.getStringAttr(is_mean_op ? \"Div\" : \"Id\"),\n+          /*communication_hint=*/\n+          builder.getStringAttr(\"nccl\"),  // TODO(tmorris):\n+                                          // this shouldn't\n+                                          // be needed\n+          /*timeout_seconds=*/builder.getF32FloatAttr(0.),\n+          /*is_stateless=*/builder.getBoolAttr(false),\n+          /*max_subdivs_per_device=*/builder.getI64IntegerAttr(16));\n   SetSingleLayoutOnOp(collective_reduce_scatter, Layout::Empty());\n   if (need_transpose) {\n     return EmitTransposeOp(builder, loc,\n@@ -394,8 +397,8 @@ mlir::Operation* EmitCollectiveAllToAll(\n         new_shape.push_back(input_shape[i]);\n       }\n     }\n-    auto reshape_op = builder.create<mlir::TF::ReshapeOp>(\n-        loc, data, ops_util::GetR1Const(new_shape, builder, loc));\n+    auto reshape_op = mlir::TF::ReshapeOp::create(\n+        builder, loc, data, ops_util::GetR1Const(new_shape, builder, loc));\n \n     std::vector<int64> perm_for_permute_transpose;\n     perm_for_permute_transpose.reserve(input_shape.size() + 1);\n@@ -420,8 +423,8 @@ mlir::Operation* EmitCollectiveAllToAll(\n                                        1LL, std::multiplies<int64>());\n   std::vector<int64> flatten_shape = {host_group_size,\n                                       num_elements / host_group_size};\n-  auto flatten_reshape_op = builder.create<mlir::TF::ReshapeOp>(\n-      loc, input, ops_util::GetR1Const(flatten_shape, builder, loc));\n+  auto flatten_reshape_op = mlir::TF::ReshapeOp::create(\n+      builder, loc, input, ops_util::GetR1Const(flatten_shape, builder, loc));\n   mlir::TensorType output_type =\n       mlir::RankedTensorType::get(flatten_shape, input_type.getElementType());\n \n@@ -432,9 +435,10 @@ mlir::Operation* EmitCollectiveAllToAll(\n                             &group_key_scalar, &instance_key_scalar);\n   mlir::Value group_size_scalar =\n       ops_util::CreateScalarConst(host_group_size, builder, loc);\n-  auto collective_alltoall = builder.create<mlir::TF::CollectiveAllToAllV2Op>(\n-      loc, /*output_type=*/output_type, flatten_reshape_op->getResult(0),\n-      group_size_scalar, group_key_scalar, instance_key_scalar,\n+  auto collective_alltoall = mlir::TF::CollectiveAllToAllV2Op::create(\n+      builder, loc, /*output_type=*/output_type,\n+      flatten_reshape_op->getResult(0), group_size_scalar, group_key_scalar,\n+      instance_key_scalar,\n       /*ordering_token=*/mlir::ValueRange({}),\n       /*communication_hint=*/builder.getStringAttr(\"\"),\n       /*timeout_seconds=*/builder.getF32FloatAttr(0.),\n@@ -444,8 +448,9 @@ mlir::Operation* EmitCollectiveAllToAll(\n \n   if (requires_transpose) {\n     // Unflatten after all-to-all.\n-    auto reshape_op = builder.create<mlir::TF::ReshapeOp>(\n-        loc, prev_op, ops_util::GetR1Const(transposed_shape, builder, loc));\n+    auto reshape_op = mlir::TF::ReshapeOp::create(\n+        builder, loc, prev_op,\n+        ops_util::GetR1Const(transposed_shape, builder, loc));\n     // Undo earlier transpose which moved split or concat dim to rank 0.\n     std::vector<int64> perm_for_transpose;\n     perm_for_transpose.reserve(input_shape.size());\n@@ -473,8 +478,8 @@ mlir::Operation* EmitCollectiveAllToAll(\n   std::vector<int64> output_shape(input_shape.begin(), input_shape.end());\n   output_shape[concat_dimension] *= host_group_size;\n   output_shape[split_dimension] /= host_group_size;\n-  auto post_reshape_op = builder.create<mlir::TF::ReshapeOp>(\n-      loc, prev_op, ops_util::GetR1Const(output_shape, builder, loc));\n+  auto post_reshape_op = mlir::TF::ReshapeOp::create(\n+      builder, loc, prev_op, ops_util::GetR1Const(output_shape, builder, loc));\n \n   return post_reshape_op;\n }\n@@ -503,8 +508,8 @@ mlir::Operation* EmitCollectiveGather(\n \n   mlir::Value group_size_scalar =\n       ops_util::CreateScalarConst(host_group_size, builder, loc);\n-  auto collective_gather = builder.create<mlir::TF::CollectiveGatherV2Op>(\n-      loc, /*output_type=*/input.getType(), input, group_size_scalar,\n+  auto collective_gather = mlir::TF::CollectiveGatherV2Op::create(\n+      builder, loc, /*output_type=*/input.getType(), input, group_size_scalar,\n       group_key_scalar, instance_key_scalar,\n       /*ordering_token=*/mlir::ValueRange({}),\n       /*communication_hint=*/builder.getStringAttr(\"\"),\n@@ -606,12 +611,10 @@ mlir::LogicalResult LowerReduceScatterOp(\n   mlir::OpBuilder builder(reduce_scatter);\n   if (reduce_scatter.getDeviceType().ends_with(\"TPU\")) {\n     // For TPUs, lower to XlaReduceScatter straightforwardly.\n-    mlir::Operation* xla_reduce_scatter =\n-        builder.create<mlir::TF::XlaReduceScatterOp>(\n-            loc, reduce_scatter.getResult().getType(),\n-            reduce_scatter.getInput(), reduce_scatter.getGroupAssignment(),\n-            reduce_scatter.getScatterDimension(),\n-            reduce_scatter.getReduceOpAttr());\n+    mlir::Operation* xla_reduce_scatter = mlir::TF::XlaReduceScatterOp::create(\n+        builder, loc, reduce_scatter.getResult().getType(),\n+        reduce_scatter.getInput(), reduce_scatter.getGroupAssignment(),\n+        reduce_scatter.getScatterDimension(), reduce_scatter.getReduceOpAttr());\n     SetSingleLayoutOnOp(xla_reduce_scatter, *output_layout);\n     reduce_scatter.replaceAllUsesWith(xla_reduce_scatter);\n   } else if (reduce_scatter.getDeviceType().ends_with(\"GPU\") &&\n@@ -653,16 +656,17 @@ mlir::LogicalResult LowerReduceScatterOp(\n       return reduce_scatter.emitOpError(input_layout.status().message());\n     }\n \n-    auto dtensor_allreduce = builder.create<mlir::TF::DTensorAllReduceOp>(\n-        reduce_scatter.getLoc(), reduce_scatter.getOperand(0).getType(),\n-        reduce_scatter.getOperand(0), reduce_scatter.getGroupAssignment(),\n-        reduce_scatter.getReduceOp(), reduce_scatter.getDeviceType());\n+    auto dtensor_allreduce = mlir::TF::DTensorAllReduceOp::create(\n+        builder, reduce_scatter.getLoc(),\n+        reduce_scatter.getOperand(0).getType(), reduce_scatter.getOperand(0),\n+        reduce_scatter.getGroupAssignment(), reduce_scatter.getReduceOp(),\n+        reduce_scatter.getDeviceType());\n     SetSingleLayoutOnOp(dtensor_allreduce, *input_layout);\n \n     mlir::Operation* dtensor_all_scatter =\n-        builder.create<mlir::TF::DTensorAllScatterOp>(\n-            reduce_scatter.getLoc(), reduce_scatter.getResult().getType(),\n-            dtensor_allreduce.getResult(),\n+        mlir::TF::DTensorAllScatterOp::create(\n+            builder, reduce_scatter.getLoc(),\n+            reduce_scatter.getResult().getType(), dtensor_allreduce.getResult(),\n             mlir::dtensor::LayoutAttr::get(builder.getContext(), *input_layout),\n             mlir::dtensor::LayoutAttr::get(builder.getContext(),\n                                            *output_layout));\n@@ -676,8 +680,9 @@ mlir::LogicalResult LowerReduceScatterOp(\n mlir::Value CreateZeroScalar(mlir::OpBuilder& builder, mlir::Location loc,\n                              mlir::RankedTensorType type) {\n   const mlir::Value zero_scalar = ops_util::CreateScalarConst(0, builder, loc);\n-  return builder.create<mlir::TF::CastOp>(\n-      loc, mlir::RankedTensorType::get({}, type.getElementType()), zero_scalar);\n+  return mlir::TF::CastOp::create(\n+      builder, loc, mlir::RankedTensorType::get({}, type.getElementType()),\n+      zero_scalar);\n }\n \n // device_id is the relative device_id in a mesh (device id - mesh's 1st device\n@@ -691,29 +696,31 @@ mlir::Value SelectElementsBasedOnId(\n       ops_util::GetR1Const(candidates_flat, builder, loc);\n   const mlir::Value candidates_shape =\n       ops_util::GetR1Const({num_devices, output_shape_size}, builder, loc);\n-  const mlir::Value candidates = builder.create<mlir::TF::ReshapeOp>(\n-      loc, candidates_flat_const, candidates_shape);\n+  const mlir::Value candidates = mlir::TF::ReshapeOp::create(\n+      builder, loc, candidates_flat_const, candidates_shape);\n \n   // Add a zero after the only value in the 1x1 device_id tensor.\n-  const mlir::Value device_id_paddings = builder.create<mlir::TF::ReshapeOp>(\n-      loc, ops_util::GetR1Const({0, 1}, builder, loc),\n+  const mlir::Value device_id_paddings = mlir::TF::ReshapeOp::create(\n+      builder, loc, ops_util::GetR1Const({0, 1}, builder, loc),\n       ops_util::GetR1Const({1, 2}, builder, loc));\n-  const mlir::Value device_id_padded = builder.create<mlir::TF::PadOp>(\n-      loc, candidates_shape.getType(), /*input=*/device_id,\n+  const mlir::Value device_id_padded = mlir::TF::PadOp::create(\n+      builder, loc, candidates_shape.getType(), /*input=*/device_id,\n       /*paddings=*/device_id_paddings);\n \n   // Slice a vertical vector out of the 2D candidates matrix.\n   const mlir::RankedTensorType chosen_shape_type = mlir::RankedTensorType::get(\n       {1, output_shape_size}, builder.getIntegerType(32));\n   const mlir::Value chosen_shape_const =\n       ops_util::GetR1Const(chosen_shape_type.getShape(), builder, loc);\n-  const mlir::Value chosen = builder.create<mlir::TF::SliceOp>(\n-      loc, chosen_shape_type, /*input=*/candidates, /*begin=*/device_id_padded,\n+  const mlir::Value chosen = mlir::TF::SliceOp::create(\n+      builder, loc, chosen_shape_type, /*input=*/candidates,\n+      /*begin=*/device_id_padded,\n       /*size=*/chosen_shape_const);\n \n   // Remove the leading dimension of size 1 before returning the result.\n-  return builder.create<mlir::TF::ReshapeOp>(\n-      loc, chosen, ops_util::GetR1Const({output_shape_size}, builder, loc));\n+  return mlir::TF::ReshapeOp::create(\n+      builder, loc, chosen,\n+      ops_util::GetR1Const({output_shape_size}, builder, loc));\n }\n \n StatusOr<const mlir::DenseIntElementsAttr> GetGroupAssignment(\n@@ -841,8 +848,8 @@ mlir::LogicalResult LowerAllGatherOpToCollective(\n       new_shape.push_back(input_shape_after_tr[j]);\n     }\n \n-    auto reshape_op = builder.create<mlir::TF::ReshapeOp>(\n-        loc, /*tensor=*/collective_op->getResult(0),\n+    auto reshape_op = mlir::TF::ReshapeOp::create(\n+        builder, loc, /*tensor=*/collective_op->getResult(0),\n         /*shape=*/ops_util::GetR1Const(new_shape, builder, loc));\n \n     prev_op_result = reshape_op->getResult(0);\n@@ -877,8 +884,8 @@ mlir::LogicalResult LowerAllGatherOpToCollective(\n     prev_op_result = post_transpose_op->getResult(0);\n   }\n \n-  auto output_reshape_op = builder.create<mlir::TF::ReshapeOp>(\n-      loc, /*tensor=*/prev_op_result,\n+  auto output_reshape_op = mlir::TF::ReshapeOp::create(\n+      builder, loc, /*tensor=*/prev_op_result,\n       /*shape=*/ops_util::GetR1Const(output_shape, builder, loc));\n   SetSingleLayoutOnOp(output_reshape_op, tgt_layout);\n   all_gather.replaceAllUsesWith(output_reshape_op->getResult(0));\n@@ -900,8 +907,8 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n   builder.setInsertionPointAfter(all_gather);\n \n   if (concat_dims.empty()) {\n-    mlir::TF::IdentityOp identity = builder.create<mlir::TF::IdentityOp>(\n-        all_gather.getLoc(), all_gather.getInput().getType(),\n+    mlir::TF::IdentityOp identity = mlir::TF::IdentityOp::create(\n+        builder, all_gather.getLoc(), all_gather.getInput().getType(),\n         all_gather.getInput());\n     SetSingleLayoutOnOp(identity, tgt_layout);\n \n@@ -942,7 +949,7 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n   const mlir::Value output_shape_const = Int64Const(builder, loc, output_shape);\n   const mlir::Value zero_scalar = CreateZeroScalar(builder, loc, input_type);\n   const mlir::Value zeros =\n-      builder.create<mlir::TF::FillOp>(loc, output_shape_const, zero_scalar);\n+      mlir::TF::FillOp::create(builder, loc, output_shape_const, zero_scalar);\n \n   // For every possible device ID, generate its strided slice ranges. Store all\n   // ranges---num_devices * output_shape_size * (begin, end, stride)---as three\n@@ -1001,12 +1008,12 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n     if (!tgt_layout.mesh().is_tpu_mesh())\n       return all_gather.emitOpError()\n              << \"source and target layout are not both on tpu\";\n-    update_result = builder.create<mlir::TF::XlaDynamicUpdateSliceOp>(\n-        loc, zeros.getType(), /*input=*/zeros,\n+    update_result = mlir::TF::XlaDynamicUpdateSliceOp::create(\n+        builder, loc, zeros.getType(), /*input=*/zeros,\n         /*update=*/all_gather.getInput(), /*indices=*/begin);\n   } else {\n-    update_result = builder.create<mlir::TF::TensorStridedSliceUpdateOp>(\n-        loc, zeros.getType(),\n+    update_result = mlir::TF::TensorStridedSliceUpdateOp::create(\n+        builder, loc, zeros.getType(),\n         /*input=*/zeros, begin, end, strides,\n         /*value=*/all_gather.getInput());\n   }\n@@ -1062,9 +1069,9 @@ mlir::LogicalResult LowerAllGatherOp(mlir::TF::DTensorAllGatherOp all_gather) {\n   absl::string_view reduce_type = kReduceOpAdd;\n   if (type && type.getElementType().isInteger(1)) reduce_type = kReduceOpAny;\n   mlir::TF::DTensorAllReduceOp all_reduce =\n-      builder.create<mlir::TF::DTensorAllReduceOp>(\n-          loc, update_result.getType(), update_result,\n-          builder.create<mlir::TF::ConstOp>(loc, group_assignment),\n+      mlir::TF::DTensorAllReduceOp::create(\n+          builder, loc, update_result.getType(), update_result,\n+          mlir::TF::ConstOp::create(builder, loc, group_assignment),\n           builder.getStringAttr(std::string(reduce_type)),\n           builder.getStringAttr(device_type));\n   SetSingleLayoutOnOp(all_reduce, tgt_layout);\n@@ -1146,12 +1153,12 @@ mlir::LogicalResult LowerAllScatterOp(\n   mlir::Attribute matrix_attr =\n       mlir::DenseIntElementsAttr::get(matrix_type, matrix);\n   mlir::Value matrix_value =\n-      builder.create<mlir::TF::ConstOp>(all_scatter.getLoc(), matrix_attr)\n+      mlir::TF::ConstOp::create(builder, all_scatter.getLoc(), matrix_attr)\n           .getResult();\n \n   // Compute the offset from mult_matrix_value and mesh_coordinates.\n-  mlir::TF::MatMulOp offset = builder.create<mlir::TF::MatMulOp>(\n-      all_scatter.getLoc(),\n+  mlir::TF::MatMulOp offset = mlir::TF::MatMulOp::create(\n+      builder, all_scatter.getLoc(),\n       mlir::RankedTensorType::get({1, original_layout.rank()},\n                                   builder.getIntegerType(32)),\n       mesh_coordinates, matrix_value);\n@@ -1164,14 +1171,14 @@ mlir::LogicalResult LowerAllScatterOp(\n   }\n \n   // Input to slice needs to be rank 1, so we need to squeeze it.\n-  mlir::TF::SqueezeOp offset_squeezed = builder.create<mlir::TF::SqueezeOp>(\n-      all_scatter.getLoc(),\n+  mlir::TF::SqueezeOp offset_squeezed = mlir::TF::SqueezeOp::create(\n+      builder, all_scatter.getLoc(),\n       mlir::RankedTensorType::get({original_layout.rank()},\n                                   builder.getIntegerType(32)),\n       offset.getProduct(), builder.getI64ArrayAttr({0}));\n \n-  auto result = builder.create<mlir::TF::SliceOp>(\n-      all_scatter.getLoc(), output_type, all_scatter.getInput(),\n+  auto result = mlir::TF::SliceOp::create(\n+      builder, all_scatter.getLoc(), output_type, all_scatter.getInput(),\n       offset_squeezed.getOutput(), slice_shape_value);\n \n   SetSingleLayoutOnOp(result, desired_layout);\n@@ -1231,9 +1238,9 @@ mlir::LogicalResult LowerAllToAllOp(mlir::TF::DTensorAllToAllOp all_to_all) {\n \n   if (mlir::StringRef(device_type).ends_with(\"TPU\")) {\n     // For TPUs, lower to XlaAllToAll.\n-    mlir::Operation* xla_all_to_all = builder.create<mlir::TF::AllToAllOp>(\n-        loc, all_to_all.getResult().getType(), all_to_all.getInput(),\n-        builder.create<mlir::TF::ConstOp>(loc, group_assignment),\n+    mlir::Operation* xla_all_to_all = mlir::TF::AllToAllOp::create(\n+        builder, loc, all_to_all.getResult().getType(), all_to_all.getInput(),\n+        mlir::TF::ConstOp::create(builder, loc, group_assignment),\n         concat_dimension, split_dimension, group_size);\n     SetSingleLayoutOnOp(xla_all_to_all, tgt_layout);\n     all_to_all.replaceAllUsesWith(xla_all_to_all);"
        },
        {
            "sha": "d8a7bcd97055212a7780b9b1effe4952b3574bc1",
            "filename": "tensorflow/dtensor/mlir/utils/update_tpu_metadata.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fupdate_tpu_metadata.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fupdate_tpu_metadata.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Futils%2Fupdate_tpu_metadata.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -254,8 +254,8 @@ mlir::LogicalResult UpdateTPUCompileMetadata(const Mesh& mesh_config,\n     if (mesh_config.use_xla_spmd()) {\n       // Create a new compile op with the appropriate new number of operands.\n       builder->setInsertionPointAfter(compile);\n-      auto new_compile_op = builder->create<mlir::TF::_TPUCompileMlirOp>(\n-          compile.getLoc(), compile.getCompilationStatus().getType(),\n+      auto new_compile_op = mlir::TF::_TPUCompileMlirOp::create(\n+          *builder, compile.getLoc(), compile.getCompilationStatus().getType(),\n           /*program=*/\n           llvm::SmallVector<mlir::Type, 8>(\n               mesh_config.num_devices(),"
        },
        {
            "sha": "edc6afb95a67abd2d64ca730e469a7c29c9cff6d",
            "filename": "tensorflow/dtensor/mlir/value_utils.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 42,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fvalue_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/tensorflow%2Fdtensor%2Fmlir%2Fvalue_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Fvalue_utils.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -103,8 +103,8 @@ mlir::Value ReshapeSizeTypeToScalar(mlir::OpBuilder builder, mlir::Location loc,\n       mlir::RankedTensorType::get({}, builder.getIntegerType(32));\n   mlir::Value scalar_shape =\n       ops_util::GetR1Const(scalar_type.getShape(), builder, loc);\n-  return builder.create<mlir::TF::ReshapeOp>(\n-      loc, mlir::ArrayRef<mlir::Type>{scalar_type},\n+  return mlir::TF::ReshapeOp::create(\n+      builder, loc, mlir::ArrayRef<mlir::Type>{scalar_type},\n       mlir::ArrayRef<mlir::Value>{tensor, scalar_shape});\n }\n \n@@ -114,7 +114,7 @@ mlir::Value IntConst(mlir::OpBuilder& builder, mlir::Location loc,\n       {static_cast<int64_t>(values.size())}, builder.getIntegerType(32));\n   mlir::Attribute const_attr =\n       mlir::DenseIntElementsAttr::get(const_type, values);\n-  return builder.create<mlir::TF::ConstOp>(loc, const_attr).getResult();\n+  return mlir::TF::ConstOp::create(builder, loc, const_attr).getResult();\n }\n \n StatusOr<llvm::SmallVector<int64_t>> GetTFShapeFromType(mlir::Type type) {\n@@ -133,7 +133,7 @@ mlir::Value Int64Const(mlir::OpBuilder& builder, mlir::Location loc,\n       {static_cast<int64_t>(values.size())}, builder.getIntegerType(64));\n   mlir::Attribute const_attr =\n       mlir::DenseIntElementsAttr::get(const_type, values);\n-  return builder.create<mlir::TF::ConstOp>(loc, const_attr).getResult();\n+  return mlir::TF::ConstOp::create(builder, loc, const_attr).getResult();\n }\n \n mlir::Value FloatConst(mlir::OpBuilder& builder, mlir::Location loc,\n@@ -142,16 +142,17 @@ mlir::Value FloatConst(mlir::OpBuilder& builder, mlir::Location loc,\n       {static_cast<int64_t>(values.size())}, builder.getF32Type());\n   mlir::Attribute const_attr =\n       mlir::DenseFPElementsAttr::get(const_type, values);\n-  return builder.create<mlir::TF::ConstOp>(loc, const_attr).getResult();\n+  return mlir::TF::ConstOp::create(builder, loc, const_attr).getResult();\n }\n \n mlir::Value StringScalarConst(mlir::OpBuilder& builder, mlir::Location loc,\n                               llvm::StringRef value) {\n-  return builder.create<mlir::TF::ConstOp>(\n-      loc, mlir::DenseStringElementsAttr::get(\n-               mlir::RankedTensorType::get(\n-                   {}, builder.getType<mlir::TF::StringType>()),\n-               value));\n+  return mlir::TF::ConstOp::create(\n+      builder, loc,\n+      mlir::DenseStringElementsAttr::get(\n+          mlir::RankedTensorType::get({},\n+                                      builder.getType<mlir::TF::StringType>()),\n+          value));\n }\n \n mlir::Value StringConst(mlir::OpBuilder& builder, mlir::Location loc,\n@@ -161,7 +162,7 @@ mlir::Value StringConst(mlir::OpBuilder& builder, mlir::Location loc,\n                                   builder.getType<mlir::TF::StringType>());\n   mlir::Attribute const_attr =\n       mlir::DenseStringElementsAttr::get(const_type, values);\n-  return builder.create<mlir::TF::ConstOp>(loc, const_attr).getResult();\n+  return mlir::TF::ConstOp::create(builder, loc, const_attr).getResult();\n }\n \n mlir::Value IntConstWithMatchingType(mlir::OpBuilder& builder,\n@@ -213,47 +214,49 @@ absl::Status ExtractConstVectorFromValue(\n mlir::Value CreateIntScalarConst(const int64_t value, mlir::OpBuilder builder,\n                                  mlir::Location loc, bool use_int64) {\n   if (use_int64) {\n-    return builder.create<mlir::TF::ConstOp>(\n-        loc, mlir::DenseIntElementsAttr::get(\n-                 mlir::RankedTensorType::get({}, builder.getI64Type()), value));\n+    return mlir::TF::ConstOp::create(\n+        builder, loc,\n+        mlir::DenseIntElementsAttr::get(\n+            mlir::RankedTensorType::get({}, builder.getI64Type()), value));\n   } else {\n-    return builder.create<mlir::TF::ConstOp>(\n-        loc, mlir::DenseIntElementsAttr::get(\n-                 mlir::RankedTensorType::get({}, builder.getI32Type()),\n-                 static_cast<int32_t>(value)));\n+    return mlir::TF::ConstOp::create(\n+        builder, loc,\n+        mlir::DenseIntElementsAttr::get(\n+            mlir::RankedTensorType::get({}, builder.getI32Type()),\n+            static_cast<int32_t>(value)));\n   }\n }\n \n StatusOr<mlir::Value> CreateZeroScalarConst(mlir::OpBuilder& builder,\n                                             mlir::Location loc,\n                                             mlir::Type type) {\n   if (type.isF64()) {\n-    return builder\n-        .create<mlir::TF::ConstOp>(\n-            loc, mlir::DenseFPElementsAttr::get(\n-                     mlir::RankedTensorType::get({}, builder.getF64Type()),\n-                     static_cast<double>(0.)))\n+    return mlir::TF::ConstOp::create(\n+               builder, loc,\n+               mlir::DenseFPElementsAttr::get(\n+                   mlir::RankedTensorType::get({}, builder.getF64Type()),\n+                   static_cast<double>(0.)))\n         .getResult();\n   } else if (type.isF32()) {\n-    return builder\n-        .create<mlir::TF::ConstOp>(\n-            loc, mlir::DenseFPElementsAttr::get(\n-                     mlir::RankedTensorType::get({}, builder.getF32Type()),\n-                     static_cast<float>(0.f)))\n+    return mlir::TF::ConstOp::create(\n+               builder, loc,\n+               mlir::DenseFPElementsAttr::get(\n+                   mlir::RankedTensorType::get({}, builder.getF32Type()),\n+                   static_cast<float>(0.f)))\n         .getResult();\n   } else if (type.isInteger(32)) {\n-    return builder\n-        .create<mlir::TF::ConstOp>(\n-            loc, mlir::DenseIntElementsAttr::get(\n-                     mlir::RankedTensorType::get({}, builder.getI32Type()),\n-                     static_cast<int32_t>(0)))\n+    return mlir::TF::ConstOp::create(\n+               builder, loc,\n+               mlir::DenseIntElementsAttr::get(\n+                   mlir::RankedTensorType::get({}, builder.getI32Type()),\n+                   static_cast<int32_t>(0)))\n         .getResult();\n   } else if (type.isInteger(64)) {\n-    return builder\n-        .create<mlir::TF::ConstOp>(\n-            loc, mlir::DenseIntElementsAttr::get(\n-                     mlir::RankedTensorType::get({}, builder.getI64Type()),\n-                     static_cast<int64_t>(0)))\n+    return mlir::TF::ConstOp::create(\n+               builder, loc,\n+               mlir::DenseIntElementsAttr::get(\n+                   mlir::RankedTensorType::get({}, builder.getI64Type()),\n+                   static_cast<int64_t>(0)))\n         .getResult();\n   } else {\n     return errors::InvalidArgument(\n@@ -270,8 +273,9 @@ StatusOr<mlir::Value> SelectScalarValueFromArray(mlir::OpBuilder& builder,\n     return errors::InvalidArgument(\"Input array must have shape [1, N].\");\n   }\n \n-  mlir::TF::SliceOp sliced_value = builder.create<mlir::TF::SliceOp>(\n-      location, mlir::RankedTensorType::get({1, 1}, arrayType.getElementType()),\n+  mlir::TF::SliceOp sliced_value = mlir::TF::SliceOp::create(\n+      builder, location,\n+      mlir::RankedTensorType::get({1, 1}, arrayType.getElementType()),\n       /*input=*/array,\n       /*begin=*/IntConst(builder, location, {0, index}),\n       /*size=*/IntConst(builder, location, {1, 1}));\n@@ -281,8 +285,8 @@ StatusOr<mlir::Value> SelectScalarValueFromArray(mlir::OpBuilder& builder,\n       mlir::RankedTensorType::get({}, builder.getIntegerType(32));\n   mlir::Value scalar_shape = mlir::TF::collection_ops_util::GetR1Const(\n       scalar_size_type.getShape(), builder, location);\n-  mlir::Value scalar_sliced_value = builder.create<mlir::TF::ReshapeOp>(\n-      location, mlir::ArrayRef<mlir::Type>{scalar_size_type},\n+  mlir::Value scalar_sliced_value = mlir::TF::ReshapeOp::create(\n+      builder, location, mlir::ArrayRef<mlir::Type>{scalar_size_type},\n       mlir::ArrayRef<mlir::Value>{sliced_value.getOutput(), scalar_shape},\n       mlir::ArrayRef<mlir::NamedAttribute>{});\n   return scalar_sliced_value;"
        },
        {
            "sha": "bbbd45800e3f5b3ab9c1f083107571e223949176",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -227,8 +227,8 @@ absl::StatusOr<mlir::func::FuncOp> EmitEntryFunctionApi(\n   }\n \n   builder.setInsertionPointToStart(fusion_module.getBody());\n-  auto entry_func = builder.create<FuncOp>(\n-      loc, entry_function_name,\n+  auto entry_func = FuncOp::create(\n+      builder, loc, entry_function_name,\n       mlir::FunctionType::get(context, param_types, result_types),\n       /*sym_visibility=*/mlir::StringAttr{},\n       mlir::ArrayAttr::get(context, arg_attrs),"
        },
        {
            "sha": "218f246803de9b7012f2a9f345452826b7aa46d2",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_scatter_emitter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -165,19 +165,19 @@ SmallVector<Value> EmitScatterComputation(\n     ret.reserve(reduced_values.size());\n     for (const auto& [reduced_value, output_tensor] :\n          llvm::zip(reduced_values, output_tensors)) {\n-      ret.push_back(b.create<mlir::tensor::InsertOp>(reduced_value,\n-                                                     output_tensor, indices));\n+      ret.push_back(mlir::tensor::InsertOp::create(b, reduced_value,\n+                                                   output_tensor, indices));\n     }\n     return ret;\n   }\n   Value output_tensor = output_tensors.front();\n   Value update_elem = update_elems.front();\n-  auto atomic_rmw = b.create<AtomicRMWOp>(output_tensor, indices);\n+  auto atomic_rmw = AtomicRMWOp::create(b, output_tensor, indices);\n   mlir::OpBuilder body_builder = atomic_rmw.getBodyBuilder();\n   auto reduced_val =\n       emitters::InlineBlock(body_builder, reducer.getBody().front(),\n                             {atomic_rmw.getCurrentValue(), update_elem})[0];\n-  body_builder.create<xla::YieldOp>(reducer->getLoc(), reduced_val);\n+  xla::YieldOp::create(body_builder, reducer->getLoc(), reduced_val);\n   return {atomic_rmw->getResult(0)};\n }\n \n@@ -444,7 +444,7 @@ absl::Status CpuScatterFusion::EmitEntryFunction(\n                           updated_outputs);\n                     },\n                     [&](mlir::OpBuilder& else_b, mlir::Location else_loc) {\n-                      else_b.create<scf::YieldOp>(else_loc, output_tensors);\n+                      scf::YieldOp::create(else_b, else_loc, output_tensors);\n                     })\n                 .getResults();\n         return predicated_updates;"
        },
        {
            "sha": "231889025496a59349297361c5778605bcb5466d",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/peel_workgroup_loop.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpeel_workgroup_loop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpeel_workgroup_loop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpeel_workgroup_loop.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -114,9 +114,9 @@ struct PeelWorkgroupLoopPattern : public mlir::OpRewritePattern<xla::LoopOp> {\n       }\n \n       mlir::ImplicitLocOpBuilder builder(loop_op.getLoc(), rewriter);\n-      auto cmp_op = builder.create<mlir::arith::CmpIOp>(\n-          mlir::arith::CmpIPredicate::sle, work_group_dim.operand,\n-          builder.create<mlir::arith::ConstantIndexOp>(query_dimension_upper));\n+      auto cmp_op = mlir::arith::CmpIOp::create(\n+          builder, mlir::arith::CmpIPredicate::sle, work_group_dim.operand,\n+          mlir::arith::ConstantIndexOp::create(builder, query_dimension_upper));\n \n       auto loop_body_cloner = GetLoopBodyCloner(loop_op);\n \n@@ -128,11 +128,11 @@ struct PeelWorkgroupLoopPattern : public mlir::OpRewritePattern<xla::LoopOp> {\n             query_dimension_upper;\n         peeled_map.Simplify();\n \n-        auto peeled_loop =\n-            then_builder.create<LoopOp>(then_loc, peeled_map, loop_op.getDims(),\n-                                        loop_op.getInits(), loop_body_cloner);\n-        then_builder.create<mlir::scf::YieldOp>(then_loc,\n-                                                peeled_loop.getResults());\n+        auto peeled_loop = LoopOp::create(then_builder, then_loc, peeled_map,\n+                                          loop_op.getDims(), loop_op.getInits(),\n+                                          loop_body_cloner);\n+        mlir::scf::YieldOp::create(then_builder, then_loc,\n+                                   peeled_loop.getResults());\n       };\n       auto else_body_builder = [&](mlir::OpBuilder& else_builder,\n                                    mlir::Location else_loc) -> void {\n@@ -142,14 +142,14 @@ struct PeelWorkgroupLoopPattern : public mlir::OpRewritePattern<xla::LoopOp> {\n         tail_map.Simplify();\n \n         auto tail_loop =\n-            else_builder.create<LoopOp>(else_loc, tail_map, loop_op.getDims(),\n-                                        loop_op.getInits(), loop_body_cloner);\n-        else_builder.create<mlir::scf::YieldOp>(else_loc,\n-                                                tail_loop.getResults());\n+            LoopOp::create(else_builder, else_loc, tail_map, loop_op.getDims(),\n+                           loop_op.getInits(), loop_body_cloner);\n+        mlir::scf::YieldOp::create(else_builder, else_loc,\n+                                   tail_loop.getResults());\n       };\n \n-      auto if_op = builder.create<mlir::scf::IfOp>(cmp_op, then_body_builder,\n-                                                   else_body_builder);\n+      auto if_op = mlir::scf::IfOp::create(builder, cmp_op, then_body_builder,\n+                                           else_body_builder);\n \n       rewriter.replaceOp(loop_op, if_op.getResults());\n       return mlir::success();"
        },
        {
            "sha": "8eee5850bbdde4a972cf8925bb23bf560c07637d",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/xla_cpu_rewrite_patterns.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 35,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fxla_cpu_rewrite_patterns.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fxla_cpu_rewrite_patterns.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fxla_cpu_rewrite_patterns.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -90,24 +90,24 @@ struct LowerLoadOp : public mlir::OpRewritePattern<LoadOp> {\n     auto kernel_arg = KernelArgType(b.getContext());\n \n     // Get a pointer to the first `KernelArg` struct.\n-    auto cast = b.create<mlir::UnrealizedConversionCastOp>(op.getLoc(), ptr,\n-                                                           op.getCallFrame())\n+    auto cast = mlir::UnrealizedConversionCastOp::create(b, op.getLoc(), ptr,\n+                                                         op.getCallFrame())\n                     .getResult(0);\n-    auto args_gep = b.create<mlir::LLVM::GEPOp>(\n-        ptr, kernel_call_frame, cast,\n+    auto args_gep = mlir::LLVM::GEPOp::create(\n+        b, ptr, kernel_call_frame, cast,\n         llvm::SmallVector<mlir::LLVM::GEPArg, 2>{mlir::LLVM::GEPArg(0),\n                                                  mlir::LLVM::GEPArg(3)},\n         mlir::LLVM::GEPNoWrapFlags::inbounds);\n-    auto args_ptr = b.create<mlir::LLVM::LoadOp>(ptr, args_gep);\n+    auto args_ptr = mlir::LLVM::LoadOp::create(b, ptr, args_gep);\n     args_ptr.setInvariant(true);\n \n     // Get a pointer to the `KernelArg` at the given index.\n-    auto arg_gep = b.create<mlir::LLVM::GEPOp>(\n-        ptr, kernel_arg, args_ptr,\n+    auto arg_gep = mlir::LLVM::GEPOp::create(\n+        b, ptr, kernel_arg, args_ptr,\n         llvm::SmallVector<mlir::LLVM::GEPArg, 2>{\n             mlir::LLVM::GEPArg(op.getIndex()), mlir::LLVM::GEPArg(0)},\n         mlir::LLVM::GEPNoWrapFlags::inbounds);\n-    auto arg_ptr = b.create<mlir::LLVM::LoadOp>(ptr, arg_gep);\n+    auto arg_ptr = mlir::LLVM::LoadOp::create(b, ptr, arg_gep);\n     arg_ptr.setInvariant(true);\n \n     if (auto dereferenceable = op->getAttrOfType<mlir::IntegerAttr>(\n@@ -121,12 +121,12 @@ struct LowerLoadOp : public mlir::OpRewritePattern<LoadOp> {\n       mlir::LLVMTypeConverter converter(rewriter.getContext());\n       mlir::Value memref_desc = mlir::MemRefDescriptor::fromStaticShape(\n           b, op.getLoc(), converter, memref_type, arg_ptr);\n-      auto memref_cast = b.create<mlir::UnrealizedConversionCastOp>(\n-          op.getLoc(), op.getResult().getType(), memref_desc);\n+      auto memref_cast = mlir::UnrealizedConversionCastOp::create(\n+          b, op.getLoc(), op.getResult().getType(), memref_desc);\n       rewriter.replaceOp(op, memref_cast);\n     } else {\n-      auto arg_ptr_cast = b.create<mlir::UnrealizedConversionCastOp>(\n-          op.getLoc(), op.getResult().getType(), arg_ptr.getResult());\n+      auto arg_ptr_cast = mlir::UnrealizedConversionCastOp::create(\n+          b, op.getLoc(), op.getResult().getType(), arg_ptr.getResult());\n       rewriter.replaceOp(op, arg_ptr_cast.getResult(0));\n     }\n     return mlir::success();\n@@ -149,38 +149,38 @@ struct LowerExtractWorkgroupIdOp\n     auto i64_ty = builder.getI64Type();\n \n     // Get a pointer to the `WorkGroupThread` struct.\n-    auto cast = builder\n-                    .create<mlir::UnrealizedConversionCastOp>(ptr_type,\n-                                                              op.getCallFrame())\n+    auto cast = mlir::UnrealizedConversionCastOp::create(builder, ptr_type,\n+                                                         op.getCallFrame())\n                     .getResult(0);\n-    auto workgroup_gep = builder.create<mlir::LLVM::GEPOp>(\n-        ptr_type, kernel_call_frame, cast,\n+    auto workgroup_gep = mlir::LLVM::GEPOp::create(\n+        builder, ptr_type, kernel_call_frame, cast,\n         mlir::ArrayRef<mlir::LLVM::GEPArg>{mlir::LLVM::GEPArg(0),\n                                            mlir::LLVM::GEPArg(1)},\n         mlir::LLVM::GEPNoWrapFlags::inbounds);\n     auto workgroup_ptr =\n-        builder.create<mlir::LLVM::LoadOp>(ptr_type, workgroup_gep);\n+        mlir::LLVM::LoadOp::create(builder, ptr_type, workgroup_gep);\n \n     int32_t workgroup_dim_idx = static_cast<int32_t>(op.getDimension());\n-    auto workgroup_dim_gep = builder.create<mlir::LLVM::GEPOp>(\n-        ptr_type, kernel_dim, workgroup_ptr,\n+    auto workgroup_dim_gep = mlir::LLVM::GEPOp::create(\n+        builder, ptr_type, kernel_dim, workgroup_ptr,\n         mlir::ArrayRef<mlir::LLVM::GEPArg>{\n             mlir::LLVM::GEPArg(0), mlir::LLVM::GEPArg(workgroup_dim_idx)},\n         mlir::LLVM::GEPNoWrapFlags::inbounds);\n     auto workgroup_dim_load =\n-        builder.create<mlir::LLVM::LoadOp>(i64_ty, workgroup_dim_gep);\n+        mlir::LLVM::LoadOp::create(builder, i64_ty, workgroup_dim_gep);\n     workgroup_dim_load.setInvariant(true);\n \n     mlir::Value workgroup_dim = workgroup_dim_load.getResult();\n     auto index_ty = builder.getIntegerType(\n         mlir::DataLayout::closest(builder.getInsertionBlock()->getParentOp())\n             .getTypeSizeInBits(mlir::IndexType::get(context)));\n     if (index_ty != i64_ty) {\n-      workgroup_dim = builder.create<mlir::LLVM::TruncOp>(\n-          index_ty, workgroup_dim, mlir::LLVM::IntegerOverflowFlags::nsw);\n+      workgroup_dim =\n+          mlir::LLVM::TruncOp::create(builder, index_ty, workgroup_dim,\n+                                      mlir::LLVM::IntegerOverflowFlags::nsw);\n     }\n-    auto workgroup_dim_cast = builder.create<mlir::UnrealizedConversionCastOp>(\n-        mlir::IndexType::get(context), workgroup_dim);\n+    auto workgroup_dim_cast = mlir::UnrealizedConversionCastOp::create(\n+        builder, mlir::IndexType::get(context), workgroup_dim);\n \n     rewriter.replaceOp(op, workgroup_dim_cast.getResult(0));\n \n@@ -252,8 +252,8 @@ struct RewriteFunctionSignatures : mlir::OpRewritePattern<mlir::func::FuncOp> {\n     llvm::SmallVector<mlir::Type> new_operands{ptr};\n     rewriter.setInsertionPointToStart(&op.getBody().front());\n \n-    auto cast = rewriter.create<mlir::UnrealizedConversionCastOp>(\n-        op.getLoc(), func_type.getInput(0), op.getArgument(0));\n+    auto cast = mlir::UnrealizedConversionCastOp::create(\n+        rewriter, op.getLoc(), func_type.getInput(0), op.getArgument(0));\n     op.getArgument(0).replaceAllUsesExcept(cast.getResult(0), cast);\n     op.setFunctionType(rewriter.getFunctionType(new_operands, {ptr}));\n     auto& entry = op->getRegion(0).front();\n@@ -301,8 +301,9 @@ class WrapEntryWithCallFrame\n \n     auto call_frame_type = CallFrameType::get(context);\n     auto error_type = ErrorType::get(context);\n-    mlir::func::FuncOp kernel_func = builder.create<mlir::func::FuncOp>(\n-        kernel_name, rewriter.getFunctionType({call_frame_type}, {error_type}));\n+    mlir::func::FuncOp kernel_func = mlir::func::FuncOp::create(\n+        builder, kernel_name,\n+        rewriter.getFunctionType({call_frame_type}, {error_type}));\n \n     builder.setInsertionPointToStart(kernel_func.addEntryBlock());\n \n@@ -316,7 +317,7 @@ class WrapEntryWithCallFrame\n       mlir::DictionaryAttr arg_attr =\n           arg_attrs ? mlir::dyn_cast<mlir::DictionaryAttr>(arg_attrs[idx])\n                     : nullptr;\n-      LoadOp load = builder.create<LoadOp>(arg.getType(), call_frame_arg, idx);\n+      LoadOp load = LoadOp::create(builder, arg.getType(), call_frame_arg, idx);\n       if (arg_attr) {\n         load->setAttrs(arg_attr);\n       }\n@@ -325,16 +326,17 @@ class WrapEntryWithCallFrame\n \n     for (auto workgroup_id : {WorkGroupDimension::x, WorkGroupDimension::y,\n                               WorkGroupDimension::z}) {\n-      call_args.push_back(builder.create<ExtractWorkgroupIdOp>(\n-          mlir::IndexType::get(context), call_frame_arg, workgroup_id));\n+      call_args.push_back(\n+          ExtractWorkgroupIdOp::create(builder, mlir::IndexType::get(context),\n+                                       call_frame_arg, workgroup_id));\n     }\n \n     // Use func::call here rather than pure call to avoid the entry function\n     // being DCEd.\n-    builder.create<mlir::func::CallOp>(op, call_args);\n+    mlir::func::CallOp::create(builder, op, call_args);\n \n-    auto error = builder.create<cpu::SuccessOp>(error_type);\n-    builder.create<mlir::func::ReturnOp>(error.getResult());\n+    auto error = cpu::SuccessOp::create(builder, error_type);\n+    mlir::func::ReturnOp::create(builder, error.getResult());\n \n     op->setAttr(\"xla.cpu.is_wrapped\", builder.getUnitAttr());\n     op.setPrivate();"
        },
        {
            "sha": "b82d03d47c699e74953559b24801616208557b0a",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/lower_xtile_entry.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 26,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -76,8 +76,9 @@ struct LowerXtileEntry : mlir::OpRewritePattern<xtile::EntryFuncOp> {\n       }\n     }\n \n-    auto new_func_op = rewriter.create<mlir::func::FuncOp>(\n-        op->getLoc(), op.getSymName(), op.getFunctionType(), filtered_attrs);\n+    auto new_func_op =\n+        mlir::func::FuncOp::create(rewriter, op->getLoc(), op.getSymName(),\n+                                   op.getFunctionType(), filtered_attrs);\n     new_func_op.setArgAttrsAttr(op.getArgAttrsAttr());\n \n     // Move the region from the old function to the new one.\n@@ -99,7 +100,8 @@ struct LowerXTileEntryReturn\n   mlir::LogicalResult matchAndRewrite(\n       xtile::EntryFuncReturnOp op,\n       mlir::PatternRewriter& rewriter) const override {\n-    rewriter.replaceOp(op, rewriter.create<mlir::func::ReturnOp>(op->getLoc()));\n+    rewriter.replaceOp(op,\n+                       mlir::func::ReturnOp::create(rewriter, op->getLoc()));\n     return mlir::success();\n   }\n };\n@@ -151,8 +153,8 @@ class LowerXTileEntryPass\n       auto call_frame_type = CallFrameType::get(context);\n       auto error_type = ErrorType::get(context);\n       builder.setInsertionPointToStart(module.getBody());\n-      mlir::func::FuncOp kernel_func = builder.create<mlir::func::FuncOp>(\n-          kernel_name,\n+      mlir::func::FuncOp kernel_func = mlir::func::FuncOp::create(\n+          builder, kernel_name,\n           builder.getFunctionType({call_frame_type}, {error_type}));\n \n       builder.setInsertionPointToStart(kernel_func.addEntryBlock());\n@@ -162,7 +164,7 @@ class LowerXTileEntryPass\n       llvm::SmallVector<mlir::Value> call_args;\n       for (const auto& [idx, arg] :\n            llvm::enumerate(entry_func.getBufferArgs())) {\n-        LoadOp load = builder.create<LoadOp>(arg.getType(), call_frame, idx);\n+        LoadOp load = LoadOp::create(builder, arg.getType(), call_frame, idx);\n         call_args.push_back(load);\n       }\n \n@@ -177,48 +179,49 @@ class LowerXTileEntryPass\n       int32_t tiles_per_workgroup = tile_info.getTilesPerWorkgroup();\n \n       mlir::Value tile_count_value =\n-          builder.create<mlir::arith::ConstantIndexOp>(tile_count);\n+          mlir::arith::ConstantIndexOp::create(builder, tile_count);\n       mlir::Value tiles_per_workgroup_value =\n-          builder.create<mlir::arith::ConstantIndexOp>(tiles_per_workgroup);\n-      mlir::Value workgroup_id = builder.create<ExtractWorkgroupIdOp>(\n-          builder.getIndexType(), call_frame, WorkGroupDimension::x);\n+          mlir::arith::ConstantIndexOp::create(builder, tiles_per_workgroup);\n+      mlir::Value workgroup_id = ExtractWorkgroupIdOp::create(\n+          builder, builder.getIndexType(), call_frame, WorkGroupDimension::x);\n \n       auto flags = mlir::arith::IntegerOverflowFlags::nsw |\n                    mlir::arith::IntegerOverflowFlags::nuw;\n \n       // This isn't needed for correctness as the workgroup id passed from the\n       // runtime will always be in bounds but it constrains the range which LLVM\n       // can then take advantage of.\n-      mlir::Value bounded_workgroup_id = builder.create<mlir::arith::MaxSIOp>(\n-          workgroup_id, builder.create<mlir::arith::ConstantIndexOp>(0));\n+      mlir::Value bounded_workgroup_id = mlir::arith::MaxSIOp::create(\n+          builder, workgroup_id,\n+          mlir::arith::ConstantIndexOp::create(builder, 0));\n \n-      mlir::Value start_tile_id = builder.create<mlir::arith::MulIOp>(\n-          bounded_workgroup_id, tiles_per_workgroup_value, flags);\n-      mlir::Value bounded_start_tile_id =\n-          builder.create<mlir::arith::MinSIOp>(start_tile_id, tile_count_value);\n+      mlir::Value start_tile_id = mlir::arith::MulIOp::create(\n+          builder, bounded_workgroup_id, tiles_per_workgroup_value, flags);\n+      mlir::Value bounded_start_tile_id = mlir::arith::MinSIOp::create(\n+          builder, start_tile_id, tile_count_value);\n \n-      mlir::Value end_tile_id = builder.create<mlir::arith::AddIOp>(\n-          start_tile_id, tiles_per_workgroup_value, flags);\n+      mlir::Value end_tile_id = mlir::arith::AddIOp::create(\n+          builder, start_tile_id, tiles_per_workgroup_value, flags);\n       mlir::Value bounded_end_tile_id =\n-          builder.create<mlir::arith::MinSIOp>(end_tile_id, tile_count_value);\n+          mlir::arith::MinSIOp::create(builder, end_tile_id, tile_count_value);\n \n-      mlir::Value step = builder.create<mlir::arith::ConstantIndexOp>(1);\n+      mlir::Value step = mlir::arith::ConstantIndexOp::create(builder, 1);\n \n-      auto for_op = builder.create<mlir::scf::ForOp>(bounded_start_tile_id,\n-                                                     bounded_end_tile_id, step);\n+      auto for_op = mlir::scf::ForOp::create(builder, bounded_start_tile_id,\n+                                             bounded_end_tile_id, step);\n       {\n         mlir::ImplicitLocOpBuilder body_builder(entry_func->getLoc(),\n                                                 entry_func);\n         body_builder.setInsertionPointToStart(for_op.getBody());\n \n         call_args.push_back(for_op.getInductionVar());\n \n-        body_builder.create<mlir::func::CallOp>(kernel_impl_name,\n-                                                mlir::TypeRange(), call_args);\n+        mlir::func::CallOp::create(body_builder, kernel_impl_name,\n+                                   mlir::TypeRange(), call_args);\n       }\n \n-      auto error = builder.create<cpu::SuccessOp>(error_type);\n-      builder.create<mlir::func::ReturnOp>(error.getResult());\n+      auto error = cpu::SuccessOp::create(builder, error_type);\n+      mlir::func::ReturnOp::create(builder, error.getResult());\n     }\n \n     return mlir::success();"
        },
        {
            "sha": "0eae2b15ec6e66f48904adc6b1441384505e4f48",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/lowering_utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -44,8 +44,9 @@ mlir::VectorType GetVectorType(mlir::ShapedType type) {\n mlir::TypedValue<mlir::VectorType> ReadTensorToVector(mlir::OpBuilder& builder,\n                                                       mlir::Value input) {\n   if (input.getType().isIntOrFloat()) {\n-    return builder.create<mlir::vector::FromElementsOp>(\n-        input.getLoc(), mlir::VectorType::get({}, input.getType()), input);\n+    return mlir::vector::FromElementsOp::create(\n+        builder, input.getLoc(), mlir::VectorType::get({}, input.getType()),\n+        input);\n   }\n \n   auto input_tensor =\n@@ -65,9 +66,9 @@ mlir::RankedTensorType GetTensorType(mlir::ShapedType type) {\n mlir::TypedValue<mlir::RankedTensorType> WriteVectorToTensor(\n     mlir::OpBuilder& builder, mlir::Value input) {\n   if (input.getType().isIntOrFloat()) {\n-    return builder.create<mlir::tensor::FromElementsOp>(\n-        input.getLoc(), mlir::RankedTensorType::get({}, input.getType()),\n-        input);\n+    return mlir::tensor::FromElementsOp::create(\n+        builder, input.getLoc(),\n+        mlir::RankedTensorType::get({}, input.getType()), input);\n   }\n \n   auto input_vector = mlir::cast<mlir::TypedValue<mlir::VectorType>>(input);"
        },
        {
            "sha": "913ff9426f5cfdf1ff7326a2ff6faafa0aa22c43",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/shlo_to_vector.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -180,9 +180,9 @@ struct LowerDotGeneral : mlir::OpRewritePattern<mlir::stablehlo::DotGeneralOp> {\n     mlir::ArrayAttr iterator_types = GetIteratorTypes(\n         rewriter, iterator_count, lhs_batch.size(), lhs_contracting.size());\n \n-    mlir::Value result = rewriter.create<mlir::vector::ContractionOp>(\n-        op->getLoc(), lhs_vector, rhs_vector, accumulator, indexing_maps,\n-        iterator_types);\n+    mlir::Value result = mlir::vector::ContractionOp::create(\n+        rewriter, op->getLoc(), lhs_vector, rhs_vector, accumulator,\n+        indexing_maps, iterator_types);\n \n     rewriter.replaceOp(op, WriteVectorToTensor(rewriter, result));\n \n@@ -193,16 +193,16 @@ struct LowerDotGeneral : mlir::OpRewritePattern<mlir::stablehlo::DotGeneralOp> {\n   mlir::Value GetAccumulator(mlir::OpBuilder& builder, mlir::Location loc,\n                              mlir::RankedTensorType result_type) const {\n     mlir::Type element_type = result_type.getElementType();\n-    auto zero_const = builder.create<mlir::arith::ConstantOp>(\n-        loc, element_type, builder.getZeroAttr(element_type));\n+    auto zero_const = mlir::arith::ConstantOp::create(\n+        builder, loc, element_type, builder.getZeroAttr(element_type));\n \n     if (result_type.getRank() == 0) {\n       return zero_const;\n     }\n \n     auto result_vector_type = GetVectorType(result_type);\n-    return builder.create<mlir::vector::BroadcastOp>(loc, result_vector_type,\n-                                                     zero_const);\n+    return mlir::vector::BroadcastOp::create(builder, loc, result_vector_type,\n+                                             zero_const);\n   }\n };\n \n@@ -215,8 +215,8 @@ struct LowerTranspose : mlir::OpRewritePattern<mlir::stablehlo::TransposeOp> {\n     mlir::Value source_vector = ReadTensorToVector(rewriter, op.getOperand());\n \n     mlir::TypedValue<mlir::VectorType> dest_vector =\n-        rewriter.create<mlir::vector::TransposeOp>(op->getLoc(), source_vector,\n-                                                   op.getPermutation());\n+        mlir::vector::TransposeOp::create(rewriter, op->getLoc(), source_vector,\n+                                          op.getPermutation());\n \n     mlir::Value dest_tensor = WriteVectorToTensor(rewriter, dest_vector);\n \n@@ -242,8 +242,9 @@ struct LowerReduce : mlir::OpRewritePattern<mlir::stablehlo::ReduceOp> {\n     auto result_type =\n         mlir::cast<mlir::RankedTensorType>(result_tensor.getType());\n \n-    mlir::Value init_value = rewriter.create<mlir::tensor::ExtractOp>(\n-        op->getLoc(), result_type.getElementType(), op.getInitValues().front());\n+    mlir::Value init_value = mlir::tensor::ExtractOp::create(\n+        rewriter, op->getLoc(), result_type.getElementType(),\n+        op.getInitValues().front());\n \n     // Ensure the reduction dimensions are sorted so we can easily check if the\n     // minor dimension is reduced."
        },
        {
            "sha": "b0711dba1037b27453fbb7c354f7452521dd467e",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -73,7 +73,7 @@ static void InsertValue(mlir::OpBuilder& builder, mlir::Location loc,\n   llvm::SmallVector<mlir::Value> padded_indices(indices);\n   while (padded_indices.size() < buffer.getType().getRank()) {\n     padded_indices.push_back(\n-        builder.create<mlir::arith::ConstantIndexOp>(loc, 0));\n+        mlir::arith::ConstantIndexOp::create(builder, loc, 0));\n   }\n \n   if (mlir::isa<mlir::VectorType>(value.getType())) {\n@@ -105,14 +105,14 @@ static std::array<llvm::SmallVector<mlir::Value>, 3> GetLoopBounds(\n     llvm::ArrayRef<int64_t> upper_bounds, int64_t lower_bound = 0) {\n   llvm::SmallVector<mlir::Value> lbs(\n       upper_bounds.size(),\n-      builder.create<mlir::arith::ConstantIndexOp>(loc, lower_bound));\n+      mlir::arith::ConstantIndexOp::create(builder, loc, lower_bound));\n   llvm::SmallVector<mlir::Value> ubs =\n       llvm::map_to_vector(upper_bounds, [&](int64_t size) -> mlir::Value {\n-        return builder.create<mlir::arith::ConstantIndexOp>(loc, size);\n+        return mlir::arith::ConstantIndexOp::create(builder, loc, size);\n       });\n   llvm::SmallVector<mlir::Value> step(\n       upper_bounds.size(),\n-      builder.create<mlir::arith::ConstantIndexOp>(loc, 1));\n+      mlir::arith::ConstantIndexOp::create(builder, loc, 1));\n   return {lbs, ubs, step};\n }\n "
        },
        {
            "sha": "1d7e637a0eaa9092ab9cb78eb7761bead179fa00",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/int4_passes.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -172,15 +172,15 @@ class I4ToI8Converter : public TypeConverter {\n // Divides a value by an integer constant.\n Value div(ConversionPatternRewriter& r, Value value, int64_t constant) {\n   auto const_attr = r.getIntegerAttr(value.getType(), constant);\n-  auto const_op = r.template create<ma::ConstantOp>(value.getLoc(), const_attr);\n-  return r.template create<ma::DivSIOp>(value.getLoc(), value, const_op);\n+  auto const_op = ma::ConstantOp::create(r, value.getLoc(), const_attr);\n+  return ma::DivSIOp::create(r, value.getLoc(), value, const_op);\n }\n \n // Divides a value by an integer constant.\n Value ceilDiv(ConversionPatternRewriter& r, Value value, int64_t constant) {\n   auto const_attr = r.getIntegerAttr(value.getType(), constant);\n-  auto const_op = r.template create<ma::ConstantOp>(value.getLoc(), const_attr);\n-  return r.template create<ma::CeilDivSIOp>(value.getLoc(), value, const_op);\n+  auto const_op = ma::ConstantOp::create(r, value.getLoc(), const_attr);\n+  return ma::CeilDivSIOp::create(r, value.getLoc(), value, const_op);\n }\n \n // Returns the integer value of a constant op."
        },
        {
            "sha": "22d70efcf3f33b3fce7a28879c425e2049f0f643",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/expand_float_ops.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 59,
            "changes": 118,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fexpand_float_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fexpand_float_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fexpand_float_ops.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -88,25 +88,25 @@ struct RewriteErf32Pattern : public mlir::OpRewritePattern<mlir::math::ErfOp> {\n \n     mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n     auto c = [&](float v) -> Value {\n-      return b.create<ma::ConstantFloatOp>(rewriter.getF32Type(),\n-                                           llvm::APFloat(v));\n+      return ma::ConstantFloatOp::create(b, rewriter.getF32Type(),\n+                                         llvm::APFloat(v));\n     };\n \n     auto poly = [&](auto x, auto coefficients) -> Value {\n       auto r = c(coefficients[0]);\n       for (int i = 1; i < coefficients.size(); ++i) {\n-        r = b.create<mlir::math::FmaOp>(r, x, c(coefficients[i]));\n+        r = mlir::math::FmaOp::create(b, r, x, c(coefficients[i]));\n       }\n       return r;\n     };\n \n     Value x = op.getOperand();\n-    x = b.create<ma::MaximumFOp>(x, c(-kErfInvOneMinusHalfULP));\n-    x = b.create<ma::MinimumFOp>(x, c(kErfInvOneMinusHalfULP));\n-    Value x2 = b.create<ma::MulFOp>(x, x);\n+    x = ma::MaximumFOp::create(b, x, c(-kErfInvOneMinusHalfULP));\n+    x = ma::MinimumFOp::create(b, x, c(kErfInvOneMinusHalfULP));\n+    Value x2 = ma::MulFOp::create(b, x, x);\n \n     rewriter.replaceOpWithNewOp<ma::DivFOp>(\n-        op, b.create<ma::MulFOp>(x, poly(x2, kAlpha)), poly(x2, kBeta));\n+        op, ma::MulFOp::create(b, x, poly(x2, kAlpha)), poly(x2, kBeta));\n \n     return mlir::success();\n   }\n@@ -129,39 +129,39 @@ bool IsFNUZ(mlir::FloatType ty) {\n Value IsInf(Value value, mlir::ImplicitLocOpBuilder& b) {\n   auto ty = mlir::cast<mlir::FloatType>(value.getType());\n   if (mlir::LLVM::isCompatibleOuterType(ty)) {\n-    value = b.create<mlir::math::AbsFOp>(value);\n-    Value inf = b.create<ma::ConstantFloatOp>(\n-        ty, llvm::APFloat::getInf(ty.getFloatSemantics()));\n-    return b.create<ma::CmpFOp>(ma::CmpFPredicate::OEQ, value, inf);\n+    value = mlir::math::AbsFOp::create(b, value);\n+    Value inf = ma::ConstantFloatOp::create(\n+        b, ty, llvm::APFloat::getInf(ty.getFloatSemantics()));\n+    return ma::CmpFOp::create(b, ma::CmpFPredicate::OEQ, value, inf);\n   }\n \n   assert(ty.getIntOrFloatBitWidth() <= 8);\n   // F8E5M2, F8E4M3, F8E3M4 are the only 8 bit float with infinities.\n   if (llvm::isa<mlir::Float8E5M2Type>(ty)) {\n-    Val bits{b.create<ma::BitcastOp>(b.getI8Type(), value), &b};\n+    Val bits{ma::BitcastOp::create(b, b.getI8Type(), value), &b};\n     return (bits & 0x7F) == 0x7C;\n   } else if (llvm::isa<mlir::Float8E4M3Type>(ty)) {\n-    Val bits{b.create<ma::BitcastOp>(b.getI8Type(), value), &b};\n+    Val bits{ma::BitcastOp::create(b, b.getI8Type(), value), &b};\n     return (bits & 0x7F) == 0x78;\n   } else if (llvm::isa<mlir::Float8E3M4Type>(ty)) {\n-    Val bits{b.create<ma::BitcastOp>(b.getI8Type(), value), &b};\n+    Val bits{ma::BitcastOp::create(b, b.getI8Type(), value), &b};\n     return (bits & 0x7F) == 0x70;\n   } else {\n-    return b.create<ma::ConstantIntOp>(b.getI1Type(), false);\n+    return ma::ConstantIntOp::create(b, b.getI1Type(), false);\n   }\n }\n \n Value IsNaN(Value value, mlir::ImplicitLocOpBuilder& b) {\n   auto ty = value.getType();\n   if (mlir::LLVM::isCompatibleOuterType(ty)) {\n-    return b.create<ma::CmpFOp>(ma::CmpFPredicate::UNO, value, value);\n+    return ma::CmpFOp::create(b, ma::CmpFPredicate::UNO, value, value);\n   }\n   if (llvm::isa<mlir::Float4E2M1FNType>(ty)) {\n-    return b.create<ma::ConstantIntOp>(b.getI1Type(), false);\n+    return ma::ConstantIntOp::create(b, b.getI1Type(), false);\n   }\n \n   assert(ty.getIntOrFloatBitWidth() == 8);\n-  Val bits{b.create<ma::BitcastOp>(b.getI8Type(), value), &b};\n+  Val bits{ma::BitcastOp::create(b, b.getI8Type(), value), &b};\n   if (llvm::isa<mlir::Float8E5M2Type>(ty)) {\n     return (bits & 0b0111'1111).cmp(ma::CmpIPredicate::ugt, 0b0111'1100);\n   } else if (llvm::isa<mlir::Float8E4M3Type>(ty)) {\n@@ -189,21 +189,21 @@ Value EmitReducePrecision(Value value, int exponent_bits, int mantissa_bits,\n }\n \n Value EmitF16ToF8e5m2(Value in, mlir::ImplicitLocOpBuilder& b) {\n-  Val in_bits{b.create<ma::BitcastOp>(b.getI16Type(), in), &b};\n+  Val in_bits{ma::BitcastOp::create(b, b.getI16Type(), in), &b};\n   // Use this method of checking for NaN because it's the same as what's used\n   // in the reduce precision lowering.\n   Value is_nan = (in_bits & 32767).cmp(ma::CmpIPredicate::ugt, 31744);\n \n   Value value = EmitReducePrecision(in, 5, 2, b);\n-  value = b.create<ma::BitcastOp>(b.getI16Type(), value);\n-  value = b.create<ma::ShRUIOp>(value,\n-                                b.create<ma::ConstantIntOp>(b.getI16Type(), 8));\n-  value = b.create<ma::TruncIOp>(b.getI8Type(), value);\n+  value = ma::BitcastOp::create(b, b.getI16Type(), value);\n+  value = ma::ShRUIOp::create(b, value,\n+                              ma::ConstantIntOp::create(b, b.getI16Type(), 8));\n+  value = ma::TruncIOp::create(b, b.getI8Type(), value);\n   // When the input is NaN, just truncating can turn a NaN into an inf if the\n   // mantissa becomes 0.\n-  value = b.create<ma::SelectOp>(\n-      is_nan, b.create<ma::ConstantIntOp>(value.getType(), 0x7F), value);\n-  return b.create<ma::BitcastOp>(b.getType<mlir::Float8E5M2Type>(), value);\n+  value = ma::SelectOp::create(\n+      b, is_nan, ma::ConstantIntOp::create(b, value.getType(), 0x7F), value);\n+  return ma::BitcastOp::create(b, b.getType<mlir::Float8E5M2Type>(), value);\n }\n \n Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n@@ -220,8 +220,8 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n     // Going through f32 and f16 is significantly faster than the fallback code\n     // below.\n     return EmitF16ToF8e5m2(\n-        b.create<ma::TruncFOp>(b.getF16Type(),\n-                               b.create<ma::ExtFOp>(b.getF32Type(), value)),\n+        ma::TruncFOp::create(b, b.getF16Type(),\n+                             ma::ExtFOp::create(b, b.getF32Type(), value)),\n         b);\n   }\n \n@@ -265,23 +265,23 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n       return {v, &b};\n     }\n     if (ty.getIntOrFloatBitWidth() < v.getType().getIntOrFloatBitWidth()) {\n-      return {b.create<ma::TruncIOp>(ty, v), &b};\n+      return {ma::TruncIOp::create(b, ty, v), &b};\n     }\n-    return {b.create<ma::ExtUIOp>(ty, v), &b};\n+    return {ma::ExtUIOp::create(b, ty, v), &b};\n   };\n \n   int64_t exp_offset = to_bias - from_bias;\n   int digit_shift = to_mantissa - from_mantissa;\n \n   int from_width = value.getType().getIntOrFloatBitWidth();\n-  Val from_bits{b.create<ma::BitcastOp>(b.getIntegerType(from_width), value),\n+  Val from_bits{ma::BitcastOp::create(b, b.getIntegerType(from_width), value),\n                 &b};\n   if (from_width < 8) {\n     from_bits = convert_int(b.getIntegerType(8), from_bits);\n   }\n \n   auto cst = [&](mlir::Type ty, int64_t n) -> Val {\n-    return {b.create<ma::ConstantIntOp>(ty, n), &b};\n+    return {ma::ConstantIntOp::create(b, ty, n), &b};\n   };\n \n   // Shift bits to destination type, without sign bit.\n@@ -368,8 +368,8 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n     // `From` supports larger values than `To`, we may overflow.\n     if (std::make_pair(to_max_exp, to_mantissa) <\n         std::make_pair(from_max_exp, from_mantissa)) {\n-      result = b.create<SelectOp>(\n-          rounded_from_bits.cmp(CmpIPredicate::ugt, aligned_highest), to_inf,\n+      result = SelectOp::create(\n+          b, rounded_from_bits.cmp(CmpIPredicate::ugt, aligned_highest), to_inf,\n           result);\n     }\n   }\n@@ -386,7 +386,7 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n \n     // Determine exponent in target type.\n     Value clz = convert_int(\n-        i32_ty, b.create<mlir::math::CountLeadingZerosOp>(from_bits));\n+        i32_ty, mlir::math::CountLeadingZerosOp::create(b, from_bits));\n     Value msb = cst(i32_ty, std::max(from_width, 8) - 1) - clz;\n     Value normalization_factor = cst(i32_ty, from_mantissa) - msb;\n \n@@ -408,7 +408,7 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n \n     Value biased_exp_sle_zero = biased_exponent.cmp(CmpIPredicate::sle, 0);\n     bits = Val(\n-        b.create<SelectOp>(biased_exp_sle_zero, subnormal_bits, normal_bits),\n+        SelectOp::create(b, biased_exp_sle_zero, subnormal_bits, normal_bits),\n         &b);\n     if (digit_shift >= 0) {\n       bits = bits.shl(digit_shift);\n@@ -420,7 +420,7 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n     }\n     bits = convert_int(to_int_ty, bits);\n \n-    result = b.create<SelectOp>(biased_from_exp == 0, bits, result);\n+    result = SelectOp::create(b, biased_from_exp == 0, bits, result);\n   } else if (to_min_exp > from_min_exp) {\n     // `To` supports fewer exponents near zero which means that some values in\n     // `From` may become subnormal.\n@@ -451,19 +451,19 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n             .shrui(exponent_shift_from_ty));\n     // To avoid UB, limit rounding and shifting to the full mantissa plus\n     // leading 1.\n-    positive_bits =\n-        Val(b.create<SelectOp>(\n-                exponent_shift_i32.cmp(CmpIPredicate::sle, from_mantissa + 1),\n-                positive_bits, to_zero),\n-            &b);\n+    positive_bits = Val(\n+        SelectOp::create(\n+            b, exponent_shift_i32.cmp(CmpIPredicate::sle, from_mantissa + 1),\n+            positive_bits, to_zero),\n+        &b);\n \n     Val negative_bits = convert_int(to_int_ty, rounded_from_bits)\n                             .shl(to_zero - exponent_shift_to_ty);\n     Value bits =\n-        b.create<SelectOp>(exponent_shift_i32.cmp(CmpIPredicate::sgt, 0),\n-                           positive_bits, negative_bits);\n-    result = b.create<SelectOp>(biased_to_exp.cmp(CmpIPredicate::sle, 0), bits,\n-                                result);\n+        SelectOp::create(b, exponent_shift_i32.cmp(CmpIPredicate::sgt, 0),\n+                         positive_bits, negative_bits);\n+    result = SelectOp::create(b, biased_to_exp.cmp(CmpIPredicate::sle, 0), bits,\n+                              result);\n   }\n \n   Value result_is_inf = IsInf(value, b);\n@@ -485,17 +485,17 @@ Value EmitFloatConversion(Value value, mlir::FloatType to_ty,\n   }\n \n   if (!llvm::isa<mlir::Float8E8M0FNUType>(from_ty)) {\n-    result = b.create<SelectOp>(from_bits == 0, to_zero, result);\n+    result = SelectOp::create(b, from_bits == 0, to_zero, result);\n   }\n-  result = b.create<SelectOp>(result_is_inf, to_inf, result);\n-  result = b.create<SelectOp>(input_is_nan, to_nan, result);\n+  result = SelectOp::create(b, result_is_inf, to_inf, result);\n+  result = SelectOp::create(b, input_is_nan, to_nan, result);\n \n   // Insert sign bit.\n   if (!llvm::isa<mlir::Float8E8M0FNUType>(from_ty)) {\n     Value neg_result = Val{result, &b} | (1ll << (to_int_ty.getWidth() - 1));\n-    result = b.create<SelectOp>(from_sign_bit, neg_result, result);\n+    result = SelectOp::create(b, from_sign_bit, neg_result, result);\n   }\n-  result = b.create<ma::BitcastOp>(to_ty, result);\n+  result = ma::BitcastOp::create(b, to_ty, result);\n   return result;\n }\n \n@@ -569,22 +569,22 @@ struct RewriteF8Cst : public mlir::OpRewritePattern<ma::CmpFOp> {\n     if (op.getPredicate() == ma::CmpFPredicate::UNE &&\n         mlir::matchPattern(rhs, mlir::m_ConstantFloat(&rhs_cst))) {\n       mlir::Type int_ty = rewriter.getIntegerType(lhs.getType().getWidth());\n-      Val int_value{b.create<ma::BitcastOp>(int_ty, lhs), &b};\n+      Val int_value{ma::BitcastOp::create(b, int_ty, lhs), &b};\n       int64_t constant = rhs_cst.bitcastToAPInt().getZExtValue();\n       // If we're comparing to +-0, compare the absolute values.\n       if (rhs_cst.isZero() && !IsFNUZ(lhs.getType())) {\n         int64_t mask = (1 << (lhs.getType().getWidth() - 1)) - 1;\n         int_value = int_value & mask;\n         constant &= mask;\n       }\n-      auto cst = b.create<ma::ConstantIntOp>(int_ty, constant);\n+      auto cst = ma::ConstantIntOp::create(b, int_ty, constant);\n       rewriter.replaceOpWithNewOp<ma::CmpIOp>(op, ma::CmpIPredicate::ne,\n                                               int_value, cst);\n       return mlir::success();\n     }\n \n-    auto lhs_ext = b.create<ma::ExtFOp>(b.getF32Type(), lhs);\n-    auto rhs_ext = b.create<ma::ExtFOp>(b.getF32Type(), rhs);\n+    auto lhs_ext = ma::ExtFOp::create(b, b.getF32Type(), lhs);\n+    auto rhs_ext = ma::ExtFOp::create(b, b.getF32Type(), rhs);\n     rewriter.replaceOpWithNewOp<ma::CmpFOp>(op, op->getResultTypes(),\n                                             mlir::ValueRange{lhs_ext, rhs_ext},\n                                             op->getAttrs());\n@@ -618,7 +618,7 @@ struct RewriteAbsFPattern : public mlir::OpRewritePattern<mlir::math::AbsFOp> {\n \n     mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n     mlir::Type i_ty = rewriter.getIntegerType(src.getType().getWidth());\n-    Val value{b.create<ma::BitcastOp>(i_ty, src), &b};\n+    Val value{ma::BitcastOp::create(b, i_ty, src), &b};\n     int64_t mask = (1ull << (src.getType().getWidth() - 1)) - 1;\n     value = value & mask;\n     rewriter.replaceOpWithNewOp<ma::BitcastOp>(op, src.getType(), value);\n@@ -636,7 +636,7 @@ struct RewriteIToFpPattern : public mlir::OpRewritePattern<Op> {\n       return rewriter.notifyMatchFailure(op, \"not an f8 (or less) itofp\");\n     }\n     Value to_float =\n-        rewriter.create<Op>(op.getLoc(), rewriter.getF32Type(), op.getIn());\n+        Op::create(rewriter, op.getLoc(), rewriter.getF32Type(), op.getIn());\n     rewriter.replaceOpWithNewOp<ma::TruncFOp>(op, op.getType(), to_float);\n     return mlir::success();\n   }\n@@ -652,8 +652,8 @@ struct RewriteFpToIPattern : public mlir::OpRewritePattern<Op> {\n         op.getIn().getType().getIntOrFloatBitWidth() > 8) {\n       return rewriter.notifyMatchFailure(op, \"not an f8 (or less) fptoi\");\n     }\n-    Value to_f32 = rewriter.create<ma::ExtFOp>(\n-        op.getLoc(), rewriter.getF32Type(), op.getIn());\n+    Value to_f32 = ma::ExtFOp::create(rewriter, op.getLoc(),\n+                                      rewriter.getF32Type(), op.getIn());\n     rewriter.replaceOpWithNewOp<Op>(op, op.getType(), to_f32);\n     return mlir::success();\n   }"
        },
        {
            "sha": "7f937e13a9fa82c1d2543d69311f0fb8523dd540",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/flatten_tensors.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -385,13 +385,13 @@ struct RewriteTensorInsert : OpRewritePattern<InsertOp> {\n     auto linear_index = LinearizeIndex(loc, tensor_type, op.getIndices(),\n                                        rewriter, tensor_type.getEncoding());\n     mlir::ImplicitLocOpBuilder b(loc, rewriter);\n-    auto tensor_1D = b.create<UnrealizedConversionCastOp>(\n-                          GetFlattenedType(tensor_type), tensor)\n+    auto tensor_1D = UnrealizedConversionCastOp::create(\n+                         b, GetFlattenedType(tensor_type), tensor)\n                          .getResult(0);\n     auto new_insert =\n-        b.create<InsertOp>(op.getScalar(), tensor_1D, linear_index);\n-    auto cast_to_orig_type = b.create<UnrealizedConversionCastOp>(\n-        tensor_type, new_insert.getResult());\n+        InsertOp::create(b, op.getScalar(), tensor_1D, linear_index);\n+    auto cast_to_orig_type = UnrealizedConversionCastOp::create(\n+        b, tensor_type, new_insert.getResult());\n     rewriter.replaceOp(op, cast_to_orig_type.getResult(0));\n     return mlir::success();\n   }\n@@ -411,13 +411,13 @@ struct RewriteVectorInsert : OpRewritePattern<mv::InsertOp> {\n     auto indices = mv::getAsValues(rewriter, loc, op.getMixedPosition());\n     auto linear_index = LinearizeIndex(loc, vector_type, indices, rewriter);\n     mlir::ImplicitLocOpBuilder b(loc, rewriter);\n-    auto vector_1D = b.create<UnrealizedConversionCastOp>(\n-                          GetFlattenedType(vector_type), vector)\n+    auto vector_1D = UnrealizedConversionCastOp::create(\n+                         b, GetFlattenedType(vector_type), vector)\n                          .getResult(0);\n     auto new_insert =\n-        b.create<mv::InsertOp>(op.getValueToStore(), vector_1D, linear_index);\n-    auto cast_to_orig_type = b.create<UnrealizedConversionCastOp>(\n-        vector_type, new_insert.getResult());\n+        mv::InsertOp::create(b, op.getValueToStore(), vector_1D, linear_index);\n+    auto cast_to_orig_type = UnrealizedConversionCastOp::create(\n+        b, vector_type, new_insert.getResult());\n     rewriter.replaceOp(op, cast_to_orig_type.getResult(0));\n     return mlir::success();\n   }\n@@ -435,10 +435,10 @@ struct RewriteVectorFromElements : OpRewritePattern<mv::FromElementsOp> {\n     }\n     auto loc = op.getLoc();\n     mlir::ImplicitLocOpBuilder b(loc, rewriter);\n-    auto new_from_elements = b.create<mv::FromElementsOp>(\n-        GetFlattenedType(vector_type), op.getElements());\n-    auto cast_to_orig_type = b.create<UnrealizedConversionCastOp>(\n-        vector_type, new_from_elements.getResult());\n+    auto new_from_elements = mv::FromElementsOp::create(\n+        b, GetFlattenedType(vector_type), op.getElements());\n+    auto cast_to_orig_type = UnrealizedConversionCastOp::create(\n+        b, vector_type, new_from_elements.getResult());\n     rewriter.replaceOp(op, cast_to_orig_type.getResult(0));\n     return mlir::success();\n   }\n@@ -458,14 +458,14 @@ struct RewriteAtomicRMW : OpRewritePattern<AtomicRMWOp> {\n     auto linear_index = LinearizeIndex(loc, tensor_type, op.getIndices(),\n                                        rewriter, tensor_type.getEncoding());\n     mlir::ImplicitLocOpBuilder b(loc, rewriter);\n-    auto tensor_1D = b.create<UnrealizedConversionCastOp>(\n-                          GetFlattenedType(tensor_type), tensor)\n+    auto tensor_1D = UnrealizedConversionCastOp::create(\n+                         b, GetFlattenedType(tensor_type), tensor)\n                          .getResult(0);\n-    auto new_atomic_rmw = b.create<AtomicRMWOp>(tensor_1D, linear_index);\n+    auto new_atomic_rmw = AtomicRMWOp::create(b, tensor_1D, linear_index);\n     rewriter.inlineRegionBefore(op.getRegion(),\n                                 &new_atomic_rmw.getRegion().front());\n-    auto cast_to_orig_type = b.create<UnrealizedConversionCastOp>(\n-        tensor_type, new_atomic_rmw.getResult());\n+    auto cast_to_orig_type = UnrealizedConversionCastOp::create(\n+        b, tensor_type, new_atomic_rmw.getResult());\n     rewriter.replaceOp(op, cast_to_orig_type.getResult(0));\n     return mlir::success();\n   }"
        },
        {
            "sha": "261ad5326095c01d24c7bf3a67cd6f8415a15cc7",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/lower_xla_intrinsic_lib.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_intrinsic_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_intrinsic_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_intrinsic_lib.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -74,7 +74,7 @@ mlir::func::FuncOp GetOrInsertDeclaration(mlir::PatternRewriter& rewriter,\n   rewriter.setInsertionPointToStart(module_op.getBody());\n \n   auto func_decl =\n-      rewriter.create<mlir::func::FuncOp>(module_op.getLoc(), name, func_type);\n+      mlir::func::FuncOp::create(rewriter, module_op.getLoc(), name, func_type);\n   func_decl.setPrivate();\n   return func_decl;\n }\n@@ -110,14 +110,14 @@ class LowerErfPattern : public mlir::OpRewritePattern<mlir::math::ErfOp> {\n       mlir::Type f32_type = get_vector_type(b.getF32Type());\n \n       mlir::Value input_value =\n-          b.create<mlir::arith::ExtFOp>(f32_type, op.getOperand());\n+          mlir::arith::ExtFOp::create(b, f32_type, op.getOperand());\n \n       auto erf_decl = codegen::intrinsics::Erf::GetOrInsertDeclaration(\n           rewriter, module_op_, Type::TypeFromIrType(f32_type));\n-      auto call_op = b.create<mlir::func::CallOp>(erf_decl, input_value);\n+      auto call_op = mlir::func::CallOp::create(b, erf_decl, input_value);\n \n       mlir::Value f32_result = call_op.getResult(0);\n-      mlir::Value result = b.create<mlir::arith::TruncFOp>(type, f32_result);\n+      mlir::Value result = mlir::arith::TruncFOp::create(b, type, f32_result);\n \n       rewriter.replaceOp(op, result);\n       return mlir::success();\n@@ -129,7 +129,7 @@ class LowerErfPattern : public mlir::OpRewritePattern<mlir::math::ErfOp> {\n       auto erf_decl = GetErf64Declaration(rewriter);\n \n       if (!maybe_vector_type) {\n-        auto call_op = b.create<mlir::func::CallOp>(erf_decl, op.getOperand());\n+        auto call_op = mlir::func::CallOp::create(b, erf_decl, op.getOperand());\n         rewriter.replaceOp(op, call_op->getResults());\n         return mlir::success();\n       }\n@@ -139,7 +139,7 @@ class LowerErfPattern : public mlir::OpRewritePattern<mlir::math::ErfOp> {\n         mlir::Value extracted = mlir::vector::ExtractOp::create(\n             rewriter, op.getLoc(), op.getOperand(), idx);\n         mlir::Value scalar_erf =\n-            b.create<mlir::func::CallOp>(erf_decl, extracted).getResult(0);\n+            mlir::func::CallOp::create(b, erf_decl, extracted).getResult(0);\n         scalar_erf_results.push_back(scalar_erf);\n       }\n       rewriter.replaceOpWithNewOp<mlir::vector::FromElementsOp>(\n@@ -196,7 +196,7 @@ class LowerTruncF32BF16FPattern\n         codegen::intrinsics::FpTrunc::GetOrInsertDeclaration(\n             rewriter, module_op_, src_type, dst_type);\n     auto call_op =\n-        b.create<mlir::func::CallOp>(f32_to_bf16_decl, op.getOperand());\n+        mlir::func::CallOp::create(b, f32_to_bf16_decl, op.getOperand());\n     rewriter.replaceOp(op, call_op->getResults());\n     return mlir::success();\n   }"
        },
        {
            "sha": "2d91bf339d1ec79a34570f5c871d7847154ffa86",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/lower_xla_to_scf.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_to_scf.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_to_scf.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_to_scf.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -77,13 +77,14 @@ struct RewritePredicatedInsert : mlir::OpRewritePattern<PredicatedInsertOp> {\n     rewriter.replaceOpWithNewOp<mlir::scf::IfOp>(\n         op, op.getCondition(),\n         [&](mlir::OpBuilder& b, mlir::Location loc) {\n-          b.create<mlir::scf::YieldOp>(\n-              loc, b.create<mlir::tensor::InsertOp>(\n-                        loc, op.getValue(), op.getDest(), op.getIndices())\n-                       .getResult());\n+          mlir::scf::YieldOp::create(\n+              b, loc,\n+              mlir::tensor::InsertOp::create(b, loc, op.getValue(),\n+                                             op.getDest(), op.getIndices())\n+                  .getResult());\n         },\n         [&](mlir::OpBuilder& b, mlir::Location loc) {\n-          b.create<mlir::scf::YieldOp>(loc, op.getDest());\n+          mlir::scf::YieldOp::create(b, loc, op.getDest());\n         });\n     return success();\n   }\n@@ -99,13 +100,13 @@ struct RewritePredicatedExtract : mlir::OpRewritePattern<PredicatedExtractOp> {\n     rewriter.replaceOpWithNewOp<mlir::scf::IfOp>(\n         op, op.getCondition(),\n         [&](mlir::OpBuilder& b, mlir::Location loc) {\n-          b.create<mlir::scf::YieldOp>(\n-              loc, b.create<mlir::tensor::ExtractOp>(loc, op.getSrc(),\n-                                                     op.getIndices())\n-                       .getResult());\n+          mlir::scf::YieldOp::create(b, loc,\n+                                     mlir::tensor::ExtractOp::create(\n+                                         b, loc, op.getSrc(), op.getIndices())\n+                                         .getResult());\n         },\n         [&](mlir::OpBuilder& b, mlir::Location loc) {\n-          b.create<mlir::scf::YieldOp>(loc, op.getFallback());\n+          mlir::scf::YieldOp::create(b, loc, op.getFallback());\n         });\n     return success();\n   }\n@@ -222,8 +223,8 @@ struct RewriteXlaLoop : mlir::OpRewritePattern<LoopOp> {\n           mlir::ImplicitLocOpBuilder nested_b(loc, nested_builder);\n           auto is_in_bounds = emitters::CheckConstraints(\n               indexing_map, op.getDims(), symbol_values, nested_b);\n-          auto if_op = nested_b.create<mlir::scf::IfOp>(\n-              is_in_bounds,\n+          auto if_op = mlir::scf::IfOp::create(\n+              nested_b, is_in_bounds,\n               [&](OpBuilder& then_builder, Location then_loc) -> void {\n                 ImplicitLocOpBuilder then_b(then_loc, then_builder);\n                 mlir::IRMapping mapping;"
        },
        {
            "sha": "f151bce51ba1c811b36f16cb94d628b01c23ea53",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/unswitch_loops.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Funswitch_loops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Funswitch_loops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Funswitch_loops.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -57,28 +57,31 @@ struct UnswitchLoop : mlir::OpRewritePattern<mlir::scf::ForOp> {\n       return rewriter.notifyMatchFailure(op, \"condition is a constant\");\n     }\n \n-    auto true_cst = rewriter.create<mlir::arith::ConstantOp>(\n-        op.getLoc(), rewriter.getIntegerAttr(rewriter.getI1Type(), 1));\n-    auto false_cst = rewriter.create<mlir::arith::ConstantOp>(\n-        op.getLoc(), rewriter.getIntegerAttr(rewriter.getI1Type(), 0));\n+    auto true_cst = mlir::arith::ConstantOp::create(\n+        rewriter, op.getLoc(),\n+        rewriter.getIntegerAttr(rewriter.getI1Type(), 1));\n+    auto false_cst = mlir::arith::ConstantOp::create(\n+        rewriter, op.getLoc(),\n+        rewriter.getIntegerAttr(rewriter.getI1Type(), 0));\n     rewriter.setInsertionPoint(op);\n     mlir::IRMapping mapping;\n     mapping.map(if_op.getCondition(), false_cst);\n     auto false_branch_loop = op->clone(mapping);\n-    auto new_if = rewriter.create<mlir::scf::IfOp>(\n-        op.getLoc(), op.getResultTypes(), if_op.getCondition(), true, true);\n+    auto new_if =\n+        mlir::scf::IfOp::create(rewriter, op.getLoc(), op.getResultTypes(),\n+                                if_op.getCondition(), true, true);\n     rewriter.replaceAllUsesWith(op.getResults(), new_if.getResults());\n \n     auto then_builder = new_if.getThenBodyBuilder(rewriter.getListener());\n     auto then_yield =\n-        then_builder.create<mlir::scf::YieldOp>(op.getLoc(), op.getResults());\n+        mlir::scf::YieldOp::create(then_builder, op.getLoc(), op.getResults());\n     rewriter.moveOpBefore(op, then_yield);\n     rewriter.modifyOpInPlace(if_op, [&]() { if_op->setOperand(0, true_cst); });\n \n     auto else_builder = new_if.getElseBodyBuilder(rewriter.getListener());\n     else_builder.insert(false_branch_loop);\n-    else_builder.create<mlir::scf::YieldOp>(op.getLoc(),\n-                                            false_branch_loop->getResults());\n+    mlir::scf::YieldOp::create(else_builder, op.getLoc(),\n+                               false_branch_loop->getResults());\n \n     return mlir::success();\n   }"
        },
        {
            "sha": "63bb6361e55939bd0c960bc569ecf34f37cdb37b",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/vectorize_loads_stores.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fvectorize_loads_stores.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fvectorize_loads_stores.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fvectorize_loads_stores.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -198,7 +198,7 @@ std::optional<Value> GetVectorBaseIndices(Value index, scf::ForOp loop,\n                                           mlir::ImplicitLocOpBuilder& b) {\n   Value induction_var = loop.getInductionVar();\n   if (index == induction_var) {\n-    return b.create<arith::ConstantIndexOp>(0);\n+    return arith::ConstantIndexOp::create(b, 0);\n   }\n \n   auto apply_indexing =\n@@ -248,9 +248,9 @@ std::optional<Value> GetVectorBaseIndices(Value index, scf::ForOp loop,\n   }\n \n   auto operands = llvm::to_vector(apply_indexing.getOperands());\n-  operands[induction_var_operand_index] = b.create<arith::ConstantIndexOp>(0);\n+  operands[induction_var_operand_index] = arith::ConstantIndexOp::create(b, 0);\n \n-  return b.create<ApplyIndexingOp>(operands, apply_indexing.getIndexingMap())\n+  return ApplyIndexingOp::create(b, operands, apply_indexing.getIndexingMap())\n       ->getResult(0);\n }\n \n@@ -287,8 +287,8 @@ struct VectorizeLoad : mlir::OpRewritePattern<mlir::tensor::ExtractOp> {\n       return rewriter.notifyMatchFailure(\n           op, \"the instruction does not access contiguous elements\");\n     }\n-    auto loaded_vector = b.create<mlir::vector::TransferReadOp>(\n-        vector_type, op.getTensor(), *vector_index, /*padding=*/std::nullopt,\n+    auto loaded_vector = mlir::vector::TransferReadOp::create(\n+        b, vector_type, op.getTensor(), *vector_index, /*padding=*/std::nullopt,\n         llvm::ArrayRef<bool>{true});\n     rewriter.replaceOpWithNewOp<mlir::vector::ExtractOp>(\n         op, loaded_vector, loop.getInductionVar());\n@@ -356,15 +356,15 @@ class VectorizeAtomicRMW : public mlir::OpRewritePattern<AtomicRMWOp> {\n     }\n \n     auto init =\n-        b.create<arith::ConstantOp>(b.getZeroAttr(vector_type)).getResult();\n+        arith::ConstantOp::create(b, b.getZeroAttr(vector_type)).getResult();\n \n     auto yield_fn = [&](mlir::OpBuilder& yield_b, mlir::Location yield_loc,\n                         llvm::ArrayRef<mlir::BlockArgument> bbarg) {\n       auto induction_var =\n           mlir::cast<scf::ForOp>(bbarg.front().getOwner()->getParentOp())\n               .getInductionVar();\n-      auto insert_op = yield_b.create<mlir::vector::InsertOp>(\n-          yield_loc, atomic_modifier_parameters->first, bbarg.front(),\n+      auto insert_op = mlir::vector::InsertOp::create(\n+          yield_b, yield_loc, atomic_modifier_parameters->first, bbarg.front(),\n           induction_var);\n       return llvm::SmallVector<Value>{insert_op.getResult()};\n     };\n@@ -377,14 +377,14 @@ class VectorizeAtomicRMW : public mlir::OpRewritePattern<AtomicRMWOp> {\n     rewriter.replaceOp(op, op->getOpOperand(0).get());\n \n     auto filled_vector = new_for->getResults().back();\n-    auto new_atomic_rmw = b.create<AtomicRMWOp>(\n-        new_for.getInits()[result_index], *vector_index, vector_type);\n+    auto new_atomic_rmw = AtomicRMWOp::create(\n+        b, new_for.getInits()[result_index], *vector_index, vector_type);\n     mlir::ImplicitLocOpBuilder body_builder(new_atomic_rmw.getLoc(),\n                                             new_atomic_rmw.getBodyBuilder());\n-    auto addf_op = body_builder.create<arith::AddFOp>(\n-        body_builder.getLoc(), vector_type, new_atomic_rmw.getCurrentValue(),\n-        filled_vector);\n-    body_builder.create<xla::YieldOp>(addf_op.getResult());\n+    auto addf_op =\n+        arith::AddFOp::create(body_builder, body_builder.getLoc(), vector_type,\n+                              new_atomic_rmw.getCurrentValue(), filled_vector);\n+    xla::YieldOp::create(body_builder, addf_op.getResult());\n     new_for->getResult(result_index)\n         .replaceAllUsesWith(new_atomic_rmw.getResult());\n \n@@ -422,15 +422,15 @@ struct VectorizeStore : mlir::OpRewritePattern<mlir::tensor::InsertOp> {\n     }\n \n     auto init =\n-        b.create<arith::ConstantOp>(b.getZeroAttr(vector_type)).getResult();\n+        arith::ConstantOp::create(b, b.getZeroAttr(vector_type)).getResult();\n \n     auto yield_fn = [&](mlir::OpBuilder& yield_b, mlir::Location yield_loc,\n                         llvm::ArrayRef<mlir::BlockArgument> bbarg) {\n       auto induction_var =\n           mlir::cast<scf::ForOp>(bbarg.front().getOwner()->getParentOp())\n               .getInductionVar();\n-      auto insert_op = yield_b.create<mlir::vector::InsertOp>(\n-          yield_loc, op.getScalar(), bbarg.front(), induction_var);\n+      auto insert_op = mlir::vector::InsertOp::create(\n+          yield_b, yield_loc, op.getScalar(), bbarg.front(), induction_var);\n       return llvm::SmallVector<Value>{insert_op.getResult()};\n     };\n     int result_index = op->use_begin()->getOperandNumber();\n@@ -442,8 +442,8 @@ struct VectorizeStore : mlir::OpRewritePattern<mlir::tensor::InsertOp> {\n     rewriter.replaceOp(op, op.getDest());\n \n     auto filled_vector = new_for->getResults().back();\n-    auto written = b.create<mlir::vector::TransferWriteOp>(\n-        filled_vector, new_for.getInits()[result_index], *vector_index,\n+    auto written = mlir::vector::TransferWriteOp::create(\n+        b, filled_vector, new_for.getInits()[result_index], *vector_index,\n         llvm::ArrayRef<bool>{true});\n     new_for->getResult(result_index).replaceAllUsesWith(written.getResult());\n "
        },
        {
            "sha": "27e673ac05c711d081de15275501c4b2df68ada6",
            "filename": "third_party/xla/xla/codegen/intrinsic/intrinsic.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fintrinsic.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fintrinsic.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Fintrinsic.h?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -137,7 +137,7 @@ class Intrinsic {\n     mlir::OpBuilder::InsertionGuard guard(b);\n     b.setInsertionPointToStart(module.getBody());\n \n-    auto decl = b.create<mlir::func::FuncOp>(module.getLoc(), name, type);\n+    auto decl = mlir::func::FuncOp::create(b, module.getLoc(), name, type);\n     decl.setPrivate();\n     return decl;\n   }"
        },
        {
            "sha": "8054b819c4a1798036920a41ee3486b96776f4e0",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_stablehlo_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_stablehlo_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_stablehlo_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_stablehlo_pass.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -156,7 +156,7 @@ class AutoShardingWrapperPass\n     mlir::OpBuilder builder(context);\n \n     auto original_mesh_op =\n-        builder.create<sdy::MeshOp>(module_op.getLoc(), \"mesh\", sdy_mesh);\n+        sdy::MeshOp::create(builder, module_op.getLoc(), \"mesh\", sdy_mesh);\n     symbol_table.insert(original_mesh_op, module_op.getBody()->begin());\n     mlir::PassManager dedup_pm(context);\n     dedup_pm.addPass(xla::sdy::createSdyRoundTripDedupMeshesPass());"
        },
        {
            "sha": "c091d6e9d645fc6217eea486eb9a6754a5314be5",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/async_importer.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fasync_importer.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -127,18 +127,18 @@ absl::StatusOr<mlir::Operation*> ImportOldStyleAsyncStart(\n       async_builder\n           .createBlock(&function.getBody(), {}, Untuple(result_types[0]), locs)\n           ->getArguments();\n-  auto sync_operation = async_builder.create<sync_op>(\n-      loc, Untuple(result_types[1]), sync_operand, attributes);\n-  async_builder.create<mlir::func::ReturnOp>(loc, sync_operation->getResults());\n+  auto sync_operation = sync_op::create(\n+      async_builder, loc, Untuple(result_types[1]), sync_operand, attributes);\n+  mlir::func::ReturnOp::create(async_builder, loc,\n+                               sync_operation->getResults());\n   TF_RETURN_IF_ERROR(mutate_op(sync_operation));\n \n   function->setAttr(kExecutionThread, builder->getStringAttr(\"main\"));\n \n   auto bundle_result_type =\n       mlir::mhlo::AsyncBundleType::get(context, result_types);\n-  return builder\n-      ->create<mlir::mhlo::AsyncStartOp>(loc, bundle_result_type, operands,\n-                                         async_attributes)\n+  return mlir::mhlo::AsyncStartOp::create(*builder, loc, bundle_result_type,\n+                                          operands, async_attributes)\n       .getOperation();\n }\n \n@@ -166,13 +166,13 @@ absl::StatusOr<mlir::Operation*> ImportOldStyleAsyncDone(\n   auto start_tuple =\n       llvm::dyn_cast<mlir::TupleType>(async_bundle.getTypes()[1]);\n   if (start_tuple && llvm::isa<mlir::TupleType>(start_tuple.getType(0))) {\n-    auto op = builder->create<mlir::mhlo::AsyncDoneOp>(loc, result_type,\n-                                                       operands, attributes);\n+    auto op = mlir::mhlo::AsyncDoneOp::create(*builder, loc, result_type,\n+                                              operands, attributes);\n     return {op};\n   }\n   if (useBundleResult) result_type = async_bundle.getTypes()[1];\n-  auto op = builder->create<mlir::mhlo::AsyncDoneOp>(loc, Untuple(result_type),\n-                                                     operands, attributes);\n+  auto op = mlir::mhlo::AsyncDoneOp::create(*builder, loc, Untuple(result_type),\n+                                            operands, attributes);\n   return CreateTupleFromOpResults(builder, loc, op.getOperation(), result_type);\n }\n \n@@ -233,9 +233,8 @@ absl::StatusOr<mlir::Operation*> ImportSend(\n   if (args.size() == 2 && IsEmptyTuple(args[0].getType())) {\n     args = args.drop_front(1);\n   }\n-  auto send = builder\n-                  ->create<mlir::stablehlo::SendOp>(loc, token.getType(), args,\n-                                                    attributes)\n+  auto send = mlir::stablehlo::SendOp::create(*builder, loc, token.getType(),\n+                                              args, attributes)\n                   .getOperation();\n   if (instruction->has_sharding()) {\n     const HloSharding& sharding = instruction->sharding();\n@@ -305,8 +304,9 @@ absl::StatusOr<mlir::Operation*> ImportRecv(\n \n   // Return recv op for non-pipelined send, skip empty tuple result type\n   if (!IsEmptyTuple(result_types[0])) {\n-    auto recv = builder->create<mlir::stablehlo::RecvOp>(\n-        loc, llvm::SmallVector<mlir::Type>{result_types[0], result_types[2]},\n+    auto recv = mlir::stablehlo::RecvOp::create(\n+        *builder, loc,\n+        llvm::SmallVector<mlir::Type>{result_types[0], result_types[2]},\n         operands, attributes);\n     if (instruction->has_sharding()) {\n       const HloSharding& sharding = instruction->sharding();\n@@ -328,14 +328,14 @@ absl::StatusOr<mlir::Operation*> ImportRecv(\n \n   // Recv with no result, only token.\n   // To keep parity, if op only returns token, wrap in tuple<tuple<>, token>\n-  auto recv = builder->create<mlir::stablehlo::RecvOp>(\n-      loc, llvm::SmallVector<mlir::Type>{result_types[2]}, operands,\n+  auto recv = mlir::stablehlo::RecvOp::create(\n+      *builder, loc, llvm::SmallVector<mlir::Type>{result_types[2]}, operands,\n       attributes);\n-  auto empty_tuple = builder->create<mlir::stablehlo::TupleOp>(\n-      loc, llvm::ArrayRef<mlir::Value>{});\n+  auto empty_tuple = mlir::stablehlo::TupleOp::create(\n+      *builder, loc, llvm::ArrayRef<mlir::Value>{});\n \n-  return builder->create<mlir::stablehlo::TupleOp>(\n-      loc,\n+  return mlir::stablehlo::TupleOp::create(\n+      *builder, loc,\n       llvm::ArrayRef<mlir::Value>{empty_tuple.getResult(), recv.getResult(0)});\n }\n "
        },
        {
            "sha": "f3a7f691b703fd4101a5bc044474622b00eca050",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/custom_call_importer.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 18,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fcustom_call_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fcustom_call_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fcustom_call_importer.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -65,10 +65,9 @@ absl::StatusOr<mlir::Operation*> ImportDynamicBroadcastInDimOp(\n         mlir::cast<mlir::IntegerAttr>(broadcast_dimension).getInt();\n   }\n \n-  return builder\n-      ->create<mlir::stablehlo::DynamicBroadcastInDimOp>(\n-          loc, result_type, operands[0], operands[1],\n-          builder->getDenseI64ArrayAttr(broadcast_dimensions))\n+  return mlir::stablehlo::DynamicBroadcastInDimOp::create(\n+             *builder, loc, result_type, operands[0], operands[1],\n+             builder->getDenseI64ArrayAttr(broadcast_dimensions))\n       .getOperation();\n }\n \n@@ -78,8 +77,8 @@ absl::StatusOr<mlir::Operation*> ImportDynamicReshapeOp(\n   if (!backend_config.empty()) {\n     return Internal(\"backend_config attribute must be empty.\");\n   }\n-  return builder\n-      ->create<mlir::stablehlo::DynamicReshapeOp>(loc, result_type, operands)\n+  return mlir::stablehlo::DynamicReshapeOp::create(*builder, loc, result_type,\n+                                                   operands)\n       .getOperation();\n }\n \n@@ -89,8 +88,8 @@ absl::StatusOr<mlir::Operation*> ImportRealDynamicSliceOp(\n   if (!backend_config.empty()) {\n     return Internal(\"backend_config attribute must be empty.\");\n   }\n-  return builder\n-      ->create<mlir::stablehlo::RealDynamicSliceOp>(loc, result_type, operands)\n+  return mlir::stablehlo::RealDynamicSliceOp::create(*builder, loc, result_type,\n+                                                     operands)\n       .getOperation();\n }\n \n@@ -185,20 +184,18 @@ absl::StatusOr<mlir::Operation*> ImportCustomCallAsOp(\n   }\n \n   if (custom_call_target == \"mhlo.uniform_quantize\") {\n-    return builder\n-        ->create<mlir::stablehlo::UniformQuantizeOp>(\n-            loc,\n-            mlir::RankedTensorType::get(\n-                mlir::cast<mlir::RankedTensorType>(result_type).getShape(),\n-                getQuantizedType(backend_config)),\n-            operands)\n+    return mlir::stablehlo::UniformQuantizeOp::create(\n+               *builder, loc,\n+               mlir::RankedTensorType::get(\n+                   mlir::cast<mlir::RankedTensorType>(result_type).getShape(),\n+                   getQuantizedType(backend_config)),\n+               operands)\n         .getOperation();\n   }\n \n   if (custom_call_target == \"mhlo.uniform_dequantize\") {\n-    return builder\n-        ->create<mlir::stablehlo::UniformDequantizeOp>(loc, result_type,\n-                                                       operands)\n+    return mlir::stablehlo::UniformDequantizeOp::create(*builder, loc,\n+                                                        result_type, operands)\n         .getOperation();\n   }\n   return InvalidArgument(\"Unsupported MHLO op custom_call %s\","
        },
        {
            "sha": "380d72b54d9c6a85e62e511923e0449286222234",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/hlo_utils.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_utils.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -203,7 +203,8 @@ mlir::Value CreateTupleValue(mlir::OpBuilder* func_builder, mlir::Location loc,\n         CreateTupleValue(func_builder, loc, flatten_values, child_type));\n   }\n \n-  return func_builder->create<mlir::stablehlo::TupleOp>(loc, flatten_sub_values)\n+  return mlir::stablehlo::TupleOp::create(*func_builder, loc,\n+                                          flatten_sub_values)\n       .getResult();\n }\n "
        },
        {
            "sha": "913635ba3d209bcd40948a9427bf796b91418813",
            "filename": "third_party/xla/xla/mlir/framework/transforms/outline_with_xla_framework.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fmlir%2Fframework%2Ftransforms%2Foutline_with_xla_framework.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fmlir%2Fframework%2Ftransforms%2Foutline_with_xla_framework.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir%2Fframework%2Ftransforms%2Foutline_with_xla_framework.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -103,9 +103,9 @@ struct OutlineXLAFunc : public RewritePattern {\n \n     // The wrapper function will have the same name but with _xla_framework\n     // appended and will be annotated with the attribute \"xla_entry\".\n-    auto outline_func = rewriter.create<func::FuncOp>(\n-        loc, func.getSymName().str() + \"_xla_framework\", func_type, attrs,\n-        arg_attrs);\n+    auto outline_func = func::FuncOp::create(\n+        rewriter, loc, func.getSymName().str() + \"_xla_framework\", func_type,\n+        attrs, arg_attrs);\n     outline_func->setAttr(\"outlined\", BoolAttr::get(ctx, true));\n     outline_func->setAttr(\"xla_entry\", BoolAttr::get(ctx, true));\n     auto *b = rewriter.createBlock(&outline_func.getBody(), {},\n@@ -114,20 +114,20 @@ struct OutlineXLAFunc : public RewritePattern {\n     // Unwrap arguments\n     SmallVector<Value> args;\n     for (const auto &t : llvm::enumerate(func.getFunctionType().getInputs())) {\n-      args.push_back(rewriter.create<xla_framework::XLABufferToMemOp>(\n-          loc, t.value(), b->getArgument(t.index())));\n+      args.push_back(xla_framework::XLABufferToMemOp::create(\n+          rewriter, loc, t.value(), b->getArgument(t.index())));\n     }\n \n-    auto call = rewriter.create<func::CallOp>(\n-        loc, func.getSymName(), func.getFunctionType().getResults(), args);\n+    auto call = func::CallOp::create(rewriter, loc, func.getSymName(),\n+                                     func.getFunctionType().getResults(), args);\n     // Wrap results\n     SmallVector<Value> results;\n     for (auto t : call.getResults()) {\n-      results.push_back(rewriter.create<xla_framework::MemToXLABufferOp>(\n-          loc, ::mlir::xla_framework::BufferType::get(ctx), t));\n+      results.push_back(xla_framework::MemToXLABufferOp::create(\n+          rewriter, loc, ::mlir::xla_framework::BufferType::get(ctx), t));\n     }\n \n-    rewriter.create<func::ReturnOp>(loc, results);\n+    func::ReturnOp::create(rewriter, loc, results);\n \n     // Finally, mark the called function as private to prevent users from\n     // accidentally trying to use it."
        },
        {
            "sha": "cabf3d31fb707c4b9fd22766876fd4a87e3e0d94",
            "filename": "third_party/xla/xla/mlir/framework/transforms/xla_framework_to_llvm_pass.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 17,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fmlir%2Fframework%2Ftransforms%2Fxla_framework_to_llvm_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fmlir%2Fframework%2Ftransforms%2Fxla_framework_to_llvm_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir%2Fframework%2Ftransforms%2Fxla_framework_to_llvm_pass.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -79,8 +79,9 @@ struct BarePtrFuncOpConversion : public ConvertOpToLLVMPattern<func::FuncOp> {\n   Value LoadValue(ConversionPatternRewriter &rewriter, Location loc,\n                   Value pointer, Value index) const {\n     auto ptr = LLVM::LLVMPointerType::get(rewriter.getContext());\n-    return rewriter.create<LLVM::LoadOp>(\n-        loc, ptr, rewriter.create<LLVM::GEPOp>(loc, ptr, ptr, pointer, index));\n+    return LLVM::LoadOp::create(\n+        rewriter, loc, ptr,\n+        LLVM::GEPOp::create(rewriter, loc, ptr, ptr, pointer, index));\n   }\n \n   mlir::func::FuncOp convertFuncOpToLLVMFuncOp(\n@@ -101,8 +102,9 @@ struct BarePtrFuncOpConversion : public ConvertOpToLLVMPattern<func::FuncOp> {\n     if (!llvm_type) return nullptr;\n \n     rewriter.setInsertionPoint(funcOp);\n-    auto new_func_op = rewriter.create<mlir::func::FuncOp>(\n-        loc, funcOp.getName(), llvm_type, llvm::SmallVector<NamedAttribute>());\n+    auto new_func_op =\n+        mlir::func::FuncOp::create(rewriter, loc, funcOp.getName(), llvm_type,\n+                                   llvm::SmallVector<NamedAttribute>());\n     auto locs = llvm::SmallVector<mlir::Location>(arg_types.size(), loc);\n     Block *new_entry =\n         rewriter.createBlock(&new_func_op.getBody(), {}, arg_types, locs);\n@@ -118,16 +120,18 @@ struct BarePtrFuncOpConversion : public ConvertOpToLLVMPattern<func::FuncOp> {\n     auto result_index = 0;\n     for (unsigned i = 0; i < num_refs; ++i) {\n       if (funcOp.getArgAttr(i, \"xla_framework.input_mapping\")) {\n-        Value index = rewriter.create<LLVM::ConstantOp>(\n-            loc, typeConverter->convertType(rewriter.getIntegerType(32)),\n+        Value index = LLVM::ConstantOp::create(\n+            rewriter, loc,\n+            typeConverter->convertType(rewriter.getIntegerType(32)),\n             funcOp.getArgAttrOfType<mlir::IntegerAttr>(\n                 i, \"xla_framework.input_mapping\"));\n \n         Value ptr = LoadValue(rewriter, loc, new_entry->getArgument(3), index);\n         mapping.map(funcOp.front().getArgument(i), ptr);\n       } else {\n-        Value index = rewriter.create<LLVM::ConstantOp>(\n-            loc, typeConverter->convertType(rewriter.getIntegerType(32)),\n+        Value index = LLVM::ConstantOp::create(\n+            rewriter, loc,\n+            typeConverter->convertType(rewriter.getIntegerType(32)),\n             funcOp->getAttrOfType<mlir::IntegerAttr>(\n                 \"xla_framework.result_mapping\"));\n         Value first_load =\n@@ -136,8 +140,9 @@ struct BarePtrFuncOpConversion : public ConvertOpToLLVMPattern<func::FuncOp> {\n         // Handle multi-value results which are wrapped in a tuple.\n         if (funcOp->hasAttr(\"xla_framework.result_inner_mapping\")) {\n           auto current_index = result_index++;\n-          Value inner_index = rewriter.create<LLVM::ConstantOp>(\n-              loc, typeConverter->convertType(rewriter.getIntegerType(32)),\n+          Value inner_index = LLVM::ConstantOp::create(\n+              rewriter, loc,\n+              typeConverter->convertType(rewriter.getIntegerType(32)),\n               rewriter.getI32IntegerAttr(static_cast<int32_t>(\n                   mlir::cast<mlir::IntegerAttr>(\n                       funcOp\n@@ -152,13 +157,14 @@ struct BarePtrFuncOpConversion : public ConvertOpToLLVMPattern<func::FuncOp> {\n           mapping.map(funcOp.front().getArgument(i), ptr);\n \n           auto ptr_type = LLVM::LLVMPointerType::get(rewriter.getContext());\n-          Value second_index = rewriter.create<LLVM::ConstantOp>(\n-              loc, typeConverter->convertType(rewriter.getIntegerType(32)),\n+          Value second_index = LLVM::ConstantOp::create(\n+              rewriter, loc,\n+              typeConverter->convertType(rewriter.getIntegerType(32)),\n               rewriter.getI32IntegerAttr(current_index));\n-          rewriter.create<LLVM::StoreOp>(\n-              loc, ptr,\n-              rewriter.create<LLVM::GEPOp>(loc, ptr_type, ptr_type, first_load,\n-                                           llvm::ArrayRef(second_index)));\n+          LLVM::StoreOp::create(\n+              rewriter, loc, ptr,\n+              LLVM::GEPOp::create(rewriter, loc, ptr_type, ptr_type, first_load,\n+                                  llvm::ArrayRef(second_index)));\n \n         } else {\n           // Non tuple outputs can be simply mapped to the first load op.\n@@ -171,7 +177,7 @@ struct BarePtrFuncOpConversion : public ConvertOpToLLVMPattern<func::FuncOp> {\n     // return values now.\n     for (auto &op : funcOp.front()) {\n       if (isa<mlir::func::ReturnOp>(op)) {\n-        rewriter.create<mlir::func::ReturnOp>(loc, ValueRange());\n+        mlir::func::ReturnOp::create(rewriter, loc, ValueRange());\n       } else {\n         rewriter.clone(op, mapping);\n       }"
        },
        {
            "sha": "617c529add6407b305a3d8dbac7075b2041d7fa9",
            "filename": "third_party/xla/xla/mlir_hlo/utils/hlo_utils.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Fhlo_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Fhlo_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Futils%2Fhlo_utils.h?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -126,7 +126,7 @@ static Value getConstantLike(OpBuilder& b, Location loc, T constant,\n       return complex::NumberAttr::get(complexTy, constant, 0);\n     llvm_unreachable(\"unhandled element type\");\n   };\n-  return b.create<ConstantLikeOp>(loc, cast<TypedAttr>(getAttr()), val);\n+  return ConstantLikeOp::create(b, loc, cast<TypedAttr>(getAttr()), val);\n }\n \n Value getConstantLike(OpBuilder& b, Location loc, const APFloat& constant,"
        },
        {
            "sha": "f085752125d0ed1a2daf7f3f63da78d951b7a509",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/ifrt_merge_reshards_pass.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_merge_reshards_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/84ad581652911f91120f53bb07bcaeed812f34a3/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_merge_reshards_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fifrt_merge_reshards_pass.cc?ref=84ad581652911f91120f53bb07bcaeed812f34a3",
            "patch": "@@ -133,13 +133,13 @@ bool MergeReshardsIgnoringControlDependencies(mlir::func::FuncOp func_op) {\n     // order after the merge.\n     rewriter.setInsertionPoint(reshards.back());\n     auto merged_reshard =\n-        rewriter.create<ReshardOp>(rewriter.getFusedLoc(locs),\n-                                   /*outputs=*/output_types,\n-                                   /*control_output=*/\n-                                   IfrtControlType::get(rewriter.getContext()),\n-                                   /*inputs=*/inputs,\n-                                   /*donated=*/reshards.front().getDonated(),\n-                                   /*control_inputs=*/mlir::ValueRange());\n+        ReshardOp::create(rewriter, rewriter.getFusedLoc(locs),\n+                          /*outputs=*/output_types,\n+                          /*control_output=*/\n+                          IfrtControlType::get(rewriter.getContext()),\n+                          /*inputs=*/inputs,\n+                          /*donated=*/reshards.front().getDonated(),\n+                          /*control_inputs=*/mlir::ValueRange());\n \n     // Replace the original reshards with the new merged reshard.\n     for (auto [index, reshard] : llvm::enumerate(reshards)) {"
        }
    ],
    "stats": {
        "total": 1734,
        "additions": 879,
        "deletions": 855
    }
}