{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Replace deprecated constructor usage for absl::MutexLock\n\nabsl::MutexLock constructor by pointer is deprecated in favour of the one\nthrough reference. This commit is a simple change for removal of the addressof\noperator.\n\nChanges were done through:\n```\nag -l 'absl::MutexLock.*&' third_party/tensorflow/compiler/xla/backends/gpu | \\\n  xargs sed -i 's/absl::MutexLock\\(.*\\)&\\(.*\\)/absl::MutexLock\\1\\2/g'\n```\n\nPiperOrigin-RevId: 807549670",
    "sha": "c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
    "files": [
        {
            "sha": "985084b1d10db5e36c7fb13b5b4692a93744aa94",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_cliques.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_cliques.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -156,7 +156,7 @@ static void GpuCliqueHeartBeatMonitorThread() {\n   while (true) {\n     absl::SleepFor(absl::Seconds(30));\n     ProcessGpuCliques& cliques = GetProcessGpuCliques();\n-    absl::MutexLock lock(&cliques.mu);\n+    absl::MutexLock lock(cliques.mu);\n     VLOG(5) << \"Checking GPU communicators for errors\"\n             << \"; num_cliques=\" << cliques.map.size();\n     for (auto& [clique_key, lockable_clique] : cliques.map) {\n@@ -317,7 +317,7 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         clique_ids.fingerprint(), peer_access_enabled);\n \n     ProcessGpuCliques& cliques = GetProcessGpuCliques();\n-    absl::MutexLock lock(&cliques.mu);\n+    absl::MutexLock lock(cliques.mu);\n \n     // Create a new clique with given clique key and communicators.\n     auto emplaced =\n@@ -491,7 +491,7 @@ InitializeGpuClique(GpuCollectives* collectives, se::StreamExecutor* device,\n         absl::StrJoin(rank_mapping, \",\", rank_mapping_formatter));\n \n     ProcessGpuCliques& cliques = GetProcessGpuCliques();\n-    absl::MutexLock lock(&cliques.mu);\n+    absl::MutexLock lock(cliques.mu);\n \n     // Create a new clique with given clique key and communicators.\n     auto emplaced =\n@@ -562,7 +562,7 @@ absl::StatusOr<std::shared_ptr<LockableGpuClique::Lock>> AcquireGpuClique(\n \n             // Returns nullptr if we do not have a clique for `clique_key`.\n             auto lockable_clique = [&]() -> LockableGpuClique* {\n-              absl::MutexLock lock(&cliques.mu);\n+              absl::MutexLock lock(cliques.mu);\n               auto it = cliques.map.find(clique_key);\n               return it == cliques.map.end() ? nullptr : &it->second;\n             }();\n@@ -627,7 +627,7 @@ absl::Status AbortCliquesWithIncarnations(\n   const absl::flat_hash_set<IncarnationId> incarnation_set(incarnations.begin(),\n                                                            incarnations.end());\n   ProcessGpuCliques& cliques = GetProcessGpuCliques();\n-  absl::MutexLock lock(&cliques.mu);\n+  absl::MutexLock lock(cliques.mu);\n   absl::Status result;\n   for (auto it = cliques.map.begin(); it != cliques.map.end();) {\n     auto copy = it++;"
        },
        {
            "sha": "e5f78e7de02e28bffa09280ce158fe9e41ae913a",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_collectives.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_collectives.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -327,7 +327,7 @@ class NcclIdStore {\n     // unique keys, otherwise the global key-value store may hold the wrong\n     // value.\n     {\n-      absl::MutexLock lock(&mu_);\n+      absl::MutexLock lock(mu_);\n       auto it = cache_.find(*gpu_key);\n       if (it != cache_.end()) {\n         return it->second;\n@@ -346,7 +346,7 @@ class NcclIdStore {\n           kv_store_->Get(gpu_key->ToString(), absl::Minutes(10)));\n       clique_id = CliqueId(id_str);\n     }\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     auto result = cache_.emplace(*gpu_key, std::move(clique_id));\n     TF_RET_CHECK(result.second) << \"Unique ID already in cache.\";\n     return result.first->second;"
        },
        {
            "sha": "df5b9c179679540b22bea17d3102ad62b89a6737",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -361,7 +361,7 @@ absl::Status NcclCommunicator::RegisterBufferOnce(\n     bool use_symmetric_buffer) {\n   bool need_reg = false;\n   {\n-    absl::MutexLock lock(&registered_buffers_.mu);\n+    absl::MutexLock lock(registered_buffers_.mu);\n     if (!registered_buffers_.range_to_handle.contains(buffer_range.opaque())) {\n       need_reg = true;\n     } else {\n@@ -380,7 +380,7 @@ absl::Status NcclCommunicator::RegisterBufferOnce(\n     TF_ASSIGN_OR_RETURN(\n         auto handle,\n         RegisterBuffer(buffer_range, device_ordinal, use_symmetric_buffer));\n-    absl::MutexLock lock(&registered_buffers_.mu);\n+    absl::MutexLock lock(registered_buffers_.mu);\n     registered_buffers_.range_to_handle[buffer_range.opaque()] =\n         std::move(handle);\n   }"
        },
        {
            "sha": "b4a7030795311599044e5e98454ab06ff7599dca",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -133,7 +133,7 @@ absl::Status AllToAllStartThunk::Initialize(const InitializeParams& params) {\n     TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm_handle.comm->NumRanks());\n     se::StreamExecutor* executor = params.executor;\n     {\n-      absl::MutexLock lock(&pointer_maps_mutex_);\n+      absl::MutexLock lock(pointer_maps_mutex_);\n       if (!receive_pointer_maps_.count(executor)) {\n         TF_ASSIGN_OR_RETURN(\n             std::unique_ptr<se::MemoryAllocation> alloc,\n@@ -144,7 +144,7 @@ absl::Status AllToAllStartThunk::Initialize(const InitializeParams& params) {\n       }\n     }\n     {\n-      absl::MutexLock lock(&events_mutex_);\n+      absl::MutexLock lock(events_mutex_);\n       if (!events_.count(executor)) {\n         TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Event> event,\n                             executor->CreateEvent());\n@@ -196,7 +196,7 @@ absl::Status AllToAllStartThunk::Initialize(const InitializeParams& params) {\n           (rank.value().value() - peer + num_ranks) % num_ranks;\n       uint64_t* recv_ptr;\n       {\n-        absl::MutexLock lock(&pointer_maps_mutex_);\n+        absl::MutexLock lock(pointer_maps_mutex_);\n         recv_ptr = reinterpret_cast<uint64_t*>(\n             receive_pointer_maps_[executor]->opaque());\n       }\n@@ -218,20 +218,20 @@ absl::StatusOr<bool> AllToAllStartThunk::RunCollective(\n   if (is_local() && p2p_memcpy_enabled_) {\n     uint64_t* receive_pointer_map = nullptr;\n     {\n-      absl::MutexLock lock(&pointer_maps_mutex_);\n+      absl::MutexLock lock(pointer_maps_mutex_);\n       receive_pointer_map = reinterpret_cast<uint64_t*>(\n           receive_pointer_maps_[stream.parent()]->opaque());\n     }\n     std::optional<RankId> rank =\n         comm_handle.clique_key.rank(params.collective_params->global_device_id);\n     se::Event* event = nullptr;\n     {\n-      absl::MutexLock lock(&events_mutex_);\n+      absl::MutexLock lock(events_mutex_);\n       event = events_[stream.parent()].get();\n     }\n     std::vector<se::Event*> events;\n     {\n-      absl::MutexLock lock(&events_mutex_);\n+      absl::MutexLock lock(events_mutex_);\n       absl::c_transform(events_, std::back_inserter(events),\n                         [](const auto& pair) { return pair.second.get(); });\n     }"
        },
        {
            "sha": "80b09c0fdeab0f0b6f383487c34e840055b21e42",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -201,7 +201,7 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n       GetAllReduceStrategy(GetInputSizeBytes()));\n   StreamState* state = nullptr;\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     if (!per_stream_state_.contains(params.executor)) {\n       // Step1: Allocate local buffer\n       TF_ASSIGN_OR_RETURN(\n@@ -276,7 +276,7 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n       << \"is not in the clique.\";\n   StreamState* state = nullptr;\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     auto it = per_stream_state_.find(stream->parent());\n     TF_RET_CHECK(it != per_stream_state_.end())\n         << \"Stream not found in per_stream_state_\";"
        },
        {
            "sha": "dbd8a600825b4ff0668cd23c50eaf40a88ea02cd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -190,7 +190,7 @@ absl::Status CollectivePermuteStartThunk::Initialize(\n     TF_ASSIGN_OR_RETURN(const int64_t current_id,\n                         GetCurrentId(params.collective_params, config_));\n     {\n-      absl::MutexLock lock(&barrier_mutex_);\n+      absl::MutexLock lock(barrier_mutex_);\n       if (receiver_barrier_events_.find(current_id) ==\n           receiver_barrier_events_.end()) {\n         TF_ASSIGN_OR_RETURN(auto receiver_event,\n@@ -274,7 +274,7 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n     // Receiving side will record an event and the sender will wait for the\n     // event before proceeding.\n     if (source_id) {\n-      absl::MutexLock lock(&barrier_mutex_);\n+      absl::MutexLock lock(barrier_mutex_);\n       auto receiver_event = receiver_barrier_events_.find(current_id);\n       TF_RETURN_IF_ERROR(stream.RecordEvent(receiver_event->second.get()));\n     }\n@@ -298,7 +298,7 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n \n     // For sending side, wait for the recorded event from the receiving side.\n     if (target_id) {\n-      absl::MutexLock lock(&barrier_mutex_);\n+      absl::MutexLock lock(barrier_mutex_);\n       auto receiver_event = receiver_barrier_events_.find(*target_id);\n       TF_RETURN_IF_ERROR(stream.WaitFor(receiver_event->second.get()));\n     }\n@@ -316,7 +316,7 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n     // wait for the sender's event before proceeding to ensure\n     // data has been copied.\n     if (target_id) {\n-      absl::MutexLock lock(&barrier_mutex_);\n+      absl::MutexLock lock(barrier_mutex_);\n       auto sender_event = sender_barrier_events_.find(current_id);\n       TF_RETURN_IF_ERROR(stream.RecordEvent(sender_event->second.get()));\n     }\n@@ -340,7 +340,7 @@ absl::StatusOr<bool> CollectivePermuteStartThunk::RunCollective(\n \n     // For receiving side, wait for the recorded event from the sending side.\n     if (source_id) {\n-      absl::MutexLock lock(&barrier_mutex_);\n+      absl::MutexLock lock(barrier_mutex_);\n       auto sender_event = sender_barrier_events_.find(*source_id);\n       TF_RETURN_IF_ERROR(stream.WaitFor(sender_event->second.get()));\n     }"
        },
        {
            "sha": "00426412ceee168c8c0f983a9da8eeb8e55e1b20",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.h?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -50,12 +50,12 @@ class CollectivePermuteStartThunk : public CollectiveThunk {\n   class RecvPtrMap {\n    public:\n     bool IsInitialized(int64_t current_id) {\n-      absl::MutexLock lock(&mutex_);\n+      absl::MutexLock lock(mutex_);\n       return recv_ptrs_.find(current_id) != recv_ptrs_.end();\n     }\n \n     absl::Status InitializeId(int64_t current_id) {\n-      absl::MutexLock lock(&mutex_);\n+      absl::MutexLock lock(mutex_);\n       recv_ptrs_[current_id] =\n           tsl::MakeUnconstructedAsyncValueRef<std::vector<void*>>();\n       return absl::OkStatus();\n@@ -67,7 +67,7 @@ class CollectivePermuteStartThunk : public CollectiveThunk {\n         return absl::InternalError(absl::StrCat(\"Current ID \", current_id,\n                                                 \" has not been initialized!\"));\n       }\n-      absl::MutexLock lock(&mutex_);\n+      absl::MutexLock lock(mutex_);\n       if (recv_ptrs_.at(current_id).IsUnavailable()) {\n         VLOG(3) << \"Putting pointers to current_id \" << current_id;\n         recv_ptrs_.at(current_id).emplace(ptrs);\n@@ -81,7 +81,7 @@ class CollectivePermuteStartThunk : public CollectiveThunk {\n         return absl::InternalError(absl::StrCat(\"Target ID \", target_id,\n                                                 \" has not been initialized!\"));\n       }\n-      absl::MutexLock lock(&mutex_);\n+      absl::MutexLock lock(mutex_);\n       return recv_ptrs_[target_id];\n     }\n "
        },
        {
            "sha": "07a2aff95045a1bc8ee5f432d1175b10bf776648",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -387,7 +387,7 @@ absl::Status MaybeRegisterBuffers(se::StreamExecutor* executor,\n \n absl::Status CollectiveThunk::AsyncEvents::Initialize(\n     se::StreamExecutor* executor) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   if (events_.contains(executor)) return absl::OkStatus();\n \n   TF_ASSIGN_OR_RETURN(auto event, executor->CreateEvent());\n@@ -398,7 +398,7 @@ absl::Status CollectiveThunk::AsyncEvents::Initialize(\n \n absl::StatusOr<se::Event*> CollectiveThunk::AsyncEvents::GetEvent(\n     se::StreamExecutor* executor) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n \n   auto event = events_.find(executor);\n   if (event == events_.end()) {"
        },
        {
            "sha": "052397a18e6e9c7e992c0a6d7cc5600fbb20750b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -1026,7 +1026,7 @@ LaunchCmd::LaunchCmd(\n absl::Status LaunchCmd::Initialize(const Thunk::InitializeParams& params,\n                                    StateManager& state) {\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     if (kernels_.contains(params.executor)) {\n       return absl::OkStatus();\n     }\n@@ -1044,7 +1044,7 @@ absl::Status LaunchCmd::Initialize(const Thunk::InitializeParams& params,\n                              params.executor, shmem_bytes_));\n   }\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   kernels_.emplace(params.executor, std::move(kernel));\n   return absl::OkStatus();\n }\n@@ -1058,7 +1058,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> LaunchCmd::Record(\n \n   se::StreamExecutor* executor = execute_params.stream->parent();\n   se::Kernel* kernel = [&] {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     return kernels_[executor].get();\n   }();\n \n@@ -1134,7 +1134,7 @@ CustomKernelLaunchCmd::CustomKernelLaunchCmd(\n absl::Status CustomKernelLaunchCmd::Initialize(\n     const Thunk::InitializeParams& params, StateManager& state) {\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     if (kernels_.contains(params.executor)) {\n       return absl::OkStatus();\n     }\n@@ -1144,7 +1144,7 @@ absl::Status CustomKernelLaunchCmd::Initialize(\n       std::unique_ptr<se::Kernel> kernel,\n       params.executor->LoadKernel(custom_kernel_.kernel_spec()));\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   kernels_.emplace(params.executor, std::move(kernel));\n   return absl::OkStatus();\n }\n@@ -1156,7 +1156,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> CustomKernelLaunchCmd::Record(\n   VLOG(5) << \"CustomKernelLaunchCmd: custom_kernel=\" << custom_kernel_.name();\n \n   se::Kernel* kernel = [&] {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     return kernels_[execute_params.stream->parent()].get();\n   }();\n \n@@ -2398,7 +2398,7 @@ bool DynamicSliceFusionCmd::requires_initialization() {\n absl::Status DynamicSliceFusionCmd::Initialize(\n     const Thunk::InitializeParams& params, StateManager& state) {\n   TF_RETURN_IF_ERROR(embedded_commands_.Initialize(params, state));\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (offsets_allocs_.contains(params.executor)) {\n     return absl::OkStatus();\n   }\n@@ -2449,7 +2449,7 @@ absl::StatusOr<const se::CommandBuffer::Command*> DynamicSliceFusionCmd::Record(\n \n   // Get memory allocation for copying offsets from device.\n   int64_t* offsets_alloc = [&] {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     return reinterpret_cast<int64_t*>(\n         offsets_allocs_.at(stream.parent())->opaque());\n   }();"
        },
        {
            "sha": "a3009882a6b7058b6334cee3d217253318683b51",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -146,7 +146,7 @@ absl::Status CommandBufferThunk::Initialize(const InitializeParams& params) {\n \n   TF_ASSIGN_OR_RETURN(std::shared_ptr<ExecutorCommandBuffer> cmd_buffer,\n                       GetOrCreateCommandBuffer(params.executor));\n-  absl::MutexLock lock(&cmd_buffer->mutex);\n+  absl::MutexLock lock(cmd_buffer->mutex);\n \n   // Initialize commands.\n   TF_RETURN_IF_ERROR(commands_.Initialize(params, cmd_buffer->state));\n@@ -236,7 +236,7 @@ absl::Status CommandBufferThunk::ExecuteOnStream(const ExecuteParams& params) {\n   TF_ASSIGN_OR_RETURN(std::shared_ptr<ExecutorCommandBuffer> cmd_buffer,\n                       GetOrCreateCommandBuffer(executor));\n \n-  absl::MutexLock lock(&cmd_buffer->mutex);\n+  absl::MutexLock lock(cmd_buffer->mutex);\n \n   // Update buffer allocations and collect all allocations that changed since\n   // the last command buffer execution.\n@@ -289,7 +289,7 @@ absl::Status CommandBufferThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n absl::StatusOr<std::shared_ptr<CommandBufferThunk::ExecutorCommandBuffer>>\n CommandBufferThunk::GetOrCreateCommandBuffer(se::StreamExecutor* executor) {\n-  absl::MutexLock lock(&state_->mutex);\n+  absl::MutexLock lock(state_->mutex);\n \n   // Check if command buffer already exists\n   if (auto it = state_->command_buffers.find(executor);\n@@ -326,15 +326,15 @@ CommandBufferThunk::GlobalState* CommandBufferThunk::GetGlobalState() {\n void CommandBufferThunk::TrackCommandBuffers(\n     std::weak_ptr<CommandBufferThunk::State> state) {\n   auto* global_state = GetGlobalState();\n-  absl::MutexLock global_state_lock(&global_state->mutex);\n+  absl::MutexLock global_state_lock(global_state->mutex);\n   global_state->state.push_back(state);\n }\n \n void CommandBufferThunk::EvictCommandBuffers() {\n   TraceMe trace([&] { return \"EvictCommandBuffers\"; });\n \n   auto* global_state = GetGlobalState();\n-  absl::MutexLock global_state_lock(&global_state->mutex);\n+  absl::MutexLock global_state_lock(global_state->mutex);\n   VLOG(3) << \"Evict command buffer thunk command buffers; tracked thunks = \"\n           << global_state->state.size();\n \n@@ -353,7 +353,7 @@ void CommandBufferThunk::EvictCommandBuffers() {\n     }\n \n     // Evict all command buffers.\n-    absl::MutexLock state_lock(&ptr->mutex);\n+    absl::MutexLock state_lock(ptr->mutex);\n     num_evicted += ptr->command_buffers.size();\n     ptr->command_buffers.clear();\n   }"
        },
        {
            "sha": "f97246580796d71b2fb13b3094f741fec1b03736",
            "filename": "third_party/xla/xla/backends/gpu/runtime/conditional_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconditional_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -76,7 +76,7 @@ absl::Status ConditionalThunk::Initialize(const InitializeParams& params) {\n     TF_RETURN_IF_ERROR(branch_thunk->Initialize(params));\n   }\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n \n   if (!host_memory_pools_.contains(params.executor)) {\n     PrimitiveType type =\n@@ -94,7 +94,7 @@ absl::Status ConditionalThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   HostMemoryPool* pool;\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     pool = host_memory_pools_.at(stream.parent()).get();\n   }\n   TF_ASSIGN_OR_RETURN(HostMemoryPool::Handle handle, pool->Acquire());"
        },
        {
            "sha": "6d0b5b89b188fdb0739aa08156aab7677625558a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -56,7 +56,7 @@ ConvolutionThunk::ConvolutionThunk(\n \n GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(\n     const stream_executor::Stream* stream, bool* runner_created) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   auto it = runner_cache_.find(stream);\n   *runner_created = (it == runner_cache_.end());\n   if (*runner_created) {"
        },
        {
            "sha": "32b156ce23e33da68d6644bf8b6920f4eac73994",
            "filename": "third_party/xla/xla/backends/gpu/runtime/copy_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcopy_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -118,7 +118,7 @@ absl::Status CopyThunk::AsyncEvents::Emplace(se::StreamExecutor* executor,\n                                              const HloInstruction* instr,\n                                              std::unique_ptr<se::Event> event) {\n   Key key = {executor, instr};\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   VLOG(3) << \"Emplace event \" << event.get();\n   if (auto [it, inserted] = events_.try_emplace(key, std::move(event));\n       inserted) {\n@@ -132,7 +132,7 @@ absl::Status CopyThunk::AsyncEvents::Emplace(se::StreamExecutor* executor,\n absl::StatusOr<std::unique_ptr<se::Event>> CopyThunk::AsyncEvents::Extract(\n     se::StreamExecutor* executor, const HloInstruction* instr) {\n   Key key = {executor, instr};\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (auto event = events_.extract(key)) {\n     VLOG(3) << \"Extract event \" << event.mapped().get();\n     return std::move(event.mapped());"
        },
        {
            "sha": "0fb0a82f8fd64a0b010ac56df1d670d322b96357",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -151,7 +151,7 @@ absl::Status DynamicSliceThunk::Prepare(\n absl::Status DynamicSliceThunk::Initialize(const InitializeParams& params) {\n   TF_RETURN_IF_ERROR(embedded_thunk_->Initialize(params));\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (offsets_allocs_.contains(params.executor)) return absl::OkStatus();\n \n   VLOG(2) << \"Allocate \" << offsets_allocs_size_\n@@ -173,7 +173,7 @@ absl::Status DynamicSliceThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   // Get memory allocation for copying offsets from device.\n   int64_t* offsets_alloc = [&] {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     return reinterpret_cast<int64_t*>(\n         offsets_allocs_.at(stream.parent())->opaque());\n   }();"
        },
        {
            "sha": "b9bdd2cefc12c5c9031bf3a86c79d74eefd33892",
            "filename": "third_party/xla/xla/backends/gpu/runtime/fft_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -144,7 +144,7 @@ absl::Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,\n \n   // CuFFT thread-safety requires that separate host threads not share plans;\n   // protect each plan with a mutex.\n-  absl::MutexLock lock(&fft_plan_ptr->mu);\n+  absl::MutexLock lock(fft_plan_ptr->mu);\n   std::unique_ptr<se::fft::Plan>& fft_plan = fft_plan_ptr->plan;\n   TF_ASSIGN_OR_RETURN(auto fft, GetFft(stream));\n   if (fft_plan == nullptr) {"
        },
        {
            "sha": "0bc4588a30c6f9eb9bab327e90e26015d5b3e5ff",
            "filename": "third_party/xla/xla/backends/gpu/runtime/fft_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ffft_thunk.h?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -49,7 +49,7 @@ class FftPlanCache {\n  public:\n   // Returnes Fft plan cached for the given device ordinal or creates a new one.\n   FftPlan* GetOrCreate(int device_ordinal) {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     std::unique_ptr<FftPlan>& plan = fft_plans_[device_ordinal];\n     if (!plan) plan = std::make_unique<FftPlan>();\n     return plan.get();"
        },
        {
            "sha": "a0101cce44ef49dc9324f93ec2f17d952253c98d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_execute_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -321,7 +321,7 @@ HostExecuteAsyncEvents::CreateEvent(se::StreamExecutor* executor,\n   auto event = tsl::MakeConstructedAsyncValueRef<std::unique_ptr<se::Event>>(\n       std::move(host_to_device_stream_event));\n \n-  absl::MutexLock lock(&events_mu_);\n+  absl::MutexLock lock(events_mu_);\n   auto [it, inserted] =\n       events_.emplace(std::make_pair(executor, run_id), event);\n \n@@ -340,7 +340,7 @@ HostExecuteAsyncEvents::ExtractEvent(se::StreamExecutor* executor,\n   VLOG(6) << \"Extracting event for executor at address \" << executor\n           << \" and event id \" << run_id.ToInt();\n \n-  absl::MutexLock lock(&events_mu_);\n+  absl::MutexLock lock(events_mu_);\n   auto it = events_.find(std::make_pair(executor, run_id));\n   if (it == events_.end()) {\n     return FailedPrecondition("
        },
        {
            "sha": "cbd2839011ae07c0521c215c55bd908be762803c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_memory_pool.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_memory_pool.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_memory_pool.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_memory_pool.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -41,7 +41,7 @@ absl::StatusOr<std::unique_ptr<HostMemoryPool>> HostMemoryPool::Create(\n }\n \n absl::StatusOr<HostMemoryPool::Handle> HostMemoryPool::Acquire() {\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (free_list_.empty()) {\n     return absl::ResourceExhaustedError(\n         absl::StrCat(\"All \", kNumElems,"
        },
        {
            "sha": "7140e7a97eec84ef61747ef296e0214b664bf892",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_memory_pool.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_memory_pool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_memory_pool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_memory_pool.h?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -89,7 +89,7 @@ class HostMemoryPool {\n                  PrimitiveType type);\n \n   void Release(void* ptr) {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     free_list_.push(ptr);\n   }\n "
        },
        {
            "sha": "543f3987ad2ddf1c8ef883b28e0c5e370bdad73f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_send_recv_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_send_recv_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -75,7 +75,7 @@ absl::Status HostSendRecvAsyncEvents::Emplace(\n     tsl::AsyncValueRef<std::unique_ptr<se::Event>> event) {\n   Key key = {executor, channel_id};\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (auto it = events_.try_emplace(key, std::move(event)); it.second)\n     return absl::OkStatus();\n \n@@ -88,7 +88,7 @@ HostSendRecvAsyncEvents::Extract(se::StreamExecutor* executor,\n                                  int32_t channel_id) {\n   Key key = {executor, channel_id};\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (auto event = events_.extract(key)) return std::move(event.mapped());\n \n   return absl::InternalError(absl::StrFormat("
        },
        {
            "sha": "b675a6002be243ed24786fbd44fb505b54e5109b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -138,7 +138,7 @@ absl::StatusOr<std::unique_ptr<KernelThunk>> KernelThunk::FromProto(\n }\n \n absl::Status KernelThunk::Initialize(const InitializeParams& params) {\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n \n   // Load the kernel into the device if necessary.\n   //\n@@ -208,7 +208,7 @@ absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n       GetStreamForExecution(Thunk::execution_stream_id(), params));\n \n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     auto it = kernel_cache_.find(executor);\n     CHECK(it != kernel_cache_.end())\n         << \"Initialize() not called for StreamExecutor \" << executor;\n@@ -267,7 +267,7 @@ std::string CustomKernelThunk::ToString(int indent) const {\n }\n \n absl::Status CustomKernelThunk::Initialize(const InitializeParams& params) {\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n \n   if (!kernel_cache_.contains(params.executor)) {\n     TF_ASSIGN_OR_RETURN(\n@@ -283,7 +283,7 @@ absl::Status CustomKernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n   se::StreamExecutor* executor = params.stream->parent();\n \n   se::Kernel* kernel = [&] {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     return kernel_cache_[executor].get();\n   }();\n "
        },
        {
            "sha": "b921e9f91224556f853ff2b1ed8ca0e8063b02ee",
            "filename": "third_party/xla/xla/backends/gpu/runtime/norm_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnorm_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -52,7 +52,7 @@ NormThunk::NormThunk(ThunkInfo thunk_info, GpuNormConfig config,\n \n NormRunner& NormThunk::GetOrCreateRunner(\n     const stream_executor::Stream* stream) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   auto it = runner_cache_.find(stream);\n   if (it == runner_cache_.end()) {\n     it = runner_cache_.insert({stream, std::make_unique<NormRunner>(config_)})"
        },
        {
            "sha": "1db022e135605f7206f6929e516f6d0411932b1c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -173,7 +173,7 @@ absl::Status IsValidNvshmemOperand(Shape shape, Thunk::Kind reduction_op) {\n \n absl::StatusOr<void*> NvshmemBufferAddresses::GetNvshmemPtr(\n     int device_ordinal) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   auto it = buffer_addrs_.find(device_ordinal);\n   if (it != buffer_addrs_.end()) {\n     return it->second;\n@@ -183,7 +183,7 @@ absl::StatusOr<void*> NvshmemBufferAddresses::GetNvshmemPtr(\n \n void NvshmemBufferAddresses::StoreNvshmemPtr(int device_ordinal,\n                                              void* buffer_addr) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   buffer_addrs_[device_ordinal] = buffer_addr;\n }\n "
        },
        {
            "sha": "74ae615954b8ca9c2e6bee9942ed99294133bad4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/p2p_thunk_common.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fp2p_thunk_common.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -43,7 +43,7 @@ namespace gpu {\n \n absl::Status ExecutionCounters::Initialize(se::StreamExecutor* executor,\n                                            RunId run_id) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   CounterKey key = {executor, run_id};\n   if (counters_.contains(key)) return absl::OkStatus();\n   counters_.emplace(key, 0);\n@@ -52,7 +52,7 @@ absl::Status ExecutionCounters::Initialize(se::StreamExecutor* executor,\n \n absl::StatusOr<int64_t*> ExecutionCounters::GetCounter(\n     se::StreamExecutor* executor, RunId run_id) {\n-  absl::MutexLock lock(&mu_);\n+  absl::MutexLock lock(mu_);\n   CounterKey key = {executor, run_id};\n   auto counter = counters_.find(key);\n   if (counter == counters_.end()) {"
        },
        {
            "sha": "df695ee623022a0ec303532eec916252c7f39e97",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -396,7 +396,7 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n \n   StreamState* state = nullptr;\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n \n     // If the stream state already exists, it means that the thunk has been\n     // initialized for this executor.\n@@ -497,7 +497,7 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n \n   StreamState* state = nullptr;\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     state = per_stream_states_[stream.parent()].get();\n   }\n "
        },
        {
            "sha": "7ba3b851e0942eccd428a83576696224c1e3526f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/while_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c4399445b7dec3d0a6ed75134171bc3d2ad775d4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc?ref=c4399445b7dec3d0a6ed75134171bc3d2ad775d4",
            "patch": "@@ -110,7 +110,7 @@ absl::Status WhileThunk::Initialize(const InitializeParams& params) {\n   TF_RETURN_IF_ERROR(condition_thunk_sequence_->Initialize(params));\n   TF_RETURN_IF_ERROR(body_thunk_sequence_->Initialize(params));\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (!host_memory_pools_.contains(params.executor)) {\n     TF_ASSIGN_OR_RETURN(\n         std::unique_ptr<HostMemoryPool> pool,\n@@ -142,7 +142,7 @@ absl::Status WhileThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   HostMemoryPool* pool;\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     pool = host_memory_pools_.at(stream.parent()).get();\n   }\n   TF_ASSIGN_OR_RETURN(HostMemoryPool::Handle handle, pool->Acquire());"
        }
    ],
    "stats": {
        "total": 140,
        "additions": 70,
        "deletions": 70
    }
}