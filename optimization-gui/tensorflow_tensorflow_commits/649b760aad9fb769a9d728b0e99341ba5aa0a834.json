{
    "author": "ermilovmaxim",
    "message": "Add proto serialization for AllToAllStartThunk\n\nPiperOrigin-RevId: 846342263",
    "sha": "649b760aad9fb769a9d728b0e99341ba5aa0a834",
    "files": [
        {
            "sha": "78274594b54b8c7e24774fbf1af6e11f9e9c1123",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=649b760aad9fb769a9d728b0e99341ba5aa0a834",
            "patch": "@@ -1455,6 +1455,7 @@ cc_library(\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/stream_executor:device_address\",\n@@ -1463,6 +1464,7 @@ cc_library(\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -1477,6 +1479,21 @@ cc_library(\n     ],\n )\n \n+xla_cc_test(\n+    name = \"all_to_all_thunk_test\",\n+    srcs = [\"all_to_all_thunk_test.cc\"],\n+    deps = [\n+        \":all_to_all_thunk\",\n+        \":collective_thunk\",\n+        \":thunk\",\n+        \":thunk_proto_cc\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"ragged_all_to_all_thunk\",\n     srcs = [\"ragged_all_to_all_thunk.cc\"],\n@@ -2886,6 +2903,7 @@ cc_library(\n     deps = [\n         \":all_gather_thunk\",\n         \":all_reduce_thunk\",\n+        \":all_to_all_thunk\",\n         \":collective_thunk\",\n         \":conditional_thunk\",\n         \":convolution_reorder_thunk\","
        },
        {
            "sha": "ddc596e4ec9eb50da329f6e89e13e9603cf35eea",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 5,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc?ref=649b760aad9fb769a9d728b0e99341ba5aa0a834",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape.h\"\n@@ -56,6 +57,7 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla {\n namespace gpu {\n@@ -77,17 +79,27 @@ struct BufferRendezvousValue {\n }  // namespace\n \n AllToAllStartThunk::AllToAllStartThunk(\n-    ThunkInfo thunk_info, const HloAllToAllInstruction* instr,\n-    std::vector<CollectiveThunk::Buffer> buffers, bool p2p_memcpy_enabled)\n-    : CollectiveThunk(Thunk::kAllToAllStart, thunk_info,\n-                      IsGPUSyncCollective(*instr),\n+    ThunkInfo thunk_info, std::shared_ptr<AsyncEvents> async_events,\n+    const AllToAllConfig& config, std::vector<CollectiveThunk::Buffer> buffers,\n+    bool p2p_memcpy_enabled)\n+    : CollectiveThunk(Thunk::kAllToAllStart, thunk_info, async_events,\n                       AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE),\n-      config_(GetAllToAllConfig(instr)),\n+      config_(config),\n       buffers_(std::move(buffers)),\n       p2p_memcpy_enabled_(p2p_memcpy_enabled) {\n   CHECK_EQ(config_.config.operand_element_type.size(), buffers_.size());\n }\n \n+AllToAllStartThunk::AllToAllStartThunk(\n+    ThunkInfo thunk_info, const HloAllToAllInstruction* instr,\n+    std::vector<CollectiveThunk::Buffer> buffers, bool p2p_memcpy_enabled)\n+    : AllToAllStartThunk(std::move(thunk_info),\n+                         IsGPUSyncCollective(*instr)\n+                             ? nullptr\n+                             : std::make_shared<CollectiveThunk::AsyncEvents>(),\n+                         GetAllToAllConfig(instr), std::move(buffers),\n+                         p2p_memcpy_enabled) {}\n+\n /*static*/ absl::Status AllToAllStartThunk::CheckImplementable(\n     const HloAllToAllInstruction* instr, int64_t replica_count,\n     int64_t partition_count) {\n@@ -271,6 +283,60 @@ bool AllToAllStartThunk::is_local() const {\n   return true;\n }\n \n+absl::StatusOr<std::unique_ptr<AllToAllStartThunk>>\n+AllToAllStartThunk::FromProto(\n+    ThunkInfo thunk_info, const AllToAllStartThunkProto& thunk_proto,\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    CollectiveThunk::AsyncEventsMap& async_events_map) {\n+  std::vector<CollectiveThunk::Buffer> buffers;\n+  buffers.reserve(thunk_proto.buffers_size());\n+  for (const CollectiveBufferProto& proto : thunk_proto.buffers()) {\n+    ASSIGN_OR_RETURN(\n+        CollectiveThunk::Buffer buffer,\n+        CollectiveThunk::Buffer::FromProto(proto, buffer_allocations));\n+    buffers.push_back(buffer);\n+  }\n+\n+  std::shared_ptr<CollectiveThunk::AsyncEvents>& async_events =\n+      async_events_map[AsyncEventsUniqueId{\n+          thunk_proto.async_events_unique_id()}];\n+  if (!async_events) {\n+    async_events = std::make_shared<CollectiveThunk::AsyncEvents>();\n+  }\n+\n+  CollectiveConfig config =\n+      CollectiveConfig::FromProto(thunk_proto.collective_config());\n+\n+  return std::make_unique<AllToAllStartThunk>(\n+      std::move(thunk_info), async_events,\n+      AllToAllConfig{config, thunk_proto.has_split_dimension()}, buffers,\n+      thunk_proto.p2p_memcpy_enabled());\n+}\n+\n+absl::StatusOr<ThunkProto> AllToAllStartThunk::ToProto() const {\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+\n+  AllToAllStartThunkProto* thunk_proto = proto.mutable_all_to_all_start_thunk();\n+\n+  std::optional<AsyncEventsUniqueId> async_events_id = GetAsyncEventsUniqueId();\n+  if (!async_events_id.has_value()) {\n+    return absl::FailedPreconditionError(\"AsyncEvents is not set.\");\n+  }\n+  thunk_proto->set_async_events_unique_id(async_events_id->value());\n+\n+  for (const Buffer& buffer : buffers_) {\n+    ASSIGN_OR_RETURN(*thunk_proto->add_buffers(), buffer.ToProto());\n+  }\n+\n+  *thunk_proto->mutable_collective_config() = config_.config.ToProto();\n+\n+  thunk_proto->set_has_split_dimension(has_split_dimension());\n+  thunk_proto->set_p2p_memcpy_enabled(p2p_memcpy_enabled_);\n+\n+  return proto;\n+}\n+\n absl::Status RunAllToAll(bool has_split_dimension,\n                          std::vector<DeviceBufferPair>& buffers,\n                          se::Stream& stream, Communicator& comm,"
        },
        {
            "sha": "b35f1340688605ef424fbcff18dc285cbca8f589",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk.h",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.h?ref=649b760aad9fb769a9d728b0e99341ba5aa0a834",
            "patch": "@@ -24,13 +24,15 @@ limitations under the License.\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -49,6 +51,12 @@ class AllToAllStartThunk : public CollectiveThunk {\n   AllToAllStartThunk(ThunkInfo thunk_info, const HloAllToAllInstruction* instr,\n                      std::vector<Buffer> buffers, bool p2p_memcpy_enabled);\n \n+  AllToAllStartThunk(ThunkInfo thunk_info,\n+                     std::shared_ptr<AsyncEvents> async_events,\n+                     const AllToAllConfig& config,\n+                     std::vector<CollectiveThunk::Buffer> buffers,\n+                     bool p2p_memcpy_enabled);\n+\n   // Returns whether the given instruction can be lowered to an all-to-all\n   // call.\n   static absl::Status CheckImplementable(const HloAllToAllInstruction* instr,\n@@ -57,11 +65,18 @@ class AllToAllStartThunk : public CollectiveThunk {\n \n   absl::Status Initialize(const InitializeParams& params) override;\n \n-  static const char* GetHloOpName() { return \"all-to-all-start\"; }\n+  static absl::string_view GetHloOpName() { return \"all-to-all-start\"; }\n \n   static CollectiveOpGroupMode GetGroupMode(\n       const HloAllToAllInstruction* instr);\n \n+  static absl::StatusOr<std::unique_ptr<AllToAllStartThunk>> FromProto(\n+      ThunkInfo thunk_info, const AllToAllStartThunkProto& thunk_proto,\n+      absl::Span<const BufferAllocation> buffer_allocations,\n+      CollectiveThunk::AsyncEventsMap& async_events_map);\n+\n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n   const CollectiveConfig& config() const override { return config_.config; }\n   bool has_split_dimension() const { return config_.has_split_dimension; }\n   absl::Span<const Buffer> buffers() const { return buffers_; }"
        },
        {
            "sha": "521151a2ee9df3e8654ae5d76564e5f4fd2234e5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk_test.cc",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk_test.cc?ref=649b760aad9fb769a9d728b0e99341ba5aa0a834",
            "patch": "@@ -0,0 +1,75 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::tsl::proto_testing::EqualsProto;\n+\n+TEST(CollectiveThunkTest, ProtoRoundTrip) {\n+  ThunkProto proto = tsl::proto_testing::ParseTextProtoOrDie<ThunkProto>(\n+      R\"pb(\n+        thunk_info {\n+          profile_annotation: \"partition_id_profile_annotation\"\n+          execution_stream_id: 2\n+        }\n+        all_to_all_start_thunk {\n+          async_events_unique_id: 3\n+          collective_config {}\n+          has_split_dimension: false\n+          p2p_memcpy_enabled: true\n+        }\n+      )pb\");\n+\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.profile_annotation = proto.thunk_info().profile_annotation();\n+  thunk_info.execution_stream_id = xla::gpu::ExecutionStreamId{\n+      static_cast<xla::gpu::ExecutionStreamId::ValueType>(\n+          proto.thunk_info().execution_stream_id())};\n+\n+  CollectiveThunk::AsyncEventsMap async_events_map;\n+  std::vector<BufferAllocation> buffer_allocations = {\n+      BufferAllocation(/*index=*/0, /*size=*/4, /*color=*/0)};\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<AllToAllStartThunk> thunk,\n+      AllToAllStartThunk::FromProto(thunk_info, proto.all_to_all_start_thunk(),\n+                                    buffer_allocations, async_events_map));\n+  ASSERT_NE(thunk->async_events(), nullptr);\n+\n+  ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n+\n+  // Ids are unique and expected to differ.\n+  proto.mutable_all_to_all_start_thunk()->set_async_events_unique_id(\n+      round_trip_proto.all_to_all_start_thunk().async_events_unique_id());\n+  EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "ae3ae5022909ae2b0670fd4518c8d989e5db5966",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=649b760aad9fb769a9d728b0e99341ba5aa0a834",
            "patch": "@@ -426,6 +426,15 @@ message AllReduceStartThunkProto {\n   bool is_async = 9;\n }\n \n+message AllToAllStartThunkProto {\n+  uint64 async_events_unique_id = 1;\n+  CollectiveConfigProto collective_config = 2;\n+  repeated CollectiveBufferProto buffers = 3;\n+\n+  bool has_split_dimension = 4;\n+  bool p2p_memcpy_enabled = 5;\n+}\n+\n message CollectiveDoneThunkProto {\n   ThunkKindProto thunk_kind = 1;\n   AsyncStreamKind async_stream_kind = 2;\n@@ -473,6 +482,7 @@ message ThunkProto {\n     CollectiveDoneThunkProto collective_done_thunk = 37;\n     AllGatherStartThunkProto all_gather_start_thunk = 38;\n     AllReduceStartThunkProto all_reduce_start_thunk = 39;\n+    AllToAllStartThunkProto all_to_all_start_thunk = 40;\n   }\n }\n "
        },
        {
            "sha": "3700f70f033d1c9b1e0cd95707aa3709e6149c48",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/649b760aad9fb769a9d728b0e99341ba5aa0a834/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc?ref=649b760aad9fb769a9d728b0e99341ba5aa0a834",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"google/protobuf/message.h\"\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n+#include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n #include \"xla/backends/gpu/runtime/convolution_reorder_thunk.h\"\n@@ -252,6 +253,10 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProtoImpl(\n       return AllReduceStartThunk::FromProto(\n           std::move(thunk_info), thunk_proto.all_reduce_start_thunk(),\n           buffer_allocations, collective_async_events_map);\n+    case ThunkProto::kAllToAllStartThunk:\n+      return AllToAllStartThunk::FromProto(\n+          std::move(thunk_info), thunk_proto.all_to_all_start_thunk(),\n+          buffer_allocations, collective_async_events_map);\n     default:\n       std::optional<absl::string_view> unsupported_thunk_type =\n           GetStoredThunkTypeName(thunk_proto);"
        }
    ],
    "stats": {
        "total": 201,
        "additions": 195,
        "deletions": 6
    }
}