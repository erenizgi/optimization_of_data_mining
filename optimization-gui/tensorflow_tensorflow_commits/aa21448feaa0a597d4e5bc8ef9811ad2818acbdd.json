{
    "author": "pschuh",
    "message": "Remove PjRtStreamExecutorBuffer in favor of CommonPjRtBufferImpl.\n\nPiperOrigin-RevId: 823081955",
    "sha": "aa21448feaa0a597d4e5bc8ef9811ad2818acbdd",
    "files": [
        {
            "sha": "7ab4a754a42acf391539a1bb06fa7bd881b6337a",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 255,
            "changes": 264,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa21448feaa0a597d4e5bc8ef9811ad2818acbdd/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa21448feaa0a597d4e5bc8ef9811ad2818acbdd/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=aa21448feaa0a597d4e5bc8ef9811ad2818acbdd",
            "patch": "@@ -530,9 +530,8 @@ PjRtStreamExecutorClient::DefineBuffer(\n           ->device_buffer(),\n       definition_events);\n \n-  auto py_buffer = std::make_unique<PjRtStreamExecutorBuffer>(\n-      on_device_shape, std::move(dst_device_buffer), this, device,\n-      memory_space);\n+  auto py_buffer = std::make_unique<CommonPjRtBufferImpl>(\n+      on_device_shape, std::move(dst_device_buffer), memory_space);\n   return py_buffer;\n }\n \n@@ -548,128 +547,6 @@ void PjRtStreamExecutorClient::WaitForAllocation(\n   }\n }\n \n-absl::StatusOr<std::unique_ptr<PjRtStreamExecutorBuffer>>\n-AllocateDestinationBuffer(const Shape& on_host_shape, PjRtDevice* device,\n-                          LocalDeviceState* local_device,\n-                          se::Stream* copy_stream, bool is_uninitialized_create,\n-                          PjRtStreamExecutorClient* client,\n-                          BufferSequencingEventRef definition_event,\n-                          PjRtMemorySpace* memory_space) {\n-  if (on_host_shape.IsTuple()) {\n-    return InvalidArgument(\n-        \"Cannot allocate a PjRtStreamExecutorBuffer for a tuple.\");\n-  }\n-\n-  if (!memory_space) {\n-    memory_space = device->default_memory_space().value_or(nullptr);\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(\n-      Shape on_device_shape,\n-      client->MakeDefaultShapeForMemorySpace(\n-          memory_space, on_host_shape,\n-          on_host_shape.has_layout() ? &on_host_shape.layout() : nullptr));\n-  TF_ASSIGN_OR_RETURN(\n-      size_t on_device_bytes_count,\n-      client->GetOnDeviceBytesCount(memory_space, on_device_shape));\n-  tsl::RCReference<RawSEDeviceMemory> mem;\n-  {\n-    bool is_pinned_host_memory =\n-        memory_space && (memory_space->kind() == PinnedHostMemorySpace::kKind);\n-    // Only allow pinned host memory or device memory.\n-    PjRtMemorySpace* default_memory_space =\n-        device->default_memory_space().value_or(nullptr);\n-    if (memory_space != default_memory_space && !is_pinned_host_memory) {\n-      return InvalidArgument(\"Buffer allocation: invalid memory space\");\n-    }\n-\n-    auto* se_client = tensorflow::down_cast<PjRtStreamExecutorClient*>(client);\n-    TransferManager* transfer_manager =\n-        se_client->client()->backend().transfer_manager();\n-\n-    // Communicate the desired memory space to the allocator via the shape\n-    // callback.\n-    auto memory_space_shape_fn = [is_pinned_host_memory,\n-                                  transfer_manager](const Shape& shape) {\n-      Shape result = transfer_manager->HostShapeToDeviceShape(shape);\n-      if (is_pinned_host_memory) {\n-        result.mutable_layout()->set_memory_space(Layout::kHostMemorySpace);\n-      }\n-      return result;\n-    };\n-\n-    TF_ASSIGN_OR_RETURN(\n-        ScopedShapedBuffer dst_buffer,\n-        transfer_manager->AllocateScopedShapedBuffer(\n-            on_host_shape, se_client->allocator(),\n-            local_device->local_device_id().value(),\n-            local_device->local_hardware_id().value(), memory_space_shape_fn));\n-    Shape old_on_device_shape = dst_buffer.on_device_shape();\n-    DCHECK_EQ(on_device_shape, old_on_device_shape)\n-        << on_device_shape.ToString(true) << \" vs \"\n-        << old_on_device_shape.ToString(true);\n-    DCHECK_EQ(on_device_bytes_count, dst_buffer.buffer({}).size());\n-    mem = RawSEDeviceMemory::Create(dst_buffer.buffer({}), local_device,\n-                                    dst_buffer.memory_allocator());\n-    dst_buffer.clear();\n-    if (local_device->allocation_model() !=\n-        LocalDeviceState::kComputeSynchronized) {\n-      DCHECK(client->client()\n-                 ->backend()\n-                 .transfer_manager()\n-                 ->CanBufferBeAccessedNow(\n-                     local_device->compute_stream()->parent(), mem->mem()));\n-    }\n-  }\n-  if (local_device->allocation_model() ==\n-      LocalDeviceState::kComputeSynchronized) {\n-    if (copy_stream == nullptr) {\n-      CHECK(is_uninitialized_create);\n-    } else {\n-      CHECK(copy_stream->WaitFor(local_device->compute_stream()).ok());\n-    }\n-  }\n-\n-  absl::InlinedVector<BufferSequencingEventRef, 2> definition_events;\n-  if (is_uninitialized_create) {\n-    // There is not going to be any copy into the buffer so in general we don't\n-    // need a definition event.\n-    // But if the caller provided a definition event then we record that. Also\n-    // put it as the first definition event so that we can guarantee only the\n-    // first one might not have event recorded.\n-    if (definition_event) {\n-      definition_events.push_back(definition_event);\n-    }\n-    if (local_device->allocation_model() ==\n-        LocalDeviceState::kComputeSynchronized) {\n-      // The allocation is not valid until the compute stream passes this point,\n-      // so add a definition event in the compute stream.\n-      definition_events.emplace_back(\n-          BufferSequencingEvent::Create(client->thread_pool()));\n-      TF_RETURN_IF_ERROR(\n-          client->AllocateAndRecordEvent(definition_events.back(), local_device,\n-                                         local_device->compute_stream()));\n-    }\n-  } else {\n-    // We have at least one definition event, for the copy completing to\n-    // the device buffers.\n-    if (definition_event) {\n-      definition_events.push_back(definition_event);\n-    } else {\n-      definition_events.emplace_back(\n-          BufferSequencingEvent::Create(client->thread_pool()));\n-    }\n-  }\n-\n-  auto dst_device_buffer = std::make_unique<TrackedDeviceBuffer>(\n-      device, std::move(mem), definition_events);\n-\n-  auto py_buffer = std::make_unique<PjRtStreamExecutorBuffer>(\n-      on_device_shape, std::move(dst_device_buffer), client, device,\n-      memory_space);\n-  return py_buffer;\n-}\n-\n bool PjRtStreamExecutorClient::IsOnCpu(PjRtMemorySpace* memory_space) {\n   return memory_space->kind() == PinnedHostMemorySpace::kKind;\n }\n@@ -916,8 +793,8 @@ PjRtStreamExecutorClient::CreateErrorBuffer(absl::Status error,\n       device, tsl::RCReference<RawSEDeviceMemory>(),\n       absl::MakeSpan(&definition_event, 1));\n \n-  return std::make_unique<PjRtStreamExecutorBuffer>(\n-      shape, std::move(dummy_device_buffer), this, device, memory);\n+  return std::make_unique<CommonPjRtBufferImpl>(\n+      shape, std::move(dummy_device_buffer), memory);\n }\n \n absl::StatusOr<tsl::RCReference<PjRtDeviceEvent>>\n@@ -1011,9 +888,8 @@ PjRtStreamExecutorClient::CreateViewOfDeviceBuffer(\n \n   auto device_buffer = std::make_unique<TrackedDeviceBuffer>(\n       device, std::move(buffer), definition_events);\n-  return std::unique_ptr<PjRtBuffer>(std::make_unique<PjRtStreamExecutorBuffer>(\n-      shape, std::move(device_buffer), this, device,\n-      device->default_memory_space().value_or(nullptr)));\n+  return std::unique_ptr<PjRtBuffer>(std::make_unique<CommonPjRtBufferImpl>(\n+      shape, std::move(device_buffer), memory_space));\n }\n \n absl::Status PjRtStreamExecutorClient::DmaMap(void* data, size_t buffer_size) {\n@@ -1152,127 +1028,6 @@ absl::Span<PjRtMemorySpace* const> PjRtStreamExecutorClient::memory_spaces()\n   return memory_spaces_;\n }\n \n-PjRtStreamExecutorBuffer::PjRtStreamExecutorBuffer(\n-    Shape on_device_shape, std::unique_ptr<TrackedDeviceBuffer> device_buffer,\n-    PjRtClient* client, PjRtDevice* device, PjRtMemorySpace* memory_space)\n-    : CommonPjRtBufferImpl(std::move(on_device_shape), std::move(device_buffer),\n-                           memory_space) {}\n-\n-PjRtStreamExecutorBuffer::~PjRtStreamExecutorBuffer() { Delete(); }\n-\n-absl::StatusOr<tsl::RCReference<RawSEDeviceMemory>>\n-PjRtStreamExecutorBuffer::Release(bool wait_for_operations_to_complete) {\n-  tsl::profiler::TraceMe trace_me(\"PjRtStreamExecutorBuffer::Release\");\n-  std::unique_ptr<TrackedDeviceBuffer> device_buffer(\n-      static_cast<TrackedDeviceBuffer*>(ReleaseBuffer().release()));\n-  if (device_buffer == nullptr) {\n-    return tsl::RCReference<RawSEDeviceMemory>();\n-  }\n-  TrackedDeviceBuffer::StreamAndEventContainer events =\n-      device_buffer->LockUseAndTransferUsageEvents();\n-  auto device_memory = device_buffer->device_memory();\n-  auto* se_device = tensorflow::down_cast<PjRtStreamExecutorDevice*>(device());\n-  LocalDeviceState* local_device_state = se_device->local_device_state();\n-  if (wait_for_operations_to_complete) {\n-    // Block the host until all usage events have completed. Usage events\n-    // dominate definition events, so this also waits for the buffer to be\n-    // defined.\n-    std::unique_ptr<se::Stream> stream;\n-    for (const auto& stream_and_event : events) {\n-      if (!stream_and_event.event->IsComplete()) {\n-        if (stream == nullptr) {\n-          stream = local_device_state->BorrowStreamFromPool();\n-        }\n-        stream_and_event.event->WaitForEventOnStream(stream.get());\n-      }\n-    }\n-    if (stream != nullptr) {\n-      TF_RETURN_IF_ERROR(stream->BlockHostUntilDone());\n-      local_device_state->ReturnStreamToPool(std::move(stream));\n-    }\n-  } else {\n-    if (local_device_state->allocation_model() ==\n-        LocalDeviceState::kComputeSynchronized) {\n-      se::Stream* block_stream = nullptr;\n-      // If an event is not defined yet, we wait for it to be defined in a new\n-      // thread in the thread pool.\n-      // This allows the host to schedule:\n-      //   create buffer -> use -> delete -> fulfill\n-      absl::InlinedVector<BufferSequencingEventRef, 5>\n-          events_to_wait_for_in_a_different_thread;\n-      auto maybe_wait_for_event_on_block_stream_or_add_to_events_to_wait =\n-          [&events_to_wait_for_in_a_different_thread, local_device_state,\n-           &block_stream](const BufferSequencingEventRef& event) {\n-            if (local_device_state->allow_delete_before_fulfill() &&\n-                !event->IsDefined()) {\n-              // Wait for the event to be defined in a different thread.\n-              events_to_wait_for_in_a_different_thread.push_back(event);\n-            } else {\n-              MaybeWaitForEventOnStream(event, local_device_state,\n-                                        block_stream);\n-            }\n-          };\n-      for (const auto& stream_and_event : events) {\n-        VLOG(4)\n-            << \"Checking whether need to wait for stream_and_event: stream: \"\n-            << (stream_and_event.event->IsDefined()\n-                    ? stream_and_event.event->definition_stream()\n-                    : nullptr)\n-            << \"; event: \" << &*stream_and_event.event\n-            << \"; reference_held: \" << stream_and_event.reference_held\n-            << \"; is_predetermined_error: \"\n-            << stream_and_event.event->IsPredeterminedError();\n-        // We only need to do something for events that didn't already acquire a\n-        // reference to the buffer and for other situations described in the\n-        // comment of MaybeWaitForEventOnStream()\n-        if (!stream_and_event.reference_held) {\n-          maybe_wait_for_event_on_block_stream_or_add_to_events_to_wait(\n-              stream_and_event.event);\n-        }\n-      }\n-      for (const auto& definition_event : device_buffer->definition_events()) {\n-        VLOG(4) << \"Checking whether need to wait for definition_event: \"\n-                << &*definition_event << \"; is_predetermined_error: \"\n-                << definition_event->IsPredeterminedError();\n-        // Here we wait for the definition events to complete on block_stream as\n-        // well, in case they are not also usage events.\n-        maybe_wait_for_event_on_block_stream_or_add_to_events_to_wait(\n-            definition_event);\n-      }\n-      if (!events_to_wait_for_in_a_different_thread.empty()) {\n-        VLOG(3) << \"Going to wait for \"\n-                << events_to_wait_for_in_a_different_thread.size()\n-                << \" events in a different thread.\";\n-        // We always use the cleanup_thread instead of using the\n-        // client->thread_pool() here to avoid exhausting the client thread\n-        // pool.\n-        local_device_state->cleanup_thread()->Schedule(\n-            [events_to_wait_for_in_a_different_thread =\n-                 std::move(events_to_wait_for_in_a_different_thread),\n-             local_device_state, device_memory, block_stream]() mutable {\n-              for (const auto& event :\n-                   events_to_wait_for_in_a_different_thread) {\n-                MaybeWaitForEventOnStream(event, local_device_state,\n-                                          block_stream);\n-              }\n-              if (block_stream != nullptr) {\n-                TF_CHECK_OK(local_device_state->ThenExecuteCallback(\n-                    block_stream, [device_memory]() {\n-                      // Drops device_memory shared pointer.\n-                    }));\n-              }\n-            });\n-      } else if (block_stream != nullptr) {\n-        TF_RETURN_IF_ERROR(local_device_state->ThenExecuteCallback(\n-            block_stream, [device_memory]() {\n-              // Drops device_memory shared pointer.\n-            }));\n-      }\n-    }\n-  }\n-  return device_memory;\n-}\n-\n namespace {\n \n // Helper struct for the tuple that is transiently constructed to hold the\n@@ -1449,9 +1204,8 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> OutputBufferHelper(\n                          shape.layout().memory_space()));\n     }\n   }\n-  auto pjrt_buffer = std::make_unique<PjRtStreamExecutorBuffer>(\n-      result_buffer.shape(), std::move(out_buffer), client, device,\n-      memory_space);\n+  auto pjrt_buffer = std::make_unique<CommonPjRtBufferImpl>(\n+      result_buffer.shape(), std::move(out_buffer), memory_space);\n   return std::unique_ptr<PjRtBuffer>(std::move(pjrt_buffer));\n }\n \n@@ -1947,7 +1701,7 @@ PjRtStreamExecutorLoadedExecutable::EnqueueExecution(\n   donation_clashes.reserve(argument_handles.size());\n   for (int i = 0; i < argument_handles.size(); ++i) {\n     auto* handle =\n-        tensorflow::down_cast<PjRtStreamExecutorBuffer*>(argument_handles[i]);\n+        tensorflow::down_cast<CommonPjRtBuffer*>(argument_handles[i]);\n     if (handle->device() != device) {\n       return InvalidArgument(\n           \"Buffer passed to Execute() as argument %d to replica %d is on \""
        },
        {
            "sha": "561b9543311245e7915b3296823d800ce50175a6",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.h",
            "status": "modified",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa21448feaa0a597d4e5bc8ef9811ad2818acbdd/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa21448feaa0a597d4e5bc8ef9811ad2818acbdd/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h?ref=aa21448feaa0a597d4e5bc8ef9811ad2818acbdd",
            "patch": "@@ -421,7 +421,6 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n                          const CommonPjRtRawBuffer& raw_buffer);\n \n  protected:\n-  friend class PjRtStreamExecutorBuffer;\n   friend class PjRtStreamExecutorRawBuffer;\n \n   // Helper function for creating PjRtStreamExecutorExecutables. Modifies\n@@ -526,59 +525,6 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n absl::StatusOr<DeviceAssignment> DevicesToDeviceAssignment(\n     absl::Span<const std::vector<PjRtDevice*>> devices);\n \n-class PjRtStreamExecutorBuffer : public CommonPjRtBufferImpl {\n- public:\n-  PjRtStreamExecutorBuffer(Shape on_device_shape,\n-                           std::unique_ptr<TrackedDeviceBuffer> device_buffer,\n-                           PjRtClient* client, PjRtDevice* device,\n-                           PjRtMemorySpace* memory_space);\n-  ~PjRtStreamExecutorBuffer() override;\n-\n-  PjRtStreamExecutorBuffer(const PjRtStreamExecutorBuffer&) = delete;\n-  PjRtStreamExecutorBuffer(PjRtStreamExecutorBuffer&&) = delete;\n-  PjRtStreamExecutorBuffer& operator=(const PjRtStreamExecutorBuffer&) = delete;\n-  PjRtStreamExecutorBuffer& operator=(PjRtStreamExecutorBuffer&&) = delete;\n-\n-\n-  // Similar to Delete, drops the buffer's reference to its associated device\n-  // memory, leaving the buffer in an invalid state, but returns the\n-  // TrackedDeviceBuffer rather than freeing the device memory, so that another\n-  // framework can take ownership of it.\n-  //\n-  // When called with wait_for_operations_to_complete=false, the buffer returned\n-  // from Release should be dropped on the compute stream, since the only events\n-  // that Release doesn't wait for are events defined on the compute stream.\n-  //\n-  // If wait_for_operations_to_complete=true, the host will block until any\n-  // potentially outstanding asynchronous operations have completed before\n-  // returning, in which case it is safe to read or mutate the returned buffer.\n-  // If the buffer was shared via an external reference it is the client's\n-  // responsibility that accesses via that reference do not interfere with\n-  // accesses via the buffer returned from Release.\n-  absl::StatusOr<tsl::RCReference<RawSEDeviceMemory>> Release(\n-      bool wait_for_operations_to_complete);\n-};\n-\n-// Allocates the device buffers for a buffer that will be used as the\n-// destination of a copy, either from the host or another device. copy_stream\n-// may be nullptr, e.g., when allocating a buffer for a cross-host copy. If the\n-// buffer is a tuple then the tuple tables are allocated, and all necessary\n-// synchronization for them is dealt with, before the buffer is returned.\n-//\n-// It is safe to delete the returned PjRtBuffer without further\n-// synchronization if an error occurs before the buffer is used.\n-//\n-// The caller may optionally provide a definition event to be recorded in\n-// the buffer.\n-// TODO(phawkins): replace on_host_shape here with on_device_shape.\n-absl::StatusOr<std::unique_ptr<PjRtStreamExecutorBuffer>>\n-AllocateDestinationBuffer(const Shape& on_host_shape, PjRtDevice* device,\n-                          LocalDeviceState* local_device,\n-                          se::Stream* copy_stream, bool is_uninitialized_create,\n-                          PjRtStreamExecutorClient* client,\n-                          BufferSequencingEventRef definition_event = nullptr,\n-                          PjRtMemorySpace* memory_space = nullptr);\n-\n // Wraps one or more XLA LocalExecutables (one per partition, as specified by\n // the build options).\n class PjRtStreamExecutorLoadedExecutable : public PjRtLoadedExecutable {"
        }
    ],
    "stats": {
        "total": 318,
        "additions": 9,
        "deletions": 309
    }
}