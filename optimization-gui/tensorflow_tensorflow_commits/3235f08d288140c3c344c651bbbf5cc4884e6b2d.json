{
    "author": "tensorflower-gardener",
    "message": "Deduplicate functions fully if `deduplicateFunctionsFully` option is true.\n\nThe default of `deduplicateFunctionsFully` option is false, and it is not set anywhere yet. Hence this change is no-op.\n\nWhen deduplicating fully, it deduplicates functions on the same name ignoring their input and output shardings. Note it still does not ignore manual axes.\n\nPiperOrigin-RevId: 810819101",
    "sha": "3235f08d288140c3c344c651bbbf5cc4884e6b2d",
    "files": [
        {
            "sha": "75134550a29721358bc303c26d57f050b0cb7be0",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/export_named_computations.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc?ref=3235f08d288140c3c344c651bbbf5cc4884e6b2d",
            "patch": "@@ -108,13 +108,15 @@ StringAttr createFuncOpOrGetFromCache(\n     mlir::IRRewriter& rewriter, SymbolTable& symbolTable,\n     ManualAxesAttr manualAxesAttr,\n     std::optional<TensorShardingPerValueAttr> inShardings,\n-    std::optional<TensorShardingPerValueAttr> outShardings) {\n-  auto key = std::make_tuple(namedComputationOp.getName(),\n-                             namedComputationOp.getInShardings().value_or(\n-                                 TensorShardingPerValueAttr()),\n-                             namedComputationOp.getOutShardings().value_or(\n-                                 TensorShardingPerValueAttr()),\n-                             manualAxesAttr);\n+    std::optional<TensorShardingPerValueAttr> outShardings,\n+    bool dedupFunctionsFully) {\n+  auto key = std::make_tuple(\n+      namedComputationOp.getName(),\n+      dedupFunctionsFully ? TensorShardingPerValueAttr()\n+                          : inShardings.value_or(TensorShardingPerValueAttr()),\n+      dedupFunctionsFully ? TensorShardingPerValueAttr()\n+                          : outShardings.value_or(TensorShardingPerValueAttr()),\n+      manualAxesAttr);\n   if (auto it = funcCache.find(key); it != funcCache.end()) {\n     return it->second;\n   }\n@@ -168,7 +170,7 @@ class ExportNamedComputationsPass\n       }\n       StringAttr funcSymName = createFuncOpOrGetFromCache(\n           namedComputationOp, funcCache, rewriter, symbolTable, manualAxesAttr,\n-          inShardings, outShardings);\n+          inShardings, outShardings, dedupFunctionsFully);\n \n       // Replace the `NamedComputationOp` with a `CallOp`.\n       rewriter.setInsertionPoint(namedComputationOp);\n@@ -204,10 +206,9 @@ class ExportNamedComputationsPass\n       *this, \"dedup-functions-fully\",\n       llvm::cl::desc(\n           \"Whether to deduplicate functions fully, regardless of the input and \"\n-          \"output shardings of functions, and it keeps one function for each \"\n-          \"input function. The default is false, meaning it will deduplicate \"\n-          \"only if the input and output shardings are the same. Not \"\n-          \"implemented yet.\"),\n+          \"output shardings of functions, and it keeps one callee function for \"\n+          \"each caller function. The default is false, meaning it will \"\n+          \"deduplicate only if the input and output shardings are the same.\"),\n       llvm::cl::init(false)};\n };\n "
        },
        {
            "sha": "d089b1a2a9f45f9ee9575a1a39e08aa1c350f510",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_export.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.h?ref=3235f08d288140c3c344c651bbbf5cc4884e6b2d",
            "patch": "@@ -37,9 +37,9 @@ struct StablehloExportPipelineOptions\n       *this, \"dedup-functions-fully\",\n       llvm::cl::desc(\n           \"Whether to deduplicate functions fully, regardless of the input and \"\n-          \"output shardings of functions, and it keeps one function for each \"\n-          \"input function. The default is false, meaning it will deduplicate \"\n-          \"only if the input and output shardings are the same.\"),\n+          \"output shardings of functions, and it keeps one callee function for \"\n+          \"each caller function. The default is false, meaning it will \"\n+          \"deduplicate only if the input and output shardings are the same.\"),\n       llvm::cl::init(false)};\n };\n "
        },
        {
            "sha": "8a8fbd591bdeb9f836fdd8f6fc563721b72393d1",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/export_named_computations.mlir",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations.mlir?ref=3235f08d288140c3c344c651bbbf5cc4884e6b2d",
            "patch": "@@ -780,3 +780,49 @@ func.func @nested_manual_computations(%arg0: tensor<8xf32> {sdy.sharding = #sdy.\n // CHECK-NEXT:    %0 = stablehlo.abs %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}]>]>} : tensor<8xf32>\n // CHECK-NEXT:    return %0 : tensor<8xf32>\n // CHECK-NEXT:  }\n+\n+// -----\n+\n+sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(\n+// CHECK-SAME:      %arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}\n+// CHECK-SAME:      -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n+// CHECK:       %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n+// CHECK-NEXT:    %2 = func.call @foo(%arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+// CHECK-NEXT:    %3 = func.call @foo_0(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+// CHECK-NEXT:    sdy.return %3 : tensor<4xf32>\n+// CHECK-NEXT:  } : (tensor<8xf32>) -> tensor<8xf32>\n+// CHECK-NEXT:  %1 = call @foo_1(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : (tensor<8xf32>) -> tensor<8xf32>\n+// CHECK-NEXT:  return %1 : tensor<8xf32>\n+func.func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n+  %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n+    %1 = sdy.named_computation<\"foo\">(%arg1) in_shardings=[<@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}]>] (%arg2: tensor<4xf32>) {\n+      %2 = stablehlo.abs %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<4xf32>\n+      sdy.return %2 : tensor<4xf32>\n+    } {xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+    %3 = sdy.named_computation<\"foo\">(%1) in_shardings=[<@mesh, [{}]>] out_shardings=[<@mesh, [{}]>] (%arg2: tensor<4xf32>) {\n+      %4 = stablehlo.abs %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<4xf32>\n+      sdy.return %4 : tensor<4xf32>\n+    } {xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+    sdy.return %3 : tensor<4xf32>\n+  } : (tensor<8xf32>) -> tensor<8xf32>\n+  %5 = sdy.named_computation<\"foo\">(%0) in_shardings=[<@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}]>] (%arg1: tensor<8xf32>) {\n+    %6 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<8xf32>\n+    sdy.return %6 : tensor<8xf32>\n+  } : (tensor<8xf32>) -> tensor<8xf32>\n+  return %5 : tensor<8xf32>\n+}\n+\n+// CHECK-LABEL: func private @foo(\n+// CHECK-SAME:      %arg0: tensor<4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>}\n+// CHECK-SAME:      -> (tensor<4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>}) {\n+\n+// CHECK-LABEL: func private @foo_0(\n+// CHECK-SAME:      %arg0: tensor<4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>}\n+// CHECK-SAME:      -> (tensor<4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>}) {\n+\n+// CHECK-LABEL: func private @foo_1(\n+// CHECK-SAME:      %arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>}\n+// CHECK-SAME:      -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>}) {\n+"
        },
        {
            "sha": "96c07bbbd1a9a3a9012883d5d6b8c86b6fddf9ed",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/export_named_computations_deduplicate_functions_fully.mlir",
            "status": "added",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3235f08d288140c3c344c651bbbf5cc4884e6b2d/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir?ref=3235f08d288140c3c344c651bbbf5cc4884e6b2d",
            "patch": "@@ -0,0 +1,64 @@\n+// RUN: sdy_opt %s -xla-sdy-export-named-computations='dedup-functions-fully=true' -split-input-file | FileCheck %s\n+\n+sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @multiple_same_named_computations_different_shardings(\n+func.func @multiple_same_named_computations_different_shardings(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}, {\"x\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n+  // CHECK-NEXT: %0 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %1 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: return %1 : tensor<8x2xi32>\n+  %0 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {}]>] (%arg1: tensor<8x2xi32>) {\n+    %2 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>} : tensor<8x2xi32>\n+    sdy.return %2 : tensor<8x2xi32>\n+  } : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+    %3 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>} : tensor<8x2xi32>\n+    sdy.return %3 : tensor<8x2xi32>\n+  } : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-LABEL: func private @baz(\n+// CHECK-SAME:    %arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>})\n+// CHECK-SAME:    -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>})\n+\n+// -----\n+\n+sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(\n+// CHECK-SAME:      %arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}\n+// CHECK-SAME:      -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n+// CHECK:       %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n+// CHECK-NEXT:    %2 = func.call @foo(%arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+// CHECK-NEXT:    %3 = func.call @foo(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+// CHECK-NEXT:    sdy.return %3 : tensor<4xf32>\n+// CHECK-NEXT:  } : (tensor<8xf32>) -> tensor<8xf32>\n+// CHECK-NEXT:  %1 = call @foo_0(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : (tensor<8xf32>) -> tensor<8xf32>\n+// CHECK-NEXT:  return %1 : tensor<8xf32>\n+func.func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n+  %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n+    %1 = sdy.named_computation<\"foo\">(%arg1) in_shardings=[<@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}]>] (%arg2: tensor<4xf32>) {\n+      %2 = stablehlo.abs %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<4xf32>\n+      sdy.return %2 : tensor<4xf32>\n+    } {xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+    %3 = sdy.named_computation<\"foo\">(%1) in_shardings=[<@mesh, [{}]>] out_shardings=[<@mesh, [{}]>] (%arg2: tensor<4xf32>) {\n+      %4 = stablehlo.abs %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<4xf32>\n+      sdy.return %4 : tensor<4xf32>\n+    } {xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+    sdy.return %3 : tensor<4xf32>\n+  } : (tensor<8xf32>) -> tensor<8xf32>\n+  %5 = sdy.named_computation<\"foo\">(%0) in_shardings=[<@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}]>] (%arg1: tensor<8xf32>) {\n+    %6 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<8xf32>\n+    sdy.return %6 : tensor<8xf32>\n+  } : (tensor<8xf32>) -> tensor<8xf32>\n+  return %5 : tensor<8xf32>\n+}\n+\n+// CHECK-LABEL: func private @foo(\n+// CHECK-SAME:      %arg0: tensor<4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>}\n+// CHECK-SAME:      -> (tensor<4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>}) {\n+\n+// CHECK-LABEL: func private @foo_0(\n+// CHECK-SAME:      %arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>}\n+// CHECK-SAME:      -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>}) {"
        }
    ],
    "stats": {
        "total": 141,
        "additions": 126,
        "deletions": 15
    }
}