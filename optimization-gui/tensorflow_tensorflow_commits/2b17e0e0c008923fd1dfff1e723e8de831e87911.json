{
    "author": "liepieshov",
    "message": "Support SparseActivationsUnstack and SparseActivationsUnstackInterleaved custom call always return tuple result\n\nPiperOrigin-RevId: 819743515",
    "sha": "2b17e0e0c008923fd1dfff1e723e8de831e87911",
    "files": [
        {
            "sha": "bb5f52e4783769d545b1c60409974477130a9317",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 3,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2b17e0e0c008923fd1dfff1e723e8de831e87911/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2b17e0e0c008923fd1dfff1e723e8de831e87911/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=2b17e0e0c008923fd1dfff1e723e8de831e87911",
            "patch": "@@ -136,6 +136,9 @@ using ::tsl::uint8;\n constexpr char kAggregateToTopk[] = \"aggregate_to_topk\";\n constexpr char kApiVersion[] = \"api_version\";\n constexpr char kApproxTopK[] = \"ApproxTopK\";\n+constexpr char kSparseActivationsUnstack[] = \"SparseActivationsUnstack\";\n+constexpr char kSparseActivationsUnstackInterleaved[] =\n+    \"SparseActivationsUnstackInterleaved\";\n constexpr char kBackendConfig[] = \"backend_config\";\n constexpr char kCallTargetName[] = \"call_target_name\";\n constexpr char kCalledComputations[] = \"called_computations\";\n@@ -170,6 +173,11 @@ T* Unwrap(const std::unique_ptr<T>& t) {\n   return t.get();\n }\n \n+constexpr bool CustomCallOpReturnTuple(absl::string_view name) {\n+  return name == kSparseActivationsUnstack ||\n+         name == kSparseActivationsUnstackInterleaved;\n+}\n+\n static mlir::LogicalResult GetXlaOp(\n     mlir::Value val, const llvm::DenseMap<mlir::Value, xla::XlaOp>& val_map,\n     xla::XlaOp* result, mlir::Operation* op) {\n@@ -1315,7 +1323,6 @@ void BuildGetTupleElementsForTupleResults(\n     mlir::Operation* op, xla::XlaOp tuple, xla::XlaBuilder* builder,\n     llvm::DenseMap<mlir::Value, xla::XlaOp>& values,\n     unsigned num_implicit_results = 0) {\n-\n   const std::optional<xla::OpSharding>& sharding = builder->sharding();\n   if (sharding.has_value()) {\n     bool is_tuple_sharding = sharding->type() == xla::OpSharding::TUPLE;\n@@ -2658,6 +2665,11 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n     }\n     result_shape = xla::ShapeUtil::MakeTupleShape(subshapes);\n   }\n+  bool return_tuple = false;\n+  if (!result_shape.IsTuple() && CustomCallOpReturnTuple(call_target_name)) {\n+    return_tuple = true;\n+    result_shape = xla::ShapeUtil::MakeTupleShape({result_shape});\n+  }\n \n   xla::XlaOp custom_call;\n   if (op.getCalledComputations().size() == 1 && op.getOperandLayouts() &&\n@@ -2705,7 +2717,7 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n         custom_call_schedule, *xla_api_version);\n   }\n \n-  if (op->getNumResults() == 1) {\n+  if (op->getNumResults() == 1 && !return_tuple) {\n     value_map[op.getResult(0)] = custom_call;\n   } else {\n     BuildGetTupleElementsForTupleResults(op, custom_call, ctx);\n@@ -4364,6 +4376,11 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n     }\n     result_shape = xla::ShapeUtil::MakeTupleShape(subshapes);\n   }\n+  bool return_tuple = false;\n+  if (!result_shape.IsTuple() && CustomCallOpReturnTuple(call_target_name)) {\n+    return_tuple = true;\n+    result_shape = xla::ShapeUtil::MakeTupleShape({result_shape});\n+  }\n \n   xla::XlaOp custom_call;\n   if (op.getCalledComputations().size() == 1 && op.getOperandLayouts() &&\n@@ -4409,7 +4426,7 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n         *custom_call_schedule, *xla_api_version);\n   }\n \n-  if (op->getNumResults() == 1) {\n+  if (op->getNumResults() == 1 && !return_tuple) {\n     value_map[op.getResult(0)] = custom_call;\n   } else {\n     BuildGetTupleElementsForTupleResults(op, custom_call, ctx);"
        },
        {
            "sha": "5e32a9f13be4246fbced27704a1a3e8cd6820c06",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/export.mlir",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2b17e0e0c008923fd1dfff1e723e8de831e87911/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2b17e0e0c008923fd1dfff1e723e8de831e87911/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir?ref=2b17e0e0c008923fd1dfff1e723e8de831e87911",
            "patch": "@@ -3342,3 +3342,49 @@ func.func @main(%arg0: tensor<i1>, %arg1: memref<2xf32>) -> memref<2xf32> {\n     }\n   func.return %0#1: memref<2xf32>\n }\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstack\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"mhlo.custom_call\"(%arg0) {call_target_name = \"SparseActivationsUnstack\", backend_config = \"\", operand_layouts = [dense<[1, 0]> : tensor<2xindex>], result_layout = [dense<[0, 1]> : tensor<2xindex>], result_layouts = [dense<[0, 1]> : tensor<2xindex>], xla_shape = \"(f32[8,6]{0,1})\"} : (tensor<8x8xf32>) -> tensor<8x6xf32>\n+  func.return %0: tensor<8x6xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstackInterleaved\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"mhlo.custom_call\"(%arg0) {call_target_name = \"SparseActivationsUnstackInterleaved\", backend_config = \"\", operand_layouts = [dense<[1, 0]> : tensor<2xindex>], result_layout = [dense<[0, 1]> : tensor<2xindex>], result_layouts = [dense<[0, 1]> : tensor<2xindex>], xla_shape = \"(f32[8,6]{0,1})\"} : (tensor<8x8xf32>) -> tensor<8x6xf32>\n+  func.return %0: tensor<8x6xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstack\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"mhlo.custom_call\"(%arg0) {call_target_name = \"SparseActivationsUnstack\", backend_config = \"\", operand_layouts = [dense<[1, 0]> : tensor<2xindex>], result_layout = [dense<[0, 1]> : tensor<2xindex>], result_layouts = [dense<[0, 1]> : tensor<2xindex>], xla_shape = \"(f32[8,6]{0,1})\"} : (tensor<8x8xf32>) -> tuple<tensor<8x6xf32>>\n+  %1 = mhlo.get_tuple_element %0[0] : (tuple<tensor<8x6xf32>>) -> tensor<8x6xf32>\n+  func.return %1: tensor<8x6xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6], f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstack\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"mhlo.custom_call\"(%arg0, %arg0) {call_target_name = \"SparseActivationsUnstack\", backend_config = \"\", xla_shape = \"(f32[8,6]{0,1}, f32[8,6]{0,1})\"} : (tensor<8x8xf32>, tensor<8x8xf32>) -> tuple<tensor<8x6xf32>, tensor<8x6xf32>>\n+  %1 = mhlo.get_tuple_element %0[0] : (tuple<tensor<8x6xf32>, tensor<8x6xf32>>) -> tensor<8x6xf32>\n+  func.return %1: tensor<8x6xf32>\n+}"
        },
        {
            "sha": "fa099a9e75b81dda20c67b84bd85f88ef9a2f935",
            "filename": "third_party/xla/xla/hlo/translate/tests/stablehlo.mlir",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2b17e0e0c008923fd1dfff1e723e8de831e87911/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2b17e0e0c008923fd1dfff1e723e8de831e87911/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir?ref=2b17e0e0c008923fd1dfff1e723e8de831e87911",
            "patch": "@@ -2169,3 +2169,49 @@ func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<i32>) -> tensor<?xf32> {\n   %cast = tensor.cast %0 : tensor<?xf32, #stablehlo.bounds<4>> to tensor<?xf32>\n   return %cast : tensor<?xf32>\n }\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstackInterleaved\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"stablehlo.custom_call\"(%arg0) {call_target_name = \"SparseActivationsUnstackInterleaved\", backend_config = \"\", operand_layouts = [dense<[1, 0]> : tensor<2xindex>], result_layout = [dense<[0, 1]> : tensor<2xindex>], result_layouts = [dense<[0, 1]> : tensor<2xindex>], xla_shape = \"(f32[8,6]{0,1})\"} : (tensor<8x8xf32>) -> tensor<8x6xf32>\n+  func.return %0: tensor<8x6xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstack\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"stablehlo.custom_call\"(%arg0) {call_target_name = \"SparseActivationsUnstack\", backend_config = \"\", operand_layouts = [dense<[1, 0]> : tensor<2xindex>], result_layout = [dense<[0, 1]> : tensor<2xindex>], result_layouts = [dense<[0, 1]> : tensor<2xindex>], xla_shape = \"(f32[8,6]{0,1})\"} : (tensor<8x8xf32>) -> tensor<8x6xf32>\n+  func.return %0: tensor<8x6xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstack\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"stablehlo.custom_call\"(%arg0) {call_target_name = \"SparseActivationsUnstack\", backend_config = \"\", operand_layouts = [dense<[1, 0]> : tensor<2xindex>], result_layout = [dense<[0, 1]> : tensor<2xindex>], result_layouts = [dense<[0, 1]> : tensor<2xindex>], xla_shape = \"(f32[8,6]{0,1})\"} : (tensor<8x8xf32>) -> tuple<tensor<8x6xf32>>\n+  %1 = stablehlo.get_tuple_element %0[0] : (tuple<tensor<8x6xf32>>) -> tensor<8x6xf32>\n+  func.return %1: tensor<8x6xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x6xf32> {\n+  // CHECK: [[RES:%custom.*]] = (f32[8,6], f32[8,6]) custom-call(%{{.*}}), custom_call_target=\"SparseActivationsUnstack\"\n+  // CHECK-NEXT: ROOT %[[ROOT:.*]] = f32[8,6] get-tuple-element([[RES]]), index=0\n+  %0 = \"stablehlo.custom_call\"(%arg0, %arg0) {call_target_name = \"SparseActivationsUnstack\", backend_config = \"\", xla_shape = \"(f32[8,6]{0,1}, f32[8,6]{0,1})\"} : (tensor<8x8xf32>, tensor<8x8xf32>) -> tuple<tensor<8x6xf32>, tensor<8x6xf32>>\n+  %1 = stablehlo.get_tuple_element %0[0] : (tuple<tensor<8x6xf32>, tensor<8x6xf32>>) -> tensor<8x6xf32>\n+  func.return %1: tensor<8x6xf32>\n+}"
        }
    ],
    "stats": {
        "total": 115,
        "additions": 112,
        "deletions": 3
    }
}