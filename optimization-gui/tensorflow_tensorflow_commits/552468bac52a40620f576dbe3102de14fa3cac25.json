{
    "author": "derdrdirk",
    "message": "[XLA:GPU] Enable AutotunerPass for ConvAlgorithmPicker for NVIDIA, not AMD, if --xla_gpu_experimental_use_autotuner_pass == true.\n\nPiperOrigin-RevId: 814171739",
    "sha": "552468bac52a40620f576dbe3102de14fa3cac25",
    "files": [
        {
            "sha": "0d77d56c35e518df9132c0762e1d2ac4a050de5a",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/552468bac52a40620f576dbe3102de14fa3cac25/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/552468bac52a40620f576dbe3102de14fa3cac25/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=552468bac52a40620f576dbe3102de14fa3cac25",
            "patch": "@@ -1922,6 +1922,7 @@ cc_library(\n         \"//xla/backends/gpu/autotuner:block_level_emitter\",\n         \"//xla/backends/gpu/autotuner:cublas\",\n         \"//xla/backends/gpu/autotuner:cublaslt\",\n+        \"//xla/backends/gpu/autotuner:cudnn\",\n         \"//xla/backends/gpu/autotuner:native_emitter\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\","
        },
        {
            "sha": "075c3ecda0c524d24e687a7624ba7330be773b82",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/552468bac52a40620f576dbe3102de14fa3cac25/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/552468bac52a40620f576dbe3102de14fa3cac25/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=552468bac52a40620f576dbe3102de14fa3cac25",
            "patch": "@@ -240,12 +240,12 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n     return absl::OkStatus();\n   }\n \n-  // TODO(b/407495801): Cached Gemm as well as Conv autotuning results are\n-  // loaded in the GpuConvAlgorithmPicker but should be loaded in the autotuner.\n+  // TODO(b/407494793): Remove the GpuConvAlgorithmPicker and use the autotuner\n+  // it supports ROCM.\n   pipeline->AddPass<GpuConvAlgorithmPicker>(autotune_config);\n \n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n-  // TODO: b/407494793 - Add proper support for ROCM. Currently the Cublas\n+  // TODO(b/407494793): - Add proper support for ROCM. Currently the Cublas\n   // backend uses the same API as rocBLAS.\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_exec, &debug_options, this, target_config));"
        },
        {
            "sha": "23e89262db4eff14e490377776ccbc31270e20f9",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/552468bac52a40620f576dbe3102de14fa3cac25/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/552468bac52a40620f576dbe3102de14fa3cac25/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=552468bac52a40620f576dbe3102de14fa3cac25",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/block_level_emitter.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n+#include \"xla/backends/gpu/autotuner/cudnn.h\"\n #include \"xla/backends/gpu/autotuner/native_emitter.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -338,22 +339,17 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n     return absl::OkStatus();\n   }\n \n-  // TODO(b/407495801): Cached Gemm as well as Conv autotuning results are\n-  // loaded in the GpuConvAlgorithmPicker but should be loaded in the autotuner.\n-  pipeline->AddPass<GpuConvAlgorithmPicker>(autotune_config);\n-\n-  if (stream_exec == nullptr) {\n-    return absl::OkStatus();\n-  }\n-\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_exec, &debug_options, this, target_config));\n   backends.push_back(std::make_unique<CublasLtBackend>(\n       stream_exec, &debug_options, this, target_config));\n+  backends.push_back(std::make_unique<CudnnBackend>(stream_exec, &debug_options,\n+                                                    this, target_config));\n   auto should_autotune = [](const HloInstruction& instruction) -> bool {\n     return instruction.opcode() == HloOpcode::kCustomCall &&\n-           IsCublasGemm(instruction);\n+           (IsCublasGemm(instruction) ||\n+            IsCustomCallToDnnConvolution(instruction));\n   };\n \n   TF_ASSIGN_OR_RETURN("
        }
    ],
    "stats": {
        "total": 21,
        "additions": 9,
        "deletions": 12
    }
}