{
    "author": "hyeontaek",
    "message": "[PjRt-IFRT] Internally track the output spec of `ifrt::PjRtExecutable`\n\nThis change adds dtype/shape/sharding/layout discovery within `ifrt::PjRtExecutable`. This closely matches the internals of `ifrt::PjRtLoadedExecutable`.\n\nThis output spec information is not used at the moment, but will be used for implementing unified `Serialize()` methods that store IFRT-level metadata of serialized executables in the `SerializedXlaExecutableMetadata` proto format (and make this information preserved across serialization/deserialization roundtrip).\n\nPiperOrigin-RevId: 845909500",
    "sha": "1262b408a282a7c1a98685eb5b7ed5198430f91f",
    "files": [
        {
            "sha": "a700a495ca48a8e186800c13f8922bd539af0a7e",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 17,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1262b408a282a7c1a98685eb5b7ed5198430f91f/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1262b408a282a7c1a98685eb5b7ed5198430f91f/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=1262b408a282a7c1a98685eb5b7ed5198430f91f",
            "patch": "@@ -33,6 +33,8 @@ limitations under the License.\n #include \"llvm/Support/Casting.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/Types.h\"\n #include \"xla/ffi/execution_context.h\"\n #include \"xla/ffi/type_registry.h\"\n #include \"xla/future.h\"\n@@ -345,6 +347,23 @@ absl::StatusOr<std::vector<xla::LayoutMode>> GetOutputLayoutModesFromHloModules(\n                         output_dtypes.size());\n }\n \n+// Returns a list of result shapes from the given MLIR module.\n+absl::StatusOr<std::vector<xla::Shape>> ResultShapesOfModule(\n+    mlir::ModuleOp module) {\n+  mlir::func::FuncOp main = module.lookupSymbol<mlir::func::FuncOp>(\"main\");\n+  if (!main) {\n+    return InvalidArgument(\"MLIR module has no main function\");\n+  }\n+  mlir::FunctionType type = main.getFunctionType();\n+  std::vector<xla::Shape> result_shapes;\n+  result_shapes.reserve(type.getNumResults());\n+  for (unsigned i = 0; i < type.getNumResults(); ++i) {\n+    mlir::Type result_type = type.getResult(i);\n+    result_shapes.push_back(xla::TypeToShape(result_type));\n+  }\n+  return result_shapes;\n+}\n+\n // Returns a new `DeviceListRef` that contains the addressable devices of the\n // PjRt executable if the supplied `executable_devices` has an incomplete set of\n // devices.\n@@ -421,12 +440,56 @@ char PjRtLoadedExecutable::ID = 0;\n absl::StatusOr<ExecutableRef> PjRtExecutable::Create(\n     mlir::ModuleOp module, xla::CompileOptions compile_options,\n     const xla::PjRtTopologyDescription& topology) {\n+  // We have to do process the MLIR before the compile call, since the latter\n+  // will use the MLIR as scratch space, or possibly even deallocate it.\n+  TF_ASSIGN_OR_RETURN(\n+      const std::vector<xla::Shape> mlir_module_output_xla_shapes,\n+      ResultShapesOfModule(module));\n+  TF_ASSIGN_OR_RETURN(const std::vector<xla::LayoutMode> output_layout_modes,\n+                      GetOutputLayoutModes(module));\n+\n   TF_ASSIGN_OR_RETURN(auto pjrt_executable,\n                       PjRtCompile(std::move(compile_options), std::move(module),\n                                   topology, /*client=*/nullptr));\n-  return ExecutableRef(new PjRtExecutable(std::move(pjrt_executable)));\n+\n+  TF_ASSIGN_OR_RETURN(auto output_dtypes_and_shapes,\n+                      GetDTypesAndShapes(mlir_module_output_xla_shapes));\n+  std::vector<DType> output_dtypes = std::move(output_dtypes_and_shapes.first);\n+  std::vector<Shape> output_shapes = std::move(output_dtypes_and_shapes.second);\n+  TF_ASSIGN_OR_RETURN(\n+      std::optional<std::vector<xla::HloSharding>> output_hlo_shardings,\n+      GetHloShardings(pjrt_executable->GetOutputShardings(), output_dtypes,\n+                      /*is_output=*/true));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<absl::string_view> output_memory_kinds,\n+      GetMemoryKinds(pjrt_executable->GetOutputMemoryKinds(), output_dtypes));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+          output_layouts,\n+      GetLayouts(pjrt_executable->GetOutputLayouts(), output_layout_modes));\n+\n+  return ExecutableRef(new PjRtExecutable(\n+      std::move(pjrt_executable), std::move(output_dtypes),\n+      std::move(output_shapes), std::move(output_hlo_shardings),\n+      std::move(output_memory_kinds), std::move(output_layouts)));\n }\n \n+PjRtExecutable::PjRtExecutable(\n+    std::shared_ptr<xla::PjRtExecutable> pjrt_executable,\n+    std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n+    std::optional<std::vector<xla::HloSharding>> output_hlo_shardings,\n+    std::vector<absl::string_view> output_memory_kinds,\n+    std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+        output_layouts)\n+    : pjrt_executable_(std::move(pjrt_executable)),\n+      output_dtypes_(std::move(output_dtypes)),\n+      output_shapes_(std::move(output_shapes)),\n+      output_hlo_shardings_(std::move(output_hlo_shardings)),\n+      output_memory_kinds_(std::move(output_memory_kinds)),\n+      output_layouts_(std::move(output_layouts)) {}\n+\n absl::StatusOr<std::optional<std::string>> PjRtExecutable::Fingerprint() const {\n   DCHECK(this);\n   return pjrt_executable_->FingerprintExecutable();\n@@ -507,22 +570,6 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n       std::move(output_layouts)));\n }\n \n-static absl::StatusOr<std::vector<xla::Shape>> ResultShapesOfModule(\n-    mlir::ModuleOp module) {\n-  auto main = module.lookupSymbol<mlir::func::FuncOp>(\"main\");\n-  if (!main) {\n-    return InvalidArgument(\"MLIR module has no main function\");\n-  }\n-  auto type = main.getFunctionType();\n-  std::vector<xla::Shape> result_shapes;\n-  result_shapes.reserve(type.getNumResults());\n-  for (unsigned i = 0; i < type.getNumResults(); ++i) {\n-    auto result_type = type.getResult(i);\n-    result_shapes.push_back(xla::TypeToShape(result_type));\n-  }\n-  return result_shapes;\n-}\n-\n absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n     PjRtClient* client, mlir::ModuleOp module,\n     xla::CompileOptions compile_options,"
        },
        {
            "sha": "976a905b7afefc65093c0f08c4aef65cc320938a",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.h",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1262b408a282a7c1a98685eb5b7ed5198430f91f/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1262b408a282a7c1a98685eb5b7ed5198430f91f/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h?ref=1262b408a282a7c1a98685eb5b7ed5198430f91f",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/base/attributes.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n@@ -32,6 +31,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/Support/ExtensibleRTTI.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n@@ -175,10 +175,23 @@ class PjRtExecutable final\n   static char ID;  // NOLINT\n \n  protected:\n-  explicit PjRtExecutable(std::shared_ptr<xla::PjRtExecutable> pjrt_executable)\n-      : pjrt_executable_(std::move(pjrt_executable)) {}\n+  PjRtExecutable(\n+      std::shared_ptr<xla::PjRtExecutable> pjrt_executable,\n+      std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n+      std::optional<std::vector<xla::HloSharding>> output_hlo_shardings,\n+      std::vector<absl::string_view> output_memory_kinds,\n+      std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+          output_layouts);\n \n   std::shared_ptr<xla::PjRtExecutable> pjrt_executable_;\n+\n+  // Output array specs.\n+  std::vector<DType> output_dtypes_;\n+  std::vector<Shape> output_shapes_;\n+  std::optional<std::vector<xla::HloSharding>> output_hlo_shardings_;\n+  std::vector<absl::string_view> output_memory_kinds_;\n+  std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+      output_layouts_;\n };\n \n // `LoadedExecutable` implementation that wraps a `xla::PjRtLoadedExecutable`."
        }
    ],
    "stats": {
        "total": 100,
        "additions": 80,
        "deletions": 20
    }
}