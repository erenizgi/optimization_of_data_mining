{
    "author": "thcmbs",
    "message": "[XLA:GPU] Only mark HoistFusedBitcasts as changed when modifications occur.\n\nPiperOrigin-RevId: 846719569",
    "sha": "6df9bab4d35808b4faa6d79b4671e61417629290",
    "files": [
        {
            "sha": "1f3979a00abb012b3cf54417d81039761e2e49db",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6df9bab4d35808b4faa6d79b4671e61417629290/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6df9bab4d35808b4faa6d79b4671e61417629290/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=6df9bab4d35808b4faa6d79b4671e61417629290",
            "patch": "@@ -1891,6 +1891,7 @@ cc_library(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "ca1a24b835ec1479cc272f77a2b4a27789f49eb4",
            "filename": "third_party/xla/xla/service/gpu/transforms/hoist_fused_bitcasts.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 14,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6df9bab4d35808b4faa6d79b4671e61417629290/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fhoist_fused_bitcasts.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6df9bab4d35808b4faa6d79b4671e61417629290/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fhoist_fused_bitcasts.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fhoist_fused_bitcasts.cc?ref=6df9bab4d35808b4faa6d79b4671e61417629290",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -799,15 +800,15 @@ absl::Status HoistBitcastUpwardsToCallers(HloInstruction* bitcast,\n // shape. The bitcast is chosen so that it cancels out bitcasts and reshapes\n // along the way up to the dot. Updates the callers of the dot to expect the new\n // root shape.\n-absl::Status MaybeInsertRootBitcast(HloInstruction* dot,\n-                                    absl::Span<HloInstruction*> callers) {\n+absl::StatusOr<bool> MaybeInsertRootBitcast(\n+    HloInstruction* dot, absl::Span<HloInstruction*> callers) {\n   TF_ASSIGN_OR_RETURN(Shape root_shape,\n                       ComputeRootShapeAfterHoistingBitcasts(dot));\n \n   HloComputation* computation = dot->parent();\n   HloInstruction* root = computation->root_instruction();\n   if (root->shape() == root_shape) {\n-    return absl::OkStatus();\n+    return false;\n   }\n \n   // Insert a new bitcast at the root.\n@@ -822,24 +823,28 @@ absl::Status MaybeInsertRootBitcast(HloInstruction* dot,\n     *caller->mutable_shape() = root_shape;\n   }\n \n-  return absl::OkStatus();\n+  return true;\n }\n \n // Try hoisting bitcasts and reshapes in the computation away from 'dot' to the\n // callers of the computation. Some bitcasts or reshapes may remain in the\n // computation, because they cannot be hoisted across all ops, e.g. across some\n // transposes and broadcasts. This is not reported as an error.\n-absl::Status TryHoistBitcastsInComputationToCallers(HloInstruction* dot,\n-                                                    CallGraph* call_graph) {\n+absl::StatusOr<bool> TryHoistBitcastsInComputationToCallers(\n+    HloInstruction* dot, CallGraph* call_graph) {\n+  bool changed = false;\n   // Instead of implementing a logic to hoist bitcast upwards and downwards\n   // we insert a bitcast at the root that and always hoist bitcasts upwards.\n   // That significantly simplifies the implementation.\n   VLOG(2) << \"Before hoisting bitcasts: \" << dot->parent()->ToString();\n \n   auto callers = call_graph->GetComputationCallers(dot->parent());\n-  if (auto status = MaybeInsertRootBitcast(dot, absl::MakeSpan(callers));\n-      !status.ok()) {\n-    VLOG(2) << \"Failed to insert root bitcast: \" << status;\n+  absl::StatusOr<bool> inserted =\n+      MaybeInsertRootBitcast(dot, absl::MakeSpan(callers));\n+  if (!inserted.ok()) {\n+    VLOG(2) << \"Failed to insert root bitcast: \" << inserted.status();\n+  } else {\n+    changed |= *inserted;\n   }\n   VLOG(2) << \"After inserting root bitcast: \" << dot->parent()->ToString();\n \n@@ -856,11 +861,13 @@ absl::Status TryHoistBitcastsInComputationToCallers(HloInstruction* dot,\n     if (!status.ok()) {\n       VLOG(2) << \"Failed to hoist \" << instruction->ToString()\n               << \" upwards: \" << status;\n+    } else {\n+      changed = true;\n     }\n   }\n \n   VLOG(2) << \"After hoisting bitcasts: \" << dot->parent()->ToString();\n-  return absl::OkStatus();\n+  return changed;\n }\n \n class HoistFusedBitcastsVisitor : public DfsHloRewriteVisitor {\n@@ -884,10 +891,11 @@ class HoistFusedBitcastsVisitor : public DfsHloRewriteVisitor {\n       }\n     }\n \n-    TF_RETURN_IF_ERROR(\n-        TryHoistBitcastsInComputationToCallers(instr, call_graph));\n-    // TODO(b/446827313): don't mark as changed if no changes were made.\n-    MarkAsChanged();\n+    ASSIGN_OR_RETURN(bool changed,\n+                     TryHoistBitcastsInComputationToCallers(instr, call_graph));\n+    if (changed) {\n+      MarkAsChanged();\n+    }\n     return absl::OkStatus();\n   }\n "
        },
        {
            "sha": "f6cde67faa95d6f2e4b96d066d0583745d38b1c5",
            "filename": "third_party/xla/xla/service/gpu/transforms/hoist_fused_bitcasts_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6df9bab4d35808b4faa6d79b4671e61417629290/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fhoist_fused_bitcasts_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6df9bab4d35808b4faa6d79b4671e61417629290/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fhoist_fused_bitcasts_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fhoist_fused_bitcasts_test.cc?ref=6df9bab4d35808b4faa6d79b4671e61417629290",
            "patch": "@@ -535,7 +535,8 @@ ENTRY e {\n     \"split_k\":1,\"num_stages\":1,\"num_warps\":4,\"num_ctas\":1}}}}\n )\";\n   std::unique_ptr<VerifiedHloModule> module =\n-      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)));\n+      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)),\n+                            /*expect_change=*/false);\n   // Cos should not be rewritten as we cannot hoist bitcast.\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -571,7 +572,8 @@ ENTRY e {\n     \"split_k\":1,\"num_stages\":1,\"num_warps\":4,\"num_ctas\":1}}}}\n )\";\n   std::unique_ptr<VerifiedHloModule> module =\n-      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)));\n+      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)),\n+                            /*expect_change=*/false);\n   // Cos should not be rewritten as we cannot hoist bitcast.\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -834,7 +836,8 @@ ENTRY e {\n     \"split_k\":1,\"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}}\n )\";\n   std::unique_ptr<VerifiedHloModule> module =\n-      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)));\n+      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)),\n+                            /*expect_change=*/false);\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK:      transpose\n@@ -866,7 +869,8 @@ ENTRY e {\n     \"split_k\":1,\"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}}\n )\";\n   std::unique_ptr<VerifiedHloModule> module =\n-      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)));\n+      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)),\n+                            /*expect_change=*/false);\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n CHECK:      f32[2,3,5]{2,1,0} $0\n@@ -1010,7 +1014,7 @@ CHECK-SAME: dimensions={0,1}\n }\n \n TEST_P(HoistFusedBitcastsReshapeTest,\n-       BitcastsAreHoistedDownThroughBroadcastsWithNonDefaultLayout) {\n+       BitcastsAreNotHoistedDownThroughBroadcastsWithNonDefaultLayout) {\n   HloOpcode opcode = GetParam();\n   absl::string_view hlo = R\"(\n triton_dot {\n@@ -1031,7 +1035,8 @@ ENTRY e {\n     \"split_k\":1,\"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}}\n )\";\n   std::unique_ptr<VerifiedHloModule> module =\n-      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)));\n+      RunHoistFusedBitcasts(absl::Substitute(hlo, HloOpcodeString(opcode)),\n+                            /*expect_change=*/false);\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n CHECK:      f32[2,3,5]{2,1,0} $0(dot)"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 34,
        "deletions": 20
    }
}