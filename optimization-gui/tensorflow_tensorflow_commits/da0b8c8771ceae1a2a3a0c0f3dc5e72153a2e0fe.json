{
    "author": "hawkinsp",
    "message": "[PJRT] Simplify and broaden the interface of TransposePlan.\n\n* allow both an input striding and an input tiling. There's no reason these are mutually exclusive. If both a striding and a tiling are provided, the striding is the stride between tiles.\n* use std::optional<> for input and output stridings and tilings.\n* refactor test code so it supports input striding and tiling, and make a few small cleanup.\n\nPiperOrigin-RevId: 845287645",
    "sha": "da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
    "files": [
        {
            "sha": "129a2a7dd32df3389e3679020f6cab922298371f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/copy_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fcopy_thunk.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -84,7 +84,7 @@ CopyThunk::CopyThunk(Info info, BufferAllocation::Slice src_buffer,\n \n     auto byte_strides = ShapeUtil::ByteStrides(src_shape_);\n     CHECK(byte_strides.has_value());\n-    options.input_layout = TransposePlan::Striding{*byte_strides};\n+    options.input_striding = TransposePlan::Striding{*byte_strides};\n \n     absl::InlinedVector<int64_t, 4> permutation(options.dims.size());\n     absl::c_reverse_copy(dst_shape_.layout().minor_to_major(),"
        },
        {
            "sha": "a0bdf45320c23efe399d400411d7e2558cb37cbd",
            "filename": "third_party/xla/xla/pjrt/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -951,6 +951,7 @@ cc_library(\n         \"//xla:ef57\",\n         \"//xla:permutation_util\",\n         \"//xla:util\",\n+        \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -975,24 +976,26 @@ xla_cc_test(\n         \":transpose\",\n         \"//xla:array\",\n         \"//xla:permutation_util\",\n-        \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/hlo/testlib:test\",\n         \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test_benchmark\",\n+        \"//xla/tsl/platform:test_main\",\n         \"//xla/tsl/protobuf:error_codes_proto_impl_cc\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/numeric:int128\",\n+        \"@com_google_absl//absl/random\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n         \"@eigen_archive//:eigen3\",\n-        \"@local_tsl//tsl/platform:env\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-        \"@local_tsl//tsl/platform:test_benchmark\",\n-        \"@local_tsl//tsl/platform:test_main\",\n     ],\n )\n "
        },
        {
            "sha": "04a00dc9b7d776457975a6b3235a7620609dc7f0",
            "filename": "third_party/xla/xla/pjrt/cpu/raw_buffer.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fraw_buffer.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -232,7 +232,7 @@ CpuRawBuffer::CopyFromHostBuffer(\n       options.dims = dims;\n       options.permutation = permutation;\n       if (byte_strides) {\n-        options.input_layout = TransposePlan::Striding{*byte_strides};\n+        options.input_striding = TransposePlan::Striding{*byte_strides};\n       }\n       if (thread_pool) {\n         options.num_threads ="
        },
        {
            "sha": "40e62765ffd351208477627a4df81785eead5ee4",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -384,7 +384,7 @@ Future<> TfrtGpuBuffer::ToLiteralHelper(Future<MutableLiteralBase*> literal) {\n                 primitive_util::ByteWidth(on_device_shape.element_type());\n             options.dims = on_device_shape.dimensions();\n             options.permutation = permutation;\n-            options.input_layout = TransposePlan::Striding{byte_strides};\n+            options.input_striding = TransposePlan::Striding{byte_strides};\n             {\n               absl::MutexLock lock(client->transpose_mu_);\n               absl::StatusOr<std::shared_ptr<TransposePlan>> t ="
        },
        {
            "sha": "14c44cf1c53925dc80685827e322bad8fe062f55",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -887,7 +887,7 @@ absl::StatusOr<std::unique_ptr<PjRtBuffer>> TfrtGpuClient::BufferFromHostBuffer(\n     options.elem_size_in_bytes = primitive_util::ByteWidth(type);\n     options.dims = dims;\n     options.permutation = permutation;\n-    options.input_layout = TransposePlan::Striding{*byte_strides};\n+    options.input_striding = TransposePlan::Striding{*byte_strides};\n     absl::MutexLock lock(transpose_mu_);\n     TF_ASSIGN_OR_RETURN(transpose, transpose_cache_.GetOrCreate(options));\n   }"
        },
        {
            "sha": "b47aaf7f446ea003d8f5f959adabd0b18255a6d9",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -644,7 +644,7 @@ PjRtStreamExecutorClient::LinearizeHostBufferInto(\n     options.elem_size_in_bytes = primitive_util::ByteWidth(type);\n     options.dims = dims;\n     options.permutation = permutation;\n-    options.input_layout = TransposePlan::Striding{*byte_strides};\n+    options.input_striding = TransposePlan::Striding{*byte_strides};\n     absl::MutexLock lock(transpose_mu_);\n     TF_ASSIGN_OR_RETURN(transpose, transpose_cache_.GetOrCreate(options));\n   }"
        },
        {
            "sha": "641e892394bfa8f2f01b56f09ca8d31c77a6119b",
            "filename": "third_party/xla/xla/pjrt/se_raw_buffer.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -308,7 +308,7 @@ void PjRtStreamExecutorRawBuffer::CopyToLiteralAsync(\n                 primitive_util::ByteWidth(on_device_shape.element_type());\n             options.dims = on_device_shape.dimensions();\n             options.permutation = permutation;\n-            options.input_layout = TransposePlan::Striding{byte_strides};\n+            options.input_striding = TransposePlan::Striding{byte_strides};\n             {\n               absl::MutexLock lock(client->transpose_mu_);\n               absl::StatusOr<std::shared_ptr<TransposePlan>> t ="
        },
        {
            "sha": "ab20440d7105d8f0ed627fedff77d5fb9f4a7d15",
            "filename": "third_party/xla/xla/pjrt/transpose.cc",
            "status": "modified",
            "additions": 97,
            "deletions": 74,
            "changes": 171,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -98,6 +98,7 @@ limitations under the License.\n #include \"xla/ef57.h\"\n #include \"xla/permutation_util.h\"\n #include \"xla/pjrt/transpose_kernels.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -682,9 +683,13 @@ int64_t TransposePlan::OutputNumElems() const {\n \n // Parses and validates a tiling specification, and populates `tiling`.\n static absl::Status ParseTilingSpecification(\n-    int ndim, absl::Span<int64_t const> tiling_spec,\n+    int ndim, const std::optional<TransposePlan::Tiling>& tiling_opt,\n     absl::InlinedVector<int64_t, 4>& tiling) {\n   tiling.resize(ndim, 1);\n+  if (!tiling_opt) {\n+    return absl::OkStatus();\n+  }\n+  absl::Span<int64_t const> tiling_spec = tiling_opt->tiling;\n   if (tiling_spec.size() > ndim) {\n     return InvalidArgument(\n         \"Tiling (%s) must have at most as many dimensions as the array (%d)\",\n@@ -909,7 +914,15 @@ void TransposePlan::BuildPlanNodes(\n }\n \n absl::StatusOr<std::unique_ptr<TransposePlan>> TransposePlan::Create(\n-    const Options& o) {\n+    Options o) {\n+  if (o.input_layout.has_value()) {\n+    if (const auto* t = std::get_if<Tiling>(&*o.input_layout)) {\n+      o.input_tiling = *t;\n+    } else if (const auto* s = std::get_if<Striding>(&*o.input_layout)) {\n+      o.input_striding = *s;\n+    }\n+  }\n+\n   auto is_negative = [](int64_t d) { return d < 0; };\n   if (absl::c_find_if(o.dims, is_negative) != o.dims.end()) {\n     return InvalidArgument(\"dims must be non-negative, got %s\",\n@@ -952,68 +965,82 @@ absl::StatusOr<std::unique_ptr<TransposePlan>> TransposePlan::Create(\n   plan->original_b_dims_ = Permute(o.dims, o.permutation);\n \n   TF_RETURN_IF_ERROR(\n-      ParseTilingSpecification(ndim, o.output_tiling.tiling, plan->b_tiling_));\n+      ParseTilingSpecification(ndim, o.output_tiling, plan->b_tiling_));\n \n-  // Handles strides.\n-  if (std::holds_alternative<Striding>(o.input_layout)) {\n+  // Temporary vectors to hold un-permuted attributes\n+  absl::InlinedVector<int64_t, 4> temp_lda, temp_lda_tile, temp_a_tiling;\n+\n+  // Parse the tile and stride specifications.\n+  TF_RETURN_IF_ERROR(\n+      ParseTilingSpecification(ndim, o.input_tiling, temp_a_tiling));\n+  ComputeStrides(plan->elem_size_in_bytes_, o.dims, temp_a_tiling, temp_lda,\n+                 temp_lda_tile);\n+\n+  // Determine tile (outer) strides\n+  absl::InlinedVector<int64_t, 4> input_outer_strides;\n+  if (o.input_striding) {\n     absl::Span<int64_t const> input_strides_in_bytes =\n-        std::get<Striding>(o.input_layout).strides_in_bytes;\n+        o.input_striding->strides_in_bytes;\n     if (input_strides_in_bytes.size() != o.dims.size()) {\n       return InvalidArgument(\n-          \"dims and input_strides_in_bytes must have equal sizes, got %d \"\n-          \"and %d\",\n+          \"dims and input_striding must have equal sizes, \"\n+          \"got %d and %d\",\n           o.dims.size(), input_strides_in_bytes.size());\n     }\n+    input_outer_strides.assign(input_strides_in_bytes.begin(),\n+                               input_strides_in_bytes.end());\n+    // Also save original strides if explicit\n     plan->original_a_strides_.resize(ndim);\n     absl::c_copy(input_strides_in_bytes, plan->original_a_strides_.begin());\n-    // Sort the dimensions from slowest-varying (largest strides) to\n-    // fastest-varying (smallest strides).\n-    std::vector<int64_t> dim_order(ndim);\n-    absl::c_iota(dim_order, 0);\n-\n-    auto cost = [&](int k) {\n-      int64_t stride = input_strides_in_bytes.at(k);\n-      // If there is a dimension with size equal to the element size, sort it\n-      // last. This ensures that we place any stride-1 dimension last.\n-      bool is_stride1 = stride == o.elem_size_in_bytes;\n-      // If there are multiple stride-1 dimensions, we'd prefer the one that\n-      // matches the stride-1 dimension of the output.\n-      // Failing that, we'd just prefer the largest stride-1 dimension last.\n-      bool is_trailing_dim_in_b = o.permutation.back() == k;\n-\n-      // If we are applying ef57 conversion, we want a size-2 stride-1\n-      // dimension last.\n-      bool ef57_even =\n-          (is_stride1 && o.transformation == Transformation::kF64ToEf57 &&\n-           o.dims[k] == 2);\n-\n-      return std::make_tuple(is_stride1, -std::abs(stride), ef57_even,\n-                             is_trailing_dim_in_b, o.dims[k]);\n-    };\n-    absl::c_stable_sort(dim_order,\n-                        [&cost](int i, int j) { return cost(i) < cost(j); });\n-    // dim_order maps new input dim -> old input dim, we need its inverse to\n-    // compute the new permutation.\n-    auto inv_dim_order = InversePermutation(dim_order);\n-    plan->lda_.reserve(ndim);\n-    plan->a_dims_.reserve(ndim);\n-    plan->permutation_.reserve(ndim);\n-    for (int i = 0; i < ndim; ++i) {\n-      plan->lda_.push_back(input_strides_in_bytes.at(dim_order[i]));\n-      plan->a_dims_.push_back(o.dims[dim_order[i]]);\n-      plan->permutation_.push_back(inv_dim_order[o.permutation[i]]);\n-    }\n-    plan->lda_tile_.resize(ndim, 1);\n-    plan->a_tiling_.resize(ndim, 1);\n   } else {\n-    TF_RETURN_IF_ERROR(ParseTilingSpecification(\n-        ndim, std::get<Tiling>(o.input_layout).tiling, plan->a_tiling_));\n-\n-    plan->a_dims_ = plan->original_a_dims_;\n-    plan->permutation_.resize(ndim);\n-    absl::c_copy(o.permutation, plan->permutation_.begin());\n-    ComputeStrides(plan->elem_size_in_bytes_, plan->a_dims_, plan->a_tiling_,\n-                   plan->lda_, plan->lda_tile_);\n+    input_outer_strides = temp_lda;\n+  }\n+\n+  // Sort the dimensions from slowest-varying (largest strides) to\n+  // fastest-varying (smallest strides).\n+  // Maps new input dim -> old input dim\n+  std::vector<int64_t> dim_order(ndim);\n+  absl::c_iota(dim_order, 0);\n+\n+  auto cost = [&](int k) {\n+    int64_t stride = input_outer_strides.at(k);\n+    // If there is a dimension with size equal to the element size, sort it\n+    // last. This ensures that we place any stride-1 dimension last.\n+    bool is_stride1 = stride == o.elem_size_in_bytes;\n+    // If there are multiple stride-1 dimensions, we'd prefer the one that\n+    // matches the stride-1 dimension of the output.\n+    // Failing that, we'd just prefer the largest stride-1 dimension last.\n+    bool is_trailing_dim_in_b = o.permutation.back() == k;\n+\n+    // If we are applying ef57 conversion, we want a size-2 stride-1\n+    // dimension last.\n+    bool ef57_even =\n+        (is_stride1 && o.transformation == Transformation::kF64ToEf57 &&\n+         o.dims[k] == 2);\n+\n+    return std::make_tuple(is_stride1, -std::abs(stride), ef57_even,\n+                           is_trailing_dim_in_b, o.dims[k]);\n+  };\n+  absl::c_stable_sort(dim_order,\n+                      [&cost](int i, int j) { return cost(i) < cost(j); });\n+\n+  // Apply permutation to all plan attributes\n+  // dim_order maps new input dim -> old input dim, we need its inverse to\n+  // compute the new permutation.\n+  auto inv_dim_order = InversePermutation(dim_order);\n+  plan->lda_.reserve(ndim);\n+  plan->lda_tile_.reserve(ndim);\n+  plan->a_dims_.reserve(ndim);\n+  plan->permutation_.reserve(ndim);\n+  plan->a_tiling_.reserve(ndim);\n+\n+  for (int i = 0; i < ndim; ++i) {\n+    int old_idx = dim_order[i];\n+    plan->lda_.push_back(input_outer_strides.at(old_idx));\n+    plan->lda_tile_.push_back(temp_lda_tile.at(old_idx));\n+    plan->a_dims_.push_back(o.dims[old_idx]);\n+    plan->permutation_.push_back(inv_dim_order[o.permutation[i]]);\n+    plan->a_tiling_.push_back(temp_a_tiling[old_idx]);\n   }\n \n   auto is_not_one = [](int64_t x) { return x != 1; };\n@@ -1354,19 +1381,18 @@ bool TransposePlanCacheKey::operator==(\n     const TransposePlanCacheKey& other) const {\n   return elem_size_in_bytes == other.elem_size_in_bytes && dims == other.dims &&\n          permutation == other.permutation &&\n-         input_layout_is_tiling == other.input_layout_is_tiling &&\n-         input_layout == other.input_layout &&\n+         input_tiling == other.input_tiling &&\n+         input_striding == other.input_striding &&\n          output_tiling == other.output_tiling &&\n          transformation == other.transformation &&\n          num_threads == other.num_threads;\n }\n \n template <typename H>\n H AbslHashValue(H h, const TransposePlanCacheKey& key) {\n-  return H::combine(std::move(h), key.elem_size_in_bytes,\n-                    key.input_layout_is_tiling, key.num_threads,\n+  return H::combine(std::move(h), key.elem_size_in_bytes, key.num_threads,\n                     key.transformation, key.dims, key.permutation,\n-                    key.input_layout, key.output_tiling);\n+                    key.input_tiling, key.input_striding, key.output_tiling);\n }\n \n TransposePlanCache::TransposePlanCache(int capacity)\n@@ -1382,21 +1408,18 @@ absl::StatusOr<std::shared_ptr<TransposePlan>> TransposePlanCache::GetOrCreate(\n   absl::c_copy(o.dims, key.dims.begin());\n   key.permutation.resize(o.permutation.size());\n   absl::c_copy(o.permutation, key.permutation.begin());\n-  if (std::holds_alternative<TransposePlan::Striding>(o.input_layout)) {\n-    absl::Span<int64_t const> input_strides_in_bytes =\n-        std::get<TransposePlan::Striding>(o.input_layout).strides_in_bytes;\n-    key.input_layout = absl::InlinedVector<int64_t, 4>(\n-        input_strides_in_bytes.begin(), input_strides_in_bytes.end());\n-    key.input_layout_is_tiling = false;\n-  } else {\n-    absl::Span<int64_t const> input_tiling =\n-        std::get<TransposePlan::Tiling>(o.input_layout).tiling;\n-    key.input_layout = absl::InlinedVector<int64_t, 4>(input_tiling.begin(),\n-                                                       input_tiling.end());\n-    key.input_layout_is_tiling = true;\n+  if (o.input_tiling) {\n+    key.input_tiling.emplace(o.input_tiling->tiling.begin(),\n+                             o.input_tiling->tiling.end());\n+  }\n+  if (o.input_striding) {\n+    key.input_striding.emplace(o.input_striding->strides_in_bytes.begin(),\n+                               o.input_striding->strides_in_bytes.end());\n+  }\n+  if (o.output_tiling) {\n+    key.output_tiling.emplace(o.output_tiling->tiling.begin(),\n+                              o.output_tiling->tiling.end());\n   }\n-  key.output_tiling.resize(o.output_tiling.tiling.size());\n-  absl::c_copy(o.output_tiling.tiling, key.output_tiling.begin());\n   key.transformation = o.transformation;\n   key.num_threads = o.num_threads;\n   return cache_.GetOrCreateIfAbsent("
        },
        {
            "sha": "975e2bccc22c0c6bcb96692bf48b4a44d6b53956",
            "filename": "third_party/xla/xla/pjrt/transpose.h",
            "status": "modified",
            "additions": 22,
            "deletions": 13,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose.h?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -50,7 +50,9 @@ class TransposePlan {\n   // dims: the input shape, in elements.\n   // permutation: for each output dimension, gives the number of the\n   //   corresponding input dimension. Must be a permutation of [0..dims.size())\n-  // input_layout: either byte strides or an input tiling.\n+  // input_tiling: optional input tiling.\n+  // input_striding: optional input byte strides.\n+  // output_tiling: optional output tiling.\n   //\n   // A Striding represents the strides of the input array in bytes. (N.B. not\n   // elements).\n@@ -71,7 +73,9 @@ class TransposePlan {\n   // tiled dimensions. This is acceptable because in the intended use case for\n   // this code we expect at most 2 tiled dimensions on input and output.\n   //\n-  // The input may have either a striding or a tiling but not both.\n+  // The input may have both a tiling and a striding. If both are present,\n+  // the striding determines the strides between tiles (in bytes).\n+  //\n   //\n   // num_threads: is the number of threads requested. The actual number of\n   //   threads used may be smaller if there isn't enough work per thread.\n@@ -94,14 +98,19 @@ class TransposePlan {\n     size_t elem_size_in_bytes;\n     absl::Span<int64_t const> dims;\n     absl::Span<int64_t const> permutation;\n-    std::variant<Tiling, Striding> input_layout = Tiling{};\n-    Tiling output_tiling;\n+    std::optional<Tiling> input_tiling = std::nullopt;\n+    std::optional<Striding> input_striding = std::nullopt;\n+    std::optional<Tiling> output_tiling = std::nullopt;\n     Transformation transformation = Transformation::kNone;\n     int num_threads = 1;\n+\n+    // DEPRECATED: Use input_tiling or input_striding instead.\n+    // This field is only present for backward compatibility.\n+    // TODO(phawkins): remove me.\n+    std::optional<std::variant<Tiling, Striding>> input_layout = std::nullopt;\n   };\n \n-  static absl::StatusOr<std::unique_ptr<TransposePlan>> Create(\n-      const Options& options);\n+  static absl::StatusOr<std::unique_ptr<TransposePlan>> Create(Options options);\n \n   TransposePlan();\n   ~TransposePlan();\n@@ -199,10 +208,10 @@ class TransposePlan {\n   absl::InlinedVector<int64_t, 4> permutation_;\n \n   // Leading-dimension sizes (byte strides) of each dimension.\n-  absl::InlinedVector<int64_t, 4> lda_;\n-  absl::InlinedVector<int64_t, 4> lda_tile_;\n-  absl::InlinedVector<int64_t, 4> ldb_;\n-  absl::InlinedVector<int64_t, 4> ldb_tile_;\n+  absl::InlinedVector<int64_t, 4> lda_;       // Strides for tiles\n+  absl::InlinedVector<int64_t, 4> lda_tile_;  // Strides for tile interiors\n+  absl::InlinedVector<int64_t, 4> ldb_;       // Strides for tiles\n+  absl::InlinedVector<int64_t, 4> ldb_tile_;  // Strides for tile interiors\n \n   // Tile sizes in each dimension. Has size equal to the number of dimensions.\n   // A 1 entry means that dimension is not tiled.\n@@ -257,9 +266,9 @@ struct TransposePlanCacheKey {\n   size_t elem_size_in_bytes;\n   absl::InlinedVector<int64_t, 4> dims;\n   absl::InlinedVector<int64_t, 4> permutation;\n-  bool input_layout_is_tiling;\n-  absl::InlinedVector<int64_t, 4> input_layout;\n-  absl::InlinedVector<int64_t, 4> output_tiling;\n+  std::optional<absl::InlinedVector<int64_t, 4>> input_tiling;\n+  std::optional<absl::InlinedVector<int64_t, 4>> input_striding;\n+  std::optional<absl::InlinedVector<int64_t, 4>> output_tiling;\n   TransposePlan::Transformation transformation;\n   int num_threads;\n "
        },
        {
            "sha": "4ba51f5bcff8f5ba6fb78f956297901941208339",
            "filename": "third_party/xla/xla/pjrt/transpose_test.cc",
            "status": "modified",
            "additions": 161,
            "deletions": 68,
            "changes": 229,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftranspose_test.cc?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -23,14 +23,17 @@ limitations under the License.\n #include <ostream>\n #include <string>\n #include <tuple>\n+#include <type_traits>\n #include <utility>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n+#include \"absl/base/casts.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/numeric/int128.h\"\n+#include \"absl/random/random.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n@@ -39,13 +42,13 @@ limitations under the License.\n #include \"xla/array.h\"\n #include \"xla/hlo/testlib/test.h\"\n #include \"xla/permutation_util.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test_benchmark.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/tsl/protobuf/error_codes.pb.h\"\n #include \"xla/util.h\"\n-#include \"tsl/platform/statusor.h\"\n-#include \"tsl/platform/test_benchmark.h\"\n-#include \"tsl/platform/threadpool.h\"\n \n namespace xla {\n \n@@ -138,7 +141,7 @@ TEST(TransposeTest, InvalidTilings) {\n   options.permutation = perm;\n   std::vector<int64_t> input_tiling = {8, 128};\n   std::vector<int64_t> output_tiling = {4};\n-  options.input_layout = TransposePlan::Tiling{input_tiling};\n+  options.input_tiling = TransposePlan::Tiling{input_tiling};\n   options.output_tiling = TransposePlan::Tiling{output_tiling};\n   auto plan = TransposePlan::Create(options);\n   EXPECT_EQ(plan.status().code(), tsl::error::UNIMPLEMENTED);\n@@ -156,8 +159,6 @@ TEST(TransposeTest, LargeDimensions) {\n   options.elem_size_in_bytes = 8;\n   options.dims = dims;\n   options.permutation = permutation;\n-  options.input_layout = TransposePlan::Tiling{};\n-  options.output_tiling = TransposePlan::Tiling{};\n   options.transformation = TransposePlan::Transformation::kNone;\n   TF_EXPECT_OK(TransposePlan::Create(options).status());\n }\n@@ -192,52 +193,60 @@ bool BumpIndices(absl::Span<int64_t const> shape, absl::Span<int64_t> indices) {\n   return false;\n }\n \n+// Helper to pad tiling to match shape rank. (Suffix alignment).\n+std::vector<int64_t> PadTiling(absl::Span<int64_t const> shape,\n+                               absl::Span<int64_t const> tiling) {\n+  CHECK_LE(tiling.size(), shape.size());\n+  std::vector<int64_t> full_tiling(shape.size(), 1);\n+  absl::c_copy(tiling, full_tiling.end() - tiling.size());\n+  return full_tiling;\n+}\n+\n+std::vector<int64_t> ComputeDefaultStrides(\n+    absl::Span<int64_t const> shape, absl::Span<int64_t const> full_tiling,\n+    int elem_size_bytes) {\n+  CHECK_EQ(full_tiling.size(), shape.size());\n+  std::vector<int64_t> strides(shape.size());\n+  int64_t stride = elem_size_bytes;\n+  for (int64_t t : full_tiling) {\n+    stride *= t;\n+  }\n+\n+  for (int i = shape.size() - 1; i >= 0; --i) {\n+    strides[i] = stride;\n+    stride *= CeilOfRatio(shape[i], full_tiling[i]);\n+  }\n+  return strides;\n+}\n+\n // Converts a multidimensional index `indices` into an array with `shape` and\n // tiling `tiling` into a linear offset into a buffer.\n+// `striding` is the stride between tiles in bytes.\n int64_t IndexToLinearIndex(absl::Span<int64_t const> shape,\n-                           absl::Span<int64_t const> tiling,\n-                           absl::Span<int64_t const> indices) {\n-  CHECK_LE(tiling.size(), shape.size());\n+                           absl::Span<int64_t const> full_tiling,\n+                           absl::Span<int64_t const> indices,\n+                           absl::Span<int64_t const> striding,\n+                           int elem_size_bytes) {\n+  CHECK_EQ(full_tiling.size(), shape.size());\n   CHECK_EQ(shape.size(), indices.size());\n+  CHECK_EQ(shape.size(), striding.size());\n+\n   int64_t stride = 1;\n   int64_t offset = 0;\n \n-  auto index_it = indices.rbegin();\n-  auto tile_it = tiling.rbegin();\n-  for (; tile_it != tiling.rend(); ++index_it, ++tile_it) {\n-    offset += (*index_it % *tile_it) * stride;\n-    stride *= *tile_it;\n-  }\n-  index_it = indices.rbegin();\n-  tile_it = tiling.rbegin();\n-  auto shape_it = shape.rbegin();\n-  for (; tile_it != tiling.rend(); ++index_it, ++shape_it, ++tile_it) {\n-    offset += (*index_it / *tile_it) * stride;\n-    stride *= CeilOfRatio(*shape_it, *tile_it);\n+  // Strides within a tiling are always the default strides.\n+  for (int i = shape.size() - 1; i >= 0; --i) {\n+    offset += (indices[i] % full_tiling[i]) * stride;\n+    stride *= full_tiling[i];\n   }\n-  for (; shape_it != shape.rend(); ++index_it, ++shape_it) {\n-    offset += *index_it * stride;\n-    stride *= *shape_it;\n+  // Strides outside a tiling are the input strides.\n+  for (size_t i = 0; i < shape.size(); ++i) {\n+    int64_t outer_idx = indices[i] / full_tiling[i];\n+    offset += outer_idx * (striding[i] / elem_size_bytes);\n   }\n   return offset;\n }\n \n-// Slow reference code that converts an array from an untiled layout into a\n-// tiled layout.\n-template <typename T>\n-std::vector<T> TileArray(const Array<T>& in, absl::Span<int64_t const> tiling) {\n-  std::vector<T> out(SizeOfTiledArray(in.dimensions(), tiling), -1);\n-  if (in.num_elements() == 0) {\n-    return out;\n-  }\n-  std::vector<int64_t> indices(in.num_dimensions(), 0);\n-  do {\n-    int64_t i = IndexToLinearIndex(in.dimensions(), tiling, indices);\n-    out.at(i) = in(indices);\n-  } while (BumpIndices(in.dimensions(), absl::MakeSpan(indices)));\n-  return out;\n-}\n-\n // Reference implementation: transpose using Eigen.\n template <typename T, int NDIMS>\n void TransposeUsingEigenNd(const T* input, T* output,\n@@ -291,25 +300,70 @@ void TransposeUsingEigen(const T* input, T* output,\n   }\n }\n \n+template <typename T>\n+void FillRandom(absl::Span<T> input) {\n+  absl::BitGen gen;\n+  for (auto& val : input) {\n+    if constexpr (std::is_same_v<T, absl::int128>) {\n+      val = absl::MakeInt128(absl::Uniform<uint64_t>(gen),\n+                             absl::Uniform<uint64_t>(gen));\n+    } else {\n+      using U = std::make_unsigned_t<T>;\n+      val = absl::bit_cast<T>(absl::Uniform<U>(gen));\n+    }\n+  }\n+}\n+\n+// Reference implementation of transpose that handles tiling and striding.\n+template <typename T>\n+void ReferenceTranspose(absl::Span<int64_t const> dims,\n+                        absl::Span<int64_t const> permutation,\n+                        absl::Span<int64_t const> input_tiling,\n+                        absl::Span<int64_t const> input_striding,\n+                        absl::Span<int64_t const> output_tiling,\n+                        absl::Span<int64_t const> output_striding,\n+                        absl::Span<const T> input, absl::Span<T> output) {\n+  std::vector<int64_t> output_dims = Permute(dims, permutation);\n+  std::vector<int64_t> indices(dims.size(), 0);\n+  std::vector<int64_t> output_indices(dims.size());\n+\n+  do {\n+    int64_t input_linear_idx = IndexToLinearIndex(dims, input_tiling, indices,\n+                                                  input_striding, sizeof(T));\n+    T val = input[input_linear_idx];\n+\n+    for (size_t i = 0; i < dims.size(); ++i) {\n+      output_indices[i] = indices[permutation[i]];\n+    }\n+    int64_t output_linear_idx = IndexToLinearIndex(\n+        output_dims, output_tiling, output_indices, output_striding, sizeof(T));\n+    output[output_linear_idx] = val;\n+  } while (BumpIndices(dims, absl::MakeSpan(indices)));\n+}\n+\n struct TransposeTestCase {\n   TransposeTestCase(std::vector<int64_t> dims, std::vector<int64_t> permutation,\n                     std::vector<int64_t> input_tiling = {},\n-                    std::vector<int64_t> output_tiling = {})\n+                    std::vector<int64_t> output_tiling = {},\n+                    std::vector<int64_t> input_striding = {})\n       : dims(std::move(dims)),\n         permutation(std::move(permutation)),\n         input_tiling(std::move(input_tiling)),\n+        input_striding(std::move(input_striding)),\n         output_tiling(std::move(output_tiling)) {}\n \n   std::vector<int64_t> dims;\n   std::vector<int64_t> permutation;\n   std::vector<int64_t> input_tiling;\n+  std::vector<int64_t> input_striding;\n   std::vector<int64_t> output_tiling;\n \n   std::string ToString() const {\n     return absl::StrFormat(\n-        \"[%s],perm=[%s],tiling=[%s]/[%s]\", absl::StrJoin(dims, \",\"),\n-        absl::StrJoin(permutation, \",\"), absl::StrJoin(input_tiling, \",\"),\n-        absl::StrJoin(output_tiling, \",\"));\n+        \"[%s],perm=[%s],tiling=[%s]/[%s],striding=[%s]\",\n+        absl::StrJoin(dims, \",\"), absl::StrJoin(permutation, \",\"),\n+        absl::StrJoin(input_tiling, \",\"), absl::StrJoin(output_tiling, \",\"),\n+        input_striding.empty() ? \"none\" : absl::StrJoin(input_striding, \",\"));\n   }\n };\n \n@@ -320,6 +374,7 @@ std::ostream& operator<<(std::ostream& os, const TransposeTestCase& test) {\n \n std::vector<TransposeTestCase> GetTransposeTestCases() {\n   std::vector<TransposeTestCase> cases = {\n+      TransposeTestCase(/*dims=*/{}, /*permutation=*/{}),\n       TransposeTestCase(/*dims=*/{1}, /*permutation=*/{0}),\n       TransposeTestCase(/*dims=*/{4}, /*permutation=*/{0}),\n       TransposeTestCase(/*dims=*/{27}, /*permutation=*/{0}),\n@@ -376,6 +431,12 @@ std::vector<TransposeTestCase> GetTransposeTestCases() {\n                         /*input_tiling=*/{2, 4}),\n       TransposeTestCase(/*dims=*/{12, 7}, /*permutation=*/{1, 0},\n                         /*input_tiling=*/{}, /*output_tiling=*/{5, 2}),\n+      TransposeTestCase(/*dims=*/{4, 6}, /*permutation=*/{1, 0},\n+                        /*input_tiling=*/{2, 3},\n+                        /*output_tiling=*/{}, /*input_striding=*/{512, 128}),\n+      TransposeTestCase(/*dims=*/{13, 9}, /*permutation=*/{1, 0},\n+                        /*input_tiling=*/{2, 3},\n+                        /*output_tiling=*/{}, /*input_striding=*/{0, 0}),\n       TransposeTestCase(/*dims=*/{128, 224, 224, 3},\n                         /*permutation=*/{3, 1, 2, 0},\n                         /*input_tiling=*/{},\n@@ -396,29 +457,63 @@ class TransposeTest : public ::testing::TestWithParam<TransposeTestCase> {\n     options.elem_size_in_bytes = sizeof(T);\n     options.dims = test.dims;\n     options.permutation = test.permutation;\n-    options.input_layout = TransposePlan::Tiling{test.input_tiling};\n-    options.output_tiling = TransposePlan::Tiling{test.output_tiling};\n+    if (!test.input_striding.empty()) {\n+      options.input_striding = TransposePlan::Striding{test.input_striding};\n+    }\n+    if (!test.input_tiling.empty()) {\n+      options.input_tiling = TransposePlan::Tiling{test.input_tiling};\n+    }\n+    if (!test.output_tiling.empty()) {\n+      options.output_tiling = TransposePlan::Tiling{test.output_tiling};\n+    }\n     options.transformation = TransposePlan::Transformation::kNone;\n     options.num_threads = parallelism;\n     TF_ASSERT_OK_AND_ASSIGN(auto plan, TransposePlan::Create(options));\n     VLOG(1) << plan->ToString();\n-    xla::Array<T> untiled_input(test.dims);\n-    untiled_input.FillIota(0);\n-    xla::Array<T> expected_untiled_output(output_dims);\n-    TransposeUsingEigen(untiled_input.data(), expected_untiled_output.data(),\n-                        test.dims, output_dims, test.permutation);\n-\n-    auto tiled_input = TileArray(untiled_input, test.input_tiling);\n-    auto expected_tiled_output =\n-        TileArray(expected_untiled_output, test.output_tiling);\n-\n-    std::vector<T> output(\n-        SizeOfTiledArray(plan->OutputDims(), test.output_tiling), -1);\n-    plan->Execute(\n-        tiled_input.data(), output.data(),\n-        [&](std::function<void()> fn) { threadpool.Schedule(std::move(fn)); });\n-\n-    EXPECT_EQ(expected_tiled_output, output);\n+\n+    // Allocate sufficiently large buffers.\n+    // We can use SizeOfTiledArray for output which is always tiled/dense.\n+    int64_t output_size =\n+        SizeOfTiledArray(plan->OutputDims(), test.output_tiling);\n+    std::vector<T> output(output_size, -1);\n+\n+    std::vector<int64_t> input_striding = test.input_striding;\n+    std::vector<int64_t> input_tiling = PadTiling(test.dims, test.input_tiling);\n+    if (input_striding.empty()) {\n+      input_striding =\n+          ComputeDefaultStrides(test.dims, input_tiling, sizeof(T));\n+    }\n+    int64_t input_tile_size = absl::c_accumulate(input_tiling, int64_t{1},\n+                                                 std::multiplies<int64_t>());\n+\n+    std::vector<int64_t> output_tiling =\n+        PadTiling(output_dims, test.output_tiling);\n+    std::vector<int64_t> output_striding =\n+        ComputeDefaultStrides(output_dims, output_tiling, sizeof(T));\n+\n+    int64_t input_size = 1;\n+    if (!test.dims.empty()) {\n+      std::vector<int64_t> max_indices = test.dims;\n+      for (int i = 0; i < test.dims.size(); ++i) {\n+        max_indices[i] = RoundDownTo(max_indices[i], input_tiling[i]);\n+      }\n+      input_size = IndexToLinearIndex(test.dims, input_tiling, max_indices,\n+                                      input_striding, sizeof(T)) +\n+                   input_tile_size;\n+    }\n+    std::vector<T> input(input_size);\n+    FillRandom<T>(absl::MakeSpan(input));\n+    std::vector<T> expected_output(output_size, -1);\n+\n+    ReferenceTranspose<T>(test.dims, test.permutation, input_tiling,\n+                          input_striding, output_tiling, output_striding, input,\n+                          absl::MakeSpan(expected_output));\n+\n+    plan->Execute(input.data(), output.data(), [&](std::function<void()> fn) {\n+      threadpool.Schedule(std::move(fn));\n+    });\n+\n+    EXPECT_EQ(output, expected_output);\n   }\n };\n \n@@ -448,7 +543,7 @@ TEST(TransposeTest, NegativeStrides1D) {\n   options.dims = dims;\n   options.permutation = permutation;\n   std::vector<int64_t> strides = {-int64_t{sizeof(int32_t)}};\n-  options.input_layout = TransposePlan::Striding{strides};\n+  options.input_striding = TransposePlan::Striding{strides};\n   TF_ASSERT_OK_AND_ASSIGN(auto plan, TransposePlan::Create(options));\n   plan->Execute(input.data() + (n - 1), output.data());\n   EXPECT_EQ(expected, output);\n@@ -475,7 +570,7 @@ TEST(TransposeTest, NegativeStrides2D) {\n   options.permutation = permutation;\n   std::vector<int64_t> strides = {4 * sizeof(int16_t),\n                                   -int64_t{sizeof(int16_t)}};\n-  options.input_layout = TransposePlan::Striding{strides};\n+  options.input_striding = TransposePlan::Striding{strides};\n   TF_ASSERT_OK_AND_ASSIGN(auto plan, TransposePlan::Create(options));\n   plan->Execute(input.data() + 3, output.data());\n   EXPECT_EQ(expected, output);\n@@ -546,8 +641,6 @@ void BM_Transpose(const TransposeTestCase& bm, int parallelism,\n   options.elem_size_in_bytes = sizeof(T);\n   options.dims = bm.dims;\n   options.permutation = bm.permutation;\n-  options.input_layout = TransposePlan::Tiling{};\n-  options.output_tiling = TransposePlan::Tiling{};\n   options.transformation = TransposePlan::Transformation::kNone;\n   options.num_threads = parallelism;\n   TF_ASSERT_OK_AND_ASSIGN(auto plan, TransposePlan::Create(options));"
        },
        {
            "sha": "6ee37c651884676f9d64aa5dc8d795da9eba0832",
            "filename": "third_party/xla/xla/python/version.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h?ref=da0b8c8771ceae1a2a3a0c0f3dc5e72153a2e0fe",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n \n // An increasing version number to protect jax code against breaking changes.\n // In JAX, reference this via jax._src.lib.ifrt_version.\n-#define JAX_IFRT_VERSION_NUMBER \\\n-  42  // PjRtExecutable is created using IFRT Compiler::Compile() API.\n+#define JAX_IFRT_VERSION_NUMBER 43  // Transpose API update\n \n #endif  // XLA_PYTHON_VERSION_H_"
        }
    ],
    "stats": {
        "total": 463,
        "additions": 295,
        "deletions": 168
    }
}