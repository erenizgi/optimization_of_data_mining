{
    "author": "hawkinsp",
    "message": "Remove HloModuleGroupUtil, which is unused.\n\nPiperOrigin-RevId: 814344524",
    "sha": "4080be5ba817cef353dec47a6c3b99f8ef300a45",
    "files": [
        {
            "sha": "8be9bccba9757ed5a988a183401cba6f88944044",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4080be5ba817cef353dec47a6c3b99f8ef300a45/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4080be5ba817cef353dec47a6c3b99f8ef300a45/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=4080be5ba817cef353dec47a6c3b99f8ef300a45",
            "patch": "@@ -1897,30 +1897,6 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"hlo_module_group_util\",\n-    srcs = [\"hlo_module_group_util.cc\"],\n-    hdrs = [\"hlo_module_group_util.h\"],\n-    deps = [\n-        \":hlo_module_group_metadata\",\n-        \"//xla:status_macros\",\n-        \"//xla:types\",\n-        \"//xla:util\",\n-        \"//xla/hlo/analysis:hlo_reachability\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/functional:function_ref\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:status\",\n-    ],\n-)\n-\n xla_cc_test(\n     name = \"hlo_schedule_test\",\n     srcs = [\"hlo_schedule_test.cc\"],"
        },
        {
            "sha": "de4b61736e74745ad93fcc707f8b73494c0789e2",
            "filename": "third_party/xla/xla/service/hlo_module_group_util.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 404,
            "changes": 404,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6bad56c6ba34c0f10bfc53775de3ab18b2284e3a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_group_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6bad56c6ba34c0f10bfc53775de3ab18b2284e3a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_group_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_group_util.cc?ref=6bad56c6ba34c0f10bfc53775de3ab18b2284e3a",
            "patch": "@@ -1,404 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/hlo_module_group_util.h\"\n-\n-#include <algorithm>\n-#include <list>\n-#include <memory>\n-#include <queue>\n-#include <stack>\n-#include <string>\n-#include <utility>\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"xla/hlo/analysis/hlo_reachability.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/status_macros.h\"\n-#include \"xla/types.h\"\n-#include \"xla/util.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/logging.h\"\n-\n-namespace xla {\n-\n-std::vector<HloInstruction*> HloModuleGroupUtil::GlobalPredecessors(\n-    HloInstruction* instruction) {\n-  std::vector<HloInstruction*>\n-      predecessors;  // Use a vector to avoid non-determinism.\n-  absl::flat_hash_set<HloInstruction*> unique;\n-\n-  // Adds to the unique predecessors list; if the predecessors is a companion\n-  // instruction, also add companion instructions; if the predecessors is a\n-  // cross-module all-reduce, also add the all-reduce instructions in the same\n-  // group.\n-  auto add_unique_predecessor = [&](HloInstruction* predecessor) {\n-    if (unique.find(predecessor) != unique.end()) {\n-      return;\n-    }\n-    if (metadata_.IsCompanionInstruction(predecessor)) {\n-      for (HloInstruction* instr : metadata_.Companions(predecessor)) {\n-        if (unique.insert(instr).second) {\n-          predecessors.push_back(instr);\n-        }\n-      }\n-      return;\n-    }\n-    if (predecessor->IsCrossModuleAllReduce()) {\n-      for (HloInstruction* instr :\n-           metadata_.GetAllReduceGroup(*predecessor->channel_id())) {\n-        if (unique.insert(instr).second) {\n-          predecessors.push_back(instr);\n-        }\n-      }\n-      return;\n-    }\n-    unique.insert(predecessor);\n-    predecessors.push_back(predecessor);\n-  };\n-  // If the given instruction is a companion instruction, we need to find the\n-  // predecessors of all of its companion instructions. If the instruction is an\n-  // all-reduce, we need to find the predecessors of all the peer all-reduce\n-  // instructions.\n-  std::vector<HloInstruction*> instruction_group;\n-  if (metadata_.IsCompanionInstruction(instruction)) {\n-    for (HloInstruction* companion : metadata_.Companions(instruction)) {\n-      instruction_group.push_back(companion);\n-    }\n-  } else if (instruction->IsCrossModuleAllReduce()) {\n-    instruction_group = metadata_.GetAllReduceGroup(*instruction->channel_id());\n-  } else {\n-    instruction_group.push_back(instruction);\n-  }\n-\n-  for (HloInstruction* hlo : instruction_group) {\n-    for (HloInstruction* operand : hlo->operands()) {\n-      add_unique_predecessor(operand);\n-    }\n-    for (HloInstruction* control_predecessor : hlo->control_predecessors()) {\n-      add_unique_predecessor(control_predecessor);\n-    }\n-  }\n-  if (instruction->opcode() == HloOpcode::kRecvDone &&\n-      !DynCast<HloRecvDoneInstruction>(instruction)->is_host_transfer()) {\n-    // Send is a remote predecessor of RecvDone.\n-    HloInstruction* send =\n-        metadata_.GetChannel(*instruction->channel_id()).send;\n-    add_unique_predecessor(send);\n-  }\n-  if (instruction->opcode() == HloOpcode::kSend &&\n-      !DynCast<HloSendInstruction>(instruction)->is_host_transfer()) {\n-    // Recv is a remote predecessor of Send.\n-    HloInstruction* recv_done =\n-        metadata_.GetChannel(*instruction->channel_id()).recv_done;\n-    CHECK(recv_done->opcode() == HloOpcode::kRecvDone);\n-    CHECK_EQ(recv_done->operand_count(), 1);\n-    HloInstruction* recv = recv_done->mutable_operand(0);\n-    add_unique_predecessor(recv);\n-  }\n-  return predecessors;\n-}\n-\n-std::vector<HloInstruction*> HloModuleGroupUtil::GlobalSuccessors(\n-    HloInstruction* instruction) {\n-  std::vector<HloInstruction*>\n-      successors;  // Use a vector to avoid non-determinism.\n-  absl::flat_hash_set<HloInstruction*> unique;\n-\n-  // Adds to the unique successors list; if the successor is a companion\n-  // instruction, also add companion instructions; if the successor is a\n-  // cross-module all-reduce, also add the all-reduce instructions in the same\n-  // group.\n-  auto add_unique_successor = [&](HloInstruction* successor) {\n-    if (unique.find(successor) != unique.end()) {\n-      return;\n-    }\n-    if (metadata_.IsCompanionInstruction(successor)) {\n-      for (HloInstruction* instr : metadata_.Companions(successor)) {\n-        if (unique.insert(instr).second) {\n-          successors.push_back(instr);\n-        }\n-      }\n-      return;\n-    }\n-    if (successor->IsCrossModuleAllReduce()) {\n-      for (HloInstruction* instr :\n-           metadata_.GetAllReduceGroup(*successor->channel_id())) {\n-        if (unique.insert(instr).second) {\n-          successors.push_back(instr);\n-        }\n-      }\n-      return;\n-    }\n-    unique.insert(successor);\n-    successors.push_back(successor);\n-  };\n-\n-  // If the given instruction is a companion instruction, we need to find the\n-  // successors of all of its companion instructions. If the instruction is an\n-  // all-reduce, we need to find the successors of all its peer all-reduce\n-  // instructions.\n-  std::vector<HloInstruction*> instruction_group;\n-  if (metadata_.IsCompanionInstruction(instruction)) {\n-    for (HloInstruction* companion : metadata_.Companions(instruction)) {\n-      instruction_group.push_back(companion);\n-    }\n-  } else if (instruction->IsCrossModuleAllReduce()) {\n-    instruction_group = metadata_.GetAllReduceGroup(*instruction->channel_id());\n-  } else {\n-    instruction_group.push_back(instruction);\n-  }\n-\n-  for (HloInstruction* hlo : instruction_group) {\n-    for (HloInstruction* user : hlo->users()) {\n-      add_unique_successor(user);\n-    }\n-    for (HloInstruction* control_successor : hlo->control_successors()) {\n-      add_unique_successor(control_successor);\n-    }\n-  }\n-  if (instruction->opcode() == HloOpcode::kRecv &&\n-      !DynCast<HloRecvInstruction>(instruction)->is_host_transfer()) {\n-    // Send is a remote successor of Recv.\n-    const HloInstruction* recv_done = instruction->users().front();\n-    CHECK(recv_done->opcode() == HloOpcode::kRecvDone);\n-    HloInstruction* send =\n-        metadata_.GetChannel(*instruction->channel_id()).send;\n-    add_unique_successor(send);\n-  }\n-  if (instruction->opcode() == HloOpcode::kSend &&\n-      !DynCast<HloSendInstruction>(instruction)->is_host_transfer()) {\n-    // RecvDone is a remote successor of Send.\n-    HloInstruction* recv_done =\n-        metadata_.GetChannel(*instruction->channel_id()).recv_done;\n-    add_unique_successor(recv_done);\n-  }\n-  return successors;\n-}\n-\n-std::vector<HloInstruction*> HloModuleGroupUtil::RootInstructions(\n-    absl::Span<HloComputation* const> computations) {\n-  std::vector<HloInstruction*> roots;\n-  for (HloComputation* computation : computations) {\n-    for (HloInstruction* instruction : computation->instructions()) {\n-      if (GlobalSuccessors(instruction).empty()) {\n-        // An instruction that has no successors, e.g., an unused instruction,\n-        // is in roots, even though it's not the ROOT of its computation.\n-        roots.push_back(instruction);\n-      }\n-    }\n-  }\n-  return roots;\n-}\n-\n-std::string HloModuleGroupUtil::CycleToString(\n-    HloInstruction* init_instruction) {\n-  std::vector<absl::string_view> names;\n-  absl::flat_hash_set<HloInstruction*> seen;\n-\n-  std::function<bool(HloInstruction*)> helper =\n-      [&](HloInstruction* instruction) {\n-        if (seen.find(instruction) != seen.end()) {\n-          if (instruction == init_instruction) {\n-            names.push_back(instruction->name());\n-            return true;\n-          }\n-          return false;\n-        }\n-        seen.insert(instruction);\n-        for (HloInstruction* predecessor : GlobalPredecessors(instruction)) {\n-          bool init_found = helper(predecessor);\n-          if (init_found) {\n-            names.push_back(instruction->name());\n-            return true;\n-          }\n-        }\n-        return false;\n-      };\n-\n-  helper(init_instruction);\n-  return absl::StrJoin(names, \" --> \");\n-}\n-\n-absl::Status HloModuleGroupUtil::VisitTopologicalOrder(\n-    VisitStates* visit_state, VisitFunction visit_function,\n-    HloInstruction* root, bool send_recv_as_one_group) {\n-  // Stack of HLO instructions visited in DFS order.\n-  std::stack<HloInstruction*> stack;\n-  stack.push(root);\n-\n-  while (!stack.empty()) {\n-    HloInstruction* hlo = stack.top();\n-\n-    // Find the instruction group of the currently visited instruction. The\n-    // instruction group represents all companion instructions of the current\n-    // instruction, or all the all-reduce instructions that belong to the same\n-    // group, or are considered to be a single entity for the purpose of the\n-    // traversal (i.e., they must always be in the same visit state).\n-    std::vector<HloInstruction*> instruction_group;\n-    if (metadata_.IsCompanionInstruction(hlo)) {\n-      for (HloInstruction* companion : metadata_.Companions(hlo)) {\n-        instruction_group.push_back(companion);\n-      }\n-    } else if (hlo->IsCrossModuleAllReduce()) {\n-      instruction_group = metadata_.GetAllReduceGroup(*hlo->channel_id());\n-    } else if (send_recv_as_one_group && metadata_.IsChannelInstruction(hlo)) {\n-      auto channel = metadata_.GetChannel(*hlo->channel_id());\n-      if (channel.recv) {\n-        instruction_group.push_back(channel.recv);\n-      }\n-      if (channel.send) {\n-        instruction_group.push_back(channel.send);\n-      }\n-      if (channel.recv_done) {\n-        instruction_group.push_back(channel.recv_done);\n-      }\n-      if (channel.send_done) {\n-        instruction_group.push_back(channel.send_done);\n-      }\n-    } else {\n-      instruction_group.push_back(hlo);\n-    }\n-\n-    if ((*visit_state)[hlo] == VisitState::kVisited) {\n-      // All instructions in the group must be in the same state.\n-      for (HloInstruction* instruction : instruction_group) {\n-        TF_RET_CHECK((*visit_state)[instruction] == VisitState::kVisited);\n-      }\n-      stack.pop();\n-      continue;\n-    }\n-\n-    if ((*visit_state)[hlo] == VisitState::kVisiting) {\n-      TF_RETURN_IF_ERROR(visit_function(hlo, instruction_group));\n-\n-      // Set the visit state of all instructions in the group to kVisited.\n-      for (HloInstruction* instruction : instruction_group) {\n-        TF_RET_CHECK((*visit_state)[instruction] == VisitState::kVisiting);\n-        (*visit_state)[instruction] = VisitState::kVisited;\n-      }\n-      stack.pop();\n-      continue;\n-    }\n-\n-    // Set the visit state of all instructions in the group to kVisiting.\n-    for (HloInstruction* instruction : instruction_group) {\n-      TF_RET_CHECK((*visit_state)[instruction] == VisitState::kNotVisited)\n-          << instruction->ToString();\n-      (*visit_state)[instruction] = VisitState::kVisiting;\n-    }\n-\n-    // For each instruction in the group, visit its predecessors (operands,\n-    // control predecessors and remote predecessors).\n-    for (HloInstruction* instruction : instruction_group) {\n-      for (HloInstruction* predecessor : GlobalPredecessors(instruction)) {\n-        if (std::find(instruction_group.begin(), instruction_group.end(),\n-                      predecessor) != instruction_group.end()) {\n-          // Ignore the predecessor if it is in the instruction_group.\n-          continue;\n-        }\n-        if ((*visit_state)[predecessor] == VisitState::kVisiting) {\n-          // Visiting a node that is already being visited implies that there is\n-          // a cycle. Generate an error with the list of instructions in the\n-          // cycle.\n-          return FailedPrecondition(\n-              \"Cross-computation cycle detected via communicating nodes.\\n%s\",\n-              CycleToString(predecessor));\n-        } else if ((*visit_state)[predecessor] == VisitState::kNotVisited) {\n-          // We can ignore the visited predecessor and only consider the\n-          // unvisited one.\n-          stack.push(predecessor);\n-        }\n-      }\n-    }\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n-absl::Status HloModuleGroupUtil::VerifyComputations(\n-    absl::Span<HloComputation* const> computations) {\n-  auto visit_function =\n-      [&](HloInstruction* instruction,\n-          const std::vector<HloInstruction*>& instruction_group) {\n-        return absl::OkStatus();\n-      };\n-  int64_t instructions_count = 0;\n-  VisitStates visit_states;\n-  for (HloComputation* computation : computations) {\n-    // Visit all instructions, and not just from the root instruction of the\n-    // computation. This allows us to detect dead cycles (i.e., cycles that\n-    // are not reachable from the root) or to enforce an order for the\n-    // communication instructions that are not reachable from any roots.\n-    for (HloInstruction* instruction : computation->instructions()) {\n-      TF_RETURN_IF_ERROR(\n-          VisitTopologicalOrder(&visit_states, visit_function, instruction));\n-    }\n-    instructions_count += computation->instruction_count();\n-  }\n-\n-  // Check if all instructions are visited and are in the visited state.\n-  TF_RET_CHECK(visit_states.size() == instructions_count);\n-  for (auto& state : visit_states) {\n-    TF_RET_CHECK(state.second == VisitState::kVisited);\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n-absl::StatusOr<std::unique_ptr<HloReachabilityMap>>\n-HloModuleGroupUtil::ComputeReachability(\n-    absl::Span<HloComputation* const> computations) {\n-  std::vector<HloInstruction*> post_order;\n-  auto visit_function =\n-      [&](HloInstruction* instruction,\n-          const std::vector<HloInstruction*>& instruction_group) {\n-        post_order.insert(post_order.end(), instruction_group.begin(),\n-                          instruction_group.end());\n-        return absl::OkStatus();\n-      };\n-  HloModuleGroupUtil::VisitStates visit_states;\n-  for (HloInstruction* root : RootInstructions(computations)) {\n-    TF_RETURN_IF_ERROR(\n-        VisitTopologicalOrder(&visit_states, visit_function, root));\n-  }\n-  auto reachability = std::make_unique<HloReachabilityMap>(post_order);\n-  for (HloInstruction* hlo : post_order) {\n-    reachability->FastSetReachabilityToUnion(GlobalPredecessors(hlo), hlo);\n-  }\n-  return std::move(reachability);\n-}\n-\n-void HloModuleGroupUtil::UpdateReachabilityThroughInstruction(\n-    HloInstruction* instruction, HloReachabilityMap* reachability_map) {\n-  std::queue<HloInstruction*> worklist;\n-  worklist.push(instruction);\n-\n-  while (!worklist.empty()) {\n-    HloInstruction* item = worklist.front();\n-    worklist.pop();\n-    if (reachability_map->SetReachabilityToUnion(GlobalPredecessors(item),\n-                                                 item)) {\n-      for (HloInstruction* successor : GlobalSuccessors(item)) {\n-        worklist.push(successor);\n-      }\n-    }\n-  }\n-}\n-\n-}  // namespace xla"
        },
        {
            "sha": "9f3e28a60686c6e56f2b8968c114fd1cf70a5e1f",
            "filename": "third_party/xla/xla/service/hlo_module_group_util.h",
            "status": "removed",
            "additions": 0,
            "deletions": 123,
            "changes": 123,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6bad56c6ba34c0f10bfc53775de3ab18b2284e3a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_group_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6bad56c6ba34c0f10bfc53775de3ab18b2284e3a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_group_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_group_util.h?ref=6bad56c6ba34c0f10bfc53775de3ab18b2284e3a",
            "patch": "@@ -1,123 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_SERVICE_HLO_MODULE_GROUP_UTIL_H_\n-#define XLA_SERVICE_HLO_MODULE_GROUP_UTIL_H_\n-\n-#include <functional>\n-#include <memory>\n-#include <vector>\n-\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/functional/function_ref.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/hlo/analysis/hlo_reachability.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/service/hlo_module_group_metadata.h\"\n-#include \"tsl/platform/status.h\"\n-\n-namespace xla {\n-\n-// Collection of utilities for handling HloModuleGroups.\n-class HloModuleGroupUtil {\n- public:\n-  explicit HloModuleGroupUtil(const HloModuleGroupMetadata& metadata)\n-      : metadata_(metadata) {}\n-\n-  // Returns all unique predecessors of the instruction. This includes:\n-  // * predecessors in the same computation: operands and control predecessors\n-  // * Recv is a predecessor of Send\n-  // * Send is a predecessor of RecvDone\n-  // * predecessors of companions (if the instruction is a companion while)\n-  // * predecessors' companions (for any predecessor that is a companion while)\n-  std::vector<HloInstruction*> GlobalPredecessors(HloInstruction* instruction);\n-\n-  // Returns all unique successors of the instruction. This includes:\n-  // * successors in the same computation: users and control successors\n-  // * Send is a successor of Recv\n-  // * RecvDone is a successor of Send\n-  // * successors of companions (if the instruction is a companion while)\n-  // * successors' companions (for any successor that is a companion while)\n-  std::vector<HloInstruction*> GlobalSuccessors(HloInstruction* instruction);\n-\n-  // Returns the root instructions of the computations.\n-  std::vector<HloInstruction*> RootInstructions(\n-      absl::Span<HloComputation* const> computations);\n-\n-  // Visit state of each instruction during DFS traversal.\n-  enum VisitState {\n-    kNotVisited = 0,\n-    kVisiting,\n-    kVisited,\n-  };\n-\n-  // Function called on each instruction group during the DFS traversal. See the\n-  // comment for VisitTopologicalOrder()).\n-  using VisitFunction = absl::FunctionRef<absl::Status(\n-      HloInstruction* hlo,\n-      const std::vector<HloInstruction*>& instruction_group)>;\n-\n-  // Given the hlo instruction as the root, recursively visits all its\n-  // predecessor instructions in DFS order to visit nodes in topological order.\n-  //\n-  // Note that the DFS traversal does not only visit nodes in the same\n-  // computation (parent of the root instruction), but also visits nodes in\n-  // different computations connected via communication instructions. During the\n-  // traversal, companion While instructions (see the class comment in\n-  // HloModuleGroupMetadata) are treated as a single instruction (called\n-  // instruction group, which contains only a single instruction if the visiting\n-  // node is not a companion while) -- visiting one of the instructions in the\n-  // group effectively visits all other instructions in the group, and then all\n-  // predecessor instructions of the group are visited.\n-  //\n-  // * visit_state: map from each instruction to its visit state.\n-  // * visit_function: function called when each instruction group.\n-  // * root: the root instruction of the traversal.\n-  // * send_recv_as_one_group: if true, treat (Recv, Send, RecvDone, SendDone)\n-  // as one group.\n-  using VisitStates = absl::flat_hash_map<HloInstruction*, VisitState>;\n-  absl::Status VisitTopologicalOrder(VisitStates* visit_state,\n-                                     VisitFunction visit_function,\n-                                     HloInstruction* root,\n-                                     bool send_recv_as_one_group = false);\n-\n-  // Verifies that the computations are well-formed (e.g., no cycles).\n-  absl::Status VerifyComputations(\n-      absl::Span<HloComputation* const> computations);\n-\n-  // Below Reachability utils resemble those in HloComputation, except that\n-  // they can handle instructions across multiple computations.\n-  //\n-  // Creates the reachability map for the instructions in the computations.\n-  absl::StatusOr<std::unique_ptr<HloReachabilityMap>> ComputeReachability(\n-      absl::Span<HloComputation* const> computations);\n-\n-  // Updates the reachability of the given instruction, taking the global\n-  // predecessors and successors into account.\n-  void UpdateReachabilityThroughInstruction(\n-      HloInstruction* instruction, HloReachabilityMap* reachability_map);\n-\n- private:\n-  std::string CycleToString(HloInstruction* instruction);\n-\n-  const HloModuleGroupMetadata& metadata_;\n-};\n-\n-}  // namespace xla\n-\n-#endif  // XLA_SERVICE_HLO_MODULE_GROUP_UTIL_H_"
        }
    ],
    "stats": {
        "total": 551,
        "additions": 0,
        "deletions": 551
    }
}