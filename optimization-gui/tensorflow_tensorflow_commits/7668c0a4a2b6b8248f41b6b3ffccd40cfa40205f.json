{
    "author": "bchetioui",
    "message": "Rollback of PR #32748\n\nRevert https://github.com/openxla/xla/pull/32748 due to test breakages.\n\nReverts 12bda18eb7037e360275b29a714255428ddfa6a0\n\nPiperOrigin-RevId: 834174542",
    "sha": "7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
    "files": [
        {
            "sha": "8f06af032c53b793ef472b85c0faaca1fc2e5c8c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 19,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -48,7 +48,6 @@ namespace gpu {\n struct ComparisonParams {\n   double relative_tol = 0.1;\n   bool verbose = true;\n-  bool run_host_compare = true;\n   const Shape* shape = nullptr;\n   se::Stream* stream = nullptr;\n   se::DeviceMemoryBase current{};\n@@ -86,14 +85,6 @@ static absl::StatusOr<bool> DeviceCompare(const ComparisonParams& params) {\n \n   LaunchDimensions dim =\n       CalculateLaunchDimensions(*params.shape, gpu_device_info);\n-  // Limit # of blocks to some meaningful number which is large enough to\n-  // occupy all GPU cores if necessary but not too large to reduce # of idle\n-  // blocks\n-  dim = LaunchDimensions(\n-      se::BlockDim(std::min(dim.num_blocks(),\n-                            BufferComparator::kMaxNumThreadBlocksForKernel),\n-                   1, 1),\n-      dim.thread_counts_per_block());\n \n   se::DeviceMemory<uint64_t> as_uint64(out.memory());\n   TF_RETURN_IF_ERROR(comparison_kernel.Launch(\n@@ -170,8 +161,9 @@ static absl::StatusOr<bool> CompareEqualParameterized(\n     const ComparisonParams& params) {\n   XLA_SCOPED_LOGGING_TIMER(\"BufferComparator::CompareEqual\");\n   TF_ASSIGN_OR_RETURN(bool result, DeviceCompare<ElementT>(params));\n-  if (result) return true;\n-  if (!params.run_host_compare) return false;\n+  if (result) {\n+    return true;\n+  }\n \n   TF_ASSIGN_OR_RETURN(bool host_return,\n                       (HostCompare<ElementT, ComparisonT>(params)));\n@@ -181,9 +173,9 @@ static absl::StatusOr<bool> CompareEqualParameterized(\n }\n \n absl::StatusOr<bool> BufferComparator::CompareEqual(\n-    se::Stream* stream, const se::DeviceMemoryBase& current,\n-    const se::DeviceMemoryBase& expected) const {\n-  ComparisonParams params{relative_tol_, verbose_, run_host_compare_, &shape_,\n+    se::Stream* stream, se::DeviceMemoryBase current,\n+    se::DeviceMemoryBase expected) const {\n+  ComparisonParams params{relative_tol_, verbose_, &shape_,\n                           stream,        current,  expected};\n \n   auto do_compare = [&](auto cst_type) {\n@@ -214,11 +206,8 @@ absl::StatusOr<bool> BufferComparator::CompareEqual(\n }\n \n BufferComparator::BufferComparator(const Shape& shape, double tolerance,\n-                                   bool verbose, bool run_host_compare)\n-    : shape_(shape),\n-      relative_tol_(tolerance),\n-      verbose_(verbose),\n-      run_host_compare_(run_host_compare) {\n+                                   bool verbose)\n+    : shape_(shape), relative_tol_(tolerance), verbose_(verbose) {\n   // Normalize complex shapes: since we treat the passed array as a contiguous\n   // storage it does not matter which dimension are we doubling.\n   auto double_dim_size = [&]() {"
        },
        {
            "sha": "d3b016b7ca111293d403e6c180cdb0dfe68f67d3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator.h",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -26,14 +26,11 @@ namespace xla::gpu {\n // A device-side comparator that compares buffers.\n class BufferComparator {\n  public:\n-  // Maximum number of thread blocks to be used for comparator kernel\n-  static constexpr uint64_t kMaxNumThreadBlocksForKernel = 32768;\n-\n   BufferComparator(const BufferComparator&) = delete;\n-  BufferComparator(BufferComparator&&) noexcept = default;\n+  BufferComparator(BufferComparator&&) = default;\n \n   explicit BufferComparator(const Shape& shape, double tolerance = 0.1,\n-                            bool verbose = true, bool run_host_compare = true);\n+                            bool verbose = true);\n \n   // Returns true if the two buffers compare equal. The definition of \"equal\"\n   // is:\n@@ -45,15 +42,12 @@ class BufferComparator {\n   //\n   // See the implementation for the tolerance value.\n   absl::StatusOr<bool> CompareEqual(se::Stream* stream,\n-                                    const se::DeviceMemoryBase& current,\n-                                    const se::DeviceMemoryBase& expected) const;\n-\n+                                    se::DeviceMemoryBase current,\n+                                    se::DeviceMemoryBase expected) const;\n  private:\n   Shape shape_;\n   double relative_tol_;  // relative tolerance for comparison\n   bool verbose_;         // whether to print out error message on mismatch\n-  // enable host-side compare if device compare reports a mismatch\n-  bool run_host_compare_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "a72e6d2f7b05b51403168ddce6519fb136aee955",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -448,45 +448,6 @@ TEST_F(BufferComparatorTest, BF16) {\n                    .value());\n }\n \n-TEST_F(BufferComparatorTest, VeryLargeArray) {\n-  constexpr PrimitiveType number_type = U8;\n-  using NT = primitive_util::PrimitiveTypeToNative<number_type>::type;\n-\n-  // Set non-power-of-two element count on purpose, use aligned byffer siuze\n-  int64_t n_elems = (1LL << 33) - 11,\n-          // Buffer size must be 4-bytes aligned for Memset32\n-      buf_size = (((n_elems + 1) * sizeof(NT)) + 3) & ~3;\n-  auto stream = stream_exec_->CreateStream().value();\n-\n-  // Use host memory here since there is a limitation of 4GB per test on\n-  // device memory alloc\n-  TF_ASSERT_OK_AND_ASSIGN(auto base,\n-                          stream_exec_->HostMemoryAllocate(buf_size));\n-\n-  // We use overlapping lhs and rhs arrays to reduce memory usage, also this\n-  // serves as an extra test for possible pointer aliasing problems\n-  se::DeviceMemoryBase lhs(base->opaque(), n_elems * sizeof(NT)),\n-      rhs(static_cast<NT*>(base->opaque()) + 1, lhs.size());\n-\n-  constexpr uint32_t pattern = 0xABABABAB;\n-  TF_CHECK_OK(stream->Memset32(&lhs, pattern, buf_size));\n-\n-  // First we do \"positive\" test to make sure lhs and rhs are indeed equal:\n-  // disable host comparison here since it could take a while for ~8GB array\n-  BufferComparator comparator(ShapeUtil::MakeShape(number_type, {n_elems}),\n-                              /*tolerance*/ 0.1, /* verbose */ false,\n-                              /*run_host_compare*/ false);\n-  EXPECT_TRUE(comparator.CompareEqual(stream.get(), lhs, rhs).value());\n-\n-  se::DeviceMemoryBase last_word(\n-      static_cast<uint8_t*>(base->opaque()) + (n_elems & ~3), sizeof(uint32_t));\n-  // Change only the very last entry of rhs to verify that the whole arrays are\n-  // compared (if the grid dimensions are not computed correctly, this might\n-  // not be the case).\n-  TF_CHECK_OK(stream->Memset32(&last_word, 0x11223344, last_word.size()));\n-  EXPECT_FALSE(comparator.CompareEqual(stream.get(), lhs, rhs).value());\n-}\n-\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "830c4f6663087720258b5b2bc60401f86429dc06",
            "filename": "third_party/xla/xla/service/gpu/launch_dimensions.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 8,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.cc?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -54,16 +54,36 @@ LaunchDimensions CalculateLaunchDimensions(\n   num_elements = CeilOfRatio(num_elements, int64_t{dim_config.unroll_factor});\n   const int kWarpSchedulers = 4;\n \n-  int64_t threads_per_block = std::min<int64_t>(\n-      gpu_device_info.threads_per_warp() * kWarpSchedulers, num_elements);\n+  if (xla::PlatformUtil::CanonicalPlatformName(\"gpu\").value() == \"rocm\") {\n+    int64_t threads_per_block_x = std::min<int64_t>(\n+        gpu_device_info.threads_per_warp() * kWarpSchedulers, num_elements);\n \n-  int64_t num_blocks_total = CeilOfRatio(num_elements, threads_per_block);\n-  int64_t num_blocks_y = CeilOfRatio<uint64_t>(\n-      num_blocks_total, gpu_device_info.block_dim_limit().x);\n-  int64_t num_blocks_x = CeilOfRatio(num_blocks_total, num_blocks_y);\n+    int64_t num_blocks = CeilOfRatio(num_elements, threads_per_block_x);\n+    CHECK(num_blocks < gpu_device_info.block_dim_limit().x);\n \n-  return LaunchDimensions(se::BlockDim(num_blocks_x, num_blocks_y, 1),\n-                          se::ThreadDim(threads_per_block, 1, 1));\n+    int threads_per_block_y = 1;\n+    while ((num_blocks * threads_per_block_x) >\n+           std::numeric_limits<uint32_t>::max()) {\n+      threads_per_block_x /= 2;\n+      threads_per_block_y *= 2;\n+    }\n+\n+    return LaunchDimensions(\n+        se::BlockDim(num_blocks, 1, 1),\n+        se::ThreadDim(threads_per_block_x, threads_per_block_y, 1));\n+\n+  } else {\n+    int64_t threads_per_block = std::min<int64_t>(\n+        gpu_device_info.threads_per_warp() * kWarpSchedulers, num_elements);\n+\n+    int64_t num_blocks_total = CeilOfRatio(num_elements, threads_per_block);\n+    int64_t num_blocks_y = CeilOfRatio<uint64_t>(\n+        num_blocks_total, gpu_device_info.block_dim_limit().x);\n+    int64_t num_blocks_x = CeilOfRatio(num_blocks_total, num_blocks_y);\n+\n+    return LaunchDimensions(se::BlockDim(num_blocks_x, num_blocks_y, 1),\n+                            se::ThreadDim(threads_per_block, 1, 1));\n+  }\n }\n \n LaunchDimensionsProto LaunchDimensions::ToProto() const {"
        },
        {
            "sha": "bb42b6a8af886e0bfe4e4a78e23183588cf73bc2",
            "filename": "third_party/xla/xla/service/gpu/launch_dimensions.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.h?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -49,9 +49,9 @@ class LaunchDimensions {\n       : block_counts_(block_counts),\n         thread_counts_per_block_(thread_counts_per_block) {}\n \n-  const se::BlockDim& block_counts() const { return block_counts_; }\n+  se::BlockDim block_counts() const { return block_counts_; }\n \n-  const se::ThreadDim& thread_counts_per_block() const {\n+  se::ThreadDim thread_counts_per_block() const {\n     return thread_counts_per_block_;\n   }\n "
        },
        {
            "sha": "2d9f26160e0d2582940470a91928d732ca0fba94",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_comparator_kernel_lib.cu.h",
            "status": "modified",
            "additions": 41,
            "deletions": 45,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel_lib.cu.h?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -61,32 +61,32 @@ template <typename T>\n __global__ void xla_fp_comparison(T* buffer_a, T* buffer_b,\n                                   float rel_error_threshold,\n                                   uint64_t buffer_length, int* mismatch_count) {\n-  const uint64_t block_dim_x = static_cast<uint64_t>(blockDim.x),\n-                 stride = block_dim_x * gridDim.x;\n-  for (uint64_t idx = threadIdx.x + blockIdx.x * block_dim_x;\n-       idx < buffer_length; idx += stride) {\n-    auto elem_a = Canonicalize(buffer_a[idx]);\n-    auto elem_b = Canonicalize(buffer_b[idx]);\n-\n-    // NaN's are considered equal.\n-    if (Eigen::numext::isnan(elem_a) && Eigen::numext::isnan(elem_b)) {\n-      continue;\n-    }\n-    // Two infinities are considered equal. Computing relative error would\n-    // otherwise result in NaN.\n-    if (elem_a == elem_b) {\n-      continue;\n-    }\n-\n-    float rel_error = Eigen::numext::abs(elem_a - elem_b) /\n-                      (Eigen::numext::maxi(Eigen::numext::abs(elem_a),\n-                                           Eigen::numext::abs(elem_b)) +\n-                       1);\n-\n-    if (rel_error > rel_error_threshold || Eigen::numext::isnan(rel_error)) {\n-      atomicAdd(mismatch_count, 1);\n-    }\n-  }  // for\n+  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n+  if (idx >= buffer_length) {\n+    return;\n+  }\n+\n+  auto elem_a = Canonicalize(buffer_a[idx]);\n+  auto elem_b = Canonicalize(buffer_b[idx]);\n+\n+  // NaN's are considered equal.\n+  if (Eigen::numext::isnan(elem_a) && Eigen::numext::isnan(elem_b)) {\n+    return;\n+  }\n+\n+  // Two infinities are considered equal. Computing relative error would\n+  // otherwise result in NaN.\n+  if (elem_a == elem_b) {\n+    return;\n+  }\n+\n+  float rel_error = Eigen::numext::abs(elem_a - elem_b) /\n+                    (Eigen::numext::maxi(Eigen::numext::abs(elem_a),\n+                                         Eigen::numext::abs(elem_b)) +\n+                     1);\n+\n+  if (rel_error > rel_error_threshold || Eigen::numext::isnan(rel_error))\n+    atomicAdd(mismatch_count, 1);\n }\n \n // TODO(b/191520348): The comparison below requires exact equality.\n@@ -95,25 +95,21 @@ __global__ void xla_int_comparison(T* buffer_a, T* buffer_b,\n                                    float rel_error_threshold,\n                                    uint64_t buffer_length,\n                                    int* mismatch_count) {\n-  const uint64_t block_dim_x = static_cast<uint64_t>(blockDim.x),\n-                 stride = block_dim_x * gridDim.x;\n-  for (uint64_t idx = threadIdx.x + blockIdx.x * block_dim_x;\n-       idx < buffer_length; idx += stride) {\n-    float elem_a;\n-    float elem_b;\n-    if constexpr (std::numeric_limits<T>::is_signed) {\n-      elem_a = static_cast<int64_t>(buffer_a[idx]);\n-      elem_b = static_cast<int64_t>(buffer_b[idx]);\n-    } else {\n-      elem_a = static_cast<uint64_t>(buffer_a[idx]);\n-      elem_b = static_cast<uint64_t>(buffer_b[idx]);\n-    }\n-    float rel_error =\n-        fabs(elem_a - elem_b) / (fmax(fabs(elem_a), fabs(elem_b)) + 1);\n-    if (rel_error > rel_error_threshold || isnan(rel_error)) {\n-      atomicAdd(mismatch_count, 1);\n-    }\n-  }  // for\n+  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n+  if (idx >= buffer_length) return;\n+  float elem_a;\n+  float elem_b;\n+  if constexpr (std::numeric_limits<T>::is_signed) {\n+    elem_a = static_cast<int64_t>(buffer_a[idx]);\n+    elem_b = static_cast<int64_t>(buffer_b[idx]);\n+  } else {\n+    elem_a = static_cast<uint64_t>(buffer_a[idx]);\n+    elem_b = static_cast<uint64_t>(buffer_b[idx]);\n+  }\n+  float rel_error =\n+      fabs(elem_a - elem_b) / (fmax(fabs(elem_a), fabs(elem_b)) + 1);\n+  if (rel_error > rel_error_threshold || isnan(rel_error))\n+    atomicAdd(mismatch_count, 1);\n }\n \n template <typename NativeT>"
        },
        {
            "sha": "e698987b7dc6d2564ad0f0a894d5784fe036b8c8",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -178,8 +178,7 @@ static absl::Status RunRedzoneChecker(\n   int64_t threads_per_block = std::min(\n       executor->GetDeviceDescription().threads_per_block_limit(), num_elements);\n   int64_t block_count =\n-      std::min(tsl::MathUtil::CeilOfRatio(num_elements, threads_per_block),\n-               RedzoneAllocator::kMaxNumThreadBlocksForKernel);\n+      tsl::MathUtil::CeilOfRatio(num_elements, threads_per_block);\n \n   TF_RETURN_IF_ERROR(comparison_kernel.Launch(\n       ThreadDim(threads_per_block), BlockDim(block_count), stream, redzone,"
        },
        {
            "sha": "dba6fe2edd5af6c45f760f442ebafe37063d929d",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator.h",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -47,9 +47,6 @@ class RedzoneAllocator : public ScratchAllocator {\n   static constexpr int64_t kDefaultRedzoneSize =\n       1LL << 23;  // 8MiB per side, 16MiB total.\n   static constexpr uint8_t kDefaultRedzonePattern = -1;  // NOLINT\n-  // Maximum number of thread blocks to be used for redzone checker kernel\n-  static constexpr int64_t kMaxNumThreadBlocksForKernel = 32768;\n-\n   RedzoneAllocator(Stream* stream, DeviceMemoryAllocator* memory_allocator,\n                    int64_t memory_limit = (1LL << 32),  // 4GB\n                    int64_t redzone_size = kDefaultRedzoneSize,"
        },
        {
            "sha": "5624ecea6e9333287b015ff68a096adf8f99b462",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator_kernel_lib.cu.h",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel_lib.cu.h?ref=7668c0a4a2b6b8248f41b6b3ffccd40cfa40205f",
            "patch": "@@ -24,11 +24,12 @@ __global__ void RedzoneAllocatorKernelImpl(uint8_t* input_buffer,\n                                            uint8_t redzone_pattern,\n                                            uint64_t buffer_length,\n                                            uint32_t* out_mismatched_ptr) {\n-  const uint64_t block_dim_x = static_cast<uint64_t>(blockDim.x),\n-                 stride = block_dim_x * gridDim.x;\n-  for (uint64_t idx = threadIdx.x + blockIdx.x * block_dim_x;\n-       idx < buffer_length; idx += stride) {\n-    if (input_buffer[idx] != redzone_pattern) atomicAdd(out_mismatched_ptr, 1);\n+  uint64_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n+  if (idx >= buffer_length) {\n+    return;\n+  }\n+  if (input_buffer[idx] != redzone_pattern) {\n+    atomicAdd(out_mismatched_ptr, 1);\n   }\n }\n }  // namespace stream_executor::gpu"
        }
    ],
    "stats": {
        "total": 223,
        "additions": 90,
        "deletions": 133
    }
}