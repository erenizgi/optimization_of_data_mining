{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 847070976",
    "sha": "cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
    "files": [
        {
            "sha": "343eba3db82f975f51f26238efbe5d98697a5ed2",
            "filename": "tensorflow/core/kernels/matmul_op_fused.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_fused.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_fused.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_fused.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -199,7 +199,7 @@ struct LaunchFusedMatMulOp<CPUDevice, T> {\n namespace {\n \n #if GOOGLE_CUDA || TF_HIPBLASLT\n-StatusOr<se::gpu::BlasLt::Epilogue> GetBlasLtEpilogOp(\n+absl::StatusOr<stream_executor::gpu::BlasLt::Epilogue> GetBlasLtEpilogOp(\n     FusedComputationType fusion) {\n   if (fusion == FusedComputationType::kBiasAdd) {\n     return se::gpu::BlasLt::Epilogue::kBias;\n@@ -235,7 +235,7 @@ se::blas::AlgorithmConfig AutotuneMatmul(\n       // scratch space is deallocated between runs.\n       BlasScratchAllocator scratch_allocator(context);\n \n-      Status cublaslt_launch =\n+      absl::Status cublaslt_launch =\n           launch_func(scratch_allocator, i, &profile_result);\n \n       VLOG(4) << \"  Autotune algorithm \" << i\n@@ -265,7 +265,7 @@ se::blas::AlgorithmConfig AutotuneMatmul(\n #endif\n \n template <typename LaunchFunc, typename Sig>\n-StatusOr<std::vector<xla::AutotuneResult>> AutotuneMatMulImpl(\n+absl::StatusOr<std::vector<xla::AutotuneResult>> AutotuneMatMulImpl(\n     OpKernelContext* ctx,\n     std::vector<std::unique_ptr<const se::dnn::OpRunner<Sig>>>& runners,\n     bool actually_do_autotune, const LaunchFunc& launch_func,\n@@ -292,10 +292,10 @@ StatusOr<std::vector<xla::AutotuneResult>> AutotuneMatMulImpl(\n \n     TF_ASSIGN_OR_RETURN(auto desc, runner->ToAlgorithmDesc());\n     se::dnn::ProfileResult profile_result;\n-    Status cudnn_launch_status =\n+    absl::Status cudnn_launch_status =\n         actually_do_autotune\n             ? launch_func(allocator_used, runner, &profile_result)\n-            : OkStatus();\n+            : absl::OkStatus();\n     if (!actually_do_autotune) {\n       // Make the result valid according to `is_valid`.\n       profile_result.set_algorithm(desc);\n@@ -329,15 +329,16 @@ StatusOr<std::vector<xla::AutotuneResult>> AutotuneMatMulImpl(\n }\n \n struct FusedMatmulAutotuneGroup {\n-  static string name() { return \"FusedMatmul\"; }\n+  static std::string name() { return \"FusedMatmul\"; }\n };\n \n typedef AutotuneSingleton<FusedMatmulAutotuneGroup, MatmulParameters,\n                           AutotuneEntry<se::dnn::FusedMatmulOp>>\n     FusedMatmulAutotuneMap;\n \n template <typename T>\n-StatusOr<AutotuneEntry<se::dnn::FusedMatmulOp>> AutotuneFusedMatmul(\n+absl::StatusOr<AutotuneEntry<stream_executor::dnn::FusedMatmulOp>>\n+AutotuneFusedMatmul(\n     bool cudnn_use_autotune,\n     AutotuneMap<MatmulParameters, AutotuneEntry<se::dnn::FusedMatmulOp>>*\n         autotune_map,\n@@ -350,7 +351,7 @@ StatusOr<AutotuneEntry<se::dnn::FusedMatmulOp>> AutotuneFusedMatmul(\n   AutotuneEntry<se::dnn::FusedMatmulOp> autotune_entry;\n   auto* stream = ctx->op_device_context()->stream();\n   if (!autotune_map->Find(params, &autotune_entry)) {\n-    profiler::ScopedAnnotation trace(\"cudnn_autotuning\");\n+    tsl::profiler::ScopedAnnotation trace(\"cudnn_autotuning\");\n \n     se::TfAllocatorAdapter tf_allocator_adapter(ctx->device()->GetAllocator({}),\n                                                 stream);\n@@ -371,7 +372,7 @@ StatusOr<AutotuneEntry<se::dnn::FusedMatmulOp>> AutotuneFusedMatmul(\n     auto launch_func =\n         [&](se::ScratchAllocator* allocator_used,\n             const std::unique_ptr<const se::dnn::FusedMatmulRunner>& runner,\n-            se::dnn::ProfileResult* profile_result) -> Status {\n+            se::dnn::ProfileResult* profile_result) -> absl::Status {\n       TF_ASSIGN_OR_RETURN(auto scratch, allocator_used->AllocateBytes(\n                                             runner->GetWorkspaceSize()));\n       return (*runner)(stream, profile_result, scratch, a_ptr, b_ptr, bias_ptr,\n@@ -562,8 +563,9 @@ struct LaunchFusedMatMulOp<GPUDevice, T> {\n       auto runner_and_scratch = std::move(runner_and_scratch_or).value();\n       auto& runner =\n           *std::get<const se::dnn::FusedMatmulRunner*>(runner_and_scratch);\n-      Status cudnn_launch_status = runner(\n-          stream, nullptr, std::get<se::DeviceMemoryBase>(runner_and_scratch),\n+      absl::Status cudnn_launch_status = runner(\n+          stream, nullptr,\n+          std::get<stream_executor::DeviceAddressBase>(runner_and_scratch),\n           a_ptr, b_ptr, bias_ptr, c_ptr);\n       OP_REQUIRES_OK(context, cudnn_launch_status);\n       return;"
        },
        {
            "sha": "628e6d8dabceb28077eb108cfe30f26fe7b0d45a",
            "filename": "tensorflow/core/kernels/matmul_op_impl.h",
            "status": "modified",
            "additions": 23,
            "deletions": 19,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmatmul_op_impl.h?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -477,7 +477,7 @@ struct LaunchBatchMatMul<CPUDevice, Scalar> {\n namespace {\n // A dummy type to group matmul autotune results together.\n struct BlasLtMatmulAutoTuneGroup {\n-  static string name() { return \"MatmulLt\"; }\n+  static std::string name() { return \"MatmulLt\"; }\n };\n \n typedef AutotuneSingleton<BlasLtMatmulAutoTuneGroup, BlasLtMatmulPlanParams,\n@@ -493,7 +493,7 @@ typedef AutotuneSingleton<BlasLtMatmulAutoTuneGroup, BlasLtMatmulPlanParams,\n class BlasScratchAllocator : public se::ScratchAllocator {\n  public:\n   using Stream = se::Stream;\n-  using DeviceMemoryBytes = se::DeviceMemory<uint8>;\n+  using DeviceMemoryBytes = stream_executor::DeviceAddress<uint8>;\n \n   BlasScratchAllocator(OpKernelContext* context)\n       : memory_limit_(0), total_byte_size_(0), context_(context) {}\n@@ -503,21 +503,22 @@ class BlasScratchAllocator : public se::ScratchAllocator {\n \n   int64_t GetMemoryLimitInBytes() override { return memory_limit_; }\n \n-  tsl::StatusOr<DeviceMemoryBytes> AllocateBytes(int64_t byte_size) override {\n+  absl::StatusOr<BlasScratchAllocator::DeviceMemoryBytes> AllocateBytes(\n+      int64_t byte_size) override {\n     Tensor temporary_memory;\n \n     if (memory_limit_ > 0 && byte_size > memory_limit_) {\n-      return tsl::Status{\n+      return absl::Status{\n           absl::StatusCode::kUnavailable,\n           absl::StrCat(\"Requested memory size (\", byte_size,\n                        \") exceeds the memory limit (\", memory_limit_, \").\")};\n     }\n     AllocationAttributes allocation_attr;\n     allocation_attr.retry_on_failure = false;\n-    Status allocation_status(context_->allocate_temp(\n+    absl::Status allocation_status(context_->allocate_temp(\n         DT_UINT8, TensorShape({byte_size}), &temporary_memory));\n     if (!allocation_status.ok()) {\n-      return tsl::Status{\n+      return absl::Status{\n           absl::StatusCode::kUnavailable,\n           absl::StrCat(\"Failed to allocate requested memory of (\", byte_size,\n                        \").\")};\n@@ -526,11 +527,12 @@ class BlasScratchAllocator : public se::ScratchAllocator {\n     // allocator.\n     allocated_tensors_.push_back(temporary_memory);\n     total_byte_size_ += byte_size;\n-    return tsl::StatusOr<DeviceMemoryBytes>(DeviceMemoryBytes::MakeFromByteSize(\n-        temporary_memory.flat<uint8>().data(),\n-        temporary_memory.flat<uint8>().size()));\n+    return absl::StatusOr<BlasScratchAllocator::DeviceMemoryBytes>(\n+        DeviceMemoryBytes::MakeFromByteSize(\n+            temporary_memory.flat<uint8_t>().data(),\n+            temporary_memory.flat<uint8_t>().size()));\n   }\n-  int64 TotalByteSize() { return total_byte_size_; }\n+  int64_t TotalByteSize() { return total_byte_size_; }\n \n  private:\n   int64_t memory_limit_;\n@@ -548,9 +550,9 @@ struct LaunchBatchMatMul<GPUDevice, Scalar> {\n     se::blas::Transpose trans[] = {se::blas::Transpose::kNoTranspose,\n                                    se::blas::Transpose::kTranspose,\n                                    se::blas::Transpose::kConjugateTranspose};\n-    const uint64 m = in_x.dim_size(adj_x || trans_x ? 2 : 1);\n-    const uint64 k = in_x.dim_size(adj_x || trans_x ? 1 : 2);\n-    const uint64 n = in_y.dim_size(adj_y || trans_y ? 1 : 2);\n+    const uint64_t m = in_x.dim_size(adj_x || trans_x ? 2 : 1);\n+    const uint64_t k = in_x.dim_size(adj_x || trans_x ? 1 : 2);\n+    const uint64_t n = in_y.dim_size(adj_y || trans_y ? 1 : 2);\n     const int64_t batch_size = bcast.output_batch_size();\n     auto blas_transpose_a = trans[adj_x ? 2 : (trans_x ? 1 : 0)];\n     auto blas_transpose_b = trans[adj_y ? 2 : (trans_y ? 1 : 0)];\n@@ -574,9 +576,9 @@ struct LaunchBatchMatMul<GPUDevice, Scalar> {\n     auto* a_base_ptr = in_x.template flat<Scalar>().data();\n     auto* b_base_ptr = in_y.template flat<Scalar>().data();\n     auto* c_base_ptr = out->template flat<Scalar>().data();\n-    uint64 a_stride;\n-    uint64 b_stride;\n-    uint64 c_stride;\n+    uint64_t a_stride;\n+    uint64_t b_stride;\n+    uint64_t c_stride;\n \n     bool is_full_broadcast =\n         std::min(bcast.x_batch_size(), bcast.y_batch_size()) == 1;\n@@ -658,9 +660,11 @@ struct LaunchBatchMatMul<GPUDevice, Scalar> {\n             // Create a new scratch allocator with every autotuning run so that\n             // scratch space is deallocated between runs.\n             BlasScratchAllocator scratch_allocator(context, max_scratch_size);\n-            Status cublas_launch_status = plan_and_algorithms->ExecuteOnStream(\n-                stream, *a_ptrs[0], *b_ptrs[0], *c_ptrs[0], i,\n-                scratch_allocator, se::DeviceMemoryBase{}, &profile_result);\n+            absl::Status cublas_launch_status =\n+                plan_and_algorithms->ExecuteOnStream(\n+                    stream, *a_ptrs[0], *b_ptrs[0], *c_ptrs[0], i,\n+                    scratch_allocator, stream_executor::DeviceAddressBase{},\n+                    &profile_result);\n \n             VLOG(4) << \"  Autotune algorithm \" << i\n                     << \" result: \" << profile_result.elapsed_time_in_ms()"
        },
        {
            "sha": "cd3a950f8f5c698d3bb46fbc6b621f05531303d3",
            "filename": "tensorflow/core/kernels/matmul_util.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 21,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmatmul_util.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -36,8 +36,7 @@ int64_t GetWorkspaceLimit(int64_t default_value_in_bytes) {\n   if (workspace_limit_in_mb_str != nullptr &&\n       strcmp(workspace_limit_in_mb_str, \"\") != 0) {\n     int64_t scratch_limit_in_mb = -1;\n-    if (strings::safe_strto64(workspace_limit_in_mb_str,\n-                              &scratch_limit_in_mb)) {\n+    if (absl::SimpleAtoi(workspace_limit_in_mb_str, &scratch_limit_in_mb)) {\n       return scratch_limit_in_mb * (1 << 20);\n     } else {\n       LOG(WARNING) << \"Invalid value for TF_CUBLAS_WORKSPACE_LIMIT_IN_MB: \"\n@@ -77,7 +76,7 @@ struct BlasLtMatmulPlanMap {\n \n int MatmulMaxAutotuneAlgorithmCount() {\n   int64_t value;\n-  Status status =\n+  absl::Status status =\n       ReadInt64FromEnvVar(\"TF_MATMUL_AUTOTUNE_MAX_ALGORITHMS\", 10, &value);\n   if (!status.ok()) {\n     LOG(ERROR) << status.message();\n@@ -90,7 +89,7 @@ int MatmulMaxAutotuneAlgorithmCount() {\n   return value;\n }\n \n-StatusOr<se::blas::ComputationType> GetBlasComputationType(\n+absl::StatusOr<stream_executor::blas::ComputationType> GetBlasComputationType(\n     se::blas::DataType dtype) {\n   using se::blas::ComputationType;\n   static bool use_f32_for_f16_computation = MatmulDoFP32ComputationFP16Input();\n@@ -114,9 +113,11 @@ StatusOr<se::blas::ComputationType> GetBlasComputationType(\n \n }  // namespace\n \n-/* static */ StatusOr<const PlanAndAlgorithms*> PlanAndAlgorithms::GetOrCreate(\n-    se::Stream* stream, const BlasLtMatmulPlanParams& params,\n-    absl::Mutex** ppmu, std::optional<int> max_algorithm_count) {\n+/* static */ absl::StatusOr<const PlanAndAlgorithms*>\n+PlanAndAlgorithms::GetOrCreate(se::Stream* stream,\n+                               const BlasLtMatmulPlanParams& params,\n+                               absl::Mutex** ppmu,\n+                               std::optional<int> max_algorithm_count) {\n   static const int64_t max_scratch_size =\n       GetWorkspaceLimit(1LL << 32);  // 4GB by default\n   static const int64_t max_autotune_algorithm_count =\n@@ -189,25 +190,27 @@ StatusOr<se::blas::ComputationType> GetBlasComputationType(\n   return ptr->second.get();\n }\n \n-Status PlanAndAlgorithms::ExecuteOnStream(\n-    se::Stream* stream, const se::DeviceMemoryBase& a,\n-    const se::DeviceMemoryBase& b, se::DeviceMemoryBase& c,\n-    size_t algorithm_idx, se::ScratchAllocator& scratch_allocator,\n-    const se::DeviceMemoryBase& bias,\n+absl::Status PlanAndAlgorithms::ExecuteOnStream(\n+    se::Stream* stream, const stream_executor::DeviceAddressBase& a,\n+    const stream_executor::DeviceAddressBase& b,\n+    stream_executor::DeviceAddressBase& c, size_t algorithm_idx,\n+    se::ScratchAllocator& scratch_allocator,\n+    const stream_executor::DeviceAddressBase& bias,\n     se::blas::ProfileResult* profile_result) const {\n   if (!plan || algorithm_idx >= algorithms.size()) {\n     return errors::Internal(\"MatmulPlan or algorithms are not initialized!\");\n   }\n   TF_RETURN_IF_ERROR(plan->SetAlgorithm(algorithms[algorithm_idx]));\n-  return plan->ExecuteOnStream(stream, a, b, c, c,\n-                               bias,                    // bias_buffer\n-                               se::DeviceMemoryBase{},  // aux_buffer\n-                               se::DeviceMemoryBase{},  // a_scale_buffer\n-                               se::DeviceMemoryBase{},  // b_scale_buffer\n-                               se::DeviceMemoryBase{},  // c_scale_buffer\n-                               se::DeviceMemoryBase{},  // d_scale_buffer\n-                               se::DeviceMemoryBase{},  // d_amax_buffer\n-                               scratch_allocator, profile_result);\n+  return plan->ExecuteOnStream(\n+      stream, a, b, c, c,\n+      bias,                                  // bias_buffer\n+      stream_executor::DeviceAddressBase{},  // aux_buffer\n+      stream_executor::DeviceAddressBase{},  // a_scale_buffer\n+      stream_executor::DeviceAddressBase{},  // b_scale_buffer\n+      stream_executor::DeviceAddressBase{},  // c_scale_buffer\n+      stream_executor::DeviceAddressBase{},  // d_scale_buffer\n+      stream_executor::DeviceAddressBase{},  // d_amax_buffer\n+      scratch_allocator, profile_result);\n }\n \n }  // namespace tensorflow"
        },
        {
            "sha": "abcbe0ad1bea4490f3ff9644f23e61f7d11cf18c",
            "filename": "tensorflow/core/kernels/matmul_util.h",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmatmul_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmatmul_util.h?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -51,15 +51,17 @@ struct BlasLtMatmulPlanParams {\n };\n \n struct PlanAndAlgorithms {\n-  static StatusOr<const PlanAndAlgorithms*> GetOrCreate(\n+  static absl::StatusOr<const PlanAndAlgorithms*> GetOrCreate(\n       se::Stream* stream, const BlasLtMatmulPlanParams& params,\n       absl::Mutex** pmu, std::optional<int> max_algorithm_count = std::nullopt);\n \n-  Status ExecuteOnStream(\n-      se::Stream* stream, const se::DeviceMemoryBase& a,\n-      const se::DeviceMemoryBase& b, se::DeviceMemoryBase& c,\n-      size_t algorithm_idx, se::ScratchAllocator& scratch_allocator,\n-      const se::DeviceMemoryBase& bias = se::DeviceMemoryBase{},\n+  absl::Status ExecuteOnStream(\n+      se::Stream* stream, const stream_executor::DeviceAddressBase& a,\n+      const stream_executor::DeviceAddressBase& b,\n+      stream_executor::DeviceAddressBase& c, size_t algorithm_idx,\n+      se::ScratchAllocator& scratch_allocator,\n+      const stream_executor::DeviceAddressBase& bias =\n+          stream_executor::DeviceAddressBase{},\n       se::blas::ProfileResult* profile_result = nullptr) const;\n \n   se::gpu::BlasLt::MatmulPlanPtr plan;"
        },
        {
            "sha": "e7799161eba16c3616618c7b5d7b955bf2085e8d",
            "filename": "tensorflow/core/kernels/maxpooling_op_gpu.cu.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmaxpooling_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmaxpooling_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmaxpooling_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -70,7 +70,7 @@ __global__ void MaxPoolForwardNCHW(\n     const int channels, const int height, const int width,\n     const int pooled_height, const int pooled_width, const int kernel_h,\n     const int kernel_w, const int stride_h, const int stride_w, const int pad_t,\n-    const int pad_l, dtype* __restrict__ top_data, int64* __restrict__ mask,\n+    const int pad_l, dtype* __restrict__ top_data, int64_t* __restrict__ mask,\n     const bool include_batch_in_index) {\n   GPU_1D_KERNEL_LOOP(index, nthreads) {\n     int pw = index % pooled_width;\n@@ -110,13 +110,13 @@ __global__ void MaxPoolForwardNCHW(\n // the same X, y coordinate.\n // (so channels = outer_channels, output_size = real output size / 4).\n __global__ void MaxPoolForwardNoMaskKernel_NCHW_VECT_C(\n-    const int nthreads, const int32* __restrict__ bottom_data, const int height,\n-    const int width, const int channels, const int pooled_height,\n-    const int pooled_width, const int kernel_h, const int kernel_w,\n-    const int stride_h, const int stride_w, const int pad_t, const int pad_l,\n-    int32* __restrict__ top_data) {\n+    const int nthreads, const int32_t* __restrict__ bottom_data,\n+    const int height, const int width, const int channels,\n+    const int pooled_height, const int pooled_width, const int kernel_h,\n+    const int kernel_w, const int stride_h, const int stride_w, const int pad_t,\n+    const int pad_l, int32_t* __restrict__ top_data) {\n   // TODO(pauldonnelly): Implement a better optimized version of this kernel.\n-  const int32 kMinINT8X4 = 0x80808080;\n+  const int32_t kMinINT8X4 = 0x80808080;\n   GPU_1D_KERNEL_LOOP(index, nthreads) {\n     int pw = index % pooled_width;\n     int ph = (index / pooled_width) % pooled_height;\n@@ -128,8 +128,8 @@ __global__ void MaxPoolForwardNoMaskKernel_NCHW_VECT_C(\n     int wend = min(wstart + kernel_w, width);\n     hstart = max(hstart, 0);\n     wstart = max(wstart, 0);\n-    int32 maxval = kMinINT8X4;\n-    const int32* bottom_data_n = bottom_data + n * channels * height * width;\n+    int32_t maxval = kMinINT8X4;\n+    const int32_t* bottom_data_n = bottom_data + n * channels * height * width;\n     for (int h = hstart; h < hend; ++h) {\n       for (int w = wstart; w < wend; ++w) {\n         int idx = (c * height + h) * width + w;\n@@ -147,7 +147,7 @@ __global__ void MaxPoolForwardNHWC(\n     const int width, const int channels, const int pooled_height,\n     const int pooled_width, const int kernel_h, const int kernel_w,\n     const int stride_h, const int stride_w, const int pad_t, const int pad_l,\n-    dtype* __restrict__ top_data, int64* __restrict__ mask,\n+    dtype* __restrict__ top_data, int64_t* __restrict__ mask,\n     const bool include_batch_in_index) {\n   GPU_1D_KERNEL_LOOP(index, nthreads) {\n     int n = index;\n@@ -203,7 +203,7 @@ __global__ void MaxPoolForwardNHWC(\n template <typename dtype>\n __global__ void MaxPoolBackward(const int nthreads,\n                                 const dtype* __restrict__ top_diff,\n-                                const int64* __restrict__ mask,\n+                                const int64_t* __restrict__ mask,\n                                 const int top_offset, const int bottom_offset,\n                                 dtype* __restrict__ bottom_diff,\n                                 const bool include_batch_in_index) {\n@@ -332,7 +332,7 @@ __global__ void MaxPoolGradBackwardNoMaskNHWC(\n template <typename dtype>\n __global__ void MaxPoolGradBackward(const int nthreads,\n                                     const dtype* __restrict__ top_diff,\n-                                    const int64* __restrict__ mask,\n+                                    const int64_t* __restrict__ mask,\n                                     const int top_offset,\n                                     const int bottom_offset,\n                                     dtype* __restrict__ bottom_diff,\n@@ -353,11 +353,11 @@ namespace functor {\n // Note: channels is the outer channels (dim 1) which has already been\n // divided by 4.\n bool MaxPoolForwardNoMask_NCHW_VECT_C::operator()(\n-    const int32* bottom_data, const int batch, const int height,\n+    const int32_t* bottom_data, const int batch, const int height,\n     const int width, int channels, const int pooled_height,\n     const int pooled_width, const int kernel_h, const int kernel_w,\n     const int stride_h, const int stride_w, const int pad_t, const int pad_l,\n-    int32* top_data, const Eigen::GpuDevice& d) {\n+    int32_t* top_data, const Eigen::GpuDevice& d) {\n   const int kThreadsPerBlock = 1024;\n   const int output_size = batch * channels * pooled_height * pooled_width;\n   if (output_size == 0) return true;\n@@ -377,7 +377,7 @@ bool MaxPoolForwardWithOptionalArgmax<T>::operator()(\n     const int channels, const int pooled_height, const int pooled_width,\n     const int kernel_h, const int kernel_w, const int stride_h,\n     const int stride_w, const int pad_t, const int pad_l, T* top_data,\n-    int64* mask, const Eigen::GpuDevice& d, bool propagate_nans,\n+    int64_t* mask, const Eigen::GpuDevice& d, bool propagate_nans,\n     const bool include_batch_in_index) {\n   const int kThreadsPerBlock = 1024;\n   const int output_size = batch * channels * pooled_height * pooled_width;\n@@ -405,7 +405,7 @@ bool MaxPoolForwardWithOptionalArgmax<T>::operator()(\n template <typename T>\n bool MaxPoolBackwardWithArgmax<T>::operator()(\n     const int output_size, const int input_size, const T* top_diff,\n-    const int64* mask, const int top_offset, const int bottom_offset,\n+    const int64_t* mask, const int top_offset, const int bottom_offset,\n     T* bottom_diff, const Eigen::GpuDevice& d,\n     const bool include_batch_in_index) {\n   const int kThreadsPerBlock = 1024;\n@@ -454,7 +454,7 @@ bool MaxPoolGradBackwardNoMask<T>::operator()(\n template <typename T>\n bool MaxPoolGradBackwardWithArgmax<T>::operator()(\n     const int output_size, const int input_size, const T* top_diff,\n-    const int64* mask, const int top_offset, const int bottom_offset,\n+    const int64_t* mask, const int top_offset, const int bottom_offset,\n     T* bottom_diff, const Eigen::GpuDevice& d,\n     const bool include_batch_in_index) {\n   if (input_size == 0) return true;"
        },
        {
            "sha": "3e8ba784d9714e98bc99f65755de793bed7012a5",
            "filename": "tensorflow/core/kernels/maxpooling_op_gpu.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmaxpooling_op_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmaxpooling_op_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmaxpooling_op_gpu.h?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -44,11 +44,11 @@ struct MaxPoolForwardWithOptionalArgmax {\n };\n \n struct MaxPoolForwardNoMask_NCHW_VECT_C {\n-  bool operator()(const int32* bottom_data, const int batch, const int height,\n+  bool operator()(const int32_t* bottom_data, const int batch, const int height,\n                   const int width, int channels, const int pooled_height,\n                   const int pooled_width, const int kernel_h,\n                   const int kernel_w, const int stride_h, const int stride_w,\n-                  const int pad_t, const int pad_l, int32* top_data,\n+                  const int pad_t, const int pad_l, int32_t* top_data,\n                   const Eigen::GpuDevice& d);\n };\n "
        },
        {
            "sha": "9a76c85aba09c7ba6740874bca3a247d48a051fd",
            "filename": "tensorflow/core/kernels/multinomial_op_gpu.cu.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmultinomial_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fmultinomial_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmultinomial_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -39,8 +39,8 @@ using GPUDevice = Eigen::GpuDevice;\n // Kernel for Multinomial op.  Data is interpreted to have the following shapes:\n //   scores: [B, S, C];  maxima: [B, S];  output: [B, S].\n template <typename OutputType>\n-__global__ void MultinomialKernel(int32 nthreads, const int32 num_classes,\n-                                  const int32 num_samples,\n+__global__ void MultinomialKernel(int32_t nthreads, const int32_t num_classes,\n+                                  const int32_t num_samples,\n                                   const float* __restrict__ scores,\n                                   const float* __restrict__ maxima,\n                                   OutputType* __restrict__ output) {\n@@ -113,7 +113,7 @@ struct MultinomialFunctor<GPUDevice, T, OutputType> {\n     // Necessary for atomicMax() inside the kernel.\n     output.device(d) = output.constant(0LL);\n \n-    const int32 work_items = batch_size * num_samples * num_classes;\n+    const int32_t work_items = batch_size * num_samples * num_classes;\n     GpuLaunchConfig config = GetGpuLaunchConfig(work_items, d);\n     TF_CHECK_OK(GpuLaunchKernel(\n         MultinomialKernel<OutputType>, config.block_count,"
        },
        {
            "sha": "77eb070e628576168f05b5fd07a0cbf0888d48a7",
            "filename": "tensorflow/core/kernels/nccl_ops.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fnccl_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fnccl_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fnccl_ops.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -52,7 +52,7 @@ class NcclAsyncOpBase : public AsyncOpKernel {\n     OP_REQUIRES_OK(c, c->GetAttr(\"shared_name\", &collective_prefix_));\n   }\n \n-  string GetCollectiveKey(OpKernelContext* c) {\n+  std::string GetCollectiveKey(OpKernelContext* c) {\n     return strings::StrCat(collective_prefix_, \";\", c->step_id(), \";\",\n                            c->frame_iter().frame_id, \":\",\n                            c->frame_iter().iter_id);\n@@ -62,7 +62,7 @@ class NcclAsyncOpBase : public AsyncOpKernel {\n \n  private:\n   int num_devices_;\n-  string collective_prefix_;\n+  std::string collective_prefix_;\n \n   NcclAsyncOpBase(const NcclAsyncOpBase&) = delete;\n   void operator=(const NcclAsyncOpBase&) = delete;\n@@ -71,7 +71,7 @@ class NcclAsyncOpBase : public AsyncOpKernel {\n class NcclReduceOpBase : public NcclAsyncOpBase {\n  public:\n   explicit NcclReduceOpBase(OpKernelConstruction* c) : NcclAsyncOpBase(c) {\n-    string reduction;\n+    std::string reduction;\n     OP_REQUIRES_OK(c, c->GetAttr(\"reduction\", &reduction));\n     if (reduction == \"min\") {\n       reduction_op_ = ncclMin;\n@@ -106,7 +106,7 @@ class NcclAllReduceOpKernel : public NcclReduceOpBase {\n     OP_REQUIRES_OK_ASYNC(\n         c, c->forward_input_or_allocate_output({0}, 0, input->shape(), &output),\n         done);\n-    auto actual_done = [c, done](Status s) {\n+    auto actual_done = [c, done](absl::Status s) {\n       OP_REQUIRES_OK_ASYNC(c, s, done);\n       done();\n     };\n@@ -137,7 +137,7 @@ class NcclReduceSendKernel : public NcclReduceOpBase {\n       : NcclReduceOpBase(c) {}\n \n   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n-    auto actual_done = [c, done](Status s) {\n+    auto actual_done = [c, done](absl::Status s) {\n       OP_REQUIRES_OK_ASYNC(c, s, done);\n       done();\n     };\n@@ -173,7 +173,7 @@ class NcclReduceRecvKernel : public NcclReduceOpBase {\n     OP_REQUIRES_OK_ASYNC(c, c->allocate_output(0, input->shape(), &output),\n                          done);\n \n-    auto actual_done = [c, done](Status s) {\n+    auto actual_done = [c, done](absl::Status s) {\n       OP_REQUIRES_OK_ASYNC(c, s, done);\n       done();\n     };\n@@ -207,7 +207,7 @@ class NcclBroadcastSendKernel : public NcclAsyncOpBase {\n       : NcclAsyncOpBase(c) {}\n \n   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n-    auto actual_done = [c, done](Status s) {\n+    auto actual_done = [c, done](absl::Status s) {\n       OP_REQUIRES_OK_ASYNC(c, s, done);\n       done();\n     };\n@@ -239,11 +239,11 @@ class NcclBroadcastRecvKernel : public NcclAsyncOpBase {\n     const Tensor& shape_t = c->input(0);\n     TensorShape shape;\n     OP_REQUIRES_OK_ASYNC(\n-        c, TensorShapeUtils::MakeShape(shape_t.vec<int32>(), &shape), done);\n+        c, TensorShapeUtils::MakeShape(shape_t.vec<int32_t>(), &shape), done);\n     Tensor* output;\n     OP_REQUIRES_OK_ASYNC(c, c->allocate_output(0, shape, &output), done);\n \n-    auto actual_done = [c, done](Status s) {\n+    auto actual_done = [c, done](absl::Status s) {\n       OP_REQUIRES_OK_ASYNC(c, s, done);\n       done();\n     };"
        },
        {
            "sha": "09cb9b8d9388ea47c002eddd388ddf724954a305",
            "filename": "tensorflow/core/kernels/one_hot_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fone_hot_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fone_hot_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fone_hot_op_test.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -30,13 +30,13 @@ static Graph* OneHot(int batch_size, int num_classes, int axis) {\n   std::mt19937 gen(rd());\n   std::uniform_int_distribution<> dist(0, num_classes - 1);\n \n-  auto indices_t = indices.flat<int32>();\n+  auto indices_t = indices.flat<int32_t>();\n   for (int i = 0; i < batch_size; ++i) {\n     indices_t(i) = dist(gen);\n   }\n \n   Tensor depth(DT_INT32, TensorShape({}));\n-  depth.scalar<int32>()() = num_classes;\n+  depth.scalar<int32_t>()() = num_classes;\n \n   Tensor on_value(DT_FLOAT, TensorShape({}));\n   on_value.scalar<float>()() = 1.0f;"
        },
        {
            "sha": "f4c1db06bad96109bd8dc4306b63a6866d7935e7",
            "filename": "tensorflow/core/kernels/pack_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fpack_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fpack_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fpack_op.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -168,8 +168,8 @@ REGISTER_KERNEL_BUILDER(Name(\"Pack\")\n                             .Device(DEVICE_GPU)\n                             .HostMemory(\"values\")\n                             .HostMemory(\"output\")\n-                            .TypeConstraint<int32>(\"T\"),\n-                        PackOp<CPUDevice, int32>);\n+                            .TypeConstraint<int32_t>(\"T\"),\n+                        PackOp<CPUDevice, int32_t>);\n \n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n "
        },
        {
            "sha": "0fbb33816c8b144d6b0a9ef0cd2514b7f9267349",
            "filename": "tensorflow/core/kernels/parameterized_truncated_normal_op_gpu.cu.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fparameterized_truncated_normal_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fparameterized_truncated_normal_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fparameterized_truncated_normal_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -51,16 +51,16 @@ typedef Eigen::GpuDevice GPUDevice;\n template <typename T>\n \n __global__ void __launch_bounds__(1024)\n-    TruncatedNormalKernel(random::PhiloxRandom gen, T* data, int64 num_batches,\n-                          int64 samples_per_batch, int64 num_elements,\n-                          const T* __restrict__ means, bool single_mean,\n-                          const T* __restrict__ stddevs, bool single_stddev,\n-                          const T* __restrict__ minvals, bool single_minval,\n-                          const T* __restrict__ maxvals, bool single_maxval,\n-                          int64 kMaxIterations) {\n-  const int32 max_samples_per_item = 2 * kMaxIterations;\n+    TruncatedNormalKernel(random::PhiloxRandom gen, T* data,\n+                          int64_t num_batches, int64_t samples_per_batch,\n+                          int64_t num_elements, const T* __restrict__ means,\n+                          bool single_mean, const T* __restrict__ stddevs,\n+                          bool single_stddev, const T* __restrict__ minvals,\n+                          bool single_minval, const T* __restrict__ maxvals,\n+                          bool single_maxval, int64_t kMaxIterations) {\n+  const int32_t max_samples_per_item = 2 * kMaxIterations;\n   // Initial offset as given by GPU_1D_KERNEL_LOOP.\n-  const int32 initial_offset = blockIdx.x * blockDim.x + threadIdx.x;\n+  const int32_t initial_offset = blockIdx.x * blockDim.x + threadIdx.x;\n   gen.Skip(max_samples_per_item * initial_offset);\n   typedef random::UniformDistribution<random::PhiloxRandom, T> Uniform;\n   typedef random::NormalDistribution<random::PhiloxRandom, T> Normal;\n@@ -82,15 +82,15 @@ __global__ void __launch_bounds__(1024)\n   // skips max_samples_per_item in the generator. Then after generating this\n   // item, we need to skip the samples for one element for every thread to get\n   // to the next element that we actually process.\n-  const int32 samples_between_processed_elements =\n+  const int32_t samples_between_processed_elements =\n       max_samples_per_item * (gridDim.x * blockDim.x);\n \n   GPU_1D_KERNEL_LOOP(offset, num_elements) {\n     // Track how many more samples we need to skip before we process the next\n     // element.\n-    int32 remaining_samples = samples_between_processed_elements;\n+    int32_t remaining_samples = samples_between_processed_elements;\n \n-    const int64 batch_id = offset / samples_per_batch;\n+    const int64_t batch_id = offset / samples_per_batch;\n     T mean = means[single_mean ? 0 : batch_id];\n     const T input_stddev = stddevs[single_stddev ? 0 : batch_id];\n     T minval = minvals[single_minval ? 0 : batch_id];\n@@ -231,8 +231,8 @@ __global__ void __launch_bounds__(1024)\n // Partial specialization for GPU\n template <typename T>\n struct TruncatedNormalFunctor<GPUDevice, T> {\n-  void operator()(OpKernelContext* ctx, const GPUDevice& d, int64 num_batches,\n-                  int64 samples_per_batch, int64 num_elements,\n+  void operator()(OpKernelContext* ctx, const GPUDevice& d, int64_t num_batches,\n+                  int64_t samples_per_batch, int64_t num_elements,\n                   typename TTypes<T>::ConstFlat means,\n                   typename TTypes<T>::ConstFlat stddevs,\n                   typename TTypes<T>::ConstFlat minvals,"
        },
        {
            "sha": "7a891ddd63f2b3bff7af32dffd691aa8005c84a3",
            "filename": "tensorflow/core/kernels/pooling_ops_common_gpu.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_common_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_common_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_common_gpu.h?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -41,8 +41,8 @@ class DnnPoolingOp {\n   typedef GPUDevice Device;\n   static void Compute(OpKernelContext* context,\n                       se::dnn::PoolingMode pooling_mode,\n-                      const std::vector<int32>& size,\n-                      const std::vector<int32>& stride, Padding padding,\n+                      const std::vector<int32_t>& size,\n+                      const std::vector<int32_t>& stride, Padding padding,\n                       std::vector<int64_t> explicit_paddings,\n                       TensorFormat data_format, const Tensor& tensor_in,\n                       const TensorShape& tensor_out_shape, bool propagate_nans);\n@@ -57,8 +57,8 @@ class DnnPoolingGradOp {\n   typedef GPUDevice Device;\n   static void Compute(OpKernelContext* context,\n                       se::dnn::PoolingMode pooling_mode,\n-                      const std::vector<int32>& size,\n-                      const std::vector<int32>& stride, Padding padding,\n+                      const std::vector<int32_t>& size,\n+                      const std::vector<int32_t>& stride, Padding padding,\n                       std::vector<int64_t> explicit_paddings,\n                       TensorFormat data_format, const Tensor* tensor_in,\n                       const Tensor* tensor_out, const Tensor& out_backprop,"
        },
        {
            "sha": "7df72b3a8f0b84e6f1e1d3744b2785310f188c7c",
            "filename": "tensorflow/core/kernels/population_count_op_gpu.cu.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fpopulation_count_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fpopulation_count_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fpopulation_count_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -35,34 +35,34 @@ namespace functor {\n template <typename T>\n __global__ void PopulationCountKernel(const int size,\n                                       const T* __restrict__ input,\n-                                      uint8* __restrict__ output) {\n+                                      uint8_t* __restrict__ output) {\n   GPU_1D_KERNEL_LOOP(i, size) { output[i] = __popc(ldg(input + i)); }\n }\n \n template <>\n __global__ void PopulationCountKernel(const int size,\n-                                      const int8* __restrict__ input,\n-                                      uint8* __restrict__ output) {\n+                                      const int8_t* __restrict__ input,\n+                                      uint8_t* __restrict__ output) {\n   // For some reason, __popc on a negative int8 gets confused.\n   GPU_1D_KERNEL_LOOP(i, size) {\n-    output[i] = __popc(ldg(reinterpret_cast<const uint8*>(input + i)));\n+    output[i] = __popc(ldg(reinterpret_cast<const uint8_t*>(input + i)));\n   }\n }\n \n template <>\n __global__ void PopulationCountKernel(const int size,\n-                                      const int16* __restrict__ input,\n-                                      uint8* __restrict__ output) {\n+                                      const int16_t* __restrict__ input,\n+                                      uint8_t* __restrict__ output) {\n   // For some reason, __popc on a negative int16 gets confused.\n   GPU_1D_KERNEL_LOOP(i, size) {\n-    output[i] = __popc(ldg(reinterpret_cast<const uint16*>(input + i)));\n+    output[i] = __popc(ldg(reinterpret_cast<const uint16_t*>(input + i)));\n   }\n }\n \n template <>\n-__global__ void PopulationCountKernel<int64_t>(const int size,\n-                                               const int64* __restrict__ input,\n-                                               uint8* __restrict__ output) {\n+__global__ void PopulationCountKernel<int64_t>(\n+    const int size, const int64_t* __restrict__ input,\n+    uint8_t* __restrict__ output) {\n   GPU_1D_KERNEL_LOOP(i, size) { output[i] = __popcll(ldg(input + i)); }\n }\n "
        },
        {
            "sha": "e62b4cdf2db9d6edd0ca64edbc2d98686fe1c58d",
            "filename": "tensorflow/core/kernels/queue_base.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fqueue_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fqueue_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fqueue_base.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -51,7 +51,7 @@ absl::Status HandleSliceToElement(const Tensor& parent, Tensor* element,\n \n QueueBase::QueueBase(int32_t capacity, const DataTypeVector& component_dtypes,\n                      const std::vector<TensorShape>& component_shapes,\n-                     const string& name)\n+                     const std::string& name)\n     : capacity_(capacity),\n       component_dtypes_(component_dtypes),\n       component_shapes_(component_shapes),\n@@ -78,8 +78,9 @@ absl::Status QueueBase::ValidateTupleCommon(const Tuple& tuple) const {\n }\n \n // static\n-string QueueBase::ShapeListString(const absl::Span<const TensorShape>& shapes) {\n-  string result = \"[\";\n+std::string QueueBase::ShapeListString(\n+    const absl::Span<const TensorShape>& shapes) {\n+  std::string result = \"[\";\n   bool first = true;\n   for (const TensorShape& shape : shapes) {\n     absl::StrAppend(&result, first ? \"\" : \", \", shape.DebugString());\n@@ -90,7 +91,7 @@ string QueueBase::ShapeListString(const absl::Span<const TensorShape>& shapes) {\n }\n \n absl::Status QueueBase::MatchesNodeDefOp(const NodeDef& node_def,\n-                                         const string& op) const {\n+                                         const std::string& op) const {\n   if (node_def.op() != op) {\n     return errors::InvalidArgument(\"Shared queue '\", name_, \"' has type '\", op,\n                                    \"' that does not match type of Node '\","
        },
        {
            "sha": "9612e6bcdbabfbcb555e7b80895c3a6e2e768607",
            "filename": "tensorflow/core/kernels/ragged_cross_op.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fragged_cross_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fragged_cross_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_cross_op.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -51,7 +51,7 @@ class FeatureReader {\n   virtual int64_t FeatureCount(int64_t batch) const = 0;\n \n   // Copies the value for the specified feature to `out`.\n-  virtual void ReadValue(int64_t batch, int64_t n, uint64* out) const = 0;\n+  virtual void ReadValue(int64_t batch, int64_t n, uint64_t* out) const = 0;\n   virtual void ReadValue(int64_t batch, int64_t n, tstring* out) const = 0;\n \n   virtual ~FeatureReader() {}\n@@ -70,10 +70,10 @@ void CopyToString(const tstring& src, tstring* dst) {\n void CopyToString(int64_t src, tstring* dst) { *dst = std::to_string(src); }\n \n // Copies a feature value `src` to an int64 fingerprint `dst`.\n-void CopyToFingerprint(const tstring& feature, uint64* dst) {\n+void CopyToFingerprint(const tstring& feature, uint64_t* dst) {\n   *dst = Fingerprint64(feature);\n }\n-void CopyToFingerprint(int64_t feature, uint64* dst) { *dst = feature; }\n+void CopyToFingerprint(int64_t feature, uint64_t* dst) { *dst = feature; }\n \n // A FeatureReader that is backed by a ragged tensor.\n template <typename ValuesType, typename SplitsType>\n@@ -87,7 +87,7 @@ class RaggedFeatureReader : public FeatureReader {\n     return row_splits_(batch + 1) - row_splits_(batch);\n   }\n \n-  void ReadValue(int64_t batch, int64_t n, uint64* out) const override {\n+  void ReadValue(int64_t batch, int64_t n, uint64_t* out) const override {\n     CopyToFingerprint(values_(row_splits_(batch) + n), out);\n   }\n \n@@ -110,7 +110,7 @@ class DenseFeatureReader : public FeatureReader {\n \n   int64_t FeatureCount(int64_t batch) const override { return feature_count_; }\n \n-  void ReadValue(int64_t batch, int64_t n, uint64* out) const override {\n+  void ReadValue(int64_t batch, int64_t n, uint64_t* out) const override {\n     CopyToFingerprint(values_(batch, n), out);\n   }\n \n@@ -145,7 +145,7 @@ class SparseFeatureReader : public FeatureReader {\n     return row_splits_[batch + 1] - row_splits_[batch];\n   }\n \n-  void ReadValue(int64_t batch, int64_t n, uint64* out) const override {\n+  void ReadValue(int64_t batch, int64_t n, uint64_t* out) const override {\n     CopyToFingerprint(values_(row_splits_[batch] + n), out);\n   }\n \n@@ -179,7 +179,7 @@ class OutputWriterImpl : public OutputWriter {\n   using FlatSplits = typename TTypes<SplitsType>::ConstFlat;\n \n   OutputWriterImpl(const FeatureReaders& features, int64_t num_buckets,\n-                   uint64 hash_key, const Tensor* splits_out,\n+                   uint64_t hash_key, const Tensor* splits_out,\n                    Tensor* values_out)\n       : features_(features),\n         num_buckets_(num_buckets),\n@@ -220,9 +220,9 @@ class OutputWriterImpl : public OutputWriter {\n   void WriteCombination(int64_t batch_index,\n                         const std::vector<int>& combination, int64_t* out) {\n     // Do the fingerprint concatenation on uint64.\n-    uint64 hashed_output = hash_key_;\n+    uint64_t hashed_output = hash_key_;\n     for (size_t i = 0; i < combination.size(); ++i) {\n-      uint64 hash_i;\n+      uint64_t hash_i;\n       features_[i]->ReadValue(batch_index, combination[i], &hash_i);\n       hashed_output = FingerprintCat64(hashed_output, hash_i);\n     }\n@@ -254,7 +254,7 @@ class OutputWriterImpl : public OutputWriter {\n \n   const FeatureReaders& features_;\n   const int64_t num_buckets_;\n-  const uint64 hash_key_;\n+  const uint64_t hash_key_;\n   FlatSplits splits_out_;\n   FlatValues values_out_;\n };\n@@ -263,23 +263,23 @@ class OutputWriterImpl : public OutputWriter {\n // given tensors.\n std::unique_ptr<OutputWriter> MakeOutputWriter(const FeatureReaders& features,\n                                                int64_t num_buckets,\n-                                               uint64 hash_key,\n+                                               uint64_t hash_key,\n                                                const Tensor* splits_out,\n                                                Tensor* values_out) {\n   if (values_out->dtype() == DT_INT64) {\n     if (splits_out->dtype() == DT_INT64) {\n       return std::make_unique<OutputWriterImpl<int64_t, int64_t>>(\n           features, num_buckets, hash_key, splits_out, values_out);\n     } else {\n-      return std::make_unique<OutputWriterImpl<int64_t, int32>>(\n+      return std::make_unique<OutputWriterImpl<int64_t, int32_t>>(\n           features, num_buckets, hash_key, splits_out, values_out);\n     }\n   } else {\n     if (splits_out->dtype() == DT_INT64) {\n       return std::make_unique<OutputWriterImpl<tstring, int64_t>>(\n           features, num_buckets, hash_key, splits_out, values_out);\n     } else {\n-      return std::make_unique<OutputWriterImpl<tstring, int32>>(\n+      return std::make_unique<OutputWriterImpl<tstring, int32_t>>(\n           features, num_buckets, hash_key, splits_out, values_out);\n     }\n   }\n@@ -298,7 +298,7 @@ class RaggedCrossOp : public OpKernel {\n     // supported by REGISTER_OP.\n     int64_t signed_hash_key_;\n     OP_REQUIRES_OK(context, context->GetAttr(\"hash_key\", &signed_hash_key_));\n-    hash_key_ = static_cast<uint64>(signed_hash_key_);\n+    hash_key_ = static_cast<uint64_t>(signed_hash_key_);\n \n     int num_sparse;\n     OP_REQUIRES_OK(context, context->GetAttr(\"Nsparse\", &num_sparse));\n@@ -542,15 +542,15 @@ class RaggedCrossOp : public OpKernel {\n             new RaggedFeatureReader<int64_t, int64_t>(values, splits));\n       } else {\n         features->emplace_back(\n-            new RaggedFeatureReader<int64_t, int32>(values, splits));\n+            new RaggedFeatureReader<int64_t, int32_t>(values, splits));\n       }\n     } else {\n       if (splits.dtype() == DT_INT64) {\n         features->emplace_back(\n             new RaggedFeatureReader<tstring, int64_t>(values, splits));\n       } else {\n         features->emplace_back(\n-            new RaggedFeatureReader<tstring, int32>(values, splits));\n+            new RaggedFeatureReader<tstring, int32_t>(values, splits));\n       }\n     }\n     return absl::OkStatus();\n@@ -632,7 +632,7 @@ class RaggedCrossOp : public OpKernel {\n   }\n \n   int64_t num_buckets_;\n-  uint64 hash_key_;\n+  uint64_t hash_key_;\n   std::vector<DataType> ragged_values_types_;\n   std::vector<DataType> ragged_splits_types_;\n   std::vector<DataType> sparse_values_types_;\n@@ -642,8 +642,8 @@ class RaggedCrossOp : public OpKernel {\n \n REGISTER_KERNEL_BUILDER(Name(\"RaggedCross\")\n                             .Device(DEVICE_CPU)\n-                            .TypeConstraint<int32>(\"out_row_splits_type\"),\n-                        RaggedCrossOp<int32>);\n+                            .TypeConstraint<int32_t>(\"out_row_splits_type\"),\n+                        RaggedCrossOp<int32_t>);\n REGISTER_KERNEL_BUILDER(Name(\"RaggedCross\")\n                             .Device(DEVICE_CPU)\n                             .TypeConstraint<int64_t>(\"out_row_splits_type\"),"
        },
        {
            "sha": "cebccdd360f2d4ad5d1c0bf32f678695d0d28a6c",
            "filename": "tensorflow/core/kernels/ragged_gather_op_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fragged_gather_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fragged_gather_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_gather_op_test.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -65,7 +65,7 @@ TEST_F(RaggedGatherOpTest, RaggedGather) {\n   // indices = [2, 1, 0, 3]\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n   // params.shape = [4, None]\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({4}),                     // indices.shape\n       {2, 1, 0, 3},                         // indices\n       {{0, 3, 3, 7, 9}},                    // params_nested_splits\n@@ -87,7 +87,7 @@ TEST_F(RaggedGatherOpTest, RaggedGather_3DParams) {\n   // indices = [2, 1, 0, 2, 3]\n   // params = [[[]], [[.1, 2], [.3]], [], [[.4, .5], [.6, .7, .8]], [[.9]]]\n   // params.shape = [5, None, None]\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({5}),                             // indices.shape\n       {2, 1, 0, 2, 3},                              // indices\n       {{0, 1, 3, 3, 5, 6}, {0, 0, 2, 3, 5, 8, 9}},  // params_nested_splits\n@@ -111,7 +111,7 @@ TEST_F(RaggedGatherOpTest, RaggedGather_4DParams) {\n   // indices = [2, 1, 0, 2]\n   // params = [[[]], [[[1, 2], [3, 4], [5, 6]], [[7, 8]]], []]\n   // params.shape = [4, None, None, 2]\n-  BuildRaggedGatherGraph<int32, int32>(\n+  BuildRaggedGatherGraph<int32_t, int32_t>(\n       TensorShape({4}),              // indices.shape\n       {2, 1, 0, 2},                  // indices\n       {{0, 1, 3, 3}, {0, 0, 3, 4}},  // params_nested_splits\n@@ -129,15 +129,15 @@ TEST_F(RaggedGatherOpTest, RaggedGather_4DParams) {\n                                    test::AsTensor<int64_t>({0, 0, 2, 3, 3}));\n   test::ExpectTensorEqual<int64_t>(*GetOutput(1),\n                                    test::AsTensor<int64_t>({0, 3, 4, 4}));\n-  test::ExpectTensorEqual<int32>(\n+  test::ExpectTensorEqual<int32_t>(\n       *GetOutput(2),\n-      test::AsTensor<int32>({1, 2, 3, 4, 5, 6, 7, 8}, TensorShape({4, 2})));\n+      test::AsTensor<int32_t>({1, 2, 3, 4, 5, 6, 7, 8}, TensorShape({4, 2})));\n }\n \n TEST_F(RaggedGatherOpTest, RaggedGather_2DIndices) {\n   // indices = [[2, 1], [0, 3]]\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({2, 2}),                  // indices.shape\n       {2, 1, 0, 3},                         // indices\n       {{0, 3, 3, 7, 9}},                    // params_nested_splits\n@@ -161,7 +161,7 @@ TEST_F(RaggedGatherOpTest, RaggedGather_2DIndices) {\n TEST_F(RaggedGatherOpTest, RaggedGather_ScalarIndices) {\n   // indices = 2\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({}),                      // indices.shape\n       {2},                                  // indices\n       {{0, 3, 3, 7, 9}},                    // params_nested_splits\n@@ -178,7 +178,7 @@ TEST_F(RaggedGatherOpTest, RaggedGather_ScalarIndices) {\n TEST_F(RaggedGatherOpTest, RaggedGather_OutOfBounds) {\n   // indices = [2, 10]\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({2}),                     // indices.shape\n       {2, 10},                              // indices\n       {{0, 3, 3, 7, 9}},                    // params_nested_splits\n@@ -189,7 +189,7 @@ TEST_F(RaggedGatherOpTest, RaggedGather_OutOfBounds) {\n }\n \n TEST_F(RaggedGatherOpTest, InvalidSplitsNotSorted) {\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({2}),                     // indices.shape\n       {0, 2},                               // indices\n       {{0, 3, 5, 2, 9}},                    // params_nested_splits\n@@ -200,7 +200,7 @@ TEST_F(RaggedGatherOpTest, InvalidSplitsNotSorted) {\n }\n \n TEST_F(RaggedGatherOpTest, InvalidSplitsNegative) {\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({2}),                     // indices.shape\n       {0, 2},                               // indices\n       {{-1, 3, 2, 7, 9}},                   // params_nested_splits\n@@ -211,7 +211,7 @@ TEST_F(RaggedGatherOpTest, InvalidSplitsNegative) {\n }\n \n TEST_F(RaggedGatherOpTest, InvalidSplitsEmpty) {\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({0}),  // indices.shape\n       {},                // indices\n       {{}},              // params_nested_splits\n@@ -222,7 +222,7 @@ TEST_F(RaggedGatherOpTest, InvalidSplitsEmpty) {\n }\n \n TEST_F(RaggedGatherOpTest, InvalidSplitsTooBig) {\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({2}),                     // indices.shape\n       {0, 2},                               // indices\n       {{0, 20, 40, 80, 100}},               // params_nested_splits\n@@ -234,7 +234,7 @@ TEST_F(RaggedGatherOpTest, InvalidSplitsTooBig) {\n }\n \n TEST_F(RaggedGatherOpTest, BadValuesShape) {\n-  BuildRaggedGatherGraph<float, int32>(\n+  BuildRaggedGatherGraph<float, int32_t>(\n       TensorShape({0}),  // indices.shape\n       {},                // indices\n       {{0}},             // params_nested_splits"
        },
        {
            "sha": "dbb66c2148397da8dd8c22c1c6ddd1a6f91ff12d",
            "filename": "tensorflow/core/kernels/random_op_gpu.h",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Frandom_op_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Frandom_op_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_op_gpu.h?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -34,17 +34,17 @@ struct FillPhiloxRandomKernel;\n template <class Distribution>\n struct FillPhiloxRandomKernel<Distribution, false> {\n   typedef typename Distribution::ResultElementType T;\n-  PHILOX_DEVICE_INLINE void Run(const uint64* key, const uint64* counter,\n-                                random::PhiloxRandom gen, T* data, int64 size,\n+  PHILOX_DEVICE_INLINE void Run(const uint64_t* key, const uint64_t* counter,\n+                                random::PhiloxRandom gen, T* data, int64_t size,\n                                 Distribution dist);\n };\n \n template <class Distribution>\n struct FillPhiloxRandomKernel<Distribution, true> {\n   typedef typename Distribution::ResultElementType T;\n-  PHILOX_DEVICE_INLINE void Run(const uint64* key, const uint64* counter,\n+  PHILOX_DEVICE_INLINE void Run(const uint64_t* key, const uint64_t* counter,\n                                 random::PhiloxRandom base_gen, T* data,\n-                                int64 size, Distribution dist);\n+                                int64_t size, Distribution dist);\n };\n \n template <typename T, int ElementCount>\n@@ -83,14 +83,14 @@ class SampleCopier<float, 4> {\n };\n \n template <>\n-class SampleCopier<int32, 4> {\n+class SampleCopier<int32_t, 4> {\n  public:\n   // Copies the elements from the array to buf. buf must be 128-bit aligned,\n   // which is true for tensor data, and all offsets that are a multiple of the\n   // vector size (because the vectors are 128 bits long).\n   inline __device__ void operator()(\n-      int32* __restrict__ buf,\n-      const tensorflow::random::Array<int32, 4>& array) const {\n+      int32_t* __restrict__ buf,\n+      const tensorflow::random::Array<int32_t, 4>& array) const {\n     ::int4 vec;\n     vec.x = array[0];\n     vec.y = array[1];\n@@ -119,14 +119,14 @@ class SampleCopier<double, 2> {\n };\n \n template <>\n-class SampleCopier<int64, 2> {\n+class SampleCopier<int64_t, 2> {\n  public:\n   // Copies the elements from the array to buf. buf must be 128-bit aligned,\n   // which is true for tensor data, and all offsets that are a multiple of the\n   // vector size (because the vectors are 128 bits long).\n   inline __device__ void operator()(\n-      int64* __restrict__ buf,\n-      const tensorflow::random::Array<int64, 2>& array) const {\n+      int64_t* __restrict__ buf,\n+      const tensorflow::random::Array<int64_t, 2>& array) const {\n     longlong2 vec;\n     vec.x = array[0];\n     vec.y = array[1];\n@@ -139,13 +139,13 @@ class SampleCopier<int64, 2> {\n // distribution. Each output takes a fixed number of samples.\n template <class Distribution>\n PHILOX_DEVICE_INLINE void FillPhiloxRandomKernel<Distribution, false>::Run(\n-    const uint64* key, const uint64* counter, random::PhiloxRandom gen, T* data,\n-    int64 size, Distribution dist) {\n+    const uint64_t* key, const uint64_t* counter, random::PhiloxRandom gen,\n+    T* data, int64_t size, Distribution dist) {\n   const int kGroupSize = Distribution::kResultElementCount;\n \n-  const int32 thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n-  const int32 total_thread_count = gridDim.x * blockDim.x;\n-  int64 offset = thread_id * kGroupSize;\n+  const int32_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n+  const int32_t total_thread_count = gridDim.x * blockDim.x;\n+  int64_t offset = thread_id * kGroupSize;\n   if (key != nullptr && counter != nullptr) {\n     gen = GetPhiloxRandomFromCounterKeyMem(counter, key);\n   }\n@@ -174,8 +174,8 @@ PHILOX_DEVICE_INLINE void FillPhiloxRandomKernel<Distribution, false>::Run(\n // distribution. Each output takes a variable number of samples.\n template <class Distribution>\n PHILOX_DEVICE_INLINE void FillPhiloxRandomKernel<Distribution, true>::Run(\n-    const uint64* key, const uint64* counter, random::PhiloxRandom base_gen,\n-    T* data, int64 size, Distribution dist) {\n+    const uint64_t* key, const uint64_t* counter, random::PhiloxRandom base_gen,\n+    T* data, int64_t size, Distribution dist) {\n   if (key != nullptr && counter != nullptr) {\n     base_gen = GetPhiloxRandomFromCounterKeyMem(counter, key);\n   }\n@@ -189,10 +189,10 @@ PHILOX_DEVICE_INLINE void FillPhiloxRandomKernel<Distribution, true>::Run(\n                                            kReservedSamplesPerOutput /\n                                            PhiloxRandom::kResultElementCount;\n \n-  const int32 thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n-  const int32 total_thread_count = gridDim.x * blockDim.x;\n-  int64 group_index = thread_id;\n-  int64 offset = group_index * kGroupSize;\n+  const int32_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n+  const int32_t total_thread_count = gridDim.x * blockDim.x;\n+  int64_t group_index = thread_id;\n+  int64_t offset = group_index * kGroupSize;\n \n   while (offset < size) {\n     // Since each output takes a variable number of samples, we need to\n@@ -219,10 +219,10 @@ PHILOX_DEVICE_INLINE void FillPhiloxRandomKernel<Distribution, true>::Run(\n // A simple launch pad to call the correct function templates to fill the data\n template <class Distribution>\n __global__ void __launch_bounds__(1024)\n-    FillPhiloxRandomKernelLaunch(const uint64* key, const uint64* counter,\n+    FillPhiloxRandomKernelLaunch(const uint64_t* key, const uint64_t* counter,\n                                  random::PhiloxRandom base_gen,\n                                  typename Distribution::ResultElementType* data,\n-                                 int64 size, Distribution dist) {\n+                                 int64_t size, Distribution dist) {\n   FillPhiloxRandomKernel<Distribution,\n                          Distribution::kVariableSamplesPerOutput>()\n       .Run(key, counter, base_gen, data, size, dist);\n@@ -231,13 +231,13 @@ __global__ void __launch_bounds__(1024)\n // Partial specialization for GPU\n template <class Distribution>\n void FillPhiloxRandom<GPUDevice, Distribution>::operator()(\n-    OpKernelContext*, const GPUDevice& d, const uint64* key,\n-    const uint64* counter, random::PhiloxRandom gen,\n-    typename Distribution::ResultElementType* data, int64 size,\n+    OpKernelContext*, const GPUDevice& d, const uint64_t* key,\n+    const uint64_t* counter, random::PhiloxRandom gen,\n+    typename Distribution::ResultElementType* data, int64_t size,\n     Distribution dist) {\n   if (size == 0) return;\n-  const int32 block_size = d.maxGpuThreadsPerBlock();\n-  const int32 num_blocks =\n+  const int32_t block_size = d.maxGpuThreadsPerBlock();\n+  const int32_t num_blocks =\n       std::min<int64_t>(\n           d.getNumGpuMultiProcessors() * d.maxGpuThreadsPerMultiProcessor(),\n           size + block_size - 1) /"
        },
        {
            "sha": "b6a1ee9e57515a91aa3da9cb38c851058a30c35a",
            "filename": "tensorflow/core/kernels/relu_op_gpu.cu.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Frelu_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Frelu_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frelu_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -44,10 +44,10 @@ namespace functor {\n __global__ void ReluGradHalfKernel(const Eigen::half* __restrict__ gradient,\n                                    const Eigen::half* __restrict__ feature,\n                                    Eigen::half* __restrict__ backprop,\n-                                   int32 count) {\n-  int32 half2_count = count >> 1;\n-  int32 index = blockIdx.x * blockDim.x + threadIdx.x;\n-  const int32 total_device_threads = gridDim.x * blockDim.x;\n+                                   int32_t count) {\n+  int32_t half2_count = count >> 1;\n+  int32_t index = blockIdx.x * blockDim.x + threadIdx.x;\n+  const int32_t total_device_threads = gridDim.x * blockDim.x;\n \n   while (index < half2_count) {\n     // The fast branch.\n@@ -97,9 +97,9 @@ __global__ void ReluGradHalfKernel(const Eigen::half* __restrict__ gradient,\n __global__ void ReluGradHalfKernelVector(\n     const Eigen::half* __restrict__ gradient,\n     const Eigen::half* __restrict__ feature, Eigen::half* __restrict__ backprop,\n-    int32 count) {\n-  int32 half8_count = count / VectorSizeElements;\n-  int32 index = blockIdx.x * blockDim.x + threadIdx.x;\n+    int32_t count) {\n+  int32_t half8_count = count / VectorSizeElements;\n+  int32_t index = blockIdx.x * blockDim.x + threadIdx.x;\n \n   if (index < half8_count) {\n     // Cast to xx_h8 for vector load and store.\n@@ -174,17 +174,17 @@ struct ReluGrad<Device, Eigen::half> {\n     auto backprop_ptr = reinterpret_cast<uintptr_t>(backprop.data());\n     bool aligned = gradient_ptr % 16 == 0 && feature_ptr % 16 == 0 &&\n                    backprop_ptr % 16 == 0;\n-    int32 count = gradient.size();\n-    constexpr int32 kThreadInBlock = 512;\n+    int32_t count = gradient.size();\n+    constexpr int32_t kThreadInBlock = 512;\n     if (count == 0) return;\n     if (aligned) {\n-      int32 half8_count = Eigen::divup(count, VectorSizeElements);\n-      int32 kBlock = Eigen::divup(half8_count, kThreadInBlock);\n+      int32_t half8_count = Eigen::divup(count, VectorSizeElements);\n+      int32_t kBlock = Eigen::divup(half8_count, kThreadInBlock);\n       TF_CHECK_OK(GpuLaunchKernel(\n           ReluGradHalfKernelVector, kBlock, kThreadInBlock, 0, d.stream(),\n           gradient.data(), feature.data(), backprop.data(), count));\n     } else {\n-      int32 half2_count = Eigen::divup(count, 2);\n+      int32_t half2_count = Eigen::divup(count, 2);\n       GpuLaunchConfig config = GetGpuLaunchConfigFixedBlockSize(\n           half2_count, d, ReluGradHalfKernel, 0, kThreadInBlock);\n       TF_CHECK_OK(GpuLaunchKernel(\n@@ -195,8 +195,8 @@ struct ReluGrad<Device, Eigen::half> {\n };\n \n __global__ void Relu_int8x4_kernel(int vect_count,\n-                                   const int32* __restrict__ input,\n-                                   int32* __restrict__ output) {\n+                                   const int32_t* __restrict__ input,\n+                                   int32_t* __restrict__ output) {\n   CUDA_1D_KERNEL_LOOP(index, vect_count) {\n #if GOOGLE_CUDA\n     output[index] = __vmaxs4(input[index], 0);\n@@ -221,17 +221,17 @@ struct Relu<Device, qint8> {\n   // 'output' should have the same size as 'input'.\n   void operator()(const Device& d, typename TTypes<qint8>::ConstTensor input,\n                   typename TTypes<qint8>::Tensor output) {\n-    int32 count = input.size();\n+    int32_t count = input.size();\n     if (count == 0) return;\n \n-    int32 vect_count = Eigen::divup(count, 4);\n-    constexpr int32 kThreadInBlock = 512;\n+    int32_t vect_count = Eigen::divup(count, 4);\n+    constexpr int32_t kThreadInBlock = 512;\n     GpuLaunchConfig config = GetGpuLaunchConfigFixedBlockSize(\n         vect_count, d, Relu_int8x4_kernel, 0, kThreadInBlock);\n     TF_CHECK_OK(GpuLaunchKernel(\n         Relu_int8x4_kernel, config.block_count, config.thread_per_block, 0,\n-        d.stream(), vect_count, reinterpret_cast<const int32*>(input.data()),\n-        reinterpret_cast<int32*>(output.data())));\n+        d.stream(), vect_count, reinterpret_cast<const int32_t*>(input.data()),\n+        reinterpret_cast<int32_t*>(output.data())));\n   }\n };\n "
        },
        {
            "sha": "f3c48ef42c9ae4905e0d0f1588852cbcd62d07fc",
            "filename": "tensorflow/core/kernels/reshape_util_gpu.cu.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Freshape_util_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Freshape_util_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freshape_util_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -36,7 +36,7 @@ __global__ void ReshapeSparseTensorKernel(\n   GPU_1D_KERNEL_LOOP(sparse_index, nnz) {\n     const Tindex* input_index = &input_indices[sparse_index * input_rank];\n     Tindex* output_index = &output_indices[sparse_index * output_rank];\n-    int64 dense_index = 0;  // int64 to avoid overflow if Tindex is int32\n+    int64_t dense_index = 0;  // int64 to avoid overflow if Tindex is int32\n     // Flatten input index from slowest- to fastest-changing dimension.\n     for (int i = 0; i < input_rank; ++i) {\n       dense_index = dense_index * input_shape[i] + input_index[i];\n@@ -55,14 +55,14 @@ __global__ void ReshapeSparseTensorKernel(\n namespace functor {\n \n template <>\n-Status ReshapeSparseTensorFunctor<GPUDevice>::operator()(\n+absl::Status ReshapeSparseTensorFunctor<GPUDevice>::operator()(\n     OpKernelContext* context, const TensorShape& input_shape,\n     const TensorShape& output_shape,\n     typename TTypes<int64_t>::ConstMatrix input_indices,\n     typename TTypes<int64_t>::Matrix output_indices) const {\n-  const int64 input_rank = input_shape.dims();\n-  const int64 output_rank = output_shape.dims();\n-  const int64 nnz = input_indices.dimension(0);\n+  const int64_t input_rank = input_shape.dims();\n+  const int64_t output_rank = output_shape.dims();\n+  const int64_t nnz = input_indices.dimension(0);\n   // We copy input_shape and output_shape to the GPU and then launch a kernel\n   // to compute output_indices.\n   Tensor input_shape_gpu_t;\n@@ -75,16 +75,16 @@ Status ReshapeSparseTensorFunctor<GPUDevice>::operator()(\n   auto output_shape_gpu = output_shape_gpu_t.flat<int64_t>();\n   se::Stream* stream = context->op_device_context()->stream();\n   if (!stream) return errors::Internal(\"No GPU stream available.\");\n-  se::DeviceMemoryBase input_shape_gpu_mem(input_shape_gpu.data(),\n-                                           input_rank * sizeof(int64));\n+  stream_executor::DeviceAddressBase input_shape_gpu_mem(\n+      input_shape_gpu.data(), input_rank * sizeof(int64_t));\n   TF_RETURN_IF_ERROR(stream->Memcpy(&input_shape_gpu_mem,\n                                     input_shape.dim_sizes().data(),\n-                                    input_rank * sizeof(int64)));\n-  se::DeviceMemoryBase output_shape_gpu_mem(output_shape_gpu.data(),\n-                                            output_rank * sizeof(int64));\n+                                    input_rank * sizeof(int64_t)));\n+  stream_executor::DeviceAddressBase output_shape_gpu_mem(\n+      output_shape_gpu.data(), output_rank * sizeof(int64_t));\n   TF_RETURN_IF_ERROR(stream->Memcpy(&output_shape_gpu_mem,\n                                     output_shape.dim_sizes().data(),\n-                                    output_rank * sizeof(int64)));\n+                                    output_rank * sizeof(int64_t)));\n   const GPUDevice& device = context->template eigen_device<GPUDevice>();\n   auto config = GetGpuLaunchConfig(nnz, device);\n   return GpuLaunchKernel(ReshapeSparseTensorKernel<int64_t>, config.block_count,"
        },
        {
            "sha": "0a66a0f31d436601b9b4afbd5e0fcc9a684d9636",
            "filename": "tensorflow/core/kernels/restore_v2_op_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Frestore_v2_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Frestore_v2_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frestore_v2_op_test.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -61,9 +61,9 @@ class RestoreV2OpTest : public OpsTestBase {\n   }\n \n   void RunTest(absl::string_view save_op_to_use) {\n-    const string filename =\n+    const std::string filename =\n         io::JoinPath(testing::TmpDir(), \"tensor_simple-\", save_op_to_use);\n-    const std::vector<string> tensor_names = {\n+    const std::vector<std::string> tensor_names = {\n         \"tensor_bool\",  \"tensor_int\",    \"tensor_float\",     \"tensor_double\",\n         \"tensor_qint8\", \"tensor_qint32\", \"tensor_uint8\",     \"tensor_int8\",\n         \"tensor_int16\", \"tensor_int64\",  \"tensor_complex64\", \"tensor_half\"};\n@@ -114,12 +114,12 @@ class RestoreV2OpTest : public OpsTestBase {\n       // Input #1 is the tensor names\n       Tensor input_1 = MakeInput<tstring>(\n           TensorShape({static_cast<int>(tensor_names.size())}),\n-          [&tensor_names](int x) -> string { return tensor_names[x]; });\n+          [&tensor_names](int x) -> std::string { return tensor_names[x]; });\n       inputs.push_back({nullptr, &input_1});\n \n       Tensor shape_and_slices = MakeInput<tstring>(\n           TensorShape({static_cast<int>(tensor_names.size())}),\n-          [](int x) -> string { return \"\" /* saves in full */; });\n+          [](int x) -> std::string { return \"\" /* saves in full */; });\n       if (save_op_to_use != \"Save\") {\n         inputs.push_back({nullptr, &shape_and_slices});\n       }\n@@ -129,8 +129,8 @@ class RestoreV2OpTest : public OpsTestBase {\n                                        [](int x) -> bool { return x != 0; });\n       inputs.push_back({nullptr, &input_2});\n       // Input #3 is a 1-d integer tensor\n-      Tensor input_3 = MakeInput<int32>(TensorShape({10}),\n-                                        [](int x) -> int32 { return x + 1; });\n+      Tensor input_3 = MakeInput<int32_t>(\n+          TensorShape({10}), [](int x) -> int32_t { return x + 1; });\n       inputs.push_back({nullptr, &input_3});\n       // Input #4 is a 2-d float tensor\n       Tensor input_4 = MakeInput<float>(\n@@ -154,20 +154,20 @@ class RestoreV2OpTest : public OpsTestBase {\n           });\n       inputs.push_back({nullptr, &input_7});\n       // Input #8 is a 1-d uint8 tensor\n-      Tensor input_8 = MakeInput<uint8>(TensorShape({11}),\n-                                        [](int x) -> uint8 { return x + 1; });\n+      Tensor input_8 = MakeInput<uint8_t>(\n+          TensorShape({11}), [](int x) -> uint8_t { return x + 1; });\n       inputs.push_back({nullptr, &input_8});\n       // Input #9 is a 1-d int8 tensor\n-      Tensor input_9 = MakeInput<int8>(TensorShape({7}),\n-                                       [](int x) -> int8 { return x - 7; });\n+      Tensor input_9 = MakeInput<int8_t>(TensorShape({7}),\n+                                         [](int x) -> int8_t { return x - 7; });\n       inputs.push_back({nullptr, &input_9});\n       // Input #10 is a 1-d int16 tensor\n-      Tensor input_10 = MakeInput<int16>(TensorShape({7}),\n-                                         [](int x) -> int16 { return x - 8; });\n+      Tensor input_10 = MakeInput<int16_t>(\n+          TensorShape({7}), [](int x) -> int16_t { return x - 8; });\n       inputs.push_back({nullptr, &input_10});\n       // Input #11 is a 1-d int64 tensor\n       Tensor input_11 = MakeInput<int64_t>(\n-          TensorShape({9}), [](int x) -> int64 { return x - 9; });\n+          TensorShape({9}), [](int x) -> int64_t { return x - 9; });\n       inputs.push_back({nullptr, &input_11});\n       // Input #12 is a 1-d complex64 tensor\n       Tensor input_13 = MakeInput<complex64>(\n@@ -222,7 +222,7 @@ class RestoreV2OpTest : public OpsTestBase {\n       TensorShape expected({10});\n       EXPECT_TRUE(output->shape().IsSameSize(expected));\n       for (int i = 0; i < 10; ++i) {\n-        EXPECT_EQ(i + 1, output->flat<int32>()(i));\n+        EXPECT_EQ(i + 1, output->flat<int32_t>()(i));\n       }\n     }\n     // The 2-d float tensor\n@@ -283,7 +283,7 @@ class RestoreV2OpTest : public OpsTestBase {\n       TensorShape expected({11});\n       EXPECT_TRUE(output->shape().IsSameSize(expected));\n       for (int i = 0; i < 11; ++i) {\n-        EXPECT_EQ(i + 1, output->flat<uint8>()(i));\n+        EXPECT_EQ(i + 1, output->flat<uint8_t>()(i));\n       }\n     }\n     // The 1-d int8 tensor\n@@ -295,7 +295,7 @@ class RestoreV2OpTest : public OpsTestBase {\n       TensorShape expected({7});\n       EXPECT_TRUE(output->shape().IsSameSize(expected));\n       for (int i = 0; i < 7; ++i) {\n-        EXPECT_EQ(i - 7, output->flat<int8>()(i));\n+        EXPECT_EQ(i - 7, output->flat<int8_t>()(i));\n       }\n     }\n     // The 1-d int16 tensor\n@@ -307,7 +307,7 @@ class RestoreV2OpTest : public OpsTestBase {\n       TensorShape expected({7});\n       EXPECT_TRUE(output->shape().IsSameSize(expected));\n       for (int i = 0; i < 7; ++i) {\n-        EXPECT_EQ(i - 8, output->flat<int16>()(i));\n+        EXPECT_EQ(i - 8, output->flat<int16_t>()(i));\n       }\n     }\n     // The 1-d int64 tensor"
        },
        {
            "sha": "130bdd206b67fd39148fa51cc98b3e2fe90a9291",
            "filename": "tensorflow/core/kernels/roll_op_gpu.cu.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Froll_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Froll_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Froll_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -30,15 +30,15 @@ typedef Eigen::GpuDevice GPUDevice;\n namespace {\n \n template <typename T>\n-__global__ void RollKernel(const int32 nthreads, const int32 num_dims,\n+__global__ void RollKernel(const int32_t nthreads, const int32_t num_dims,\n                            const T* __restrict__ input, T* __restrict__ output,\n-                           const int32* __restrict__ dim_size,\n-                           const int32* __restrict__ threshold,\n-                           const int64* __restrict__ dim_range) {\n+                           const int32_t* __restrict__ dim_size,\n+                           const int32_t* __restrict__ threshold,\n+                           const int64_t* __restrict__ dim_range) {\n   CUDA_1D_KERNEL_LOOP(out_idx, nthreads) {\n-    int64 offset = 0;\n+    int64_t offset = 0;\n     for (int i = 0; i < num_dims; i++) {\n-      const int64 stride = dim_range[i] / dim_size[i];\n+      const int64_t stride = dim_range[i] / dim_size[i];\n       const int shift = dim_size[i] - threshold[i];\n       const int indx = (out_idx / stride) % dim_size[i];\n       const int shifted_indx = (indx + shift) % dim_size[i];\n@@ -53,21 +53,22 @@ namespace functor {\n \n template <typename T>\n struct Roll<GPUDevice, T> {\n-  void operator()(const OpKernelContext* context, const int64 num_elements,\n-                  const int num_dims, const gtl::ArraySlice<int32> dim_size,\n+  void operator()(const OpKernelContext* context, const int64_t num_elements,\n+                  const int num_dims, const absl::Span<const int32> dim_size,\n                   const T* input, T* output,\n-                  const gtl::ArraySlice<int32> threshold,\n-                  const gtl::ArraySlice<int64_t> dim_range, const int64 isd) {\n+                  const absl::Span<const int32> threshold,\n+                  const absl::Span<const int64_t> dim_range,\n+                  const int64_t isd) {\n     if (!num_elements) return;\n     const GPUDevice& d = context->eigen_device<GPUDevice>();\n \n-    auto dim_bytes = sizeof(int32) * dim_size.size();\n+    auto dim_bytes = sizeof(int32_t) * dim_size.size();\n     auto dim_buf = d.allocate(dim_bytes);\n \n-    auto thres_bytes = sizeof(int32) * threshold.size();\n+    auto thres_bytes = sizeof(int32_t) * threshold.size();\n     auto thres_buf = d.allocate(thres_bytes);\n \n-    auto range_bytes = sizeof(int64) * dim_range.size();\n+    auto range_bytes = sizeof(int64_t) * dim_range.size();\n     auto range_buf = d.allocate(range_bytes);\n \n     d.memcpyHostToDevice(dim_buf, dim_size.data(), dim_bytes);\n@@ -76,12 +77,12 @@ struct Roll<GPUDevice, T> {\n \n     GpuLaunchConfig cfg = GetGpuLaunchConfig(num_elements, d);\n \n-    TF_CHECK_OK(GpuLaunchKernel(RollKernel<T>, cfg.block_count,\n-                                cfg.thread_per_block, 0, d.stream(),\n-                                cfg.virtual_thread_count, num_dims, input,\n-                                output, reinterpret_cast<const int32*>(dim_buf),\n-                                reinterpret_cast<const int32*>(thres_buf),\n-                                reinterpret_cast<const int64*>(range_buf)));\n+    TF_CHECK_OK(\n+        GpuLaunchKernel(RollKernel<T>, cfg.block_count, cfg.thread_per_block, 0,\n+                        d.stream(), cfg.virtual_thread_count, num_dims, input,\n+                        output, reinterpret_cast<const int32_t*>(dim_buf),\n+                        reinterpret_cast<const int32_t*>(thres_buf),\n+                        reinterpret_cast<const int64_t*>(range_buf)));\n \n     d.deallocate(dim_buf);\n     d.deallocate(thres_buf);"
        },
        {
            "sha": "f9dac8363f8f37532e091d72bfe4b661a6ab8dc8",
            "filename": "tensorflow/core/kernels/scan_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscan_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscan_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fscan_ops.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -104,7 +104,7 @@ namespace functor {\n   DECLARE(Eigen::internal::ProdReducer<T>, T);\n \n TF_CALL_GPU_NUMBER_TYPES(DECLARE_FOR_ALL_REDUCERS);\n-DECLARE_FOR_ALL_REDUCERS(int32);\n+DECLARE_FOR_ALL_REDUCERS(int32_t);\n DECLARE_FOR_ALL_REDUCERS(int64_t);\n #undef DECLARE_FOR_ALL_REDUCERS\n \n@@ -151,7 +151,7 @@ TF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n           .HostMemory(\"axis\"),                                           \\\n       ScanOp<GPUDevice, type, Eigen::internal::SumReducer<type>, int64>)\n TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU_KERNELS);\n-REGISTER_GPU_KERNELS(int32);\n+REGISTER_GPU_KERNELS(int32_t);\n REGISTER_GPU_KERNELS(int64_t);\n #undef REGISTER_GPU_KERNELS\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n@@ -190,7 +190,7 @@ TF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n           .HostMemory(\"axis\"),                                            \\\n       ScanOp<GPUDevice, type, Eigen::internal::ProdReducer<type>, int64>)\n TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU_KERNELS);\n-REGISTER_GPU_KERNELS(int32);\n+REGISTER_GPU_KERNELS(int32_t);\n REGISTER_GPU_KERNELS(int64_t);\n #undef REGISTER_GPU_KERNELS\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
        },
        {
            "sha": "e4f43d51b46075404c0555dea141cd6ec02b0aa4",
            "filename": "tensorflow/core/kernels/scatter_functor_gpu.cu.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscatter_functor_gpu.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscatter_functor_gpu.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fscatter_functor_gpu.cu.h?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -97,7 +97,7 @@ __global__ void ScatterOpCustomKernel(T* __restrict__ params,\n       // Ignore indices that are out of range.\n       continue;\n     }\n-    int64 params_i = param_first_index * update_block + (i % update_block);\n+    int64_t params_i = param_first_index * update_block + (i % update_block);\n     body(&params[params_i], ldg(updates + updates_i));\n   }\n }"
        },
        {
            "sha": "d5e3b2ad9eb0a9d23df4e84b5eaed3d08e870e18",
            "filename": "tensorflow/core/kernels/scatter_nd_op.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscatter_nd_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscatter_nd_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fscatter_nd_op.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -1040,10 +1040,10 @@ absl::Status DoScatterNdOnCpu(OpKernelContext* c, const Tensor& indices,\n // and the GPU implementation is not. Tensor inputs to this function must be on\n // the GPU.\n template <typename T, typename Index, scatter_nd_op::UpdateOp Op>\n-Status DoScatterNdOnCpu(OpKernelContext* c, const Tensor& indices,\n-                        const Tensor& updates, const TensorShape& shape,\n-                        Tensor* out, bool allocate,\n-                        BadIndicesPolicy bad_indices_policy) {\n+absl::Status DoScatterNdOnCpu(OpKernelContext* c, const Tensor& indices,\n+                              const Tensor& updates, const TensorShape& shape,\n+                              Tensor* out, bool allocate,\n+                              BadIndicesPolicy bad_indices_policy) {\n   AllocatorAttributes alloc_attr;\n   alloc_attr.set_on_host(true);\n   alloc_attr.set_gpu_compatible(true);\n@@ -1053,7 +1053,7 @@ Status DoScatterNdOnCpu(OpKernelContext* c, const Tensor& indices,\n   Tensor host_indices;\n   TF_RETURN_IF_ERROR(c->allocate_temp(indices.dtype(), indices.shape(),\n                                       &host_indices, alloc_attr));\n-  se::DeviceMemoryBase indices_ptr(\n+  stream_executor::DeviceAddressBase indices_ptr(\n       const_cast<Tensor&>(indices).flat<Index>().data(),\n       indices.flat<Index>().size() * sizeof(Index));\n   TF_RETURN_IF_ERROR(stream->Memcpy(host_indices.flat<Index>().data(),\n@@ -1063,7 +1063,7 @@ Status DoScatterNdOnCpu(OpKernelContext* c, const Tensor& indices,\n   Tensor host_updates;\n   TF_RETURN_IF_ERROR(c->allocate_temp(updates.dtype(), updates.shape(),\n                                       &host_updates, alloc_attr));\n-  se::DeviceMemoryBase updates_ptr(\n+  stream_executor::DeviceAddressBase updates_ptr(\n       const_cast<Tensor&>(updates).flat<T>().data(),\n       updates.flat<T>().size() * sizeof(T));\n   TF_RETURN_IF_ERROR(stream->Memcpy(host_updates.flat<T>().data(), updates_ptr,\n@@ -1078,8 +1078,8 @@ Status DoScatterNdOnCpu(OpKernelContext* c, const Tensor& indices,\n     fill(c->eigen_device<CPUDevice>(), host_out.flat<T>());\n   } else {\n     CHECK_NOTNULL(out);  // Crash OK\n-    se::DeviceMemoryBase out_ptr(out->flat<T>().data(),\n-                                 out->flat<T>().size() * sizeof(T));\n+    stream_executor::DeviceAddressBase out_ptr(\n+        out->flat<T>().data(), out->flat<T>().size() * sizeof(T));\n     TF_RETURN_IF_ERROR(stream->Memcpy(host_out.flat<T>().data(), out_ptr,\n                                       host_out.NumElements() * sizeof(T)));\n   }\n@@ -1090,13 +1090,13 @@ Status DoScatterNdOnCpu(OpKernelContext* c, const Tensor& indices,\n       bad_indices_policy));\n \n   // Copy 'host_out' to device.\n-  se::DeviceMemoryBase out_ptr(out->flat<T>().data(),\n-                               out->flat<T>().size() * sizeof(T));\n+  stream_executor::DeviceAddressBase out_ptr(out->flat<T>().data(),\n+                                             out->flat<T>().size() * sizeof(T));\n   TF_RETURN_IF_ERROR(stream->Memcpy(&out_ptr, host_out.flat<T>().data(),\n                                     host_out.NumElements() * sizeof(T)));\n   // Block host, since 'host_out' cannot be destructed until the copy is done.\n   TF_RETURN_IF_ERROR(stream->BlockHostUntilDone());\n-  return OkStatus();\n+  return absl::OkStatus();\n }\n \n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
        },
        {
            "sha": "ae2402b2a228e14f8a449c473edc75c1620e731e",
            "filename": "tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscatter_nd_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fscatter_nd_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fscatter_nd_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -98,7 +98,7 @@ template <typename T, typename Index, scatter_nd_op::UpdateOp op, int IXDIM>\n __global__ void ScatterNdOpKernel(\n     const Index* indices, const T* updates, T* out,\n     const Eigen::array<Eigen::DenseIndex, IXDIM> output_shape_prefix,\n-    const Eigen::array<int64, IXDIM> batch_strides, const int64 num_indices,\n+    const Eigen::array<int64_t, IXDIM> batch_strides, const int64_t num_indices,\n     const Index slice_size) {\n   auto update = LeftUpdate<T, op>();\n \n@@ -141,7 +141,7 @@ struct ScatterNdFunctor<GPUDevice, T, Index, op, IXDIM> {\n     const Eigen::DenseIndex batch_size = Tindices.dimension(0);\n \n     // Index batch_strides[IXDIM];\n-    Eigen::array<int64, IXDIM> batch_strides;\n+    Eigen::array<int64_t, IXDIM> batch_strides;\n     if (IXDIM > 0) {\n       batch_strides[IXDIM - 1] = 1;\n     }"
        },
        {
            "sha": "10448882a9296d8c2534fac996576b1b5431a6d2",
            "filename": "tensorflow/core/kernels/searchsorted_op_gpu.cu.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fsearchsorted_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fsearchsorted_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fsearchsorted_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -60,15 +60,16 @@ __global__ void LowerBoundKernel(const T* __restrict__ sorted_inputs,\n namespace functor {\n template <typename T, typename OutType>\n struct UpperBoundFunctor<GPUDevice, T, OutType> {\n-  static Status Compute(OpKernelContext* context,\n-                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n-                        const typename TTypes<T, 1>::ConstTensor& values,\n-                        int batch_size, int num_inputs, int num_values,\n-                        typename TTypes<OutType, 1>::Tensor* output) {\n+  static absl::Status Compute(\n+      OpKernelContext* context,\n+      const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n+      const typename TTypes<T, 1>::ConstTensor& values, int batch_size,\n+      int num_inputs, int num_values,\n+      typename TTypes<OutType, 1>::Tensor* output) {\n     const GPUDevice& device = context->eigen_device<GPUDevice>();\n     if (values.size() == 0) {\n       // GetGpuLaunchConfig requires work_element_count > 0\n-      return OkStatus();\n+      return absl::OkStatus();\n     }\n     GpuLaunchConfig config = GetGpuLaunchConfig(values.size(), device);\n \n@@ -77,21 +78,22 @@ struct UpperBoundFunctor<GPUDevice, T, OutType> {\n         config.thread_per_block, 0, device.stream(), sorted_inputs.data(),\n         batch_size, num_inputs, num_values, values.data(), output->data()));\n \n-    return OkStatus();\n+    return absl::OkStatus();\n   }\n };\n \n template <typename T, typename OutType>\n struct LowerBoundFunctor<GPUDevice, T, OutType> {\n-  static Status Compute(OpKernelContext* context,\n-                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n-                        const typename TTypes<T, 1>::ConstTensor& values,\n-                        int batch_size, int num_inputs, int num_values,\n-                        typename TTypes<OutType, 1>::Tensor* output) {\n+  static absl::Status Compute(\n+      OpKernelContext* context,\n+      const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n+      const typename TTypes<T, 1>::ConstTensor& values, int batch_size,\n+      int num_inputs, int num_values,\n+      typename TTypes<OutType, 1>::Tensor* output) {\n     const GPUDevice& device = context->eigen_device<GPUDevice>();\n     if (values.size() == 0) {\n       // GetGpuLaunchConfig requires work_element_count > 0\n-      return OkStatus();\n+      return absl::OkStatus();\n     }\n     GpuLaunchConfig config = GetGpuLaunchConfig(values.size(), device);\n \n@@ -100,7 +102,7 @@ struct LowerBoundFunctor<GPUDevice, T, OutType> {\n         config.thread_per_block, 0, device.stream(), sorted_inputs.data(),\n         batch_size, num_inputs, num_values, values.data(), output->data()));\n \n-    return OkStatus();\n+    return absl::OkStatus();\n   }\n };\n }  // namespace functor"
        },
        {
            "sha": "dc63e6c56029568978b966aabe09de8b28ce3058",
            "filename": "tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h",
            "status": "modified",
            "additions": 41,
            "deletions": 39,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fsegment_reduction_ops_gpu.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fsegment_reduction_ops_gpu.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fsegment_reduction_ops_gpu.cu.h?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -165,7 +165,7 @@ __global__ void SegmentMeanNormalizeKernel(\n }\n \n template <typename SegmentId, typename Index, typename T>\n-Status LaunchSegmentMeanNormalizeKernel(\n+absl::Status LaunchSegmentMeanNormalizeKernel(\n     const GPUDevice& d, SegmentId nsegments, Index ninner,\n     const Index* __restrict__ segment_offsets,  // [nsegments + 1]\n     T* __restrict__ output) {                   // [nsegments, ninner]\n@@ -195,7 +195,7 @@ __global__ void SegmentSetEmptyKernel(\n }\n \n template <typename SegmentId, typename Index, typename T>\n-Status LaunchSegmentSetEmptyKernel(\n+absl::Status LaunchSegmentSetEmptyKernel(\n     const GPUDevice& d, SegmentId nsegments, Index ninner,\n     const Index* __restrict__ segment_offsets,  // [nsegments + 1]\n     const T empty_value,\n@@ -263,7 +263,7 @@ __global__ void SegmentOffsetsKernel(\n // value at segment_offsets[nsegments] is set to the end index of the last valid\n // ID (e.g., nsegments if all IDs are valid).\n template <typename Toffsets, typename Tsegmentids>\n-Status LaunchSegmentOffsetsKernel(\n+absl::Status LaunchSegmentOffsetsKernel(\n     const GPUDevice& d, Toffsets size, Tsegmentids nsegments,\n     const Tsegmentids* segment_ids,  // [size]\n     Toffsets* segment_offsets) {     // [nsegments + 1]\n@@ -397,7 +397,7 @@ __global__ void SegmentReduceVectorKernel(\n template <typename Treducevec, typename Tvec, typename Toffsets,\n           typename Tindices, typename Tsegmentids, typename ReduceOp,\n           typename Tinit, typename Tweights>\n-Status LaunchSegmentReduceVectorKernel(\n+absl::Status LaunchSegmentReduceVectorKernel(\n     const GPUDevice& d, Toffsets nouter, Toffsets ninner_vec,\n     Tsegmentids nsegments, ReduceOp reduce_op, Tinit initial_value,\n     Tinit empty_segment_value, bool is_mean, bool is_sqrtn,\n@@ -467,7 +467,7 @@ __global__ void SegmentReduceEpilogueKernel(\n // be a higher-precision type than the output type Tvec (e.g., float vs. half).\n template <typename Tvec, typename Treducevec, typename Toffsets,\n           typename Tsegmentids, typename Tinit>\n-Status LaunchSegmentReduceEpilogueKernel(\n+absl::Status LaunchSegmentReduceEpilogueKernel(\n     const GPUDevice& d, Tsegmentids nsegments, Tinit empty_segment_value,\n     bool is_mean, bool is_sqrtn,\n     const Treducevec* output_raw,     // [nsegments]\n@@ -542,7 +542,7 @@ MakeLookupAndScaleAndCastInputsIterator(const Tvec* input_vec,\n template <typename Treducevec, typename Tvec, typename Toffsets,\n           typename Tindices, typename Tsegmentids, typename ReduceOp,\n           typename Tinit, typename Tweights>\n-Status SegmentReduceGPUImplNoInnerDim(\n+absl::Status SegmentReduceGPUImplNoInnerDim(\n     OpKernelContext* ctx, Toffsets nouter, Tsegmentids nsegments,\n     ReduceOp reduce_op, Tinit initial_value, Tinit empty_segment_value,\n     bool is_mean, bool is_sqrtn,\n@@ -568,7 +568,7 @@ Status SegmentReduceGPUImplNoInnerDim(\n         TensorShape({static_cast<int64_t>(nsegments * sizeof(Treducevec))}),\n         &output_raw));\n     output_raw_ptr =\n-        reinterpret_cast<Treducevec*>(output_raw.flat<int8>().data());\n+        reinterpret_cast<Treducevec*>(output_raw.flat<int8_t>().data());\n   }\n   auto input_iter =\n       MakeLookupAndScaleAndCastInputsIterator<Treducevec, Toffsets>(\n@@ -586,13 +586,13 @@ Status SegmentReduceGPUImplNoInnerDim(\n         device, nsegments, empty_segment_value, is_mean, is_sqrtn,\n         output_raw_ptr, segment_offsets, output_vec));\n   }\n-  return OkStatus();\n+  return absl::OkStatus();\n }\n \n template <typename Treducevec, typename Tvec, typename Toffsets,\n           typename Tindices, typename Tsegmentids, typename ReduceOp,\n           typename Tinit, typename Tweights>\n-Status SegmentReduceGPUImpl(\n+absl::Status SegmentReduceGPUImpl(\n     OpKernelContext* ctx, Toffsets nouter, Toffsets ninner_vec,\n     Tsegmentids nsegments, ReduceOp reduce_op, Tinit initial_value,\n     Tinit empty_segment_value, bool is_mean, bool is_sqrtn,\n@@ -648,12 +648,13 @@ struct SegmentReduceGPUVectorized {\n   struct Impl {\n     template <typename T, typename Toffsets, typename Tindices,\n               typename Tsegmentids, typename ReduceOp, typename Tweights>\n-    Status operator()(OpKernelContext* ctx, Toffsets nouter, Toffsets ninner,\n-                      Tsegmentids nsegments, ReduceOp reduce_op,\n-                      T initial_value, T empty_segment_value, bool is_mean,\n-                      bool is_sqrtn, const T* input,\n-                      const Tsegmentids* segment_ids, const Tindices* indices,\n-                      const Tweights* weights, T* output) {\n+    absl::Status operator()(OpKernelContext* ctx, Toffsets nouter,\n+                            Toffsets ninner, Tsegmentids nsegments,\n+                            ReduceOp reduce_op, T initial_value,\n+                            T empty_segment_value, bool is_mean, bool is_sqrtn,\n+                            const T* input, const Tsegmentids* segment_ids,\n+                            const Tindices* indices, const Tweights* weights,\n+                            T* output) {\n       DCHECK_EQ(ninner % vec_size, 0);\n       DCHECK_EQ(reinterpret_cast<std::uintptr_t>(input) % vec_size, 0);\n       DCHECK_EQ(reinterpret_cast<std::uintptr_t>(output) % vec_size, 0);\n@@ -682,16 +683,16 @@ struct SegmentReduceGPUVectorized {\n // Note: Treduce is to allow reducing in higher precision than T.\n template <typename Treduce, typename T, typename Toffsets, typename Tindices,\n           typename Tsegmentids, typename ReduceOp, typename Tweights>\n-Status SegmentReduceGPU(OpKernelContext* ctx, Toffsets nouter, Toffsets ninner,\n-                        Tsegmentids nsegments, ReduceOp reduce_op,\n-                        T initial_value, T empty_segment_value, bool is_mean,\n-                        bool is_sqrtn,\n-                        const T* input,  // [nouter or any, ninner]\n-                        const Tsegmentids* segment_ids,  // [nouter]\n-                        const Tindices* indices,         // [nouter] (optional)\n-                        const Tweights* weights,  // [nouter or any] (optional)\n-                        T* output) {              // [nsegments, ninner]\n-  if (ninner == 0 || nsegments == 0) return OkStatus();\n+absl::Status SegmentReduceGPU(\n+    OpKernelContext* ctx, Toffsets nouter, Toffsets ninner,\n+    Tsegmentids nsegments, ReduceOp reduce_op, T initial_value,\n+    T empty_segment_value, bool is_mean, bool is_sqrtn,\n+    const T* input,                  // [nouter or any, ninner]\n+    const Tsegmentids* segment_ids,  // [nouter]\n+    const Tindices* indices,         // [nouter] (optional)\n+    const Tweights* weights,         // [nouter or any] (optional)\n+    T* output) {                     // [nsegments, ninner]\n+  if (ninner == 0 || nsegments == 0) return absl::OkStatus();\n   return DispatchToVectorized<\n       T, SegmentReduceGPUVectorized<Treduce>::template Impl>(\n       MinAlignmentOf(input, output, ninner), ctx, nouter, ninner, nsegments,\n@@ -716,7 +717,7 @@ __global__ void SegmentWeightsKernel(\n }\n \n template <typename SegmentId, typename Index, typename Tweights>\n-Status LaunchSegmentWeightsKernel(\n+absl::Status LaunchSegmentWeightsKernel(\n     const GPUDevice& d, SegmentId nsegments,\n     SparseSegmentReductionOperation operation,\n     const Index* segment_offsets,  // [nsegments + 1]\n@@ -945,7 +946,7 @@ struct UnsortedSegmentFunctor<GPUDevice, T, Index, InitialValueF, ReductionF> {\n };\n \n template <typename T, typename Index, typename SegmentId>\n-Status SparseSegmentReductionFunctor<T, Index, SegmentId>::operator()(\n+absl::Status SparseSegmentReductionFunctor<T, Index, SegmentId>::operator()(\n     OpKernelContext* context, bool is_mean, bool is_sqrtn, T default_value,\n     typename TTypes<T, 2>::ConstTensor input,\n     typename TTypes<Index>::ConstVec indices,\n@@ -1087,7 +1088,7 @@ __global__ void ScatterUniqueIndicesKernel(\n \n template <typename Toffsets, typename EdgeIndicatorIter,\n           typename TindicesCompact, typename Tindices>\n-Status LaunchScatterUniqueIndicesKernel(\n+absl::Status LaunchScatterUniqueIndicesKernel(\n     const GPUDevice& d, Toffsets nouter,\n     EdgeIndicatorIter sorted_indices_edge_indicator,     // [nouter]\n     const TindicesCompact* __restrict__ sorted_indices,  // [nouter]\n@@ -1122,7 +1123,7 @@ struct SparseSegmentGradV2Functor<GPUDevice, T, Tindices, Tsegmentids> {\n     const int64_t nouter64 = indices_vec.dimension(0);\n     // Note: nouter and ninner are not expected to be huge, so we use int32 to\n     // save memory bandwidth.\n-    using Toffsets = int32;\n+    using Toffsets = int32_t;\n     OP_REQUIRES_ASYNC(context, nouter64 <= std::numeric_limits<Toffsets>::max(),\n                       absl::InvalidArgumentError(\n                           absl::StrCat(\"Indices vector of length \", nouter64,\n@@ -1140,7 +1141,7 @@ struct SparseSegmentGradV2Functor<GPUDevice, T, Tindices, Tsegmentids> {\n     // worth it because the vector is used multiple times).\n     // Note that we can currently assume int32 is safe because the op's dense\n     // output_dim0 input is always int32.\n-    using TindicesCompact = int32;\n+    using TindicesCompact = int32_t;\n     Tensor tmp_indices_internal;\n     const TindicesCompact* indices_internal_ptr;\n     if constexpr (std::is_same<Tindices, TindicesCompact>::value) {\n@@ -1163,9 +1164,9 @@ struct SparseSegmentGradV2Functor<GPUDevice, T, Tindices, Tsegmentids> {\n           context, operation, nouter, ninner, nsegments, input_flat.data(),\n           tmp_indices_internal, indices_internal_ptr, segment_vec,\n           dense_output_shape, done);\n-    } else if (sizeof(Tsegmentids) > sizeof(int32) &&\n-               nsegments <= std::numeric_limits<int32>::max()) {\n-      CastSegmentIdsThenImpl<Toffsets, TindicesCompact, int32>(\n+    } else if (sizeof(Tsegmentids) > sizeof(int32_t) &&\n+               nsegments <= std::numeric_limits<int32_t>::max()) {\n+      CastSegmentIdsThenImpl<Toffsets, TindicesCompact, int32_t>(\n           context, operation, nouter, ninner, nsegments, input_flat.data(),\n           tmp_indices_internal, indices_internal_ptr, segment_vec,\n           dense_output_shape, done);\n@@ -1295,12 +1296,13 @@ struct SparseSegmentGradV2Functor<GPUDevice, T, Tindices, Tsegmentids> {\n     ScratchSpace<Toffsets> last_idx_host(context, 1, /*on_host=*/true);\n     OP_REQUIRES_OK_ASYNC(\n         context,\n-        stream->Memcpy(last_idx_host.mutable_data(),\n-                       se::DeviceMemoryBase(const_cast<Toffsets*>(\n-                                                sorted_indices_unique_ids_ptr) +\n-                                                (nouter - 1),\n-                                            sizeof(*last_idx_host.data())),\n-                       sizeof(*last_idx_host.data())),\n+        stream->Memcpy(\n+            last_idx_host.mutable_data(),\n+            stream_executor::DeviceAddressBase(\n+                const_cast<Toffsets*>(sorted_indices_unique_ids_ptr) +\n+                    (nouter - 1),\n+                sizeof(*last_idx_host.data())),\n+            sizeof(*last_idx_host.data())),\n         done);\n \n     auto async_finish_computation ="
        },
        {
            "sha": "4bc9c22b33bb007341f697cd78a03a3e39c030ee",
            "filename": "tensorflow/core/kernels/spacetobatch_functor_gpu.cu.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fspacetobatch_functor_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fspacetobatch_functor_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fspacetobatch_functor_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -31,11 +31,11 @@ typedef Eigen::GpuDevice GPUDevice;\n // GPU kernel.\n template <int NUM_BLOCK_DIMS>\n struct S2BParameters {\n-  int32 space_tensor_batch;\n-  int32 batch_tensor_shape[NUM_BLOCK_DIMS + 2];\n-  int32 space_tensor_spatial_shape[NUM_BLOCK_DIMS];\n-  int32 pad_start[NUM_BLOCK_DIMS];\n-  int32 block_shape[NUM_BLOCK_DIMS];\n+  int32_t space_tensor_batch;\n+  int32_t batch_tensor_shape[NUM_BLOCK_DIMS + 2];\n+  int32_t space_tensor_spatial_shape[NUM_BLOCK_DIMS];\n+  int32_t pad_start[NUM_BLOCK_DIMS];\n+  int32_t block_shape[NUM_BLOCK_DIMS];\n };\n \n // GPU kernel for space-to-batch (if B2S = false) and batch-to-space conversion\n@@ -44,13 +44,13 @@ struct S2BParameters {\n // To simplify template implementation given lack of constexpr if, both the\n // input and output pointers are non-const.\n template <typename T, int NUM_BLOCK_DIMS, bool B2S>\n-__global__ void S2B(const int32 nthreads, T* __restrict__ space_tensor_ptr,\n+__global__ void S2B(const int32_t nthreads, T* __restrict__ space_tensor_ptr,\n                     S2BParameters<NUM_BLOCK_DIMS> args,\n                     T* __restrict__ batch_tensor_ptr) {\n   GPU_1D_KERNEL_LOOP(batch_tensor_idx, nthreads) {\n-    int32 remaining_batch_tensor_idx = batch_tensor_idx;\n+    int32_t remaining_batch_tensor_idx = batch_tensor_idx;\n \n-    int32 batch_tensor_pos[NUM_BLOCK_DIMS + 2];\n+    int32_t batch_tensor_pos[NUM_BLOCK_DIMS + 2];\n \n     for (int dim = NUM_BLOCK_DIMS + 1; dim >= 1; --dim) {\n       batch_tensor_pos[dim] =\n@@ -59,17 +59,17 @@ __global__ void S2B(const int32 nthreads, T* __restrict__ space_tensor_ptr,\n     }\n     batch_tensor_pos[0] = remaining_batch_tensor_idx;\n \n-    int32 remaining_block_idx = batch_tensor_pos[0] / args.space_tensor_batch;\n-    int32 space_tensor_idx = batch_tensor_pos[NUM_BLOCK_DIMS + 1];\n-    int32 space_tensor_stride = args.batch_tensor_shape[NUM_BLOCK_DIMS + 1];\n-    const int32 space_tensor_batch_pos =\n+    int32_t remaining_block_idx = batch_tensor_pos[0] / args.space_tensor_batch;\n+    int32_t space_tensor_idx = batch_tensor_pos[NUM_BLOCK_DIMS + 1];\n+    int32_t space_tensor_stride = args.batch_tensor_shape[NUM_BLOCK_DIMS + 1];\n+    const int32_t space_tensor_batch_pos =\n         batch_tensor_pos[0] % args.space_tensor_batch;\n     for (int block_dim = NUM_BLOCK_DIMS - 1; block_dim >= 0; --block_dim) {\n-      int32 offset = remaining_block_idx;\n+      int32_t offset = remaining_block_idx;\n       if (block_dim > 0) {\n         offset %= args.block_shape[block_dim];\n       }\n-      int32 space_tensor_pos =\n+      int32_t space_tensor_pos =\n           batch_tensor_pos[block_dim + 1] * args.block_shape[block_dim] +\n           offset - args.pad_start[block_dim];\n       if (space_tensor_pos < 0 ||\n@@ -102,45 +102,45 @@ template <typename T, int NUM_BLOCK_DIMS, bool B2S>\n struct SpaceToBatchFunctor<GPUDevice, T, NUM_BLOCK_DIMS, B2S> {\n   using SpaceT = typename std::conditional<B2S, T, const T>::type;\n   using BatchT = typename std::conditional<B2S, const T, T>::type;\n-  Status operator()(\n+  absl::Status operator()(\n       const GPUDevice& d,\n       typename TTypes<SpaceT, NUM_BLOCK_DIMS + 2>::Tensor space_tensor,\n-      const int64 block_shape[NUM_BLOCK_DIMS],\n-      const int64 paddings[NUM_BLOCK_DIMS * 2],\n+      const int64_t block_shape[NUM_BLOCK_DIMS],\n+      const int64_t paddings[NUM_BLOCK_DIMS * 2],\n       typename TTypes<BatchT, NUM_BLOCK_DIMS + 2>::Tensor batch_tensor) {\n     // Kernel execution fails if number of elements is zero.\n     if (batch_tensor.size() == 0) {\n-      return OkStatus();\n+      return absl::OkStatus();\n     }\n     S2BParameters<NUM_BLOCK_DIMS> args;\n     args.space_tensor_batch = space_tensor.dimension(0);\n     for (int block_dim = 0; block_dim < NUM_BLOCK_DIMS; ++block_dim) {\n-      if (block_shape[block_dim] > std::numeric_limits<int32>::max()) {\n+      if (block_shape[block_dim] > std::numeric_limits<int32_t>::max()) {\n         return errors::InvalidArgument(\"block_shape value exceeds 2^32-1\");\n       }\n       args.block_shape[block_dim] = block_shape[block_dim];\n       if (space_tensor.dimension(block_dim + 1) >\n-          std::numeric_limits<int32>::max()) {\n+          std::numeric_limits<int32_t>::max()) {\n         return errors::InvalidArgument(\"space_tensor dimension exceeds 2^32-1\");\n       }\n       args.space_tensor_spatial_shape[block_dim] =\n           space_tensor.dimension(block_dim + 1);\n-      if (paddings[block_dim * 2] > std::numeric_limits<int32>::max()) {\n+      if (paddings[block_dim * 2] > std::numeric_limits<int32_t>::max()) {\n         return errors::InvalidArgument(\"paddings/crops value exceeds 2^32-1\");\n       }\n       args.pad_start[block_dim] = paddings[block_dim * 2];\n     }\n-    int64 total_count = 1;\n+    int64_t total_count = 1;\n     for (int dim = 0; dim < NUM_BLOCK_DIMS + 2; ++dim) {\n       args.batch_tensor_shape[dim] = batch_tensor.dimension(dim);\n       total_count *= args.batch_tensor_shape[dim];\n     }\n-    if (total_count > std::numeric_limits<int32>::max()) {\n+    if (total_count > std::numeric_limits<int32_t>::max()) {\n       return errors::InvalidArgument(\n           \"number of batch_tensor elements exceeds 2^32-1\");\n     }\n     GpuLaunchConfig config =\n-        GetGpuLaunchConfig(static_cast<int32>(total_count), d);\n+        GetGpuLaunchConfig(static_cast<int32_t>(total_count), d);\n     return GpuLaunchKernel(S2B<T, NUM_BLOCK_DIMS, B2S>, config.block_count,\n                            config.thread_per_block, 0, d.stream(),\n                            config.virtual_thread_count,"
        },
        {
            "sha": "97acca5442890db73cf6b678e271cf5569e3950e",
            "filename": "tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fspacetodepth_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe/tensorflow%2Fcore%2Fkernels%2Fspacetodepth_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fspacetodepth_op_gpu.cu.cc?ref=cd19aba91fd8bdedbd055bbfb6fbee4efd392bfe",
            "patch": "@@ -29,7 +29,7 @@ typedef Eigen::GpuDevice GPUDevice;\n // Space2Depth kernel for FORMAT_NHWC.\n // See 'spacetodepth_op.h' for a more detailed description.\n template <typename dtype>\n-__global__ void S2D_NHWC(const int32 nthreads,\n+__global__ void S2D_NHWC(const int32_t nthreads,\n                          const dtype* __restrict__ input_ptr,\n                          const int block_size, const int batch_size,\n                          const int input_height, const int input_width,\n@@ -61,7 +61,7 @@ __global__ void S2D_NHWC(const int32 nthreads,\n // Space2Depth kernel for FORMAT_NCHW.\n // See 'spacetodepth_op.h' for a more detailed description.\n template <typename dtype>\n-__global__ void S2D_NCHW(const int32 nthreads,\n+__global__ void S2D_NCHW(const int32_t nthreads,\n                          const dtype* __restrict__ input_ptr,\n                          const int block_size, const int output_width,\n                          const int input_depth_by_output_height,\n@@ -99,7 +99,7 @@ __global__ void S2D_NCHW(const int32 nthreads,\n // Space2Depth kernel for FORMAT_NCHW using a loop over block area.\n // See 'spacetodepth_op.h' for functional specification.\n template <typename dtype, int block_size>\n-__global__ void S2D_NCHW_LOOP(const int32 nthreads,\n+__global__ void S2D_NCHW_LOOP(const int32_t nthreads,\n                               const dtype* __restrict__ input,\n                               const int output_width, const int input_width,\n                               const int input_depth_by_output_area,"
        }
    ],
    "stats": {
        "total": 711,
        "additions": 364,
        "deletions": 347
    }
}