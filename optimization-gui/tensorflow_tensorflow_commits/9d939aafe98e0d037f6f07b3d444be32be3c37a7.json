{
    "author": "bchetioui",
    "message": "Revert deletion of legacy Triton emitter due to breakage.\n\nReverts b6495a6320c12bd8024679a0838ab9b0243f1a3c\n\nPiperOrigin-RevId: 834207760",
    "sha": "9d939aafe98e0d037f6f07b3d444be32be3c37a7",
    "files": [
        {
            "sha": "bdb247e4e3f577716c6d6067d33c0b0d4286de29",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -102,16 +102,6 @@ class GpuCodegenBackend : public CodegenBackend {\n     // Avoid using GPU graphs as we don't want to measure graph construction\n     // time.\n     debug_options.clear_xla_gpu_enable_command_buffer();\n-    // Make sure to use the generic Triton emitter for everything.\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n     // Avoid using async dot as we don't want to measure event overheads.\n     debug_options.set_xla_gpu_async_dot(false);\n     debug_options.set_xla_embed_ir_in_executable(false);"
        },
        {
            "sha": "929681fcdc18af8000d4030409382deea6d5d52d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 265,
            "deletions": 5,
            "changes": 270,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -35,15 +35,14 @@ cc_library(\n     ],\n     deps = [\n         \":fusion_emitter\",\n+        \":fusion_emitter_legacy_matmul\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n-        \"//xla:util\",\n         \"//xla/backends/gpu/codegen:fusion_emitter\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/codegen/tiling:tiled_hlo_computation\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service/gpu:backend_configs_cc\",\n@@ -53,12 +52,13 @@ cc_library(\n         \"//xla/service/gpu:ir_emitter_context\",\n         \"//xla/service/gpu:kernel_reuse_cache\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu:triton_fusion_analysis\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n@@ -68,6 +68,7 @@ cc_library(\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Support\",\n     ],\n )\n@@ -273,6 +274,7 @@ cc_library(\n         \":compilation_pipeline\",\n         \":dot_algorithms\",\n         \":emitter_helpers\",\n+        \":fusion_emitter_legacy_matmul\",\n         \":support\",\n         \":tma_utils\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -348,6 +350,70 @@ cc_library(\n     ]),\n )\n \n+cc_library(\n+    name = \"fusion_emitter_legacy_matmul\",\n+    srcs = if_gpu_is_configured(\n+        [\"fusion_emitter_legacy_matmul.cc\"],\n+        [\"fusion_emitter_legacy_matmul_stub.cc\"],\n+    ),\n+    hdrs = [\"fusion_emitter_legacy_matmul.h\"],\n+    deps = [\n+        \":dot_algorithms\",\n+        \":emitter_helpers\",\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:comparison_util\",\n+        \"//xla:literal\",\n+        \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n+        \"//xla/codegen:emitter_loc_op_builder\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/utils:hlo_query\",\n+        \"//xla/hlo/utils:hlo_traversal\",\n+        \"//xla/mlir_hlo\",\n+        \"//xla/mlir_hlo:map_mhlo_to_scalar_op\",\n+        \"//xla/mlir_hlo:transformation_helpers\",\n+        \"//xla/service:algorithm_util\",\n+        \"//xla/service:matmul_indexing_utils\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu:triton_fusion_analysis\",\n+        \"//xla/service/gpu:triton_tiling_propagation\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/service/llvm_ir:llvm_util\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:cord\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:FunctionInterfaces\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:MathDialect\",\n+        \"@llvm-project//mlir:NVVMDialect\",\n+        \"@llvm-project//mlir:SCFDialect\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@triton//:TritonDialects\",\n+    ],\n+)\n+\n cc_library(\n     name = \"dot_algorithms\",\n     srcs = [\"dot_algorithms.cc\"],\n@@ -383,10 +449,12 @@ cc_library(\n cc_library(\n     name = \"fusion_emitter_stub_for_testing\",\n     srcs = [\n+        \"fusion_emitter_legacy_matmul_stub.cc\",\n         \"fusion_emitter_stub.cc\",\n     ],\n     hdrs = [\n         \"fusion_emitter.h\",\n+        \"fusion_emitter_legacy_matmul.h\",\n     ],\n     deps = [\n         \":emitter_helpers\",\n@@ -470,7 +538,63 @@ xla_cc_test(\n     ],\n )\n \n-# TODO(b/393299275): Rename this test file now that the legacy emitter is gone.\n+xla_test(\n+    name = \"fusion_emitter_device_legacy_test\",\n+    size = \"large\",\n+    srcs = if_gpu_is_configured([\"fusion_emitter_device_legacy_test.cc\"]),\n+    # TODO(b/372714955): Fix the memory leak!\n+    backend_args = if_google(\n+        {\n+            \"h100\": [\"--heap_check=\"],\n+            \"a100\": [\"--heap_check=\"],\n+        },\n+        {},\n+    ),\n+    backends = [\n+        \"a100\",\n+        \"h100\",\n+        \"b200\",\n+        \"amdgpu_any\",\n+    ],\n+    shard_count = 20,\n+    tags = [\n+        \"large\",\n+        \"no_mac\",\n+    ],\n+    deps = [\n+        \":fusion_emitter\",\n+        \":test_utils\",\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:error_spec\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/hlo/analysis:symbolic_expr\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:filecheck\",\n+        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n+        \"//xla/hlo/testlib:verified_hlo_module\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status_matchers\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Pass\",\n+        \"@local_tsl//tsl/platform:path\",\n+    ],\n+)\n+\n xla_test(\n     name = \"fusion_emitter_device_legacy_port_test\",\n     srcs = if_gpu_is_configured([\"fusion_emitter_device_legacy_port_test.cc\"]),\n@@ -549,7 +673,6 @@ xla_test(\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n-        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status\",\n@@ -561,6 +684,45 @@ xla_test(\n     ],\n )\n \n+xla_test(\n+    name = \"fusion_emitter_legacy_int4_device_test\",\n+    size = \"large\",\n+    srcs = if_gpu_is_configured([\"fusion_emitter_legacy_int4_device_test.cc\"]),\n+    backends = [\n+        \"a100\",\n+        \"h100\",\n+        \"b200\",\n+        \"amdgpu_any\",\n+    ],\n+    shard_count = 10,\n+    tags = [\n+        \"large\",\n+        \"no_mac\",\n+    ],\n+    deps = [\n+        \":test_utils\",\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:error_spec\",\n+        \"//xla:literal_util\",\n+        \"//xla:shape_util\",\n+        \"//xla:types\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:filecheck\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest\",\n+        \"@local_tsl//tsl/platform:path\",\n+    ],\n+)\n+\n xla_test(\n     name = \"dot_algorithms_test\",\n     srcs = [\"dot_algorithms_test.cc\"],\n@@ -626,6 +788,72 @@ xla_test(\n     ],\n )\n \n+# TODO(b/393299275): Remove this target once the legacy emitter is removed.\n+xla_test(\n+    name = \"dot_algorithms_legacy_test\",\n+    srcs = [\"dot_algorithms_legacy_test.cc\"],\n+    backend_args = if_google(\n+        {\n+            \"b200\": [\"--heap_check=\"],\n+            \"a100\": [\"--heap_check=\"],\n+            \"h100\": [\"--heap_check=\"],\n+        },\n+        {},\n+    ),\n+    backends = [\n+        \"a100\",\n+        \"h100\",\n+        \"b200\",\n+        \"amdgpu_any\",\n+    ],\n+    env = {\n+        \"CUBLAS_EMULATE_SINGLE_PRECISION\": \"1\",  # Trigger single precision emulation (F32_F32_F32) with BF16x9 cublas algorithm. It was introduced in cublas 12.9.\n+        \"CUBLAS_EMULATION_STRATEGY\": \"performant\",  # Trigger single precision emulation (F32_F32_F32) with BF16x9 cublas algorithm. It was introduced in cublas 12.9.\n+    },\n+    shard_count = 30,\n+    tags = [\n+        \"no_mac\",\n+    ],\n+    deps = [\n+        \":test_utils\",\n+        \"//xla:autotuning_proto_cc\",\n+        \"//xla:error_spec\",\n+        \"//xla:literal\",\n+        \"//xla:literal_util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/profiler:kernel_name_tracer\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:filecheck\",\n+        \"//xla/hlo/testlib:verified_hlo_module\",\n+        \"//xla/service:dump\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tests:test_utils\",\n+        \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/time\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@local_tsl//tsl/platform:path\",\n+    ],\n+)\n+\n xla_test(\n     name = \"fusion_emitter_device_test\",\n     srcs = if_gpu_is_configured([\"fusion_emitter_device_test.cc\"]),\n@@ -794,6 +1022,38 @@ xla_test(\n     ],\n )\n \n+xla_test(\n+    name = \"fusion_emitter_parametrized_legacy_test\",\n+    srcs = if_gpu_is_configured([\"fusion_emitter_parametrized_legacy_test.cc\"]),\n+    backend_tags = {\n+        # TODO(b/445172709): Re-enable once fixed.\n+        \"b200\": [\"broken\"],\n+    },\n+    backends = [\n+        \"a100\",\n+        \"h100\",\n+        \"b200\",\n+        \"amdgpu_any\",\n+    ],\n+    shard_count = 10,\n+    tags = [\"no_mac\"],\n+    deps = [\n+        \":support\",\n+        \":test_utils\",\n+        \"//xla:comparison_util\",\n+        \"//xla:error_spec\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"fusion_emitter_shared_dialect_test\",\n     srcs = if_gpu_is_configured([\"fusion_emitter_shared_dialect_test.cc\"]),"
        },
        {
            "sha": "5dc9461d79f3b3a16729d53699ca1a7d0e29392d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms_legacy_test.cc",
            "status": "added",
            "additions": 1992,
            "deletions": 0,
            "changes": 1992,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_legacy_test.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -0,0 +1,1992 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <cstddef>\n+#include <cstdint>\n+#include <cstdlib>\n+#include <initializer_list>\n+#include <iostream>\n+#include <iterator>\n+#include <limits>\n+#include <memory>\n+#include <numeric>\n+#include <string>\n+#include <tuple>\n+#include <utility>\n+#include <variant>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/match.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_replace.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/time/clock.h\"\n+#include \"absl/time/time.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n+#include \"xla/backends/gpu/profiler/kernel_name_tracer.h\"\n+#include \"xla/error_spec.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n+#include \"xla/hlo/testlib/filecheck.h\"\n+#include \"xla/hlo/testlib/verified_hlo_module.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/service/dump.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tests/test_utils.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class AlgorithmTest : public GpuCodegenTest {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_autotune_level(0);\n+    // Use legacy Triton emitter for these tests by removing all generic\n+    // Triton emitter features\n+    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n+    return debug_options;\n+  }\n+\n+  std::string HloModuleTestName() const {\n+    auto test_info = ::testing::UnitTest::GetInstance()->current_test_info();\n+    return absl::StrReplaceAll(\n+        absl::StrCat(test_info->test_suite_name(), \"_\", test_info->name()),\n+        {{\"/\", \"_\"}});\n+  }\n+\n+  stream_executor::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+\n+  const stream_executor::GpuComputeCapability& GpuComputeComp() {\n+    return device_desc().gpu_compute_capability();\n+  }\n+\n+ protected:\n+  const stream_executor::DeviceDescription& device_desc() {\n+    return backend().default_stream_executor()->GetDeviceDescription();\n+  }\n+};\n+\n+// In these tests, we depend on \"algorithm\" annotations for selecting the 6XBF16\n+// algorithm.\n+class Triton6xBF16GemmTest : public AlgorithmTest {\n+ protected:\n+  void SetUp() override {\n+    if (!SupportsBF16(GpuComputeComp())) {\n+      GTEST_SKIP() << \"BF16 not supported.\";\n+    }\n+  }\n+};\n+\n+class BlasAlgorithmTest : public AlgorithmTest {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = AlgorithmTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_enable_triton_gemm(false);\n+    return debug_options;\n+  }\n+};\n+\n+using TritonAlgorithmTest = AlgorithmTest;\n+\n+TEST_F(AlgorithmTest, Algorithm3xBF16) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm3xBF16\n+\n+    ENTRY e {\n+      p0 = f32[128,128] parameter(0)\n+      p1 = f32[128,128] parameter(1)\n+      ROOT dot = f32[128,128] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x3\n+    }\n+  )\";\n+  EXPECT_TRUE(\n+      RunAndCompare(kHloText, ErrorSpec{/*aabs=*/0.001, /*arel=*/0.001}));\n+}\n+\n+TEST_F(AlgorithmTest, Algorithm6xBF16) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm6xBF16\n+\n+    ENTRY e {\n+      p0 = f32[128,128] parameter(0)\n+      p1 = f32[128,128] parameter(1)\n+      ROOT dot = f32[128,128] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x6\n+    }\n+  )\";\n+  EXPECT_TRUE(\n+      RunAndCompare(kHloText, ErrorSpec{/*aabs=*/0.001, /*arel=*/0.001}));\n+}\n+\n+TEST_F(BlasAlgorithmTest, Algorithm_BF16_BF16_F32) {\n+  // We check that the algorithm is propagated to the BLAS call.\n+  // We also check that the kernel name matches the algorithm for Ampere.\n+  // The algorithm for Hopper is not the one we expect because it uses TF32.\n+\n+  if (!SupportsBF16(GpuComputeComp())) {\n+    GTEST_SKIP() << \"BF16 not supported.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_BF16_BF16_F32\n+\n+    ENTRY main {\n+      lhs = f32[8512,256]{1,0} parameter(0)\n+      rhs = f32[256,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_bf16_bf16_f32,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  constexpr absl::string_view kPattern = R\"(\n+    CHECK:  %convert{{.*}} = bf16[\n+    CHECK:  %convert{{.*}} = bf16[\n+    CHECK: \"algorithm\":\"ALG_UNSET\"\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  absl::StatusOr<std::unique_ptr<KernelNameTracer>> tracer =\n+      KernelNameTracer::Create(\n+          backend().default_stream_executor()->GetPlatform()->id());\n+  if (!tracer.ok()) {\n+    GTEST_SKIP() << \"KernelNameTracer is not implemented.\";\n+  }\n+  tracer.value()->start();\n+  EXPECT_TRUE(Run(std::move(module), /*run_hlo_passes=*/false));\n+  auto kernel_names = tracer.value()->stop();\n+\n+  auto cc = GetCudaComputeCapability();\n+  using CudaComputeCapabilities =\n+      stream_executor::CudaComputeCapability::CudaComputeCapabilities;\n+  switch (cc.major) {\n+    case CudaComputeCapabilities::kBlackwell:\n+      EXPECT_THAT(kernel_names, ::testing::UnorderedElementsAre(\n+                                    ::testing::Eq(\"wrapped_convert\"),\n+                                    ::testing::Eq(\"wrapped_convert_1\"),\n+                                    ::testing::HasSubstr(\"nvjet\")));\n+      break;\n+    case CudaComputeCapabilities::kAmpere:\n+      EXPECT_THAT(kernel_names, ::testing::UnorderedElementsAre(\n+                                    ::testing::Eq(\"wrapped_convert\"),\n+                                    ::testing::Eq(\"wrapped_convert_1\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\")));\n+      break;\n+    case CudaComputeCapabilities::kHopper:\n+      // Convert to bf16+cublas works faster than dot with algorithm.\n+      EXPECT_THAT(kernel_names,\n+                  ::testing::Contains(::testing::HasSubstr(\"wrapped_convert\"))\n+                      .Times(2));\n+      break;\n+    default:\n+      GTEST_SKIP() << \"Unsupported compute capability: \" << cc.major\n+                   << \" has the kernel name: \" << kernel_names[0];\n+  }\n+}\n+\n+TEST_F(AlgorithmTest, Algorithm_BF16_BF16_F32_on_BF16_input_for_multiply) {\n+  if (!SupportsBF16(GpuComputeComp())) {\n+    GTEST_SKIP() << \"BF16 not supported.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_BF16_BF16_F32_with_BF16_input\n+\n+    ENTRY main {\n+      lhs = bf16[256,8512] parameter(0)\n+      rhs = bf16[256,8512] parameter(1)\n+      ROOT dot = f32[256] dot(lhs, rhs),\n+          algorithm=dot_bf16_bf16_f32,\n+          lhs_batch_dims={0},\n+          rhs_batch_dims={0},\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={1}\n+    }\n+  )\";\n+  // Multiply on a100 operates with f32, h100 operates with bf16.\n+  const std::string pattern = R\"(\n+    CHECK:    %[[multiply:.*]] = [[type:.*]][256,8512]{1,0} multiply([[type]]\n+    CHECK:    %[[reduce:.*]] = f32[256]{0} reduce(\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto ok,\n+      RunFileCheck(\n+          module->ToString(HloPrintOptions().set_print_operand_shape(true)),\n+          pattern));\n+  ASSERT_TRUE(ok);\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-7, /*arel=*/1e-7}));\n+}\n+\n+TEST_F(BlasAlgorithmTest, Algorithm_BF16_BF16_F32_X3) {\n+  if (!SupportsBF16(GpuComputeComp())) {\n+    GTEST_SKIP() << \"BF16 not supported.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_BF16_BF16_F32_X3\n+\n+    ENTRY main {\n+      lhs = f32[8512,256]{1,0} parameter(0)\n+      rhs = f32[256,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_bf16_bf16_f32_x3,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  // Single dot was replaced with 3 dots.\n+  constexpr absl::string_view kPattern = R\"(\n+    CHECK-COUNT-3: custom_call_target=\"__cublas$gemm\"\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  absl::StatusOr<std::unique_ptr<KernelNameTracer>> tracer =\n+      KernelNameTracer::Create(\n+          backend().default_stream_executor()->GetPlatform()->id());\n+  if (!tracer.ok()) {\n+    GTEST_SKIP() << \"KernelNameTracer is not implemented.\";\n+  }\n+  tracer.value()->start();\n+  EXPECT_TRUE(Run(std::move(module), /*run_hlo_passes=*/false));\n+  auto kernel_names = tracer.value()->stop();\n+\n+  auto cc = GetCudaComputeCapability();\n+  using CudaComputeCapabilities =\n+      stream_executor::CudaComputeCapability::CudaComputeCapabilities;\n+  switch (cc.major) {\n+    case CudaComputeCapabilities::kAmpere:\n+      EXPECT_THAT(kernel_names, ::testing::UnorderedElementsAre(\n+                                    ::testing::HasSubstr(\"loop_convert_fusion\"),\n+                                    ::testing::HasSubstr(\"loop_convert_fusion\"),\n+                                    ::testing::HasSubstr(\"loop_select_fusion\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\")));\n+      break;\n+    case CudaComputeCapabilities::kHopper:\n+    case CudaComputeCapabilities::kBlackwell:\n+      EXPECT_THAT(kernel_names, ::testing::UnorderedElementsAre(\n+                                    ::testing::HasSubstr(\"loop_convert_fusion\"),\n+                                    ::testing::HasSubstr(\"loop_convert_fusion\"),\n+                                    ::testing::HasSubstr(\"loop_select_fusion\"),\n+                                    ::testing::HasSubstr(\"nvjet\"),\n+                                    ::testing::HasSubstr(\"nvjet\"),\n+                                    ::testing::HasSubstr(\"nvjet\")));\n+      break;\n+    default:\n+      GTEST_SKIP() << \"Unsupported compute capability: \" << cc.major\n+                   << \" has the kernel name: \" << kernel_names[0];\n+  }\n+}\n+\n+TEST_F(BlasAlgorithmTest, Algorithm_BF16_BF16_F32_X6) {\n+  if (!SupportsBF16(GpuComputeComp())) {\n+    GTEST_SKIP() << \"BF16 not supported.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_BF16_BF16_F32_X6\n+\n+    ENTRY main {\n+      lhs = f32[8512,256]{1,0} parameter(0)\n+      rhs = f32[256,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_bf16_bf16_f32_x6,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  // Single dot was replaced with 3 dots.\n+  constexpr absl::string_view kPattern = R\"(\n+    CHECK-COUNT-6: custom_call_target=\"__cublas$gemm\"\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  absl::StatusOr<std::unique_ptr<KernelNameTracer>> tracer =\n+      KernelNameTracer::Create(\n+          backend().default_stream_executor()->GetPlatform()->id());\n+  if (!tracer.ok()) {\n+    GTEST_SKIP() << \"KernelNameTracer is not implemented.\";\n+  }\n+  tracer.value()->start();\n+  EXPECT_TRUE(Run(std::move(module), /*run_hlo_passes=*/false));\n+  auto kernel_names = tracer.value()->stop();\n+\n+  auto cc = GetCudaComputeCapability();\n+  using CudaComputeCapabilities =\n+      stream_executor::CudaComputeCapability::CudaComputeCapabilities;\n+  switch (cc.major) {\n+    case CudaComputeCapabilities::kAmpere:\n+      EXPECT_THAT(kernel_names, ::testing::UnorderedElementsAre(\n+                                    ::testing::HasSubstr(\"loop_convert_fusion\"),\n+                                    ::testing::HasSubstr(\"loop_convert_fusion\"),\n+                                    ::testing::HasSubstr(\"loop_select_fusion\"),\n+                                    ::testing::HasSubstr(\"wrapped_add\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\"),\n+                                    ::testing::HasSubstr(\"gemm_bf16_\")));\n+      break;\n+    case CudaComputeCapabilities::kHopper:\n+    case CudaComputeCapabilities::kBlackwell:\n+      EXPECT_THAT(\n+          kernel_names,\n+          ::testing::UnorderedElementsAre(\n+              ::testing::HasSubstr(\"loop_convert_fusion\"),\n+              ::testing::HasSubstr(\"loop_convert_fusion\"),\n+              ::testing::HasSubstr(\"loop_select_fusion\"),\n+              ::testing::HasSubstr(\"wrapped_add\"),\n+              ::testing::HasSubstr(\"nvjet\"), ::testing::HasSubstr(\"nvjet\"),\n+              ::testing::HasSubstr(\"nvjet\"), ::testing::HasSubstr(\"nvjet\"),\n+              ::testing::HasSubstr(\"nvjet\"), ::testing::HasSubstr(\"nvjet\")));\n+      break;\n+    default:\n+      GTEST_SKIP() << \"Unsupported compute capability: \" << cc.major\n+                   << \" has the kernel name: \" << kernel_names[0];\n+  }\n+}\n+\n+TEST_F(BlasAlgorithmTest, Algorithm_TF32_TF32_F32_X3) {\n+  // We check that the algorithm is propagated to the BLAS call.\n+  // We also check that the kernel name matches the algorithm for Ampere.\n+\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_TF32_TF32_F32_X3\n+\n+    ENTRY main {\n+      lhs = f32[8512,256]{1,0} parameter(0)\n+      rhs = f32[256,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_tf32_tf32_f32_x3,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  constexpr absl::string_view kPattern = R\"(\n+      CHECK: custom_call_target=\"__cublas$gemm\"{{.*}}\"algorithm\":\"ALG_DOT_TF32_TF32_F32\"\n+      CHECK: custom_call_target=\"__cublas$gemm\"{{.*}}\"algorithm\":\"ALG_DOT_TF32_TF32_F32\"\n+      CHECK: custom_call_target=\"__cublas$gemm\"{{.*}}\"algorithm\":\"ALG_DOT_TF32_TF32_F32\"\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  absl::StatusOr<std::unique_ptr<KernelNameTracer>> tracer =\n+      KernelNameTracer::Create(\n+          backend().default_stream_executor()->GetPlatform()->id());\n+  if (!tracer.ok()) {\n+    GTEST_SKIP() << \"KernelNameTracer is not implemented.\";\n+  }\n+  tracer.value()->start();\n+  EXPECT_TRUE(Run(std::move(module), /*run_hlo_passes=*/false));\n+  auto kernel_names = tracer.value()->stop();\n+\n+  auto cc = GetCudaComputeCapability();\n+  using CudaComputeCapabilities =\n+      stream_executor::CudaComputeCapability::CudaComputeCapabilities;\n+  switch (cc.major) {\n+    case CudaComputeCapabilities::kBlackwell:\n+    case CudaComputeCapabilities::kAmpere:\n+    case CudaComputeCapabilities::kHopper:\n+      EXPECT_THAT(kernel_names, ::testing::UnorderedElementsAre(\n+                                    ::testing::HasSubstr(\"loop_and_subtract\"),\n+                                    ::testing::HasSubstr(\"loop_and_subtract\"),\n+                                    ::testing::HasSubstr(\"loop_select_fusion\"),\n+                                    ::testing::HasSubstr(\"gemm_\"),\n+                                    ::testing::HasSubstr(\"gemm_\"),\n+                                    ::testing::HasSubstr(\"gemm_\")));\n+      break;\n+    default:\n+      GTEST_SKIP() << \"Unsupported compute capability: \" << cc.major\n+                   << \" has the kernel name: \" << kernel_names[0];\n+  }\n+}\n+\n+TEST_F(Triton6xBF16GemmTest, Emit6xBF16GemmWhenBothInputsAreF32) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Emit6xBF16GemmWhenBothInputsAreF32\n+\n+    triton_dot {\n+      p0 = f32[5,7] parameter(0)\n+      p1 = f32[7,33] parameter(1)\n+      ROOT dot = f32[5,33] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x6\n+    }\n+\n+    ENTRY e {\n+      p0 = f32[5,7]{1,0} parameter(0)\n+      p1 = f32[7,33]{1,0} parameter(1)\n+      ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n+        triton_gemm_config:\n+        {\"block_m\":32,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}\n+    }\n+  )\";\n+  TF_ASSERT_OK(\n+      CreateTritonIrAndFileCheckForDot(this, kHloText, \"triton_dot\", R\"(\n+CHECK:          %[[INFINITY:.*]] = arith.constant dense<0x7F800000> : tensor<32x32xf32>\n+CHECK:          %[[C0:.*]] = arith.constant dense<0.000000e+00> : tensor<32x32xf32>\n+CHECK:          %[[LHS_HI_BF16:.*]] = arith.truncf %[[LHS_INPUT:.*]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[LHS_HI_F32:.*]] = arith.extf %[[LHS_HI_BF16]] : tensor<32x32xbf16> to tensor<32x32xf32>\n+CHECK:          %[[LHS_MED_INPUT_F32:.*]] = arith.subf %[[LHS_INPUT]], %[[LHS_HI_F32]] : tensor<32x32xf32>\n+CHECK:          %[[LHS_MED_BF16:.*]] = arith.truncf %[[LHS_MED_INPUT_F32]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[LHS_MED_F32:.*]] = arith.extf %[[LHS_MED_BF16]] : tensor<32x32xbf16> to tensor<32x32xf32>\n+CHECK:          %[[LHS_LOW_INPUT_F32:.*]] = arith.subf %[[LHS_MED_INPUT_F32]], %[[LHS_MED_F32]] : tensor<32x32xf32>\n+CHECK:          %[[LHS_LOW_BF16:.*]] = arith.truncf %[[LHS_LOW_INPUT_F32]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[RHS_HI_BF16:.*]] = arith.truncf %[[RHS_INPUT:.*]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[RHS_HI_F32:.*]] = arith.extf %[[RHS_HI_BF16]] : tensor<32x32xbf16> to tensor<32x32xf32>\n+CHECK:          %[[RHS_MED_INPUT_F32:.*]] = arith.subf %[[RHS_INPUT]], %[[RHS_HI_F32]] : tensor<32x32xf32>\n+CHECK:          %[[RHS_MED_BF16:.*]] = arith.truncf %[[RHS_MED_INPUT_F32]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[RHS_MED_F32:.*]] = arith.extf %[[RHS_MED_BF16]] : tensor<32x32xbf16> to tensor<32x32xf32>\n+CHECK:          %[[RHS_LOW_INPUT_F32:.*]] = arith.subf %[[RHS_MED_INPUT_F32]], %[[RHS_MED_F32]] : tensor<32x32xf32>\n+CHECK:          %[[RHS_LOW_BF16:.*]] = arith.truncf %[[RHS_LOW_INPUT_F32]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK-COUNT-5:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<32x32xbf16> * tensor<32x32xbf16> -> tensor<32x\n+CHECK:          %[[ABS:.*]] = math.absf\n+CHECK:          %[[CMP:.*]] = arith.cmpf ogt, %[[INFINITY]], %[[ABS]] : tensor<32x32xf32>\n+CHECK:          %[[SELECT:.*]] = arith.select %[[CMP]], %{{.*}}, %[[C0]] : tensor<32x32xi1>, tensor<32x32xf32>\n+CHECK:          %[[DOT_LAST:.*]] = tt.dot %{{.*}}, %{{.*}}, %[[SELECT]] : tensor<32x32xbf16> * tensor<32x32xbf16> -> tensor<32x32xf32>\n+CHECK:          %[[ACC:.*]] = arith.addf %[[DOT_LAST]], %[[C0]] : tensor<32x32xf32>\n+    )\"));\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, ErrorSpec{/*aabs=*/1e-6,\n+                                                           /*arel=*/1e-6}));\n+}\n+\n+TEST_F(Triton6xBF16GemmTest, Triton6xBF16GemmWorksForLongContractingDimension) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Triton6xBF16GemmWorksForLongContractingDimension\n+\n+    triton_dot {\n+      p0 = f32[5,2048] parameter(0)\n+      p1 = f32[2048,33] parameter(1)\n+      ROOT dot = f32[5,33] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x6\n+    }\n+\n+    ENTRY e {\n+      p0 = f32[5,2048]{1,0} parameter(0)\n+      p1 = f32[2048,33]{1,0} parameter(1)\n+      ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n+        triton_gemm_config:\n+        {\"block_m\":64,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}\n+    }\n+  )\";\n+  TF_ASSERT_OK(\n+      CreateTritonIrAndFileCheckForDot(this, kHloText, \"triton_dot\", R\"(\n+CHECK-COUNT-6:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<64x32xbf16> * tensor<32x32xbf16> -> tensor<64x32xf32>\n+    )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, ErrorSpec{/*aabs=*/1e-5,\n+                                                           /*arel=*/1e-5}));\n+}\n+\n+TEST_F(Triton6xBF16GemmTest, Emit6xBF16GemmEndToEnd) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"ALG_DOT_BF16_BF16_F32_X6 not supported on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Emit6xBF16GemmEndToEnd\n+\n+    ENTRY e {\n+      p0 = f32[5,32] parameter(0)\n+      p1 = f32[32,7] parameter(1)\n+      ROOT dot = f32[5,7] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x6\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> verified_module,\n+                          ParseAndReturnVerifiedModule(kHloText));\n+  CompileAndOptionallyVerifyPtx(std::move(verified_module),\n+                                R\"(\n+CHECK: mma.sync.aligned.{{.*}}.row.col.f32.bf16.bf16.f32\n+CHECK-NOT: mma.sync.aligned.{{.*}}.row.col.f32.tf32.tf32.f32\n+)\");\n+  EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-6,\n+                                                /*arel=*/1e-6}));\n+}\n+\n+// In these tests, we depend on \"algorithm\" annotations for selecting the 3XBF16\n+// algorithm.\n+using Triton3xBF16GemmTest = AlgorithmTest;\n+\n+TEST_F(Triton3xBF16GemmTest, Emit3xBF16GemmWhenBothInputsAreF32) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Emit3xBF16GemmWhenBothInputsAreF32\n+\n+    triton_dot {\n+      p0 = f32[5,7] parameter(0)\n+      p1 = f32[7,33] parameter(1)\n+      ROOT dot = f32[5,33] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x3\n+    }\n+\n+    ENTRY e {\n+      p0 = f32[5,7]{1,0} parameter(0)\n+      p1 = f32[7,33]{1,0} parameter(1)\n+      ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n+        triton_gemm_config:\n+        {\"block_m\":32,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}\n+    }\n+  )\";\n+  TF_ASSERT_OK(\n+      CreateTritonIrAndFileCheckForDot(this, kHloText, \"triton_dot\", R\"(\n+CHECK:          %[[INFINITY:.*]] = arith.constant dense<0x7F800000> : tensor<32x32xf32>\n+CHECK:          %[[C0:.*]] = arith.constant dense<0.000000e+00> : tensor<32x32xf32>\n+CHECK:          %[[LHS_HI_BF16:.*]] = arith.truncf %[[LHS_INPUT:.*]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[LHS_HI_F32:.*]] = arith.extf %[[LHS_HI_BF16]] : tensor<32x32xbf16> to tensor<32x32xf32>\n+CHECK:          %[[LHS_LOW_INPUT_F32:.*]] = arith.subf %[[LHS_INPUT]], %[[LHS_HI_F32]] : tensor<32x32xf32>\n+CHECK:          %[[LHS_LOW_BF16:.*]] = arith.truncf %[[LHS_LOW_INPUT_F32]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[RHS_HI_BF16:.*]] = arith.truncf %[[RHS_INPUT:.*]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK:          %[[RHS_HI_F32:.*]] = arith.extf %[[RHS_HI_BF16]] : tensor<32x32xbf16> to tensor<32x32xf32>\n+CHECK:          %[[RHS_LOW_INPUT_F32:.*]] = arith.subf %[[RHS_INPUT]], %[[RHS_HI_F32]] : tensor<32x32xf32>\n+CHECK:          %[[RHS_LOW_BF16:.*]] = arith.truncf %[[RHS_LOW_INPUT_F32]] : tensor<32x32xf32> to tensor<32x32xbf16>\n+CHECK-COUNT-2:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<32x32xbf16> * tensor<32x32xbf16> -> tensor<32x32xf32>\n+CHECK:          %[[ABS:.*]] = math.absf\n+CHECK:          %[[CMP:.*]] = arith.cmpf ogt, %[[INFINITY]], %[[ABS]] : tensor<32x32xf32>\n+CHECK:          %[[SELECT:.*]] = arith.select %[[CMP]], %{{.*}}, %[[C0]] : tensor<32x32xi1>, tensor<32x32xf32>\n+CHECK:          %[[DOT_LAST:.*]] = tt.dot %{{.*}}, %{{.*}}, %[[SELECT]] : tensor<32x32xbf16> * tensor<32x32xbf16> -> tensor<32x32xf32>\n+CHECK:          %[[ACC:.*]] = arith.addf %[[DOT_LAST]], %[[C0]] : tensor<32x32xf32>\n+    )\"));\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, ErrorSpec{/*aabs=*/1e-5,\n+                                                           /*arel=*/1e-5}));\n+}\n+\n+TEST_F(Triton3xBF16GemmTest, Triton3xBF16GemmWorksForLongContractingDimension) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Triton3xBF16GemmWorksForLongContractingDimension\n+\n+    triton_dot {\n+      p0 = f32[5,2048] parameter(0)\n+      p1 = f32[2048,33] parameter(1)\n+      ROOT dot = f32[5,33] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x3\n+    }\n+\n+    ENTRY e {\n+      p0 = f32[5,2048]{1,0} parameter(0)\n+      p1 = f32[2048,33]{1,0} parameter(1)\n+      ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n+        triton_gemm_config:\n+        {\"block_m\":64,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}\n+    }\n+  )\";\n+  TF_ASSERT_OK(\n+      CreateTritonIrAndFileCheckForDot(this, kHloText, \"triton_dot\", R\"(\n+CHECK-COUNT-3:  %{{.*}} = tt.dot %{{.*}}, %{{.*}}, %{{.*}} : tensor<64x32xbf16> * tensor<32x32xbf16> -> tensor<64x32xf32>\n+    )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, ErrorSpec{/*aabs=*/1e-4,\n+                                                           /*arel=*/1e-4}));\n+}\n+\n+TEST_F(Triton3xBF16GemmTest, Emit3xBF16GemmEndToEnd) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"ALG_DOT_BF16_BF16_F32_X3 not supported on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Emit3xBF16GemmEndToEnd\n+\n+    ENTRY e {\n+      p0 = f32[5,32] parameter(0)\n+      p1 = f32[32,7] parameter(1)\n+      ROOT dot = f32[5,7] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=dot_bf16_bf16_f32_x3\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> verified_module,\n+                          ParseAndReturnVerifiedModule(kHloText));\n+  CompileAndOptionallyVerifyPtx(std::move(verified_module),\n+                                R\"(\n+CHECK: mma.sync.aligned.{{.*}}.row.col.f32.bf16.bf16.f32\n+CHECK-NOT: mma.sync.aligned.{{.*}}.row.col.f32.tf32.tf32.f32\n+)\");\n+  EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-5,\n+                                                /*arel=*/1e-5}));\n+}\n+\n+TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X3) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_BF16_BF16_F32_X3\n+\n+    ENTRY main {\n+      lhs = f32[8512,64]{1,0} parameter(0)\n+      rhs = f32[64,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_bf16_bf16_f32_x3,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  constexpr absl::string_view kPattern =\n+      R\"(CHECK: \"kind\":\"__triton_gemm\",\"triton_gemm_config\")\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  EXPECT_TRUE(ok);\n+}\n+\n+TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32_X6) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_BF16_BF16_F32_X6\n+\n+    ENTRY main {\n+      lhs = f32[8512,64]{1,0} parameter(0)\n+      rhs = f32[64,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_bf16_bf16_f32_x6,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  constexpr absl::string_view kPattern =\n+      R\"(CHECK: \"kind\":\"__triton_gemm\",\"triton_gemm_config\")\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  EXPECT_TRUE(ok);\n+}\n+\n+TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_TF32_TF32_F32\n+\n+    ENTRY main {\n+      lhs = f32[128,256]{1,0} parameter(0)\n+      rhs = f32[256,128]{1,0} parameter(1)\n+      ROOT dot = f32[128,128]{1,0} dot(lhs, rhs),\n+          algorithm=dot_tf32_tf32_f32,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  constexpr absl::string_view kPattern = R\"(\n+    CHECK: algorithm=dot_tf32_tf32_f32\n+    CHECK: \"kind\":\"__triton_gemm\",\"triton_gemm_config\"\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  EXPECT_TRUE(ok);\n+}\n+\n+TEST_F(TritonAlgorithmTest, Algorithm_TF32_TF32_F32_X3) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_TF32_TF32_F32_X3\n+\n+    ENTRY main {\n+      lhs = f32[8512,64]{1,0} parameter(0)\n+      rhs = f32[64,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_tf32_tf32_f32_x3,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  constexpr absl::string_view kPattern =\n+      R\"(CHECK: \"kind\":\"__triton_gemm\",\"triton_gemm_config\")\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  EXPECT_TRUE(ok);\n+}\n+\n+TEST_F(TritonAlgorithmTest, Algorithm_BF16_BF16_F32) {\n+  if (!SupportsBF16(GpuComputeComp())) {\n+    GTEST_SKIP() << \"BF16 not supported.\";\n+  }\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Triton currently disabled on ROCM.\";\n+  }\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Algorithm_BF16_BF16_F32\n+\n+    ENTRY main {\n+      lhs = f32[8512,64]{1,0} parameter(0)\n+      rhs = f32[64,8512]{1,0} parameter(1)\n+      ROOT dot = f32[8512,8512]{1,0} dot(lhs, rhs),\n+          algorithm=dot_bf16_bf16_f32,\n+          lhs_contracting_dims={1},\n+          rhs_contracting_dims={0}\n+    }\n+  )\";\n+  constexpr absl::string_view kPattern =\n+      R\"(CHECK: \"kind\":\"__triton_gemm\",\"triton_gemm_config\")\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module->ToString(), kPattern));\n+  EXPECT_TRUE(ok);\n+}\n+\n+TEST_F(TritonAlgorithmTest, Dot_BF16_X6_WithConst) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule Dot_BF16_X6_WithConst\n+\n+    %triton_fusion_dot (p_0: f32[1,258]) -> f32[258] {\n+      %c_1 = f32[] constant(-1.22474492)\n+      %r_1 = f32[1]{0} reshape(f32[] %c_1)\n+      %r_2 = f32[1,1]{1,0} reshape(f32[1]{0} %r_1)\n+      %p_0 = f32[1,258]{1,0} parameter(0)\n+      %r_3 = f32[258]{0} reshape(f32[1,258]{1,0} %p_0)\n+      %r_4 = f32[258,1]{1,0} reshape(f32[258]{0} %r_3)\n+      %dot_0 = f32[1,258]{1,0} dot(f32[1,1]{1,0} %r_2, f32[258,1]{1,0} %r_4),\n+          lhs_contracting_dims={0},\n+          rhs_contracting_dims={1},\n+          algorithm=dot_bf16_bf16_f32_x6\n+      %r_5 = f32[258]{0} reshape(f32[1,258]{1,0} %dot_0)\n+      %c_2 = f32[] constant(0.282094777)\n+      %b_0 = f32[258]{0} broadcast(f32[] %c_2), dimensions={}\n+      ROOT %m_0 = f32[258]{0} multiply(f32[258]{0} %r_5, f32[258]{0} %b_0)\n+    }\n+\n+    ENTRY %entry_computation {\n+      %p_0 = f32[1,258]{1,0} parameter(0)\n+      ROOT %dot = f32[258]{0} fusion(f32[1,258]{1,0} %p_0),\n+        kind=kCustom,\n+        calls=%triton_fusion_dot,\n+        backend_config={\n+          \"operation_queue_id\":\"0\",\n+          \"wait_on_operation_queues\":[],\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\",\n+            \"triton_gemm_config\":{\n+              \"block_m\":\"16\",\n+              \"block_n\":\"256\",\n+              \"block_k\":\"16\",\n+              \"split_k\":\"1\",\n+              \"num_stages\":\"4\",\n+              \"num_warps\":\"4\",\n+              \"num_ctas\":\"1\"\n+            }\n+          },\n+          \"force_earliest_schedule\":false\n+        }\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-6, /*arel=*/1e-6}));\n+}\n+\n+using PC = PrecisionConfig;\n+using ::testing::TestParamInfo;\n+using ::testing::WithParamInterface;\n+\n+std::string AlgorithmTestParamToString(\n+    const TestParamInfo<PC::Algorithm>& info) {\n+  return AlgorithmToString(info.param);\n+}\n+\n+class NumericTestsArguments {\n+ public:\n+  NumericTestsArguments() {\n+    InitInfinityArguments();\n+    InitNaNArguments();\n+    InitLargeExponentArguments();\n+  }\n+  std::vector<Literal*> infinity_arguments_ptrs() {\n+    return to_pointers(infinity_arguments_);\n+  }\n+  const std::vector<Literal>& infinity_arguments() {\n+    return infinity_arguments_;\n+  }\n+  std::vector<Literal*> nan_arguments_ptrs() {\n+    return to_pointers(nan_arguments_);\n+  }\n+  const std::vector<Literal>& nan_arguments() { return nan_arguments_; }\n+  std::vector<Literal*> large_exponent_arguments_ptr() {\n+    return to_pointers(large_exponent_arguments_);\n+  }\n+  const std::vector<Literal>& large_exponent_arguments() {\n+    return large_exponent_arguments_;\n+  }\n+\n+  static constexpr float kLargeExponentFloat = 0x1.0103p72f;\n+\n+ private:\n+  void InitInfinityArguments() {\n+    auto inf = +std::numeric_limits<float>::infinity();\n+    infinity_arguments_.push_back(LiteralUtil::CreateR2<float>(\n+        {{inf, inf, inf, inf, inf, inf, inf, inf},\n+         {inf, inf, inf, inf, inf, inf, inf, inf},\n+         {inf, inf, inf, inf, inf, inf, inf, inf},\n+         {inf, inf, inf, inf, inf, inf, inf, inf},\n+         {inf, inf, inf, inf, inf, inf, inf, inf},\n+         {inf, inf, inf, inf, inf, inf, inf, inf},\n+         {inf, inf, inf, inf, inf, inf, inf, inf},\n+         {inf, inf, inf, inf, inf, inf, inf, inf}}));\n+    auto one = 1.0f;\n+    infinity_arguments_.push_back(LiteralUtil::CreateR2<float>(\n+        {{one, one, one, one, one, one, one, one},\n+         {one, one, one, one, one, one, one, one},\n+         {one, one, one, one, one, one, one, one},\n+         {one, one, one, one, one, one, one, one},\n+         {one, one, one, one, one, one, one, one},\n+         {one, one, one, one, one, one, one, one},\n+         {one, one, one, one, one, one, one, one},\n+         {one, one, one, one, one, one, one, one}}));\n+  }\n+  void InitNaNArguments() {\n+    auto nan = std::numeric_limits<float>::quiet_NaN();\n+    auto inf = +std::numeric_limits<float>::infinity();\n+    auto one = 1.0f;\n+    nan_arguments_.push_back(LiteralUtil::CreateR2<float>(\n+        {{nan, nan, nan, nan, nan, nan, nan, nan},\n+         {nan, nan, nan, nan, nan, nan, nan, nan},\n+         {nan, nan, nan, nan, nan, nan, nan, nan},\n+         {nan, nan, nan, nan, nan, nan, nan, nan},\n+         {nan, nan, nan, nan, nan, nan, nan, nan},\n+         {nan, nan, nan, nan, nan, nan, nan, nan},\n+         {nan, nan, nan, nan, nan, nan, nan, nan},\n+         {nan, nan, nan, nan, nan, nan, nan, nan}}));\n+    nan_arguments_.push_back(LiteralUtil::CreateR2<float>(\n+        {{one, inf, inf, inf, inf, inf, inf, inf},\n+         {one, inf, inf, inf, inf, inf, inf, inf},\n+         {one, inf, inf, inf, inf, inf, inf, inf},\n+         {one, inf, inf, inf, inf, inf, inf, inf},\n+         {one, inf, inf, inf, inf, inf, inf, inf},\n+         {one, inf, inf, inf, inf, inf, inf, inf},\n+         {one, inf, inf, inf, inf, inf, inf, inf},\n+         {one, inf, inf, inf, inf, inf, inf, inf}}));\n+  }\n+\n+  void InitLargeExponentArguments() {\n+    auto le = kLargeExponentFloat;\n+    auto one = 1.0f;\n+    large_exponent_arguments_.push_back(LiteralUtil::CreateR2<float>(\n+        {{le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one}}));\n+    large_exponent_arguments_.push_back(LiteralUtil::CreateR2<float>(\n+        {{le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one},\n+         {-le, one, one, one, one, one, one, one}}));\n+  }\n+\n+  std::vector<Literal*> to_pointers(const std::vector<Literal>& literals) {\n+    std::vector<Literal*> result;\n+    absl::c_transform(\n+        literals, std::back_inserter(result),\n+        [](const Literal& literal) { return const_cast<Literal*>(&literal); });\n+    return result;\n+  }\n+\n+  std::vector<Literal> infinity_arguments_;\n+  std::vector<Literal> nan_arguments_;\n+  std::vector<Literal> large_exponent_arguments_;\n+};\n+\n+class NumericTestsForBlas : public BlasAlgorithmTest,\n+                            public NumericTestsArguments,\n+                            public WithParamInterface<PC::Algorithm> {\n+ public:\n+  NumericTestsForBlas() : BlasAlgorithmTest() {\n+    algorithm_ = AlgorithmToString(GetParam());\n+  }\n+\n+  std::string HloText() const {\n+    return absl::StrFormat(kHloTextTemplate, HloModuleTestName(), algorithm_);\n+  }\n+\n+  static constexpr absl::string_view kPattern = R\"(CHECK: __cublas$gemm)\";\n+\n+  static constexpr absl::string_view kReferenceHloText = R\"(\n+    HloModule %s\n+\n+    ENTRY e {\n+      p0 = f32[8,8] parameter(0)\n+      p1 = f32[8,8] parameter(1)\n+      ROOT dot = f32[8,8] dot(p0, p1),\n+        lhs_contracting_dims={1},\n+        rhs_contracting_dims={0}\n+    }\n+  )\";\n+\n+  // Takes the reference hlo and compiles it for cublas.\n+  std::unique_ptr<HloModule> GetReferenceModuleForCublas() {\n+    auto reference_options = GetDebugOptionsForTest();\n+    reference_options.set_xla_gpu_triton_gemm_any(false);\n+    reference_options.set_xla_gpu_enable_triton_gemm(false);\n+    reference_options.set_xla_gpu_cublas_fallback(true);\n+\n+    HloModuleConfig config;\n+    config.set_debug_options(reference_options);\n+    config.set_replica_count(1);\n+    config.set_num_partitions(1);\n+\n+    auto reference_module =\n+        ParseAndReturnVerifiedModule(kReferenceHloText, config);\n+    CHECK_OK(reference_module.status());\n+\n+    auto optimized_module =\n+        GetOptimizedModule(std::move(reference_module.value()));\n+    CHECK_OK(optimized_module.status());\n+    return std::move(optimized_module.value());\n+  }\n+\n+ private:\n+  static constexpr absl::string_view kHloTextTemplate = R\"(\n+    HloModule %s\n+\n+    ENTRY e {\n+      p0 = f32[8,8] parameter(0)\n+      p1 = f32[8,8] parameter(1)\n+      ROOT dot = f32[8,8] dot(p0, p1),\n+        lhs_contracting_dims={1},\n+        rhs_contracting_dims={0},\n+        algorithm=%s\n+    }\n+  )\";\n+\n+ protected:\n+  std::string algorithm_;\n+};\n+\n+class NumericTestsForTriton : public TritonAlgorithmTest,\n+                              public NumericTestsArguments,\n+                              public WithParamInterface<PC::Algorithm> {\n+ public:\n+  NumericTestsForTriton() : TritonAlgorithmTest() {\n+    algorithm_ = AlgorithmToString(GetParam());\n+  }\n+\n+  std::string HloText() const {\n+    return absl::StrFormat(kHloTextTemplate, HloModuleTestName(), algorithm_);\n+  }\n+\n+  static constexpr absl::string_view kPattern = R\"(CHECK: __triton_gemm)\";\n+\n+ private:\n+  static constexpr absl::string_view kHloTextTemplate = R\"(\n+    HloModule %s\n+\n+    triton_dot {\n+      p0 = f32[8,8] parameter(0)\n+      p1 = f32[8,8] parameter(1)\n+      ROOT dot = f32[8,8] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+        algorithm=%s\n+    }\n+\n+    ENTRY e {\n+      p0 = f32[8,8]{1, 0} parameter(0)\n+      p1 = f32[8,8]{1, 0} parameter(1)\n+      ROOT _ = f32[8,8] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n+        triton_gemm_config:\n+        {\"block_m\":32,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":1, \"num_ctas\":1}}}\n+    }\n+  )\";\n+  std::string algorithm_;\n+};\n+\n+TEST_P(NumericTestsForBlas, Infinity) {\n+  std::string hlo_text = HloText();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(hlo_text));\n+  auto module_text = module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module_text, kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  auto reference_module = GetReferenceModuleForCublas();\n+\n+  EXPECT_TRUE(RunAndCompareTwoModulesReplicated(\n+      std::move(reference_module), std::move(module), infinity_arguments(),\n+      /*run_hlo_passes=*/false,\n+      /*use_threads=*/false, ErrorSpec{/*aabs=*/0, /*arel=*/0}))\n+      << \" failed for module hlo: \\n\"\n+      << module_text;\n+}\n+\n+TEST_P(NumericTestsForBlas, NaN) {\n+  std::string hlo_text = HloText();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(hlo_text));\n+  auto module_text = module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module_text, kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  auto reference_module = GetReferenceModuleForCublas();\n+\n+  EXPECT_TRUE(RunAndCompareTwoModulesReplicated(\n+      std::move(reference_module), std::move(module), nan_arguments(),\n+      /*run_hlo_passes=*/false,\n+      /*use_threads=*/false, ErrorSpec{/*aabs=*/0, /*arel=*/0}))\n+      << \" failed for module hlo: \\n\"\n+      << module_text;\n+}\n+\n+TEST_P(NumericTestsForBlas, InputsWithLargeExponent) {\n+  std::string hlo_text = HloText();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(hlo_text));\n+  auto module_text = module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module_text, kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  auto reference_module = GetReferenceModuleForCublas();\n+\n+  EXPECT_TRUE(RunAndCompareTwoModulesReplicated(\n+      std::move(reference_module), std::move(module),\n+      large_exponent_arguments(),\n+      /*run_hlo_passes=*/false,\n+      /*use_threads=*/false,\n+      ErrorSpec{/*aabs=*/kLargeExponentFloat * 1e-4, /*arel=*/1e-6}))\n+      << \" failed for module hlo: \\n\"\n+      << module_text;\n+}\n+\n+TEST_P(NumericTestsForBlas, PrecisionCheck) {\n+  std::string hlo_text = HloText();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(hlo_text));\n+  auto module_text = module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module_text, kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  auto reference_module = GetReferenceModuleForCublas();\n+\n+  // No specific inputs are needed for this test.\n+  EXPECT_TRUE(RunAndCompareTwoModulesReplicated(\n+      std::move(reference_module), std::move(module),\n+      /*run_hlo_passes=*/false,\n+      /*use_threads=*/false, ErrorSpec{/*aabs=*/1e-4, /*arel=*/1e-4}))\n+      << \" failed for module hlo: \\n\"\n+      << module_text;\n+}\n+\n+TEST_P(NumericTestsForTriton, Infinity) {\n+  // The test proves that Triton can handle dot for one x infinity inputs.\n+  // It is the tricky cases for X3 and X6 algorithms. They should mask the NaN\n+  // intermediate results.\n+  std::string hlo_text = HloText();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(hlo_text));\n+  auto module_text = module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module_text, kPattern));\n+  ASSERT_TRUE(ok);\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module),\n+                                       infinity_arguments_ptrs(),\n+                                       ErrorSpec{/*aabs=*/0, /*arel=*/0}))\n+      << \" failed for module hlo: \\n\"\n+      << module_text;\n+}\n+\n+TEST_P(NumericTestsForTriton, NaN) {\n+  std::string hlo_text = HloText();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(hlo_text));\n+\n+  auto module_text = module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module_text, kPattern));\n+  ASSERT_TRUE(ok);\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), nan_arguments_ptrs(),\n+                                       ErrorSpec{/*aabs=*/0, /*arel=*/0}))\n+      << \" failed for module hlo: \\n\"\n+      << module_text;\n+}\n+\n+TEST_P(NumericTestsForTriton, InputsWithLargeExponent) {\n+  std::string hlo_text = HloText();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(hlo_text));\n+  auto module_text = module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto ok, RunFileCheck(module_text, kPattern));\n+  ASSERT_TRUE(ok);\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), large_exponent_arguments_ptr(),\n+      ErrorSpec{/*aabs=*/kLargeExponentFloat * 1e-4, /*arel=*/1e-6}))\n+      << \" failed for module hlo: \\n\"\n+      << module_text;\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(NumericTestsForBlas, NumericTestsForBlas,\n+                         ::testing::ValuesIn({PC::ALG_DOT_TF32_TF32_F32_X3,\n+                                              PC::ALG_DOT_BF16_BF16_F32_X3,\n+                                              PC::ALG_DOT_BF16_BF16_F32_X6,\n+                                              PC::ALG_DOT_BF16_BF16_F32_X9}),\n+                         AlgorithmTestParamToString);\n+\n+INSTANTIATE_TEST_SUITE_P(NumericTestsForTriton, NumericTestsForTriton,\n+                         ::testing::ValuesIn({PC::ALG_DOT_BF16_BF16_F32_X3,\n+                                              PC::ALG_DOT_BF16_BF16_F32_X6,\n+                                              PC::ALG_DOT_BF16_BF16_F32_X9,\n+                                              PC::ALG_DOT_TF32_TF32_F32_X3}),\n+                         AlgorithmTestParamToString);\n+\n+// Collects the results of a test. The results can be dumped in CSV format.\n+class CSVWriter {\n+ public:\n+  // Appends a value to the current row. If there is no current row, creates a\n+  // new one.\n+  template <typename V>\n+  void appendValue(V v) {\n+    if (results_.empty()) {\n+      results_.emplace_back();\n+    }\n+    results_.back().push_back(absl::StrCat(v));\n+  }\n+\n+  // Appends a new empty row.\n+  void nextRow() { results_.emplace_back(); }\n+\n+  // Appends a row with the given values.\n+  template <typename V>\n+  void appendRow(std::vector<V> v) {\n+    results_.emplace_back();\n+    for (const auto& v : v) {\n+      results_.back().push_back(absl::StrCat(v));\n+    }\n+  }\n+\n+  // Returns the results in CSV format.\n+  std::string GetResult(absl::string_view title,\n+                        absl::string_view delimiter = \", \",\n+                        bool separate_first_row = true) const {\n+    std::vector<size_t> sizes;\n+    size_t columns = 0;\n+    for (const auto& row : results_) {\n+      columns = std::max(columns, row.size());\n+      sizes.resize(columns);\n+      for (int i = 0; i < row.size(); ++i) {\n+        sizes[i] = std::max(sizes[i], row[i].size());\n+      }\n+    }\n+    std::string result = absl::StrCat(title, \"\\n\");\n+    bool first_row = true;\n+    for (const auto& row : results_) {\n+      for (int i = 0; i < row.size(); ++i) {\n+        auto format = absl::StrFormat(\"%%%ds\", sizes[i]);\n+        auto format_runtime = absl::ParsedFormat<'s'>::New(format);\n+        absl::StrAppend(&result, absl::StrFormat(*format_runtime, row[i]),\n+                        delimiter);\n+      }\n+      result += \"\\n\";\n+      if (first_row && separate_first_row) {\n+        first_row = false;\n+        auto total_size = delimiter.size() * (columns - 1);\n+        for (const auto& size : sizes) {\n+          total_size += size;\n+        }\n+        result += std::string(total_size, '-');\n+        result += \"\\n\";\n+      }\n+    }\n+    return result;\n+  }\n+\n+ private:\n+  std::vector<std::vector<std::string>> results_;\n+};\n+\n+// The tests builds a matrix of MxN for different tensor sizes with the values\n+// Yes/No/Fail for triton and blas and dumps the results in CSV format to the\n+// test output.\n+class TritonAndBlasSupportForDifferentTensorSizes\n+    : public WithParamInterface<PC::Algorithm>,\n+      public AlgorithmTest {\n+ public:\n+  static auto GetModuleConfig(const DebugOptions& debug_options) {\n+    HloModuleConfig config;\n+    config.set_debug_options(debug_options);\n+    config.set_replica_count(1);\n+    config.set_num_partitions(1);\n+    return config;\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<HloModule>> GetModule(\n+      absl::string_view hlo_template,\n+      const std::vector<std::pair<std::string, std::string>>& args,\n+      const DebugOptions& options) {\n+    auto config = GetModuleConfig(options);\n+    auto hlo_text = absl::StrReplaceAll(hlo_template, args);\n+\n+    static int counter = 0;\n+\n+    DumpToFileInDirOrStdout(options, ++counter, GetTestName(\"_\"), \"\",\n+                            \"hlo_text.before_passes.txt\", hlo_text);\n+    auto verified_module_or = ParseAndReturnVerifiedModule(hlo_text, config);\n+    if (!verified_module_or.ok()) {\n+      LOG(ERROR) << \"Failed to parse module: \" << verified_module_or.status();\n+      return verified_module_or.status();\n+    }\n+    auto module_or = backend().compiler()->RunHloPasses(\n+        std::move(verified_module_or.value()),\n+        backend().default_stream_executor(), GetAllocator());\n+    if (!module_or.ok()) {\n+      LOG(ERROR) << \"Failed to compile module: \" << module_or.status();\n+    }\n+    DumpToFileInDirOrStdout(options, counter, GetTestName(\"_\"), \"\",\n+                            \"hlo_text.after_passes.txt\",\n+                            module_or.ok() ? module_or.value()->ToString()\n+                                           : module_or.status().message());\n+    return module_or;\n+  };\n+\n+ protected:\n+  void SetUp() override {\n+    AlgorithmTest::SetUp();\n+    debug_options_ = GetDebugOptionsForTest();\n+\n+    triton_options_ = debug_options_;\n+\n+    blas_options_ = debug_options_;\n+    blas_options_.set_xla_gpu_enable_triton_gemm(false);\n+\n+    algorithm_ = AlgorithmToString(GetParam());\n+  }\n+\n+  std::string GetTestName(absl::string_view delimiter) const {\n+    auto test_info = ::testing::UnitTest::GetInstance()->current_test_info();\n+    auto suite_name = test_info->test_suite_name();\n+    std::string test_name = test_info->name();\n+    return absl::StrReplaceAll(absl::StrCat(suite_name, delimiter, test_name),\n+                               {{\"/\", \"_\"}});\n+  }\n+\n+  void DumpResults(const CSVWriter& csv, absl::string_view suffix) {\n+    auto title = absl::StrCat(\"Test name: \", GetTestName(\".\"));\n+    auto result = csv.GetResult(title, \", \");\n+    LOG(ERROR) << \"result: \\n\" << result;\n+\n+    auto test_name = GetTestName(\"_\");\n+    DumpToFileInDirOrStdout(debug_options_, 0, test_name, \"\", suffix, result);\n+  }\n+\n+  DebugOptions debug_options_;\n+\n+  DebugOptions triton_options_;\n+\n+  DebugOptions blas_options_;\n+\n+  std::string algorithm_;\n+\n+  static constexpr absl::string_view kBlasPattern = \"__cublas$gemm\";\n+  static constexpr absl::string_view kTritonGemmPattern = \"__triton_gemm\";\n+  static constexpr int kMaxSize = 8192;\n+  static constexpr int kStepSize = 8;\n+  static constexpr int kMaxK = kMaxSize;\n+};\n+\n+// The test does not fail. It just dumps the results in CSV format.\n+TEST_P(TritonAndBlasSupportForDifferentTensorSizes,\n+       DotThatWillBeConvertedToMultiply) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule ${module_name}\n+\n+    ENTRY e {\n+      p0 = f32[${b},${k}] parameter(0)\n+      p1 = f32[${b},${k}] parameter(1)\n+      ROOT dot = f32[${b}] dot(p0, p1),\n+        lhs_contracting_dims={1},\n+        rhs_contracting_dims={1},\n+        lhs_batch_dims={0},\n+        rhs_batch_dims={0},\n+        algorithm=${algorithm}\n+    }\n+  )\";\n+  CSVWriter csv;\n+  csv.appendValue(\"M/N\");\n+  for (int k = 1; k <= kMaxSize; k *= kStepSize) {\n+    csv.appendValue(k);\n+  }\n+  for (int b = 1; b <= kMaxSize; b *= kStepSize) {\n+    csv.nextRow();\n+    csv.appendValue(b);\n+    for (int k = 1; k <= kMaxSize; k *= kStepSize) {\n+      auto run = [&](absl::string_view backend, absl::string_view pattern,\n+                     const DebugOptions& options) -> absl::string_view {\n+        auto test_name = absl::StrReplaceAll(TestName(), {{\"/\", \"_\"}});\n+        auto module_name =\n+            absl::StrCat(test_name, \"_\", backend, \"_\", b, \"_\", k);\n+        auto module = GetModule(kHloText,\n+                                {{\"${module_name}\", module_name},\n+                                 {\"${algorithm}\", algorithm_},\n+                                 {\"${b}\", absl::StrCat(b)},\n+                                 {\"${k}\", absl::StrCat(k)}},\n+                                options);\n+        if (!module.ok()) {\n+          return \"Fail\";\n+        }\n+        return absl::StrContains(module.value()->ToString(), pattern) ? \" Yes\"\n+                                                                      : \"  No\";\n+      };\n+\n+      csv.appendValue(absl::StrCat(\n+          \"('triton': \", run(\"triton\", kTritonGemmPattern, triton_options_),\n+          \", 'blas': \", run(\"blas\", kBlasPattern, blas_options_), \")\"));\n+    }\n+  }\n+  DumpResults(csv, \"backend_support_matrix\");\n+}\n+\n+// The test does not fail. It just dumps the results in CSV format.\n+TEST_P(TritonAndBlasSupportForDifferentTensorSizes, Regular2DDot) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule ${module_name}\n+\n+    ENTRY e {\n+      p0 = f32[${m},${k}] parameter(0)\n+      p1 = f32[${k},${n}] parameter(1)\n+      ROOT dot = f32[${m},${n}] dot(p0, p1),\n+        lhs_contracting_dims={1},\n+        rhs_contracting_dims={0},\n+        algorithm=${algorithm}\n+    }\n+  )\";\n+  CSVWriter csv;\n+  csv.appendValue(\"M/N\");\n+  for (int n = 1; n <= kMaxSize; n *= kStepSize) {\n+    csv.appendValue(n);\n+  }\n+  for (int m = 1; m <= kMaxSize; m *= kStepSize) {\n+    csv.nextRow();\n+    csv.appendValue(m);\n+    for (int n = 1; n <= kMaxSize; n *= kStepSize) {\n+      LOG(INFO) << \"Running test for m=\" << m << \", n=\" << n;\n+      auto run = [&](std::string backend, absl::string_view pattern,\n+                     const DebugOptions& options) -> absl::string_view {\n+        auto test_name = absl::StrReplaceAll(TestName(), {{\"/\", \"_\"}});\n+        auto module_name = absl::StrCat(test_name, \"_\", backend, \"_\", m, \"_\",\n+                                        kMaxK, \"_\", n, \"_\", algorithm_);\n+        auto module = GetModule(kHloText,\n+                                {{\"${module_name}\", module_name},\n+                                 {\"${algorithm}\", algorithm_},\n+                                 {\"${m}\", absl::StrCat(m)},\n+                                 {\"${n}\", absl::StrCat(n)},\n+                                 {\"${k}\", absl::StrCat(kMaxK)}},\n+                                options);\n+        if (!module.ok()) {\n+          return \"Fail\";\n+        }\n+        return absl::StrContains(module.value()->ToString(), pattern) ? \" Yes\"\n+                                                                      : \"  No\";\n+      };\n+\n+      csv.appendValue(absl::StrCat(\n+          \"('triton': \", run(\"triton\", kTritonGemmPattern, triton_options_),\n+          \", 'blas': \", run(\"blas\", kBlasPattern, blas_options_), \")\"));\n+    }\n+  }\n+  DumpResults(csv, \"backend_support_matrix\");\n+}\n+\n+TEST_P(TritonAndBlasSupportForDifferentTensorSizes,\n+       IsDotAlgorithmSupportedByTriton) {\n+  // Here we test which dot algorithm is supported by triton.\n+  // In case of a change you need to update the expected results.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule ${module_name}\n+\n+    ENTRY e {\n+      p0 = f32[${m},${k}] parameter(0)\n+      p1 = f32[${k},${n}] parameter(1)\n+      ROOT dot = f32[${m},${n}] dot(p0, p1),\n+        lhs_contracting_dims={1},\n+        rhs_contracting_dims={0},\n+        algorithm=${algorithm}\n+    }\n+  )\";\n+  auto m = 128;\n+  auto n = 128;\n+  auto k = 128;\n+  auto run = [&](std::string backend, absl::string_view pattern,\n+                 const DebugOptions& options) -> absl::StatusOr<bool> {\n+    auto test_name = absl::StrReplaceAll(TestName(), {{\"/\", \"_\"}});\n+    auto module_name = absl::StrCat(test_name, \"_\", backend, \"_\", m, \"_\", kMaxK,\n+                                    \"_\", n, \"_\", algorithm_);\n+    auto module = GetModule(kHloText,\n+                            {{\"${module_name}\", module_name},\n+                             {\"${algorithm}\", algorithm_},\n+                             {\"${m}\", absl::StrCat(m)},\n+                             {\"${n}\", absl::StrCat(n)},\n+                             {\"${k}\", absl::StrCat(k)}},\n+                            options);\n+    if (!module.ok()) {\n+      return module.status();\n+    }\n+    std::string module_text = module.value()->ToString();\n+    if (!Run(std::move(module.value()), false)) {\n+      return absl::InternalError(\"failed to run module\");\n+    }\n+    return absl::StrContains(module_text, pattern);\n+  };\n+\n+  auto result_or_status = run(\"triton\", kTritonGemmPattern, triton_options_);\n+  switch (GetParam()) {\n+    case PC::ALG_UNSET:\n+    case PC::ALG_DOT_TF32_TF32_F32:\n+    case PC::ALG_DOT_TF32_TF32_F32_X3:\n+    case PC::ALG_DOT_BF16_BF16_F32:\n+    case PC::ALG_DOT_BF16_BF16_F32_X3:\n+    case PC::ALG_DOT_BF16_BF16_F32_X6:\n+    case PC::ALG_DOT_BF16_BF16_F32_X9:\n+    case PC::ALG_DOT_F32_F32_F32:\n+      ASSERT_TRUE(result_or_status.status().ok())\n+          << \"failed to compile \" << algorithm_;\n+      EXPECT_TRUE(result_or_status.value())\n+          << \"wrong result for \" << algorithm_;\n+      break;\n+    case PC::ALG_DOT_F64_F64_F64:\n+      EXPECT_EQ(result_or_status.status().code(),\n+                absl::StatusCode::kUnimplemented);\n+      break;\n+    default:\n+      EXPECT_TRUE(false) << \"Uncovered algorithm. Please fix: \" << algorithm_;\n+      break;\n+  }\n+}\n+\n+// Applies elementwise absolute value to all arguments to make them\n+// non-negative.\n+void MakeNonNegative(std::vector<Literal>& fake_arguments) {\n+  for (Literal& literal : fake_arguments) {\n+    absl::Span<float> data = literal.data<float>();\n+    for (int i = 0; i < data.size(); ++i) {\n+      data[i] = std::abs(data[i]);\n+    }\n+  }\n+}\n+\n+std::vector<const Literal*> GetLiteralPointers(\n+    const std::vector<Literal>& fake_arguments) {\n+  std::vector<const Literal*> fake_argument_ptrs;\n+  fake_argument_ptrs.reserve(fake_arguments.size());\n+  for (const Literal& literal : fake_arguments) {\n+    fake_argument_ptrs.push_back(&literal);\n+  }\n+  return fake_argument_ptrs;\n+}\n+\n+enum class Backend { kTriton, kBlas };\n+\n+std::string BackendToString(Backend backend) {\n+  switch (backend) {\n+    case Backend::kTriton:\n+      return \"triton\";\n+    case Backend::kBlas:\n+      return \"blas\";\n+    default:\n+      CHECK(false) << \"Uncovered backend. Please fix.\";\n+  }\n+}\n+\n+// Returns the maximum relative error for the algorithm, assuming that the\n+// majority of the error comes from rounding to narrower type, and not error\n+// due to floating point arithmetic calculation. I.e., we assume that:\n+//    <contracting dimension> << <narrowing error> / <fp arithmetic error>\n+// E.g., for BF16xBF16 -> F32, this would mean k << 2^-7 / 2^-23 = 64k\n+double GetMaxRelErrorForSmallContractingDim(Backend backend,\n+                                            PC::Algorithm algorithm) {\n+  // With `ulp` denoting the \"unit in the last place\", and proper floating point\n+  // implementation, the test does k multiplications and then k-1 additions per\n+  // output element. However, we also get an initial error per element due to\n+  // rounding to bf16, or tf32, depending on the algorithm.\n+  //\n+  // Our total error then ends up being k*ulp_f32 + 2*ulp_bf16/tf32.  We can\n+  // look at an example of a dot product of 2-value vectors [a,b] and [x,y], to\n+  // get an intuition for it:\n+  //  (1+ulp_f32)((1+ulp_f32)((1+ulp_bf16)a * (1+ulp_bf16)x)\n+  //      + (1+ulp_f32)((1+ulp_bf16)b * (1+ulp_bf16)y))\n+  //   = (1+ulp_f32)(1+ulp_f32)(1+ulp_bf16)(1+ulp_bf16)(ax+by)\n+  //  ~= (1+2ulp_f32+2ulp_bf16)(ax+by)\n+  //\n+  // In the last equality we discard any higher-order errors because they are\n+  // orders of magnitude smaller than the 1st-order term.\n+  //\n+  // Thus, we get 2*ulp_bf16 because the multiplication adds up the errors of\n+  // the factors, and addition just factors a single error term out. Then we get\n+  // k*ulp_f32 because each \"layer\" of operations adds another rounding error\n+  // (and we have 1 layer of multiplications and k-1 layers of additions).\n+  //\n+  // If we have a small k, such as k=8 then the error bounds are:\n+  //\n+  // BF16xBF16 -> F32: 8*2^-23 + 2*2^-7 = 2^-20 + 2^-6 ~= 1.6e-2\n+  // TF32xTF32 -> F32: 8*2^-23 + 2*2^-10 = 2^-20 + 2^-9 ~= 2.0e-3\n+  //\n+  // Thus, they do not actually depend on k, since f32 has much higher precision\n+  // than the rounding mode.\n+  const absl::flat_hash_map<PC::Algorithm, double> kMaxMeanRelErrorTriton = {\n+      {PC::ALG_DOT_BF16_BF16_F32, 1.6e-2},\n+      {PC::ALG_DOT_TF32_TF32_F32, 2.0e-3},\n+      // TODO: b/407744579 - Understand what the expected error is with various\n+      // precision-recovering algorithms. For now we just use the errors that\n+      // we got assuming that the implementation is correct.\n+      {PC::ALG_DOT_BF16_BF16_F32_X3, 7.9e-6},\n+      {PC::ALG_DOT_BF16_BF16_F32_X6, 1.3e-7},\n+      {PC::ALG_DOT_BF16_BF16_F32_X9, 1.2e-7},\n+      {PC::ALG_DOT_TF32_TF32_F32_X3, 5e-7},\n+      {PC::ALG_DOT_F32_F32_F32, 2e-07}};\n+\n+  const absl::flat_hash_map<PC::Algorithm, double> kMaxMeanRelErrorBlas = {\n+      {PC::ALG_DOT_BF16_BF16_F32, 3.3e-3},\n+      {PC::ALG_DOT_TF32_TF32_F32, 4.1e-4},\n+      {PC::ALG_DOT_BF16_BF16_F32_X3, 2.4e-5},\n+      {PC::ALG_DOT_TF32_TF32_F32_X3, 5e-7},\n+      {PC::ALG_DOT_BF16_BF16_F32_X6, 1.6e-7},\n+      {PC::ALG_DOT_BF16_BF16_F32_X9, 6e-8},\n+      {PC::ALG_DOT_F32_F32_F32, 2e-07}};\n+  if (backend == Backend::kTriton) {\n+    auto max_rel_error_it = kMaxMeanRelErrorTriton.find(algorithm);\n+    CHECK(max_rel_error_it != kMaxMeanRelErrorTriton.end());\n+    return max_rel_error_it->second;\n+  }\n+\n+  if (backend == Backend::kBlas) {\n+    auto max_rel_error_it = kMaxMeanRelErrorBlas.find(algorithm);\n+    CHECK(max_rel_error_it != kMaxMeanRelErrorBlas.end());\n+    return max_rel_error_it->second;\n+  }\n+\n+  CHECK(false) << \"Uncovered backend. Please fix.\";\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    TritonAndBlasSupportForDifferentTensorSizes,\n+    TritonAndBlasSupportForDifferentTensorSizes,\n+    ::testing::ValuesIn(\n+        {PC::ALG_DOT_BF16_BF16_F32, PC::ALG_DOT_BF16_BF16_F32_X3,\n+         PC::ALG_DOT_BF16_BF16_F32_X6, PC::ALG_DOT_BF16_BF16_F32_X9,\n+         PC::ALG_DOT_F32_F32_F32, PC::ALG_DOT_TF32_TF32_F32,\n+         PC::ALG_DOT_TF32_TF32_F32_X3, PC::ALG_DOT_F64_F64_F64, PC::ALG_UNSET}),\n+    AlgorithmTestParamToString);\n+\n+template <typename... Args>\n+void Log(absl::string_view name, const absl::FormatSpec<Args...>& format,\n+         const Args&... args) {\n+  std::cerr << \"stats: \" << name << \" \" << absl::StrFormat(format, args...)\n+            << \"\\n\";\n+}\n+\n+double CalculateStdDev(absl::Span<const double> values, double mean) {\n+  double sum = 0.0;\n+  for (int i = 0; i < values.size(); ++i) {\n+    sum += (values[i] - mean) * (values[i] - mean);\n+  }\n+  return std::sqrt(sum / values.size());\n+}\n+\n+template <typename T>\n+std::vector<double> CalculateRelErrors(absl::Span<T> values,\n+                                       const std::vector<double>& ref_values) {\n+  std::vector<double> rel_errors;\n+  rel_errors.reserve(values.size());\n+  for (int i = 0; i < values.size(); ++i) {\n+    double value = values[i];\n+    double ref_value = ref_values[i];\n+    double rel_error = (value - ref_value) / ref_value;\n+    rel_errors.push_back(rel_error);\n+  }\n+  return rel_errors;\n+}\n+\n+template <typename T>\n+void PrintStats(absl::string_view name, absl::Span<T> values,\n+                const std::vector<double>& expected_values) {\n+  // Build the histogram of the relative differences.\n+  std::vector<double> rel_errors = CalculateRelErrors(values, expected_values);\n+  double max_rel_error =\n+      *std::max_element(rel_errors.begin(), rel_errors.end());\n+  double min_rel_error =\n+      *std::min_element(rel_errors.begin(), rel_errors.end());\n+  double rel_error_range = max_rel_error - min_rel_error;\n+  double rel_error_sum =\n+      std::accumulate(rel_errors.begin(), rel_errors.end(), 0.0);\n+  double mean_rel_error = rel_error_sum / rel_errors.size();\n+  double std_dev_rel_error = CalculateStdDev(rel_errors, mean_rel_error);\n+\n+  int num_bins = std::ceil(std::log2(values.size() + 1));\n+  double bin_width = rel_error_range / num_bins;\n+  std::vector<int> histogram(num_bins, 0);\n+  int samples_count = rel_errors.size();\n+  for (int i = 0; i < rel_errors.size(); ++i) {\n+    int bin = static_cast<int>((rel_errors[i] - min_rel_error) / bin_width);\n+    if (bin >= num_bins) {\n+      bin = num_bins - 1;\n+    }\n+    histogram[bin]++;\n+  }\n+  int max_bin_size = *std::max_element(histogram.begin(), histogram.end());\n+  constexpr int kMaxBarHeight = 100;\n+  int64_t samples = 0;\n+  bool median_found = false;\n+  std::tuple<int, double, double> median_bin;\n+  for (int i = 0; i < num_bins; ++i) {\n+    samples += histogram[i];\n+    double bin_start = min_rel_error + i * bin_width;\n+    double bin_end = min_rel_error + (i + 1) * bin_width;\n+    int bar_size = histogram[i] * kMaxBarHeight / max_bin_size;\n+    std::string bar =\n+        absl::StrCat(std::string(bar_size, '*'), \" \", bar_size, \" \");\n+    if (!median_found && samples >= samples_count / 2) {\n+      median_bin = std::make_tuple(i, bin_start, bin_end);\n+      median_found = true;\n+      bar += \" <--- median\";\n+    }\n+    if (mean_rel_error >= bin_start && mean_rel_error < bin_end) {\n+      bar += \" <--- mean\";\n+    }\n+    if (bin_start <= 0.0 && bin_end >= 0.0) {\n+      bar += \" <--- zero\";\n+    }\n+    std::string line =\n+        absl::StrFormat(\"%2d: [% 1.3e, % 1.3e) %7d %s\\n\", i, bin_start, bin_end,\n+                        histogram[i], bar.c_str());\n+    std::cerr << \"hist: \" << line;\n+  }\n+  double max_abs_rel_error =\n+      std::max(std::abs(min_rel_error), std::abs(max_rel_error));\n+  Log(name, \"min(rel_errors), %1.3e\", min_rel_error);\n+  Log(name, \"max(rel_errors), %1.3e\", max_rel_error);\n+  Log(name, \"max(abs(rel_errors)), %1.3e\", max_abs_rel_error);\n+  Log(name, \"mean(rel_errors), %1.3e\", mean_rel_error);\n+  Log(name, \"std_dev(rel_errors), %1.3e\", std_dev_rel_error);\n+  Log(name, \"CV(rel_errors) = %1.3f\", std_dev_rel_error / mean_rel_error);\n+  Log(name, \"range(rel_errors), %1.3e\", rel_error_range);\n+  Log(name, \"median bin, %d [%1.3e - %1.3e)\", std::get<0>(median_bin),\n+      std::get<1>(median_bin), std::get<2>(median_bin));\n+}\n+\n+class PrecisionTests\n+    : public AlgorithmTest,\n+      public NumericTestsArguments,\n+      public WithParamInterface<::testing::tuple<PC::Algorithm, Backend>> {\n+ public:\n+ protected:\n+  std::vector<double> RunReferenceDot(\n+      const std::vector<const Literal*>& fake_argument_ptrs, int m_size,\n+      int n_size, int k_size) {\n+    absl::Time start = absl::Now();\n+    std::vector<double> ref_result(m_size * n_size, 0.0);\n+    auto lhs = fake_argument_ptrs[0]->data<float>();\n+    auto rhs = fake_argument_ptrs[1]->data<float>();\n+    for (int m = 0; m < m_size; ++m) {\n+      for (int n = 0; n < n_size; ++n) {\n+        for (int k = 0; k < k_size; ++k) {\n+          double lhs_val = lhs[m * k_size + k];\n+          double rhs_val = rhs[n * k_size + k];\n+          ref_result[m * n_size + n] += lhs_val * rhs_val;\n+        }\n+      }\n+    }\n+    auto duration = absl::Now() - start;\n+    std::cerr << \"Reference dot took \" << duration << \" for \" << m_size << \"x\"\n+              << n_size << \"x\" << k_size << \"\\n\";\n+    return ref_result;\n+  }\n+\n+  absl::Status CheckGemmPattern(const HloModule& module,\n+                                absl::string_view pattern) {\n+    TF_ASSIGN_OR_RETURN(bool ok, RunFileCheck(module.ToString(), pattern));\n+    if (!ok) {\n+      return absl::InternalError(\n+          absl::StrCat(\"The module does not contain the pattern: \", pattern));\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<HloModule>> GetSimpleDotModule(\n+      int lhs_outer_dim, int rhs_outer_dim, int contracting_dim,\n+      PC::Algorithm algorithm, Backend backend) {\n+    std::string hlo_text = absl::StrReplaceAll(\n+        kHloTextPattern, {{\"${test_name}\", HloModuleTestName()},\n+                          {\"${m}\", absl::StrCat(lhs_outer_dim)},\n+                          {\"${n}\", absl::StrCat(rhs_outer_dim)},\n+                          {\"${k}\", absl::StrCat(contracting_dim)},\n+                          {\"${algorithm}\", AlgorithmToString(algorithm)}});\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n+                        ParseAndReturnVerifiedModule(hlo_text));\n+    auto debug_options = module->config().debug_options();\n+    debug_options.set_xla_gpu_enable_split_k_autotuning(false);\n+    if (backend == Backend::kTriton) {\n+      debug_options.set_xla_gpu_enable_triton_gemm(true);\n+      debug_options.set_xla_gpu_cublas_fallback(false);\n+    } else if (backend == Backend::kBlas) {\n+      debug_options.set_xla_gpu_enable_triton_gemm(false);\n+      debug_options.set_xla_gpu_cublas_fallback(true);\n+    } else {\n+      return absl::InvalidArgumentError(\"Invalid backend\");\n+    }\n+    module->mutable_config().set_debug_options(debug_options);\n+    TF_ASSIGN_OR_RETURN(module, GetOptimizedModule(std::move(module)));\n+    if (backend == Backend::kTriton) {\n+      TF_RETURN_IF_ERROR(CheckGemmPattern(*module, \"CHECK: __triton_gemm\"));\n+    } else if (backend == Backend::kBlas) {\n+      TF_RETURN_IF_ERROR(CheckGemmPattern(*module, \"CHECK: __cublas$gemm\"));\n+    } else {\n+      return absl::InvalidArgumentError(\"Invalid backend\");\n+    }\n+    return module;\n+  }\n+\n+ private:\n+  static constexpr absl::string_view kHloTextPattern = R\"(\n+    HloModule ${test_name}\n+\n+    ENTRY main {\n+      p0 = f32[${m},${k}]{1,0} parameter(0)\n+      p1 = f32[${n},${k}]{1,0} parameter(1)\n+      ROOT %dot = f32[${m},${n}]{1,0} dot(p0, p1),\n+        lhs_contracting_dims={1},\n+        rhs_contracting_dims={1},\n+        algorithm=${algorithm}\n+    }\n+  )\";\n+};\n+\n+using ::testing::Combine;\n+using ::testing::Values;\n+\n+std::string AlgorithmAndBackendTestParamToString(\n+    const TestParamInfo<::testing::tuple<PC::Algorithm, Backend>>& info) {\n+  PC::Algorithm algorithm = std::get<0>(info.param);\n+  Backend backend = std::get<1>(info.param);\n+  return absl::StrCat(BackendToString(backend), \"_\",\n+                      AlgorithmToString(algorithm));\n+}\n+\n+MATCHER_P(RelativeDifferenceIsWithin, max_rel_difference, \"\") {\n+  double got = std::get<0>(arg);\n+  double expected = std::get<1>(arg);\n+  double rel_difference = std::abs((got - expected) / expected);\n+  *result_listener << \"has relative difference \" << rel_difference << \" = (\"\n+                   << got << \" - \" << expected << \") / \" << expected\n+                   << \" that should be within \" << max_rel_difference;\n+  return rel_difference <= max_rel_difference;\n+}\n+\n+TEST_P(PrecisionTests, PrecisionCheck) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Precision tests is unknown for ROCM.\";\n+  }\n+\n+  PC::Algorithm algorithm = std::get<0>(GetParam());\n+  Backend backend = std::get<1>(GetParam());\n+  if (backend == Backend::kBlas && algorithm == PC::ALG_DOT_F32_F32_F32) {\n+    auto desc = device_desc();\n+    std::cerr << \"platform version: \" << desc.platform_version();\n+    std::cerr << \"driver version: \" << desc.driver_version();\n+    std::cerr << \"runtime version: \" << desc.runtime_version();\n+    std::cerr << \"compile_time_toolkit_version: \"\n+              << desc.compile_time_toolkit_version();\n+    std::cerr << \"Name: \" << desc.name();\n+    EXPECT_THAT(absl::string_view(getenv(\"CUBLAS_EMULATE_SINGLE_PRECISION\")),\n+                ::testing::Eq(\"1\"))\n+        << \"For F32 precision and BLAS, we want to test single precision \"\n+           \"emulation with BF16x9 cublas algorithm. It was introduced in \"\n+           \"cublas 12.9.\";\n+    EXPECT_THAT(absl::string_view(getenv(\"CUBLAS_EMULATION_STRATEGY\")),\n+                ::testing::Eq(\"performant\"));\n+  }\n+  // Use small contracting dimensions to avoid false-negatives due to changing\n+  // contracting dimension tiling factors.\n+  constexpr int kLhsOuterDim = 1024;\n+  constexpr int kRhsOuterDim = 1024;\n+  constexpr int kContractingDim = 8;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> test_module,\n+      GetSimpleDotModule(kLhsOuterDim, kRhsOuterDim, kContractingDim, algorithm,\n+                         backend));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> fake_arguments,\n+      MakeFakeArguments(test_module.get(), /*pseudo_random=*/true,\n+                        /*use_large_range=*/false,\n+                        /*treat_gte_as_data_formatting=*/false,\n+                        /*max_bits_of_precision=*/23));\n+  // Ensure there are no negative arguments to avoid unbounded relative errors\n+  // due to subtracting two similarly large numbers.\n+  MakeNonNegative(fake_arguments);\n+  std::vector<const Literal*> fake_argument_ptrs =\n+      GetLiteralPointers(fake_arguments);\n+  std::vector<double> ref_result = RunReferenceDot(\n+      fake_argument_ptrs, kLhsOuterDim, kRhsOuterDim, kContractingDim);\n+  TF_ASSERT_OK_AND_ASSIGN(auto executable, test_runner().CreateExecutable(\n+                                               std::move(test_module), false));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      Literal test_result,\n+      test_runner().ExecuteWithExecutable(executable.get(), fake_arguments));\n+  ExecutionProfile profile;\n+  std::vector<uint64_t> profile_times;\n+  profile_times.reserve(100);\n+  for (int i = 0; i < 100; ++i) {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        Literal test_result,\n+        test_runner_as_hlo_runner().ExecuteWithExecutableAndProfile(\n+            executable.get(), fake_argument_ptrs, &profile));\n+    profile_times.push_back(profile.compute_time_ns());\n+  }\n+  auto min_time = *std::min_element(profile_times.begin(), profile_times.end());\n+  std::cerr << \"\\n\";\n+  auto name =\n+      absl::StrCat(BackendToString(backend), \"_\", AlgorithmToString(algorithm));\n+  PrintStats(name, test_result.data<float>(), ref_result);\n+  std::cerr << \"stats: \" << name << \" min execution time, \" << min_time\n+            << \"ns\\n\";\n+  std::cerr << \"stats: \\n\";\n+  EXPECT_THAT(llvm::zip(test_result.data<float>(), ref_result),\n+              ::testing::Each(RelativeDifferenceIsWithin(\n+                  GetMaxRelErrorForSmallContractingDim(backend, algorithm))));\n+}\n+\n+TEST_P(PrecisionTests, CheckPrecisionDegradationAlongKDimension) {\n+  // The goal of this test is to show the precision degradation along the\n+  // contracting dimension. We want to check how much the relative error\n+  // increases as we increase the size of the contracting dimension.\n+  if (!VLOG_IS_ON(1)) {\n+    GTEST_SKIP()\n+        << \"Precision degradation is only tested with vlog level > 0.\\n\"\n+        << \"To run the test, set --v=1 and rerun the test.\\n\"\n+        << \"The test is quite slow and produces output for manual inspection.\";\n+  }\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Precision tests is unknown for ROCM.\";\n+  }\n+  Backend backend = std::get<1>(GetParam());\n+  if (backend == Backend::kBlas) {\n+    GTEST_SKIP() << \"Precision degradation is only tested for Triton.\";\n+  }\n+  PC::Algorithm algorithm = std::get<0>(GetParam());\n+  // Use small m and n and go over a range of k.\n+  constexpr int kMSize = 32;\n+  constexpr int kNSize = 32;\n+  constexpr int kMinKSize = 64;\n+  constexpr int kMaxKSize = 1024 * 1024;\n+  CSVWriter csv_writer;\n+  csv_writer.appendRow<std::string>(\n+      {\"iterations_along_k\", \"max(abs(rel_errors))\", \"std_dev(rel_errors)\"});\n+  for (int k = kMinKSize; k <= kMaxKSize; k *= 2) {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::unique_ptr<HloModule> test_module,\n+        GetSimpleDotModule(kMSize, kNSize, k, algorithm, backend));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::vector<Literal> fake_arguments,\n+        MakeFakeArguments(test_module.get(), /*pseudo_random=*/\n+                          true,\n+                          /*use_large_range=*/false,\n+                          /*treat_gte_as_data_formatting=*/false,\n+                          /*max_bits_of_precision=*/23));\n+    // Ensure there are no negative arguments to avoid unbounded relative errors\n+    // due to subtracting two similarly large numbers.\n+    MakeNonNegative(fake_arguments);\n+    std::vector<const Literal*> fake_argument_ptrs =\n+        GetLiteralPointers(fake_arguments);\n+    std::vector<double> ref_result =\n+        RunReferenceDot(fake_argument_ptrs, kMSize, kNSize, k);\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        auto executable,\n+        test_runner().CreateExecutable(std::move(test_module), false));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        Literal test_result,\n+        test_runner().ExecuteWithExecutable(executable.get(), fake_arguments));\n+    std::vector<double> rel_errors =\n+        CalculateRelErrors(test_result.data<float>(), ref_result);\n+    double max_rel_error =\n+        *std::max_element(rel_errors.begin(), rel_errors.end());\n+    double min_rel_error =\n+        *std::min_element(rel_errors.begin(), rel_errors.end());\n+    double max_abs_rel_error =\n+        std::max(std::abs(min_rel_error), std::abs(max_rel_error));\n+    double mean_rel_error =\n+        std::accumulate(rel_errors.begin(), rel_errors.end(), 0.0) /\n+        rel_errors.size();\n+    double std_dev_rel_error = CalculateStdDev(rel_errors, mean_rel_error);\n+    csv_writer.nextRow();\n+    csv_writer.appendValue(k / kMinKSize);\n+    csv_writer.appendValue(absl::StrFormat(\"%1.3e\", max_abs_rel_error));\n+    csv_writer.appendValue(absl::StrFormat(\"%1.3e\", std_dev_rel_error));\n+  }\n+  auto name =\n+      absl::StrCat(BackendToString(backend), \"_\", AlgorithmToString(algorithm));\n+  std::cerr << csv_writer.GetResult(name);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    PrecisionTests, PrecisionTests,\n+    Combine(Values(PC::ALG_DOT_TF32_TF32_F32, PC::ALG_DOT_TF32_TF32_F32_X3,\n+                   PC::ALG_DOT_BF16_BF16_F32, PC::ALG_DOT_BF16_BF16_F32_X3,\n+                   PC::ALG_DOT_BF16_BF16_F32_X6, PC::ALG_DOT_BF16_BF16_F32_X9,\n+                   PC::ALG_DOT_F32_F32_F32),\n+            Values(Backend::kTriton, Backend::kBlas)),\n+    AlgorithmAndBackendTestParamToString);\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "e3ec9bc77579a6268b49154cc96f9d85d45c0c96",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 31,
            "changes": 87,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n@@ -36,13 +35,14 @@ limitations under the License.\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n+#include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -53,14 +53,15 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/kernel_reuse_cache.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n \n namespace xla {\n namespace gpu {\n@@ -176,37 +177,61 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     absl::string_view fusion_kind = backend_config.kind();\n \n     LaunchDimensions launch_dimensions;\n+    if (fusion_kind == kTritonFusionKind ||\n+        fusion_kind == kTritonNestedGemmFusionKind ||\n+        fusion_kind == kTritonScaledDotFusionKind ||\n+        fusion_kind == kTritonCollectiveFusionKind) {\n+      std::optional<LaunchConfig> launch_config;\n+      // Currently GetLaunchConfig will compute the same value as the extracted\n+      // one. They are different only when warp specialization is enabled.\n+      // Ideally we should always pass the thread_dims value extracted from\n+      // the Triton compilation. However, we are keeping the old code path\n+      // to maintain the current behavior and be safe.\n+      if (fusion.GetModule()\n+              ->config()\n+              .debug_options()\n+              .xla_gpu_experimental_enable_triton_warp_specialization()) {\n+        launch_config =\n+            this->GetLaunchConfig(triton_wrapper_result.thread_dims);\n+      } else {\n+        launch_config = this->GetLaunchConfig();\n+      }\n+      // This check should be enforced by `GenerateTritonKernelWrapper`.\n+      CHECK(launch_config.has_value());\n+      launch_dimensions = std::move(launch_config->launch_dimensions);\n+    } else {  // Must be a MatMul\n+      CHECK_EQ(fusion_kind, kTritonGemmFusionKind);\n+      // TODO(bchetioui): port matmul emitter to fully use the new\n+      // infrastructure.\n+      BlockLevelParameters block_level_parameters;\n+      if (!backend_config.has_triton_gemm_config()) {\n+        LOG(WARNING) << \"Using fallback triton GEMM config for op \"\n+                     << fusion.name();\n+        // TODO(bchetioui): deduplicate default matmul config information.\n+        auto& triton_config = *backend_config.mutable_triton_gemm_config();\n+        triton_config.set_block_m(64);\n+        triton_config.set_block_k(64);\n+        triton_config.set_block_n(64);\n+        triton_config.set_split_k(1);\n+        triton_config.set_num_stages(1);\n+        triton_config.set_num_warps(2);\n+        triton_config.set_num_ctas(1);\n+      }\n \n-    // TODO(bchetioui,pifon): this list should be consolidated; why do we need\n-    // so many different fusion kinds?\n-    const std::vector<absl::string_view> kSupportedFusionKinds = {\n-        kTritonFusionKind,\n-        kTritonNestedGemmFusionKind,\n-        kTritonScaledDotFusionKind,\n-        kTritonCollectiveFusionKind,\n-    };\n-\n-    if (!absl::c_linear_search(kSupportedFusionKinds, fusion_kind)) {\n-      return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n-    }\n+      // TODO(bchetioui): move calculation of launch dimensions to\n+      // 'launch_config()'.\n+      TF_ASSIGN_OR_RETURN(\n+          TritonGemmConfig config,\n+          TritonGemmConfig::FromProto(backend_config.triton_gemm_config()));\n \n-    std::optional<LaunchConfig> launch_config;\n-    // Currently GetLaunchConfig will compute the same value as the extracted\n-    // one. They are different only when warp specialization is enabled.\n-    // Ideally we should always pass the thread_dims value extracted from\n-    // the Triton compilation. However, we are keeping the old code path\n-    // to maintain the current behavior and be safe.\n-    if (fusion.GetModule()\n-            ->config()\n-            .debug_options()\n-            .xla_gpu_experimental_enable_triton_warp_specialization()) {\n-      launch_config = this->GetLaunchConfig(triton_wrapper_result.thread_dims);\n-    } else {\n-      launch_config = this->GetLaunchConfig();\n+      TF_ASSIGN_OR_RETURN(auto analysis, TritonFusionAnalysis::Execute(\n+                                             *hlo_computation, config.split_k));\n+\n+      TF_ASSIGN_OR_RETURN(\n+          launch_dimensions,\n+          GetMatMulLaunchDimensions(analysis, analysis_.fusion(), config,\n+                                    analysis_.device_info()));\n     }\n-    // This check should be enforced by `GenerateTritonKernelWrapper`.\n-    CHECK(launch_config.has_value());\n-    launch_dimensions = std::move(launch_config->launch_dimensions);\n \n     llvm::Function* impl_fn =\n         ir_emitter_context.llvm_module()->getFunction(impl_fn_name);"
        },
        {
            "sha": "91f3f75dcb8adef4eb94976610cfd0c886b5deeb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 24,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -98,6 +98,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+#include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n@@ -1644,7 +1645,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n       auto triton_module,\n       ir_emitter_triton_internal::EmitXTileModule(\n           fn_name, TritonEmitterConstraints::GetBuilder(device_info), fusion,\n-          block_level_parameters, symbolic_expr_context));\n+          block_level_parameters, symbolic_expr_context,\n+          ir_emitter_triton_internal::LegacyMatmulEmitter(device_info)));\n \n   const HloComputation* hlo_computation =\n       fusion->fused_instructions_computation();\n@@ -1927,6 +1929,17 @@ std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n \n namespace ir_emitter_triton_internal {\n \n+absl::Status LegacyMatmulEmitter::Emit(\n+    EmitterLocOpBuilder& b, const HloFusionInstruction* fusion,\n+    xtile::EntryFuncOp& fn,\n+    const BlockLevelParameters& block_level_parameters) {\n+  std::string libdevice_path =\n+      GetLibdevicePath(fusion->GetModule()->config(), device_info_);\n+  TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info_, fusion, fn,\n+                                block_level_parameters));\n+  return absl::OkStatus();\n+}\n+\n // TODO(b/447133106): Contrary to the name, this function still does a lot of\n // triton specific things. It should be migrated to use non-triton specific\n // utilities.\n@@ -1935,7 +1948,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context) {\n+    SymbolicExprContext& symbolic_expr_context,\n+    std::optional<LegacyMatmulEmitter> legacy_matmul_emitter) {\n   mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   LoadMlirDialectsForTriton(mlir_context);\n   const auto debug_options = fusion->GetModule()->config().debug_options();\n@@ -1957,25 +1971,6 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n       fusion->backend_config<GpuBackendConfig>()->fusion_backend_config();\n   absl::string_view fusion_kind = backend_config.kind();\n \n-  if (fusion_kind == kTritonGemmFusionKind) {\n-    return Internal(\n-        \"Attempted to emit a GEMM fusion through the legacy Triton \"\n-        \"emitter, but it has been deleted. This is a bug.\");\n-  }\n-\n-  // TODO(bchetioui,pifon): this list should be consolidated; why do we need so\n-  // many different fusion kinds?\n-  const std::vector<absl::string_view> kSupportedFusionKinds = {\n-      kTritonFusionKind,\n-      kTritonNestedGemmFusionKind,\n-      kTritonScaledDotFusionKind,\n-      kTritonCollectiveFusionKind,\n-  };\n-\n-  if (!absl::c_linear_search(kSupportedFusionKinds, fusion_kind)) {\n-    return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n-  }\n-\n   // Build Triton kernel.\n   SmallVector<Type> fn_arg_types;\n   for (HloInstruction* p : hlo_computation->parameter_instructions()) {\n@@ -2015,11 +2010,32 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n   fn.addEntryBlock();\n   b.setInsertionPointToStart(&fn.front());\n \n-  TF_RETURN_IF_ERROR(EmitGeneric(b, emitter_specific_constraints_builder,\n-                                 fusion, fn, block_level_parameters,\n-                                 &symbolic_expr_context));\n+  if (fusion_kind == kTritonGemmFusionKind) {\n+    if (absl::c_contains(\n+            fusion->GetModule()\n+                ->config()\n+                .debug_options()\n+                .xla_gpu_unsupported_generic_triton_emitter_features(),\n+            DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n+      return Internal(\"Legacy GEMM emitter is disabled.\");\n+    }\n+    CHECK(legacy_matmul_emitter.has_value())\n+        << \"emit_legacy_matmul_fn is not set\";\n+    TF_RETURN_IF_ERROR(\n+        legacy_matmul_emitter->Emit(b, fusion, fn, block_level_parameters));\n+  } else if (fusion_kind == kTritonFusionKind ||\n+             fusion_kind == kTritonNestedGemmFusionKind ||\n+             fusion_kind == kTritonScaledDotFusionKind ||\n+             fusion_kind == kTritonCollectiveFusionKind) {\n+    TF_RETURN_IF_ERROR(EmitGeneric(b, emitter_specific_constraints_builder,\n+                                   fusion, fn, block_level_parameters,\n+                                   &symbolic_expr_context));\n+  } else {\n+    return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n+  }\n \n   b.create<xtile::EntryFuncReturnOp>();\n+\n   return triton_module;\n }\n "
        },
        {
            "sha": "8b66e41ce7ff43b94faf103407c812f67b7ec218",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -146,6 +146,21 @@ absl::StatusOr<Tiling> TilingFromAnnotatedFusion(\n     const SymbolicTileAnalysis& symbolic_tile_analysis,\n     const BlockLevelParameters& block_level_parameters);\n \n+// TODO(basioli): Remove this class once the legacy matmul\n+// emitter no longer exists.\n+class LegacyMatmulEmitter {\n+ public:\n+  explicit LegacyMatmulEmitter(const se::DeviceDescription& device_info)\n+      : device_info_(device_info) {}\n+\n+  absl::Status Emit(EmitterLocOpBuilder& b, const HloFusionInstruction* fusion,\n+                    xtile::EntryFuncOp& fn,\n+                    const BlockLevelParameters& block_level_parameters);\n+\n+ private:\n+  const se::DeviceDescription& device_info_;\n+};\n+\n // This function (or its future equivalent) should emit the MLIR module in the\n // shared dialect between XLA:CPU and XLA:GPU. At the moment it is still\n // emitting GPU specific modules. It is currently exposed only for testing\n@@ -156,7 +171,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context);\n+    SymbolicExprContext& symbolic_expr_context,\n+    std::optional<LegacyMatmulEmitter> legacy_matmul_emitter = std::nullopt);\n \n // This function lowers the shared dialect module to Triton. It is exposed for\n // testing with the same motivation as EmitXTileModule."
        },
        {
            "sha": "5d09574131c0e7975fc7fc6ddeef188334b6480c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "added",
            "additions": 4305,
            "deletions": 0,
            "changes": 4305,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7"
        },
        {
            "sha": "aaab8c1d84e1136f2a44be8a87d9ca7379f87d70",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_int4_device_test.cc",
            "status": "added",
            "additions": 1228,
            "deletions": 0,
            "changes": 1228,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_int4_device_test.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -0,0 +1,1228 @@\n+/* Copyright 2023 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <variant>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_replace.h\"\n+#include \"absl/strings/str_split.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n+#include \"xla/error_spec.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/testlib/filecheck.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/types.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class TritonTest : public GpuCodegenTest {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n+    // Do not fall back to cuBLAS, we are testing Triton.\n+    debug_options.set_xla_gpu_cublas_fallback(false);\n+    // Do not autotune split-k by default, since this prevents deterministically\n+    // matching the optimized HLO.\n+    debug_options.set_xla_gpu_enable_split_k_autotuning(false);\n+    // Always rewrite Gemms with Triton regardless of size.\n+    debug_options.set_xla_gpu_gemm_rewrite_size_threshold(0);\n+    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n+    debug_options\n+        .set_xla_gpu_experimental_enable_subchannel_dequantisation_fusion(true);\n+    return debug_options;\n+  }\n+\n+  stream_executor::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+\n+  const stream_executor::GpuComputeCapability& GpuComputeComp() {\n+    return device_desc().gpu_compute_capability();\n+  }\n+  stream_executor::GpuComputeCapability CudaAmpereOrRocm() {\n+    if (GpuComputeComp().IsRocm()) {\n+      return stream_executor::GpuComputeCapability{\n+          device_desc().rocm_compute_capability()};\n+    }\n+    return stream_executor::GpuComputeCapability{\n+        stream_executor::CudaComputeCapability{\n+            stream_executor::CudaComputeCapability::kAmpere, 0}};\n+  }\n+\n+ protected:\n+  const stream_executor::DeviceDescription& device_desc() {\n+    return backend().default_stream_executor()->GetDeviceDescription();\n+  }\n+};\n+\n+TEST_F(TritonTest, DotForInt4vsIdentityBF16ReturnsCorrectResult) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FusedInt4DotBf16Identity\n+\n+    ENTRY entry_computation {\n+      w.s4 = s4[32,32]{1,0:E(4)} parameter(0)\n+      w.f32 = f32[32,32] convert(w.s4)\n+\n+      a = f32[32,32] parameter(1)\n+      ROOT dot = f32[32,32] dot(w.f32, a),\n+        lhs_contracting_dims={1},\n+        rhs_contracting_dims={0}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+\n+  // We check that conversion was fused into gemm fusion.\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    CHECK:  %[[weight_s4:.*]] = s4[32,32]{1,0:E(4)} parameter(0)\n+    CHECK:  %[[weight_f32:.*]] = f32[32,32]{1,0} convert(%[[weight_s4]])\n+    CHECK:  %[[a_f32:.*]] = f32[32,32]{1,0} parameter(1)\n+    CHECK:  ROOT %[[dot:.*]] = f32[32,32]{1,0} dot(%[[weight_f32]], %[[a_f32]])\n+  )\"));\n+\n+  // LHS is a int4 matrix with a clear pattern.\n+  // RHS is a constant identity matrix.\n+  // The result is a matrix with a clear pattern.\n+  TF_ASSERT_OK_AND_ASSIGN(auto lhs,\n+                          (LiteralUtil::CreateLiteralWithGenerator<S4, s4>(\n+                              ShapeUtil::MakeShape(S4, {32, 32}),\n+                              [](absl::Span<const int64_t> indices) {\n+                                return static_cast<s4>(indices[0] % 16 - 8);\n+                              })));\n+  TF_ASSERT_OK_AND_ASSIGN(auto rhs,\n+                          (LiteralUtil::CreateLiteralWithGenerator<F32, float>(\n+                              ShapeUtil::MakeShape(F32, {32, 32}),\n+                              [](absl::Span<const int64_t> indices) {\n+                                return indices[0] == indices[1] ? 1.0f : 0.0f;\n+                              })));\n+  auto computation =\n+      module->GetComputationWithName(\"gemm_fusion_dot_computation\");\n+  ASSERT_NE(computation, nullptr);\n+\n+  constexpr absl::string_view ttir_expectations = R\"(\n+    CHECK:  %[[lhs:.*]] = tt.load %[[lhs_ptr:.*]] : !tt.ptr<tensor<[[lhs_shape:.*]]xi4>>\n+    CHECK:  %[[rhs:.*]] = tt.load %[[rhs_ptr:.*]] : !tt.ptr<tensor<[[rhs_shape:.*]]xf32>>\n+    CHECK:  %[[lhs_i8:.*]] = arith.extsi %[[lhs]] : tensor<[[lhs_shape]]xi4> to tensor<[[lhs_shape]]xi8>\n+    CHECK:  %[[lhs_f32:.*]] = arith.sitofp %[[lhs_i8]] : tensor<[[lhs_shape]]xi8> to tensor<[[lhs_shape]]xf32>\n+    CHECK:  %[[dot:.*]] = tt.dot %[[lhs_f32]], %[[rhs]], %cst, inputPrecision = tf32 : tensor<[[lhs_shape]]xf32> * tensor<[[rhs_shape]]xf32> -> tensor<[[output_shape:.*]]xf32>\n+  )\";\n+  EXPECT_TRUE(\n+      CreateTritonIrAndFileCheckForDot(*computation, ttir_expectations).ok());\n+\n+  // LHS is a int4 matrix with a clear pattern.\n+  // RHS is a constant identity matrix.\n+  // The result is a matrix with a clear pattern.\n+  EXPECT_TRUE(RunAndCompare(std::move(module), {&lhs, &rhs}, {}));\n+}\n+\n+// The following tests are for the channel and subchannel dequantization\n+// fusions. We run the fused version to avoid the hlo passes and prove that\n+// emitters work correctly and unfused version with the goal to fail if an hlo\n+// rewrite broke the dequantization logic.\n+// For the subchannel dequantization there are two cases:\n+// 1. The case where we do:\n+//   broadcast -> multiply -> bitcast -> dot.\n+// 2. The case where we do:\n+//   broadcast -> reshape -> multiply -> dot.\n+// On top of that there could be an additional bitcast between the parameter and\n+// the broadcast.\n+TEST_F(TritonTest, FuseChannelDequantizationFused) {\n+  // This test is a Channel Dequantization fusion.\n+  // We run the fused version to avoid the hlo passes.\n+  // The case where we do:\n+  // param(1) -> bitcast -> broadcast -> multiply -> bitcast -> dot.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseChannelDequantizationFused\n+\n+    fusion {\n+      w.s4 = s4[32,128,256]{2,1,0:E(4)} parameter(0)\n+      w.s8 = s8[32,128,256] convert(w.s4)\n+      w.b16 = bf16[32,128,256] convert(w.s8)\n+\n+      s = bf16[32,1,256] parameter(1)\n+      s.bitcast = bf16[32,256] bitcast(s)\n+      s.broadcast = bf16[32,128,256] broadcast(s.bitcast), dimensions={0,2}\n+      w.scaled = bf16[32,128,256] multiply(w.b16, s.broadcast)\n+      w.scaled.bitcast = bf16[32,2,64,256] bitcast(w.scaled)\n+\n+      a = bf16[1,32,128,2,128] parameter(2)\n+      a.bitcast = bf16[32,128,256] bitcast(bf16[1,32,128,2,128] a)\n+      a.bitcast.2 = bf16[32,2,64,256] bitcast(a.bitcast)\n+      dot = f32[2,32,256,256] dot(w.scaled.bitcast, a.bitcast.2),\n+        lhs_batch_dims={1,0}, lhs_contracting_dims={2},\n+        rhs_batch_dims={1,0}, rhs_contracting_dims={2}\n+      ROOT bitcast = f32[2,32,256,2,1,128] bitcast(f32[2,32,256,256] dot)\n+    }\n+\n+    ENTRY entry_computation {\n+      w.s4 = s4[32,128,256]{2,1,0:E(4)} parameter(0)\n+      s.bf16 = bf16[32,1,256] parameter(1)\n+      a.bf16 = bf16[1,32,128,2,128] parameter(2)\n+      ROOT fusion = f32[2,32,256,2,1,128] fusion(w.s4, s.bf16, a.bf16),\n+          kind=kCustom,\n+          calls=fusion,\n+          backend_config={\n+            \"operation_queue_id\":\"0\",\n+            \"wait_on_operation_queues\":[],\n+            \"fusion_backend_config\":{\n+              \"kind\":\"__triton_gemm\",\n+              \"triton_gemm_config\":{\n+                \"block_m\":\"128\",\n+                \"block_n\":\"128\",\n+                \"block_k\":\"64\",\n+                \"split_k\":\"2\",\n+                \"num_stages\":\"1\",\n+                \"num_warps\":\"8\",\n+                \"num_ctas\":\"1\"\n+              }\n+            },\n+            \"force_earliest_schedule\":false\n+          }\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, FuseSubchannelDequantizationWithTranspose) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseSubchannelDequantizationWithTranspose\n+\n+    ENTRY FuseSubchannelDequantizationWithTranspose {\n+      w_s4 = s4[2,2048,64] parameter(1)\n+      w_s8 = s8[2,2048,64] convert(w_s4)\n+      w_s8_reshaped = s8[2,8,256,64] reshape(w_s8)\n+      w_bf16 = bf16[2,8,256,64] convert(w_s8_reshaped)\n+      s_bf16 = bf16[2,8,1,64]{3,1,0,2} parameter(0)\n+      s_bf16_reshaped = bf16[2,8,64] reshape(s_bf16)\n+      s_bf16_broadcasted = bf16[2,8,256,64] broadcast(s_bf16_reshaped),\n+          dimensions={0,1,3}\n+      w_bf16_scaled = bf16[2,8,256,64] multiply(w_bf16, s_bf16_broadcasted)\n+      w_bf16_scaled_reshaped = bf16[2,2048,64] reshape(w_bf16_scaled)\n+\n+      a_bf16 = bf16[2,2048,2,32] parameter(2)\n+      a_bf16_reshaped = bf16[2,2048,64] reshape(a_bf16)\n+      dot = bf16[2,64,64] dot(w_bf16_scaled_reshaped, a_bf16_reshaped),\n+          lhs_batch_dims={0}, lhs_contracting_dims={1},\n+          rhs_batch_dims={0}, rhs_contracting_dims={1}\n+      dot_reshaped = bf16[2,64,2,32] reshape(dot)\n+      dot_transposed = bf16[64,2,2,32] transpose(dot_reshaped),\n+          dimensions={1,0,2,3}\n+      ROOT root = bf16[2,64,2,32]{3,2,0,1} reshape(dot_transposed)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(kHloText));\n+  if (GetCudaComputeCapability().IsAtLeastHopper()) {\n+    EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+      CHECK:    %[[bitcast:.*]] = bf16[2,8,64]{2,1,0} bitcast({{.*}})\n+      CHECK:    %[[transpose:.*]] = bf16[2,64,8]{2,1,0} transpose(%[[bitcast]]), dimensions={0,2,1}\n+      CHECK:    %[[broadcast:.*]] = bf16[2,64,8,256]{3,2,1,0} broadcast(%[[transpose]]), dimensions={0,1,2}\n+      CHECK:    %[[multiply:.*]] = bf16[2,64,8,256]{3,2,1,0} multiply({{.*}}, %[[broadcast]])\n+    )\"));\n+  }\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), \"CHECK: __triton_gemm\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, FuseSubchannelDequantization) {\n+  // This test is a Subchannel Dequantization fusion.\n+  // We run the non-fused version with the goal to fail if an hlo rewrite broke\n+  // the dequantization logic. The case where we do:\n+  //  param(1) -> reshape -> broadcast -> multiply -> reshape -> dot.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseSubchannelDequantization\n+\n+    ENTRY main {\n+      w = s4[2,2048,32] parameter(0)\n+      w.s8 = s8[2,2048,32] convert(w)\n+      w.b16 = bf16[2,2048,32] convert(w.s8)\n+      w.b16.reshaped = bf16[2,8,256,32] reshape(w.b16)\n+\n+      s = bf16[2,8,1,32] parameter(1)\n+      s.reshaped = bf16[2,8,32] reshape(s)\n+      s.broadcasted = bf16[2,8,256,32] broadcast(s.reshaped), dimensions={0,1,3}\n+      w.scaled = bf16[2,8,256,32] multiply(w.b16.reshaped, s.broadcasted)\n+      w.scaled.reshaped = bf16[2,2048,32] reshape(w.scaled)\n+\n+      a = bf16[2,2,1,2048] parameter(2)\n+      a.reshaped = bf16[2,2,2048] reshape(a)\n+      ROOT dot = f32[2,32,2] dot(w.scaled.reshaped, a.reshaped),\n+          lhs_batch_dims={0}, lhs_contracting_dims={1},\n+          rhs_batch_dims={1}, rhs_contracting_dims={2}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), \"CHECK: __triton_gemm\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, FuseChannelDequantization) {\n+  // This test is a Channel Dequantization fusion.\n+  // We run the non-fused version with the goal to fail if an hlo rewrite broke\n+  // the dequantization logic. The case where we do:\n+  //  param(1) -> bitcast -> broadcast -> multiply -> dot.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseChannelDequantization\n+\n+    ENTRY main {\n+      w.s4 = s4[32,128,256] parameter(0)\n+      w.s8 = s8[32,128,256] convert(w.s4)\n+      w.bf16 = bf16[32,128,256] convert(w.s8)\n+\n+      s = bf16[32,1,256] parameter(1)\n+      s.broadcast = bf16[32,1,256] broadcast(s), dimensions={0,1,2}\n+      s.reshape = bf16[32,256] reshape(s.broadcast)\n+      s.broadcast.2 = bf16[32,128,256] broadcast(s.reshape), dimensions={0,2}\n+      w.scaled = bf16[32,128,256] multiply(w.bf16, s.broadcast.2)\n+\n+      a = bf16[2,1,32,128,128] parameter(2)\n+      ROOT dot = f32[32,256,2,1,128] dot(w.scaled, a),\n+          lhs_batch_dims={0}, lhs_contracting_dims={1},\n+          rhs_batch_dims={2}, rhs_contracting_dims={4}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), \"CHECK: __triton_gemm\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, FuseSubchannelDequantizationFused) {\n+  // This test is a Subchannel Dequantization fusion.\n+  // We run the fused version to avoid the hlo passes.\n+  // The case where we do:\n+  // param -> bitcast -> broadcast -> multiply -> bitcast -> dot.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseSubchannelDequantizationFused\n+\n+    fusion {\n+      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n+      w.s8 = s8[2,2048,32] convert(w.s4)\n+      w.s8.bitcast = s8[2,8,256,32] bitcast(w.s8)\n+      w.bf16 = bf16[2,8,256,32] convert(w.s8.bitcast)\n+\n+      s.bf16 = bf16[2,8,1,32] parameter(1)\n+      s.bf16.bitcast = bf16[2,8,32] bitcast(s.bf16)\n+      s.bf16.broadcast = bf16[2,8,256,32] broadcast(s.bf16.bitcast), dimensions={0,1,3}\n+      w = bf16[2,8,256,32] multiply(w.bf16, s.bf16.broadcast)\n+      w.bitcast = bf16[2,2048,32] bitcast(w)\n+\n+      a = bf16[2,2,1,2048] parameter(2)\n+      a.bitcast = bf16[2,2,2048] bitcast(a)\n+      ROOT dot = f32[2,32,2] dot(w.bitcast, a.bitcast),\n+          lhs_batch_dims={0}, lhs_contracting_dims={1},\n+          rhs_batch_dims={1}, rhs_contracting_dims={2}\n+    } // fusion\n+\n+    ENTRY main {\n+      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n+      s.bf16 = bf16[2,8,1,32] parameter(1)\n+      a.bf16 = bf16[2,2,1,2048] parameter(2)\n+      ROOT fusion = f32[2,32,2] fusion(w.s4, s.bf16, a.bf16),\n+        kind=kCustom,\n+        calls=fusion,\n+        backend_config={\n+          \"operation_queue_id\":\"0\",\n+          \"wait_on_operation_queues\":[],\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\",\n+            \"triton_gemm_config\":{\n+              \"block_m\":16,\n+              \"block_n\":16,\n+              \"block_k\":256,\n+              \"split_k\":1,\n+              \"num_stages\":1,\n+              \"num_warps\":2,\n+              \"num_ctas\":1\n+            }\n+          },\n+          \"force_earliest_schedule\":false\n+        }\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, FuseSubchannelDequantizationFusedWithSmallBlockKSize) {\n+  // This test is a Subchannel Dequantization fusion.\n+  // We run the fused version to avoid the hlo passes.\n+  // The case where we do:\n+  // param -> bitcast -> broadcast -> multiply -> bitcast -> dot.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseSubchannelDequantizationFusedWithSmallBlockKSize\n+\n+    fusion {\n+      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n+      w.s8 = s8[2,2048,32] convert(w.s4)\n+      w.s8.bitcast = s8[2,8,256,32] bitcast(w.s8)\n+      w.bf16 = bf16[2,8,256,32] convert(w.s8.bitcast)\n+\n+      s.bf16 = bf16[2,8,1,32] parameter(1)\n+      s.bf16.bitcast = bf16[2,8,32] bitcast(s.bf16)\n+      s.bf16.broadcast = bf16[2,8,256,32] broadcast(s.bf16.bitcast), dimensions={0,1,3}\n+      w = bf16[2,8,256,32] multiply(w.bf16, s.bf16.broadcast)\n+      w.bitcast = bf16[2,2048,32] bitcast(w)\n+\n+      a = bf16[2,2,1,2048] parameter(2)\n+      a.bitcast = bf16[2,2,2048] bitcast(a)\n+      ROOT dot = f32[2,32,2] dot(w.bitcast, a.bitcast), \n+          lhs_batch_dims={0}, lhs_contracting_dims={1},\n+          rhs_batch_dims={1}, rhs_contracting_dims={2}\n+    } // fusion\n+\n+    ENTRY main {\n+      w.s4 = s4[2,2048,32]{2,1,0:E(4)} parameter(0)\n+      s.bf16 = bf16[2,8,1,32] parameter(1)\n+      a.bf16 = bf16[2,2,1,2048] parameter(2)\n+      ROOT fusion = f32[2,32,2] fusion(w.s4, s.bf16, a.bf16),\n+        kind=kCustom,\n+        calls=fusion,\n+        backend_config={\n+          \"operation_queue_id\":\"0\",\n+          \"wait_on_operation_queues\":[],\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\",\n+            \"triton_gemm_config\":{\n+              \"block_m\":16,\n+              \"block_n\":16,\n+              \"block_k\":128,\n+              \"split_k\":1,\n+              \"num_stages\":1,\n+              \"num_warps\":2,\n+              \"num_ctas\":1\n+            }\n+          },\n+          \"force_earliest_schedule\":false\n+        }\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, FuseBroadcastInPrologue) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseBroadcastInPrologue\n+\n+    ENTRY main {\n+      lhs = bf16[2,1024] parameter(0)\n+      lhs.broadcast = bf16[2,256,1024] broadcast(lhs), dimensions={0,2}\n+\n+      rhs = bf16[2,256,512] parameter(1)\n+\n+      ROOT dot = f32[2,1024,512] dot(lhs.broadcast, rhs),\n+        lhs_batch_dims={0}, lhs_contracting_dims={1},\n+        rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    CHECK:    %[[broadcast:.*]] = bf16[2,256,1024]{2,1,0} broadcast\n+    CHECK:    %[[dot:.*]] = f32[2,1024,512]{2,1,0} dot\n+    CHECK:    ENTRY %main\n+  )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, FuseBroadcastBitcastInPrologue) {\n+  // This test is a Subchannel Dequantization fusion.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseBroadcastBitcastInPrologue\n+\n+    ENTRY main {\n+      lhs = bf16[2,1024] parameter(0)\n+      lhs.broadcast = bf16[2,128,1024] broadcast(lhs), dimensions={0,2}\n+      lhs.bitcast = bf16[256,1024] reshape(lhs.broadcast)\n+\n+      rhs = bf16[256,512] parameter(1)\n+\n+      ROOT dot = f32[1024,512] dot(lhs.bitcast, rhs),\n+        lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    CHECK:    %[[broadcast:.*]] = bf16[2,128,1024]{2,1,0} broadcast\n+    CHECK:    %[[bitcast:.*]] = bf16[256,1024]{1,0} bitcast\n+    CHECK:    ROOT %[[dot:.*]] = f32[1024,512]{1,0} dot\n+    CHECK:    ENTRY %main\n+  )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n+}\n+\n+TEST_F(TritonTest, FuseBroadcastBitcastMultiplyInPrologue) {\n+  // This test is a Subchannel Dequantization fusion.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseBroadcastBitcastMultiplyInPrologue\n+\n+    ENTRY main {\n+      lhs = bf16[2,1024] parameter(0)\n+      lhs.broadcast = bf16[2,128,1024] broadcast(lhs), dimensions={0,2}\n+      lhs.bitcast = bf16[256,1024] reshape(lhs.broadcast)\n+\n+      lhs.weights = s4[256,1024] parameter(1)\n+      lhs.weights.i8 = s8[256,1024] convert(lhs.weights)\n+      lhs.weights.bf16 = bf16[256,1024] convert(lhs.weights.i8)\n+      lhs.weights.scaled = bf16[256,1024] multiply(lhs.bitcast, lhs.weights.bf16)\n+\n+      rhs = bf16[256,512] parameter(2)\n+\n+      ROOT dot = f32[1024,512] dot(lhs.weights.scaled, rhs),\n+        lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    CHECK:    %[[broadcast:.*]] = bf16[{{.*}}]{2,1,0} broadcast\n+    CHECK:    %[[bitcast:.*]] = bf16[{{.*}}]{1,0} bitcast\n+    CHECK:    %[[multiply:.*]] = [[type:.*]][{{.*}}]{1,0} multiply\n+    CHECK:    %[[dot:.*]] = f32[1024,512]{1,0} dot\n+    CHECK:    ENTRY %main\n+  )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n+}\n+\n+TEST_F(TritonTest, DotWithI4WeightsOnLhsWithBitcastTo3dTensor) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule DotWithI4WeightsOnLhsWithBitcastTo3dTensor\n+\n+    fusion {\n+      p0 = s4[256,16]{1,0:E(4)} parameter(0)\n+      p0.2 = bf16[256,16] convert(p0)\n+      p0.3 = bf16[4,64,16] bitcast(p0.2)\n+      p1 = bf16[4,32,64] parameter(1)\n+      ROOT dot = bf16[4,16,32] dot(p0.3, p1),\n+        lhs_batch_dims={0}, lhs_contracting_dims={1},\n+        rhs_batch_dims={0}, rhs_contracting_dims={2}\n+    }\n+\n+    ENTRY entry_computation {\n+      p0 = s4[256,16]{1,0:E(4)} parameter(0)\n+      p1 = bf16[4,32,64] parameter(1)\n+      ROOT dot = bf16[4,16,32] fusion(p0, p1),\n+        kind=kCustom,\n+        calls=fusion,\n+        backend_config={\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\"\n+          }\n+        }\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n+}\n+\n+TEST_F(TritonTest,\n+       DotWithI4WeightsOnLhsWithNonStandardLayoutAndMultplyInEpilogue) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule DotWithI4WeightsOnLhsWithNonStandardLayoutAndMultplyInEpilogue\n+\n+    fusion {\n+      p0 = s4[1,128,32]{1,2,0:E(4)} parameter(0)\n+      p0.1 = s4[1,32,128]{2,1,0:E(4)} bitcast(p0)\n+      p0.2 = bf16[1,32,128] convert(p0.1)\n+      p0.3 = bf16[1,128,32]{1,2,0} bitcast(p0.2)\n+      p1 = bf16[128,1,64] parameter(1)\n+      dot = bf16[1,32,64] dot(p0.3, p1),\n+        lhs_batch_dims={0}, lhs_contracting_dims={1},\n+        rhs_batch_dims={1}, rhs_contracting_dims={0}\n+      p2 = bf16[1,1,32]{2,0,1} parameter(2)\n+      p2.1 = bf16[1,32] bitcast(p2)\n+      p2.2 = bf16[1,32,64] broadcast(p2.1), dimensions={0,1}\n+      m = bf16[1,32,64] multiply(dot, p2.2)\n+      ROOT m.1 = bf16[1,1,32,64] bitcast(m)\n+    }\n+\n+    ENTRY entry_computation {\n+      p0 = s4[1,128,32]{1,2,0:E(4)} parameter(0)\n+      p1 = bf16[128,1,64] parameter(1)\n+      p2 = bf16[1,1,32]{2,0,1} parameter(2)\n+      ROOT gemm_fusion_dot.2 = bf16[1,1,32,64] fusion(p0, p1, p2),\n+        kind=kCustom,\n+        calls=fusion,\n+        backend_config={\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\"\n+          }\n+        }\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n+}\n+\n+TEST_F(TritonTest, DotWithInt4WeightsOnLhsFusedWithMultiplyByChannelScales) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales\n+\n+    DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales {\n+      w = s4[32,64,128] parameter(0)\n+      w.i8 = s8[32,64,128] convert(w)\n+      w.bf16 = bf16[32,64,128] convert(w.i8)\n+      scales = bf16[32,128] parameter(1)\n+      scales.broadcast = bf16[32,64,128] broadcast(scales), dimensions={0,2}\n+      weights.scaled = bf16[32,64,128] multiply(w.bf16, scales.broadcast)\n+      activations = bf16[32,64,256] parameter(2)\n+      ROOT dot = f32[32,128,256] dot(weights.scaled, activations),\n+        lhs_batch_dims={0}, lhs_contracting_dims={1},\n+        rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+\n+    ENTRY main {\n+      w = s4[32,64,128] parameter(0)\n+      scales = bf16[32,128] parameter(1)\n+      p2 = bf16[32,64,256] parameter(2)\n+      ROOT dot = f32[32,128,256] fusion(w, scales, p2),\n+        kind=kCustom,\n+        calls=DotWithI4WeightsOnLhsFusedWithMultiplyByChannelScales,\n+        backend_config={\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\"\n+          }\n+        }\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}));\n+}\n+\n+TEST_F(TritonTest, FuseMultiplyInPrologue) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseMultiplyInPrologue\n+\n+    ENTRY main {\n+      t = (s4[32,64,128], bf16[32,128]{0,1}, bf16[32,64,256]) parameter(0)\n+      w = s4[32,64,128] get-tuple-element(t), index=0\n+      w.i8 = s8[32,64,128] convert(w)\n+      w.bf16 = bf16[32,64,128] convert(w.i8)\n+      scales = bf16[32,128]{0,1} get-tuple-element(t), index=1\n+      scales.broadcast = bf16[32,64,128] broadcast(scales), dimensions={0,2}\n+      weights.scaled = bf16[32,64,128] multiply(w.bf16, scales.broadcast)\n+      activations = bf16[32,64,256] get-tuple-element(t), index=2\n+      ROOT dot = f32[32,128,256] dot(weights.scaled, activations),\n+        lhs_batch_dims={0}, lhs_contracting_dims={1},\n+        rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          GetOptimizedModule(kHloText));\n+  // On Ampere the multiply result type is f32, on Hopper it is bf16.\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    CHECK:    %[[multiply:.*]] = [[type:.*]][{{.*}}]{{.*}} multiply({{.*}}, {{.*}})\n+    CHECK:    %[[dot:.*]] = f32[32,128,256]{2,1,0} dot\n+    CHECK:    ENTRY %main\n+  )\"));\n+}\n+\n+// TODO(b/449140429): Re-enable this test.\n+TEST_F(TritonTest, DISABLED_FuseMultiplyInEpilogue) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule FuseMultiplyInEpilogue\n+\n+    ENTRY main {\n+      p0 = s4[4,32,128]{2,1,0:E(4)} parameter(0)\n+      p0.1 = bf16[4,32,128] convert(p0)\n+      p1 = bf16[4,128,64] parameter(1)\n+      dot = bf16[4,32,64] dot(p0.1, p1),\n+        lhs_batch_dims={0}, lhs_contracting_dims={2},\n+        rhs_batch_dims={0}, rhs_contracting_dims={1}\n+      p2 = bf16[4,32] parameter(2)\n+      p2.1 = bf16[4,32,64] broadcast(p2), dimensions={0,1}\n+      ROOT m = bf16[4,32,64] multiply(dot, p2.1)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+      CHECK:  %[[dot:.*]] = bf16[4,64,32]{1,2,0} dot\n+      CHECK:  %[[multiply:.*]] = [[type:.*]][4,32,64]{2,1,0} multiply\n+      CHECK:  ENTRY %main\n+    )\"));\n+}\n+\n+TEST_F(TritonTest, NonstandardLayoutInt4) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule NonstandardLayoutInt4\n+\n+    ENTRY main {\n+      p0 = s4[64,128]{0,1} parameter(0)\n+      p1 = bf16[256,64] parameter(1)\n+      ROOT dot = bf16[128,256] dot(p0, p1),\n+        lhs_contracting_dims={0}, rhs_contracting_dims={1}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+using ::testing::TestParamInfo;\n+using ::testing::WithParamInterface;\n+\n+struct I4TestParams {\n+  static std::string ToString(const TestParamInfo<I4TestParams>& params) {\n+    return params.param.name;\n+  }\n+\n+  std::string Format(absl::string_view format) const {\n+    return absl::StrReplaceAll(\n+        format, {{\"${name}\", name},\n+                 {\"${lhs}\", lhs},\n+                 {\"${rhs}\", rhs},\n+                 {\"${lhs_contracting_dim}\", absl::StrCat(lhs_contracting_dim)},\n+                 {\"${rhs_contracting_dim}\", absl::StrCat(rhs_contracting_dim)},\n+                 {\"${out}\", out}});\n+  }\n+  bool HasBatchDim() const {\n+    return std::vector<std::string>(absl::StrSplit(lhs, ',')).size() > 2;\n+  }\n+\n+  std::string name;         // The name of the test.\n+  std::string lhs;          // The lhs shape like \"128,16\".\n+  std::string rhs;          // The rhs shape like \"128,256\".\n+  int lhs_contracting_dim;  // The contracting dimension of the lhs.\n+  int rhs_contracting_dim;  // The contracting dimension of the rhs.\n+  std::string out;          // The output shape like \"16,256\".\n+};\n+\n+class ParametrizedTritonTest : public TritonTest,\n+                               public WithParamInterface<I4TestParams> {};\n+\n+TEST_P(ParametrizedTritonTest, Int4WeightsOnTheLhs) {\n+  if (GetParam().HasBatchDim()) {\n+    GTEST_SKIP() << \"2d test ignores batch dim case.\";\n+  }\n+  constexpr absl::string_view kHloTextTemplate = R\"(\n+    HloModule lhs_${name}\n+\n+    lhs_${name} {\n+      w.s4 = s4[${lhs}] parameter(0)\n+      w.s8 = s8[${lhs}] convert(w.s4)\n+      w.bf16 = bf16[${lhs}] convert(w.s8)\n+      a = bf16[${rhs}] parameter(1)\n+      ROOT lhs_${name} = f32[${out}] dot(w.bf16, a),\n+        lhs_contracting_dims={${lhs_contracting_dim}},\n+        rhs_contracting_dims={${rhs_contracting_dim}}\n+    }\n+\n+    ENTRY main {\n+      w = s4[${lhs}] parameter(0)\n+      a = bf16[${rhs}] parameter(1)\n+      ROOT gemm_fusion_dot.2 = f32[${out}] fusion(w, a),\n+        kind=kCustom,\n+        calls=lhs_${name},\n+        backend_config={\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\"\n+          }\n+        }\n+    }\n+  )\";\n+  std::string hlo_text = GetParam().Format(kHloTextTemplate);\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(hlo_text,\n+                                       ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}))\n+      << \"Failed for HLO: \" << hlo_text;\n+}\n+\n+TEST_P(ParametrizedTritonTest, Int4WeightsOnTheLhsWithBatchDim) {\n+  if (!GetParam().HasBatchDim()) {\n+    GTEST_SKIP() << \"3d test ignores 2d case.\";\n+  }\n+  constexpr absl::string_view kHloTextTemplate = R\"(\n+    HloModule ${name}\n+\n+    fusion {\n+      w.s4 = s4[${lhs}] parameter(0)\n+      w.s8 = s8[${lhs}] convert(w.s4)\n+      w.bf16 = bf16[${lhs}] convert(w.s8)\n+      a = bf16[${rhs}] parameter(1)\n+      ROOT dot.0 = f32[${out}] dot(w.bf16, a),\n+        lhs_batch_dims={0}, lhs_contracting_dims={${lhs_contracting_dim}},\n+        rhs_batch_dims={0}, rhs_contracting_dims={${rhs_contracting_dim}}\n+    }\n+\n+    ENTRY gemm_fusion_dot_computation {\n+      w = s4[${lhs}] parameter(0)\n+      a = bf16[${rhs}] parameter(1)\n+      ROOT gemm_fusion_dot.2 = f32[${out}] fusion(w, a),\n+        kind=kCustom,\n+        calls=fusion,\n+        backend_config={\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\"\n+          }\n+        }\n+    }\n+  )\";\n+  std::string hlo_text = GetParam().Format(kHloTextTemplate);\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(hlo_text,\n+                                       ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}))\n+      << \"Failed for HLO: \" << hlo_text;\n+}\n+\n+TEST_P(ParametrizedTritonTest, Int4WeightsOnTheRhs) {\n+  if (GetParam().HasBatchDim()) {\n+    GTEST_SKIP() << \"2d test ignores batch dim case.\";\n+  }\n+\n+  constexpr absl::string_view kHloTextTemplate = R\"(\n+    HloModule rhs_${name}\n+\n+    rhs_${name} {\n+      a = bf16[${lhs}] parameter(0)\n+      w.s4 = s4[${rhs}] parameter(1)\n+      w.s8 = s8[${rhs}] convert(w.s4)\n+      w.bf16 = bf16[${rhs}] convert(w.s8)\n+      ROOT rhs_${name} = f32[${out}] dot(a, w.bf16),\n+        lhs_contracting_dims={${lhs_contracting_dim}},\n+        rhs_contracting_dims={${rhs_contracting_dim}}\n+    }\n+\n+    ENTRY main {\n+      a = bf16[${lhs}] parameter(0)\n+      w = s4[${rhs}] parameter(1)\n+      ROOT rhs_${name} = f32[${out}] fusion(a, w),\n+        kind=kCustom,\n+        calls=rhs_${name},\n+        backend_config={\n+          \"fusion_backend_config\":{\n+            \"kind\":\"__triton_gemm\"\n+          }\n+        }\n+    }\n+  )\";\n+  std::string hlo_text = GetParam().Format(kHloTextTemplate);\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(hlo_text,\n+                                       ErrorSpec{/*aabs=*/1e-5, /*arel=*/1e-5}))\n+      << \"Failed for HLO: \" << hlo_text;\n+}\n+\n+std::vector<I4TestParams> Int4TestCases() {\n+  return {\n+      {\"int4_dot_128_16_x_128_256\", \"128,16\", \"128,256\", 0, 0, \"16,256\"},\n+      {\"int4_dot_128_16_x_256_128\", \"128,16\", \"256,128\", 0, 1, \"16,256\"},\n+      {\"int4_dot_16_128_x_256_128\", \"16,128\", \"256,128\", 1, 1, \"16,256\"},\n+      {\"int4_dot_16_128_x_128_256\", \"16,128\", \"128,256\", 1, 0, \"16,256\"},\n+      {\"int4_dot_1_128_x_256_128\", \"1,128\", \"256,128\", 1, 1, \"1,256\"},\n+      {\"int4_dot_128_1_x_256_128\", \"128,1\", \"256,128\", 0, 1, \"1,256\"},\n+      {\"int4_dot_16_128_x_128_1\", \"16,128\", \"128,1\", 1, 0, \"16,1\"},\n+      {\"int4_dot_16_128_x_1_128\", \"16,128\", \"1,128\", 1, 1, \"16,1\"},\n+\n+      {\"dot_8_128_16_x_8_128_256\", \"8,128,16\", \"8,128,256\", 1, 1, \"8,16,256\"},\n+      {\"dot_8_128_16_x_8_256_128\", \"8,128,16\", \"8,256,128\", 1, 2, \"8,16,256\"},\n+      {\"dot_8_16_128_x_8_256_128\", \"8,16,128\", \"8,256,128\", 2, 2, \"8,16,256\"},\n+      {\"dot_8_16_128_x_8_128_256\", \"8,16,128\", \"8,128,256\", 2, 1, \"8,16,256\"},\n+      {\"dot_8_1_128_x_8_256_128\", \"8,1,128\", \"8,256,128\", 2, 2, \"8,1,256\"},\n+      {\"dot_8_128_1_x_8_256_128\", \"8,128,1\", \"8,256,128\", 1, 2, \"8,1,256\"},\n+      {\"dot_8_16_128_x_8_128_1\", \"8,16,128\", \"8,128,1\", 2, 1, \"8,16,1\"},\n+      {\"dot_8_16_128_x_8_1_128\", \"8,16,128\", \"8,1,128\", 2, 2, \"8,16,1\"},\n+  };\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(ParametrizedTritonTest, ParametrizedTritonTest,\n+                         ::testing::ValuesIn(Int4TestCases()),\n+                         I4TestParams::ToString);\n+\n+TEST_F(TritonTest, NonstandardLayoutWithManyNonContractingDims) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule NonstandardLayoutWithManyNonContractingDims\n+\n+    ENTRY main {\n+          p0 = s4[128,64,192]{1,0,2} parameter(0)\n+          p1 = bf16[256,64] parameter(1)\n+          ROOT dot = bf16[128,192,256] dot(p0, p1),\n+            lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, NonstandardLayoutWithManyNonContractingDimsReversedLayout) {\n+  // We cannot do triton_gemm and we use cuBLAS instead.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule NonstandardLayoutWithManyNonContractingDimsReversedLayout\n+\n+    ENTRY main {\n+          lhs = s4[128,64,192]{0,1,2} parameter(0)\n+          rhs = bf16[256,64] parameter(1)\n+          ROOT dot = bf16[128,192,256] dot(lhs, rhs),\n+            lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, NegatePlusConvertHLO) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule NegatePlusConvertHLO\n+\n+    ENTRY main {\n+      lhs = s4[2,32,64] parameter(0)\n+      lhs_negated = s4[2,32,64] negate(lhs)\n+      lhs_converted = bf16[2,32,64] convert(lhs_negated)\n+      rhs = bf16[2,64,16] parameter(1)\n+      ROOT dot = bf16[2,32,16] dot(lhs_converted, rhs),\n+          lhs_batch_dims={0}, lhs_contracting_dims={2},\n+          rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, RejectTritonFusionForWithMinorBatchDim) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule RejectTritonFusionForWithMinorBatchDim\n+\n+    ENTRY main {\n+      lhs = s4[32,64,2] parameter(0)\n+      lhs_converted = bf16[32,64,2] convert(lhs)\n+      rhs = bf16[2,64,16] parameter(1)\n+      ROOT dot = bf16[2,32,16] dot(lhs_converted, rhs),\n+          lhs_batch_dims={2}, lhs_contracting_dims={1},\n+          rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+  )\";\n+\n+  const std::string pattern =\n+      R\"(CHECK-NOT: \"kind\":\"__triton_gemm\",\"triton_gemm_config\")\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, GetOptimizedModule(kHloText));\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), pattern));\n+}\n+\n+TEST_F(TritonTest, LHSWithMinorDimEqualTo1) {\n+  // We prove that triton can handle int4 dot with non contracting dim size\n+  // equal to 1.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule LHSWithMinorDimEqualTo1\n+\n+    triton_computation {\n+      lhs = s4[2,1024,1] parameter(0)\n+      lhs_converted = bf16[2,1024,1] convert(lhs)\n+      rhs = bf16[2,64,1024] parameter(1)\n+      ROOT dot = bf16[2,1,64] dot(lhs_converted, rhs),\n+          lhs_batch_dims={0}, lhs_contracting_dims={1},\n+          rhs_batch_dims={0}, rhs_contracting_dims={2}\n+    }\n+\n+    ENTRY main {\n+      lhs = s4[2,1024,1] parameter(0)\n+      rhs = bf16[2,64,1024] parameter(1)\n+      ROOT dot = bf16[2,1,64] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, RHSWithMinorDimEqualTo1) {\n+  // We prove that triton can handle int4 dot with non contracting dim size\n+  // equal to 1.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule RHSWithMinorDimEqualTo1\n+\n+    triton_computation {\n+      lhs = bf16[2,1024,64] parameter(0)\n+      rhs = s4[2,1024,1] parameter(1)\n+      rhs_converted = bf16[2,1024,1] convert(rhs)\n+      ROOT dot = bf16[2,64,1] dot(lhs, rhs_converted),\n+          lhs_batch_dims={0}, lhs_contracting_dims={1},\n+          rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+\n+    ENTRY main {\n+      lhs = bf16[2,1024,64] parameter(0)\n+      rhs = s4[2,1024,1] parameter(1)\n+      ROOT dot = bf16[2,64,1] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, LHSNonMinorContractingDim) {\n+  // We prove that triton can handle int4 dot with non minor\n+  // lhs_contracting_dim.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule LHSNonMinorContractingDim\n+\n+    triton_computation {\n+      lhs = s4[1024,8] parameter(0)\n+      lhs_converted = bf16[1024,8] convert(lhs)\n+      rhs = bf16[1024,4] parameter(1)\n+      ROOT dot = bf16[8,4] dot(lhs_converted, rhs),\n+          lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+    }\n+\n+    ENTRY main {\n+      lhs = s4[1024,8] parameter(0)\n+      rhs = bf16[1024,4] parameter(1)\n+      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, LHSNonMinorContractingDimWithBatchDim0) {\n+  // We prove that triton can handle int4 dot with non minor\n+  // lhs_contracting_dim.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule LHSNonMinorContractingDimWithBatchDim0\n+\n+    triton_computation {\n+      lhs = s4[2,1024,8] parameter(0)\n+      lhs_converted = bf16[2,1024,8] convert(lhs)\n+      rhs = bf16[2,1024,4] parameter(1)\n+      ROOT dot = bf16[2,8,4] dot(lhs_converted, rhs),\n+        lhs_batch_dims={0}, lhs_contracting_dims={1},\n+        rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+\n+    ENTRY main {\n+      lhs = s4[2,1024,8] parameter(0)\n+      rhs = bf16[2,1024,4] parameter(1)\n+      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+TEST_F(TritonTest, LHSMinorContractingDim) {\n+  // We prove that triton can handle int4 dot with minor lhs_contracting_dim.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule LHSMinorContractingDim\n+\n+    triton_computation {\n+      lhs = s4[8,1024] parameter(0)\n+      lhs_converted = bf16[8,1024] convert(lhs)\n+      rhs = bf16[1024,4] parameter(1)\n+      ROOT dot = bf16[8,4] dot(lhs_converted, rhs),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    }\n+\n+    ENTRY main {\n+      lhs = s4[8,1024] parameter(0)\n+      rhs = bf16[1024,4] parameter(1)\n+      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, ConvertPlusNegate) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule ConvertPlusNegate\n+\n+    triton_computation {\n+      lhs = s4[8,1024] parameter(0)\n+      lhs_converted = bf16[8,1024] convert(lhs)\n+      lhs_negated = bf16[8,1024] negate(lhs_converted)\n+      rhs = bf16[1024,4] parameter(1)\n+      ROOT dot = bf16[8,4] dot(lhs_negated, rhs),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    }\n+\n+    ENTRY main {\n+      lhs = s4[8,1024] parameter(0)\n+      rhs = bf16[1024,4] parameter(1)\n+      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, LHSMinorContractingDimWithBatchDim0) {\n+  // We prove that triton can handle int4 dot with minor lhs_contracting_dim.\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule LHSMinorContractingDimWithBatchDim0\n+\n+    triton_computation {\n+      lhs = s4[2,8,1024] parameter(0)\n+      lhs_converted = bf16[2,8,1024] convert(lhs)\n+      rhs = bf16[2,1024,4] parameter(1)\n+      ROOT dot = bf16[2,8,4] dot(lhs_converted, rhs),\n+        lhs_batch_dims={0}, lhs_contracting_dims={2},\n+        rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+\n+    ENTRY main {\n+      lhs = s4[2,8,1024] parameter(0)\n+      rhs = bf16[2,1024,4] parameter(1)\n+      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, RHSTestWithNotMinorContractingDim) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule RHSTestWithNotMinorContractingDim\n+\n+    triton_computation {\n+      lhs = bf16[8,1024] parameter(0)\n+      rhs = s4[1024,4] parameter(1)\n+      rhs_converted = bf16[1024,4] convert(rhs)\n+      ROOT dot = bf16[8,4] dot(lhs, rhs_converted),\n+          lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    }\n+\n+    ENTRY main {\n+      lhs = bf16[8,1024] parameter(0)\n+      rhs = s4[1024,4] parameter(1)\n+      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, RHSTestWithMinorContractingDim) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule RHSTestWithMinorContractingDim\n+\n+    triton_computation {\n+      lhs = bf16[8,1024] parameter(0)\n+      rhs = s4[4,1024] parameter(1)\n+      rhs_converted = bf16[4,1024] convert(rhs)\n+      ROOT dot = bf16[8,4] dot(lhs, rhs_converted),\n+          lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+    }\n+\n+    ENTRY main {\n+      lhs = bf16[8,1024] parameter(0)\n+      rhs = s4[4,1024] parameter(1)\n+      ROOT dot = bf16[8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, RHSTestWithMinorContractingDimWithBatchDim) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule RHSTestWithMinorContractingDimWithBatchDim\n+\n+    triton_computation {\n+      lhs = bf16[2,8,1024] parameter(0)\n+      rhs = s4[2,1024,4] parameter(1)\n+      rhs_converted = bf16[2,1024,4] convert(rhs)\n+      ROOT dot = bf16[2,8,4] dot(lhs, rhs_converted),\n+          lhs_batch_dims={0}, lhs_contracting_dims={2},\n+          rhs_batch_dims={0}, rhs_contracting_dims={1}\n+    }\n+\n+    ENTRY main {\n+      lhs = bf16[2,8,1024] parameter(0)\n+      rhs = s4[2,1024,4] parameter(1)\n+      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+TEST_F(TritonTest, RHSTestWithNotMinorContractingDimWithBatchDim0) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule RHSTestWithNotMinorContractingDimWithBatchDim0\n+\n+    triton_computation {\n+      lhs = bf16[2,8,1024] parameter(0)\n+      rhs = s4[2,4,1024] parameter(1)\n+      rhs_converted = bf16[2,4,1024] convert(rhs)\n+      ROOT dot = bf16[2,8,4] dot(lhs, rhs_converted),\n+          lhs_batch_dims={0}, lhs_contracting_dims={2},\n+          rhs_batch_dims={0}, rhs_contracting_dims={2}\n+    }\n+\n+    ENTRY main {\n+      lhs = bf16[2,8,1024] parameter(0)\n+      rhs = s4[2,4,1024] parameter(1)\n+      ROOT dot = bf16[2,8,4] fusion(lhs, rhs), kind=kCustom,\n+        calls=triton_computation,\n+        backend_config={\"fusion_backend_config\": {\"kind\":\"__triton_gemm\"}}\n+    }\n+  )\";\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      kHloText, ErrorSpec{/*aabs=*/1e-2, /*arel=*/1e-2}));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "c997671af8682e00fe0486d6d9cf6c91ced9dd0b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.cc",
            "status": "added",
            "additions": 2039,
            "deletions": 0,
            "changes": 2039,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -0,0 +1,2039 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n+\n+#include <array>\n+#include <climits>\n+#include <cstddef>\n+#include <cstdint>\n+#include <optional>\n+#include <queue>\n+#include <string>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/cord.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/MathExtras.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Math/IR/Math.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Interfaces/FunctionInterfaces.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n+#include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+#include \"xla/comparison_util.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n+#include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/layout.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"xla/mlir_hlo/mhlo/transforms/map_mhlo_to_scalar_op.h\"\n+#include \"xla/mlir_hlo/mhlo/transforms/transformation_helpers.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/service/algorithm_util.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/triton_fusion_analysis.h\"\n+#include \"xla/service/gpu/triton_tiling_propagation.h\"\n+#include \"xla/service/llvm_ir/llvm_util.h\"\n+#include \"xla/service/matmul_indexing_utils.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n+\n+namespace xla::gpu {\n+\n+namespace ma = ::mlir::arith;\n+namespace mm = ::mlir::math;\n+namespace mt = ::mlir::triton;\n+namespace mh = ::mlir::mhlo;\n+\n+using ::llvm::SmallVector;\n+using ::mlir::ArrayRef;\n+using ::mlir::ShapedType;\n+using ::mlir::Type;\n+using ::mlir::Value;\n+using ::mlir::ValueRange;\n+\n+namespace {\n+\n+// Internal indices for the corresponding input scopes.\n+const int kLhsIndex = 0;\n+const int kRhsIndex = 1;\n+\n+absl::StatusOr<Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t) {\n+  switch (t) {\n+    case F64:\n+      return b.getF64Type();\n+    case F32:\n+      return b.getF32Type();\n+    case F16:\n+      return b.getF16Type();\n+    case BF16:\n+      return b.getBF16Type();\n+    case S64:\n+      return b.getI64Type();\n+    case S32:\n+      return b.getI32Type();\n+    case S16:\n+      return b.getI16Type();\n+    case PRED:\n+      return b.getI1Type();\n+    case S8:\n+      return b.getI8Type();\n+    case S4:\n+      return b.getI4Type();\n+    case F8E5M2:\n+      return b.getType<mlir::Float8E5M2Type>();\n+    case F8E4M3FN:\n+      return b.getType<mlir::Float8E4M3FNType>();\n+    default:\n+      return absl::UnimplementedError(\n+          absl::StrCat(\"This type is not supported yet: \",\n+                       primitive_util::LowercasePrimitiveTypeName(t)));\n+  }\n+}\n+\n+Type StorageType(EmitterLocOpBuilder& b, Type t) {\n+  if (t.isInteger(/*width=*/1)) {\n+    return b.getI8Type();\n+  }\n+  return t;\n+}\n+\n+// Create a scalar constant.\n+template <typename T>\n+ma::ConstantOp CreateConst(EmitterLocOpBuilder b, Type type, T value) {\n+  if (mlir::isa<mlir::IntegerType>(type)) {\n+    return b.create<ma::ConstantOp>(b.getIntegerAttr(type, value));\n+  }\n+  if (mlir::isa<mlir::FloatType>(type)) {\n+    return b.create<ma::ConstantOp>(\n+        b.getFloatAttr(type, static_cast<double>(value)));\n+  }\n+  LOG(FATAL) << \"Constant type not supported: \" << llvm_ir::DumpToString(type);\n+}\n+\n+// Create a tensor constant.\n+template <typename T>\n+ma::ConstantOp CreateConst(EmitterLocOpBuilder b, Type type, T value,\n+                           llvm::ArrayRef<int64_t> shape) {\n+  auto tensor_type = mlir::RankedTensorType::get(shape, type);\n+  if (auto int_type = mlir::dyn_cast<mlir::IntegerType>(type)) {\n+    return b.create<ma::ConstantOp>(mlir::DenseElementsAttr::get(\n+        tensor_type,\n+        mlir::APInt(int_type.getIntOrFloatBitWidth(), value,\n+                    /*isSigned=*/std::is_signed_v<T>, /*implicitTrunc=*/true)));\n+  }\n+  if (auto float_type = mlir::dyn_cast<mlir::FloatType>(type)) {\n+    return b.create<ma::ConstantOp>(mlir::DenseElementsAttr::get(\n+        tensor_type, b.getFloatAttr(type, static_cast<double>(value))));\n+  }\n+  LOG(FATAL) << \"Constant type not supported: \" << llvm_ir::DumpToString(type);\n+}\n+\n+Value ZerosLike(EmitterLocOpBuilder b, Value x) {\n+  if (auto src_shaped_ty = mlir::dyn_cast<ShapedType>(x.getType())) {\n+    Type src_ty = src_shaped_ty.getElementType();\n+    return CreateConst(b, src_ty, 0, src_shaped_ty.getShape());\n+  }\n+  return CreateConst(b, x.getType(), 0);\n+}\n+\n+Value OnesLike(EmitterLocOpBuilder b, Value x) {\n+  if (auto src_shaped_ty = mlir::dyn_cast<ShapedType>(x.getType())) {\n+    Type src_ty = src_shaped_ty.getElementType();\n+    return CreateConst(b, src_ty, 1, src_shaped_ty.getShape());\n+  }\n+  return CreateConst(b, x.getType(), 1);\n+}\n+\n+bool IsFp8Type(Type t) {\n+  return llvm::isa<mlir::Float8E5M2Type, mlir::Float8E4M3FNType,\n+                   mlir::Float8E5M2FNUZType, mlir::Float8E4M3FNUZType,\n+                   mlir::Float8E4M3B11FNUZType>(t);\n+}\n+\n+Value Subtract(EmitterLocOpBuilder b, ValueRange values) {\n+  if (mlir::isa<mlir::IntegerType>(mlir::getElementTypeOrSelf(values[0]))) {\n+    return b.create<ma::SubIOp>(values[0], values[1]);\n+  } else {\n+    return b.create<ma::SubFOp>(values[0], values[1]);\n+  }\n+}\n+\n+Value Compare(EmitterLocOpBuilder b, ValueRange values,\n+              mh::ComparisonDirection direction) {\n+  const Type type = mlir::getElementTypeOrSelf(values[0]);\n+  if (mlir::isa<mlir::IntegerType>(type)) {\n+    return b.create<ma::CmpIOp>(mh::impl::getCmpPredicate<ma::CmpIPredicate>(\n+                                    direction,\n+                                    /*isSigned=*/!type.isInteger(1))\n+                                    .value(),\n+                                values[0], values[1]);\n+  }\n+  return b.create<ma::CmpFOp>(\n+      mh::impl::getCmpPredicate<ma::CmpFPredicate>(direction,\n+                                                   /*isSigned=*/true)\n+          .value(),\n+      values[0], values[1]);\n+}\n+\n+Value Maximum(EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n+              ValueRange values) {\n+  if (mlir::isa<mlir::FloatType>(mlir::getElementTypeOrSelf(values[0]))) {\n+    return b.create<ma::MaximumFOp>(values);\n+  }\n+  // logic: isNaN(lhs) || (!isNan(rhs) && lhs >= rhs) ? lhs : rhs\n+  // See also: IEEE Std 754-2008 5.11.\n+  //\n+  // This also works, but we wanted to make it similar to minimum.\n+  // logic: isNaN(lhs) || lhs >= rhs ? lhs : rhs\n+  Value lhs_is_nan =\n+      Compare(b, {values[0], values[0]}, mh::ComparisonDirection::NE);\n+  Value rhs_is_not_nan =\n+      Compare(b, {values[1], values[1]}, mh::ComparisonDirection::EQ);\n+  Value lhs_is_ge = Compare(b, values, mh::ComparisonDirection::GE);\n+  return b.create<ma::SelectOp>(\n+      b.create<ma::OrIOp>(lhs_is_nan,\n+                          b.create<ma::AndIOp>(rhs_is_not_nan, lhs_is_ge)),\n+      values[0], values[1]);\n+}\n+\n+Value Minimum(EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n+              ValueRange values) {\n+  if (mlir::isa<mlir::FloatType>(mlir::getElementTypeOrSelf(values[0]))) {\n+    return b.create<ma::MinimumFOp>(values);\n+  }\n+  // logic: isNaN(lhs) || (!isNan(rhs) && lhs <= rhs) ? lhs : rhs\n+  // See also: IEEE Std 754-2008 5.11.\n+  //\n+  // This should also work, but the tests show that it doesn't work for\n+  // minimum(x, NaN):\n+  // logic: isNaN(lhs) || lhs <= rhs ? lhs : rhs\n+  Value lhs_is_nan =\n+      Compare(b, {values[0], values[0]}, mh::ComparisonDirection::NE);\n+  Value rhs_is_not_nan =\n+      Compare(b, {values[1], values[1]}, mh::ComparisonDirection::EQ);\n+  Value lhs_is_le = Compare(b, values, mh::ComparisonDirection::LE);\n+  return b.create<ma::SelectOp>(\n+      b.create<ma::OrIOp>(lhs_is_nan,\n+                          b.create<ma::AndIOp>(rhs_is_not_nan, lhs_is_le)),\n+      values[0], values[1]);\n+}\n+\n+Value Splat(EmitterLocOpBuilder b, Value value, ArrayRef<int64_t> shape) {\n+  auto type = mlir::RankedTensorType::get(shape, value.getType());\n+  return b.create<mt::SplatOp>(type, value);\n+}\n+\n+absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder b,\n+                                      absl::string_view libdevice_path,\n+                                      const se::DeviceDescription& device_info,\n+                                      const HloInstruction& hlo,\n+                                      ValueRange inputs) {\n+  if (triton::IsSupportedElementwiseLibdeviceFunction(hlo)) {\n+    return triton::EmitElementwiseLibdeviceFunction(b, libdevice_path,\n+                                                    device_info, hlo, inputs);\n+  }\n+  const bool is_integer = mlir::isa<mlir::IntegerType>(\n+      mlir::getElementTypeOrSelf(inputs[0].getType()));\n+\n+  switch (hlo.opcode()) {\n+    case HloOpcode::kCopy:\n+      // Dimension transformations are taken care of separately.\n+      return inputs[0];\n+    case HloOpcode::kAbs:\n+      if (is_integer) {\n+        return b.create<mm::AbsIOp>(inputs[0]);\n+      }\n+      return b.create<mm::AbsFOp>(inputs[0]);\n+    case HloOpcode::kCeil:\n+      return b.create<mm::CeilOp>(inputs[0]);\n+    case HloOpcode::kFloor:\n+      return b.create<mm::FloorOp>(inputs[0]);\n+    case HloOpcode::kNot:\n+      return b.create<ma::XOrIOp>(inputs[0], OnesLike(b, inputs[0]));\n+    case HloOpcode::kNegate:\n+      // NegFOp is not supported by Triton.\n+      return Subtract(b, {ZerosLike(b, inputs[0]), inputs[0]});\n+    case HloOpcode::kConvert: {\n+      TF_ASSIGN_OR_RETURN(Type dst_ty,\n+                          TritonType(b, hlo.shape().element_type()));\n+      return triton::Cast(b, inputs[0], dst_ty);\n+    }\n+    case HloOpcode::kAdd:\n+      if (is_integer) {\n+        return b.create<ma::AddIOp>(inputs[0], inputs[1]);\n+      }\n+      return b.create<ma::AddFOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kSubtract:\n+      return Subtract(b, inputs);\n+    case HloOpcode::kMultiply:\n+      if (is_integer) {\n+        return b.create<ma::MulIOp>(inputs[0], inputs[1]);\n+      }\n+      return b.create<ma::MulFOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kMaximum:\n+      return Maximum(b, device_info, inputs);\n+    case HloOpcode::kMinimum:\n+      return Minimum(b, device_info, inputs);\n+    case HloOpcode::kClamp:\n+      return Maximum(\n+          b, device_info,\n+          {Minimum(b, device_info, {inputs[1], inputs[2]}), inputs[0]});\n+    case HloOpcode::kAnd:\n+      return b.create<ma::AndIOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kOr:\n+      return b.create<ma::OrIOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kXor:\n+      return b.create<ma::XOrIOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kDivide:\n+      if (is_integer) {\n+        // Unsigned not supported yet.\n+        return b.create<ma::DivSIOp>(inputs[0], inputs[1]);\n+      }\n+      return b.create<ma::DivFOp>(inputs[0], inputs[1]);\n+    case HloOpcode::kCompare:\n+      return Compare(\n+          b, inputs,\n+          mh::symbolizeComparisonDirection(\n+              ComparisonDirectionToString(hlo.comparison_direction()))\n+              .value());\n+    case HloOpcode::kSelect:\n+      return b.create<ma::SelectOp>(\n+          Compare(b, {inputs[0], ZerosLike(b, inputs[0])},\n+                  mh::ComparisonDirection::NE),\n+          inputs[1], inputs[2]);\n+    case HloOpcode::kReducePrecision:\n+      return mh::reducePrecision<mt::BitcastOp>(\n+          b.getLoc(), inputs[0], hlo.exponent_bits(), hlo.mantissa_bits(), &b);\n+    default:\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Unsupported elementwise operation \", hlo.ToString()));\n+  }\n+}\n+\n+absl::StatusOr<Value> EmitConstant(EmitterLocOpBuilder b,\n+                                   const HloInstruction& constant) {\n+  CHECK_EQ(constant.opcode(), HloOpcode::kConstant);\n+  CHECK(ShapeUtil::IsEffectiveScalar(constant.shape()));\n+\n+  TF_ASSIGN_OR_RETURN(Type ty, TritonType(b, constant.shape().element_type()));\n+\n+  if (constant.shape().element_type() == U64) {\n+    TF_ASSIGN_OR_RETURN(Literal converted, constant.literal().Convert(U64));\n+    return CreateConst(b, ty, converted.GetFirstElement<uint64_t>());\n+  }\n+\n+  if (constant.shape().AreAllLeavesIntegers()) {\n+    TF_ASSIGN_OR_RETURN(Literal converted, constant.literal().Convert(S64));\n+    return CreateConst(b, ty, converted.GetFirstElement<int64_t>());\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(Literal converted, constant.literal().Convert(F64));\n+  return CreateConst(b, ty, converted.GetFirstElement<double>());\n+}\n+\n+using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n+\n+Value Broadcast(EmitterLocOpBuilder b, TensorValue value,\n+                ArrayRef<int64_t> shape) {\n+  return b.create<mt::BroadcastOp>(value.getType().clone(shape), value);\n+}\n+\n+Value Range(EmitterLocOpBuilder b, int32_t limit) {\n+  auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n+  return b.create<mt::MakeRangeOp>(type, 0, limit);\n+}\n+\n+Value AddPtr(EmitterLocOpBuilder b, Value ptr, Value offset) {\n+  return b.create<mt::AddPtrOp>(ptr.getType(), ptr, offset);\n+}\n+\n+Value EmitParameterLoad(EmitterLocOpBuilder b, Value pointer,\n+                        ArrayRef<int32_t> boundary_checks) {\n+  if (mt::isTensorPointerType(pointer.getType())) {\n+    std::optional<mt::PaddingOption> padding;\n+    if (!boundary_checks.empty()) {\n+      padding = mt::PaddingOption::PAD_ZERO;\n+    }\n+    return b.create<mt::LoadOp>(pointer, boundary_checks, padding,\n+                                mt::CacheModifier::NONE,\n+                                mt::EvictionPolicy::NORMAL,\n+                                /*isVolatile=*/false);\n+  }\n+\n+  // EmitTensorPointer will not create a MakeTensorPtrOp for scalars.\n+  return Splat(b,\n+               b.create<mt::LoadOp>(pointer, mt::CacheModifier::NONE,\n+                                    mt::EvictionPolicy::NORMAL,\n+                                    /*isVolatile=*/false),\n+               {});\n+}\n+\n+// Grouped properties of tiled dimensions used to generate block pointers.\n+struct DimProperties {\n+  DimProperties(int64_t index, Value pid, int block_size, int split_value)\n+      : index(index),\n+        pid(pid),\n+        block_size(block_size),\n+        split_value(split_value) {}\n+\n+  // Logical index of the dimension at the tiling-defining operation.\n+  int64_t index;\n+  // Block program ID corresponding to this dimension.\n+  Value pid;\n+  // Elements of the dimension to process per block program.\n+  int block_size;\n+  // Size of the major part of the dimension if it's split into two parts.\n+  int split_value;\n+};\n+\n+struct Side {\n+  explicit Side(TritonFusionAnalysis::Scope scope,\n+                std::vector<DimProperties> tiled_dims = {},\n+                std::optional<int64_t> batch_dim_idx = std::nullopt)\n+      : scope(scope), tiled_dims(tiled_dims), batch_dim_idx(batch_dim_idx) {}\n+  TritonFusionAnalysis::Scope scope;\n+  std::vector<DimProperties> tiled_dims;\n+  std::optional<int64_t> batch_dim_idx;\n+};\n+\n+absl::StatusOr<Value> EmitBroadcast(EmitterLocOpBuilder b,\n+                                    const TritonFusionAnalysis* analysis,\n+                                    const Side& side,\n+                                    const HloInstruction& broadcast,\n+                                    Value input) {\n+  TF_RET_CHECK(analysis != nullptr);\n+  std::vector<int64_t> out_shape;\n+\n+  auto tensor_input = mlir::dyn_cast<TensorValue>(input);\n+\n+  // The broadcasted dimension could be a non-trivial like broadcast + bitcast.\n+  // For example:\n+  // s8[2,256,128]broadcast(s8[2,128])\n+  // s8[512,128]bitcast(s8[2,256,128])\n+  // I.e. we broadcast the first dimension from 2 to 512.\n+  // When this is the case we don't need to expand the tile because it is\n+  // already 2d but we still need to broadcast it.\n+  bool non_trivial_broadcast = false;\n+\n+  for (const DimProperties& dim : side.tiled_dims) {\n+    const TensorIterationSpec::DimIterationSpec* spec =\n+        analysis->IterSpec(side.scope, &broadcast, dim.index);\n+    if (spec != nullptr && spec->at(0).stride > 0) {\n+      out_shape.push_back(\n+          (spec->at(0).broadcast_multiplier != 1) ? 1 : dim.block_size);\n+      non_trivial_broadcast |= spec->at(0).subfragments.size() != 1;\n+    }\n+  }\n+\n+  if (!tensor_input) {\n+    // Input is scalar.\n+    return Splat(b, input, out_shape);\n+  }\n+  if (!non_trivial_broadcast &&\n+      tensor_input.getType().getRank() == out_shape.size()) {\n+    // No dimensions to broadcast.\n+    return input;\n+  }\n+  // Add broadcasted dimensions one by one.\n+  Value expanded_input = tensor_input;\n+  int dim_idx = 0;\n+  for (const DimProperties& dim : side.tiled_dims) {\n+    const auto* output_spec =\n+        analysis->IterSpec(side.scope, &broadcast, dim.index);\n+    if (output_spec != nullptr && output_spec->at(0).stride > 0) {\n+      const auto* input_spec =\n+          analysis->IterSpec(side.scope, broadcast.operand(0), dim.index);\n+      // A dimension is broadcasted if it's either absent in the input or\n+      // if its size is increased from the input to the output.\n+      if (tensor_input.getType().getRank() != out_shape.size()) {\n+        if (input_spec == nullptr ||\n+            output_spec->at(0).count > input_spec->at(0).count) {\n+          expanded_input = b.create<mt::ExpandDimsOp>(expanded_input, dim_idx);\n+        }\n+      }\n+      ++dim_idx;\n+    }\n+  }\n+  return Broadcast(b, mlir::cast<TensorValue>(expanded_input), out_shape);\n+}\n+\n+// Emit sequence of instructions using compatible tiling ordered producers\n+// before consumers.\n+absl::StatusOr<Value> EmitScope(\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n+    const se::DeviceDescription& device_info,\n+    const TritonFusionAnalysis* analysis, const Side& side,\n+    absl::Span<const HloInstruction* const> instructions,\n+    absl::flat_hash_map<const HloInstruction*, Value>& values) {\n+  for (const HloInstruction* hlo : instructions) {\n+    Value result;\n+    if (hlo->opcode() == HloOpcode::kConvert &&\n+        hlo->operand(0)->shape().element_type() == S4) {\n+      Value unpacked;\n+      unpacked = triton::Cast(b, values[hlo->operand(0)], b.getI8Type());\n+      std::vector<Value> operands({unpacked});\n+      TF_ASSIGN_OR_RETURN(result, EmitElementwise(b, libdevice_path,\n+                                                  device_info, *hlo, operands));\n+    } else if (hlo->opcode() == HloOpcode::kConcatenate ||\n+               hlo->opcode() == HloOpcode::kDynamicSlice) {\n+      // Parameter loads and their concatenations are handled outside EmitScope.\n+      TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n+      continue;\n+    } else if (hlo->opcode() == HloOpcode::kParameter) {\n+      if (hlo->users()[0]->opcode() == HloOpcode::kConcatenate ||\n+          hlo->users()[0]->opcode() == HloOpcode::kDynamicSlice) {\n+        continue;\n+      }\n+      TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n+      continue;\n+    } else if (hlo->opcode() == HloOpcode::kConstant) {\n+      TF_ASSIGN_OR_RETURN(Value constant, EmitConstant(b, *hlo));\n+      // Splat makes it a tensor to avoid type mismatches.\n+      result = Splat(b, constant, {});\n+    } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n+      TF_ASSIGN_OR_RETURN(result, EmitBroadcast(b, analysis, side, *hlo,\n+                                                values[hlo->operand(0)]));\n+    } else if (HloInstruction::IsOpElementwise(hlo->opcode())) {\n+      std::vector<Value> operands;\n+      operands.reserve(hlo->operands().size());\n+      for (const HloInstruction* operand : hlo->operands()) {\n+        operands.push_back(values[operand]);\n+      }\n+      TF_ASSIGN_OR_RETURN(result, EmitElementwise(b, libdevice_path,\n+                                                  device_info, *hlo, operands));\n+    } else if (hlo->opcode() == HloOpcode::kTuple) {\n+      TF_RET_CHECK(hlo->IsRoot()) << hlo->ToString();\n+    } else if (hlo->opcode() == HloOpcode::kBitcast ||\n+               hlo->opcode() == HloOpcode::kTranspose ||\n+               hlo->opcode() == HloOpcode::kSlice ||\n+               hlo->opcode() == HloOpcode::kReshape ||\n+               hlo->opcode() == HloOpcode::kPad) {\n+      // All these are currently supported only as operations on indices\n+      // which are pushed to loads and stores. No operations on tiles are\n+      // performed here.\n+      result = values[hlo->operand(0)];\n+    } else {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Unsupported operation \", hlo->ToString()));\n+    }\n+    TF_RET_CHECK(values.insert({hlo, result}).second) << hlo->ToString();\n+    VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n+  }\n+  return values[instructions.back()];\n+}\n+\n+const TensorIterationSpec::DimIterationSpec* GetLhsNoncontractingSplitSpec(\n+    const TritonFusionAnalysis& analysis, int64_t lhs_noncontracting_dim_idx) {\n+  const TensorIterationSpec::DimIterationSpec* result = nullptr;\n+  for (const HloInstruction* lhs_param :\n+       analysis.ScopeParameters(TritonFusionAnalysis::Scope::LHS)) {\n+    const TensorIterationSpec::DimIterationSpec* spec =\n+        analysis.IterSpec(TritonFusionAnalysis::Scope::LHS, lhs_param,\n+                          lhs_noncontracting_dim_idx);\n+    if (spec != nullptr && spec->size() > 1) {\n+      CHECK_EQ(spec->size(), 2);\n+      if (result != nullptr) {\n+        CHECK_EQ(result->at(0).count, spec->at(0).count);\n+        CHECK_EQ(result->at(1).count, spec->at(1).count);\n+      }\n+      result = spec;\n+    }\n+  }\n+  return result;\n+}\n+\n+// Structure for parameters relating to the MatMul shape and dimension indices.\n+//\n+// Variable naming: lhs [m, k] x rhs [k, n] -> out [m, n].\n+//\n+// The logical output dimensions are always ordered as:\n+//   split-K, batch, non-contracting LHS, non-contracting RHS,\n+// where split-K and batch are optional.\n+struct MatMulDims {\n+  static absl::StatusOr<MatMulDims> Create(\n+      const TritonGemmConfig& config, const HloDotInstruction& dot,\n+      const TritonFusionAnalysis& analysis);\n+\n+  std::optional<int> out_split_k_dim_idx = std::nullopt;\n+\n+  std::optional<int> lhs_batch_dim_idx = std::nullopt;\n+  std::optional<int> rhs_batch_dim_idx = std::nullopt;\n+  std::optional<int> out_batch_dim_idx = std::nullopt;\n+\n+  // The LHS non-contracting can be split into two.\n+  std::optional<int64_t> lhs_noncontracting_split = std::nullopt;\n+\n+  int lhs_contracting_dim_idx;\n+  int lhs_noncontracting_dim_idx;\n+  int rhs_contracting_dim_idx;\n+  int rhs_noncontracting_dim_idx;\n+  // The index of the LHS noncontracting dim in the output.\n+  int out_lhs_noncontracting_dim_idx;\n+  // The index of the RHS noncontracting dim in the output.\n+  int out_rhs_noncontracting_dim_idx;\n+\n+  int64_t m;\n+  int64_t n;\n+  int64_t k;\n+  TritonGemmConfig config;\n+\n+  std::string ToString() const {\n+    return absl::StrCat(\"MxNxK: \", m, \"x\", n, \"x\", k,\n+                        \" contracting: lhs=\", lhs_contracting_dim_idx,\n+                        \" rhs=\", rhs_contracting_dim_idx);\n+  }\n+\n+ private:\n+  MatMulDims() = default;\n+};\n+\n+// Structure for parameters relating to the MatMul launch grid.\n+struct MatMulLaunchConfig {\n+  explicit MatMulLaunchConfig(const TritonGemmConfig& config,\n+                              const HloDotInstruction& dot,\n+                              const MatMulDims& dims,\n+                              const se::DeviceDescription& device_info);\n+\n+  int64_t grid_m;\n+  int64_t grid_n;\n+  LaunchDimensions launch_dims;\n+  mt::ProgramIDDim batch_program_id_dim;\n+  mt::ProgramIDDim noncontracting_program_id_dim;\n+};\n+\n+ma::ConstantOp Cst(EmitterLocOpBuilder b, Type index_ty, int64_t v) {\n+  return CreateConst(b, index_ty, v);\n+}\n+\n+AutotuneResult::TritonGemmKey DefaultTritonGemmKey() {\n+  AutotuneResult::TritonGemmKey triton_gemm_key;\n+  triton_gemm_key.set_block_m(64);\n+  triton_gemm_key.set_block_k(64);\n+  triton_gemm_key.set_block_n(64);\n+  triton_gemm_key.set_split_k(1);\n+  triton_gemm_key.set_num_stages(1);\n+  triton_gemm_key.set_num_warps(2);\n+  triton_gemm_key.set_num_ctas(1);\n+  return triton_gemm_key;\n+}\n+\n+ma::ConstantOp Cst32(EmitterLocOpBuilder b, int32_t v) {\n+  return CreateConst(b, b.getI32Type(), v);\n+}\n+\n+ma::ConstantOp Cst64(EmitterLocOpBuilder b, int64_t v) {\n+  return CreateConst(b, b.getI64Type(), v);\n+}\n+\n+/*static*/ absl::StatusOr<MatMulDims> MatMulDims::Create(\n+    const TritonGemmConfig& config, const HloDotInstruction& dot,\n+    const TritonFusionAnalysis& analysis) {\n+  MatMulDims matmul_dims;\n+  matmul_dims.config = config;\n+  if (config.split_k > 1) {\n+    // split-k is always the first logical dimension.\n+    matmul_dims.out_split_k_dim_idx = 0;\n+  }\n+\n+  int64_t num_split_k_dims = config.split_k > 1 ? 1 : 0;\n+  const auto& dims = dot.dot_dimension_numbers();\n+  matmul_dims.lhs_contracting_dim_idx = dims.lhs_contracting_dimensions(0);\n+  matmul_dims.lhs_noncontracting_dim_idx =\n+      GetNonContractingDims(dot.operand(0)->shape(),\n+                            dims.lhs_batch_dimensions(),\n+                            dims.lhs_contracting_dimensions())\n+          .value()[0];\n+  matmul_dims.rhs_contracting_dim_idx = dims.rhs_contracting_dimensions(0);\n+  matmul_dims.rhs_noncontracting_dim_idx =\n+      GetNonContractingDims(dot.operand(1)->shape(),\n+                            dims.rhs_batch_dimensions(),\n+                            dims.rhs_contracting_dimensions())\n+          .value()[0];\n+\n+  if (dims.lhs_batch_dimensions_size() > num_split_k_dims) {\n+    matmul_dims.lhs_batch_dim_idx = *dims.lhs_batch_dimensions().rbegin();\n+    matmul_dims.rhs_batch_dim_idx = *dims.rhs_batch_dimensions().rbegin();\n+    // The batch dimension (if present) comes after the split-k dimension (if\n+    // present, otherwise it's the first dimension).\n+    matmul_dims.out_batch_dim_idx = num_split_k_dims;\n+  }\n+\n+  // Logical output dimensions are always ordered as:\n+  //   split-K, batch, non-contracting LHS, non-contracting RHS,\n+  // where split-K and batch are optional.\n+  matmul_dims.out_rhs_noncontracting_dim_idx =\n+      dot.shape().dimensions().size() - 1;\n+  matmul_dims.out_lhs_noncontracting_dim_idx =\n+      dot.shape().dimensions().size() - 2;\n+\n+  auto* root = dot.parent()->root_instruction();\n+  auto iter_spec =\n+      analysis.IterSpec(TritonFusionAnalysis::Scope::OUTPUT, root,\n+                        matmul_dims.out_rhs_noncontracting_dim_idx);\n+  TF_RET_CHECK(iter_spec != nullptr);\n+  matmul_dims.n = iter_spec->at(0).count;\n+  // Contracting dimension length.\n+  if (config.split_k > 1 &&\n+      dot.operand(1)->operand(0)->opcode() == HloOpcode::kPad) {\n+    // Unpadded LHS shape:  [..., k, ...]\n+    // Padded LHS shape:    [..., padded_k, ...]\n+    // Bitcasted LHS shape: [..., split_k, padded_k / split_k, ...]\n+    TF_RET_CHECK(dot.operand(1)->opcode() == HloOpcode::kBitcast);\n+    const Shape& unpadded_rhs_shape =\n+        dot.operand(1)->operand(0)->operand(0)->shape();\n+    matmul_dims.k =\n+        unpadded_rhs_shape.dimensions(dims.rhs_contracting_dimensions(0) - 1);\n+  } else {\n+    matmul_dims.k =\n+        dot.operand(1)->shape().dimensions(dims.rhs_contracting_dimensions(0)) *\n+        config.split_k;\n+  }\n+\n+  auto* lhs_noncontracting_split_spec = GetLhsNoncontractingSplitSpec(\n+      analysis, matmul_dims.lhs_noncontracting_dim_idx);\n+  if (lhs_noncontracting_split_spec != nullptr) {\n+    // Just the fastest-varying part of it if the dimension is split.\n+    matmul_dims.m = lhs_noncontracting_split_spec->at(0).count;\n+    matmul_dims.lhs_noncontracting_split =\n+        lhs_noncontracting_split_spec->at(1).count;\n+  } else {\n+    matmul_dims.m = analysis\n+                        .IterSpec(TritonFusionAnalysis::Scope::OUTPUT, root,\n+                                  matmul_dims.out_lhs_noncontracting_dim_idx)\n+                        ->at(0)\n+                        .count;\n+  }\n+\n+  // For now split non-contracting and batch are not supported\n+  // simultaneously because they are implemented via same mechanism.\n+  TF_RET_CHECK(!(matmul_dims.out_batch_dim_idx.has_value() &&\n+                 matmul_dims.lhs_noncontracting_split.has_value()));\n+\n+  TF_RET_CHECK(matmul_dims.m >= 1);\n+  TF_RET_CHECK(matmul_dims.n >= 1);\n+  return std::move(matmul_dims);\n+}\n+\n+MatMulLaunchConfig::MatMulLaunchConfig(const TritonGemmConfig& config,\n+                                       const HloDotInstruction& dot,\n+                                       const MatMulDims& dims,\n+                                       const se::DeviceDescription& device_info)\n+    : grid_m((dims.m + config.block_m - 1) / config.block_m),\n+      grid_n((dims.n + config.block_n - 1) / config.block_n) {\n+  int64_t batch_size = dims.lhs_noncontracting_split.value_or(\n+      dims.out_batch_dim_idx.has_value()\n+          ? dot.shape().dimensions(*dims.out_batch_dim_idx)\n+          : 1);\n+  // X block size is 32-bit, Y and Z are 16-bit. Use X for large dimensions.\n+  constexpr int64_t kBlockCountYZLimit = 65536;\n+\n+  // In the imaginary situation where both batch size and grid_m * grid_n\n+  // are over 65535 we have to give up. Given the minimal m, n block sizes of 16\n+  // this requires at least 256 GB of output.\n+  CHECK_LT(batch_size * grid_m * grid_n,\n+           kBlockCountYZLimit * kBlockCountYZLimit);\n+\n+  const bool large_batch = batch_size >= kBlockCountYZLimit;\n+  if (large_batch) {\n+    batch_program_id_dim = mt::ProgramIDDim::X;\n+    noncontracting_program_id_dim = mt::ProgramIDDim::Y;\n+    launch_dims = LaunchDimensions(\n+        se::BlockDim(batch_size, grid_m * grid_n, config.split_k),\n+        se::ThreadDim(config.num_warps * WarpSize(device_info), 1, 1));\n+  } else {\n+    batch_program_id_dim = mt::ProgramIDDim::Y;\n+    noncontracting_program_id_dim = mt::ProgramIDDim::X;\n+    launch_dims = LaunchDimensions(\n+        se::BlockDim(grid_m * grid_n, batch_size, config.split_k),\n+        se::ThreadDim(config.num_warps * WarpSize(device_info), 1, 1));\n+  }\n+}\n+\n+absl::Status ValidateMatMulConfig(const TritonGemmConfig& config,\n+                                  const HloDotInstruction& dot) {\n+  TF_RET_CHECK(config.split_k >= 1);\n+  TF_RET_CHECK(config.block_m >= 16);\n+  TF_RET_CHECK(config.block_k >= 16);\n+  TF_RET_CHECK(config.block_n >= 8);\n+\n+  const auto& dims = dot.dot_dimension_numbers();\n+  int num_batch_dims =\n+      dims.lhs_batch_dimensions_size() - (config.split_k > 1 ? 1 : 0);\n+  TF_RET_CHECK(num_batch_dims <= 1);\n+  if (config.split_k > 1) {\n+    // Split-K dimension has to be the first batch one and have an index\n+    // just before the contracting one.\n+    const int lhs_split_k_dim_idx = dims.lhs_contracting_dimensions(0) - 1;\n+    const int rhs_split_k_dim_idx = dims.rhs_contracting_dimensions(0) - 1;\n+    // Size of this dimension has to match the split_k value.\n+    TF_RET_CHECK(dims.lhs_batch_dimensions(0) == lhs_split_k_dim_idx);\n+    TF_RET_CHECK(dims.rhs_batch_dimensions(0) == rhs_split_k_dim_idx);\n+    TF_RET_CHECK(config.split_k ==\n+                 dot.operand(0)->shape().dimensions(lhs_split_k_dim_idx));\n+    TF_RET_CHECK(config.split_k ==\n+                 dot.operand(1)->shape().dimensions(rhs_split_k_dim_idx));\n+  }\n+\n+  // Rely on dot decomposer: there is just one contracting and one\n+  // non-contracting dimension on each side + batch ones optionally.\n+  TF_RET_CHECK(dims.lhs_contracting_dimensions_size() == 1);\n+  TF_RET_CHECK(dims.rhs_contracting_dimensions_size() == 1);\n+\n+  TF_RET_CHECK(dot.operand(0)->shape().dimensions().size() ==\n+               2 + (config.split_k > 1 ? 1 : 0) + num_batch_dims);\n+  return absl::OkStatus();\n+}\n+\n+// if (index < limits[0]) {\n+//   return choices[0];\n+// } else if (index < limits[1]) {\n+//   return choices[1];\n+// } else if (...) {\n+// ...\n+// } else {\n+//   return choices.back();\n+// }\n+absl::StatusOr<Value> EmitMultiSelect(EmitterLocOpBuilder& b, Value index,\n+                                      ValueRange limits, ValueRange choices) {\n+  TF_RET_CHECK(choices.size() - 1 == limits.size());\n+  Value result = choices[0];\n+  for (int i = 0; i < choices.size() - 1; ++i) {\n+    result = b.create<ma::SelectOp>(\n+        b.create<ma::CmpIOp>(ma::CmpIPredicate::slt, index, limits[i]), result,\n+        choices[i + 1]);\n+  }\n+  return result;\n+}\n+\n+absl::Status UncompilableMatmul(absl::string_view explanation) {\n+  absl::Status s = absl::CancelledError(explanation);\n+  s.SetPayload(kUncompilableFusion, absl::Cord(explanation));\n+  return s;\n+}\n+\n+bool IsFp8Matmul(const HloDotInstruction* dot) {\n+  return primitive_util::IsF8Type(dot->operand(0)->shape().element_type()) &&\n+         primitive_util::IsF8Type(dot->operand(1)->shape().element_type());\n+}\n+\n+class MatMulEmitterHelper {\n+ public:\n+  MatMulEmitterHelper(absl::string_view libdevice_path,\n+                      const se::DeviceDescription& device_info,\n+                      const HloDotInstruction* dot_instr, Type index_ty,\n+                      MatMulDims dims, const MatMulLaunchConfig& launch_config,\n+                      const TritonFusionAnalysis& analysis)\n+      : libdevice_path_(libdevice_path),\n+        device_info_(device_info),\n+        dot_instr_(dot_instr),\n+        index_ty_(index_ty),\n+        analysis_(analysis),\n+        dims_(dims),\n+        launch_config_(launch_config) {}\n+\n+  std::vector<const HloInstruction*> EpiloguePostOrderTransitiveOperands(\n+      const HloInstruction* root) {\n+    // Collect all instructions of the dot's output scope.\n+    absl::flat_hash_set<const HloInstruction*> to_order;\n+    {\n+      std::queue<const HloInstruction*> to_add;\n+      if (root != dot_instr_) {\n+        to_add.push(root);\n+      }\n+      while (!to_add.empty()) {\n+        const HloInstruction* current = to_add.front();\n+        for (const HloInstruction* operand : current->operands()) {\n+          if (!to_order.contains(operand)) {\n+            if (operand != dot_instr_) {\n+              to_add.push(operand);\n+            }\n+          }\n+        }\n+        to_order.insert(current);\n+        to_add.pop();\n+      }\n+    }\n+    // Order them producers before consumers.\n+    std::vector<const HloInstruction*> to_emit;\n+    for (const HloInstruction* hlo :\n+         dot_instr_->parent()->MakeInstructionPostOrder()) {\n+      if (to_order.contains(hlo)) {\n+        to_emit.push_back(hlo);\n+      }\n+    }\n+    return to_emit;\n+  }\n+\n+  Value MakeInput(EmitterLocOpBuilder b, const Side& side,\n+                  int64_t operand_index,\n+                  absl::flat_hash_map<const HloInstruction*, Value>& values) {\n+    return *EmitScope(\n+        b, libdevice_path_, device_info_, &analysis_, side,\n+        dot_instr_->parent()->MakeInstructionPostOrderFrom(\n+            const_cast<HloInstruction&>(*dot_instr_->operand(operand_index))),\n+        values);\n+  }\n+\n+  int64_t GetNonContractingDimIdxForOperandScope(\n+      TritonFusionAnalysis::Scope scope) {\n+    if (scope == TritonFusionAnalysis::Scope::LHS) {\n+      return dims_.lhs_noncontracting_dim_idx;\n+    } else if (scope == TritonFusionAnalysis::Scope::RHS) {\n+      return dims_.rhs_noncontracting_dim_idx;\n+    } else {\n+      CHECK(false) << \"This shouldn't be called for the other scopes.\";\n+    }\n+  }\n+\n+  bool IsNonTrivialTiledDimension(TritonFusionAnalysis::Scope scope,\n+                                  int64_t dim_index) {\n+    switch (scope) {\n+      case TritonFusionAnalysis::Scope::LHS:\n+        return (dim_index == dims_.lhs_noncontracting_dim_idx && dims_.m > 1) ||\n+               (dim_index == dims_.lhs_contracting_dim_idx && dims_.k > 1);\n+      case TritonFusionAnalysis::Scope::RHS:\n+        return (dim_index == dims_.rhs_noncontracting_dim_idx && dims_.n > 1) ||\n+               (dim_index == dims_.rhs_contracting_dim_idx && dims_.k > 1);\n+      case TritonFusionAnalysis::Scope::OUTPUT:\n+        return (dim_index == dims_.out_lhs_noncontracting_dim_idx &&\n+                dims_.m > 1) ||\n+               (dim_index == dims_.out_rhs_noncontracting_dim_idx &&\n+                dims_.n > 1);\n+      default:\n+        break;\n+    }\n+    return false;\n+  }\n+\n+  bool NonTrivialTiledDimensionHasNoIterationAtParameter(\n+      TritonFusionAnalysis::Scope scope, const HloInstruction& hlo,\n+      int64_t dim_index) {\n+    const TensorIterationSpec::DimIterationSpec* spec =\n+        analysis_.IterSpec(scope, &hlo, dim_index);\n+    return spec == nullptr ||\n+           (IsNonTrivialTiledDimension(scope, dim_index) && spec->size() == 1 &&\n+            (spec->at(0).count <= 1 || spec->at(0).stride == 0));\n+  }\n+\n+  // Returns the increments necessary to advance the pointer or offset of the\n+  // given side & hlo instruction.\n+  SmallVector<Value> EmitIncrements(EmitterLocOpBuilder b, const Side& side,\n+                                    const HloInstruction& hlo,\n+                                    int64_t contracting_dimension, Value ki,\n+                                    int64_t block_k) {\n+    SmallVector<Value> increments;\n+    for (const DimProperties& dim : side.tiled_dims) {\n+      if (NonTrivialTiledDimensionHasNoIterationAtParameter(side.scope, hlo,\n+                                                            dim.index)) {\n+        continue;\n+      }\n+      // Only the contracting dimensions are advanced.\n+      if (dim.index == contracting_dimension) {\n+        const TensorIterationSpec::DimIterationSpec* spec =\n+            analysis_.IterSpec(side.scope, &hlo, dim.index);\n+        int64_t broadcast = spec->at(0).broadcast_multiplier;\n+        if (broadcast > 1) {\n+          if (broadcast != block_k) {\n+            // If the broadcast multiplier is not equal to the block_k, we need\n+            // to compute the increment conditionally. It has to advance by 1\n+            // if the current block is the last one in the broadcasted fragment,\n+            // and by 0 otherwise. Advance by 1 is computed as:\n+            // ((ki + block_k) / broadcast) * broadcast == ki + block_k.\n+            Value one = Cst32(b, 1);\n+            Value zero = Cst32(b, 0);\n+            Value add = b.create<ma::AddIOp>(ki, Cst32(b, block_k));\n+            Value div = b.create<ma::DivSIOp>(add, Cst32(b, broadcast));\n+            Value mul = b.create<ma::MulIOp>(div, Cst32(b, broadcast));\n+            Value cond = b.create<ma::CmpIOp>(ma::CmpIPredicate::eq, mul, add);\n+            Value one_or_zero = b.create<ma::SelectOp>(cond, one, zero);\n+            increments.push_back(one_or_zero);\n+          } else {\n+            // If the broadcast multiplier is equal to the block_k, we can\n+            // advance by 1 unconditionally.\n+            increments.push_back(Cst32(b, 1));\n+          }\n+        } else {\n+          increments.push_back(Cst32(b, dim.block_size * dim.split_value));\n+        }\n+      } else {\n+        increments.push_back(Cst32(b, 0));\n+      }\n+    }\n+    return increments;\n+  }\n+\n+  // Return the batch stride of the HLO passed as a parameter. If the\n+  // parameter HLO has no batch dimension, a zero stride is returned.\n+  // Also sets offset_batch and updates has_batch_offset as a side effect.\n+  absl::StatusOr<Value> GetBatchStride(EmitterLocOpBuilder b, const Side& side,\n+                                       const HloInstruction* hlo_param,\n+                                       int64_t& offset_batch,\n+                                       bool& has_batch_offset) {\n+    int64_t stride_batch = 0;\n+    if (side.scope != TritonFusionAnalysis::Scope::RHS &&\n+        dims_.lhs_noncontracting_split) {\n+      const TensorIterationSpec::DimIterationSpec* spec =\n+          analysis_.IterSpec(side.scope, hlo_param, side.tiled_dims[0].index);\n+      if (spec != nullptr) {\n+        if (spec->size() > 1) {\n+          // Support one specific kind of output transpose that splits the\n+          // dimension originating from the split LHS non-contracting one.\n+          stride_batch = spec->at(1).stride;\n+        } else {\n+          // Because the major part of the split is implemented using the\n+          // batch logic stride_batch is populated here as the stride of\n+          // the minor part times its size.\n+          stride_batch = spec->at(0).stride *\n+                         (spec->at(0).count / *dims_.lhs_noncontracting_split);\n+        }\n+        TF_RET_CHECK(stride_batch != 0);\n+      }\n+    } else if (side.batch_dim_idx.has_value()) {\n+      const TensorIterationSpec::DimIterationSpec* spec =\n+          analysis_.IterSpec(side.scope, hlo_param, *side.batch_dim_idx);\n+      if (spec != nullptr) {\n+        stride_batch = spec->at(0).stride;\n+        offset_batch = spec->at(0).slice_start;\n+        TF_RET_CHECK(stride_batch != 0);\n+      }\n+    }\n+\n+    has_batch_offset |= stride_batch != 0;\n+    return Cst(b, index_ty_, stride_batch);\n+  }\n+\n+  // bases: The base pointers of each argument.\n+  absl::StatusOr<Value> EmitTensorPointer(\n+      EmitterLocOpBuilder b, const HloInstruction* hlo, const Side& side,\n+      const ValueRange& args, Value pid_k,\n+      std::vector<int32_t>& boundary_checks) {\n+    llvm::SmallVector<mlir::Value> bases;\n+    bases.reserve(hlo->operand_count());\n+    for (mlir::Value arg : args) {\n+      if (mlir::MemRefType memref_type =\n+              mlir::dyn_cast<mlir::MemRefType>(arg.getType())) {\n+        auto ptr_type =\n+            triton::GetGlobalPointerType(memref_type.getElementType());\n+        bases.push_back(b.create<mt::xla::MemrefToPtrOp>(ptr_type, arg));\n+      } else {\n+        bases.push_back(arg);\n+      }\n+    }\n+\n+    Value base;\n+\n+    // Concatenations of parameters are handled during generation of block\n+    // pointers because of a limitation of implementation of block pointers\n+    // in the Triton compiler: block pointers are not supported inside\n+    // conditionals.\n+    // Therefore instead of directly using a conditional to emit a concatenation\n+    // and emitting its inputs inside the cases a single block pointer is\n+    // emitted for all inputs, but all its properties (base, strides etc) get\n+    // generated conditionally on the position of the current thread block\n+    // within the concatenated dimension.\n+    ConcatParams concat_params;\n+\n+    if (hlo->opcode() == HloOpcode::kConcatenate) {\n+      TF_ASSIGN_OR_RETURN(\n+          std::tie(concat_params, base),\n+          CalculateConcatParamsAndUpdateBase(b, hlo, side, bases));\n+    } else {\n+      concat_params.dim_idx = -1;\n+      base = bases[0];\n+    }\n+\n+    // Parameters of MakeTensorPtrOp to be generated by this function.\n+    TensorParams tensor_params;\n+    for (const DimProperties& dim : side.tiled_dims) {\n+      TF_RETURN_IF_ERROR(AddDimToTensorParams(b, hlo, side, dim, concat_params,\n+                                              bases, boundary_checks,\n+                                              tensor_params));\n+    }\n+\n+    TF_ASSIGN_OR_RETURN(base, UpddateBaseForBatchAndConcatCases(\n+                                  b, hlo, concat_params, side, base));\n+\n+    if (dims_.out_split_k_dim_idx.has_value()) {\n+      TF_ASSIGN_OR_RETURN(base,\n+                          UpdateBaseForSplitKCase(b, hlo, side, base, pid_k));\n+    }\n+\n+    if (tensor_params.block_dims.empty()) {\n+      // Load of a scalar.\n+      return base;\n+    }\n+\n+    auto tensor_ptr = mlir::cast<Value>(\n+        b.create<mt::MakeTensorPtrOp>(\n+             base, tensor_params.bounds, tensor_params.strides,\n+             tensor_params.tensor_offsets, tensor_params.block_dims,\n+             tensor_params.dim_order)\n+            .getResult());\n+    tensor_ptr = b.create<mt::AdvanceOp>(tensor_ptr.getType(), tensor_ptr,\n+                                         tensor_params.block_offsets);\n+    return tensor_ptr;\n+  }\n+\n+ private:\n+  struct ConcatParams {\n+    // Index of concatenated dimension if present, -1 otherwise.\n+    int dim_idx;\n+    // Offsets along the concatenated dimension at which operands change.\n+    std::vector<Value> boundaries;\n+    // Block index along the concatenated dimension * block size.\n+    Value dim_pid_offset;\n+  };\n+\n+  struct TensorParams {\n+    std::vector<Value> bounds;\n+    std::vector<Value> strides;\n+\n+    // Offsets from tensor origin, same for all thread blocks.\n+    std::vector<Value> tensor_offsets;\n+    std::vector<int32_t> block_dims;\n+    std::vector<int32_t> dim_order;\n+\n+    // Offsets for a given thread block, typically pid * block size.\n+    // Used in a one-off AdvanceOp applied to the generated MakeTensorPtrOp.\n+    std::vector<Value> block_offsets;\n+  };\n+\n+  absl::Status AddDimToTensorParams(EmitterLocOpBuilder b,\n+                                    const HloInstruction* hlo, const Side& side,\n+                                    const DimProperties& properties,\n+                                    const ConcatParams& concat_params,\n+                                    const ValueRange& bases,\n+                                    std::vector<int32_t>& boundary_checks,\n+                                    TensorParams& tensor_params) {\n+    if (NonTrivialTiledDimensionHasNoIterationAtParameter(side.scope, *hlo,\n+                                                          properties.index)) {\n+      // If a non-trivial tiled dimension has only one element at\n+      // the parameter, it's being broadcasted. Skip it in the tensor\n+      // pointer to prevent it from being padded to the tile size on load\n+      // instead of being broadcasted.\n+      return absl::OkStatus();\n+    }\n+    Value pid_offset;\n+    if (properties.pid == nullptr) {\n+      pid_offset = Cst32(b, 0);\n+    } else {\n+      pid_offset =\n+          b.create<ma::MulIOp>(properties.pid, Cst32(b, properties.block_size));\n+    }\n+    std::vector<const HloInstruction*> inputs;\n+    if (hlo->opcode() == HloOpcode::kConcatenate) {\n+      inputs.insert(inputs.end(), hlo->operands().cbegin(),\n+                    hlo->operands().cend());\n+    } else {\n+      inputs = {hlo};\n+    }\n+    std::vector<const TensorIterationSpec::DimIterationSpec*> specs;\n+    std::vector<Value> input_strides;\n+    std::vector<Value> input_offsets;\n+    std::vector<Value> input_bounds;\n+    specs.reserve(inputs.size());\n+    input_strides.reserve(inputs.size());\n+    input_offsets.reserve(inputs.size());\n+    input_bounds.reserve(inputs.size());\n+    for (const HloInstruction* input : inputs) {\n+      auto* spec = analysis_.IterSpec(side.scope, input, properties.index);\n+      specs.push_back(spec);\n+      auto dim_spec = spec->at(0);\n+      input_strides.push_back(Cst64(b, dim_spec.stride));\n+      input_offsets.push_back(\n+          b.create<ma::AddIOp>(pid_offset, Cst32(b, dim_spec.slice_start)));\n+      input_bounds.push_back(Cst64(b, dim_spec.count));\n+    }\n+    {\n+      TF_ASSIGN_OR_RETURN(\n+          Value select_input_strides,\n+          EmitMultiSelect(b, concat_params.dim_pid_offset,\n+                          concat_params.boundaries, input_strides));\n+      tensor_params.strides.push_back(select_input_strides);\n+    }\n+\n+    if (properties.index == concat_params.dim_idx) {\n+      TF_RETURN_IF_ERROR(AddConcatDimToTensorParams(\n+          b, hlo, side, properties, concat_params, pid_offset, specs.front(),\n+          input_offsets, input_bounds, tensor_params));\n+    } else if (hlo->opcode() == HloOpcode::kDynamicSlice &&\n+               (side.scope == TritonFusionAnalysis::Scope::LHS ||\n+                side.scope == TritonFusionAnalysis::Scope::RHS) &&\n+               properties.index ==\n+                   GetNonContractingDimIdxForOperandScope(side.scope)) {\n+      TF_RETURN_IF_ERROR(\n+          AddDynamicSliceDimToTensorParams(b, hlo, side, properties, pid_offset,\n+                                           specs.back(), bases, tensor_params));\n+    } else {\n+      TF_RETURN_IF_ERROR(AddStandaloneDimToTensorParams(\n+          b, side, properties, pid_offset, specs.front(), bases, tensor_params,\n+          boundary_checks));\n+    }\n+    if (specs.back()->at(0).broadcast_multiplier > 1) {\n+      if (specs.back()->at(0).broadcast_multiplier % properties.block_size) {\n+        return UncompilableMatmul(\n+            absl::StrCat(\"Broadcast multiplier is not a multiple of the block \"\n+                         \"size. block_size: \",\n+                         properties.block_size, \" vs broadcast_multiplier: \",\n+                         specs.back()->at(0).broadcast_multiplier));\n+      }\n+      if (properties.split_value > 1) {\n+        return UncompilableMatmul(\n+            \"Broadcasted dimension is split, which is not supported yet.\");\n+      }\n+      tensor_params.block_dims.push_back(1);\n+    } else {\n+      tensor_params.block_dims.push_back(properties.block_size);\n+    }\n+    tensor_params.dim_order.emplace(tensor_params.dim_order.begin(),\n+                                    tensor_params.dim_order.size());\n+    return absl::OkStatus();\n+  }\n+\n+  absl::Status AddStandaloneDimToTensorParams(\n+      EmitterLocOpBuilder b, const Side& side, const DimProperties& properties,\n+      Value pid_offset, const TensorIterationSpec::DimIterationSpec* spec,\n+      const ValueRange& bases, TensorParams& tensor_params,\n+      std::vector<int32_t>& boundary_checks) {\n+    tensor_params.tensor_offsets.push_back(Cst32(b, spec->at(0).slice_start));\n+    tensor_params.block_offsets.push_back(pid_offset);\n+    int64_t dim_bound = spec->at(0).count;\n+    if (side.scope == TritonFusionAnalysis::Scope::OUTPUT &&\n+        properties.index == dims_.out_lhs_noncontracting_dim_idx &&\n+        spec->size() == 1 && dims_.lhs_noncontracting_split.has_value()) {\n+      // Dimension of the output produced by the non-contracting LHS one\n+      // is logically split, major part is addressed using pid_batch.\n+      dim_bound /= *dims_.lhs_noncontracting_split;\n+    }\n+    tensor_params.bounds.push_back(Cst64(b, dim_bound));\n+    if (dim_bound % (properties.block_size * properties.split_value) != 0) {\n+      boundary_checks.push_back(tensor_params.bounds.size() - 1);\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  absl::Status AddDynamicSliceDimToTensorParams(\n+      EmitterLocOpBuilder b, const HloInstruction* hlo, const Side& side,\n+      const DimProperties& properties, const Value pid_offset,\n+      const TensorIterationSpec::DimIterationSpec* spec,\n+      const ValueRange& bases, TensorParams& tensor_params) {\n+    // Here we compute the offset of where we should read the slice from.\n+    // TODO(b/323255699): Add support for slices of the contracting dim.\n+    // Dynamic slices are guaranteed to only be offset along the majormost\n+    // dimension.\n+\n+    // The only fragment of the non-contracting dim of the dot's input in\n+    // the current scope:\n+    TF_RET_CHECK(spec->size() == 1);\n+    const TensorIterationSpec::IterationSpecFragment only_fragment_of_nc_dim =\n+        spec->at(0);\n+    // The majormost dim index in the dynamic slice's output.\n+    const int majormost_dim = hlo->shape().layout().minor_to_major().back();\n+\n+    // dynamic slice operands are (input, start_index0, start_index1, ...)\n+    // so the start index corresponding to the ith dimension is bases[i+1].\n+    Value majormost_dim_start_index_ptr_val = bases[majormost_dim + 1];\n+    Value majormost_dim_start_index_val = b.create<mt::LoadOp>(\n+        majormost_dim_start_index_ptr_val, mt::CacheModifier::NONE,\n+        mt::EvictionPolicy::NORMAL,\n+        /*isVolatile=*/false);\n+    int64_t majormost_dim_start_index_upper_limit =\n+        hlo->operand(0)->shape().dimensions(majormost_dim) -\n+        hlo->dynamic_slice_sizes().at(majormost_dim);\n+    // We don't want to cast S64 indices to S32, because that could result\n+    // in an incorrect value.\n+    if (majormost_dim_start_index_val.getType().isInteger() &&\n+        majormost_dim_start_index_val.getType().getIntOrFloatBitWidth() == 64) {\n+      return UncompilableMatmul(\n+          \"64 bit dynamic-slice indices are not supported yet.\");\n+    }\n+    majormost_dim_start_index_val =\n+        triton::Cast(b, majormost_dim_start_index_val, b.getI32Type());\n+    majormost_dim_start_index_val =\n+        b.create<ma::MaxSIOp>(majormost_dim_start_index_val, Cst32(b, 0));\n+    majormost_dim_start_index_val =\n+        b.create<ma::MinSIOp>(majormost_dim_start_index_val,\n+                              Cst32(b, majormost_dim_start_index_upper_limit));\n+\n+    // How many \"rows\" (non-contracting dim values) are there in a slice of\n+    // size 1?\n+    int64_t rows_per_majormost_dim = 1;\n+    for (int i = 0; i < hlo->shape().dimensions().size() - 1; ++i) {\n+      rows_per_majormost_dim *= hlo->shape().dimensions_minor(i);\n+    }\n+    rows_per_majormost_dim =\n+        rows_per_majormost_dim / only_fragment_of_nc_dim.stride;\n+    Value rows_per_majormost_dim_val = Cst32(b, rows_per_majormost_dim);\n+\n+    Value tensor_offset_val_i32 = b.create<ma::MulIOp>(\n+        majormost_dim_start_index_val, rows_per_majormost_dim_val);\n+    tensor_params.tensor_offsets.push_back(tensor_offset_val_i32);\n+\n+    // tt.make_tensor_ptr expects an i64 for shape and size, but expects\n+    // i32 for offsets. We extend the offset to calculate the upper bound.\n+    Value tensor_offset_val_i64 =\n+        b.create<ma::ExtSIOp>(b.getI64Type(), tensor_offset_val_i32);\n+    Value sliced_count_val = Cst64(b, only_fragment_of_nc_dim.sliced_count);\n+    Value upper_bound_val =\n+        b.create<ma::AddIOp>(tensor_offset_val_i64, sliced_count_val);\n+    tensor_params.bounds.push_back(upper_bound_val);\n+\n+    tensor_params.block_offsets.push_back(pid_offset);\n+    return absl::OkStatus();\n+  }\n+\n+  absl::Status AddConcatDimToTensorParams(\n+      EmitterLocOpBuilder b, const HloInstruction* hlo, const Side& side,\n+      const DimProperties& properties, const ConcatParams& concat_params,\n+      const Value pid_offset, const TensorIterationSpec::DimIterationSpec* spec,\n+      const std::vector<Value>& input_offsets, std::vector<Value>& input_bounds,\n+      TensorParams& tensor_params) {\n+    TF_ASSIGN_OR_RETURN(Value select_input_offsets,\n+                        EmitMultiSelect(b, pid_offset, concat_params.boundaries,\n+                                        input_offsets));\n+    tensor_params.block_offsets.push_back(select_input_offsets);\n+\n+    TF_ASSIGN_OR_RETURN(\n+        Value select_input_bounds,\n+        EmitMultiSelect(b, pid_offset, concat_params.boundaries, input_bounds));\n+    tensor_params.bounds.push_back(select_input_bounds);\n+    tensor_params.tensor_offsets.push_back(Cst32(b, spec->at(0).slice_start));\n+    return absl::OkStatus();\n+  }\n+\n+  absl::StatusOr<Value> UpddateBaseForBatchAndConcatCases(\n+      EmitterLocOpBuilder b, const HloInstruction* hlo,\n+      const ConcatParams& concat_params, const Side& side, Value base) {\n+    int64_t offset_batch = 0;\n+    bool has_batch_offset = false;\n+    Value batch_stride;\n+\n+    if (hlo->opcode() == HloOpcode::kConcatenate) {\n+      std::vector<Value> batch_strides;\n+      batch_strides.reserve(hlo->operands().size());\n+      for (const HloInstruction* operand : hlo->operands()) {\n+        TF_ASSIGN_OR_RETURN(\n+            Value op_stride,\n+            GetBatchStride(b, side, operand, offset_batch, has_batch_offset));\n+        batch_strides.push_back(op_stride);\n+      }\n+      TF_ASSIGN_OR_RETURN(\n+          batch_stride,\n+          EmitMultiSelect(b, concat_params.dim_pid_offset,\n+                          concat_params.boundaries, batch_strides));\n+    } else {\n+      TF_ASSIGN_OR_RETURN(\n+          batch_stride,\n+          GetBatchStride(b, side, hlo, offset_batch, has_batch_offset));\n+    }\n+\n+    // Avoid generating logic to compute batch offset if unnecessary.\n+    if (has_batch_offset) {\n+      Value pid_batch =\n+          b.create<mt::GetProgramIdOp>(launch_config_.batch_program_id_dim);\n+\n+      Value pid_offset_batch = b.create<ma::MulIOp>(\n+          b.create<ma::AddIOp>(Cst(b, index_ty_, offset_batch),\n+                               ConvertScalar(b, pid_batch)),\n+          batch_stride);\n+\n+      base = AddPtr(b, base, pid_offset_batch);\n+    }\n+    return base;\n+  }\n+\n+  absl::StatusOr<Value> UpdateBaseForSplitKCase(EmitterLocOpBuilder b,\n+                                                const HloInstruction* hlo,\n+                                                const Side& side, Value base,\n+                                                Value pid_k) {\n+    const TensorIterationSpec::DimIterationSpec* spec = analysis_.IterSpec(\n+        TritonFusionAnalysis::Scope::OUTPUT, hlo, *dims_.out_split_k_dim_idx);\n+    if (spec != nullptr && spec->at(0).count > 1) {\n+      TF_RET_CHECK(pid_k != nullptr);\n+      base =\n+          AddPtr(b, base,\n+                 b.create<ma::MulIOp>(ConvertScalar(b, pid_k),\n+                                      Cst(b, index_ty_, spec->at(0).stride)));\n+    }\n+    return base;\n+  }\n+\n+  absl::StatusOr<std::pair<ConcatParams, Value>>\n+  CalculateConcatParamsAndUpdateBase(EmitterLocOpBuilder b,\n+                                     const HloInstruction* hlo,\n+                                     const Side& side,\n+                                     const ValueRange& bases) {\n+    ConcatParams concat_params;\n+    // For now only non-contracting dimension can be concatenated.\n+    concat_params.dim_idx = (side.scope == TritonFusionAnalysis::Scope::LHS)\n+                                ? dims_.lhs_noncontracting_dim_idx\n+                                : dims_.rhs_noncontracting_dim_idx;\n+    const DimProperties& properties = [&] {\n+      for (const DimProperties& dim : side.tiled_dims) {\n+        if (dim.index == concat_params.dim_idx) {\n+          return dim;\n+        }\n+      }\n+      LOG(FATAL) << \"Missing dimension.\";\n+    }();\n+    TF_RET_CHECK(bases.size() == hlo->operand_count());\n+\n+    concat_params.boundaries.reserve(hlo->operand_count() - 1);\n+    for (int i = 0; i < hlo->operand_count() - 1; ++i) {\n+      const TensorIterationSpec::IterationSpecFragment& fragment =\n+          analysis_\n+              .IterSpec(side.scope, hlo->operand(i), concat_params.dim_idx)\n+              ->at(0);\n+      if (fragment.sliced_count % properties.block_size != 0) {\n+        return UncompilableMatmul(\n+            \"Operand is not divisible by the block size.\");\n+      }\n+      concat_params.boundaries.push_back(\n+          Cst32(b, -fragment.slice_start + fragment.sliced_count));\n+    }\n+\n+    concat_params.dim_pid_offset =\n+        b.create<ma::MulIOp>(properties.pid, Cst32(b, properties.block_size));\n+    TF_ASSIGN_OR_RETURN(Value base,\n+                        EmitMultiSelect(b, concat_params.dim_pid_offset,\n+                                        concat_params.boundaries, bases));\n+    return std::make_pair(concat_params, base);\n+  }\n+\n+  // Extend int32 indexes to int64, if necessary.\n+  Value ConvertScalar(EmitterLocOpBuilder b, Value value) {\n+    if (index_ty_.getIntOrFloatBitWidth() == 64) {\n+      return b.create<ma::ExtSIOp>(index_ty_, value);\n+    }\n+    return value;\n+  }\n+\n+  absl::string_view libdevice_path_;\n+  const se::DeviceDescription& device_info_;\n+  const HloDotInstruction* dot_instr_;\n+  Type index_ty_;\n+  TritonFusionAnalysis analysis_;\n+  MatMulDims dims_;\n+  MatMulLaunchConfig launch_config_;\n+};\n+\n+absl::StatusOr<SmallVector<Value>> GetArguments(mlir::FunctionOpInterface fn,\n+                                                const HloInstruction& input) {\n+  if (input.opcode() == HloOpcode::kParameter) {\n+    return {{fn.getArgument(input.parameter_number())}};\n+  } else if (input.opcode() == HloOpcode::kConcatenate ||\n+             input.opcode() == HloOpcode::kDynamicSlice) {\n+    // As defined in GemmFusion, all inputs of concatenate and dynamic slice are\n+    // parameters.\n+    SmallVector<Value> result;\n+    for (const HloInstruction* operand : input.operands()) {\n+      TF_RET_CHECK(operand->opcode() == HloOpcode::kParameter);\n+      result.push_back(fn.getArgument(operand->parameter_number()));\n+    }\n+    return result;\n+  }\n+  LOG(FATAL) << \"Unexpected opcode: \" << input.opcode();\n+}\n+\n+// Concatenations can currently only be applied directly to parameters;\n+// all concatenated parameters share the same block pointer. This function\n+// returns all inputs of a kernel: concatenations of parameters and standalone\n+// parameters.\n+ConstHloInstructionSet ScopeInputs(const TritonFusionAnalysis& analysis,\n+                                   const TritonFusionAnalysis::Scope scope) {\n+  ConstHloInstructionSet result;\n+  for (const HloInstruction* parameter : analysis.ScopeParameters(scope)) {\n+    if (absl::c_any_of(parameter->users(), [](const HloInstruction* user) {\n+          return user->opcode() == HloOpcode::kConcatenate ||\n+                 user->opcode() == HloOpcode::kDynamicSlice;\n+        })) {\n+      // Concatenation is always the only user of its parameters by\n+      // construction.\n+      CHECK_EQ(parameter->users().size(), 1);\n+      for (const HloInstruction* operand : parameter->users()[0]->operands()) {\n+        // All operands of a concatenation have to be computation parameters.\n+        CHECK_EQ(operand->opcode(), HloOpcode::kParameter);\n+      }\n+      result.insert(parameter->users()[0]);\n+    } else {\n+      result.insert(parameter);\n+    }\n+  }\n+  return result;\n+}\n+\n+// This is a heuristic that serves as a proxy for register usage and code size.\n+//\n+// We have noticed that tilings with very long LLVM IR code are both slow to\n+// compile and slow to run. This can be for example due to register spills. So\n+// we should skip these tilings to save time. But it's better to skip them\n+// before the LLVM IR is generated. To do that, we came up with a formula that\n+// strongly correlates with the LLVM IR size. The formula is the size of the two\n+// input and the output thread block tiles divided by the number of warps. We\n+// read https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/ as a\n+// reference, and found the formula by trial and error.\n+//\n+// To regenerate the limit, we have to run an exhaustive search on all tilings\n+// for a few different HLOs, printing the runtimes and the heuristic values.\n+//\n+// From that, we can find a limit, such that all tilings within alpha *\n+// optimal_runtime have a heuristic value less than or equal to the limit.\n+//\n+// In our measurements, all tilings which were within 1.13 * optimal_runtime had\n+// a complexity_heuristic_value <= kComplexityHeuristicLimit.\n+//\n+// See go/tiling-heuristic for more details.\n+absl::Status CheckGemmTilingComplexityHeuristic(\n+    const TritonGemmConfig& config) {\n+  constexpr int64_t kComplexityHeuristicLimit = 9000;\n+  int64_t complexity_heuristic_value =\n+      (config.block_m * config.block_n +\n+       (config.block_m + config.block_n) * config.block_k) /\n+      config.num_warps;\n+  VLOG(2) << \"Complexity heuristic: \" << complexity_heuristic_value;\n+  if (complexity_heuristic_value > kComplexityHeuristicLimit) {\n+    return ResourceExhausted(\"Tiling complexity heuristic exceeded: %d > %d\",\n+                             complexity_heuristic_value,\n+                             kComplexityHeuristicLimit);\n+  }\n+  return absl::OkStatus();\n+}\n+\n+class Scopes {\n+ public:\n+  Scopes(EmitterLocOpBuilder& b, const HloInstruction* dot_instr,\n+         const TritonFusionAnalysis& analysis, const MatMulDims& dims,\n+         const TritonGemmConfig& config, const MatMulLaunchConfig launch_config)\n+      : lhs_(TritonFusionAnalysis::Scope::LHS),\n+        rhs_(TritonFusionAnalysis::Scope::RHS),\n+        out_(TritonFusionAnalysis::Scope::OUTPUT) {\n+    constexpr int group_m = 8;\n+    const int64_t width = group_m * launch_config.grid_n;\n+\n+    auto pid_nc = b.create<mt::GetProgramIdOp>(\n+        launch_config.noncontracting_program_id_dim);\n+    pid_k_ = (config.split_k > 1)\n+                 ? b.create<mt::GetProgramIdOp>(mt::ProgramIDDim::Z)\n+                 : Value{};\n+\n+    auto group_id = b.create<ma::DivSIOp>(pid_nc, Cst32(b, width));\n+    ma::ConstantOp group_m_op = Cst32(b, group_m);\n+    auto first_pid_m = b.create<ma::MulIOp>(group_id, group_m_op);\n+    auto sub0 =\n+        b.create<ma::SubIOp>(Cst32(b, launch_config.grid_m), first_pid_m);\n+    auto group_size = b.create<ma::SelectOp>(\n+        b.create<ma::CmpIOp>(ma::CmpIPredicate::slt, sub0, group_m_op), sub0,\n+        group_m_op);\n+\n+    pid_m_ = b.create<ma::AddIOp>(first_pid_m,\n+                                  b.create<ma::RemSIOp>(pid_nc, group_size));\n+\n+    pid_n_ = b.create<ma::DivSIOp>(\n+        b.create<ma::RemSIOp>(pid_nc, Cst32(b, width)), group_size);\n+\n+    int lhs_non_contracting_block_size = config.block_m;\n+    int lhs_contracting_block_size = config.block_k;\n+    lhs_.tiled_dims = {\n+        DimProperties(dims.lhs_noncontracting_dim_idx, pid_m_,\n+                      lhs_non_contracting_block_size,\n+                      /*split_value=*/1),\n+        DimProperties(dims.lhs_contracting_dim_idx, pid_k_,\n+                      lhs_contracting_block_size, config.split_k)};\n+    lhs_.batch_dim_idx = dims.lhs_batch_dim_idx;\n+\n+    int rhs_contracting_block_size = config.block_k;\n+    int rhs_non_contracting_block_size = config.block_n;\n+    rhs_.tiled_dims = {\n+        DimProperties(dims.rhs_contracting_dim_idx, pid_k_,\n+                      rhs_contracting_block_size, config.split_k),\n+        DimProperties(dims.rhs_noncontracting_dim_idx, pid_n_,\n+                      rhs_non_contracting_block_size,\n+                      /*split_value=*/1)};\n+    rhs_.batch_dim_idx = dims.rhs_batch_dim_idx;\n+\n+    out_.tiled_dims = {DimProperties(dims.out_lhs_noncontracting_dim_idx,\n+                                     pid_m_, config.block_m,\n+                                     /*split_value=*/1),\n+                       DimProperties(dims.out_rhs_noncontracting_dim_idx,\n+                                     pid_n_, config.block_n,\n+                                     /*split_value=*/1)};\n+    out_.batch_dim_idx = dims.out_batch_dim_idx;\n+  }\n+\n+  std::vector<const Side*> input_scopes() const { return {&lhs_, &rhs_}; }\n+  const Side& lhs() const { return lhs_; }\n+  const Side& rhs() const { return rhs_; }\n+  const Side& out() const { return out_; }\n+  const Value& pid_m() const { return pid_m_; }\n+  const Value& pid_k() const { return pid_k_; }\n+  const Value& pid_n() const { return pid_n_; }\n+\n+ private:\n+  Side lhs_;\n+  Side rhs_;\n+  Side out_;\n+\n+  Value pid_m_;\n+  Value pid_k_;\n+  Value pid_n_;\n+};\n+\n+// This class represents a loadable input that needs to be partially loaded on\n+// each iteration of a ForOp. It keeps track of which iterable argument indices\n+// correspond to this input & all information necessary to load & modify\n+// iteration arguments for the next iteration.\n+class IterableInput {\n+ public:\n+  IterableInput(size_t iter_arg_index, size_t operand_index,\n+                int contracting_dimension, Type type, Type storage_type,\n+                const HloInstruction* hlo_instr, const Side* side,\n+                std::vector<int32_t> boundary_checks, int64_t block_k)\n+      : iter_arg_index_(iter_arg_index),\n+        operand_index_(operand_index),\n+        contracting_dimension_(contracting_dimension),\n+        type_(type),\n+        storage_type_(storage_type),\n+        block_k_(block_k),\n+        hlo_instr_(hlo_instr),\n+        side_(side),\n+        boundary_checks_(boundary_checks) {}\n+\n+  static absl::StatusOr<IterableInput> CreateIterableInput(\n+      size_t iter_arg_index, EmitterLocOpBuilder& b, const MatMulDims& dims,\n+      const Side* side, const HloInstruction* hlo_instr, int64_t block_k) {\n+    TF_ASSIGN_OR_RETURN(Type input_ty,\n+                        TritonType(b, hlo_instr->shape().element_type()));\n+    int contracting_dimension =\n+        (side->scope == TritonFusionAnalysis::Scope::RHS)\n+            ? dims.rhs_contracting_dim_idx\n+            : dims.lhs_contracting_dim_idx;\n+\n+    return IterableInput(iter_arg_index, static_cast<int>(side->scope),\n+                         contracting_dimension, input_ty,\n+                         StorageType(b, input_ty), hlo_instr, side,\n+                         /*boundary_checks=*/{}, block_k);\n+  }\n+\n+  Value EmitAdvance(EmitterLocOpBuilder b, MatMulEmitterHelper& emitter,\n+                    Value ki, ValueRange iter_args) const {\n+    SmallVector<Value> increments = emitter.EmitIncrements(\n+        b, *side_, *hlo_instr_, contracting_dimension_, ki, block_k_);\n+\n+    if (increments.empty()) {\n+      return iter_args[iter_arg_index_];\n+    }\n+\n+    const Value& iter_arg = iter_args[iter_arg_index_];\n+    return b.create<mt::AdvanceOp>(iter_arg.getType(), iter_arg, increments);\n+  }\n+\n+  Value EmitLoad(EmitterLocOpBuilder b, ValueRange args) const {\n+    Value param_value = EmitParameterLoad(b, args.front(), boundary_checks_);\n+    if (type_ != storage_type_) {\n+      // For example cast i8 to i1.\n+      param_value = triton::Cast(b, param_value, type_);\n+    }\n+    return param_value;\n+  }\n+\n+  // Index of the iter_arg of the ForOp associated with this input.\n+  size_t iter_arg_index_;\n+  // Index used to differentiate it between LHS and RHS inputs.\n+  size_t operand_index_;\n+  // Index of the contracting dimension in the input.\n+  int contracting_dimension_;\n+  // Type of the input.\n+  Type type_;\n+  // Storage type of the input (in case it is different & needs to be casted).\n+  Type storage_type_;\n+  // Step size of the contracting dimension.\n+  int64_t block_k_;\n+\n+  // Necessary for some operations at the moment. Maybe could be refactored out.\n+  const HloInstruction* hlo_instr_;\n+  const Side* side_;\n+\n+  // This is currently set afterwards, but needs to be associated with the input\n+  // during iteration.\n+  std::vector<int32_t> boundary_checks_;\n+};\n+\n+enum MaskExpandDimension { kMajor = 0, kMinor = 1 };\n+\n+Value EmitMaskOnInput(EmitterLocOpBuilder& b,\n+                      MaskExpandDimension expand_along_dimension, Value input,\n+                      int dim_k_denom, Value k, int64_t dims_k, int64_t block_k,\n+                      Value pid_k, int64_t other_dim_block_size) {\n+  int block_k_size = block_k / dim_k_denom;\n+  auto dim_k_elements_to_keep =\n+      b.create<ma::SubIOp>(Cst32(b, dims_k / dim_k_denom), k);\n+  auto is_last_tile_cond = b.create<ma::CmpIOp>(\n+      ma::CmpIPredicate::slt, dim_k_elements_to_keep, Cst32(b, block_k_size));\n+  auto input_type = mlir::cast<mlir::RankedTensorType>(input.getType());\n+  auto input_element_type = input_type.getElementType();\n+\n+  // If the input is a scalar, we need to expand it to a 2D tensor.\n+  // Otherwise, keep the input type.\n+  auto expanded_input_type = [&](Value input) {\n+    if (input_type.getRank() != 0) return input_type;\n+    // expand along the major dimension.\n+    if (expand_along_dimension == kMajor) {\n+      return mlir::RankedTensorType::get(\n+          ArrayRef<int64_t>{other_dim_block_size, block_k_size},\n+          input_element_type);\n+    }\n+    // expand along the minor dimension.\n+    return mlir::RankedTensorType::get(\n+        ArrayRef<int64_t>{block_k_size, other_dim_block_size},\n+        input_element_type);\n+  }(input);\n+\n+  auto expanded_input = input;\n+  // If the input is a scalar, we need to expand it to a 2D tensor.\n+  if (input_type.getRank() == 0) {\n+    expanded_input = b.create<mt::ExpandDimsOp>(expanded_input, 0);\n+    expanded_input = b.create<mt::ExpandDimsOp>(expanded_input, 0);\n+    expanded_input =\n+        b.create<mt::BroadcastOp>(expanded_input_type, expanded_input);\n+  }\n+\n+  auto if_op = b.create<mlir::scf::IfOp>(\n+      is_last_tile_cond, /*thenBranch=*/\n+      [&, &parent_builder = b](mlir::OpBuilder& builder, mlir::Location loc) {\n+        EmitterLocOpBuilder b(loc, builder, parent_builder.annotate_loc());\n+        // Make a range vector from 0 to block_k.\n+        auto range_from_0_to_k = Range(b, block_k_size);\n+        if (pid_k != nullptr) {\n+          range_from_0_to_k = b.create<ma::AddIOp>(\n+              range_from_0_to_k,\n+              Splat(b, b.create<ma::MulIOp>(pid_k, Cst32(b, block_k_size)),\n+                    block_k_size));\n+        }\n+        // Make it a 2D matrix.\n+        TensorValue range_from_0_to_k_2d = mlir::cast<TensorValue>(\n+            b.create<mt::ExpandDimsOp>(range_from_0_to_k,\n+                                       expand_along_dimension)\n+                .getResult());\n+        // Make 2d vector of dim_k_elements_to_keep.\n+        auto dim_k_elements_to_keep_2d =\n+            Splat(b, dim_k_elements_to_keep,\n+                  range_from_0_to_k_2d.getType().getShape());\n+        // The mask is true for elements in range_from_0_to_k_2d that are less\n+        // than dim_k_elements_to_keep.\n+        auto elements_mask_vector =\n+            b.create<ma::CmpIOp>(ma::CmpIPredicate::slt, range_from_0_to_k_2d,\n+                                 dim_k_elements_to_keep_2d);\n+\n+        Value elements_mask_matrix = b.create<mt::BroadcastOp>(\n+            expanded_input_type.clone(b.getI1Type()), elements_mask_vector);\n+\n+        // Zeros to use instead of the masked elements.\n+        auto zeros = CreateConst(b, input_element_type, 0,\n+                                 expanded_input_type.getShape());\n+        auto result =\n+            b.create<ma::SelectOp>(elements_mask_matrix, expanded_input, zeros);\n+        b.create<mlir::scf::YieldOp>(mlir::ValueRange(result));\n+      },\n+      /*elseBranch=*/\n+      [&, &parent_builder = b](mlir::OpBuilder& builder, mlir::Location loc) {\n+        // We don't need to mask anything but we need to expand the input.\n+        // Otherwise Triton complains.\n+        EmitterLocOpBuilder b(loc, builder, parent_builder.annotate_loc());\n+        b.create<mlir::scf::YieldOp>(mlir::ValueRange(expanded_input));\n+      });\n+  return if_op.getResult(0);\n+}\n+\n+absl::StatusOr<TritonGemmConfig> GetTritonGemmConfig(\n+    const HloFusionInstruction* fusion) {\n+  auto backend_config =\n+      fusion->backend_config<GpuBackendConfig>()->fusion_backend_config();\n+\n+  if (!backend_config.has_triton_gemm_config()) {\n+    // TODO(bchetioui): consolidate default parameters. At the moment, these\n+    // may be constructed in two distinct places.\n+    LOG(WARNING) << \"Using fallback triton GEMM config for op \"\n+                 << fusion->name();\n+    *backend_config.mutable_triton_gemm_config() = DefaultTritonGemmKey();\n+  }\n+\n+  return TritonGemmConfig::FromProto(backend_config.triton_gemm_config());\n+}\n+\n+Type GetIndexType(EmitterLocOpBuilder& b, const HloDotInstruction& dot_instr,\n+                  const TritonGemmConfig& config) {\n+  // Use 32-bit indexing if addressing any of the inputs or the output (which\n+  // could grow if split_k is set) does not cross the INT_MAX boundary.\n+  // Otherwise, fall back to 64-bit indexing, which is slower.\n+  bool use_64bit_indexing =\n+      ShapeUtil::ElementsIn(dot_instr.operand(0)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr.operand(1)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr.shape()) * config.split_k > INT_MAX;\n+  return b.getIntegerType(use_64bit_indexing ? 64 : 32);\n+}\n+\n+absl::Status EmitForLoopBody(EmitterLocOpBuilder& b,\n+                             MatMulEmitterHelper& emitter, const Scopes& scopes,\n+                             const HloDotInstruction* dot_instr,\n+                             const MatMulDims& dims,\n+                             const llvm::SmallVector<IterableInput>& inputs,\n+                             Value ki, ValueRange iter_args) {\n+  SmallVector<Value> args_for_yield;\n+  std::array<absl::flat_hash_map<const HloInstruction*, Value>, 2> values;\n+\n+  // Load tiles of all parameters of LHS and RHS scopes and advance pointers.\n+  for (const IterableInput& input : inputs) {\n+    Value param_value = input.EmitLoad(b, iter_args[input.iter_arg_index_]);\n+    CHECK(values[input.operand_index_]\n+              .insert({input.hlo_instr_, param_value})\n+              .second);\n+    args_for_yield.push_back(input.EmitAdvance(b, emitter, ki, iter_args));\n+  }\n+\n+  // Emit all operations of LHS and RHS scopes.\n+  Value dot_lhs =\n+      emitter.MakeInput(b, scopes.lhs(), kLhsIndex, values[kLhsIndex]);\n+  Value dot_rhs =\n+      emitter.MakeInput(b, scopes.rhs(), kRhsIndex, values[kRhsIndex]);\n+\n+  // Operation in the fusion before the dot can alter the elements of the\n+  // tiles that were zero masked during loads. These have to be zeroed here\n+  // again just before the dot so that they do not affect the output.\n+  // Only the K dimension needs masking here because unnecessary elements in\n+  // the other two get discarded by the masked store at the end.\n+  const bool need_masking =\n+      dims.k % (dims.config.block_k * dims.config.split_k) > 0;\n+  if (need_masking) {\n+    dot_lhs = EmitMaskOnInput(b, MaskExpandDimension::kMajor, dot_lhs, 1, ki,\n+                              dims.k, dims.config.block_k, scopes.pid_k(),\n+                              dims.config.block_m);\n+    dot_rhs = EmitMaskOnInput(b, MaskExpandDimension::kMinor, dot_rhs, 1, ki,\n+                              dims.k, dims.config.block_k, scopes.pid_k(),\n+                              dims.config.block_n);\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      Value acc_next,\n+      triton::EmitSingleTileDot(\n+          b, *dot_instr,\n+          triton::DotOperands{dot_lhs, dot_rhs, iter_args.back()}));\n+  args_for_yield.push_back(acc_next);\n+  b.create<mlir::scf::YieldOp>(args_for_yield);\n+\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace\n+\n+// Use tiling and execution parameters from 'config'. BlockLevelParameters are\n+// ignored.\n+// Variable naming: lhs [m, k] x rhs [k, n] -> out [m, n].\n+absl::Status EmitMatMul(EmitterLocOpBuilder& b,\n+                        absl::string_view libdevice_path,\n+                        const se::DeviceDescription& device_info,\n+                        const HloFusionInstruction* fusion,\n+                        xtile::EntryFuncOp fn, const BlockLevelParameters&) {\n+  TF_ASSIGN_OR_RETURN(TritonGemmConfig config, GetTritonGemmConfig(fusion));\n+  TF_ASSIGN_OR_RETURN(auto analysis,\n+                      TritonFusionAnalysis::Execute(\n+                          *fusion->called_computation(), config.split_k));\n+\n+  absl::Status status = CheckGemmTilingComplexityHeuristic(config);\n+  if (!status.ok()) {\n+    VLOG(1) << \"EmitMatMul heuristic check failed: \"\n+            << fusion->called_computation()->ToString() << status.message();\n+    return status;\n+  }\n+\n+  const HloComputation* computation = fusion->fused_instructions_computation();\n+  const HloInstruction* instr =\n+      hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n+  const HloDotInstruction* dot_instr = DynCast<HloDotInstruction>(instr);\n+\n+  Type index_ty = GetIndexType(b, *dot_instr, config);\n+\n+  const HloInstruction* root = dot_instr->parent()->root_instruction();\n+  TF_RET_CHECK(!root->shape().IsTuple());\n+\n+  TF_RETURN_IF_ERROR(ValidateMatMulConfig(config, *dot_instr));\n+  const int split_k = config.split_k;\n+  const int block_m = config.block_m;\n+  const int block_k = config.block_k;\n+  const int block_n = config.block_n;\n+\n+  TF_ASSIGN_OR_RETURN(const MatMulDims dims,\n+                      MatMulDims::Create(config, *dot_instr, analysis));\n+  const MatMulLaunchConfig launch_config(config, *dot_instr, dims, device_info);\n+  VLOG(6) << analysis.ToString();\n+\n+  MatMulEmitterHelper emitter(libdevice_path, device_info, dot_instr, index_ty,\n+                              dims, launch_config, analysis);\n+\n+  TF_ASSIGN_OR_RETURN(mlir::Type acc_ty,\n+                      triton::GetDotAccumulatorType(b, *dot_instr));\n+\n+  ma::ConstantOp accumulator_init =\n+      CreateConst(b, acc_ty, 0, {block_m, block_n});\n+\n+  // Calculate the sizes of the lhs, rhs, and output sides.\n+  Scopes scopes(b, dot_instr, analysis, dims, config, launch_config);\n+\n+  llvm::SmallVector<IterableInput> inputs;\n+\n+  // Pointers to inputs of LHS scope, then RHS, then the accumulator\n+  // that change with every loop iteration and are passed between them.\n+  SmallVector<Value> iter_args;\n+  int64_t step_k = block_k * split_k;\n+  for (const Side* side : scopes.input_scopes()) {\n+    for (const HloInstruction* input_hlo : ScopeInputs(analysis, side->scope)) {\n+      TF_ASSIGN_OR_RETURN(SmallVector<Value> arguments,\n+                          GetArguments(fn, *input_hlo));\n+      TF_ASSIGN_OR_RETURN(\n+          IterableInput iter_input,\n+          IterableInput::CreateIterableInput(iter_args.size(), b, dims, side,\n+                                             input_hlo, step_k));\n+      TF_ASSIGN_OR_RETURN(Value tensor_ptr,\n+                          emitter.EmitTensorPointer(\n+                              b, input_hlo, *side, arguments, scopes.pid_k(),\n+                              iter_input.boundary_checks_));\n+      inputs.push_back(iter_input);\n+      iter_args.push_back(tensor_ptr);\n+    }\n+  }\n+\n+  auto body_builder_callback = [&](mlir::OpBuilder&, mlir::Location, Value ki,\n+                                   ValueRange iter_args) -> void {\n+    CHECK_OK(EmitForLoopBody(b, emitter, scopes, dot_instr, dims, inputs, ki,\n+                             iter_args));\n+  };\n+\n+  iter_args.push_back(accumulator_init);\n+  Value acc_final = b.create<mlir::scf::ForOp>(\n+                         /*lowerBound*/ Cst32(b, 0),\n+                         /*upperBound*/ Cst32(b, dims.k),\n+                         /*step*/ Cst32(b, step_k),\n+                         /*iterArgs*/ iter_args, body_builder_callback)\n+                        .getResult(iter_args.size() - 1);\n+  absl::flat_hash_map<const HloInstruction*, Value> values_out;\n+  TF_ASSIGN_OR_RETURN(Type acc_final_ty,\n+                      TritonType(b, dot_instr->shape().element_type()));\n+  values_out[dot_instr] = triton::Cast(b, acc_final, acc_final_ty);\n+\n+  // Emit the output scope.\n+  if (std::vector<const HloInstruction*> to_emit =\n+          emitter.EpiloguePostOrderTransitiveOperands(root);\n+      !to_emit.empty()) {\n+    for (const HloInstruction* input :\n+         ScopeInputs(analysis, TritonFusionAnalysis::Scope::OUTPUT)) {\n+      std::vector<int32_t> boundary_checks;\n+      TF_ASSIGN_OR_RETURN(SmallVector<Value> arguments,\n+                          GetArguments(fn, *input));\n+      TF_ASSIGN_OR_RETURN(\n+          Value tensor_pointer,\n+          emitter.EmitTensorPointer(b, input, scopes.out(), arguments,\n+                                    scopes.pid_k(), boundary_checks));\n+      TF_RET_CHECK(values_out\n+                       .insert({input, EmitParameterLoad(b, tensor_pointer,\n+                                                         boundary_checks)})\n+                       .second);\n+    }\n+    TF_RETURN_IF_ERROR(EmitScope(b, libdevice_path, device_info, &analysis,\n+                                 scopes.out(), to_emit, values_out)\n+                           .status());\n+  }\n+\n+  // Emit tensor store operations for all outputs.\n+  for (int i = 0;\n+       i < fn.getBufferArgs().size() - dot_instr->parent()->num_parameters();\n+       ++i) {\n+    const HloInstruction* producer =\n+        root->shape().IsTuple() ? root->operand(i) : root;\n+    std::vector<int32_t> boundary_checks;\n+    TF_ASSIGN_OR_RETURN(\n+        Value tensor_pointer,\n+        emitter.EmitTensorPointer(\n+            b, producer, scopes.out(),\n+            {fn.getArgument(i + dot_instr->parent()->num_parameters())},\n+            scopes.pid_k(), boundary_checks));\n+    b.create<mt::StoreOp>(tensor_pointer, values_out[producer], boundary_checks,\n+                          mt::CacheModifier::NONE, mt::EvictionPolicy::NORMAL);\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<LaunchDimensions> GetMatMulLaunchDimensions(\n+    const TritonFusionAnalysis& analysis, const HloFusionAdaptor& fusion,\n+    const TritonGemmConfig& config, const se::DeviceDescription& device_info) {\n+  auto dot = HloBfsFindIf(fusion.GetRoots(), fusion, [](auto node) {\n+    return node.opcode() == HloOpcode::kDot;\n+  });\n+  TF_RET_CHECK(dot != std::nullopt);\n+  const auto& dot_instr =\n+      *static_cast<const HloDotInstruction*>(&dot->instruction());\n+  TF_ASSIGN_OR_RETURN(MatMulDims dims,\n+                      MatMulDims::Create(config, dot_instr, analysis));\n+  MatMulLaunchConfig launch_config(config, dot_instr, dims, device_info);\n+  return launch_config.launch_dims;\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "cf765afb7474e45ce398166956577061fd3ef4cb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h",
            "status": "added",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.h?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -0,0 +1,51 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_CODEGEN_TRITON_FUSION_EMITTER_LEGACY_MATMUL_H_\n+#define XLA_BACKENDS_GPU_CODEGEN_TRITON_FUSION_EMITTER_LEGACY_MATMUL_H_\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir/Interfaces/FunctionInterfaces.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/triton_fusion_analysis.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+\n+namespace xla::gpu {\n+\n+// Compute the launch dimensions for the given Triton MatMul.\n+absl::StatusOr<LaunchDimensions> GetMatMulLaunchDimensions(\n+    const TritonFusionAnalysis& analysis, const HloFusionAdaptor& fusion,\n+    const TritonGemmConfig& config, const se::DeviceDescription& device_info);\n+\n+// Use tiling and execution parameters from 'config'. BlockLevelParameters are\n+// ignored.\n+// Variable naming: lhs [m, k] x rhs [k, n] -> out [m, n].\n+absl::Status EmitMatMul(EmitterLocOpBuilder& builder,\n+                        absl::string_view libdevice_path,\n+                        const se::DeviceDescription& device_info,\n+                        const HloFusionInstruction* fusion,\n+                        xtile::EntryFuncOp fn, const BlockLevelParameters&);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_FUSION_EMITTER_LEGACY_MATMUL_H_"
        },
        {
            "sha": "13e60202238d0d9cbff096e9f529fd32c764895f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul_stub.cc",
            "status": "added",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul_stub.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -0,0 +1,48 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir/Interfaces/FunctionInterfaces.h\"\n+#include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/triton_fusion_analysis.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+\n+namespace xla::gpu {\n+\n+// Compute the launch dimensions for the given Triton MatMul.\n+absl::StatusOr<LaunchDimensions> GetMatMulLaunchDimensions(\n+    const TritonFusionAnalysis& analysis, const HloFusionAdaptor& fusion,\n+    const TritonGemmConfig& config, const se::DeviceDescription& device_info) {\n+  return absl::UnimplementedError(\"not supported for this build configuration\");\n+}\n+\n+absl::Status EmitMatMul(EmitterLocOpBuilder& builder,\n+                        absl::string_view libdevice_path,\n+                        const se::DeviceDescription& device_info,\n+                        const HloFusionInstruction* fusion,\n+                        xtile::EntryFuncOp fn, const BlockLevelParameters&) {\n+  return absl::UnimplementedError(\"not supported for this build configuration\");\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "afce47b2e43b35234d9a55c4e08f1d04e62a19e5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_parametrized_legacy_test.cc",
            "status": "added",
            "additions": 909,
            "deletions": 0,
            "changes": 909,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_legacy_test.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -0,0 +1,909 @@\n+/* Copyright 2023 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <algorithm>\n+#include <array>\n+#include <string>\n+#include <tuple>\n+#include <variant>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/base/optimization.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_replace.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/strings/substitute.h\"\n+#include \"xla/backends/gpu/codegen/triton/support_legacy.h\"\n+#include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n+#include \"xla/comparison_util.h\"\n+#include \"xla/error_spec.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+struct MixTypeParams {\n+  PrimitiveType lhs_ty;\n+  PrimitiveType rhs_ty;\n+  int m;\n+  int k;\n+  int n;\n+  float aabs = 1e-6;\n+  float arel = 1e-6;\n+};\n+\n+class MixedTypeTest : public GpuCodegenTest,\n+                      public ::testing::WithParamInterface<MixTypeParams> {\n+ public:\n+  se::GpuComputeCapability GetGpuComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability();\n+  }\n+\n+  void SetUp() override {\n+    if (GetGpuComputeCapability().IsRocm()) {\n+      GTEST_SKIP()\n+          << \"Related fusions are not performed on ROCm without Triton.\";\n+    }\n+  }\n+\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n+    // We are testing Triton, remove cuBLAS fallback for these tests.\n+    debug_options.set_xla_gpu_cublas_fallback(false);\n+    // Always rewrite Gemms with Triton regardless of size.\n+    debug_options.set_xla_gpu_gemm_rewrite_size_threshold(0);\n+    // That is a test for legacy Triton emitter that is being replaced by the\n+    // generic Triton emitter.\n+    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n+    return debug_options;\n+  }\n+};\n+\n+TEST_P(MixedTypeTest, MixedTypeDotProducesCorrectResult) {\n+  MixTypeParams params = GetParam();\n+  const std::string hlo_string_template = R\"(\n+HloModule m\n+\n+ENTRY e {\n+  p0 = $0[$2,$3] parameter(0)\n+  p0c = $1[$2,$3] convert(p0)\n+  p1 = $1[$3,$4] parameter(1)\n+  ROOT _ = $1[$2,$4] dot(p0c, p1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+})\";\n+  std::string hlo_string = absl::Substitute(\n+      hlo_string_template,\n+      primitive_util::LowercasePrimitiveTypeName(params.lhs_ty),\n+      primitive_util::LowercasePrimitiveTypeName(params.rhs_ty), params.m,\n+      params.k, params.n);\n+  MatchOptimizedHlo(hlo_string, R\"(\n+; CHECK: ENTRY\n+; CHECK-NEXT: parameter\n+; CHECK-NEXT: parameter\n+; CHECK-NEXT: kCustom\n+)\");\n+\n+  EXPECT_TRUE(RunAndCompare(hlo_string, ErrorSpec{params.aabs, params.arel}));\n+}\n+\n+std::string GemmTestParamsParamsToString(\n+    const ::testing::TestParamInfo<MixTypeParams>& data) {\n+  return absl::StrCat(\n+      primitive_util::LowercasePrimitiveTypeName(data.param.lhs_ty), \"_\",\n+      primitive_util::LowercasePrimitiveTypeName(data.param.rhs_ty), \"_\",\n+      data.param.m, \"_\", data.param.k, \"_\", data.param.n);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(RewriteTestSuite, MixedTypeTest,\n+                         ::testing::ValuesIn({\n+                             MixTypeParams{PRED, F16, 16, 32, 8},\n+                             MixTypeParams{PRED, BF16, 16, 32, 8},\n+                             MixTypeParams{PRED, F32, 16, 32, 8, 2e-4, 2e-3},\n+                             MixTypeParams{S8, F16, 16, 32, 8},\n+                             MixTypeParams{S8, BF16, 16, 32, 8},\n+                             MixTypeParams{S8, F32, 16, 32, 8, 5e-2, 1e-2},\n+                             MixTypeParams{S8, F32, 101, 7, 303, 0.1, 0.1},\n+                             MixTypeParams{S8, F32, 101, 32, 303, 0.1, 0.1},\n+                             MixTypeParams{S8, F32, 101, 2048, 303, 0.5, 0.1},\n+                             MixTypeParams{S8, F32, 101, 2555, 303, 0.5, 0.1},\n+                             // Is supported but overflows.\n+                             //  GemmTestParams{S32, F16},\n+                             MixTypeParams{S16, F16, 30, 19, 12},\n+                             MixTypeParams{S32, F32, 4, 4, 4, 1, 1e-2},\n+                             MixTypeParams{F16, BF16, 16, 32, 8},\n+                             MixTypeParams{F16, F32, 16, 32, 8, 1e-3, 1e-6},\n+                             MixTypeParams{BF16, F16, 16, 32, 8, 1e-3, 1e-6},\n+                             MixTypeParams{BF16, F32, 16, 32, 8, 1e-3, 1e-6},\n+                             MixTypeParams{S8, BF16, 24, 40, 8},\n+                             MixTypeParams{S8, F16, 80, 16, 32, 1e-3, 1e-6},\n+                             MixTypeParams{F16, F32, 127, 3, 300, 1e-2, 1e-2},\n+                             MixTypeParams{F16, BF16, 544, 96, 16, 1e-3, 1e-3},\n+                             MixTypeParams{BF16, F32, 77, 500, 333, 3e-3, 3e-3},\n+                         }),\n+                         GemmTestParamsParamsToString);\n+\n+class TritonTest : public GpuCodegenTest {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_cublas_fallback(false);\n+    // Always rewrite Gemms with Triton regardless of size.\n+    debug_options.set_xla_gpu_gemm_rewrite_size_threshold(0);\n+    // Use legacy Triton emitter for these tests by removing all generic\n+    // Triton emitter features.\n+    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n+    return debug_options;\n+  }\n+\n+  se::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+};\n+\n+class ElementwiseTest : public TritonTest,\n+                        public ::testing::WithParamInterface<\n+                            std::tuple<PrimitiveType, HloOpcode, float>> {};\n+\n+std::string ElementwiseTestParamsToString(\n+    const ::testing::TestParamInfo<std::tuple<PrimitiveType, HloOpcode, float>>&\n+        data) {\n+  PrimitiveType data_type;\n+  HloOpcode opcode;\n+  float tolerance;\n+  std::tie(data_type, opcode, tolerance) = data.param;\n+  return absl::StrCat(\n+      primitive_util::LowercasePrimitiveTypeName(data_type), \"_\",\n+      absl::StrReplaceAll(HloOpcodeString(opcode), {{\"-\", \"_\"}}));\n+}\n+\n+using UnaryElementwiseTest = ElementwiseTest;\n+\n+TEST_P(UnaryElementwiseTest, ElementwiseFusionExecutesCorrectly) {\n+  PrimitiveType data_type;\n+  HloOpcode opcode;\n+  float tolerance;\n+  std::tie(data_type, opcode, tolerance) = GetParam();\n+\n+  const std::string kHloTestTemplate = R\"(\n+triton_gemm___computation {\n+  parameter_0 = f32[15,33]{1,0} parameter(0)\n+  parameter_1 = $0[33,68]{1,0} parameter(1)\n+  f1.1 = $0[33,68]{1,0} $1(parameter_1)\n+  c.1 = f32[33,68]{1,0} convert(f1.1)\n+  ROOT _.1 = f32[15,68]{1,0} dot(parameter_0, c.1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    operand_precision={HIGH, HIGH}\n+}\n+\n+ENTRY e {\n+  p1 = $0[33,68]{1,0} parameter(1)\n+  p0 = f32[15,33]{1,0} parameter(0)\n+  ROOT triton_gemm__ = f32[15,68]{1,0} fusion(p0, p1), kind=kCustom,\n+    calls=triton_gemm___computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n+                    \"triton_gemm_config\":\n+                      {\"block_m\":\"32\",\n+                       \"block_n\":\"32\",\n+                       \"block_k\":\"32\",\n+                       \"split_k\":\"1\",\n+                       \"num_stages\":\"1\",\n+                       \"num_warps\":\"4\",\n+                       \"num_ctas\":\"1\"}}}\n+})\";\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  const std::string kHloRefTemplate = R\"(\n+fused_computation {\n+  param_0.1 = $0[33,68]{1,0} parameter(0)\n+  f.1 = $0[33,68]{1,0} $1(param_0.1)\n+  ROOT convert.1 = f32[33,68]{1,0} convert(f.1)\n+}\n+\n+ENTRY e {\n+  p1 = $0[33,68]{1,0} parameter(1)\n+  p0 = f32[15,33]{1,0} parameter(0)\n+  fusion = f32[33,68]{1,0} fusion(p1), kind=kLoop, calls=fused_computation\n+  gemm = (f32[15,68]{1,0}, s8[0]{0}) custom-call(p0, fusion),\n+    custom_call_target=\"__cublas$$gemm\",\n+    backend_config={\"gemm_backend_config\":{\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":\n+      {\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\n+      \"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\n+      \"alpha_imag\":0,\"precision_config\":\n+      {\"operand_precision\":[\"HIGHEST\",\"HIGHEST\"]},\"epilogue\":\"DEFAULT\"}}\n+   ROOT get-tuple-element = f32[15,68]{1,0} get-tuple-element((f32[15,68]{1,0}, s8[0]{0}) gemm), index=0\n+})\";\n+  const std::string hlo_ref = absl::Substitute(\n+      kHloRefTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(\n+      hlo_ref, hlo_test, ErrorSpec{/*aabs=*/tolerance, /*arel=*/tolerance},\n+      /*run_hlo_passes=*/false));\n+}\n+\n+TEST_P(UnaryElementwiseTest, ElementwiseUnaryOpExecutesCorrectly) {\n+  PrimitiveType data_type;\n+  HloOpcode opcode;\n+  float tolerance;\n+  std::tie(data_type, opcode, tolerance) = GetParam();\n+\n+  const std::string kHloTestTemplate = R\"(\n+triton_computation {\n+  parameter_0 = $0[33,68]{1,0} parameter(0)\n+  output = $0[33,68]{1,0} $1(parameter_0)\n+  ROOT convert = f32[33,68]{1,0} convert(output)\n+}\n+\n+ENTRY e {\n+  p0 = $0[33,68]{1,0} parameter(0)\n+  ROOT triton_fusion = f32[33,68]{1,0} fusion(p0), kind=kCustom,\n+    calls=triton_computation, backend_config={\n+      \"fusion_backend_config\":{\n+      \"kind\":\"__triton\",\n+      \"block_level_fusion_config\":{\n+        \"output_tiles\":[{\"sizes\":[\"1\", \"1\"]}],\n+        \"num_warps\":\"1\",\n+        \"num_ctas\":\"1\",\n+        \"num_stages\":\"1\"}}}\n+})\";\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  const std::string kHloRefTemplate = R\"(\n+fused_computation {\n+  param_0.1 = $0[33,68]{1,0} parameter(0)\n+  output = $0[33,68]{1,0} $1(param_0.1)\n+  ROOT convert = f32[33,68]{1,0} convert(output)\n+}\n+\n+ENTRY e {\n+  p0 = $0[33,68]{1,0} parameter(0)\n+  ROOT fusion = f32[33,68]{1,0} fusion(p0), kind=kLoop, calls=fused_computation\n+})\";\n+  const std::string hlo_ref = absl::Substitute(\n+      kHloRefTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(\n+      hlo_ref, hlo_test, ErrorSpec{/*aabs=*/tolerance, /*arel=*/tolerance},\n+      /*run_hlo_passes=*/false));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuitePRED, UnaryElementwiseTest,\n+    ::testing::Combine(\n+        ::testing::Values(PRED),\n+        ::testing::ValuesIn(\n+            legacy_triton::\n+                TritonSupportedUnaryElementwiseUpToFloatNormalization(PRED)),\n+        ::testing::Values(3e-2)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteS8, UnaryElementwiseTest,\n+    ::testing::Combine(\n+        ::testing::Values(S8),\n+        ::testing::ValuesIn(\n+            legacy_triton::\n+                TritonSupportedUnaryElementwiseUpToFloatNormalization(S8)),\n+        ::testing::Values(3e-2)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteS16, UnaryElementwiseTest,\n+    ::testing::Combine(\n+        ::testing::Values(S16),\n+        ::testing::ValuesIn(\n+            legacy_triton::\n+                TritonSupportedUnaryElementwiseUpToFloatNormalization(S16)),\n+        ::testing::Values(1e-3)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteS32, UnaryElementwiseTest,\n+    ::testing::Combine(\n+        ::testing::Values(S32),\n+        ::testing::ValuesIn(\n+            legacy_triton::\n+                TritonSupportedUnaryElementwiseUpToFloatNormalization(S32)),\n+        ::testing::Values(1e-3)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteF16, UnaryElementwiseTest,\n+    ::testing::Combine(\n+        ::testing::Values(F16),\n+        ::testing::ValuesIn(\n+            legacy_triton::\n+                TritonSupportedUnaryElementwiseUpToFloatNormalization(F16)),\n+        ::testing::Values(2e-4)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteF32, UnaryElementwiseTest,\n+    ::testing::Combine(\n+        ::testing::Values(F32),\n+        ::testing::ValuesIn(\n+            legacy_triton::\n+                TritonSupportedUnaryElementwiseUpToFloatNormalization(F32)),\n+        ::testing::Values(1e-6)),\n+    ElementwiseTestParamsToString);\n+\n+using BinaryElementwiseTest = ElementwiseTest;\n+\n+TEST_P(BinaryElementwiseTest, ElementwiseFusionExecutesCorrectly) {\n+  PrimitiveType data_type;\n+  HloOpcode opcode;\n+  float tolerance;\n+  std::tie(data_type, opcode, tolerance) = GetParam();\n+\n+  const std::string kHloTestTemplate = R\"(\n+triton_gemm___computation {\n+  parameter_0 = f32[92,11]{1,0} parameter(0)\n+  parameter_1 = $0[11,63]{1,0} parameter(1)\n+  parameter_2 = $0[11,63]{1,0} parameter(2)\n+  f1.1 = $0[11,63]{1,0} $1(parameter_1, parameter_2)\n+  c.1 = f32[11,63]{1,0} convert(f1.1)\n+  ROOT _.1 = f32[92,63]{1,0} dot(parameter_0, c.1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    operand_precision={HIGH, HIGH}\n+}\n+\n+ENTRY e {\n+  p0 = f32[92,11]{1,0} parameter(0)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  p2 = $0[11,63]{1,0} parameter(2)\n+  ROOT triton_gemm__ = f32[92,63]{1,0} fusion(p0, p1, p2), kind=kCustom,\n+    calls=triton_gemm___computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n+                    \"triton_gemm_config\":\n+                      {\"block_m\":\"64\",\n+                       \"block_n\":\"32\",\n+                       \"block_k\":\"64\",\n+                       \"split_k\":\"1\",\n+                       \"num_stages\":\"2\",\n+                       \"num_warps\":\"2\",\n+                       \"num_ctas\":\"1\"}}}\n+})\";\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  const std::string kHloRefTemplate = R\"(\n+fused_computation {\n+  p0 = $0[11,63]{1,0} parameter(0)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  f.1 = $0[11,63]{1,0} $1(p0, p1)\n+  ROOT convert.1 = f32[11,63]{1,0} convert(f.1)\n+}\n+\n+ENTRY e {\n+  p2 = $0[11,63]{1,0} parameter(2)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  p0 = f32[92,11]{1,0} parameter(0)\n+  fusion = f32[11,63]{1,0} fusion(p1, p2), kind=kLoop, calls=fused_computation\n+  gemm = (f32[92,63]{1,0}, s8[0]{0}) custom-call(p0, fusion),\n+    custom_call_target=\"__cublas$$gemm\",\n+    backend_config={\"gemm_backend_config\":{\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":\n+      {\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\n+      \"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\n+      \"alpha_imag\":0,\"precision_config\":\n+      {\"operand_precision\":[\"HIGHEST\",\"HIGHEST\"]},\"epilogue\":\"DEFAULT\"}}\n+  ROOT get-tuple-element = f32[92,63]{1,0} get-tuple-element((f32[92,63]{1,0}, s8[0]{0}) gemm), index=0\n+})\";\n+  const std::string hlo_ref = absl::Substitute(\n+      kHloRefTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(\n+      hlo_ref, hlo_test, ErrorSpec{/*aabs=*/tolerance, /*arel=*/tolerance},\n+      /*run_hlo_passes=*/false, /*args_max_bits_of_precision=*/6));\n+}\n+\n+TEST_P(BinaryElementwiseTest, ElementwiseBinaryOpExecutesCorrectly) {\n+  PrimitiveType data_type;\n+  HloOpcode opcode;\n+  float tolerance;\n+  std::tie(data_type, opcode, tolerance) = GetParam();\n+\n+  const std::string kHloTestTemplate = R\"(\n+triton_computation {\n+  parameter_0 = $0[11,63]{1,0} parameter(0)\n+  parameter_1 = $0[11,63]{1,0} parameter(1)\n+  output = $0[11,63]{1,0} $1(parameter_0, parameter_1)\n+  ROOT c.1 = f32[11,63]{1,0} convert(output)\n+}\n+\n+ENTRY e {\n+  p0 = $0[11,63]{1,0} parameter(0)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  ROOT triton_fusion = f32[11,63]{1,0} fusion(p0, p1), kind=kCustom,\n+    calls=triton_computation, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"1\", \"1\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_ctas\":\"1\",\n+          \"num_stages\":\"1\"}}}\n+})\";\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  const std::string kHloRefTemplate = R\"(\n+fused_computation {\n+  p0 = $0[11,63]{1,0} parameter(0)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  output = $0[11,63]{1,0} $1(p0, p1)\n+  ROOT convert.1 = f32[11,63]{1,0} convert(output)\n+}\n+\n+ENTRY e {\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  p0 = $0[11,63]{1,0} parameter(0)\n+  ROOT fusion = f32[11,63]{1,0} fusion(p0, p1), kind=kLoop, calls=fused_computation\n+})\";\n+  const std::string hlo_ref = absl::Substitute(\n+      kHloRefTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      HloOpcodeString(opcode));\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(\n+      hlo_ref, hlo_test, ErrorSpec{/*aabs=*/tolerance, /*arel=*/tolerance},\n+      /*run_hlo_passes=*/false, /*args_max_bits_of_precision=*/6));\n+}\n+\n+bool HloOpcodeIsComparison(HloOpcode opcode) {\n+  return opcode == HloOpcode::kCompare;\n+}\n+std::vector<HloOpcode> TestedBinaryElementwise(PrimitiveType element_type) {\n+  std::vector<HloOpcode> ret =\n+      legacy_triton::TritonSupportedBinaryElementwiseUpToFloatNormalization(\n+          element_type);\n+  // Comparison requires an additional property.\n+  ret.erase(std::remove_if(ret.begin(), ret.end(), HloOpcodeIsComparison),\n+            ret.end());\n+  return ret;\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuitePRED, BinaryElementwiseTest,\n+    ::testing::Combine(::testing::Values(PRED),\n+                       ::testing::ValuesIn(TestedBinaryElementwise(PRED)),\n+                       ::testing::Values(0)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteS8, BinaryElementwiseTest,\n+    ::testing::Combine(::testing::Values(S8),\n+                       ::testing::ValuesIn(TestedBinaryElementwise(S8)),\n+                       ::testing::Values(0)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteS16, BinaryElementwiseTest,\n+    ::testing::Combine(::testing::Values(S16),\n+                       ::testing::ValuesIn(TestedBinaryElementwise(S16)),\n+                       ::testing::Values(0)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteS32, BinaryElementwiseTest,\n+    ::testing::Combine(::testing::Values(S32),\n+                       ::testing::ValuesIn(TestedBinaryElementwise(S32)),\n+                       ::testing::Values(0)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteF16, BinaryElementwiseTest,\n+    ::testing::Combine(::testing::Values(F16),\n+                       ::testing::ValuesIn(TestedBinaryElementwise(F16)),\n+                       ::testing::Values(2e-4)),\n+    ElementwiseTestParamsToString);\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ElementwiseTestSuiteF32, BinaryElementwiseTest,\n+    ::testing::Combine(::testing::Values(F32),\n+                       ::testing::ValuesIn(TestedBinaryElementwise(F32)),\n+                       ::testing::Values(1e-6)),\n+    ElementwiseTestParamsToString);\n+\n+class CompareTest : public TritonTest,\n+                    public ::testing::WithParamInterface<\n+                        std::tuple<PrimitiveType, Comparison::Direction>> {};\n+\n+std::string CompareTestParamsToString(\n+    const ::testing::TestParamInfo<\n+        std::tuple<PrimitiveType, Comparison::Direction>>& data) {\n+  PrimitiveType data_type;\n+  Comparison::Direction direction;\n+  std::tie(data_type, direction) = data.param;\n+  return absl::StrCat(primitive_util::LowercasePrimitiveTypeName(data_type),\n+                      \"_\", ComparisonDirectionToString(direction));\n+}\n+\n+TEST_P(CompareTest, CompareFusionExecutesCorrectly) {\n+  PrimitiveType data_type;\n+  Comparison::Direction direction;\n+  std::tie(data_type, direction) = GetParam();\n+\n+  const std::string kHloTestTemplate = R\"(\n+triton_gemm___computation {\n+  parameter_0 = f32[92,11]{1,0} parameter(0)\n+  parameter_1 = $0[11,63]{1,0} parameter(1)\n+  parameter_2 = $0[11,63]{1,0} parameter(2)\n+  f1.1 = pred[11,63]{1,0} compare(parameter_1, parameter_2), direction=$1\n+  c.1 = f32[11,63]{1,0} convert(f1.1)\n+  ROOT _.1 = f32[92,63]{1,0} dot(parameter_0, c.1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    operand_precision={HIGH, HIGH}\n+}\n+\n+ENTRY e {\n+  p0 = f32[92,11]{1,0} parameter(0)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  p2 = $0[11,63]{1,0} parameter(2)\n+  ROOT triton_gemm__ = f32[92,63]{1,0} fusion(p0, p1, p2), kind=kCustom,\n+    calls=triton_gemm___computation, backend_config={\n+      \"fusion_backend_config\":{\n+      \"kind\":\"__triton_gemm\",\n+      \"triton_gemm_config\": {\n+        \"block_m\":\"16\",\n+        \"block_n\":\"64\",\n+        \"block_k\":\"16\",\n+        \"split_k\":\"1\",\n+        \"num_stages\":\"3\",\n+        \"num_warps\":\"2\",\n+        \"num_ctas\":\"1\"}}}\n+})\";\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      ComparisonDirectionToString(direction));\n+\n+  const std::string kHloRefTemplate = R\"(\n+fused_computation {\n+  p0 = $0[11,63]{1,0} parameter(0)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  f.1 = pred[11,63]{1,0} compare(p0, p1), direction=$1\n+  ROOT convert.1 = f32[11,63]{1,0} convert(f.1)\n+}\n+\n+ENTRY e {\n+  p2 = $0[11,63]{1,0} parameter(2)\n+  p1 = $0[11,63]{1,0} parameter(1)\n+  p0 = f32[92,11]{1,0} parameter(0)\n+  fusion = f32[11,63]{1,0} fusion(p1, p2), kind=kLoop, calls=fused_computation\n+  gemm = (f32[92,63]{1,0}, s8[0]{0}) custom-call(p0, fusion),\n+    custom_call_target=\"__cublas$$gemm\",\n+    backend_config={\"gemm_backend_config\":{\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":\n+      {\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\n+      \"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\n+      \"alpha_imag\":0,\"precision_config\":\n+      {\"operand_precision\":[\"HIGHEST\",\"HIGHEST\"]},\"epilogue\":\"DEFAULT\"}}\n+  ROOT get-tuple-element = f32[92,63]{1,0} get-tuple-element((f32[92,63]{1,0}, s8[0]{0}) gemm), index=0\n+})\";\n+  const std::string hlo_ref = absl::Substitute(\n+      kHloRefTemplate, primitive_util::LowercasePrimitiveTypeName(data_type),\n+      ComparisonDirectionToString(direction));\n+\n+  float tolerance;\n+  switch (data_type) {\n+    case F32:\n+      tolerance = 1e-6;\n+      break;\n+    case F16:\n+      tolerance = 2e-4;\n+      break;\n+    case PRED:\n+    case S8:\n+      tolerance = 3e-2;\n+      break;\n+    case S16:\n+      tolerance = 1e-3;\n+      break;\n+    case S32:\n+      tolerance = 1e-5;\n+      break;\n+    default:\n+      ABSL_UNREACHABLE();\n+  }\n+  EXPECT_TRUE(RunAndCompareTwoModules(\n+      hlo_ref, hlo_test, ErrorSpec{/*aabs=*/tolerance, /*arel=*/tolerance},\n+      /*run_hlo_passes=*/false));\n+}\n+\n+using cd = Comparison::Direction;\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    CompareTestSuite, CompareTest,\n+    ::testing::Combine(::testing::Values(PRED, S8, S16, S32, F16, F32),\n+                       ::testing::Values(cd::kEq, cd::kNe, cd::kGe, cd::kGt,\n+                                         cd::kLe, cd::kLt)),\n+    CompareTestParamsToString);\n+\n+class SelectTest : public TritonTest,\n+                   public ::testing::WithParamInterface<\n+                       std::tuple<PrimitiveType, PrimitiveType>> {};\n+\n+TEST_P(SelectTest, SelectFusionExecutesCorrectly) {\n+  PrimitiveType data_type1, data_type2;\n+  std::tie(data_type1, data_type2) = GetParam();\n+  for (const PrimitiveType type : {data_type1, data_type2}) {\n+    if (!legacy_triton::IsTritonSupportedDataType(type,\n+                                                  GetCudaComputeCapability())) {\n+      GTEST_SKIP() << absl::Substitute(\n+          \"Unsupported data type: $0\",\n+          primitive_util::LowercasePrimitiveTypeName(type));\n+    }\n+  }\n+\n+  const std::string kHloTestTemplate = R\"(\n+triton_gemm___computation {\n+  parameter_0 = $1[92,13]{1,0} parameter(0)\n+  parameter_1 = $0[13,63]{1,0} parameter(1)\n+  parameter_2 = $0[13,63]{1,0} parameter(2)\n+  parameter_3 = pred[13,63]{1,0} parameter(3)\n+  f1.1 = $0[13,63]{1,0} select(parameter_3, parameter_1, parameter_2)\n+  c.1 = $1[13,63]{1,0} convert(f1.1)\n+  ROOT _.1 = $1[92,63]{1,0} dot(parameter_0, c.1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    operand_precision={HIGH, HIGH}\n+}\n+\n+ENTRY e {\n+  p0 = $1[92,13]{1,0} parameter(0)\n+  p1 = $0[13,63]{1,0} parameter(1)\n+  p2 = $0[13,63]{1,0} parameter(2)\n+  p3 = pred[13,63]{1,0} parameter(3)\n+  ROOT triton_gemm__ = $1[92,63]{1,0} fusion(p0, p1, p2, p3), kind=kCustom,\n+    calls=triton_gemm___computation, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_gemm\",\n+        \"triton_gemm_config\": {\n+          \"block_m\":\"16\",\n+          \"block_n\":\"64\",\n+          \"block_k\":\"16\",\n+          \"split_k\":\"1\",\n+          \"num_stages\":\"3\",\n+          \"num_warps\":\"2\",\n+          \"num_ctas\":\"1\"}}}\n+})\";\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type1),\n+      primitive_util::LowercasePrimitiveTypeName(data_type2));\n+\n+  const std::string kHloRefTemplate = R\"(\n+fused_computation {\n+  p0 = $0[13,63]{1,0} parameter(0)\n+  p1 = $0[13,63]{1,0} parameter(1)\n+  p2 = pred[13,63]{1,0} parameter(2)\n+  f.1 = $0[13,63]{1,0} select(p2, p0, p1)\n+  ROOT convert.1 = $1[13,63]{1,0} convert(f.1)\n+}\n+\n+ENTRY e {\n+  p3 = pred[13,63]{1,0} parameter(3)\n+  p2 = $0[13,63]{1,0} parameter(2)\n+  p1 = $0[13,63]{1,0} parameter(1)\n+  p0 = $1[92,13]{1,0} parameter(0)\n+  fusion = $1[13,63]{1,0} fusion(p1, p2, p3), kind=kLoop,\n+    calls=fused_computation\n+  gemm = ($1[92,63]{1,0}, s8[0]{0}) custom-call(p0, fusion),\n+    custom_call_target=\"__cublas$$gemm\",\n+    backend_config={\"gemm_backend_config\":{\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":\n+      {\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\n+      \"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\n+      \"alpha_imag\":0,\"precision_config\":\n+      {\"operand_precision\":[\"HIGHEST\",\"HIGHEST\"]},\"epilogue\":\"DEFAULT\"}}\n+  ROOT get-tuple-element = $1[92,63]{1,0} get-tuple-element(($1[92,63]{1,0}, s8[0]{0}) gemm), index=0\n+})\";\n+  const std::string hlo_ref = absl::Substitute(\n+      kHloRefTemplate, primitive_util::LowercasePrimitiveTypeName(data_type1),\n+      primitive_util::LowercasePrimitiveTypeName(data_type2));\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(\n+      hlo_ref, hlo_test, ErrorSpec{/*aabs=*/0, /*arel=*/0},\n+      /*run_hlo_passes=*/false, /*args_max_bits_of_precision=*/9));\n+}\n+\n+std::string TwoPrimitiveTypesToString(\n+    const ::testing::TestParamInfo<std::tuple<PrimitiveType, PrimitiveType>>&\n+        data) {\n+  PrimitiveType data_type1;\n+  PrimitiveType data_type2;\n+  std::tie(data_type1, data_type2) = data.param;\n+  return absl::StrCat(primitive_util::LowercasePrimitiveTypeName(data_type1),\n+                      \"_\",\n+                      primitive_util::LowercasePrimitiveTypeName(data_type2));\n+}\n+\n+// BF16: depending on the GPU generation.\n+constexpr std::array<PrimitiveType, 7> kSupportedDataTypes{PRED, S8,  S16, S32,\n+                                                           F16,  F32, BF16};\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    SelectTestSuite, SelectTest,\n+    ::testing::Combine(::testing::ValuesIn(kSupportedDataTypes),\n+                       ::testing::Values(F16, BF16, F32)),\n+    TwoPrimitiveTypesToString);\n+\n+class ConstantTest : public TritonTest,\n+                     public ::testing::WithParamInterface<PrimitiveType> {};\n+\n+TEST_P(ConstantTest, ConstantFusionExecutesCorrectly) {\n+  const PrimitiveType data_type = GetParam();\n+  if (!legacy_triton::IsTritonSupportedDataType(data_type,\n+                                                GetCudaComputeCapability())) {\n+    GTEST_SKIP() << absl::Substitute(\n+        \"Unsupported data type: $0\",\n+        primitive_util::LowercasePrimitiveTypeName(data_type));\n+  }\n+\n+  const std::string kHloTestTemplate = R\"(\n+triton_gemm___computation {\n+  parameter_0 = f32[92,11]{1,0} parameter(0)\n+  parameter_1 = f32[11,63]{1,0} parameter(1)\n+  c = $0[] constant(123)\n+  b = $0[11,63] broadcast(c)\n+  cv = f32[11,63] convert(b)\n+  m = f32[11,63] multiply(cv, parameter_1)\n+  ROOT _.1 = f32[92,63]{1,0} dot(parameter_0, m),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    operand_precision={HIGH, HIGH}\n+}\n+\n+ENTRY e {\n+  p0 = f32[92,11]{1,0} parameter(0)\n+  p1 = f32[11,63]{1,0} parameter(1)\n+  ROOT triton_gemm__ = f32[92,63]{1,0} fusion(p0, p1), kind=kCustom,\n+    calls=triton_gemm___computation, backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_gemm\",\n+        \"triton_gemm_config\":{\n+          \"block_m\":\"16\",\n+          \"block_n\":\"64\",\n+          \"block_k\":\"16\",\n+          \"split_k\":\"1\",\n+          \"num_stages\":\"3\",\n+          \"num_warps\":\"2\",\n+          \"num_ctas\":\"1\"}}}\n+})\";\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type));\n+\n+  const std::string kHloRefTemplate = R\"(\n+fused_computation {\n+  p0 = f32[11,63]{1,0} parameter(0)\n+  c = $0[] constant(123)\n+  b = $0[11,63] broadcast(c)\n+  cv = f32[11,63] convert(b)\n+  ROOT m = f32[11,63] multiply(cv, p0)\n+}\n+\n+ENTRY e {\n+  p1 = f32[11,63]{1,0} parameter(1)\n+  p0 = f32[92,11]{1,0} parameter(0)\n+  fusion = f32[11,63]{1,0} fusion(p1), kind=kLoop,\n+    calls=fused_computation\n+  gemm = (f32[92,63]{1,0}, s8[0]{0}) custom-call(p0, fusion),\n+    custom_call_target=\"__cublas$$gemm\",\n+    backend_config={\"gemm_backend_config\":{\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":\n+      {\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\n+      \"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\n+      \"alpha_imag\":0,\"precision_config\":\n+      {\"operand_precision\":[\"HIGHEST\",\"HIGHEST\"]},\"epilogue\":\"DEFAULT\"}}\n+  ROOT get-tuple-element = f32[92,63]{1, 0} get-tuple-element((f32[92,63]{1, 0}, s8[0]{0}) gemm), index=0\n+})\";\n+  const std::string hlo_ref = absl::Substitute(\n+      kHloRefTemplate, primitive_util::LowercasePrimitiveTypeName(data_type));\n+\n+  float tolerance;\n+  switch (data_type) {\n+    case F32:\n+    case BF16:\n+      tolerance = 1e-6;\n+      break;\n+    case F16:\n+      tolerance = 2e-4;\n+      break;\n+    case PRED:\n+    case S8:\n+      tolerance = 3e-2;\n+      break;\n+    case S16:\n+      tolerance = 1e-3;\n+      break;\n+    case S32:\n+      tolerance = 1e-5;\n+      break;\n+    default:\n+      ABSL_UNREACHABLE();\n+  }\n+  EXPECT_TRUE(RunAndCompareTwoModules(\n+      hlo_ref, hlo_test, ErrorSpec{/*aabs=*/tolerance, /*arel=*/tolerance},\n+      /*run_hlo_passes=*/false));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(ConstantTestSuite, ConstantTest,\n+                         ::testing::ValuesIn(kSupportedDataTypes),\n+                         TritonSupportTestTypeToString);\n+\n+class ConvertTest : public TritonTest,\n+                    public ::testing::WithParamInterface<\n+                        std::tuple<PrimitiveType, PrimitiveType>> {};\n+\n+TEST_P(ConvertTest, ConvertFusionExecutesCorrectly) {\n+  PrimitiveType data_type1, data_type2;\n+  std::tie(data_type1, data_type2) = GetParam();\n+  for (const PrimitiveType type : {data_type1, data_type2}) {\n+    if (!legacy_triton::IsTritonSupportedDataType(type,\n+                                                  GetCudaComputeCapability())) {\n+      GTEST_SKIP() << absl::Substitute(\n+          \"Unsupported data type: $0\",\n+          primitive_util::LowercasePrimitiveTypeName(type));\n+    }\n+  }\n+\n+  const std::string hlo_text = absl::Substitute(\n+      R\"(\n+t {\n+  p0 = $0[2,2] parameter(0)\n+  p0c = $1[2,2] convert(p0)\n+  p0cc = f32[2,2] convert(p0c)\n+  p1 = f32[2,2] parameter(1)\n+  ROOT r = f32[2,2] dot(p0cc, p1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    operand_precision={HIGH, HIGH}\n+}\n+\n+ENTRY e {\n+  p0 = $0[2,2] parameter(0)\n+  p1 = f32[2,2] parameter(1)\n+  ROOT r = f32[2,2] fusion(p0, p1), kind=kCustom, calls=t,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n+})\",\n+      primitive_util::LowercasePrimitiveTypeName(data_type1),\n+      primitive_util::LowercasePrimitiveTypeName(data_type2));\n+\n+  MatchOptimizedHlo(hlo_text, R\"(\n+CHECK: block_m\n+  )\");\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    ConvertTestSuite, ConvertTest,\n+    ::testing::Combine(::testing::ValuesIn(kSupportedDataTypes),\n+                       ::testing::ValuesIn(kSupportedDataTypes)),\n+    TwoPrimitiveTypesToString);\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "41a2edde543cc2573d07bf1190103128d6bb70bc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -14,13 +14,18 @@ limitations under the License.\n ==============================================================================*/\n \n #include <gtest/gtest.h>\n+#include \"mlir/IR/Location.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n+#include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/hlo_module_config.h\"\n \n@@ -53,6 +58,16 @@ TEST(TritonStub, CallStubApi) {\n   EXPECT_TRUE(tiled_hlo.ok());\n }\n \n+TEST(TritonStub, CallLegacyMatMulApis) {\n+  HloConstantInstruction constant(Literal{});\n+  auto adaptor = HloFusionAdaptor::ForInstruction(&constant);\n+  EXPECT_FALSE(GetMatMulLaunchDimensions({}, *adaptor.get(), {}, {}).ok());\n+\n+  mlir::MLIRContext context;\n+  EmitterLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n+  EXPECT_FALSE(EmitMatMul(builder, {}, {}, nullptr, {}, {}).ok());\n+}\n+\n }  // namespace\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "605597bda175fd78f3cdb68bafad95f3ab1b5488",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -173,7 +173,9 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n           \"xtile_dialect_fn\",\n           TritonEmitterConstraints::GetBuilder(\n               TestGpuDeviceInfo::RTXA6000DeviceInfo()),\n-          fusion, block_level_parameters, *test->symbolic_expr_context()));\n+          fusion, block_level_parameters, *test->symbolic_expr_context(),\n+          ir_emitter_triton_internal::LegacyMatmulEmitter(\n+              TestGpuDeviceInfo::RTXA6000DeviceInfo())));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);"
        },
        {
            "sha": "3d0ab6735eb3544795213376ee412cc08cb04c73",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d939aafe98e0d037f6f07b3d444be32be3c37a7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h?ref=9d939aafe98e0d037f6f07b3d444be32be3c37a7",
            "patch": "@@ -32,7 +32,7 @@ class AutotuneCacheKey {\n   // Tie a version to the cache key in order to invalidate the cache when\n   // necessary. This should be incremented on triton upgrades or any other\n   // changes that may affect the autotuning results.\n-  static constexpr int kCurrentVersion = 17;\n+  static constexpr int kCurrentVersion = 18;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n                    const HloInstruction& instruction,"
        }
    ],
    "stats": {
        "total": 11042,
        "additions": 10969,
        "deletions": 73
    }
}