{
    "author": "qukhan",
    "message": "Cleanup `InterpreterBuilder::operator()`.\n\n- Instead of creating a `cleanup_on_error()` lambda that must be called when\n  returning an error, create the new interpreter using a scope bound\n  `unique_ptr` and `move` it to the output parameter when the function has\n  succeeded.\n- Cleanup on error would always reset the output parameter, so do that\n  unconditionally at the beginning of the function.\n- Fix overload that would not check if the output parameter was null before\n  dereferencing it.\n- Add a log when delegates fail to apply instead of silently failing.\n\nPiperOrigin-RevId: 802250347",
    "sha": "dc6d17a407b72e057ea4d34a918c021e41275ab4",
    "files": [
        {
            "sha": "67ba43b54e8ca8efc7d34d8f877375b49a105791",
            "filename": "tensorflow/lite/core/interpreter_builder.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 39,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dc6d17a407b72e057ea4d34a918c021e41275ab4/tensorflow%2Flite%2Fcore%2Finterpreter_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dc6d17a407b72e057ea4d34a918c021e41275ab4/tensorflow%2Flite%2Fcore%2Finterpreter_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Finterpreter_builder.cc?ref=dc6d17a407b72e057ea4d34a918c021e41275ab4",
            "patch": "@@ -778,7 +778,9 @@ TfLiteStatus InterpreterBuilder::operator()(\n     std::unique_ptr<Interpreter>* interpreter, int num_threads) {\n   TfLiteStatus status = SetNumThreads(num_threads);\n   if (status != kTfLiteOk) {\n-    interpreter->reset();\n+    if (interpreter) {\n+      interpreter->reset();\n+    }\n     return status;\n   }\n   return (*this)(interpreter);\n@@ -791,30 +793,24 @@ TfLiteStatus InterpreterBuilder::operator()(\n                          \"Null output pointer passed to InterpreterBuilder.\");\n     return kTfLiteError;\n   }\n-\n-  // Safe exit by deleting partially created interpreter, to reduce verbosity\n-  // on error conditions. Use by return cleanup_on_error();\n-  auto cleanup_and_error = [&interpreter]() {\n-    interpreter->reset();\n-    return kTfLiteError;\n-  };\n+  interpreter->reset();\n \n   if (!model_) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"Null pointer passed in as model.\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (model_->version() != TFLITE_SCHEMA_VERSION) {\n     TF_LITE_REPORT_ERROR(error_reporter_,\n                          \"Model provided is schema version %d not equal \"\n                          \"to supported version %d.\\n\",\n                          model_->version(), TFLITE_SCHEMA_VERSION);\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (BuildLocalIndexToRegistrationMapping() != kTfLiteOk) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"Registration failed.\\n\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   // Flatbuffer model schemas define a list of opcodes independent of the\n@@ -827,32 +823,32 @@ TfLiteStatus InterpreterBuilder::operator()(\n \n   if (subgraphs->size() == 0) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"No subgraph in the model.\\n\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (!buffers) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"No buffers in the model.\\n\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n-  *interpreter = std::make_unique<Interpreter>(error_reporter_);\n+  auto tmp_interpreter = std::make_unique<Interpreter>(error_reporter_);\n   if (subgraphs->size() > 1) {\n-    (*interpreter)->AddSubgraphs(subgraphs->size() - 1);\n+    tmp_interpreter->AddSubgraphs(subgraphs->size() - 1);\n   }\n \n   // Set num threads after all the subgraphs are added.\n-  (*interpreter)->SetNumThreads(num_threads_);\n+  tmp_interpreter->SetNumThreads(num_threads_);\n \n   // Set Interpreter options\n-  (*interpreter)->ApplyOptionsImpl(&options_);\n+  tmp_interpreter->ApplyOptionsImpl(&options_);\n \n-  (*interpreter)\n-      ->SetProfilerImpl(tflite::profiling::MaybeCreatePlatformProfiler());\n+  tmp_interpreter->SetProfilerImpl(\n+      tflite::profiling::MaybeCreatePlatformProfiler());\n \n   bool telemetry_registered = telemetry_profiler_ != nullptr;\n   std::unique_ptr<TfLiteTelemetryInterpreterSettings> telemetry_settings;\n   if (telemetry_registered) {\n-    (*interpreter)->AddProfiler(std::move(telemetry_profiler_));\n+    tmp_interpreter->AddProfiler(std::move(telemetry_profiler_));\n     telemetry_settings = std::make_unique<TfLiteTelemetryInterpreterSettings>();\n     telemetry_settings->subgraph_infos.resize(subgraphs->size());\n   }\n@@ -861,7 +857,7 @@ TfLiteStatus InterpreterBuilder::operator()(\n        ++subgraph_index) {\n     const tflite::SubGraph* subgraph = (*subgraphs)[subgraph_index];\n     tflite::Subgraph* modified_subgraph =\n-        (*interpreter)->subgraph(subgraph_index);\n+        tmp_interpreter->subgraph(subgraph_index);\n     modified_subgraph->allocation_ = allocation_;\n     auto* subgraph_info =\n         telemetry_registered\n@@ -873,10 +869,10 @@ TfLiteStatus InterpreterBuilder::operator()(\n       TF_LITE_REPORT_ERROR(error_reporter_,\n                            \"Did not get tensors in subgraph %d.\\n\",\n                            subgraph_index);\n-      return cleanup_and_error();\n+      return kTfLiteError;\n     }\n     if (modified_subgraph->AddTensors(tensors->size()) != kTfLiteOk) {\n-      return cleanup_and_error();\n+      return kTfLiteError;\n     }\n     // Parse inputs/outputs\n     modified_subgraph->SetInputs(\n@@ -889,9 +885,9 @@ TfLiteStatus InterpreterBuilder::operator()(\n     // nodes.\n     if (ParseTensors(buffers, tensors, modified_subgraph, subgraph_info) !=\n         kTfLiteOk)\n-      return cleanup_and_error();\n+      return kTfLiteError;\n     if (operators && ParseNodes(operators, modified_subgraph) != kTfLiteOk)\n-      return cleanup_and_error();\n+      return kTfLiteError;\n \n     std::vector<int> variables;\n     for (int i = 0; i < modified_subgraph->tensors_size(); ++i) {\n@@ -906,14 +902,14 @@ TfLiteStatus InterpreterBuilder::operator()(\n     }\n   }\n \n-  if (ParseSignatureDefs(model_->signature_defs(), interpreter->get()) !=\n+  if (ParseSignatureDefs(model_->signature_defs(), tmp_interpreter.get()) !=\n       kTfLiteOk) {\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (options_.GetUseSignatureTensorNames()) {\n-    for (auto& signature_def : (*interpreter)->signature_defs_) {\n-      auto* subgraph = (*interpreter)->subgraph(signature_def.subgraph_index);\n+    for (auto& signature_def : tmp_interpreter->signature_defs_) {\n+      auto* subgraph = tmp_interpreter->subgraph(signature_def.subgraph_index);\n       for (auto& [name, tensor_index] : signature_def.inputs) {\n         auto tensor = subgraph->tensor(tensor_index);\n         tensor->name = name.c_str();\n@@ -925,33 +921,35 @@ TfLiteStatus InterpreterBuilder::operator()(\n     }\n   }\n \n-  if ((*interpreter)->SetMetadata(metadata_) != kTfLiteOk) {\n-    return cleanup_and_error();\n+  if (tmp_interpreter->SetMetadata(metadata_) != kTfLiteOk) {\n+    return kTfLiteError;\n   }\n \n   if (ShouldCreateLazyDelegateProviders(num_fp32_tensors_)) {\n-    (*interpreter)->lazy_delegate_providers_ =\n+    tmp_interpreter->lazy_delegate_providers_ =\n         op_resolver_.GetDelegateCreators();\n   }\n \n   if (telemetry_registered) {\n     ParseConversionMetadata(telemetry_settings.get());\n-    (*interpreter)->SetTelemetrySettings(std::move(telemetry_settings));\n+    tmp_interpreter->SetTelemetrySettings(std::move(telemetry_settings));\n     // Reports model and interpreter settings if telemetry is applied.\n-    (*interpreter)->ReportTelemetrySettings(kTelemetryBuilderEventName);\n+    tmp_interpreter->ReportTelemetrySettings(kTelemetryBuilderEventName);\n   }\n \n-  TfLiteStatus status = ApplyDelegates(interpreter->get());\n-  if (status != kTfLiteOk) {\n-    interpreter->reset();\n+  if (TfLiteStatus status = ApplyDelegates(tmp_interpreter.get());\n+      status != kTfLiteOk) {\n+    TF_LITE_REPORT_ERROR(error_reporter_, \"Failed to apply delegates.\\n\");\n+    return status;\n   }\n \n   // Apply Interpreter options again for dynamic allocation.\n   if (options_.GetDynamicAllocationForLargeTensors()) {\n-    (*interpreter)->ApplyOptionsImpl(&options_);\n+    tmp_interpreter->ApplyOptionsImpl(&options_);\n   }\n \n-  return status;\n+  *interpreter = std::move(tmp_interpreter);\n+  return kTfLiteOk;\n }\n \n void InterpreterBuilder::AddDelegate(TfLiteDelegate* delegate) {"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 37,
        "deletions": 39
    }
}