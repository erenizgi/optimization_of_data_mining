{
    "author": "jreiffers",
    "message": "PR #26187: Simplify HloComputation::IsFusionComputation semantics.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/26187\n\nAlso remove the instruction_and_type_ field.\n\nHloComputation::IsFusionComputation has complicated semantics: a computation that was ever set to be a fusion computation will remember this, even when the accompanying fusion instruction no longer exists. The documentation claims this is needed for scheduling purposes, but I wasn't able to find any evidence of this in the public parts of XLA. Maybe I missed something or there's something in the Google internal parts of XLA.\nCopybara import of the project:\n\n--\n2a474b7513442d7bb4b80048a065aa1c7ebba93e by Johannes Reifferscheid <jreiffers@nvidia.com>:\n\nSimplify HloComputation::IsFusionComputation semantics.\n\nAlso remove the instruction_and_type_ field.\n\nHloComputation::IsFusionComputation has complicated semantics:\na computation that was ever set to be a fusion computation will\nremember this, even when the accompanying fusion instruction no\nlonger exists. The documentation claims this is needed for\nscheduling purposes, but I wasn't able to find any evidence of this\nin the public parts of XLA. Maybe I missed something or there's\nsomething in the Google internal parts of XLA.\n\nMerging this change closes #26187\n\nPiperOrigin-RevId: 797662806",
    "sha": "db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a",
    "files": [
        {
            "sha": "13a2d82c5732cdf63eb26d3d6fb4c32eba2b1a9f",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc?ref=db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a",
            "patch": "@@ -218,28 +218,6 @@ void HloComputation::ClearCalledComputations() {\n   CHECK(callee_computations_.empty());\n }\n \n-void HloComputation::SetInstruction(HloInstruction* instruction,\n-                                    InstructionType type) {\n-  static_assert(alignof(HloInstruction) == kInstructionTypeMask + 1,\n-                \"HloInstruction should be aligned as a QWORD\");\n-\n-  DCHECK(type != InstructionType::kUnset)\n-      << \"Set instruction must be called with a valid type, not kUnset.\";\n-  DCHECK(instruction_type() == InstructionType::kUnset ||\n-         instruction_type() == type)\n-      << \"Unexpected instruction type. Current type is \"\n-      << static_cast<int>(instruction_type()) << \" and it cannot be reset to \"\n-      << static_cast<int>(type);\n-\n-  // If `instruction` is nullptr, we need to preserve the existing type.\n-  if (instruction == nullptr) {\n-    type = instruction_type();\n-  }\n-\n-  instruction_and_type_ =\n-      reinterpret_cast<uintptr_t>(instruction) | static_cast<uintptr_t>(type);\n-}\n-\n HloInstruction* HloComputation::AddInstruction(\n     std::unique_ptr<HloInstruction> instruction, absl::string_view new_name) {\n   CHECK(instruction->opcode() != HloOpcode::kParameter)\n@@ -1288,10 +1266,6 @@ HloComputation::CreateFromProto(\n   auto computation = absl::WrapUnique(\n       new HloComputation(proto.name(), parameter_count, &instructions, root));\n   computation->SetUniqueIdHelper(proto.id());\n-  if (proto.is_fusion_computation()) {\n-    computation->instruction_and_type_ =\n-        static_cast<uintptr_t>(InstructionType::kFusion);\n-  }\n   if (!proto.execution_thread().empty()) {\n     computation->SetExecutionThread(proto.execution_thread());\n   }"
        },
        {
            "sha": "423adffefcaa0e44a9062f47d1fcda1b3b019ae0",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "status": "modified",
            "additions": 16,
            "deletions": 40,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h?ref=db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a",
            "patch": "@@ -207,31 +207,6 @@ class HloComputation {\n \n   ~HloComputation();\n \n-  enum class InstructionType : uint8_t {\n-    kUnset,\n-    // This computation is a fusion computation. A fusion computation ordinarily\n-    // also has a non-null instruction. However, if a fusion instruction\n-    // is removed during compilation, the fusion computation becomes\n-    // unreachable, and its instruction is set to null. We still need to regard\n-    // such computations as fusion computations for HLO scheduling purposes.\n-    kFusion,\n-    // Last Value for range checking.\n-    kLast = kFusion,\n-  };\n-  static constexpr uintptr_t kInstructionTypeMask = 0b111;\n-  static_assert(static_cast<int>(InstructionType::kUnset) == 0,\n-                \"kUnset must be 0.\");\n-\n-  InstructionType instruction_type() const {\n-    return static_cast<InstructionType>(instruction_and_type_ &\n-                                        kInstructionTypeMask);\n-  }\n-\n-  HloInstruction* instruction() const {\n-    DCHECK(instruction_type() <= InstructionType::kLast);\n-    return reinterpret_cast<HloInstruction*>(instruction_and_type_ &\n-                                             ~kInstructionTypeMask);\n-  }\n   // Add an instruction to the computation. The computation takes ownership of\n   // the instruction.\n   HloInstruction* AddInstruction(std::unique_ptr<HloInstruction> instruction,\n@@ -788,23 +763,30 @@ class HloComputation {\n   bool HasSideEffect() const;\n \n   // Returns if this computation is a fusion computation.\n-  // Do not use this method to determine if fusion_instruction_ != nullptr.\n-  // Instead, directly do: FusionInstruction() != nullptr\n   bool IsFusionComputation() const {\n-    return instruction_type() == InstructionType::kFusion;\n+    // TODO(b/418034360): There should be at most one fusion instruction calling\n+    // a fusion computation. Assert this and fix all related tests.\n+    return !caller_instructions(HloOpcode::kFusion).empty();\n   }\n \n   // Returns if this computation is the entry computation of the module.\n   bool IsEntryComputation() const;\n \n+  // Returns if this computation is dead. A computation is dead if it is not\n+  // the entry computation and it is not called by any other computation.\n+  bool IsDeadComputation() const {\n+    return !IsEntryComputation() && caller_computations().empty();\n+  }\n+\n   // Returns the owning fusion instruction, or nullptr if this is not a fusion\n-  // computation.\n+  // computation. Note that this is just one of the fusion instructions that\n+  // calls this computation, there may be more than one callers.\n+  //\n+  // TODO(b/418034360): There should be at most one fusion instruction calling\n+  // a fusion computation. Assert this and fix all related tests.\n   HloInstruction* FusionInstruction() const {\n-    return instruction_type() == InstructionType::kFusion ? instruction()\n-                                                          : nullptr;\n-  }\n-  void SetFusionInstruction(HloInstruction* fusion_instruction) {\n-    SetInstruction(fusion_instruction, InstructionType::kFusion);\n+    auto callers = caller_instructions(HloOpcode::kFusion);\n+    return callers.empty() ? nullptr : callers.front();\n   }\n \n   // Returns if this computation is an async computation.\n@@ -988,8 +970,6 @@ class HloComputation {\n   absl::Status RemoveInstructionImpl(HloInstruction* instruction,\n                                      bool ignore_safety_check);\n \n-  void SetInstruction(HloInstruction* instruction, InstructionType type);\n-\n   // Private, because only HloModule should be able to set the parent.\n   // We maintain the invariant that a computation has a parent() if and only if\n   // the computation has been added to a module. Accordingly, the only way to\n@@ -1020,10 +1000,6 @@ class HloComputation {\n   // Module containing this computation.\n   HloModule* parent_ = nullptr;\n \n-  // Contains HloInstruction* and its type.\n-  // The respective type in the least significant three bits.\n-  uintptr_t instruction_and_type_ = 0;\n-\n   // Contains an HloInstruction* or an absl::flat_hash_map<HloInstruction*,\n   // /*count=*/int> in the high bits and a CallersType in the least significant\n   // bit."
        },
        {
            "sha": "2c8b65851bdce40153c24b20a86052b41c8b7748",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc?ref=db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a",
            "patch": "@@ -1983,10 +1983,6 @@ HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     auto* new_computation = CHECK_NOTNULL(instruction_to_append->GetModule())\n                                 ->AddEmbeddedComputation(builder.Build());\n     AppendComputation(new_computation);\n-    if (opcode() == HloOpcode::kFusion) {\n-      new_computation->SetFusionInstruction(this);\n-    }\n-\n     clone = called_computation_root();\n   } else {\n     // When add_output is false, instruction_to_append is necessarily an\n@@ -2211,31 +2207,6 @@ HloFusionInstruction::HloFusionInstruction(\n     : HloCallableInstruction(HloOpcode::kFusion, shape, operands,\n                              fusion_computation, prefix),\n       fusion_kind_(fusion_kind) {\n-  fusion_computation->SetFusionInstruction(this);\n-}\n-\n-HloFusionInstruction::~HloFusionInstruction() {\n-  ClearFusionComputationInstruction();\n-}\n-\n-void HloFusionInstruction::ClearFusionComputationInstruction() {\n-  // Each fusion calls a single computation, but we use called_computations()\n-  // instead of fused_instructions_computation(), because the order in which\n-  // things get destructed can vary; the fusion computation's back-pointer may\n-  // already be null, which violates a check in\n-  // fused_instructions_computation.\n-  for (HloComputation* computation : called_computations()) {\n-    // Some passes that rewrite fusions may reassign a fusion computation to a\n-    // different fusion instruction as this instruction gets destructed.\n-    if (computation->FusionInstruction() == this) {\n-      computation->SetFusionInstruction(nullptr);\n-    }\n-  }\n-}\n-\n-void HloFusionInstruction::ClearCalledComputations() {\n-  ClearFusionComputationInstruction();\n-  HloInstruction::ClearCalledComputations();\n }\n \n HloInstruction*\n@@ -2491,11 +2462,7 @@ void HloFusionInstruction::MergeFusionInstructionIntoMultiOutput(\n \n HloComputation* HloFusionInstruction::fused_instructions_computation() const {\n   CHECK_EQ(called_computations().size(), 1);\n-  auto* fused_instructions_computation = called_computations().front();\n-  CHECK(fused_instructions_computation->IsFusionComputation())\n-      << \"Computation \" << fused_instructions_computation->name()\n-      << \" is not a fusion kind\";\n-  return fused_instructions_computation;\n+  return called_computations().front();\n }\n \n HloInstruction* HloFusionInstruction::fused_expression_root() const {"
        },
        {
            "sha": "9315b34a8ee8e9870ce12e58898027f69447ba3d",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.h",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h?ref=db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a",
            "patch": "@@ -1497,14 +1497,6 @@ class HloFusionInstruction : public HloCallableInstruction {\n                                 HloComputation* fusion_computation,\n                                 absl::string_view prefix = \"\");\n \n-  ~HloFusionInstruction() override;\n-\n-  void ClearCalledComputations() override;\n-\n-  // When a fusion instruction is being destructed, clear the back pointer of\n-  // its fusion computation, to avoid referencing freed memory.\n-  void ClearFusionComputationInstruction();\n-\n   // Clones the given instruction_to_append and inserts the clone into this\n   // callable instruction.\n   HloInstruction* CloneAndAppendInstructionIntoCalledComputation("
        },
        {
            "sha": "e852d6004f20822a65112351b4af52bed40b051a",
            "filename": "third_party/xla/xla/hlo/ir/hlo_schedule.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc?ref=db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/hlo/ir/hlo_schedule.h\"\n \n+#include <algorithm>\n #include <cstdint>\n #include <ostream>\n #include <queue>\n@@ -321,7 +322,25 @@ absl::Status HloSchedule::Verify() const {\n        sequence_num_by_execution_threads) {\n     std::vector<HloComputation*> nonfusion_computations =\n         module_->MakeNonfusionComputations({thread_name});\n-    TF_RET_CHECK(nonfusion_computations.size() == sequence_size)\n+\n+    // TODO(dasenov): Replace with std::erase_if after XLA uses C++20.\n+    auto remove_it = std::remove_if(nonfusion_computations.begin(),\n+                                    nonfusion_computations.end(),\n+                                    [](const HloComputation* computation) {\n+                                      return computation->IsDeadComputation();\n+                                    });\n+    nonfusion_computations.erase(remove_it, nonfusion_computations.end());\n+\n+    // It's possible to have more sequences than non_fusion_computations.\n+    // This is because in some cases computations that have schedules are\n+    // actually dead. The important thing to check is that each live non-fusion\n+    // computation has a sequence.\n+    //\n+    // TODO(b/418034360): Consider strenghtening this check to equality. That\n+    // would require cleaning up dead computations and/or recomputing the\n+    // schedule in a number of tests. In its present state (using less or equal)\n+    // this check is subsumed by the next one.\n+    TF_RET_CHECK(nonfusion_computations.size() <= sequence_size)\n         << \"For thread \" << thread_name << \", schedule has \" << sequence_size\n         << \" sequences, but module has \" << nonfusion_computations.size()\n         << \" non-fusion computations for thread \" << thread_name;"
        },
        {
            "sha": "7bd5f94133d165773b6ccdd2e07075dfe7363f19",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/flatten_call_graph.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 43,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fflatten_call_graph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fflatten_call_graph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fflatten_call_graph.cc?ref=db75f3bda8bb8a6bf26b0286a5e9fcbe394d938a",
            "patch": "@@ -88,58 +88,21 @@ absl::StatusOr<bool> FlattenNode(const CallGraphNode& node) {\n   return changed;\n }\n \n-// Annotates flatten computations with callee instruction types.\n-absl::Status AnnotateNode(const CallGraphNode& node) {\n-  for (auto& callsite : node.callsites()) {\n-    HloInstruction* instruction = callsite.instruction();\n-\n-    if (instruction->opcode() == HloOpcode::kFusion) {\n-      for (HloComputation* computation : instruction->called_computations()) {\n-        computation->SetFusionInstruction(instruction);\n-      }\n-    }\n-  }\n-\n-  // Correctly handle dead code: if a fusion computation is no longer used, it\n-  // should not have a fusion instruction set.\n-  if (node.callers().empty() &&\n-      node.computation()->FusionInstruction() != nullptr) {\n-    node.computation()->SetFusionInstruction(nullptr);\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n }  // namespace\n \n absl::StatusOr<bool> FlattenCallGraph::Run(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   XLA_VLOG_LINES(3, \"Before flatten call graph:\\n\" + module->ToString());\n \n-  bool changed = false;\n-  {  // Flatten original call graph.\n-    std::unique_ptr<CallGraph> call_graph =\n-        CallGraph::Build(module, execution_threads);\n-    TF_ASSIGN_OR_RETURN(bool flattened,\n-                        call_graph->VisitNodesWithReturn(FlattenNode));\n-    changed |= flattened;\n-  }\n-\n-  if (!changed) {\n-    return false;\n-  }\n-\n-  // TODO(b/418034360): Remove this step once the fusion instruction is\n-  // automatically maintained.\n-  {  // Annotate flattened computations with callee types.\n-    std::unique_ptr<CallGraph> call_graph =\n-        CallGraph::Build(module, execution_threads);\n-    TF_RETURN_IF_ERROR(call_graph->VisitNodes(AnnotateNode));\n-  }\n+  // Flatten original call graph.\n+  std::unique_ptr<CallGraph> call_graph =\n+      CallGraph::Build(module, execution_threads);\n+  TF_ASSIGN_OR_RETURN(bool changed,\n+                      call_graph->VisitNodesWithReturn(FlattenNode));\n \n   XLA_VLOG_LINES(3, \"After flatten call graph:\\n\" + module->ToString());\n-  return true;\n+  return changed;\n }\n \n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 195,
        "additions": 43,
        "deletions": 152
    }
}