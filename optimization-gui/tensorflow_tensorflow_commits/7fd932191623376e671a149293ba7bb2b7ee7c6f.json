{
    "author": "JoelWee",
    "message": "Introduce Shardy:MPMD to IFRT conversion passes\n\nPiperOrigin-RevId: 811785043",
    "sha": "7fd932191623376e671a149293ba7bb2b7ee7c6f",
    "files": [
        {
            "sha": "9b7a52a69b1282d7ed04dd875a3b78181067502a",
            "filename": "third_party/xla/xla/python/ifrt/ir/conversions/mpmd/BUILD",
            "status": "added",
            "additions": 125,
            "deletions": 0,
            "changes": 125,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2FBUILD?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,125 @@\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n+load(\"//xla/tsl:tsl.bzl\", \"internal_visibility\")\n+load(\"//xla/tsl:tsl.default.bzl\", \"tsl_pybind_extension\")\n+\n+package_group(\n+    name = \"internal\",\n+    packages = [\n+        \"//xla/python/ifrt/ir/conversions/mpmd/...\",\n+    ],\n+)\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = internal_visibility([\n+        \":internal\",\n+    ]),\n+    licenses = [\"notice\"],\n+)\n+\n+cc_library(\n+    name = \"utils\",\n+    srcs = [\"utils.cc\"],\n+    hdrs = [\"utils.h\"],\n+    deps = [\n+        \"//xla/hlo/ir:hlo_sharding\",\n+        \"//xla/python/ifrt/ir\",\n+        \"//xla/python/ifrt/ir:sharding_param\",\n+        \"//xla/python/ifrt/support:sharding_conversions\",\n+        \"//xla/service/spmd/shardy:constants\",\n+        \"//xla/service/spmd/shardy/stablehlo_round_trip:export_shardings\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@llvm-project//mlir:FuncDialect\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@shardy//shardy/dialect/mpmd/ir:dialect\",\n+        \"@shardy//shardy/dialect/sdy/ir:dialect\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"lower_to_ifrt\",\n+    srcs = [\"lower_to_ifrt.cc\"],\n+    hdrs = [\"lower_to_ifrt.h\"],\n+    visibility = [\"//xla/python/ifrt:users\"],\n+    deps = [\n+        \":utils\",\n+        \"//xla/client:executable_build_options\",\n+        \"//xla/pjrt:pjrt_executable\",\n+        \"//xla/python/ifrt/ir\",\n+        \"//xla/python/ifrt/ir/transforms:built_in_spmd_expansions\",\n+        \"//xla/python/ifrt/ir/transforms:debug\",\n+        \"//xla/python/ifrt/ir/transforms:passes\",\n+        \"//xla/service:compilation_environments\",\n+        \"//xla/service:computation_placer_hdr\",\n+        \"//xla/service/spmd/shardy:constants\",\n+        \"//xla/service/spmd/shardy:utils\",\n+        \"//xla/service/spmd/shardy/sdy_round_trip:pipelines\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:FuncDialect\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Pass\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@llvm-project//mlir:TransformUtils\",\n+        \"@shardy//shardy/dialect/mpmd/ir:dialect\",\n+        \"@shardy//shardy/dialect/mpmd/transforms/export:utils\",\n+        \"@shardy//shardy/dialect/sdy/ir:dialect\",\n+        \"@stablehlo//:stablehlo_ops\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"utils_test\",\n+    srcs = [\"utils_test.cc\"],\n+    # Not sure why it doesn't work on arm64 CPU, but these tests are just validating business logic\n+    # and so we just need to make sure they pass on other systems.\n+    tags = [\"not_run:arm\"],\n+    deps = [\n+        \":utils\",\n+        \"//xla/hlo/ir:hlo_sharding\",\n+        \"//xla/python/ifrt/support:sharding_conversions\",\n+        \"//xla/service/spmd/shardy/stablehlo_round_trip:stablehlo_import\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Parser\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@shardy//shardy/dialect/mpmd/ir:dialect\",\n+        \"@shardy//shardy/dialect/sdy/ir:dialect\",\n+    ],\n+)\n+\n+tsl_pybind_extension(\n+    name = \"ifrt_mpmd_py\",\n+    srcs = [\"ifrt_mpmd_py.cc\"],\n+    copts = [\n+        \"-fexceptions\",\n+        \"-fno-strict-aliasing\",\n+    ],\n+    features = [\"-use_header_modules\"],\n+    visibility = [\"//xla/python/ifrt:users\"],\n+    deps = [\n+        \":lower_to_ifrt\",\n+        \"//xla/pjrt:status_casters\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:CAPIIRHeaders\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:MLIRBindingsPythonNanobindHeaders\",\n+        \"@nanobind\",\n+        \"@pybind11\",\n+        \"@pybind11_abseil//pybind11_abseil:absl_casters\",\n+    ],\n+)"
        },
        {
            "sha": "72ca3460606ada30547d8b316be56c4d4369300e",
            "filename": "third_party/xla/xla/python/ifrt/ir/conversions/mpmd/ifrt_mpmd_py.cc",
            "status": "added",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Fifrt_mpmd_py.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Fifrt_mpmd_py.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Fifrt_mpmd_py.cc?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,64 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir-c/IR.h\"\n+#include \"mlir/Bindings/Python/PybindAdaptors.h\"  // IWYU pragma: keep; Needed to allow MlirModule -> ModuleOp.\n+#include \"mlir/CAPI/IR.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/OperationSupport.h\"\n+#include \"nanobind/nanobind.h\"\n+#include \"pybind11/detail/common.h\"\n+#include \"pybind11/pybind11.h\"\n+#include \"pybind11/pytypes.h\"\n+#include \"pybind11_abseil/absl_casters.h\"\n+#include \"xla/pjrt/status_casters.h\"  // IWYU pragma: keep; Needed for ValueOrThrow\n+#include \"xla/python/ifrt/ir/conversions/mpmd/lower_to_ifrt.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::ifrt::mpmd {\n+\n+PYBIND11_MODULE(ifrt_mpmd_py, m) {\n+  m.def(\n+      \"lower_to_ifrt\",\n+      [](MlirModule module) -> void {\n+        return xla::ThrowIfError(LowerToIfrt(unwrap(module)));\n+      },\n+      py::arg(\"module\"));\n+\n+  m.def(\"get_compile_options\",\n+        [](MlirModule c_module,\n+           const absl::flat_hash_map<std::string, const EnvOptionsOverride>&\n+               compile_options_overrides) -> absl::StatusOr<py::dict> {\n+          auto module = unwrap(c_module);\n+          TF_ASSIGN_OR_RETURN(\n+              auto compile_options_map,\n+              GetCompileOptions(module, compile_options_overrides));\n+          py::dict out;\n+          for (const auto& [name, options] : compile_options_map) {\n+            out[py::cast(name)] = py::reinterpret_steal<py::object>(\n+                nanobind::cast(options).release().ptr());\n+          }\n+          return out;\n+        });\n+}\n+\n+}  // namespace xla::ifrt::mpmd"
        },
        {
            "sha": "e098d232f18761e6fd8fc0ff7b1a4f3e016e5bea",
            "filename": "third_party/xla/xla/python/ifrt/ir/conversions/mpmd/lower_to_ifrt.cc",
            "status": "added",
            "additions": 660,
            "deletions": 0,
            "changes": 660,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Flower_to_ifrt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Flower_to_ifrt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Flower_to_ifrt.cc?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,660 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/ifrt/ir/conversions/mpmd/lower_to_ifrt.h\"\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/STLFunctionalExtras.h\"\n+#include \"llvm/ADT/StringMap.h\"\n+#include \"llvm/Support/LogicalResult.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/SymbolTable.h\"\n+#include \"mlir/IR/TypeRange.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Interfaces/CallInterfaces.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Pass/PassRegistry.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/TypeID.h\"\n+#include \"mlir/Support/WalkResult.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"shardy/dialect/mpmd/ir/dialect.h\"\n+#include \"shardy/dialect/mpmd/ir/utils.h\"\n+#include \"shardy/dialect/mpmd/transforms/export/utils.h\"\n+#include \"shardy/dialect/sdy/ir/dialect.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n+#include \"xla/client/executable_build_options.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n+#include \"xla/python/ifrt/ir/constants.h\"\n+#include \"xla/python/ifrt/ir/conversions/mpmd/utils.h\"\n+#include \"xla/python/ifrt/ir/ifrt_dialect.h\"\n+#include \"xla/python/ifrt/ir/ifrt_ops.h\"\n+#include \"xla/python/ifrt/ir/transforms/built_in_spmd_expansions.h\"\n+#include \"xla/python/ifrt/ir/transforms/debug.h\"\n+#include \"xla/python/ifrt/ir/transforms/passes.h\"\n+#include \"xla/service/compilation_environments.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/service/spmd/shardy/constants.h\"\n+#include \"xla/service/spmd/shardy/sdy_round_trip/pipelines.h\"\n+#include \"xla/service/spmd/shardy/utils.h\"\n+\n+namespace xla::ifrt::mpmd {\n+namespace {\n+\n+using llvm::DenseMap;\n+using mlir::ArrayRef;\n+using mlir::Attribute;\n+using mlir::BlockArgument;\n+using mlir::CallOpInterface;\n+using mlir::ConversionPatternRewriter;\n+using mlir::DenseSet;\n+using mlir::failed;\n+using mlir::failure;\n+using mlir::FunctionType;\n+using mlir::IntegerAttr;\n+using mlir::LogicalResult;\n+using mlir::MLIRContext;\n+using mlir::ModuleOp;\n+using mlir::OpConversionPattern;\n+using mlir::Pass;\n+using mlir::PassWrapper;\n+using mlir::SmallVector;\n+using mlir::StringAttr;\n+using mlir::StringRef;\n+using mlir::success;\n+using mlir::SymbolTableCollection;\n+using mlir::Type;\n+using mlir::TypeConverter;\n+using mlir::TypedValue;\n+using mlir::ValueRange;\n+using mlir::WalkResult;\n+using mlir::func::FuncOp;\n+using mlir::func::ReturnOp;\n+using mlir::mpmd::FragmentCallOp;\n+using mlir::mpmd::GetMainFunction;\n+using mlir::mpmd::IsMainFunction;\n+using mlir::mpmd::kAliasingAttrName;\n+using mlir::mpmd::kBufferDonationAttrName;\n+using mlir::mpmd::kReservedHbmBytes;\n+using mlir::mpmd::MeshTensorType;\n+using mlir::mpmd::MpmdDialect;\n+using mlir::mpmd::RemoveMesh;\n+using mlir::mpmd::TransferOp;\n+using xla::ifrt::IfrtControlType;\n+using xla::ifrt::IfrtDevicesAttr;\n+using xla::ifrt::InitPassManager;\n+using xla::ifrt::StatusScopedDiagnosticHandler;\n+\n+namespace mpmd = ::mlir::mpmd;\n+namespace sdy = mlir::sdy;\n+\n+bool IsIfrtArray(Type t) { return mlir::isa<xla::ifrt::IfrtArrayType>(t); }\n+\n+// Checks if a FuncOp is legal.\n+bool IsFuncOpLegal(FuncOp op) {\n+  if (IsMainFunction(op)) {\n+    // All the inputs and results of the main func should be IFRT arrays.\n+    return absl::c_all_of(op.getFunctionType().getInputs(), IsIfrtArray) &&\n+           absl::c_all_of(op.getFunctionType().getResults(), IsIfrtArray);\n+  }\n+  // Any non-main function should not have mesh_shape attribute.\n+  return !op->hasAttr(mpmd::kMeshShapeAttr);\n+}\n+\n+// Converts a PartIR:MPMD MeshTensor to an IFRT Array.\n+//\n+// The conversion constructs an IFRT Array with the global shape of the\n+// PartIR:MPMD MeshTensor, an IFRT Devices Attribute (i.e., list of IFRT devices\n+// corresponding to the MeshTensor's mesh), and an IFRT ShardingParam.\n+Type MeshTensorToArray(\n+    const llvm::StringMap<IfrtDevicesAttr>& mesh_name_to_devices_attr,\n+    MeshTensorType mesh_tensor_type, sdy::MeshAttr mesh_attr) {\n+  MLIRContext& ctx = *mesh_tensor_type.getContext();\n+  auto sharding_param =\n+      MeshTensorTypeToShardingParam(mesh_tensor_type, mesh_attr);\n+  CHECK_OK(sharding_param.status());\n+  return xla::ifrt::IfrtArrayType::get(\n+      &ctx, mesh_tensor_type.getGlobalTensorType(),\n+      xla::ifrt::IfrtShardingParamAttr::get(&ctx, sharding_param.value()),\n+      mesh_name_to_devices_attr.at(mesh_tensor_type.getMeshName()),\n+      mesh_tensor_type.getMemoryKind(), /*layout_attr=*/nullptr);\n+}\n+\n+// Converts an MPMD TransferOp into an IFRT ReshardOp.\n+class TransferOpPattern final : public OpConversionPattern<TransferOp> {\n+ public:\n+  TransferOpPattern(const TypeConverter& type_converter, MLIRContext* context)\n+      : OpConversionPattern(type_converter, context) {}\n+\n+  LogicalResult matchAndRewrite(\n+      TransferOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter& rewriter) const final {\n+    const TypeConverter& type_converter = *getTypeConverter();\n+    Type converted_result_type = type_converter.convertType(op.getType());\n+    auto ifrt_reshard_op = xla::ifrt::ReshardOp::create(\n+        rewriter, op.getLoc(),\n+        /*outputs=*/converted_result_type,\n+        /*control_output=*/\n+        IfrtControlType::get(rewriter.getContext()),\n+        /*inputs=*/adaptor.getTensor(),\n+        /*donated=*/false,\n+        /*control_inputs=*/ValueRange());\n+    rewriter.replaceOp(op, ifrt_reshard_op.getOutputs());\n+    return success();\n+  }\n+};\n+\n+// Converts an MPMD FragmentCallOp into an IFRT CallOp.\n+class FragmentCallOpPattern final : public OpConversionPattern<FragmentCallOp> {\n+ public:\n+  FragmentCallOpPattern(\n+      const TypeConverter& type_converter, MLIRContext* context,\n+      const llvm::StringMap<IfrtDevicesAttr>& mesh_name_to_devices_attr)\n+      : OpConversionPattern(type_converter, context),\n+        mesh_name_to_devices_attr_(mesh_name_to_devices_attr) {}\n+\n+  LogicalResult matchAndRewrite(\n+      FragmentCallOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter& rewriter) const final {\n+    const TypeConverter& type_converter = *getTypeConverter();\n+    // Convert the result types of the op.\n+    SmallVector<Type> converted_result_types;\n+    if (failed(type_converter.convertTypes(op->getResultTypes(),\n+                                           converted_result_types))) {\n+      return failure();\n+    }\n+\n+    // Get the aliased inputs from the callee in order to construct the\n+    // io_aliases required by the ifrt.CallOp.\n+    std::vector<Attribute> io_aliases;\n+    std::vector<int> donated_input_indices;\n+    FuncOp callee =\n+        mlir::cast<FuncOp>(mlir::cast<CallOpInterface>(*op).resolveCallable());\n+    for (BlockArgument arg : callee.getArguments()) {\n+      if (auto donation_attr = callee.getArgAttrOfType<mlir::BoolAttr>(\n+              arg.getArgNumber(), kBufferDonationAttrName)) {\n+        donated_input_indices.push_back(arg.getArgNumber());\n+      }\n+      if (auto aliasing_attr = callee.getArgAttrOfType<mlir::IntegerAttr>(\n+              arg.getArgNumber(), kAliasingAttrName)) {\n+        io_aliases.push_back(rewriter.getDenseI32ArrayAttr(\n+            {static_cast<int32_t>(arg.getArgNumber()),\n+             static_cast<int32_t>(aliasing_attr.getInt())}));\n+      }\n+    }\n+\n+    StringRef mesh_name = op.getMeshName();\n+    auto ifrt_call_op = xla::ifrt::CallOp::create(\n+        rewriter, op.getLoc(),\n+        /*outputs=*/converted_result_types,\n+        /*control_output=*/IfrtControlType::get(rewriter.getContext()),\n+        /*inputs=*/adaptor.getOperands(),\n+        /*control_inputs=*/ValueRange{},\n+        /*arg_attrs=*/nullptr,\n+        /*res_attrs=*/nullptr,\n+        /*callee=*/op.getCalleeAttr(),\n+        /*devices=*/mesh_name_to_devices_attr_.at(mesh_name),\n+        /*io_aliases=*/rewriter.getArrayAttr(io_aliases),\n+        /*donated_input_indices=*/\n+        rewriter.getDenseI32ArrayAttr(donated_input_indices));\n+    // Set the mesh name in an attribute. The mesh name is used to get optional\n+    // per-mesh compile options users might have provided.\n+    ifrt_call_op->setAttr(kIfrtMeshNameAttrName,\n+                          rewriter.getStringAttr(mesh_name));\n+    if (op->hasAttr(mpmd::kIsSdyPartitioned)) {\n+      ifrt_call_op->setAttr(xla::ifrt::kIsSdyPartitioned,\n+                            rewriter.getUnitAttr());\n+    }\n+    rewriter.replaceOp(op, ifrt_call_op.getOutputs());\n+    return success();\n+  }\n+\n+ private:\n+  const llvm::StringMap<IfrtDevicesAttr>& mesh_name_to_devices_attr_;\n+};\n+\n+// Pattern for converting the types of ReturnOps. It is used to ensure that the\n+// return of the main func is of type IFRT Array.\n+class ReturnOpPattern : public OpConversionPattern<ReturnOp> {\n+  using OpConversionPattern::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      ReturnOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter& rewriter) const final {\n+    rewriter.modifyOpInPlace(op,\n+                             [&] { op->setOperands(adaptor.getOperands()); });\n+    return success();\n+  }\n+};\n+\n+// Updates the main function signature from MeshTensor to IFRT Array, and\n+// removes mesh shapes attributes from non-main functions.\n+class FuncOpPattern final : public OpConversionPattern<FuncOp> {\n+ public:\n+  FuncOpPattern(const TypeConverter& type_converter, MLIRContext* context)\n+      : OpConversionPattern(type_converter, context) {}\n+\n+  LogicalResult matchAndRewrite(\n+      FuncOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter& rewriter) const final {\n+    if (!IsMainFunction(op)) {\n+      // We only need to remove the mesh attributed for the non-main functions.\n+      rewriter.modifyOpInPlace(op, [&] { RemoveMesh(op); });\n+      return success();\n+    }\n+    FunctionType func_type = op.getFunctionType();\n+    // Convert the function signature.\n+    TypeConverter::SignatureConversion converted_args(func_type.getNumInputs());\n+    const TypeConverter& type_converter = *getTypeConverter();\n+    if (failed(type_converter.convertSignatureArgs(func_type.getInputs(),\n+                                                   converted_args))) {\n+      return failure();\n+    }\n+    // Convert the function results.\n+    SmallVector<Type> converted_results;\n+    if (failed(type_converter.convertTypes(func_type.getResults(),\n+                                           converted_results))) {\n+      return failure();\n+    }\n+    // Replace the types of the region arguments as per the signature result.\n+    if (failed(rewriter.convertRegionTypes(&op.getBody(), type_converter,\n+                                           &converted_args))) {\n+      return failure();\n+    }\n+    // Update the function signature.\n+    rewriter.modifyOpInPlace(op, [&] {\n+      op.setType(FunctionType::get(rewriter.getContext(),\n+                                   converted_args.getConvertedTypes(),\n+                                   converted_results));\n+      op->removeAttr(mpmd::kTopologyAttr);\n+      op->setAttr(xla::ifrt::kIfrtFunctionAttrName, rewriter.getUnitAttr());\n+    });\n+    return success();\n+  }\n+};\n+\n+// Replaces aliasing and donation attributes with the IFRT donated arg\n+// attribute. The attributes are removed here because they will be removed\n+// implicitly during stable serialization (i.e., conversion to VIFRT), which\n+// requires the attributes to be from the IFRT dialect. Thus, we replace them\n+// with `ifrt.donated` unit attribute, which is supported by VIFRT.\n+void ReplaceAliasingArgAttrsWithIfrtDonatedArgAttrs(FuncOp func) {\n+  // Mark donated args with the IFRT donated unit attribute.\n+  for (int arg_num = 0; arg_num < func.getNumArguments(); arg_num++) {\n+    if (func.getArgAttrOfType<mlir::BoolAttr>(arg_num,\n+                                              kBufferDonationAttrName)) {\n+      func.removeArgAttr(arg_num, kBufferDonationAttrName);\n+      func.setArgAttr(arg_num, xla::ifrt::kIfrtDonatedArgAttrName,\n+                      mlir::UnitAttr::get(func.getContext()));\n+    } else if (func.getArgAttrOfType<mlir::IntegerAttr>(arg_num,\n+                                                        kAliasingAttrName)) {\n+      func.removeArgAttr(arg_num, kAliasingAttrName);\n+      func.setArgAttr(arg_num, xla::ifrt::kIfrtDonatedArgAttrName,\n+                      mlir::UnitAttr::get(func.getContext()));\n+    }\n+  }\n+}\n+\n+class LowerToIfrtPass\n+    : public PassWrapper<LowerToIfrtPass, mlir::OperationPass<ModuleOp>> {\n+ public:\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(LowerToIfrtPass)\n+\n+ private:\n+  void getDependentDialects(mlir::DialectRegistry& registry) const final {\n+    registry.insert<xla::ifrt::IfrtDialect>();\n+    xla::ifrt::AttachBuiltInSpmdExpansions(registry);\n+  }\n+\n+  void runOnOperation() final {\n+    ModuleOp module_op = getOperation();\n+    MLIRContext& ctx = getContext();\n+    FuncOp main_func = GetMainFunction(module_op);\n+\n+    ReplaceAliasingArgAttrsWithIfrtDonatedArgAttrs(main_func);\n+\n+    // Construct mapping from mesh name to IFRT Device Attributes.\n+    llvm::StringMap<IfrtDevicesAttr> mesh_name_to_devices_attr;\n+    ArrayRef<mpmd::NamedMeshAttr> meshes = mpmd::GetTopologyMeshes(main_func);\n+    int total_devices = 0;\n+    for (const mpmd::NamedMeshAttr& mesh : meshes) {\n+      int num_mesh_devices = 1;\n+      for (sdy::MeshAxisAttr axis : mesh.getMesh().getAxes()) {\n+        num_mesh_devices *= axis.getSize();\n+      }\n+      SmallVector<int> mesh_devices(num_mesh_devices);\n+      absl::c_iota(mesh_devices, total_devices);\n+      total_devices += num_mesh_devices;\n+      mesh_name_to_devices_attr[mesh.getName()] =\n+          IfrtDevicesAttr::get(&ctx, mesh_devices);\n+    }\n+\n+    mlir::ConversionTarget target(ctx);\n+    target.addIllegalDialect<MpmdDialect>();\n+    target.addLegalDialect<xla::ifrt::IfrtDialect,\n+                           mlir::stablehlo::StablehloDialect,\n+                           mlir::func::FuncDialect>();\n+    // The main func op should only have IFRT Array types, and the non-main\n+    // func ops should not have PartIR mesh_shape attribute.\n+    target.addDynamicallyLegalOp<FuncOp>(IsFuncOpLegal);\n+    target.addDynamicallyLegalOp<ReturnOp>([](ReturnOp op) {\n+      return absl::c_all_of(op.getOperandTypes(), [](Type t) {\n+        return !mlir::isa<MeshTensorType>(t);\n+      });\n+    });\n+\n+    // Set conversion from MeshTensorType to IFRT Array.\n+    TypeConverter type_converter;\n+    type_converter.addConversion([&meshes, &mesh_name_to_devices_attr](\n+                                     MeshTensorType mesh_tensor_type) -> Type {\n+      auto it = absl::c_find_if(\n+          meshes, [mesh_tensor_type](const mpmd::NamedMeshAttr& mesh) {\n+            return mesh.getName() == mesh_tensor_type.getMeshName();\n+          });\n+      CHECK(it != meshes.end())\n+          << \"Mesh `\" << mesh_tensor_type.getMeshName().str()\n+          << \"` not found in topology.\";\n+      sdy::MeshAttr mesh_attr = it->getMesh();\n+      return MeshTensorToArray(mesh_name_to_devices_attr, mesh_tensor_type,\n+                               mesh_attr);\n+    });\n+    // Do not convert if an Array is already an IFRT Array.\n+    type_converter.addConversion(\n+        [](xla::ifrt::IfrtArrayType ifrt_array_type) -> Type {\n+          return ifrt_array_type;\n+        });\n+\n+    mlir::RewritePatternSet patterns(&ctx);\n+    patterns.add<FuncOpPattern, TransferOpPattern>(type_converter, &ctx);\n+    patterns.add<ReturnOpPattern>(&ctx);\n+    patterns.add<FragmentCallOpPattern>(type_converter, &ctx,\n+                                        mesh_name_to_devices_attr);\n+    if (failed(mlir::applyPartialConversion(module_op, target,\n+                                            std::move(patterns)))) {\n+      signalPassFailure();\n+    }\n+\n+    // Convert the xla.sdy.meshes attribute to ifrt.sdy.meshes attribute so\n+    // that the attribute is preserved during IFRT versioning. This is safe\n+    // to do because the attribute if forward and backward compatible.\n+    if (auto front_end_attr = xla::sdy::getFrontendAttrs(module_op)) {\n+      if (auto meshes_round_trip_attr =\n+              front_end_attr.get(xla::sdy::kMeshesRoundTripAttr)) {\n+        module_op->setAttr(xla::ifrt::kIfrtSdyMeshesRoundTripAttr,\n+                           meshes_round_trip_attr);\n+      }\n+    }\n+\n+    // Clean up the sdy meshes.\n+    mlir::IRRewriter rewriter(&ctx);\n+    auto sdy_mesh_op_s = module_op.getOps<sdy::MeshOp>();\n+    for (auto it = sdy_mesh_op_s.begin(); it != sdy_mesh_op_s.end();) {\n+      rewriter.eraseOp(*it++);\n+    }\n+  }\n+\n+  StringRef getArgument() const override { return \"mpmd-lower-to-ifrt\"; }\n+\n+  StringRef getDescription() const override {\n+    return \"Lowers PartIR:MPMD to IFRT IR.\";\n+  }\n+};\n+\n+class AddCtrlDependenciesPass\n+    : public PassWrapper<AddCtrlDependenciesPass,\n+                         mlir::OperationPass<mlir::func::FuncOp>> {\n+ public:\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(AddCtrlDependenciesPass)\n+\n+ private:\n+  void runOnOperation() override {\n+    FuncOp func_op = getOperation();\n+    // Mapping between a hash of devices used by the CallOp and the control\n+    // output of the last IFRT CallOp encountered that uses the same devices.\n+    DenseMap<llvm::ArrayRef<int>, TypedValue<IfrtControlType>>\n+        call_op_to_control_output;\n+    func_op.walk([&](xla::ifrt::CallOp call_op) {\n+      llvm::ArrayRef<int> devices = call_op.getDevices();\n+      if (TypedValue<IfrtControlType> ctrl_input =\n+              call_op_to_control_output.lookup(devices)) {\n+        call_op.getControlInputsMutable().append(ctrl_input);\n+      }\n+      call_op_to_control_output[devices] = call_op.getControlOutput();\n+      return WalkResult::skip();\n+    });\n+  }\n+\n+  StringRef getArgument() const override {\n+    return \"mpmd-ifrt-add-ctrl-dependencies\";\n+  }\n+\n+  StringRef getDescription() const override {\n+    return \"Adds IFRT IR control dependencies to IFRT CallOps.\";\n+  }\n+};\n+\n+class BuildCompileOptionsPass\n+    : public PassWrapper<BuildCompileOptionsPass,\n+                         mlir::OperationPass<ModuleOp>> {\n+ public:\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(BuildCompileOptionsPass)\n+\n+  explicit BuildCompileOptionsPass(\n+      CompileOptionsMap& compile_options_map,\n+      const absl::flat_hash_map<std::string, const EnvOptionsOverride>&\n+          compile_options_overrides,\n+      int threshold_for_argument_tupling,\n+      llvm::function_ref<void(xla::ExecutableBuildOptions&, int64_t)>\n+          set_reserved_bytes)\n+      : compile_options_map_(compile_options_map),\n+        compile_options_overrides_(compile_options_overrides),\n+        threshold_for_argument_tupling_(threshold_for_argument_tupling),\n+        set_reserved_bytes_(set_reserved_bytes) {}\n+\n+ private:\n+  CompileOptionsMap& compile_options_map_;\n+  const absl::flat_hash_map<std::string, const EnvOptionsOverride>&\n+      compile_options_overrides_;\n+  int threshold_for_argument_tupling_;\n+  std::function<void(xla::ExecutableBuildOptions&, int64_t)>\n+      set_reserved_bytes_;\n+\n+  void runOnOperation() override {\n+    ModuleOp module = getOperation();\n+    SymbolTableCollection symbol_table;\n+    FuncOp func_op = GetMainFunction(module);\n+    int threshold_for_argument_tupling = threshold_for_argument_tupling_;\n+    auto walk_result = func_op.walk([&](xla::ifrt::CallOp call_op) {\n+      xla::CompileOptions compile_options;\n+      xla::ExecutableBuildOptions& exec_build_options =\n+          compile_options.executable_build_options;\n+      ArrayRef<int> logical_device_ids = call_op.getDevicesAttr().getIds();\n+      exec_build_options.set_num_replicas(1);\n+      exec_build_options.set_num_partitions(logical_device_ids.size());\n+      xla::DeviceAssignment device_assignment(1, logical_device_ids.size());\n+      // Build options use IFRT logical device ids.\n+      for (const auto [i, device_id] : llvm::enumerate(logical_device_ids)) {\n+        device_assignment(0, i) = device_id;\n+      }\n+      exec_build_options.set_device_assignment(device_assignment);\n+      exec_build_options.set_use_spmd_partitioning(true);\n+      if (call_op->hasAttr(xla::ifrt::kIsSdyPartitioned)) {\n+        exec_build_options.set_use_shardy_partitioner(true);\n+      }\n+      FuncOp callee = call_op.getCalleeOp(symbol_table);\n+      if (auto reserved_hbm_bytes =\n+              callee->getAttrOfType<IntegerAttr>(kReservedHbmBytes)) {\n+        set_reserved_bytes_(exec_build_options, reserved_hbm_bytes.getInt());\n+      };\n+      std::string callee_name =\n+          callee->getParentOfType<mlir::ModuleOp>().getSymName()->str();\n+      auto mesh_name_attr =\n+          call_op->getAttrOfType<StringAttr>(kIfrtMeshNameAttrName);\n+      CHECK(mesh_name_attr != nullptr)\n+          << \"ifrt.CallOp `\" << callee_name << \"` is missing \"\n+          << kIfrtMeshNameAttrName.str() << \" attribute.\";\n+      // While the users provide per-mesh compilation options, we need to\n+      // include callee name in the key because fragments assigned to the same\n+      // mesh might have different `xla_tpu_user_reserved_hbm_bytes`.\n+      const std::string compile_options_key =\n+          absl::StrCat(callee_name, \"_mesh_\", mesh_name_attr.str());\n+      call_op->setAttr(\n+          xla::ifrt::kIfrtCompileOptionsKey,\n+          StringAttr::get(call_op->getContext(), compile_options_key));\n+      // Apply the user-provided per-mesh compile option overrides.\n+      if (auto option_overrides =\n+              compile_options_overrides_.find(mesh_name_attr.str());\n+          option_overrides != compile_options_overrides_.end()) {\n+        compile_options.env_option_overrides = option_overrides->second;\n+      }\n+      if (threshold_for_argument_tupling > 0 &&\n+          callee.getNumArguments() > threshold_for_argument_tupling) {\n+        compile_options.parameter_is_tupled_arguments = true;\n+      }\n+      compile_options_map_.emplace(compile_options_key, compile_options);\n+      return WalkResult::skip();\n+    });\n+    if (walk_result.wasInterrupted()) {\n+      signalPassFailure();\n+    }\n+  }\n+\n+  StringRef getArgument() const override {\n+    return \"mpmd-ifrt-build-compile-options\";\n+  }\n+\n+  StringRef getDescription() const override {\n+    return \"Gets the compile options for each IFRT atom program.\";\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<Pass> CreateLowerToIfrtPass() {\n+  return std::make_unique<LowerToIfrtPass>();\n+}\n+\n+std::unique_ptr<Pass> CreateAddCtrlDependenciesPass() {\n+  return std::make_unique<AddCtrlDependenciesPass>();\n+}\n+\n+std::unique_ptr<Pass> CreateBuildCompileOptionsPass(\n+    CompileOptionsMap& compile_options_map,\n+    const absl::flat_hash_map<std::string, const EnvOptionsOverride>&\n+        compile_options_overrides,\n+    int threshold_for_argument_tupling,\n+    llvm::function_ref<void(xla::ExecutableBuildOptions&, int64_t)>\n+        set_reserved_bytes) {\n+  return std::make_unique<BuildCompileOptionsPass>(\n+      compile_options_map, compile_options_overrides,\n+      threshold_for_argument_tupling, set_reserved_bytes);\n+}\n+\n+void AddLowerToIfrtPasses(mlir::OpPassManager& pm,\n+                          bool add_control_dependencies) {\n+  pm.addPass(CreateLowerToIfrtPass());\n+  // IfrtMergeReshardsPass doesn't handle control dependencies, so we need to\n+  // run it before adding the control dependencies.\n+  pm.addNestedPass<mlir::func::FuncOp>(\n+      xla::ifrt::createIfrtMergeReshardsPass());\n+  if (add_control_dependencies) {\n+    pm.addNestedPass<FuncOp>(CreateAddCtrlDependenciesPass());\n+  }\n+  // Outline the IFRT atom programs to modules.\n+  xla::ifrt::IfrtToOutlinedAtomProgramsPipelineOptions outline_pipeline_options;\n+  outline_pipeline_options.propagate_shardings = false;\n+  xla::ifrt::createIfrtToOutlinedAtomProgramsPipeline(pm,\n+                                                      outline_pipeline_options);\n+}\n+\n+void RegisterLowerToIfrtPasses() {\n+  mlir::registerPass(CreateLowerToIfrtPass);\n+  mlir::registerPass(xla::ifrt::createIfrtMergeReshardsPass);\n+  mlir::registerPass(CreateAddCtrlDependenciesPass);\n+\n+  mlir::PassPipelineRegistration<> mpmd_lower_to_ifrt_pipeline(\n+      \"ifrt-mpmd-lower-to-ifrt-pipeline\", \"Run the passes for lowering to ifrt\",\n+      [](mlir::OpPassManager& pm) {\n+        AddLowerToIfrtPasses(pm, /*add_control_dependencies=*/true);\n+      });\n+}\n+\n+absl::Status LowerToIfrt(mlir::ModuleOp module, bool add_control_dependencies) {\n+  FuncOp main_func = GetMainFunction(module);\n+  if (!mpmd::IsMpmdFunction(main_func)) {\n+    return absl::InvalidArgumentError(\"MLIR module is not an MPMD module.\");\n+  }\n+  mlir::PassManager pm(module->getContext());\n+  InitPassManager(pm, \"mpmd-lower-to-ifrt\");\n+  pm.enableVerifier();\n+\n+  // If we are lowered with SDY, we need to run the SDY round trip pipeline.\n+  if (mlir::mpmd::IsLoweredWithSdy(module)) {\n+    xla::sdy::addSdyRoundTripExportPipeline(pm);\n+  }\n+  AddLowerToIfrtPasses(pm, add_control_dependencies);\n+  StatusScopedDiagnosticHandler diagnostic_handler(module.getContext());\n+  if (mlir::failed(pm.run(module))) {\n+    return diagnostic_handler.ConsumeStatus();\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<CompileOptionsMap> GetCompileOptions(\n+    mlir::ModuleOp module,\n+    const absl::flat_hash_map<std::string, const EnvOptionsOverride>&\n+        compile_options_overrides,\n+    int threshold_for_argument_tupling,\n+    llvm::function_ref<void(xla::ExecutableBuildOptions&, int64_t)>\n+        set_reserved_bytes) {\n+  FuncOp main_func = GetMainFunction(module);\n+  if (!IsIfrtFunction(main_func)) {\n+    return absl::InvalidArgumentError(\"MLIR module is not an IFRT module.\");\n+  }\n+  mlir::PassManager pm(module->getContext());\n+  InitPassManager(pm, \"mpmd-ifrt-build-compile-options\");\n+  CompileOptionsMap compile_options_map;\n+  pm.addPass(CreateBuildCompileOptionsPass(\n+      compile_options_map, compile_options_overrides,\n+      threshold_for_argument_tupling, set_reserved_bytes));\n+  StatusScopedDiagnosticHandler diagnostic_handler(module.getContext());\n+  if (mlir::failed(pm.run(module))) {\n+    return diagnostic_handler.ConsumeStatus();\n+  }\n+  return compile_options_map;\n+}\n+\n+}  // namespace xla::ifrt::mpmd"
        },
        {
            "sha": "cb3d2e1c40f436f49faa10e5ca5f1c241e46ff88",
            "filename": "third_party/xla/xla/python/ifrt/ir/conversions/mpmd/lower_to_ifrt.h",
            "status": "added",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Flower_to_ifrt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Flower_to_ifrt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Flower_to_ifrt.h?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,81 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PYTHON_IFRT_IR_CONVERSIONS_MPMD_LOWER_TO_IFRT_H_\n+#define XLA_PYTHON_IFRT_IR_CONVERSIONS_MPMD_LOWER_TO_IFRT_H_\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n+\n+namespace xla::ifrt::mpmd {\n+\n+// Compile environment options override is a vector of pairs of compile option\n+// flag and value strings.\n+using EnvOptionsOverride =\n+    std::vector<std::pair<std::string, xla::CompileOptions::OptionOverride>>;\n+\n+// Mapping between a string identifier (e.g., mesh name) and XLA compile\n+// options.\n+using CompileOptionsMap = absl::flat_hash_map<std::string, xla::CompileOptions>;\n+\n+// Name of StringAttr on ifrt.CallOp used to contain the mesh name the IFRT\n+// atom program will execute on. The mesh name is used as a key into a mapping\n+// of optional compile options provided by users per mesh. If no compile options\n+// are provided for a mesh, then the default compile options are used.\n+inline constexpr llvm::StringLiteral kIfrtMeshNameAttrName = \"ifrt.mesh_name\";\n+\n+std::unique_ptr<mlir::Pass> CreateLowerToIfrtPass();\n+\n+// Registers:\n+// -mpmd-lower-to-ifrt:\n+// -mpmd-ifrt-add-ctrl-dependencies\n+void RegisterLowerToIfrtPasses();\n+\n+// Lowers a Shardy:MPMD module as an IFRT module.\n+// `add_control_dependencies` is used to add control dependencies between\n+// fragments. This will enforce strict fragment execution order and is useful\n+// for guaranteeing correct pipelining.\n+absl::Status LowerToIfrt(mlir::ModuleOp module,\n+                         bool add_control_dependencies = true);\n+\n+// Gets the per fragment compile options from the IFRT IR module.\n+// `compile_options_overrides` is a mapping from mesh name to compile option\n+// overrides. The overrides apply to all the computations assigned to a mesh.\n+// `threshold_for_argument_tupling` is the threshold for argument tupling.\n+// `set_reserved_bytes` is a callback to inform the compiler of how much memory\n+// is expected to be already used by other programs, when compiling a given\n+// fragment. This is platform dependent.\n+//\n+absl::StatusOr<CompileOptionsMap> GetCompileOptions(\n+    mlir::ModuleOp module,\n+    const absl::flat_hash_map<std::string, const EnvOptionsOverride>&\n+        compile_options_overrides,\n+    int threshold_for_argument_tupling = 2000,\n+    llvm::function_ref<void(xla::ExecutableBuildOptions&, int64_t)>\n+        set_reserved_bytes = [](xla::ExecutableBuildOptions&, int64_t) {});\n+\n+}  // namespace xla::ifrt::mpmd\n+\n+#endif  // XLA_PYTHON_IFRT_IR_CONVERSIONS_MPMD_LOWER_TO_IFRT_H_"
        },
        {
            "sha": "652b7c760c18ed582f0159dcfbc01a0d1f3056e4",
            "filename": "third_party/xla/xla/python/ifrt/ir/conversions/mpmd/utils.cc",
            "status": "added",
            "additions": 79,
            "deletions": 0,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils.cc?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,79 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/python/ifrt/ir/conversions/mpmd/utils.h\"\n+\n+#include <cstdint>\n+#include <functional>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/log/check.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"shardy/dialect/mpmd/ir/dialect.h\"\n+#include \"shardy/dialect/sdy/ir/dialect.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/python/ifrt/ir/constants.h\"\n+#include \"xla/python/ifrt/ir/sharding_param.h\"\n+#include \"xla/python/ifrt/support/sharding_conversions.h\"\n+#include \"xla/service/spmd/shardy/constants.h\"\n+#include \"xla/service/spmd/shardy/stablehlo_round_trip/export_shardings.h\"\n+\n+namespace xla::ifrt::mpmd {\n+\n+namespace sdy = ::mlir::sdy;\n+using ::mlir::DenseSet;\n+using ::mlir::func::FuncOp;\n+using ::mlir::mpmd::MeshTensorType;\n+\n+xla::HloSharding GetHloSharding(MeshTensorType mesh_tensor_type,\n+                                sdy::MeshAttr sdy_mesh_attr) {\n+  sdy::TensorShardingAttr sharding = mesh_tensor_type.getSharding();\n+  // If there is no sharding, it means the mesh_tensor_type is fully replicated\n+  // but we need to pass a non-null sharding to the conversion function.\n+  if (!sharding) {\n+    sharding = sdy::TensorShardingAttr::getFullyClosed(\n+        mesh_tensor_type.getContext(),\n+        mesh_tensor_type.getRankedTensorType().getRank(),\n+        xla::sdy::kGlobalMeshName);\n+  }\n+  return xla::sdy::convertToHloSharding(\n+      sharding, [&](sdy::TensorShardingAttr sharding) { return sdy_mesh_attr; },\n+      /*manualAxes=*/{});\n+}\n+\n+// TODO: b/353920283 - Directly convert to ShardingParam from sdy sharding. This\n+// implementation now converts sdy sharding to hlo sharding and then to sharding\n+// param.\n+absl::StatusOr<xla::ifrt::ShardingParam> MeshTensorTypeToShardingParam(\n+    MeshTensorType mesh_tensor_type, sdy::MeshAttr mesh_attr) {\n+  return xla::ifrt::support::ToShardingParam(\n+      /*hlo_sharding=*/GetHloSharding(mesh_tensor_type, mesh_attr),\n+      /*rank=*/mesh_tensor_type.getRankedTensorType().getRank(),\n+      /*num_devices=*/\n+      absl::c_accumulate(mesh_attr.getAxes(), 1,\n+                         [](int64_t acc, sdy::MeshAxisAttr axis) {\n+                           return acc * axis.getSize();\n+                         }));\n+}\n+\n+bool IsIfrtFunction(FuncOp func_op) {\n+  return func_op->hasAttr(xla::ifrt::kIfrtFunctionAttrName);\n+}\n+\n+}  // namespace xla::ifrt::mpmd"
        },
        {
            "sha": "c86812c61d63f3f0193d19dd1e6ec10b4f7a4198",
            "filename": "third_party/xla/xla/python/ifrt/ir/conversions/mpmd/utils.h",
            "status": "added",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils.h?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,49 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef XLA_PYTHON_IFRT_IR_CONVERSIONS_MPMD_UTILS_H_\n+#define XLA_PYTHON_IFRT_IR_CONVERSIONS_MPMD_UTILS_H_\n+\n+#include \"absl/status/statusor.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"shardy/dialect/mpmd/ir/dialect.h\"\n+#include \"shardy/dialect/sdy/ir/dialect.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/python/ifrt/ir/sharding_param.h\"\n+\n+namespace xla::ifrt::mpmd {\n+\n+// Converts a MeshTensorType to a ShardingParam.\n+//\n+// The ShardingParam has: 1) `dim_shards` matching the rank of the tensor, with\n+// each entry representing the number of shards for the corresponding dimension\n+// 2) `axis_sizes` with the sizes of the mesh dimensions, and 3) `permutations`\n+// of the same length as `axis_sizes` telling how the shards are mapped over\n+// the axis in `minor_to_major` order.\n+//\n+// For example, a MeshTensor with <[\"x\":range<2>,\"y\":range<1>], f32[4{1},2{0}]>\n+// is converted to a ShardingParam with `dim_shards` 1x2, `permutations` [1, 0].\n+absl::StatusOr<xla::ifrt::ShardingParam> MeshTensorTypeToShardingParam(\n+    mlir::mpmd::MeshTensorType mesh_tensor_type, mlir::sdy::MeshAttr mesh_attr);\n+\n+// Converts a MeshTensorType to an HloSharding.\n+xla::HloSharding GetHloSharding(mlir::mpmd::MeshTensorType mesh_tensor_type,\n+                                mlir::sdy::MeshAttr sdy_mesh_attr);\n+\n+// Returns true if the function is annotated with the ifrt.function attribute.\n+bool IsIfrtFunction(mlir::func::FuncOp func_op);\n+\n+}  // namespace xla::ifrt::mpmd\n+\n+#endif  // XLA_PYTHON_IFRT_IR_CONVERSIONS_MPMD_UTILS_H_"
        },
        {
            "sha": "dc74a9c2e18b090df267fff317ab5311d9c679f5",
            "filename": "third_party/xla/xla/python/ifrt/ir/conversions/mpmd/utils_test.cc",
            "status": "added",
            "additions": 201,
            "deletions": 0,
            "changes": 201,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Fconversions%2Fmpmd%2Futils_test.cc?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,201 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/python/ifrt/ir/conversions/mpmd/utils.h\"\n+\n+#include <cstdint>\n+\n+#include <gtest/gtest.h>\n+#include \"llvm/ADT/DenseMap.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/Parser/Parser.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"shardy/dialect/mpmd/ir/dialect.h\"\n+#include \"shardy/dialect/sdy/ir/dialect.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/python/ifrt/support/sharding_conversions.h\"\n+#include \"xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_import.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+using ::llvm::SmallVector;\n+using ::mlir::ArrayRef;\n+using ::mlir::DenseSet;\n+using ::mlir::MLIRContext;\n+using ::mlir::StringRef;\n+using ::mlir::mpmd::MeshTensorType;\n+using ::xla::HloSharding;\n+using ::xla::TileAssignment;\n+\n+namespace xla::ifrt::mpmd {\n+namespace {\n+\n+struct ShardingParamConversionTestStruct {\n+  HloSharding hlo_sharding;\n+  ArrayRef<int64_t> tensor_shape;\n+  ArrayRef<StringRef> axes_names;\n+  ArrayRef<int64_t> axes_sizes;\n+};\n+\n+using ShardingParamConversionTest =\n+    ::testing::TestWithParam<ShardingParamConversionTestStruct>;\n+\n+TEST_P(ShardingParamConversionTest, MeshTensorTypeToShardingParam) {\n+  // The test verifies the conversion MeshTensorType to ShardingParam\n+  // by checking whether the conversion to HloSharding matches.\n+  // We assume that the following conversions are correct:\n+  // MeshTensorType => HloSharding <=> ShardingParam. No useful\n+  // information is lost in the process of conversion. Thus, to verify\n+  // MeshTensorType => ShardingParam, we can convert both to HloSharding\n+  // and verify equality of the HloShardings obtained.\n+  const auto& param = GetParam();\n+\n+  MLIRContext context;\n+  context.loadDialect<mlir::sdy::SdyDialect, mlir::mpmd::MpmdDialect>();\n+\n+  SmallVector<mlir::sdy::MeshAxisAttr> mesh_axes;\n+  mesh_axes.reserve(param.axes_names.size());\n+  for (const auto& [name, size] :\n+       llvm::zip(param.axes_names, param.axes_sizes)) {\n+    mesh_axes.push_back(mlir::sdy::MeshAxisAttr::get(&context, name, size));\n+  }\n+  mlir::sdy::MeshAttr mesh = mlir::sdy::MeshAttr::get(&context, mesh_axes);\n+\n+  mlir::RankedTensorType ranked_tensor_type = mlir::RankedTensorType::get(\n+      param.tensor_shape, mlir::Float32Type::get(&context));\n+  MeshTensorType mesh_tensor_type = MeshTensorType::get(\n+      mesh.getContext(), \"mesh\",\n+      /*ranked_tensor_type=*/ranked_tensor_type, /*sharding=*/\n+      xla::sdy::convertToSdySharding(\n+          param.hlo_sharding, mesh,\n+          /*deviceIdToMaximalMeshName=*/\n+          llvm::SmallDenseMap<int64_t, mlir::StringRef>(),\n+          ranked_tensor_type.getRank()));\n+\n+  // Check the HloSharding obtained from the MeshTensorType is the same as the\n+  // original HloSharding.\n+  EXPECT_EQ(GetHloSharding(mesh_tensor_type, mesh).ToString(),\n+            param.hlo_sharding.ToString());\n+\n+  // Convert MeshTensorType to ShardingParam.\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto actual_sharding_param,\n+      MeshTensorTypeToShardingParam(mesh_tensor_type, mesh));\n+  TF_EXPECT_OK(actual_sharding_param.verify());\n+\n+  // Convert ShardingParam to HloSharding.\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto actual_hlo_sharding,\n+      xla::ifrt::support::ToHloSharding(actual_sharding_param));\n+\n+  EXPECT_EQ(param.hlo_sharding, actual_hlo_sharding);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    HloShardingToShardingParamTests, ShardingParamConversionTest,\n+    testing::ValuesIn<ShardingParamConversionTestStruct>({\n+        {HloSharding::IotaTile({4, 2}), {8, 4}, {\"x\", \"y\"}, {4, 2}},\n+        {HloSharding::IotaTile({2, 4}, {4, 2}, {1, 0}),\n+         {8, 4},\n+         {\"x\", \"y\"},\n+         {4, 2}},\n+        {HloSharding::IotaTile({8, 1}), {8, 4}, {\"x\", \"y\"}, {4, 2}},\n+        {HloSharding::IotaTile({8, 1}, {4, 2}, {1, 0}),\n+         {8, 4},\n+         {\"x\", \"y\"},\n+         {4, 2}},\n+        {HloSharding::PartialTile(TileAssignment({4, 1, 2}, {8}, {0})),\n+         {8, 4},\n+         {\"x\", \"y\"},\n+         {4, 2}},\n+        {HloSharding::PartialTile(TileAssignment({2, 1, 4}, {4, 2}, {1, 0})),\n+         {8, 4},\n+         {\"x\", \"y\"},\n+         {4, 2}},\n+        {HloSharding::PartialTile(TileAssignment({1, 4, 2}, {8}, {0})),\n+         {8, 4},\n+         {\"x\", \"y\"},\n+         {4, 2}},\n+        {HloSharding::PartialTile(TileAssignment({1, 2, 4}, {4, 2}, {1, 0})),\n+         {8, 4},\n+         {\"x\", \"y\"},\n+         {4, 2}},\n+        {HloSharding::PartialTile(TileAssignment({4, 3, 2}, {2, 3, 4},\n+                                                 {2, 1, 0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({4, 2, 3}, {6, 4}, {1, 0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({6, 1, 4}, {24}, {0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({12, 1, 2}, {2, 12}, {1, 0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({8, 1, 3}, {6, 4}, {1, 0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({2, 1, 12}, {24}, {0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({3, 1, 8}, {2, 3, 4},\n+                                                 {1, 0, 2})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({1, 4, 6}, {6, 4}, {1, 0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({1, 12, 2}, {2, 12}, {1, 0})),\n+         {120, 96},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({3, 2, 1, 4}, {2, 3, 4},\n+                                                 {1, 0, 2})),\n+         {120, 96, 72},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({2, 4, 1, 3}, {2, 3, 4},\n+                                                 {0, 2, 1})),\n+         {120, 96, 72},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({4, 3, 1, 2}, {2, 3, 4},\n+                                                 {2, 1, 0})),\n+         {120, 96, 72},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::PartialTile(TileAssignment({12, 1, 1, 2}, {2, 12},\n+                                                 {1, 0})),\n+         {120, 96, 72},\n+         {\"x\", \"y\", \"z\"},\n+         {2, 3, 4}},\n+        {HloSharding::Replicate(), {120, 96, 72}, {\"x\", \"y\", \"z\"}, {2, 3, 4}},\n+    }));\n+\n+}  // namespace\n+}  // namespace xla::ifrt::mpmd"
        },
        {
            "sha": "19de11f791cc1af953890b5f3e6186f18f4faea4",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2FBUILD?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -43,6 +43,7 @@ xla_cc_binary(\n         \"//xla/python/ifrt/ir\",\n         \"//xla/python/ifrt/ir:atom_program_compiler\",\n         \"//xla/python/ifrt/ir:ifrt_ir_program\",\n+        \"//xla/python/ifrt/ir/conversions/mpmd:lower_to_ifrt\",\n         \"//xla/python/ifrt/ir/transforms:passes\",\n         \"//xla/python/ifrt/support:module_parsing\",\n         \"//xla/tsl/platform:test\",\n@@ -60,6 +61,8 @@ xla_cc_binary(\n         \"@llvm-project//mlir:RegisterAllPasses\",  # buildcleaner: keep\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:platform_port\",\n+        \"@shardy//shardy/dialect/mpmd/ir:dialect\",\n+        \"@shardy//shardy/dialect/sdy/ir:dialect\",\n     ],\n )\n "
        },
        {
            "sha": "e7727ace912a14ca021b137a816cba4214a7b2bd",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/ifrt-opt.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt-opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt-opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fifrt-opt.cc?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -29,11 +29,14 @@ limitations under the License.\n #include \"mlir/IR/DialectRegistry.h\"\n #include \"mlir/InitAllPasses.h\"\n #include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n+#include \"shardy/dialect/mpmd/ir/dialect.h\"\n+#include \"shardy/dialect/sdy/ir/dialect.h\"\n #include \"xla/mlir_hlo/mhlo/IR/register.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/python/ifrt/dtype.h\"\n #include \"xla/python/ifrt/hlo/hlo_program.h\"\n #include \"xla/python/ifrt/ir/atom_program_compiler.h\"\n+#include \"xla/python/ifrt/ir/conversions/mpmd/lower_to_ifrt.h\"\n #include \"xla/python/ifrt/ir/ifrt_dialect.h\"\n #include \"xla/python/ifrt/ir/ifrt_ir_program.h\"\n #include \"xla/python/ifrt/ir/transforms/passes.h\"\n@@ -152,10 +155,12 @@ int main(int argc, char** argv) {\n   mlir::registerAllPasses();\n   xla::ifrt::registerIfrtPassesAndPipelines(\n       compiler, compile_options, atom_executable_map, bound_executable_map);\n+  xla::ifrt::mpmd::RegisterLowerToIfrtPasses();\n   mlir::DialectRegistry registry;\n   xla::ifrt::support::InitializeMlirDialectRegistry(registry);\n   // Register dialects that are only used in the MLIR lit tests.\n-  registry.insert<mlir::math::MathDialect>();\n+  registry.insert<mlir::math::MathDialect, mlir::sdy::SdyDialect,\n+                  mlir::mpmd::MpmdDialect>();\n \n   return mlir::asMainReturnCode(\n       mlir::MlirOptMain(argc, argv, \"IFRT IR dialect driver\\n\", registry));"
        },
        {
            "sha": "1917e63182dd03a72d32cf7289754793374993ca",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/mpmd_add_control_dependencies.mlir",
            "status": "added",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_add_control_dependencies.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_add_control_dependencies.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_add_control_dependencies.mlir?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,43 @@\n+// RUN: ifrt-opt %s -mpmd-lower-to-ifrt -mpmd-ifrt-add-ctrl-dependencies 2>&1 | FileCheck %s\n+\n+!arg_0_tensor = !mpmd.mesh_tensor<\"mesh1\", tensor<3x5xf32>>\n+!arg_1_tensor = !mpmd.mesh_tensor<\"mesh2\", tensor<5x7xf32>>\n+!arg_2_tensor = !mpmd.mesh_tensor<\"mesh1\", tensor<10x3xf32>>\n+!tmp_tensor_mesh1 = !mpmd.mesh_tensor<\"mesh1\", tensor<10x5xf32>>\n+!tmp_tensor_mesh2 = !mpmd.mesh_tensor<\"mesh2\", tensor<10x5xf32>>\n+!res_tensor = !mpmd.mesh_tensor<\"mesh2\", tensor<10x7xf32>>\n+\n+// CHECK-LABEL: module\n+// CHECK: func.func public @main\n+// CHECK-SAME:      %arg0: !ifrt.array<tensor<3x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+// CHECK-SAME:      %arg1: !ifrt.array<tensor<5x7xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>\n+// CHECK-SAME:      %arg2: !ifrt.array<tensor<10x3xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+func.func public @main(%arg0: !arg_0_tensor,\n+                       %arg1: !arg_1_tensor,\n+                       %arg2: !arg_2_tensor)\n+  -> (!res_tensor, !tmp_tensor_mesh1) attributes {\n+    topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=4]>>, <\"mesh2\" : <[\"x\"=4]>>>} {\n+// CHECK-NEXT: %[[CALL_0:.*]], %[[CONTROL_OUTPUT_0:.*]] = ifrt.Call @stage1(%arg0, %arg2)   on devices [0, 1, 2, 3]  {ifrt.mesh_name = \"mesh1\"} : (!ifrt.array<tensor<3x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>, !ifrt.array<tensor<10x3xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>) -> !ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+// CHECK-NEXT: %[[RESHARD:.*]], %{{.+}} = ifrt.Reshard(%[[CALL_0]]) : (!ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>) -> !ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>\n+// CHECK-NEXT: %[[CALL_1:.*]], %[[CONTROL_OUTPUT_1:.*]] = ifrt.Call @stage2(%arg1, %[[RESHARD]])   on devices [4, 5, 6, 7]  {ifrt.mesh_name = \"mesh2\"} : (!ifrt.array<tensor<5x7xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>, !ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>) -> !ifrt.array<tensor<10x7xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>\n+// CHECK-NEXT: %[[CALL_2:.*]], %[[CONTROL_OUTPUT_2:.*]] = ifrt.Call @stage1(%arg0, %arg2)   after %[[CONTROL_OUTPUT_0]] on devices [0, 1, 2, 3] {ifrt.mesh_name = \"mesh1\"} : (!ifrt.array<tensor<3x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>, !ifrt.array<tensor<10x3xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>) -> !ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+// CHECK-NEXT: return %[[CALL_1]], %[[CALL_2]] : !ifrt.array<tensor<10x7xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>, !ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+\n+  %0 = mpmd.fragment_call<mesh=\"mesh1\", origin=[]> @stage1(%arg0, %arg2) : (!arg_0_tensor, !arg_2_tensor) -> !tmp_tensor_mesh1\n+  %1 = mpmd.transfer %0 : (!tmp_tensor_mesh1) -> !tmp_tensor_mesh2\n+  %2 = mpmd.fragment_call<mesh=\"mesh2\", origin=[]> @stage2(%arg1, %1) : (!arg_1_tensor, !tmp_tensor_mesh2) -> !res_tensor\n+  %3 = mpmd.fragment_call<mesh=\"mesh1\", origin=[]> @stage1(%arg0, %arg2) : (!arg_0_tensor, !arg_2_tensor) -> !tmp_tensor_mesh1\n+  return %2, %3 : !res_tensor, !tmp_tensor_mesh1\n+}\n+// CHECK: func.func @stage1(%arg0: tensor<3x5xf32>, %arg1: tensor<10x3xf32>) -> tensor<10x5xf32> {\n+func.func @stage1(%arg0: tensor<3x5xf32>, %arg1: tensor<10x3xf32>)\n+  -> tensor<10x5xf32> attributes {mesh_shape = #sdy.mesh<[\"x\"=4]>} {\n+    %0 = \"stablehlo.dot\"(%arg1, %arg0) : (tensor<10x3xf32>, tensor<3x5xf32>) -> tensor<10x5xf32>\n+    return %0 : tensor<10x5xf32>\n+}\n+// CHECK: func.func @stage2(%arg0: tensor<5x7xf32>, %arg1: tensor<10x5xf32>) -> tensor<10x7xf32> {\n+func.func @stage2(%arg0: tensor<5x7xf32>, %arg1: tensor<10x5xf32>)\n+  -> tensor<10x7xf32> attributes {mesh_shape = #sdy.mesh<[\"x\"=4]>} {\n+    %0 = \"stablehlo.dot\"(%arg1, %arg0) : (tensor<10x5xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>\n+    return %0 : tensor<10x7xf32>\n+}"
        },
        {
            "sha": "bac1ea5bf1547614d322ffb96893aaf584743717",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/mpmd_lower_to_ifrt.mlir",
            "status": "added",
            "additions": 195,
            "deletions": 0,
            "changes": 195,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_lower_to_ifrt.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_lower_to_ifrt.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_lower_to_ifrt.mlir?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,195 @@\n+// RUN: ifrt-opt %s -mpmd-lower-to-ifrt -verify-diagnostics -split-input-file 2>&1 | FileCheck %s\n+\n+// Check that the MeshTensors are correctly converted to IFRT Arrays.\n+!arg_type_0 = !mpmd.mesh_tensor<\"mesh1\", tensor<1x2x3xf32>>\n+!arg_type_1 = !mpmd.mesh_tensor<\"mesh1\", tensor<1xf32>>\n+!arg_type_2 = !mpmd.mesh_tensor<\"mesh2\", tensor<4x2x8xf32>, sharding=<@mesh, [{\"y\"}, {\"x\"}, {\"z\"}]>>\n+!arg_type_3 = !mpmd.mesh_tensor<\"mesh2\", tensor<12x2x9xf32>, sharding=<@mesh, [{\"x\", \"y\",\"z\"}, {?}, {?}]>>\n+!arg_type_4 = !mpmd.mesh_tensor<\"mesh2\", tensor<12x2x9xf32>, sharding=<@mesh, [{\"y\"}, {?}, {?}]>>\n+\n+// CHECK-LABEL: module\n+\n+// CHECK-NOT: sdy.mesh\n+sdy.mesh @mesh_0 = <[\"x\"=2, \"y\"=2, \"z\"=2]>\n+sdy.mesh @mesh_1 = <[\"axis_0\"=2, \"axis_1\"=4, \"axis_2\"=4]>\n+\n+// CHECK: func.func public @main\n+// CHECK-SAME:   %arg0: !ifrt.array<tensor<1x2x3xf32>, #ifrt.sharding_param<1x1x1 to [0] on 8>, #devices>\n+// CHECK-SAME:   %arg1: !ifrt.array<tensor<1xf32>, #ifrt.sharding_param<1 to [0] on 8>, #devices>\n+// CHECK-SAME:   %arg2: !ifrt.array<tensor<4x2x8xf32>, #ifrt.sharding_param<2x2x2 to [0, 2, 1] on 2x2x2>, #devices1>\n+// CHECK-SAME:   %arg3: !ifrt.array<tensor<12x2x9xf32>, #ifrt.sharding_param<8x1x1 to [0] on 8>, #devices1>\n+// CHECK-SAME:   %arg4: !ifrt.array<tensor<12x2x9xf32>, #ifrt.sharding_param<2x1x1 to [0, 2, 1] on 2x2x2>, #devices1>\n+// CHECK-SAME:   xla_tpu_user_reserved_hbm_bytes = 128 : i64\n+func.func public @main(%arg0: !arg_type_0,\n+                       %arg1: !arg_type_1,\n+                       %arg2: !arg_type_2,\n+                       %arg3: !arg_type_3,\n+                       %arg4: !arg_type_4)\n+    -> (!arg_type_0, !arg_type_1, !arg_type_2, !arg_type_3, !arg_type_4) attributes {\n+    topology = #mpmd.topology<\n+        <\"mesh1\" : <[\"x\"=2, \"y\"=2, \"z\"=2]>>, <\"mesh2\" : <[\"x\"=2, \"y\"=2, \"z\"=2]>>>,\n+        xla_tpu_user_reserved_hbm_bytes = 128 : i64} {\n+      return %arg0, %arg1, %arg2, %arg3, %arg4 : !arg_type_0, !arg_type_1, !arg_type_2, !arg_type_3, !arg_type_4\n+}\n+\n+// -----\n+\n+!arg_0_tensor = !mpmd.mesh_tensor<\"mesh1\", tensor<3x5xf32>>\n+!arg_1_tensor = !mpmd.mesh_tensor<\"mesh2\", tensor<5x7xf32>>\n+!arg_2_tensor = !mpmd.mesh_tensor<\"mesh1\", tensor<10x3xf32>>\n+!tmp_tensor_mesh1 = !mpmd.mesh_tensor<\"mesh1\", tensor<10x5xf32>>\n+!tmp_tensor_mesh2 = !mpmd.mesh_tensor<\"mesh2\", tensor<10x5xf32>>\n+!res_tensor = !mpmd.mesh_tensor<\"mesh2\", tensor<10x7xf32>>\n+\n+// CHECK-LABEL: module\n+// CHECK: func.func public @main\n+// CHECK-SAME:      %arg0: !ifrt.array<tensor<3x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+// CHECK-SAME:      %arg1: !ifrt.array<tensor<5x7xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>\n+// CHECK-SAME:      %arg2: !ifrt.array<tensor<10x3xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+// CHECK-SAME:      xla_tpu_user_reserved_hbm_bytes = 256 : i64\n+func.func public @main(%arg0: !arg_0_tensor,\n+                       %arg1: !arg_1_tensor,\n+                       %arg2: !arg_2_tensor)\n+  -> (!res_tensor) attributes {\n+    topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=4]>>, <\"mesh2\" : <[\"x\"=4]>>>,\n+    xla_tpu_user_reserved_hbm_bytes = 256 : i64} {\n+      // CHECK-NEXT: %[[OUTPUTS_0:.*]], %[[CONTROL_OUTPUT_0:.*]] = ifrt.Call @stage1(%arg0, %arg2) on devices [0, 1, 2, 3] {ifrt.mesh_name = \"mesh1\"} : (!ifrt.array<tensor<3x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>, !ifrt.array<tensor<10x3xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>) -> !ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>\n+      // CHECK-NEXT: %[[RESHARD:.*]], %{{.+}} = ifrt.Reshard(%[[OUTPUTS_0]]) : (!ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>) -> !ifrt.array<tensor<10x5xf32>, #ifrt.sharding_param<1x1 to [0] on 4>, [4, 5, 6, 7]>\n+      // CHECK-NEXT: %[[OUTPUTS_1:.*]], %[[CONTROL_OUTPUT_1:.*]] = ifrt.Call @stage2(%arg1, %[[RESHARD]])\n+      // CHECK-NEXT: return %[[OUTPUTS_1]]\n+      %0 = mpmd.fragment_call<mesh=\"mesh1\", origin=[]> @stage1(%arg0, %arg2) {mpmd.is_gspmd_partitioned} : (!arg_0_tensor, !arg_2_tensor) -> !tmp_tensor_mesh1\n+      %1 = mpmd.transfer %0 : (!tmp_tensor_mesh1) -> !tmp_tensor_mesh2\n+      %2 = mpmd.fragment_call<mesh=\"mesh2\", origin=[]> @stage2(%arg1, %1) : (!arg_1_tensor, !tmp_tensor_mesh2) -> !res_tensor\n+      return %2 : !res_tensor\n+}\n+// CHECK: func.func @stage1(%arg0: tensor<3x5xf32>, %arg1: tensor<10x3xf32>) -> tensor<10x5xf32> {\n+func.func @stage1(%arg0: tensor<3x5xf32>, %arg1: tensor<10x3xf32>)\n+  -> tensor<10x5xf32> attributes {mesh_shape = #sdy.mesh<[\"x\"=4]>} {\n+    %0 = \"stablehlo.dot\"(%arg1, %arg0) : (tensor<10x3xf32>, tensor<3x5xf32>) -> tensor<10x5xf32>\n+    return %0 : tensor<10x5xf32>\n+}\n+// CHECK: func.func @stage2(%arg0: tensor<5x7xf32>, %arg1: tensor<10x5xf32>) -> tensor<10x7xf32> {\n+func.func @stage2(%arg0: tensor<5x7xf32>, %arg1: tensor<10x5xf32>)\n+  -> tensor<10x7xf32> attributes {mesh_shape = #sdy.mesh<[\"x\"=4]>} {\n+    %0 = \"stablehlo.dot\"(%arg1, %arg0) : (tensor<10x5xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>\n+    return %0 : tensor<10x7xf32>\n+}\n+\n+// -----\n+\n+!tensor = !mpmd.mesh_tensor<\"mesh1\", tensor<2x2xi32>>\n+\n+// CHECK-LABEL: module @aliasing_output_to_io_aliases\n+module @aliasing_output_to_io_aliases {\n+  func.func public @main(%arg0: !tensor) -> (!tensor)\n+      attributes {topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=2]>>>} {\n+    // CHECK: %[[OUT:.+]], %{{.+}} = ifrt.Call @add_args(%arg0, %arg0)\n+    // CHECK-SAME: on devices [0, 1]\n+    // CHECK-SAME: {\n+    // CHECK-DAG:    ifrt.mesh_name = \"mesh1\"\n+    // CHECK-DAG:    io_aliases = [array<i32: 1, 0>]\n+    // CHECK-SAME: }\n+    %0 = mpmd.fragment_call<mesh=\"mesh1\", origin=[]> @add_args(%arg0, %arg0) {mpmd.is_gspmd_partitioned} : (!tensor, !tensor) -> (!tensor)\n+    return %0 : !tensor\n+  }\n+\n+  // CHECK: func.func @add_args(%arg0: tensor<2x2xi32>, %arg1: tensor<2x2xi32> {tf.aliasing_output = 0 : i32}) -> tensor<2x2xi32> {\n+  func.func @add_args(%arg0: tensor<2x2xi32>,\n+                      %arg1: tensor<2x2xi32> {tf.aliasing_output = 0 : i32})\n+      -> tensor<2x2xi32> attributes {mesh_shape = #sdy.mesh<[\"x\"=2]>} {\n+    %0 = stablehlo.add %arg0, %arg1 : tensor<2x2xi32>\n+    return %0 : tensor<2x2xi32>\n+  }\n+}\n+\n+// -----\n+\n+!tensor_on_host = !mpmd.mesh_tensor<\"mesh1\", tensor<2x2xi32>, memory_kind = \"pinned_host\">\n+!tensor_on_device = !mpmd.mesh_tensor<\"mesh1\", tensor<2x2xi32>, memory_kind = \"device\">\n+\n+// CHECK-LABEL: module @fetch_from_host_to_device\n+module @fetch_from_host_to_device {\n+  func.func public @main(%arg0: !tensor_on_host) -> (!tensor_on_device)\n+      attributes {topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=2]>>>} {\n+    // CHECK: ifrt.Reshard(%arg0)\n+    // CHECK-SAME: (!ifrt.array<tensor<2x2xi32>, #ifrt.sharding_param<1x1 to [0] on 2>, [0, 1], memory_kind = \"pinned_host\">) ->\n+    // CHECK-SAME: !ifrt.array<tensor<2x2xi32>, #ifrt.sharding_param<1x1 to [0] on 2>, [0, 1], memory_kind = \"device\">\n+    %0 = mpmd.transfer %arg0 : (!tensor_on_host) -> (!tensor_on_device)\n+    return %0 : !tensor_on_device\n+  }\n+}\n+\n+// -----\n+\n+!arg0_tensor = !mpmd.mesh_tensor<\"m1\", tensor<4x8xf32>,\n+                                        sharding=<@mesh, [{\"x\"}, {?}]>>\n+!res_tensor = !mpmd.mesh_tensor<\"m1\", tensor<4x8xf32>,\n+                                       sharding=<@mesh, [{\"x\"}, {?}]>>\n+\n+// CHECK-LABEL: module @sdy_lowered_fragment\n+// CHECK-SAME: attributes {\n+// CHECK-DAG:    mhlo.frontend_attributes = {xla.sdy.meshes = \"{mesh = #sdy.mesh<[\\\\\\22x\\\\\\22=2]>}\"}\n+// CHECK-SAME: }\n+module @sdy_lowered_fragment attributes {\n+    mhlo.frontend_attributes = {\n+      xla.sdy.meshes =\"{mesh = #sdy.mesh<[\\\\\\22x\\\\\\22=2]>}\"}}  {\n+  sdy.mesh @mesh = <[\"x\"=2]>\n+  func.func public @main(%arg0: !arg0_tensor) ->  !res_tensor attributes {\n+      topology = #mpmd.topology<<\"m1\" : <[\"x\"=2]>>>} {\n+    // CHECK: ifrt.Call @f(%arg0) on devices [0, 1]\n+    // CHECK-SAME: {\n+    // CHECK-DAG:    ifrt.is_sdy_partitioned\n+    // CHECK-DAG:    ifrt.mesh_name = \"m1\"\n+    // CHECK-SAME: }\n+    // CHECK-SAME: (!ifrt.array<tensor<4x8xf32>, #ifrt.sharding_param<2x1 to [0] on 2>, [0, 1]>) ->\n+    // CHECK-SAME: !ifrt.array<tensor<4x8xf32>, #ifrt.sharding_param<2x1 to [0] on 2>, [0, 1]>\n+    %0 = mpmd.fragment_call<mesh=\"m1\", origin=[\"f1\"]> @\"f\"(%arg0) {\n+      mpmd.is_sdy_partitioned} : (!arg0_tensor) -> !res_tensor\n+    return %0 : !res_tensor\n+  }\n+  // CHECK: func.func @f(%arg0: tensor<4x8xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\\\\\22x\\\\\\22, ?}, {?}]>\"}})\n+  // CHECK-SAME: -> (tensor<4x8xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\\\\\22x\\\\\\22, ?}, {?}]>\"}})\n+  func.func @\"f\"(%arg0: tensor<4x8xf32> {\n+      mhlo.frontend_attributes = {\n+        xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\\\\\22x\\\\\\22, ?}, {?}]>\"}})\n+      -> (tensor<4x8xf32> {\n+        mhlo.frontend_attributes = {\n+          xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\\\\\22x\\\\\\22, ?}, {?}]>\"}})\n+    attributes {mesh_shape = #sdy.mesh<[\"x\"=2]>} {\n+    return %arg0 : tensor<4x8xf32>\n+  }\n+}\n+\n+// -----\n+\n+!tensor_on_mesh1 = !mpmd.mesh_tensor<\"mesh1\", tensor<2x2xi32>, sharding=<@mesh, [{}, {}]>>\n+!tensor_on_mesh2 = !mpmd.mesh_tensor<\"mesh2\", tensor<2x2xi32>>\n+\n+// CHECK-LABEL: module @copy_from_mesh1_to_mesh2_same_shape\n+module @copy_from_mesh1_to_mesh2_same_shape {\n+  func.func public @main(%arg0: !tensor_on_mesh1) -> (!tensor_on_mesh2)\n+      attributes {topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=2]>>, <\"mesh2\" : <[\"x\"=2]>>>} {\n+    // CHECK: ifrt.Reshard(%arg0)\n+    // CHECK-SAME: (!ifrt.array<tensor<2x2xi32>, #ifrt.sharding_param<1x1 to [0] on 2>, [0, 1]>) ->\n+    // CHECK-SAME: !ifrt.array<tensor<2x2xi32>, #ifrt.sharding_param<1x1 to [0] on 2>, [2, 3]>\n+    %0 = mpmd.transfer %arg0 : (!tensor_on_mesh1) -> (!tensor_on_mesh2)\n+    return %0 : !tensor_on_mesh2\n+  }\n+}\n+\n+// -----\n+\n+!tensor_on_mesh1 = !mpmd.mesh_tensor<\"mesh1\", tensor<2x2xi32>, sharding=<@mesh, [{}, {}]>>\n+!tensor_on_mesh2 = !mpmd.mesh_tensor<\"mesh2\", tensor<2x2xi32>>\n+\n+// CHECK-LABEL: module @copy_from_mesh1_to_mesh2_different_shape\n+module @copy_from_mesh1_to_mesh2_different_shape {\n+  func.func public @main(%arg0: !tensor_on_mesh1) -> (!tensor_on_mesh2)\n+      attributes {topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=4]>>, <\"mesh2\" : <[\"x\"=2]>>>} {\n+    // CHECK: ifrt.Reshard(%arg0)\n+    // CHECK-SAME: (!ifrt.array<tensor<2x2xi32>, #ifrt.sharding_param<1x1 to [0] on 4>, [0, 1, 2, 3]>) ->\n+    // CHECK-SAME: !ifrt.array<tensor<2x2xi32>, #ifrt.sharding_param<1x1 to [0] on 2>, [4, 5]>\n+    %0 = mpmd.transfer %arg0 : (!tensor_on_mesh1) -> (!tensor_on_mesh2)\n+    return %0 : !tensor_on_mesh2\n+  }\n+}"
        },
        {
            "sha": "04f855bd3d497a5376edca171ffdbe7f5da93401",
            "filename": "third_party/xla/xla/python/ifrt/ir/tests/mpmd_merge_reshards.mlir",
            "status": "added",
            "additions": 92,
            "deletions": 0,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_merge_reshards.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7fd932191623376e671a149293ba7bb2b7ee7c6f/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_merge_reshards.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftests%2Fmpmd_merge_reshards.mlir?ref=7fd932191623376e671a149293ba7bb2b7ee7c6f",
            "patch": "@@ -0,0 +1,92 @@\n+// RUN: ifrt-opt %s -mpmd-lower-to-ifrt -ifrt-merge-reshards -split-input-file 2>&1 | FileCheck %s\n+\n+// Reshards out of Call Op are merged.\n+\n+!m1_4x8 = !mpmd.mesh_tensor<\"mesh1\", tensor<4x8xf32>>\n+!m2_4x8 = !mpmd.mesh_tensor<\"mesh2\", tensor<4x8xf32>>\n+\n+// CHECK-LABEL: module\n+// CHECK: func.func public @main\n+func.func public @main(%arg0: !m1_4x8, %arg1: !m1_4x8)\n+  -> (!m2_4x8, !m2_4x8) attributes {\n+    topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=4]>>, <\"mesh2\" : <[\"x\"=4]>>>,\n+    xla_tpu_user_reserved_hbm_bytes = 256 : i64} {\n+// Note that we batch the reshards out of the first call, but not the reshards\n+// into the second call.\n+// CHECK-NEXT: %[[M1_C0:.*]]:2, %{{.*}} = ifrt.Call @f(%arg0, %arg1)\n+// CHECK-NEXT: %[[M1_C1:.*]]:2, %{{.*}} = ifrt.Call @f(%arg0, %arg1)\n+// CHECK-NEXT: %[[BATCHED_RESHARD:.*]]:2, %{{.*}} = ifrt.Reshard(%[[M1_C0]]#0, %[[M1_C0]]#1)\n+// CHECK-NEXT: %[[RESHARD1:.*]], %{{.*}} = ifrt.Reshard(%[[M1_C1]]#0)\n+// CHECK-NEXT: %[[M2_C0:.*]]:2, %{{.*}} = ifrt.Call @f(%[[BATCHED_RESHARD]]#0, %[[BATCHED_RESHARD]]#1)\n+// CHECK-NEXT: %[[M2_C1:.*]]:2, %{{.*}} = ifrt.Call @f(%[[BATCHED_RESHARD]]#1, %[[RESHARD1]])\n+// CHECK-NEXT: return %[[M2_C0]]#0, %[[M2_C1]]#1\n+  %m1_c0:2 = mpmd.fragment_call<mesh=\"mesh1\", origin=[]> @f(%arg0, %arg1) : (!m1_4x8, !m1_4x8) -> (!m1_4x8, !m1_4x8)\n+  %m1_c1:2 = mpmd.fragment_call<mesh=\"mesh1\", origin=[]> @f(%arg0, %arg1) : (!m1_4x8, !m1_4x8) -> (!m1_4x8, !m1_4x8)\n+\n+  %t0 = mpmd.transfer %m1_c0#0 : (!m1_4x8) -> !m2_4x8\n+  %t1 = mpmd.transfer %m1_c0#1 : (!m1_4x8) -> !m2_4x8\n+  %t2 = mpmd.transfer %m1_c1#0 : (!m1_4x8) -> !m2_4x8\n+\n+  %m2_c0:2 = mpmd.fragment_call<mesh=\"mesh2\", origin=[]> @f(%t0, %t1) : (!m2_4x8, !m2_4x8) -> (!m2_4x8, !m2_4x8)\n+  %m2_c1:2 = mpmd.fragment_call<mesh=\"mesh2\", origin=[]> @f(%t1, %t2) : (!m2_4x8, !m2_4x8) -> (!m2_4x8, !m2_4x8)\n+  return %m2_c0#0, %m2_c1#1 : !m2_4x8, !m2_4x8\n+}\n+\n+func.func @f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)\n+  -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {mesh_shape = #sdy.mesh<[\"x\"=4]>} {\n+    return %arg0, %arg1 : tensor<4x8xf32>, tensor<4x8xf32>\n+}\n+\n+// -----\n+\n+// Reshards out of Func Args are merged.\n+\n+!m1_4x8 = !mpmd.mesh_tensor<\"mesh1\", tensor<4x8xf32>>\n+!m2_4x8 = !mpmd.mesh_tensor<\"mesh2\", tensor<4x8xf32>>\n+\n+// CHECK-LABEL: module\n+// CHECK: func.func public @main\n+func.func public @main(%arg0: !m1_4x8, %arg1: !m1_4x8)\n+  -> (!m2_4x8, !m2_4x8) attributes {\n+    topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=4]>>, <\"mesh2\" : <[\"x\"=4]>>>,\n+    xla_tpu_user_reserved_hbm_bytes = 256 : i64} {\n+// CHECK-NEXT: %[[BATCHED_RESHARD:.*]]:2, %{{.*}} = ifrt.Reshard(%arg0, %arg1)\n+// CHECK-NEXT: %[[M2_C0:.*]]:2, %{{.*}} = ifrt.Call @f(%[[BATCHED_RESHARD]]#0, %[[BATCHED_RESHARD]]#1)\n+// CHECK-NEXT: return %[[M2_C0]]#0, %[[M2_C0]]#1\n+\n+  %t0 = mpmd.transfer %arg0 : (!m1_4x8) -> !m2_4x8\n+  %t1 = mpmd.transfer %arg1 : (!m1_4x8) -> !m2_4x8\n+\n+  %m2_c0:2 = mpmd.fragment_call<mesh=\"mesh2\", origin=[]> @f(%t0, %t1) : (!m2_4x8, !m2_4x8) -> (!m2_4x8, !m2_4x8)\n+  return %m2_c0#0, %m2_c0#1 : !m2_4x8, !m2_4x8\n+}\n+\n+func.func @f(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>)\n+  -> (tensor<4x8xf32>, tensor<4x8xf32>) attributes {mesh_shape = #sdy.mesh<[\"x\"=4]>} {\n+    return %arg0, %arg1 : tensor<4x8xf32>, tensor<4x8xf32>\n+}\n+\n+// -----\n+\n+// Reshards into ReturnOp are grouped by destination\n+\n+!m1_4x8 = !mpmd.mesh_tensor<\"mesh1\", tensor<4x8xf32>>\n+!m2_4x8 = !mpmd.mesh_tensor<\"mesh2\", tensor<4x8xf32>>\n+!m3_4x8 = !mpmd.mesh_tensor<\"mesh3\", tensor<4x8xf32>>\n+\n+// CHECK-LABEL: module\n+// CHECK: func.func public @main\n+func.func public @main(%arg0: !m1_4x8, %arg1: !m1_4x8, %arg2: !m1_4x8)\n+  -> (!m2_4x8, !m2_4x8, !m3_4x8) attributes {\n+    topology = #mpmd.topology<<\"mesh1\" : <[\"x\"=4]>>, <\"mesh2\" : <[\"x\"=4]>>, <\"mesh3\" : <[\"x\"=4]>>>,\n+    xla_tpu_user_reserved_hbm_bytes = 256 : i64} {\n+// CHECK-NEXT: %[[BATCHED_RESHARD:.*]]:2, %{{.*}} = ifrt.Reshard(%arg0, %arg1)\n+// CHECK-NEXT: %[[RESHARD1:.*]], %{{.*}} = ifrt.Reshard(%arg2)\n+// CHECK-NEXT: return %[[BATCHED_RESHARD]]#0, %[[BATCHED_RESHARD]]#1, %[[RESHARD1]]\n+\n+  %t0 = mpmd.transfer %arg0 : (!m1_4x8) -> !m2_4x8\n+  %t1 = mpmd.transfer %arg1 : (!m1_4x8) -> !m2_4x8\n+  %t2 = mpmd.transfer %arg2 : (!m1_4x8) -> !m3_4x8\n+\n+  return %t0, %t1, %t2 : !m2_4x8, !m2_4x8, !m3_4x8\n+}"
        }
    ],
    "stats": {
        "total": 1599,
        "additions": 1598,
        "deletions": 1
    }
}