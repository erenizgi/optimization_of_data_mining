{
    "author": "derdrdirk",
    "message": "[Autotuner] Add cublas support for scaled dot fusion.\n\nPiperOrigin-RevId: 842746529",
    "sha": "28cd67b54e279fa9f8c9352284c477b93a18531a",
    "files": [
        {
            "sha": "5b4eea2a700a9ad01dce8a4f543bb32d6061e15c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=28cd67b54e279fa9f8c9352284c477b93a18531a",
            "patch": "@@ -612,6 +612,7 @@ cc_library(\n         \"//xla/service:compiler\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n+        \"//xla/service/gpu/transforms:scaled_dot_rewriter\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n@@ -764,6 +765,7 @@ xla_test(\n         \"//xla/service/gpu/transforms:custom_kernel_fusion_rewriter\",\n         \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n         \"//xla/service/gpu/transforms:gemm_rewriter\",\n+        \"//xla/service/gpu/transforms:scaled_dot_rewriter\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "28b5786357d1a8c1530edb0045eaf69ac0e62d12",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=28cd67b54e279fa9f8c9352284c477b93a18531a",
            "patch": "@@ -1,3 +1,4 @@\n+#include \"xla/service/gpu/transforms/scaled_dot_rewriter.h\"\n /* Copyright 2025 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -46,6 +47,7 @@ using ::mlir::MLIRContext;\n std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n     const se::DeviceDescription& device_description) {\n   auto pipeline = std::make_unique<HloPassPipeline>(\"cublas_rewriter_pipeline\");\n+  pipeline->AddPass(std::make_unique<ScaledDotRewriter>());\n   pipeline->AddPass(std::make_unique<DotAlgorithmRewriter>());\n   for (GemmRewriterOptions::DType dtype :\n        {GemmRewriterOptions::DType::kFp8Only,"
        },
        {
            "sha": "0400562457db38b5f7ab2aec215f61269dfd6553",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend_test.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc?ref=28cd67b54e279fa9f8c9352284c477b93a18531a",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/custom_kernel_fusion_rewriter.h\"\n #include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n+#include \"xla/service/gpu/transforms/scaled_dot_rewriter.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -90,6 +91,27 @@ ENTRY main {\n   ROOT %dot.0 = f32[64,64]{1,0} fusion(p0, p1), kind=kCustom, calls=gemm_fusion, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"},\"force_earliest_schedule\":false}\n })\";\n \n+const char kScaledDotFusionHlo[] = R\"(\n+HloModule module\n+\n+fusion_computation {\n+  p0 = f32[1024,1024] parameter(0)\n+  p1 = f32[1024,1024] parameter(1)\n+  p0_scale = f32[1024,8] parameter(2)\n+  p1_scale = f32[8,1024] parameter(3)\n+  ROOT r = f32[1024,1024] scaled-dot(p0, p1, p0_scale, p1_scale),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  p0 = f32[1024,1024] parameter(0)\n+  p1 = f32[1024,1024] parameter(1)\n+  p0_scale = f32[1024,8] parameter(2)\n+  p1_scale = f32[8,1024] parameter(3)\n+  ROOT r = f32[1024,1024] fusion(p0, p1, p0_scale, p1_scale),\n+    kind=kCustom, calls=fusion_computation\n+})\";\n+\n const char kUnsupportedFusionHlo[] = R\"(\n   HloModule module\n   computation {\n@@ -131,6 +153,7 @@ class FissionTest : public HloHardwareIndependentTestBase,\n   static std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n       const se::DeviceDescription& device_description) {\n     auto pipeline = std::make_unique<HloPassPipeline>(\"fission_pipeline\");\n+    pipeline->AddPass(std::make_unique<ScaledDotRewriter>());\n     pipeline->AddPass(std::make_unique<DotAlgorithmRewriter>());\n     for (GemmRewriterOptions::DType dtype :\n          {GemmRewriterOptions::DType::kFp8Only,\n@@ -287,6 +310,14 @@ INSTANTIATE_TEST_SUITE_P(\n              \"\\\"kind\\\":\\\"__custom_fusion\\\"\",\n          },\n          /*expected_backend_name=*/\"CustomKernel_fission\"},\n+        {\"ScaledDotFusion_Cublas\",\n+         kScaledDotFusionHlo,\n+         &FissionTest::GetCublasRewriterPipeline,\n+         &FissionTest::CreateCublasBackend,\n+         /*expected_module_substrings=*/\n+         {\"custom_call_target=\\\"__cublas$gemm\\\"\",\n+          \"\\\"selected_algorithm\\\":\\\"-1\\\"\"},\n+         /*expected_backend_name=*/\"Cublas_fission\"},\n     }),\n     [](const ::testing::TestParamInfo<FissionTest::ParamType>& info) {\n       return info.param.test_name;"
        },
        {
            "sha": "548c9281bda6253da99ecaa8971df1a5fff32d2b",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/28cd67b54e279fa9f8c9352284c477b93a18531a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=28cd67b54e279fa9f8c9352284c477b93a18531a",
            "patch": "@@ -145,6 +145,7 @@ namespace {\n std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n     const se::DeviceDescription* device_description) {\n   auto pipeline = std::make_unique<HloPassPipeline>(\"cublas_rewriter_pipeline\");\n+  pipeline->AddPass(std::make_unique<ScaledDotRewriter>());\n   pipeline->AddPass(std::make_unique<DotAlgorithmRewriter>());\n   for (GemmRewriterOptions::DType dtype :\n        {GemmRewriterOptions::DType::kFp8Only,"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 36,
        "deletions": 0
    }
}