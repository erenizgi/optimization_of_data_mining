{
    "author": "sergey-kozub",
    "message": "PR #30967: [XLA:GPU] Allow split-k transformation on block scaled dot fusions\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30967\n\nüìù Summary of Changes\nModifies the SplitK GEMM rewriter to support fusions around scaled dot HLO op.\n\nüéØ Justification\nThis PR will allow the triton legacy emitter to handle block scaled dot.\n\nüöÄ Kind of Contribution\n‚ú® New Feature (part of a CL chain)\n\n- If padding is needed, pad the scaling tensors first and multiply by the block size to get the contracting dimension size (note that block sizes may be different or missing, i.e. scalar scale);\n- Adds DCE in case the rewriter fails after updating the computation (also affects regular dot fusion);\n- Exposes block sizes in `TritonFusionAnalysis` (nullopt for scalars);\n- Adds `MakeScaledDotHlo` helper and scaled dot pattern matcher;\nCopybara import of the project:\n\n--\n2f528688f72984fdfe87893a28f8f0afee4e4058 by Sergey Kozub <skozub@nvidia.com>:\n\n[XLA:GPU] Allow split-k transformation on block scaled dot fusions\n\nMerging this change closes #30967\n\nPiperOrigin-RevId: 809901128",
    "sha": "f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
    "files": [
        {
            "sha": "65afe534f1e03fda8d763ec034c16a5ffe830884",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/hlo_dce.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_dce.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_dce.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_dce.h?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -49,7 +49,8 @@ class HloDCE : public HloModulePass {\n \n   // Run DCE on a computation.\n   static absl::StatusOr<bool> RunOnComputation(\n-      HloComputation* computation, bool remove_cross_partition_collective_ops,\n+      HloComputation* computation,\n+      bool remove_cross_partition_collective_ops = false,\n       CallGraph* call_graph = nullptr);\n \n   // Run the pass on the given module. Returns whether the module was changed"
        },
        {
            "sha": "6f4fc301f340dc4226238394b8ade758e3bed9bb",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -952,6 +952,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:algorithm_util\",\n         \"//xla/service:hlo_creation_utils\","
        },
        {
            "sha": "6f366b8e745ab8cdb2cb8e7d286fc03853cf62e4",
            "filename": "third_party/xla/xla/service/gpu/split_k_gemm_rewriter.cc",
            "status": "modified",
            "additions": 116,
            "deletions": 26,
            "changes": 142,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/layout.h\"\n #include \"xla/literal_util.h\"\n@@ -112,14 +113,13 @@ absl::Status UncompilableMatmul(absl::string_view explanation) {\n absl::StatusOr<HloInstruction*> MakeSplitKOperand(\n     HloInstruction& dot, const TritonFusionAnalysis& analysis,\n     const TritonGemmConfig& config, const int64_t contracting_dim_idx,\n-    const int operand_number) {\n+    TritonFusionAnalysis::Scope scope, const int operand_number,\n+    std::optional<int64_t> padded_k_size = std::nullopt) {\n   HloInstruction* operand = dot.mutable_operand(operand_number);\n   const int64_t k = operand->shape().dimensions(contracting_dim_idx);\n-  const bool need_padding = k % config.split_k != 0;\n+  const bool need_padding =\n+      padded_k_size.has_value() ? k < *padded_k_size : k % config.split_k != 0;\n \n-  TritonFusionAnalysis::Scope scope = (operand_number == 0)\n-                                          ? TritonFusionAnalysis::Scope::LHS\n-                                          : TritonFusionAnalysis::Scope::RHS;\n   auto check_if_supported = [&](const HloInstruction& hlo,\n                                 bool check_divisibility) {\n     const TensorIterationSpec::DimIterationSpec* spec =\n@@ -140,7 +140,10 @@ absl::StatusOr<HloInstruction*> MakeSplitKOperand(\n                                   fragment.subfragments, config.split_k)) {\n       return UncompilableMatmul(\"Contracting dimension is too fragmented.\");\n     }\n-    if (config.split_k > ceil(1.0 * fragment.count / config.block_k)) {\n+    bool is_scale = scope == TritonFusionAnalysis::Scope::LHS_SCALE ||\n+                    scope == TritonFusionAnalysis::Scope::RHS_SCALE;\n+    if (!is_scale ? config.split_k > ceil(1.0 * fragment.count / config.block_k)\n+                  : config.split_k >= fragment.count) {\n       return UncompilableMatmul(\n           \"Too small divisible part of the contracting dimension.\");\n     }\n@@ -168,10 +171,13 @@ absl::StatusOr<HloInstruction*> MakeSplitKOperand(\n         dot.parent()->AddInstruction(HloInstruction::CreateConstant(\n             LiteralUtil::Zero(operand->shape().element_type())));\n \n+    int64_t padding = padded_k_size.has_value()\n+                          ? *padded_k_size - k\n+                          : config.split_k - k % config.split_k;\n     PaddingConfig padding_config =\n         MakeNoPaddingConfig(operand->shape().dimensions().size());\n     padding_config.mutable_dimensions(contracting_dim_idx)\n-        ->set_edge_padding_high(config.split_k - k % config.split_k);\n+        ->set_edge_padding_high(padding);\n \n     TF_ASSIGN_OR_RETURN(HloInstruction * pad,\n                         MakePadHlo(operand, zero, padding_config));\n@@ -216,8 +222,14 @@ absl::StatusOr<HloInstruction*> MakeSplitKOperand(\n // dimensions.\n absl::Status MakeDotComputationSplitKBatch(HloComputation* computation,\n                                            const TritonGemmConfig& config) {\n-  HloDotInstruction* dot = Cast<HloDotInstruction>(\n-      hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot));\n+  HloInstruction* dot =\n+      hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n+  if (dot == nullptr) {\n+    dot = hlo_query::GetFirstInstructionWithOpcode(*computation,\n+                                                   HloOpcode::kScaledDot);\n+    CHECK(dot != nullptr);\n+  }\n+\n   TF_ASSIGN_OR_RETURN(const auto analysis,\n                       TritonFusionAnalysis::Execute(*computation));\n   const DotDimensionNumbers& old_dim_numbers = dot->dot_dimension_numbers();\n@@ -265,29 +277,100 @@ absl::Status MakeDotComputationSplitKBatch(HloComputation* computation,\n     }\n   } while (true);\n \n-  // Process the collected HLOs from computation root to dot.\n+  // Keep track of whether any of the operands were padded.\n   bool did_pad = false;\n-  PrimitiveType accumulator_dtype = GetGemmAccumulatorType(dot);\n+  auto is_padded = [](const HloInstruction* op) {\n+    return op->opcode() == HloOpcode::kBitcast &&\n+           op->operand(0)->opcode() == HloOpcode::kPad;\n+  };\n+\n+  // Process the collected HLOs from computation root to dot.\n+  HloDotInstruction* dot_cast = DynCast<HloDotInstruction>(dot);\n+  PrimitiveType accumulator_dtype =\n+      dot_cast != nullptr ? GetGemmAccumulatorType(dot_cast) : F32;\n   while (!to_process.empty()) {\n     HloInstruction* current = to_process.top();\n     to_process.pop();\n     // Add split-K dimension to `current`.\n     HloInstruction* expanded;\n     if (current == dot) {\n-      TF_ASSIGN_OR_RETURN(\n-          HloInstruction * lhs,\n-          MakeSplitKOperand(*dot, analysis, config, lhs_contracting_idx, 0));\n-      TF_ASSIGN_OR_RETURN(\n-          HloInstruction * rhs,\n-          MakeSplitKOperand(*dot, analysis, config, rhs_contracting_idx, 1));\n-      if (lhs->operand(0)->opcode() == HloOpcode::kPad) {\n-        CHECK_EQ(rhs->operand(0)->opcode(), HloOpcode::kPad);\n-        did_pad = true;\n+      if (dot_cast != nullptr) {\n+        // Dot operation.\n+        TF_ASSIGN_OR_RETURN(\n+            HloInstruction * lhs,\n+            MakeSplitKOperand(*dot, analysis, config, lhs_contracting_idx,\n+                              TritonFusionAnalysis::Scope::LHS, 0));\n+        TF_ASSIGN_OR_RETURN(\n+            HloInstruction * rhs,\n+            MakeSplitKOperand(*dot, analysis, config, rhs_contracting_idx,\n+                              TritonFusionAnalysis::Scope::RHS, 1));\n+        did_pad = is_padded(lhs) || is_padded(rhs);\n+        // Keep the precision of the accumulator type for the dot output.\n+        TF_ASSIGN_OR_RETURN(\n+            expanded, MakeDotHlo(lhs, rhs, new_dim_numbers,\n+                                 dot->precision_config(), accumulator_dtype));\n+      } else {\n+        // Scaled dot operation.\n+        // At least one scaling operand is not a scalar and will define the\n+        // contracting dimension size.\n+        std::optional<int64_t> padded_k_size;\n+        auto assign_scale_operand =\n+            [&](TritonFusionAnalysis::Scope scope, int contracting_idx,\n+                int operand_idx,\n+                int64_t block_size) -> absl::StatusOr<HloInstruction*> {\n+          TF_ASSIGN_OR_RETURN(\n+              HloInstruction * scale,\n+              MakeSplitKOperand(*dot, analysis, config, contracting_idx, scope,\n+                                operand_idx));\n+          padded_k_size = scale->shape().dimensions(contracting_idx + 1) *\n+                          block_size * config.split_k;\n+          return scale;\n+        };\n+        HloInstruction* lhs_scale = dot->mutable_operand(1);\n+        if (analysis.lhs_scale_block_size().has_value()) {\n+          TF_ASSIGN_OR_RETURN(\n+              lhs_scale,\n+              assign_scale_operand(TritonFusionAnalysis::Scope::LHS_SCALE,\n+                                   lhs_contracting_idx, 1,\n+                                   *analysis.lhs_scale_block_size()));\n+        }\n+        HloInstruction* rhs_scale = dot->mutable_operand(3);\n+        if (analysis.rhs_scale_block_size().has_value()) {\n+          if (padded_k_size.has_value()) {\n+            int64_t rhs_block_size = analysis.rhs_scale_block_size().value();\n+            if (*padded_k_size % (rhs_block_size * config.split_k) != 0) {\n+              return UncompilableMatmul(\"Unable to split-K block scaled dot.\");\n+            }\n+            TF_ASSIGN_OR_RETURN(\n+                rhs_scale,\n+                MakeSplitKOperand(*dot, analysis, config, rhs_contracting_idx,\n+                                  TritonFusionAnalysis::Scope::RHS_SCALE, 3,\n+                                  *padded_k_size / rhs_block_size));\n+          } else {\n+            TF_ASSIGN_OR_RETURN(\n+                rhs_scale,\n+                assign_scale_operand(TritonFusionAnalysis::Scope::RHS_SCALE,\n+                                     rhs_contracting_idx, 3,\n+                                     *analysis.rhs_scale_block_size()));\n+          }\n+        }\n+        did_pad = is_padded(lhs_scale) || is_padded(rhs_scale);\n+        // Make LHS/RHS input operands with fixed contracting dimension size.\n+        TF_ASSIGN_OR_RETURN(\n+            HloInstruction * lhs,\n+            MakeSplitKOperand(*dot, analysis, config, lhs_contracting_idx,\n+                              TritonFusionAnalysis::Scope::LHS, 0,\n+                              padded_k_size));\n+        TF_ASSIGN_OR_RETURN(\n+            HloInstruction * rhs,\n+            MakeSplitKOperand(*dot, analysis, config, rhs_contracting_idx,\n+                              TritonFusionAnalysis::Scope::RHS, 2,\n+                              padded_k_size));\n+        TF_ASSIGN_OR_RETURN(\n+            expanded,\n+            MakeScaledDotHlo(lhs, lhs_scale, rhs, rhs_scale, new_dim_numbers,\n+                             dot->precision_config(), accumulator_dtype));\n       }\n-      // Keep the precision of the accumulator type for the dot output.\n-      expanded = MakeDotHlo(lhs, rhs, new_dim_numbers, dot->precision_config(),\n-                            accumulator_dtype)\n-                     .value();\n       // Make the added batch dimension the major-most, keep the order of the\n       // original dimensions.\n       expanded->mutable_shape()->mutable_layout()->clear_minor_to_major();\n@@ -366,8 +449,15 @@ absl::Status MakeDotSplitKBatch(HloInstruction* dot_fusion,\n   const PrimitiveType output_type = dot_fusion->shape().element_type();\n   const Layout output_layout = dot_fusion->shape().layout();\n \n-  TF_RETURN_IF_ERROR(MakeDotComputationSplitKBatch(\n-      dot_fusion->fused_instructions_computation(), config));\n+  auto status = MakeDotComputationSplitKBatch(\n+      dot_fusion->fused_instructions_computation(), config);\n+  if (!status.ok()) {\n+    TF_RETURN_IF_ERROR(\n+        HloDCE()\n+            .RunOnComputation(dot_fusion->fused_instructions_computation())\n+            .status());\n+    return status;\n+  }\n   const HloInstruction* root = dot_fusion->fused_expression_root();\n \n   *dot_fusion->mutable_shape() = root->shape();"
        },
        {
            "sha": "25f1d00afdcd05481c18e2d8aeadb86d25498ac6",
            "filename": "third_party/xla/xla/service/gpu/split_k_gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -792,6 +792,235 @@ ENTRY %entry_computation {\n               GmockMatch(m::Reduce(m::Fusion(), m::Constant())));\n }\n \n+TEST_F(SplitKTest, CleanupUncompilable) {\n+  // Test that the pass doesn't create dangling ops.\n+  const std::string hlo_text = R\"(\n+triton_gemm_dot {\n+  lhs = f32[16,128] parameter(0)\n+  rhs = f32[32,256] parameter(1)\n+  rhs_sliced = f32[32,128] slice(rhs), slice={[0:32],[0:128]}\n+  ROOT dot = f32[16,32] dot(lhs, rhs_sliced),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY %entry_computation {\n+  lhs = f32[16,128] parameter(0)\n+  rhs = f32[32,256] parameter(1)\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs), kind=kCustom,\n+    calls=triton_gemm_dot\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  TritonGemmConfig config(16, 16, 16, /*split_k=*/4, 1, 1);\n+  EXPECT_THAT(MakeDotSplitKBatch(\n+                  module->entry_computation()->root_instruction(), config),\n+              absl_testing::StatusIs(\n+                  tsl::error::CANCELLED,\n+                  \"Sliced contracting dimension is not supported yet.\"));\n+}\n+\n+TEST_F(SplitKTest, ScaledDot_SameBlockSize) {\n+  const std::string hlo_text = R\"(\n+triton_gemm_dot {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+  rhs = f8e5m2[32,128] parameter(2)\n+  rhs_scale = f8e8m0fnu[32,4] parameter(3)\n+  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY %entry_computation {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+  rhs = f8e5m2[32,128] parameter(2)\n+  rhs_scale = f8e8m0fnu[32,4] parameter(3)\n+  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+      kind=kCustom, calls=triton_gemm_dot\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  TritonGemmConfig config(16, 16, 16, /*split_k=*/3, 1, 1);\n+  TF_EXPECT_OK(MakeDotSplitKBatch(\n+      module->entry_computation()->root_instruction(), config));\n+\n+  HloInstruction* dot_fusion;\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n+  EXPECT_THAT(\n+      dot_fusion->called_computations()[0]->root_instruction(),\n+      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n+                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n+                              m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                              m::Op().WithShape(F8E8M0FNU, {32, 3, 2}))));\n+}\n+\n+TEST_F(SplitKTest, ScaledDot_DifferentBlockSize) {\n+  const std::string hlo_text = R\"(\n+triton_gemm_dot {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+  rhs = f8e5m2[32,128] parameter(2)\n+  rhs_scale = f8e8m0fnu[32,8] parameter(3)\n+  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY %entry_computation {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+  rhs = f8e5m2[32,128] parameter(2)\n+  rhs_scale = f8e8m0fnu[32,8] parameter(3)\n+  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+      kind=kCustom, calls=triton_gemm_dot\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  TritonGemmConfig config(16, 16, 16, /*split_k=*/3, 1, 1);\n+  TF_EXPECT_OK(MakeDotSplitKBatch(\n+      module->entry_computation()->root_instruction(), config));\n+\n+  HloInstruction* dot_fusion;\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n+  EXPECT_THAT(\n+      dot_fusion->called_computations()[0]->root_instruction(),\n+      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n+                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n+                              m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                              m::Op().WithShape(F8E8M0FNU, {32, 3, 4}))));\n+}\n+\n+TEST_F(SplitKTest, ScaledDot_LhsOnly) {\n+  const std::string hlo_text = R\"(\n+triton_gemm_dot {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+  rhs = f8e5m2[32,128] parameter(2)\n+  rhs_scale = f8e5m2[] constant(1.0)\n+  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY %entry_computation {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+  rhs = f8e5m2[32,128] parameter(2)\n+  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs),\n+      kind=kCustom, calls=triton_gemm_dot\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  TritonGemmConfig config(16, 16, 16, /*split_k=*/3, 1, 1);\n+  TF_EXPECT_OK(MakeDotSplitKBatch(\n+      module->entry_computation()->root_instruction(), config));\n+\n+  HloInstruction* dot_fusion;\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n+  EXPECT_THAT(dot_fusion->called_computations()[0]->root_instruction(),\n+              GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n+                                      m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n+                                      m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                                      m::Op().WithShape(F8E5M2, {}))));\n+}\n+\n+TEST_F(SplitKTest, ScaledDot_RhsOnly) {\n+  const std::string hlo_text = R\"(\n+triton_gemm_dot {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  lhs_scale = f8e4m3fn[] constant(1.0)\n+  rhs = f8e5m2[32,128] parameter(1)\n+  rhs_scale = f8e8m0fnu[32,4] parameter(2)\n+  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY %entry_computation {\n+  lhs = f8e4m3fn[16,128] parameter(0)\n+  rhs = f8e5m2[32,128] parameter(1)\n+  rhs_scale = f8e8m0fnu[32,4] parameter(2)\n+  ROOT fusion = f32[16,32] fusion(lhs, rhs, rhs_scale),\n+      kind=kCustom, calls=triton_gemm_dot\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  TritonGemmConfig config(16, 16, 16, /*split_k=*/3, 1, 1);\n+  TF_EXPECT_OK(MakeDotSplitKBatch(\n+      module->entry_computation()->root_instruction(), config));\n+\n+  HloInstruction* dot_fusion;\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n+  EXPECT_THAT(\n+      dot_fusion->called_computations()[0]->root_instruction(),\n+      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n+                              m::Op().WithShape(F8E4M3FN, {}),\n+                              m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+                              m::Op().WithShape(F8E8M0FNU, {32, 3, 2}))));\n+}\n+\n+TEST_F(SplitKTest, ScaledDot_IncompatibleBlockSize) {\n+  const std::string hlo_text = R\"(\n+triton_gemm_dot {\n+  lhs = f8e4m3fn[16,35] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,7] parameter(1)\n+  rhs = f8e5m2[32,35] parameter(2)\n+  rhs_scale = f8e8m0fnu[32,5] parameter(3)\n+  ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY %entry_computation {\n+  lhs = f8e4m3fn[16,35] parameter(0)\n+  lhs_scale = f8e8m0fnu[16,7] parameter(1)\n+  rhs = f8e5m2[32,35] parameter(2)\n+  rhs_scale = f8e8m0fnu[32,5] parameter(3)\n+  ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+      kind=kCustom, calls=triton_gemm_dot\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  TritonGemmConfig config(16, 16, 16, /*split_k=*/3, 1, 1);\n+  EXPECT_THAT(MakeDotSplitKBatch(\n+                  module->entry_computation()->root_instruction(), config),\n+              absl_testing::StatusIs(tsl::error::CANCELLED,\n+                                     \"Unable to split-K block scaled dot.\"));\n+}\n+\n+TEST_F(SplitKTest, ScaledDot_SmallDimension) {\n+  const std::string hlo_text = R\"(\n+    triton_gemm_dot {\n+      lhs = f8e4m3fn[16,128] parameter(0)\n+      lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+      rhs = f8e5m2[32,128] parameter(2)\n+      rhs_scale = f8e8m0fnu[32,4] parameter(3)\n+      ROOT dot = f32[16,32] scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+    }\n+\n+    ENTRY %entry_computation {\n+      lhs = f8e4m3fn[16,128] parameter(0)\n+      lhs_scale = f8e8m0fnu[16,4] parameter(1)\n+      rhs = f8e5m2[32,128] parameter(2)\n+      rhs_scale = f8e8m0fnu[32,4] parameter(3)\n+      ROOT fusion = f32[16,32] fusion(lhs, lhs_scale, rhs, rhs_scale),\n+          kind=kCustom, calls=triton_gemm_dot\n+    })\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  TritonGemmConfig config(16, 16, 16, /*split_k=*/4, 1, 1);\n+  EXPECT_THAT(MakeDotSplitKBatch(\n+                  module->entry_computation()->root_instruction(), config),\n+              absl_testing::StatusIs(\n+                  tsl::error::CANCELLED,\n+                  \"Too small divisible part of the contracting dimension.\"));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "370bea7039f96e1e49d46b74bac8e726b22a55a8",
            "filename": "third_party/xla/xla/service/gpu/triton_fusion_analysis.cc",
            "status": "modified",
            "additions": 49,
            "deletions": 2,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.cc?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -186,8 +186,44 @@ absl::Status FusionContext::PropagateDimensionOrdersToParameters(\n   return absl::OkStatus();\n }\n \n+absl::StatusOr<std::optional<int64_t>> GetBlockSize(\n+    const HloInstruction& dot, TritonFusionAnalysis::Scope scope) {\n+  CHECK(dot.opcode() == HloOpcode::kScaledDot);\n+  CHECK(scope == TritonFusionAnalysis::Scope::LHS ||\n+        scope == TritonFusionAnalysis::Scope::RHS);\n+  int operand_number = scope == TritonFusionAnalysis::Scope::LHS ? 0 : 2;\n+  const Shape& input = dot.operand(operand_number)->shape();\n+  const Shape& scale = dot.operand(operand_number + 1)->shape();\n+\n+  if (!ShapeUtil::IsScalar(scale)) {\n+    TF_ASSIGN_OR_RETURN(int dim_idx,\n+                        ContractingDimensionIndex(dot, operand_number));\n+    return input.dimensions(dim_idx) / scale.dimensions(dim_idx);\n+  }\n+  return std::nullopt;\n+}\n+\n }  // namespace triton_fusion\n \n+namespace {\n+\n+int ScopeToScaledDotOperandIdx(TritonFusionAnalysis::Scope scope) {\n+  switch (scope) {\n+    case TritonFusionAnalysis::Scope::LHS:\n+      return 0;\n+    case TritonFusionAnalysis::Scope::LHS_SCALE:\n+      return 1;\n+    case TritonFusionAnalysis::Scope::RHS:\n+      return 2;\n+    case TritonFusionAnalysis::Scope::RHS_SCALE:\n+      return 3;\n+    default:\n+      LOG(FATAL) << \"Unsupported scope.\";\n+  }\n+}\n+\n+}  // namespace\n+\n absl::StatusOr<TritonFusionAnalysis> TritonFusionAnalysis::Execute(\n     const HloComputation& computation, const int split_k) {\n   VLOG(5) << computation.ToString(HloPrintOptions::ShortParsable());\n@@ -238,6 +274,12 @@ bool TritonFusionAnalysis::IsBatchDimMinorForInt4Parameter(\n absl::Status TritonFusionAnalysis::ExecuteForDotFusion(\n     const HloInstruction& dot, const int split_k) {\n   is_scaled_dot_ = dot.opcode() == HloOpcode::kScaledDot;\n+  if (is_scaled_dot_) {\n+    TF_ASSIGN_OR_RETURN(lhs_scale_block_size_,\n+                        triton_fusion::GetBlockSize(dot, Scope::LHS));\n+    TF_ASSIGN_OR_RETURN(rhs_scale_block_size_,\n+                        triton_fusion::GetBlockSize(dot, Scope::RHS));\n+  }\n \n   DotRequirements lhs_requirements(kNoSplitRequirement);\n   for (const Scope scope :\n@@ -246,9 +288,14 @@ absl::Status TritonFusionAnalysis::ExecuteForDotFusion(\n     if (operand_number >= dot.operand_count()) {\n       continue;  // Scale operands are optional.\n     }\n-    if (is_scaled_dot_ && (operand_number == 1 || operand_number == 2)) {\n+    if (is_scaled_dot_) {\n       // Operands for scaled dot: (lhs, lhs_scale, rhs, rhs_scale)\n-      operand_number = 3 - operand_number;\n+      operand_number = ScopeToScaledDotOperandIdx(scope);\n+      // Scalar scales are skipped.\n+      if ((scope == Scope::LHS_SCALE || scope == Scope::RHS_SCALE) &&\n+          ShapeUtil::IsScalar(dot.operand(operand_number)->shape())) {\n+        continue;\n+      }\n     }\n     TF_ASSIGN_OR_RETURN(auto context, FusionContext::FromDotOperand(\n                                           dot, operand_number, split_k));"
        },
        {
            "sha": "dc6ded0d470b7b07710b0ff97fedfa72564b9b62",
            "filename": "third_party/xla/xla/service/gpu/triton_fusion_analysis.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis.h?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -97,14 +97,24 @@ class TritonFusionAnalysis {\n   bool IsBatchDimMinorForInt4Parameter(const HloInstruction& dot,\n                                        Scope scope) const;\n \n+  // Block scaled dot support.\n   bool is_scaled_dot() const { return is_scaled_dot_; }\n \n+  std::optional<int64_t> lhs_scale_block_size() const {\n+    return lhs_scale_block_size_;\n+  }\n+  std::optional<int64_t> rhs_scale_block_size() const {\n+    return rhs_scale_block_size_;\n+  }\n+\n  private:\n   IterationSpecByInstructionByScopeMap iter_specs_;\n   // HLO computation parameters per scope.\n   std::map<Scope, ConstHloInstructionSet> parameters_;\n   // Scaled dot has additional scale scopes.\n   bool is_scaled_dot_ = false;\n+  std::optional<int64_t> lhs_scale_block_size_;\n+  std::optional<int64_t> rhs_scale_block_size_;\n };\n \n // The details of the Triton fusion / tiling propagation are in a separate"
        },
        {
            "sha": "351f9aa9d498490afe0a035200f5a0e9ed62e39f",
            "filename": "third_party/xla/xla/service/hlo_creation_utils.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -406,6 +406,24 @@ absl::StatusOr<HloInstruction*> MakeRaggedDotHlo(\n       ragged_dot_shape, lhs, rhs, group_sizes, dim_numbers, precision_config));\n }\n \n+absl::StatusOr<HloInstruction*> MakeScaledDotHlo(\n+    HloInstruction* lhs, HloInstruction* lhs_scale, HloInstruction* rhs,\n+    HloInstruction* rhs_scale, const DotDimensionNumbers& dim_numbers,\n+    const PrecisionConfig& precision_config,\n+    std::optional<PrimitiveType> preferred_element_type) {\n+  HloComputation* computation = lhs->parent();\n+  CHECK_EQ(computation, lhs_scale->parent());\n+  CHECK_EQ(computation, rhs->parent());\n+  CHECK_EQ(computation, rhs_scale->parent());\n+  TF_ASSIGN_OR_RETURN(\n+      Shape dot_shape,\n+      ShapeInference::InferDotOpShape(lhs->shape(), rhs->shape(), dim_numbers,\n+                                      preferred_element_type));\n+  return computation->AddInstruction(\n+      HloInstruction::CreateScaledDot(dot_shape, lhs, lhs_scale, rhs, rhs_scale,\n+                                      dim_numbers, precision_config));\n+}\n+\n absl::StatusOr<HloInstruction*> MakeMapHlo(\n     absl::Span<HloInstruction* const> operands, HloComputation* map_computation,\n     const OpMetadata* metadata) {"
        },
        {
            "sha": "9acc4f3c7f985fa519ddb2f824c84df00c7952a2",
            "filename": "third_party/xla/xla/service/hlo_creation_utils.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.h?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -191,6 +191,16 @@ absl::StatusOr<HloInstruction*> MakeRaggedDotHlo(\n     const PrecisionConfig& precision_config,\n     std::optional<PrimitiveType> preferred_element_type);\n \n+// Creates a ScaledDot HLO instruction and adds it to the computation containing\n+// `lhs`, `lhs_scale`, `rhs`, and `rhs_scale` (all must be in the same\n+// computation). An optional preferred_element_type can be specified to override\n+// the element type.\n+absl::StatusOr<HloInstruction*> MakeScaledDotHlo(\n+    HloInstruction* lhs, HloInstruction* lhs_scale, HloInstruction* rhs,\n+    HloInstruction* rhs_scale, const DotDimensionNumbers& dim_numbers,\n+    const PrecisionConfig& precision_config,\n+    std::optional<PrimitiveType> preferred_element_type);\n+\n // Creates a Map HLO instruction and adds it to the computation containing the\n // operands. All operands must be in the same computation.\n absl::StatusOr<HloInstruction*> MakeMapHlo("
        },
        {
            "sha": "debfeb76c30c3dc52b4518a261b8c05d10d2fb24",
            "filename": "third_party/xla/xla/service/pattern_matcher.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fpattern_matcher.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f9160ff7fc4a7ed547df3bfbb9500a4d7a728783/third_party%2Fxla%2Fxla%2Fservice%2Fpattern_matcher.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fpattern_matcher.h?ref=f9160ff7fc4a7ed547df3bfbb9500a4d7a728783",
            "patch": "@@ -2930,6 +2930,7 @@ XLA_VARIADIC_OP_PATTERN(Map)\n XLA_VARIADIC_OP_PATTERN(Reduce);\n XLA_VARIADIC_OP_PATTERN(ReduceScatter)\n XLA_VARIADIC_OP_PATTERN(ReduceWindow)\n+XLA_VARIADIC_OP_PATTERN(ScaledDot);\n XLA_VARIADIC_OP_PATTERN(Scatter);\n XLA_VARIADIC_OP_PATTERN(Sort);\n XLA_VARIADIC_OP_PATTERN(Tuple);"
        }
    ],
    "stats": {
        "total": 465,
        "additions": 436,
        "deletions": 29
    }
}