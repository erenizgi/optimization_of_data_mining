{
    "author": "chsigg",
    "message": "[xla:gpu] Migrate Triton tests to use `__triton_nested_gemm_fusion`.\n\nThe legacy emitter code path is deprecated and will be removed.\n\nThis change updates various Triton-related tests to use the new `__triton_nested_gemm_fusion` kind, which involves structuring the HLO with nested fusions for the LHS and RHS of the dot operation. The backend configurations are updated to use `block_level_fusion_config` with `output_tiles` instead of the deprecated `triton_gemm_config`. Several previously disabled tests are re-enabled as part of this migration.\n\nPiperOrigin-RevId: 827986685",
    "sha": "ef7bca0bc510c501d2244dd03106a384b26560b8",
    "files": [
        {
            "sha": "63ee1fc0fd11dc2822800df6c60d17113ad35ee6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=ef7bca0bc510c501d2244dd03106a384b26560b8",
            "patch": "@@ -511,9 +511,9 @@ xla_cc_test(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:xla_internal_test_main\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//llvm:ir_headers\",\n@@ -996,6 +996,7 @@ xla_test(\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "97fe1b6d855d3687f00baf9babb8bbc7558c59d2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 10,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=ef7bca0bc510c501d2244dd03106a384b26560b8",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"mlir/IR/MLIRContext.h\"\n@@ -34,14 +35,12 @@ limitations under the License.\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n \n namespace xla::gpu {\n namespace {\n \n-using ::tsl::testing::IsOkAndHolds;\n using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n \n using TritonEmitterDevicelessTest = HloHardwareIndependentTestBase;\n@@ -72,10 +71,25 @@ TEST_F(AnnotationsTest, Annotations) {\n   static constexpr absl::string_view kHloText = R\"(\n HloModule Annotations\n \n+triton_dot_lhs {\n+  p0 = f32[8,8] parameter(0)\n+  ROOT copy = f32[8,8] copy(p0)\n+}\n+triton_dot_rhs {\n+  p1 = f32[8,8] parameter(0)\n+  ROOT copy = f32[8,8] copy(p1)\n+}\n+\n triton_dot {\n   p0 = f32[8,8] parameter(0)\n   p1 = f32[8,8] parameter(1)\n-  ROOT dot = f32[8,8] dot(p0, p1),\n+  a = f32[8,8] fusion(p0), kind=kCustom, calls=triton_dot_lhs,\n+    backend_config={\"fusion_backend_config\": {kind: \"__triton_nested_gemm_fusion\",\n+    block_level_fusion_config: {output_tiles:[{sizes:[\"8\",\"8\"]}]}}}\n+  b = f32[8,8] fusion(p1), kind=kCustom, calls=triton_dot_rhs,\n+    backend_config={\"fusion_backend_config\": {kind: \"__triton_nested_gemm_fusion\",\n+    block_level_fusion_config: {output_tiles:[{sizes:[\"8\",\"8\"]}]}}}\n+  ROOT dot = f32[8,8] dot(a, b),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     algorithm=dot_bf16_bf16_f32_x3\n }\n@@ -84,13 +98,10 @@ ENTRY e {\n   p0 = f32[8,8]{1, 0} parameter(0)\n   p1 = f32[8,8]{1, 0} parameter(1)\n   ROOT _ = f32[8,8] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n-    backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n-      triton_gemm_config:\n+    backend_config={\"fusion_backend_config\": {kind: \"__triton_nested_gemm_fusion\",\n+      block_level_fusion_config:\n       {\n-        \"block_m\":32,\n-        \"block_n\":32,\n-        \"block_k\":32,\n-        \"split_k\":1,\n+        \"output_tiles\":[{\"sizes\":[\"8\",\"8\"]}],\n         \"num_stages\":1,\n         \"num_warps\":1,\n         \"num_ctas\":1\n@@ -109,7 +120,11 @@ ENTRY e {\n       auto triton_module,\n       CreateTritonModule(\"triton_fn\", fusion,\n                          TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         BlockLevelParameters(), symbolic_expr_context));\n+                         BlockLevelParameters::FromBlockLevelFusionConfig(\n+                             fusion->backend_config<GpuBackendConfig>()\n+                                 ->fusion_backend_config()\n+                                 .block_level_fusion_config()),\n+                         symbolic_expr_context));\n \n   std::string annotated_ir = DumpTritonIR(triton_module.get(), true);\n "
        },
        {
            "sha": "52d45cdabf0e9c0dc18b4f47ffdcaaf7625f6740",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_parametrized_test.cc",
            "status": "modified",
            "additions": 102,
            "deletions": 52,
            "changes": 154,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_test.cc?ref=ef7bca0bc510c501d2244dd03106a384b26560b8",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -185,19 +186,31 @@ std::string ElementwiseTestParamsToString(\n \n using UnaryElementwiseTest = ElementwiseTest;\n \n-TEST_P(UnaryElementwiseTest, DISABLED_ElementwiseFusionExecutesCorrectly) {\n+TEST_P(UnaryElementwiseTest, ElementwiseFusionExecutesCorrectly) {\n   PrimitiveType data_type;\n   HloOpcode opcode;\n   float tolerance;\n   std::tie(data_type, opcode, tolerance) = GetParam();\n \n   const std::string kHloTestTemplate = R\"(\n+lhs_computation {\n+  ROOT parameter_0 = f32[15,33]{1,0} parameter(0)\n+}\n+\n+rhs_computation {\n+  parameter_1 = $0[33,68]{1,0} parameter(0)\n+  f1.1 = $0[33,68]{1,0} $1(parameter_1)\n+  ROOT c.1 = f32[33,68]{1,0} convert(f1.1)\n+}\n+\n triton_gemm___computation {\n   parameter_0 = f32[15,33]{1,0} parameter(0)\n   parameter_1 = $0[33,68]{1,0} parameter(1)\n-  f1.1 = $0[33,68]{1,0} $1(parameter_1)\n-  c.1 = f32[33,68]{1,0} convert(f1.1)\n-  ROOT _.1 = f32[15,68]{1,0} dot(parameter_0, c.1),\n+  lhs = f32[15,33]{1,0} fusion(parameter_0), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+  rhs = f32[33,68]{1,0} fusion(parameter_1), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+  ROOT _.1 = f32[15,68]{1,0} dot(lhs, rhs),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     operand_precision={HIGH, HIGH}\n }\n@@ -207,12 +220,9 @@ ENTRY e {\n   p0 = f32[15,33]{1,0} parameter(0)\n   ROOT triton_gemm__ = f32[15,68]{1,0} fusion(p0, p1), kind=kCustom,\n     calls=triton_gemm___computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n-                    \"triton_gemm_config\":\n-                      {\"block_m\":\"32\",\n-                       \"block_n\":\"32\",\n-                       \"block_k\":\"32\",\n-                       \"split_k\":\"1\",\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+                    \"block_level_fusion_config\":\n+                      {\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}],\n                        \"num_stages\":\"1\",\n                        \"num_warps\":\"4\",\n                        \"num_ctas\":\"1\"}}}\n@@ -250,7 +260,7 @@ ENTRY e {\n       /*run_hlo_passes=*/false));\n }\n \n-TEST_P(UnaryElementwiseTest, DISABLED_ElementwiseUnaryOpExecutesCorrectly) {\n+TEST_P(UnaryElementwiseTest, ElementwiseUnaryOpExecutesCorrectly) {\n   PrimitiveType data_type;\n   HloOpcode opcode;\n   float tolerance;\n@@ -361,20 +371,33 @@ INSTANTIATE_TEST_SUITE_P(\n \n using BinaryElementwiseTest = ElementwiseTest;\n \n-TEST_P(BinaryElementwiseTest, DISABLED_ElementwiseFusionExecutesCorrectly) {\n+TEST_P(BinaryElementwiseTest, ElementwiseFusionExecutesCorrectly) {\n   PrimitiveType data_type;\n   HloOpcode opcode;\n   float tolerance;\n   std::tie(data_type, opcode, tolerance) = GetParam();\n \n   const std::string kHloTestTemplate = R\"(\n+lhs_computation {\n+  ROOT parameter_0 = f32[92,11]{1,0} parameter(0)\n+}\n+\n+rhs_computation {\n+  parameter_1 = $0[11,63]{1,0} parameter(0)\n+  parameter_2 = $0[11,63]{1,0} parameter(1)\n+  f1.1 = $0[11,63]{1,0} $1(parameter_1, parameter_2)\n+  ROOT c.1 = f32[11,63]{1,0} convert(f1.1)\n+}\n+\n triton_gemm___computation {\n   parameter_0 = f32[92,11]{1,0} parameter(0)\n   parameter_1 = $0[11,63]{1,0} parameter(1)\n   parameter_2 = $0[11,63]{1,0} parameter(2)\n-  f1.1 = $0[11,63]{1,0} $1(parameter_1, parameter_2)\n-  c.1 = f32[11,63]{1,0} convert(f1.1)\n-  ROOT _.1 = f32[92,63]{1,0} dot(parameter_0, c.1),\n+  lhs = f32[92,11]{1,0} fusion(parameter_0), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\",\"32\"]}]}}}\n+  rhs = f32[11,63]{1,0} fusion(parameter_1, parameter_2), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\",\"32\"]}]}}}\n+  ROOT _.1 = f32[92,63]{1,0} dot(lhs, rhs),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     operand_precision={HIGH, HIGH}\n }\n@@ -385,12 +408,9 @@ ENTRY e {\n   p2 = $0[11,63]{1,0} parameter(2)\n   ROOT triton_gemm__ = f32[92,63]{1,0} fusion(p0, p1, p2), kind=kCustom,\n     calls=triton_gemm___computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n-                    \"triton_gemm_config\":\n-                      {\"block_m\":\"64\",\n-                       \"block_n\":\"32\",\n-                       \"block_k\":\"64\",\n-                       \"split_k\":\"1\",\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+                    \"block_level_fusion_config\":\n+                      {\"output_tiles\":[{\"sizes\":[\"64\",\"32\"]}],\n                        \"num_stages\":\"2\",\n                        \"num_warps\":\"2\",\n                        \"num_ctas\":\"1\"}}}\n@@ -430,7 +450,7 @@ ENTRY e {\n       /*run_hlo_passes=*/false, /*args_max_bits_of_precision=*/6));\n }\n \n-TEST_P(BinaryElementwiseTest, DISABLED_ElementwiseBinaryOpExecutesCorrectly) {\n+TEST_P(BinaryElementwiseTest, ElementwiseBinaryOpExecutesCorrectly) {\n   PrimitiveType data_type;\n   HloOpcode opcode;\n   float tolerance;\n@@ -611,7 +631,7 @@ class SelectTest : public TritonTest,\n                    public ::testing::WithParamInterface<\n                        std::tuple<PrimitiveType, PrimitiveType>> {};\n \n-TEST_P(SelectTest, DISABLED_SelectFusionExecutesCorrectly) {\n+TEST_P(SelectTest, SelectFusionExecutesCorrectly) {\n   PrimitiveType data_type1, data_type2;\n   std::tie(data_type1, data_type2) = GetParam();\n   for (const PrimitiveType type : {data_type1, data_type2}) {\n@@ -624,14 +644,28 @@ TEST_P(SelectTest, DISABLED_SelectFusionExecutesCorrectly) {\n   }\n \n   const std::string kHloTestTemplate = R\"(\n+lhs_computation {\n+  ROOT parameter_0 = $1[92,13]{1,0} parameter(0)\n+}\n+\n+rhs_computation {\n+  parameter_1 = $0[13,63]{1,0} parameter(0)\n+  parameter_2 = $0[13,63]{1,0} parameter(1)\n+  parameter_3 = pred[13,63]{1,0} parameter(2)\n+  f1.1 = $0[13,63]{1,0} select(parameter_3, parameter_1, parameter_2)\n+  ROOT c.1 = $1[13,63]{1,0} convert(f1.1)\n+}\n+\n triton_gemm___computation {\n   parameter_0 = $1[92,13]{1,0} parameter(0)\n   parameter_1 = $0[13,63]{1,0} parameter(1)\n   parameter_2 = $0[13,63]{1,0} parameter(2)\n   parameter_3 = pred[13,63]{1,0} parameter(3)\n-  f1.1 = $0[13,63]{1,0} select(parameter_3, parameter_1, parameter_2)\n-  c.1 = $1[13,63]{1,0} convert(f1.1)\n-  ROOT _.1 = $1[92,63]{1,0} dot(parameter_0, c.1),\n+  lhs = $1[92,13]{1,0} fusion(parameter_0), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"64\"]}]}}}\n+  rhs = $1[13,63]{1,0} fusion(parameter_1, parameter_2, parameter_3), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"64\"]}]}}}\n+  ROOT _.1 = $1[92,63]{1,0} dot(lhs, rhs),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     operand_precision={HIGH, HIGH}\n }\n@@ -644,12 +678,9 @@ ENTRY e {\n   ROOT triton_gemm__ = $1[92,63]{1,0} fusion(p0, p1, p2, p3), kind=kCustom,\n     calls=triton_gemm___computation, backend_config={\n       \"fusion_backend_config\":{\n-        \"kind\":\"__triton_gemm\",\n-        \"triton_gemm_config\": {\n-          \"block_m\":\"16\",\n-          \"block_n\":\"64\",\n-          \"block_k\":\"16\",\n-          \"split_k\":\"1\",\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\": {\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"64\"]}],\n           \"num_stages\":\"3\",\n           \"num_warps\":\"2\",\n           \"num_ctas\":\"1\"}}}\n@@ -716,7 +747,7 @@ INSTANTIATE_TEST_SUITE_P(\n class ConstantTest : public TritonTest,\n                      public ::testing::WithParamInterface<PrimitiveType> {};\n \n-TEST_P(ConstantTest, DISABLED_ConstantFusionExecutesCorrectly) {\n+TEST_P(ConstantTest, ConstantFusionExecutesCorrectly) {\n   const PrimitiveType data_type = GetParam();\n   if (!legacy_triton::IsTritonSupportedDataType(data_type,\n                                                 GetCudaComputeCapability())) {\n@@ -726,14 +757,26 @@ TEST_P(ConstantTest, DISABLED_ConstantFusionExecutesCorrectly) {\n   }\n \n   const std::string kHloTestTemplate = R\"(\n-triton_gemm___computation {\n-  parameter_0 = f32[92,11]{1,0} parameter(0)\n-  parameter_1 = f32[11,63]{1,0} parameter(1)\n+lhs_computation {\n+  ROOT parameter_0 = f32[92,11]{1,0} parameter(0)\n+}\n+\n+rhs_computation {\n+  parameter_1 = f32[11,63]{1,0} parameter(0)\n   c = $0[] constant(123)\n   b = $0[11,63] broadcast(c)\n   cv = f32[11,63] convert(b)\n-  m = f32[11,63] multiply(cv, parameter_1)\n-  ROOT _.1 = f32[92,63]{1,0} dot(parameter_0, m),\n+  ROOT m = f32[11,63] multiply(cv, parameter_1)\n+}\n+\n+triton_gemm___computation {\n+  parameter_0 = f32[92,11]{1,0} parameter(0)\n+  parameter_1 = f32[11,63]{1,0} parameter(1)\n+  lhs = f32[92,11]{1,0} fusion(parameter_0), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"64\"]}]}}}\n+  rhs = f32[11,63]{1,0} fusion(parameter_1), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"64\"]}]}}}\n+  ROOT _.1 = f32[92,63]{1,0} dot(lhs, rhs),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     operand_precision={HIGH, HIGH}\n }\n@@ -744,12 +787,9 @@ ENTRY e {\n   ROOT triton_gemm__ = f32[92,63]{1,0} fusion(p0, p1), kind=kCustom,\n     calls=triton_gemm___computation, backend_config={\n       \"fusion_backend_config\":{\n-        \"kind\":\"__triton_gemm\",\n-        \"triton_gemm_config\":{\n-          \"block_m\":\"16\",\n-          \"block_n\":\"64\",\n-          \"block_k\":\"16\",\n-          \"split_k\":\"1\",\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"64\"]}],\n           \"num_stages\":\"3\",\n           \"num_warps\":\"2\",\n           \"num_ctas\":\"1\"}}}\n@@ -818,7 +858,7 @@ class ConvertTest : public TritonTest,\n                     public ::testing::WithParamInterface<\n                         std::tuple<PrimitiveType, PrimitiveType>> {};\n \n-TEST_P(ConvertTest, DISABLED_ConvertFusionExecutesCorrectly) {\n+TEST_P(ConvertTest, ConvertFusionExecutesCorrectly) {\n   PrimitiveType data_type1, data_type2;\n   std::tie(data_type1, data_type2) = GetParam();\n   for (const PrimitiveType type : {data_type1, data_type2}) {\n@@ -832,12 +872,24 @@ TEST_P(ConvertTest, DISABLED_ConvertFusionExecutesCorrectly) {\n \n   const std::string hlo_text = absl::Substitute(\n       R\"(\n-t {\n+lhs_computation {\n   p0 = $0[2,2] parameter(0)\n   p0c = $1[2,2] convert(p0)\n-  p0cc = f32[2,2] convert(p0c)\n+  ROOT p0cc = f32[2,2] convert(p0c)\n+}\n+\n+rhs_computation {\n+  ROOT p1 = f32[2,2] parameter(0)\n+}\n+\n+t {\n+  p0 = $0[2,2] parameter(0)\n   p1 = f32[2,2] parameter(1)\n-  ROOT r = f32[2,2] dot(p0cc, p1),\n+  lhs = f32[2,2] fusion(p0), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\"}}\n+  rhs = f32[2,2] fusion(p1), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\"}}\n+  ROOT r = f32[2,2] dot(lhs, rhs),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0},\n     operand_precision={HIGH, HIGH}\n }\n@@ -846,14 +898,12 @@ ENTRY e {\n   p0 = $0[2,2] parameter(0)\n   p1 = f32[2,2] parameter(1)\n   ROOT r = f32[2,2] fusion(p0, p1), kind=kCustom, calls=t,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\"}}\n })\",\n       primitive_util::LowercasePrimitiveTypeName(data_type1),\n       primitive_util::LowercasePrimitiveTypeName(data_type2));\n \n-  MatchOptimizedHlo(hlo_text, R\"(\n-CHECK: block_m\n-  )\");\n+  TF_ASSERT_OK(GetOptimizedModule(hlo_text).status());\n }\n \n INSTANTIATE_TEST_SUITE_P("
        },
        {
            "sha": "1a14c91c517da88c40316a1d4ed6c9f3ac54cd1a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 72,
            "changes": 136,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=ef7bca0bc510c501d2244dd03106a384b26560b8",
            "patch": "@@ -628,20 +628,32 @@ TEST_F(GemmFusionAutotunerTest, DoNotRunAutotuningKernelSpillingRegisters) {\n   const std::string kHloText = R\"(\n HloModule m\n \n+lhs_computation {\n+  %p0 = s8[12288,1536] parameter(0)\n+  ROOT %convert = f16[12288,1536] convert(%p0)\n+}\n+\n+rhs_computation {\n+  %p1 = s8[4,12288] parameter(0)\n+  ROOT %convert = f16[4,12288] convert(%p1)\n+}\n+\n %triton_gemm_dot {\n-  %p1 = s8[4,12288]{1,0} parameter(1)\n-  %p0 = s8[12288,1536]{1,0} parameter(0)\n-  %convert.p0 = f16[12288,1536]{1,0} convert(s8[12288,1536]{1,0} %p0)\n-  %convert.p1 = f16[4,12288]{1,0} convert(s8[4,12288]{1,0} %p1)\n-  %dot = f16[4,1536]{1,0} dot(f16[4,12288]{1,0} %convert.p1, f16[12288,1536]{1,0} %convert.p0), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-  ROOT %convert = s8[4,1536]{1,0} convert(f16[4,1536]{1,0} %dot)\n+  %p0 = s8[12288,1536] parameter(0)\n+  %p1 = s8[4,12288] parameter(1)\n+  %lhs = f16[12288,1536] fusion(%p0), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"256\",\"16\"]}]}}}\n+  %rhs = f16[4,12288] fusion(%p1), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"256\"]}]}}}\n+  %dot = f16[4,1536] dot(%rhs, %lhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  ROOT %convert = s8[4,1536] convert(%dot)\n }\n \n ENTRY %e {\n-  %get-tuple-element.7020 = s8[12288,1536]{1,0} parameter(0)\n-  %convert = s8[4,12288]{1,0} parameter(1)\n-  ROOT %triton = s8[4,1536]{1,0} fusion(s8[12288,1536]{1,0} %get-tuple-element.7020, s8[4,12288]{1,0} %convert), kind=kCustom, calls=%triton_gemm_dot,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"256\",\"block_n\":\"256\",\"block_k\":\"16\",\"split_k\":\"1\",\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\"}}}\n+  %p0 = s8[12288,1536] parameter(0)\n+  %convert = s8[4,12288] parameter(1)\n+  ROOT %triton = s8[4,1536] fusion(%p0, %convert), kind=kCustom, calls=%triton_gemm_dot,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"256\",\"256\"]}],\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\"}}}\n })\";\n \n   auto module = ParseAndReturnVerifiedModule(kHloText).value();\n@@ -674,20 +686,32 @@ TEST_F(GemmFusionAutotunerTest,\n   const std::string kHloText = R\"(\n HloModule m\n \n+rhs_computation {\n+  %p0 = s8[12288,1536] parameter(0)\n+  ROOT %convert = f16[12288,1536] convert(%p0)\n+}\n+\n+lhs_computation {\n+  %p1 = s8[4,12288] parameter(0)\n+  ROOT %convert = f16[4,12288] convert(%p1)\n+}\n+\n %triton_gemm_dot {\n-  %p1 = s8[4,12288]{1,0} parameter(1)\n-  %p0 = s8[12288,1536]{1,0} parameter(0)\n-  %convert.p0 = f16[12288,1536]{1,0} convert(s8[12288,1536]{1,0} %p0)\n-  %convert.p1 = f16[4,12288]{1,0} convert(s8[4,12288]{1,0} %p1)\n-  %dot = f16[4,1536]{1,0} dot(f16[4,12288]{1,0} %convert.p1, f16[12288,1536]{1,0} %convert.p0), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-  ROOT %convert = s8[4,1536]{1,0} convert(f16[4,1536]{1,0} %dot)\n+  %p0 = s8[12288,1536] parameter(0)\n+  %p1 = s8[4,12288] parameter(1)\n+  %rhs = f16[12288,1536] fusion(%p0), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"256\",\"16\"]}]}}}\n+  %lhs = f16[4,12288] fusion(%p1), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"256\"]}]}}}\n+  %dot = f16[4,1536] dot(%lhs, %rhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  ROOT %convert = s8[4,1536] convert(%dot)\n }\n \n ENTRY %e {\n-  %get-tuple-element.7020 = s8[12288,1536]{1,0} parameter(0)\n-  %convert = s8[4,12288]{1,0} parameter(1)\n-  ROOT %triton = s8[4,1536]{1,0} fusion(s8[12288,1536]{1,0} %get-tuple-element.7020, s8[4,12288]{1,0} %convert), kind=kCustom, calls=%triton_gemm_dot,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"256\",\"block_n\":\"256\",\"block_k\":\"16\",\"split_k\":\"1\",\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\"}}}\n+  %p0 = s8[12288,1536] parameter(0)\n+  %p1 = s8[4,12288] parameter(1)\n+  ROOT %triton = s8[4,1536] fusion(%p0, %p1), kind=kCustom, calls=%triton_gemm_dot,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"256\",\"256\"]}],\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\"}}}\n })\";\n \n   auto module = ParseAndReturnVerifiedModule(kHloText).value();\n@@ -714,18 +738,30 @@ TEST_F(GemmFusionAutotunerTest, RunAutotuningKernelNotSpillingRegisters) {\n   const std::string kHloText = R\"(\n HloModule m\n \n+rhs_computation {\n+  %p0 = s8[12288,1536] parameter(0)\n+  ROOT %convert = f16[12288,1536] convert(%p0)\n+}\n+\n+lhs_computation {\n+  ROOT %p1 = f16[4,12288] parameter(0)\n+}\n+\n %triton_gemm_dot {\n-  %p1 = f16[4,12288]{1,0} parameter(1)\n-  %p0 = s8[12288,1536]{1,0} parameter(0)\n-  %convert.10406 = f16[12288,1536]{1,0} convert(s8[12288,1536]{1,0} %p0)\n-  ROOT %dot = f16[4,1536]{1,0} dot(f16[4,12288]{1,0} %p1, f16[12288,1536]{1,0} %convert.10406), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  %p0 = s8[12288,1536] parameter(0)\n+  %p1 = f16[4,12288] parameter(1)\n+  %rhs = f16[12288,1536] fusion(%p0), kind=kCustom, calls=rhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"16\"]}]}}}\n+  %lhs = f16[4,12288] fusion(%p1), kind=kCustom, calls=lhs_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}]}}}\n+  ROOT %dot = f16[4,1536] dot(%lhs, %rhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n ENTRY %e {\n-  %p0 = s8[12288,1536]{1,0} parameter(0)\n-  %p1 = f16[4,12288]{1,0} parameter(1)\n-  ROOT %triton_dot = f16[4,1536]{1,0} fusion(s8[12288,1536]{1,0} %p0, f16[4,12288]{1,0} %p1), kind=kCustom, calls=%triton_gemm_dot,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"16\",\"block_n\":\"32\",\"block_k\":\"16\",\"split_k\":\"1\",\"num_stages\":\"1\",\"num_warps\":\"2\",\"num_ctas\":\"1\"}}}\n+  %p0 = s8[12288,1536] parameter(0)\n+  %p1 = f16[4,12288] parameter(1)\n+  ROOT %triton_dot = f16[4,1536] fusion(%p0, %p1), kind=kCustom, calls=%triton_gemm_dot,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}],\"num_stages\":\"1\",\"num_warps\":\"2\",\"num_ctas\":\"1\"}}}\n })\";\n \n   auto module = ParseAndReturnVerifiedModule(kHloText).value();\n@@ -917,46 +953,6 @@ ENTRY e {\n // TODO(b/281489442): Write a testcase called\n // `SkipConfigsProducingDeviantResults` or similar.\n \n-// TODO(b/393299275): remove when the legacy GEMM emitter is removed.\n-class GemmFusionAutotunerLevelLegacyEmitterTest\n-    : public StatelessAutotunerTest,\n-      public ::testing::WithParamInterface<int> {\n- public:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options =\n-        StatelessAutotunerTest::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_autotune_level(GetParam());\n-    debug_options.set_xla_gpu_cublas_fallback(false);\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-    return debug_options;\n-  }\n-};\n-\n-TEST_P(GemmFusionAutotunerLevelLegacyEmitterTest,\n-       AllAutotuningLevelsWorkCorrectly) {\n-  const std::string kHloText = R\"(\n-HloModule m\n-\n-ENTRY e {\n-  p0 = pred[64,10] parameter(0)\n-  p0c = f32[64,10] convert(p0)\n-  p1 = f32[10,128] parameter(1)\n-  ROOT r = f32[64,128] dot(p0c, p1),\n-    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-})\";\n-\n-  MatchOptimizedHlo(kHloText, R\"(\n-; CHECK: kind=kCustom\n-; CHECK-SAME: __triton_gemm\n-      )\");\n-\n-  EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(GemmFusionAutotunerLevelSweep,\n-                         GemmFusionAutotunerLevelLegacyEmitterTest,\n-                         ::testing::Range(0, 5));\n-\n class GemmFusionAutotunerLevelTest : public StatelessAutotunerTest,\n                                      public ::testing::WithParamInterface<int> {\n  public:\n@@ -965,10 +961,6 @@ class GemmFusionAutotunerLevelTest : public StatelessAutotunerTest,\n         StatelessAutotunerTest::GetDebugOptionsForTest();\n     debug_options.set_xla_gpu_autotune_level(GetParam());\n     debug_options.set_xla_gpu_cublas_fallback(false);\n-    // TODO(b/393299275): remove when the flag is enabled by default.\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n-    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n     return debug_options;\n   }\n };"
        },
        {
            "sha": "26fa9db336994b8ad843551b882e66c3f6b09755",
            "filename": "third_party/xla/xla/service/gpu/tests/kernel_reuse.hlo",
            "status": "modified",
            "additions": 26,
            "deletions": 6,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fkernel_reuse.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef7bca0bc510c501d2244dd03106a384b26560b8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fkernel_reuse.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fkernel_reuse.hlo?ref=ef7bca0bc510c501d2244dd03106a384b26560b8",
            "patch": "@@ -55,27 +55,47 @@ ENTRY main {\n \n HloModule t, is_scheduled=true\n \n+c0 {\n+  p0 = f16[15,19]{1,0} parameter(0)\n+  ROOT r = f16[15,19]{1,0} copy(p0)\n+}\n+c1 {\n+  p0 = s8[19,17]{1,0} parameter(0)\n+  c = f16[19,17]{1,0} convert(p0)\n+  ROOT r = f16[19,17]{1,0} copy(c)\n+}\n triton_gemm_dot0 {\n   parameter_1 = f16[15,19]{1,0} parameter(1)\n   parameter_0 = s8[19,17]{1,0} parameter(0)\n-  cp1.1 = f16[19,17]{1,0} convert(parameter_0)\n-  ROOT dot0.1 = f16[15,17]{1,0} dot(parameter_1, cp1.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  a = f16[15,19] fusion(parameter_1), kind=kCustom, calls=c0, backend_config=\"{\\\"fusion_backend_config\\\": {kind: \\\"__triton_nested_gemm_fusion\\\", block_level_fusion_config: {\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"16\\\",\\\"16\\\"]}]}}}\"\n+  b = f16[19,17] fusion(parameter_0), kind=kCustom, calls=c1, backend_config=\"{\\\"fusion_backend_config\\\": {kind: \\\"__triton_nested_gemm_fusion\\\", block_level_fusion_config: {\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"16\\\",\\\"16\\\"]}]}}}\"\n+  ROOT dot0.1 = f16[15,17]{1,0} dot(a, b), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n+c2 {\n+  p0 = f16[15,19]{1,0} parameter(0)\n+  ROOT r = f16[15,19]{1,0} copy(p0)\n+}\n+c3 {\n+  p0 = s8[19,17]{1,0} parameter(0)\n+  c = f16[19,17]{1,0} convert(p0)\n+  ROOT r = f16[19,17]{1,0} copy(c)\n+}\n triton_gemm_dot1 {\n   parameter_1.1 = f16[15,19]{1,0} parameter(1)\n   parameter_0.1 = s8[19,17]{1,0} parameter(0)\n-  cp3.1 = f16[19,17]{1,0} convert(parameter_0.1)\n-  ROOT dot1.1 = f16[15,17]{1,0} dot(parameter_1.1, cp3.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  a = f16[15,19] fusion(parameter_1.1), kind=kCustom, calls=c2, backend_config=\"{\\\"fusion_backend_config\\\": {kind: \\\"__triton_nested_gemm_fusion\\\", block_level_fusion_config: {\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"16\\\",\\\"16\\\"]}]}}}\"\n+  b = f16[19,17] fusion(parameter_0.1), kind=kCustom, calls=c3, backend_config=\"{\\\"fusion_backend_config\\\": {kind: \\\"__triton_nested_gemm_fusion\\\", block_level_fusion_config: {\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"16\\\",\\\"16\\\"]}]}}}\"\n+  ROOT dot1.1 = f16[15,17]{1,0} dot(a, b), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n ENTRY e {\n   p3 = s8[19,17]{1,0} parameter(3)\n   p2 = f16[15,19]{1,0} parameter(2)\n   p1 = s8[19,17]{1,0} parameter(1)\n   p0 = f16[15,19]{1,0} parameter(0)\n-  triton_gemm_dot1 = f16[15,17]{1,0} fusion(p3, p2), kind=kCustom, calls=triton_gemm_dot1, backend_config=\"{ \\\"fusion_backend_config\\\": {kind: \\\"__triton_gemm\\\", triton_gemm_config: {\\\"block_m\\\":\\\"64\\\",\\\"block_n\\\":\\\"32\\\",\\\"block_k\\\":\\\"64\\\",\\\"split_k\\\":\\\"1\\\",\\\"num_stages\\\":\\\"4\\\",\\\"num_warps\\\":\\\"4\\\",\\\"num_ctas\\\":\\\"1\\\"}}}\"\n-  triton_gemm_dot0 = f16[15,17]{1,0} fusion(p1, p0), kind=kCustom, calls=triton_gemm_dot0, backend_config=\"{ \\\"fusion_backend_config\\\": {kind: \\\"__triton_gemm\\\", triton_gemm_config: {\\\"block_m\\\":\\\"64\\\",\\\"block_n\\\":\\\"32\\\",\\\"block_k\\\":\\\"64\\\",\\\"split_k\\\":\\\"1\\\",\\\"num_stages\\\":\\\"4\\\",\\\"num_warps\\\":\\\"4\\\",\\\"num_ctas\\\":\\\"1\\\"}}}\"\n+  triton_gemm_dot1 = f16[15,17]{1,0} fusion(p3, p2), kind=kCustom, calls=triton_gemm_dot1, backend_config=\"{\\\"fusion_backend_config\\\": {kind: \\\"__triton_nested_gemm_fusion\\\", block_level_fusion_config: {\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"16\\\",\\\"16\\\"]}], \\\"num_stages\\\":\\\"4\\\", \\\"num_warps\\\":\\\"4\\\", \\\"num_ctas\\\":\\\"1\\\"}}}\"\n+  triton_gemm_dot0 = f16[15,17]{1,0} fusion(p1, p0), kind=kCustom, calls=triton_gemm_dot0, backend_config=\"{\\\"fusion_backend_config\\\": {kind: \\\"__triton_nested_gemm_fusion\\\", block_level_fusion_config: {\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"16\\\",\\\"16\\\"]}], \\\"num_stages\\\":\\\"4\\\", \\\"num_warps\\\":\\\"4\\\", \\\"num_ctas\\\":\\\"1\\\"}}}\"\n   ROOT tuple = (f16[15,17]{1,0}, f16[15,17]{1,0}) tuple(triton_gemm_dot0, triton_gemm_dot1)\n }\n "
        }
    ],
    "stats": {
        "total": 360,
        "additions": 219,
        "deletions": 141
    }
}