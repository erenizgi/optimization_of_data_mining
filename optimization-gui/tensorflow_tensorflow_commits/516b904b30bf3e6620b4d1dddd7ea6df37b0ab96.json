{
    "author": "chsigg",
    "message": "Migrate Triton GEMM tests to use nested fusion structure.\n\nThe legacy emitter will be deleted soon. Change tests to use the new generic emitter instead.\n\nUpdates various HLO test cases from using the `__triton_gemm` backend config to the new `__triton_nested_gemm_fusion` structure, which involves defining separate nested fusions for the LHS and RHS of the dot operation. Also adjusts legacy test configurations.\n\nPiperOrigin-RevId: 830888333",
    "sha": "516b904b30bf3e6620b4d1dddd7ea6df37b0ab96",
    "files": [
        {
            "sha": "4fc3561941ee56833440418c14c9ad2daea64480",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=516b904b30bf3e6620b4d1dddd7ea6df37b0ab96",
            "patch": "@@ -1146,6 +1146,7 @@ xla_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:triton_fusion_analysis\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "cf93b80b8d2e6ef6ef6bfb0bb902d26ef27aa3b0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=516b904b30bf3e6620b4d1dddd7ea6df37b0ab96",
            "patch": "@@ -86,6 +86,13 @@ class TritonTest : public GpuCodegenTest {\n     }\n   }\n \n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n+    // This is a legacy test, we are testing the old emitter.\n+    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n+    return debug_options;\n+  }\n+\n  protected:\n   const stream_executor::DeviceDescription& device_desc() {\n     return backend().default_stream_executor()->GetDeviceDescription();\n@@ -104,7 +111,6 @@ class TritonGemmTest : public TritonTest {\n     debug_options.set_xla_gpu_enable_split_k_autotuning(false);\n     // Always rewrite Gemms with Triton regardless of size.\n     debug_options.set_xla_gpu_gemm_rewrite_size_threshold(0);\n-    debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n     return debug_options;\n   }\n "
        },
        {
            "sha": "4af7ab9cbf76e1b98d97166113680140f47427e1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_large_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 4,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_large_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_large_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_large_test.cc?ref=516b904b30bf3e6620b4d1dddd7ea6df37b0ab96",
            "patch": "@@ -67,18 +67,30 @@ ENTRY e {\n   const char* kHloTextTest = R\"(\n HloModule t\n \n-triton_dot {\n+lhs {\n+  ROOT p0 = f16[65536,32800] parameter(0)\n+}\n+\n+rhs {\n+  ROOT p1 = f16[32800,32] parameter(0)\n+}\n+\n+triton_dot_computation {\n   p0 = f16[65536,32800] parameter(0)\n   p1 = f16[32800,32] parameter(1)\n-  ROOT dot = f16[65536,32] dot(p0, p1),\n+  lhs = f16[65536,32800] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config=\"{\\\"fusion_backend_config\\\":{\\\"kind\\\":\\\"__triton_nested_gemm_fusion\\\",\\\"block_level_fusion_config\\\":{\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"32\\\",\\\"32\\\"]}]}}}\"\n+  rhs = f16[32800,32] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config=\"{\\\"fusion_backend_config\\\":{\\\"kind\\\":\\\"__triton_nested_gemm_fusion\\\",\\\"block_level_fusion_config\\\":{\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"32\\\",\\\"32\\\"]}]}}}\"\n+  ROOT dot = f16[65536,32] dot(lhs, rhs),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n ENTRY e {\n   p0 = f16[65536,32800] parameter(0)\n   p1 = f16[32800,32] parameter(1)\n-  ROOT _ = f16[65536,32] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n-    backend_config=\"{\\\"fusion_backend_config\\\": {kind: \\\"__triton_gemm\\\", triton_gemm_config: {\\\"block_m\\\":\\\"32\\\",\\\"block_n\\\":\\\"32\\\",\\\"block_k\\\":\\\"32\\\",\\\"split_k\\\":\\\"1\\\",\\\"num_stages\\\":\\\"1\\\",\\\"num_warps\\\":\\\"1\\\",\\\"num_ctas\\\":\\\"1\\\"}}}\"\n+  ROOT _ = f16[65536,32] fusion(p0, p1), kind=kCustom, calls=triton_dot_computation,\n+    backend_config=\"{\\\"fusion_backend_config\\\":{\\\"kind\\\":\\\"__triton_nested_gemm_fusion\\\",\\\"block_level_fusion_config\\\":{\\\"output_tiles\\\":[{\\\"sizes\\\":[\\\"32\\\",\\\"32\\\"]}],\\\"num_stages\\\":1,\\\"num_warps\\\":1,\\\"num_ctas\\\":1}}}\"\n }\n )\";\n "
        },
        {
            "sha": "f956f47ec88c7cb01481598a8f306385e5414f12",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 163,
            "deletions": 79,
            "changes": 242,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=516b904b30bf3e6620b4d1dddd7ea6df37b0ab96",
            "patch": "@@ -31,10 +31,12 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n@@ -82,10 +84,22 @@ class DotTest : public TritonSupportTestBaseWithParam {\n         primitive_util::LowercasePrimitiveTypeName(output_type);\n \n     const std::string kHloTestTemplate = R\"(\n+lhs {\n+  ROOT p0 = $0[92,11]{1,0} parameter(0)\n+}\n+rhs {\n+  ROOT p0 = $1[11,63]{1,0} parameter(0)\n+}\n triton_computation {\n-  parameter_0 = $0[92,11]{1,0} parameter(0)\n-  parameter_1 = $1[11,63]{1,0} parameter(1)\n-  ROOT dot = $2[92,63]{1,0} $3(parameter_0, parameter_1),\n+  p0 = $0[92,11]{1,0} parameter(0)\n+  p1 = $1[11,63]{1,0} parameter(1)\n+  lhs_fusion = $0[92,11]{1,0} fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,16]}]}}}\n+  rhs_fusion = $1[11,63]{1,0} fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}]}}}\n+  ROOT dot = $2[92,63]{1,0} $3(lhs_fusion, rhs_fusion),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n@@ -94,11 +108,9 @@ ENTRY e {\n   parameter_1 = $1[11,63]{1,0} parameter(1)\n   ROOT triton_op = $2[92,63]{1,0} fusion(parameter_0, parameter_1), kind=kCustom,\n     calls=triton_computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n-      triton_gemm_config:\n-        {\"block_m\":16,\"block_n\":32,\"block_k\":512,\n-         \"split_k\":1,\"num_stages\":4,\"num_warps\":8,\n-         \"num_ctas\":1}}}\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}],\n+       \"num_stages\":4,\"num_warps\":8,\"num_ctas\":1}}}\n })\";\n     const std::string hlo_test = absl::Substitute(\n         kHloTestTemplate, lhs, rhs, output, HloOpcodeString(opcode));\n@@ -120,10 +132,12 @@ ENTRY e {\n       }\n       const se::DeviceDescription dev_info =\n           TestGpuDeviceInfo::RTXA6000DeviceInfo(GetComputeCapability());\n-      BlockLevelParameters block_level_parameters;\n-      block_level_parameters.num_ctas = 1;\n-      block_level_parameters.num_stages = 4;\n-      block_level_parameters.num_warps = 8;\n+      auto block_level_parameters =\n+          BlockLevelParameters::FromBlockLevelFusionConfig(\n+              ti.TritonFusion()\n+                  .backend_config<GpuBackendConfig>()\n+                  ->fusion_backend_config()\n+                  .block_level_fusion_config());\n       EXPECT_THAT(\n           TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n                         dev_info, block_level_parameters, &llvm_module_,\n@@ -201,15 +215,28 @@ TEST_P(DynamicSliceTest, IsTritonSupportedDynamicSlice) {\n \n   constexpr absl::string_view kHloTestTemplate =\n       R\"(\n+lhs {\n+  p0 = $0[$2,$3] parameter(0)\n+  p1 = $1[] parameter(1)\n+  p2 = $1[] parameter(2)\n+  ds = $0[5,2] dynamic-slice(p0, p1, p2), dynamic_slice_sizes={5,2}\n+  ROOT convert = f32[5,2] convert(ds)\n+}\n+rhs {\n+  ROOT p0 = f32[2,4] parameter(0)\n+}\n triton_computation {\n   dynamic_slice_input = $0[$2,$3] parameter(0)\n   dot_rhs = f32[2,4] parameter(1)\n   start_index0 = $1[] parameter(2)\n   start_index1 = $1[] parameter(3)\n-  dynamic_slice = $0[5,2] dynamic-slice(dynamic_slice_input, start_index0, start_index1),\n-                  dynamic_slice_sizes={5,2}\n-  convert = f32[5,2] convert(dynamic_slice)\n-  ROOT dot = f32[5, 4] dot(convert, dot_rhs),\n+  lhs = f32[5,2] fusion(dynamic_slice_input, start_index0, start_index1), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[32,32]}]}}}\n+  rhs = f32[2,4] fusion(dot_rhs), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[32,32]}]}}}\n+  ROOT dot = f32[5, 4] dot(lhs, rhs),\n           lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n \n@@ -222,9 +249,9 @@ ENTRY e {\n        kind=kCustom, calls=triton_computation,\n        backend_config={\n          \"fusion_backend_config\":{\n-           \"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\n-             \"block_m\":\"32\",\"block_n\":\"32\",\"block_k\":\"32\",\"split_k\":\"1\",\n-             \"num_stages\":\"1\",\"num_warps\":\"4\",\"num_ctas\":\"1\"}}}\n+           \"kind\":\"__triton_nested_gemm_fusion\",\n+            \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[32,32]}],\n+            \"num_stages\":1,\"num_warps\":4,\"num_ctas\":1}}}\n })\";\n \n   const std::string hlo_test = absl::Substitute(\n@@ -236,27 +263,32 @@ ENTRY e {\n       param.is_the_majormost_dim_being_sliced ? 1 : 0,  // start_index0\n       param.is_the_majormost_dim_being_sliced ? 0 : 1   // start_index1\n   );\n-  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti, ParseTemplateAndGetInstruction(\n-                                                    hlo_test, /*data_type=*/{},\n-                                                    HloOpcode::kDynamicSlice));\n+  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction dot,\n+                          ParseTemplateAndGetInstruction(\n+                              hlo_test, /*data_type=*/{}, HloOpcode::kDot));\n+  HloInstruction* dynamic_slice =\n+      FindInstruction(dot.Module().get(), HloOpcode::kDynamicSlice);\n+  ASSERT_NE(dynamic_slice, nullptr);\n \n   const bool is_supported_instruction =\n-      legacy_triton::IsTritonSupportedInstruction(ti.Instruction(),\n+      legacy_triton::IsTritonSupportedInstruction(*dynamic_slice,\n                                                   GetComputeCapability())\n           .CanFuse();\n   const bool is_supported_dynamic_slice =\n       legacy_triton::IsTritonSupportedDynamicSlice(\n-          *Cast<HloDynamicSliceInstruction>(&ti.Instruction()))\n+          *Cast<HloDynamicSliceInstruction>(dynamic_slice))\n           .CanFuse();\n   EXPECT_EQ(is_supported_instruction, is_supported_dynamic_slice);\n \n   if (is_supported_instruction) {\n+    // TODO(goncharov): Change to `EXPECT_FALSE(is_supported_instruction)`.\n+    GTEST_SKIP() << \"The generic emitter does not support dynamic slice yet.\";\n     TF_EXPECT_OK(\n-        ApplyFloatNormalization(ti.Module().get(), GetComputeCapability()));\n+        ApplyFloatNormalization(dot.Module().get(), GetComputeCapability()));\n     EXPECT_TRUE(RunAndCompareNoHloPasses(\n-        std::move(ti.Module()), ErrorSpec{/*aabs=*/2e-4, /*arel=*/2e-4}));\n+        std::move(dot.Module()), ErrorSpec{/*aabs=*/2e-4, /*arel=*/2e-4}));\n   } else {\n-    EXPECT_THAT(TritonFusionAnalysis::Execute(ti.TritonComputation()),\n+    EXPECT_THAT(TritonFusionAnalysis::Execute(dot.TritonComputation()),\n                 absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n   }\n }\n@@ -271,22 +303,32 @@ INSTANTIATE_TEST_SUITE_P(\n TEST_F(TritonSupportTestBase,\n        UnsupportedDotOutputTypeFailsCanTritonHandleGEMM) {\n   const std::string kHloTest = R\"(\n+lhs {\n+  ROOT p0 = f32[92,11]{1,0} parameter(0)\n+}\n+rhs {\n+  ROOT p0 = f32[11,63]{1,0} parameter(0)\n+}\n triton_computation {\n-  parameter_0 = f32[92,11]{1,0} parameter(0)\n-  parameter_1 = f32[11,63]{1,0} parameter(1)\n-  ROOT dot = pred[92,63]{1,0} dot(parameter_0, parameter_1),\n+  p0 = f32[92,11]{1,0} parameter(0)\n+  p1 = f32[11,63]{1,0} parameter(1)\n+  lhs_fusion = f32[92,11]{1,0} fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,16]}]}}}\n+  rhs_fusion = f32[11,63]{1,0} fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}]}}}\n+  ROOT dot = pred[92,63]{1,0} dot(lhs_fusion, rhs_fusion),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n ENTRY e {\n   parameter_0 = f32[92,11]{1,0} parameter(0)\n   parameter_1 = f32[11,63]{1,0} parameter(1)\n   ROOT triton_op = pred[92,63]{1,0} fusion(parameter_0, parameter_1), kind=kCustom,\n     calls=triton_computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n-      triton_gemm_config:\n-        {\"block_m\":16,\"block_n\":32,\"block_k\":512,\n-         \"split_k\":1,\"num_stages\":4,\"num_warps\":8,\n-         \"num_ctas\":1}}}\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}],\n+       \"num_stages\":4,\"num_warps\":8,\"num_ctas\":1}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n                           ParseTemplateAndGetInstruction(\n@@ -300,22 +342,32 @@ ENTRY e {\n \n TEST_F(TritonSupportTestBase, UnsupportedIntFloatDotFailsCanTritonHandleGEMM) {\n   const std::string kHloTest = R\"(\n+lhs {\n+  ROOT p0 = s8[92,11]{1,0} parameter(0)\n+}\n+rhs {\n+  ROOT p0 = s8[11,63]{1,0} parameter(0)\n+}\n triton_computation {\n-  parameter_0 = s8[92,11]{1,0} parameter(0)\n-  parameter_1 = s8[11,63]{1,0} parameter(1)\n-  ROOT dot = f32[92,63]{1,0} dot(parameter_0, parameter_1),\n+  p0 = s8[92,11]{1,0} parameter(0)\n+  p1 = s8[11,63]{1,0} parameter(1)\n+  lhs_fusion = s8[92,11]{1,0} fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,16]}]}}}\n+  rhs_fusion = s8[11,63]{1,0} fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}]}}}\n+  ROOT dot = f32[92,63]{1,0} dot(lhs_fusion, rhs_fusion),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n ENTRY e {\n   parameter_0 = s8[92,11]{1,0} parameter(0)\n   parameter_1 = s8[11,63]{1,0} parameter(1)\n   ROOT triton_op = f32[92,63]{1,0} fusion(parameter_0, parameter_1), kind=kCustom,\n     calls=triton_computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n-      triton_gemm_config:\n-        {\"block_m\":16,\"block_n\":32,\"block_k\":512,\n-         \"split_k\":1,\"num_stages\":4,\"num_warps\":8,\n-         \"num_ctas\":1}}}\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}],\n+       \"num_stages\":4,\"num_warps\":8,\"num_ctas\":1}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n                           ParseTemplateAndGetInstruction(\n@@ -331,22 +383,32 @@ ENTRY e {\n TEST_F(TritonSupportTestBase,\n        UnsupportedDifferentOperandTypesDotFailsCanTritonHandleGEMM) {\n   const std::string kHloTest = R\"(\n+lhs {\n+  ROOT p0 = f16[92,11]{1,0} parameter(0)\n+}\n+rhs {\n+  ROOT p0 = f32[11,63]{1,0} parameter(0)\n+}\n triton_computation {\n-  parameter_0 = f16[92,11]{1,0} parameter(0)\n-  parameter_1 = f32[11,63]{1,0} parameter(1)\n-  ROOT dot = f32[92,63]{1,0} dot(parameter_0, parameter_1),\n+  p0 = f16[92,11]{1,0} parameter(0)\n+  p1 = f32[11,63]{1,0} parameter(1)\n+  lhs_fusion = f16[92,11]{1,0} fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,16]}]}}}\n+  rhs_fusion = f32[11,63]{1,0} fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}]}}}\n+  ROOT dot = f32[92,63]{1,0} dot(lhs_fusion, rhs_fusion),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n }\n ENTRY e {\n   parameter_0 = f16[92,11]{1,0} parameter(0)\n   parameter_1 = f32[11,63]{1,0} parameter(1)\n   ROOT triton_op = f32[92,63]{1,0} fusion(parameter_0, parameter_1), kind=kCustom,\n     calls=triton_computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n-      triton_gemm_config:\n-        {\"block_m\":16,\"block_n\":32,\"block_k\":512,\n-         \"split_k\":1,\"num_stages\":4,\"num_warps\":8,\n-         \"num_ctas\":1}}}\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[16,32]}],\n+       \"num_stages\":4,\"num_warps\":8,\"num_ctas\":1}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n                           ParseTemplateAndGetInstruction(\n@@ -359,26 +421,36 @@ ENTRY e {\n }\n \n TEST_F(TritonSupportTestBase,\n-       UnsupportedDotWithMultipleBatchDimensionsFailsGracefullyWithTriton) {\n+       DotWithMultipleBatchDimensionsIsSupportedWithTriton) {\n   const std::string kHloTest = R\"(\n+lhs {\n+  ROOT p0 = f32[2,2,2,2] parameter(0)\n+}\n+rhs {\n+  ROOT p0 = f32[2,2,2,2] parameter(0)\n+}\n triton_computation {\n-  parameter_0 = f32[2,2,2,2]{3,2,1,0} parameter(0)\n-  parameter_1 = f32[2,2,2,2]{3,2,1,0} parameter(1)\n-  ROOT dot = f32[2,2,2,2]{3,2,1,0} dot(parameter_0, parameter_1),\n-    lhs_contracting_dims={3}, lhs_batch_dims={1,0}, rhs_contracting_dims={2},\n-    rhs_batch_dims={1,0}\n+  p0 = f32[2,2,2,2] parameter(0)\n+  p1 = f32[2,2,2,2] parameter(1)\n+  lhs_fusion = f32[2,2,2,2] fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[1,1,2,2]}]}}}\n+  rhs_fusion = f32[2,2,2,2] fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[1,1,2,2]}]}}}\n+  ROOT dot = f32[2,2,2,2] dot(lhs_fusion, rhs_fusion),\n+    lhs_contracting_dims={3}, lhs_batch_dims={1,0},\n+    rhs_contracting_dims={2}, rhs_batch_dims={1,0}\n }\n \n ENTRY e {\n-  parameter_0 = f32[2,2,2,2]{3,2,1,0} parameter(0)\n-  parameter_1 = f32[2,2,2,2]{3,2,1,0} parameter(1)\n-  ROOT triton_op = f32[2,2,2,2]{3,2,1,0} fusion(parameter_0, parameter_1),\n+  parameter_0 = f32[2,2,2,2] parameter(0)\n+  parameter_1 = f32[2,2,2,2] parameter(1)\n+  ROOT triton_op = f32[2,2,2,2] fusion(parameter_0, parameter_1),\n     kind=kCustom, calls=triton_computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\n-      triton_gemm_config:\n-        {\"block_m\":16,\"block_n\":32,\"block_k\":512,\n-         \"split_k\":1,\"num_stages\":4,\"num_warps\":8,\n-         \"num_ctas\":1}}}\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[1,1,2,2]}],\n+       \"num_stages\":4,\"num_warps\":8,\"num_ctas\":1}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n                           ParseTemplateAndGetInstruction(\n@@ -389,25 +461,36 @@ ENTRY e {\n                   ti.Instruction(), GetComputeCapability())\n                   .Explain(),\n               ::testing::HasSubstr(\"Multiple batch dimensions\"));\n-  BlockLevelParameters block_level_parameters;\n-  block_level_parameters.num_ctas = 1;\n-  block_level_parameters.num_stages = 4;\n-  block_level_parameters.num_warps = 8;\n-  EXPECT_THAT(\n-      TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n-                    dev_info, block_level_parameters, &llvm_module_,\n-                    symbolic_expr_context_),\n-      absl_testing::StatusIs(absl::StatusCode::kInternal,\n-                             ::testing::HasSubstr(\"num_batch_dims <= 1\")));\n+  auto block_level_parameters =\n+      BlockLevelParameters::FromBlockLevelFusionConfig(\n+          ti.TritonFusion()\n+              .backend_config<GpuBackendConfig>()\n+              ->fusion_backend_config()\n+              .block_level_fusion_config());\n+  TF_EXPECT_OK(TritonWrapper(\n+      \"test_fn\", &ti.TritonFusion(), GetComputeCapability(), dev_info,\n+      block_level_parameters, &llvm_module_, symbolic_expr_context_));\n }\n \n TEST_F(TritonSupportTestBase,\n        UnsupportedDotWithNoNonContractingDimensionsFailsGracefullyWithTriton) {\n   const std::string kHloTest = R\"(\n+lhs {\n+  ROOT p0 = f32[2]{0} parameter(0)\n+}\n+rhs {\n+  ROOT p0 = f32[2]{0} parameter(0)\n+}\n triton_computation {\n-  parameter_0 = f32[2]{0} parameter(0)\n-  parameter_1 = f32[2]{0} parameter(1)\n-  ROOT dot = f32[] dot(parameter_0, parameter_1),\n+  p0 = f32[2]{0} parameter(0)\n+  p1 = f32[2]{0} parameter(1)\n+  lhs_fusion = f32[2]{0} fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[1,1]}]}}}\n+  rhs_fusion = f32[2]{0} fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[1,1]}]}}}\n+  ROOT dot = f32[] dot(lhs_fusion, rhs_fusion),\n     lhs_contracting_dims={0}, rhs_contracting_dims={0}\n }\n \n@@ -416,7 +499,8 @@ ENTRY e {\n   parameter_1 = f32[2]{0} parameter(1)\n   ROOT triton_op = f32[] fusion(parameter_0, parameter_1), kind=kCustom,\n     calls=triton_computation,\n-    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_nested_gemm_fusion\",\n+    \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[1,1]}]}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n                           ParseTemplateAndGetInstruction("
        },
        {
            "sha": "b39dfbc09dff9291c004a1803c82a8d25281e7d0",
            "filename": "third_party/xla/xla/service/gpu/ptx_compilation_test.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc?ref=516b904b30bf3e6620b4d1dddd7ea6df37b0ab96",
            "patch": "@@ -81,11 +81,25 @@ ENTRY main {\n )\";\n \n constexpr absl::string_view kSM90AHlo = R\"(\n+lhs {\n+  ROOT p0 = f16[64,1024]{1,0} parameter(0)\n+}\n+rhs {\n+  p0 = f16[1024,32,32]{2,1,0} parameter(0)\n+  ROOT bitcast = f16[1024,1024]{0,1} bitcast(p0)\n+}\n gemm_fusion_dot {\n-  %p0 = f16[64,1024]{1,0} parameter(0)\n-  %p1 = f16[1024,32,32]{2,1,0} parameter(1)\n-  %bitcast.74246 = f16[1024,1024]{0,1} bitcast(f16[1024,32,32]{2,1,0} %p1)\n-  ROOT %dot.1302 = f16[64,1024]{1,0} dot(f16[64,1024]{1,0} %p0, f16[1024,1024]{0,1} %bitcast.74246), lhs_contracting_dims={1}, rhs_contracting_dims={0}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n+  p0 = f16[64,1024]{1,0} parameter(0)\n+  p1 = f16[1024,32,32]{2,1,0} parameter(1)\n+  lhs = f16[64,1024]{1,0} fusion(p0), kind=kCustom, calls=lhs,\n+    backend_config={fusion_backend_config:{kind:\"__triton_nested_gemm_fusion\",\n+      block_level_fusion_config:{output_tiles:[{sizes:[64,32]}]}}}\n+  rhs = f16[1024,1024]{0,1} fusion(p1), kind=kCustom, calls=rhs,\n+    backend_config={fusion_backend_config:{kind:\"__triton_nested_gemm_fusion\",\n+      block_level_fusion_config:{output_tiles:[{sizes:[32,32]}]}}}\n+  ROOT dot = f16[64,1024]{1,0} dot(lhs, rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0},\n+    frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n }\n \n ENTRY e {\n@@ -95,11 +109,8 @@ ENTRY e {\n   // whether we properly enable SM 9.0A in all compilation and linking paths.\n   ROOT triton_gemm_fusion_dot = f16[64,1024]{1,0} fusion(p0, p1), kind=kCustom,\n     calls=gemm_fusion_dot,\n-    backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n-      triton_gemm_config:\n-        {\"block_m\":64,\"block_n\":32,\"block_k\":32,\n-         \"split_k\":1,\"num_stages\":1,\"num_warps\":4,\n-         \"num_ctas\":1}}}\n+    backend_config={fusion_backend_config:{kind:\"__triton_nested_gemm_fusion\",\n+      block_level_fusion_config:{output_tiles:[{sizes:[64,32]}],num_stages:1,num_warps:4,num_ctas:1}}}\n })\";\n \n constexpr absl::string_view kResultsInNoPtxHlo = R\"("
        },
        {
            "sha": "0578a2b058fcdeec27277c994b6d99aa80271749",
            "filename": "third_party/xla/xla/service/gpu/tests/triton_naming.hlo",
            "status": "modified",
            "additions": 48,
            "deletions": 10,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Ftriton_naming.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/516b904b30bf3e6620b4d1dddd7ea6df37b0ab96/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Ftriton_naming.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Ftriton_naming.hlo?ref=516b904b30bf3e6620b4d1dddd7ea6df37b0ab96",
            "patch": "@@ -3,17 +3,55 @@\n // CHECK-PTX: define ptx_kernel void @triton_gemm_r(\n // CHECK-GCN: define amdgpu_kernel void @triton_gemm_r(\n \n-HloModule t, is_scheduled=true, entry_computation_layout={(f16[15,19]{1,0},s8[19,17]{1,0})->f16[15,17]{1,0}}\n+HloModule t, is_scheduled=true, entry_computation_layout={(f16[15,19],s8[19,17])->f16[15,17]}\n \n-%triton_gemm_r (parameter_0: s8[19,17], parameter_1: f16[15,19]) -> f16[15,17] {\n-  %parameter_1 = f16[15,19]{1,0} parameter(1)\n-  %parameter_0 = s8[19,17]{1,0} parameter(0)\n-  %cp1.1 = f16[19,17]{1,0} convert(%parameter_0)\n-  ROOT %r.1 = f16[15,17]{1,0} dot(%parameter_1, %cp1.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+lhs {\n+  ROOT p0 = f16[15,19] parameter(0)\n }\n \n-ENTRY %e (p0: f16[15,19], p1: s8[19,17]) -> f16[15,17] {\n-  %p1 = s8[19,17]{1,0} parameter(1)\n-  %p0 = f16[15,19]{1,0} parameter(0)\n-  ROOT %triton_gemm_r = f16[15,17]{1,0} fusion(%p1, %p0), kind=kCustom, calls=%triton_gemm_r, backend_config=\"{ \\\"fusion_backend_config\\\": {kind: \\\"__triton_gemm\\\", triton_gemm_config: {\\\"block_m\\\":\\\"64\\\",\\\"block_n\\\":\\\"32\\\",\\\"block_k\\\":\\\"64\\\",\\\"split_k\\\":\\\"1\\\",\\\"num_stages\\\":\\\"2\\\",\\\"num_warps\\\":\\\"8\\\",\\\"num_ctas\\\":\\\"1\\\"}}}\"\n+rhs {\n+  p0 = s8[19,17] parameter(0)\n+  ROOT convert = f16[19,17] convert(p0)\n+\n+}\n+\n+dot_computation {\n+  p1 = f16[15,19] parameter(1)\n+  lhs = f16[15,19] fusion(p1), kind=kCustom, calls=lhs,\n+    backend_config={\n+      fusion_backend_config:{\n+        kind:\"__triton_nested_gemm_fusion\",\n+        block_level_fusion_config:{\n+          output_tiles:[{sizes:[64,64]}]\n+        }\n+      }\n+    }\n+  p0 = s8[19,17] parameter(0)\n+  rhs = f16[19,17] fusion(p0), kind=kCustom, calls=rhs,\n+    backend_config={\n+      fusion_backend_config:{\n+        kind:\"__triton_nested_gemm_fusion\",\n+        block_level_fusion_config:{\n+          output_tiles:[{sizes:[64,32]}]\n+        }\n+      }\n+    }\n+  ROOT dot = f16[15,17] dot(lhs, rhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY e (p0: f16[15,19], p1: s8[19,17]) -> f16[15,17] {\n+  p1 = s8[19,17] parameter(1)\n+  p0 = f16[15,19] parameter(0)\n+  ROOT triton_gemm_r = f16[15,17] fusion(p1, p0),\n+    kind=kCustom, calls=dot_computation, backend_config={\n+      fusion_backend_config:{\n+        kind:\"__triton_nested_gemm_fusion\",\n+        block_level_fusion_config:{\n+          output_tiles:[{sizes:[64,32]}],\n+          num_stages:2,\n+          num_warps:8,\n+          num_ctas:1\n+        }\n+      }\n+    }\n }"
        }
    ],
    "stats": {
        "total": 358,
        "additions": 255,
        "deletions": 103
    }
}