{
    "author": "GleasonK",
    "message": "[StableHLO broadcast] Fix broadcast bounded lowering for non-splat expands\n\nPiperOrigin-RevId: 833905937",
    "sha": "4ea98e8a5141d376d751105a3b08d2b152cb405f",
    "files": [
        {
            "sha": "e5f90645567c10c290caae6f6295e0049602a3f4",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 131,
            "deletions": 52,
            "changes": 183,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ea98e8a5141d376d751105a3b08d2b152cb405f/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ea98e8a5141d376d751105a3b08d2b152cb405f/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=4ea98e8a5141d376d751105a3b08d2b152cb405f",
            "patch": "@@ -320,7 +320,7 @@ diff --ruN a/stablehlo/stablehlo/tests/TestUtils.td b/stablehlo/stablehlo/tests/\n diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stablehlo/tests/ops_broadcasting.mlir\n --- stablehlo/stablehlo/tests/ops_broadcasting.mlir\n +++ stablehlo/stablehlo/tests/ops_broadcasting.mlir\n-@@ -0,0 +1,249 @@\n+@@ -0,0 +1,322 @@\n +// RUN: stablehlo-opt %s --hlo-test-broadcast --split-input-file --allow-unregistered-dialect | FileCheck %s\n +\n +/////////\n@@ -415,6 +415,8 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n +// [<=10] x [1] => [<=10]\n +// [1] x [<=10] => [<=10]\n +// [1] x [1, <=10, 1] => [1, <=10, 1]\n++// [5] x [10, 1] => [10, 5]\n++// [5] x [<=10, 1] => [<=10, 5]\n +\n +\n +// [1] x [1] => [1]\n@@ -555,6 +557,38 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n +\n +// -----\n +\n++// [5] x [10, 1] => [10, 5]\n++// CHECK-LABEL: func @tensor_broadcast_5_x_10_1\n++func.func @tensor_broadcast_5_x_10_1(%arg0: tensor<5xf64>, %arg1: tensor<10x1xf64>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf64>) -> tensor<10x5xf64>\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<10x1xf64>) -> tensor<10x5xf64>\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<5xf64>, tensor<10x1xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [<=10, 1] x [5] => [<=10, 5]\n++// CHECK-LABEL: func @tensor_broadcast_b5_1_x_5\n++func.func @tensor_broadcast_b5_1_x_5(\n++  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n++  %arg1: tensor<5xf64>\n++) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x5xf64, #stablehlo.bounds<10, ?>>\n++  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf64>) -> tensor<10x5xf64>\n++  // CHECK: %[[ARG0_DIM0_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[ARG0_DIM0_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (\n++    tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n++    tensor<5xf64>\n++  ) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n +//////\n +// N-ary broadcast tests.\n +\n@@ -570,6 +604,45 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n +  return %0 : !stablehlo.token\n +}\n +\n++// -----\n++\n++/////\n++// Broadcast errors\n++\n++// [10] x [5] => error\n++// expected-error @+1 {{incompatible shapes for broadcasting 10 and 5}}\n++func.func @broadcast_error_10_x_5(%arg0: tensor<10xf64>, %arg1: tensor<5xf64>) -> !stablehlo.token {\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, tensor<5xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [10] x [<=10] => error\n++// expected-error @+1 {{cannot mix bounded and static dimensions in broadcast}}\n++func.func @broadcast_error_10_x_b10(%arg0: tensor<10xf64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [10] x not_tensor => error\n++func.func @broadcast_error_not_tensor(%arg0: tensor<10xf64>, %arg1: !stablehlo.token) -> !stablehlo.token {\n++  // expected-error @+1 {{expected ranked tensor type for broadcast inputs}}\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10xf64>, !stablehlo.token) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [] => error\n++func.func @broadcast_error_empty() -> !stablehlo.token {\n++  // expected-error @+1 {{requires at least one operand to broadcast}}\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"() : () -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n@@ -763,7 +836,7 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_o\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-@@ -0,0 +1,293 @@\n+@@ -0,0 +1,298 @@\n +/* Copyright 2025 The StableHLO Authors.\n +\n +Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -830,7 +903,8 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +  // Get tensor type\n +  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n +  if (!tensor_type)\n-+    return emitError(op.getLoc(), \"expected ranked tensor type\");\n++    return emitError(op.getLoc(),\n++                     \"expected ranked tensor type for broadcast inputs\");\n +\n +  auto encoding =\n +      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n@@ -845,10 +919,11 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +  return dimensions;\n +}\n +\n-+FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(\n-+    const Dimensions& a, const Dimensions& b) {\n++FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(Value op,\n++                                                       const Dimensions& a,\n++                                                       const Dimensions& b) {\n +  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] inputs: \"\n-+                          << toString(a) << \" * \" << toString(b));\n++                          << toString(a) << \" * \" << toString(b) << \"\\n\");\n +  size_t max_rank = std::max(a.size(), b.size());\n +  Dimensions result(max_rank);\n +\n@@ -877,14 +952,14 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +\n +    // If both LHS and RHS are not 1, dim size must match.\n +    if (dim_a.size != dim_b.size) {\n-+      return emitError(a[a_idx].boundOp.value().getLoc(),\n-+                       \"incompatible shapes for broadcasting \")\n++      // FIXME\n++      return emitError(op.getLoc(), \"incompatible shapes for broadcasting \")\n +             << dim_a.size << \" and \" << dim_b.size;\n +    }\n +\n +    // If bounded both must be bounded\n +    if (dim_a.boundOp.has_value() != dim_b.boundOp.has_value()) {\n-+      return emitError(a[a_idx].boundOp.value().getLoc(),\n++      return emitError(op.getLoc(),\n +                       \"cannot mix bounded and static dimensions in broadcast\");\n +    }\n +\n@@ -893,7 +968,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +  }\n +\n +  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n-+                          << toString(result));\n++                          << toString(result) << \"\\n\");\n +  return result;\n +}\n +\n@@ -922,9 +997,11 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +\n +}  // namespace\n +\n-+\n-+FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops) {\n-+  if (ops.empty()) return failure();\n++FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n++                                             ArrayRef<Value> ops) {\n++  if (ops.empty())\n++    return emitError(builder.getInsertionPoint()->getLoc(),\n++                     \"requires at least one operand to broadcast\");\n +\n +  Value first = ops[0];\n +  auto bcastShapeOrFail = getDimensions(first);\n@@ -936,7 +1013,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +    auto dims = getDimensions(currOp);\n +    if (failed(dims)) return failure();\n +    auto currBcastShapeOrFail =\n-+        getNumpyBroadcastShapeWithBounds(bcastShape, *dims);\n++        getNumpyBroadcastShapeWithBounds(currOp, bcastShape, *dims);\n +    if (failed(currBcastShapeOrFail)) return failure();\n +    bcastShape = std::move(*currBcastShapeOrFail);\n +  }\n@@ -960,7 +1037,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n +                                                     ArrayRef<Value> operands) {\n +  // Figure out the broadcast shape\n-+  auto bcastShapeOrFail = getNumpyBroadcastShape(operands);\n++  auto bcastShapeOrFail = getNumpyBroadcastShape(builder, operands);\n +  if (failed(bcastShapeOrFail)) return failure();\n +  Dimensions bcastShape = std::move(*bcastShapeOrFail);\n +\n@@ -976,91 +1053,92 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta\n +\n +FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n +                                        const Dimensions& shape) {\n-+  LLVM_DEBUG(llvm::dbgs() << \"[BroadcastIfNeeded] input: \" << input\n-+                          << \" shape: \" << toString(shape));\n++  LLVM_DEBUG(llvm::dbgs() << \"[numpyBroadcastIfNeeded] Broadcasting input \"\n++                          << input.getType() << \" => \" << toString(shape)\n++                          << \"\\n\");\n +  auto loc = input.getLoc();\n-+  mlir::RankedTensorType input_type =\n++  mlir::RankedTensorType inputType =\n +      dyn_cast<RankedTensorType>(input.getType());\n-+  if (!input_type) return emitError(input.getLoc(), \"expected tensor type\");\n-+  mlir::RankedTensorType output_type =\n-+      getRankedTensorType(shape, input_type.getElementType());\n++  if (!inputType)\n++    return emitError(loc, \"expected ranked tensor type for broadcast inputs\");\n++  mlir::RankedTensorType outputType =\n++      getRankedTensorType(shape, inputType.getElementType());\n +\n +  // Short circuit if no broadcasting is needed.\n-+  if (input_type == output_type) return input;\n++  if (inputType == outputType) return input;\n +\n-+  int64_t input_rank = input_type.getRank();\n-+  int64_t output_rank = output_type.getRank();\n-+  if (input_rank > output_rank)\n++  int64_t inputRank = inputType.getRank();\n++  int64_t outputRank = outputType.getRank();\n++  if (inputRank > outputRank)\n +    return emitError(loc, \"input rank must be <= output rank, got \")\n-+           << input_rank << \" vs \" << output_rank;\n-+\n-+  size_t rank_diff = output_rank - input_rank;\n-+  SmallVector<int64_t> bcast_dims;\n-+  bcast_dims.reserve(input_rank);\n++           << inputRank << \" vs \" << outputRank;\n +\n++  size_t rankDiff = outputRank - inputRank;\n +  auto inputShapeOrFail = getDimensions(input);\n +  if (failed(inputShapeOrFail)) return failure();\n +  Dimensions inputShape = std::move(*inputShapeOrFail);\n +\n +  // Construct broadcast dimensions.\n +  auto broadcastDimensions = llvm::to_vector(\n-+      llvm::seq<int64_t>(output_rank - input_rank, output_rank));\n++      llvm::seq<int64_t>(outputRank - inputRank, outputRank));\n +\n +  // Construct the result type of the broadcast\n +  //  - If input is static and target shape is static, use static shape.\n +  //  - If input has bounded dim, target shape must be bounded, use bounded dim.\n +  //  - If input is not bounded, but target shape is bounded, broadcast to\n +  //    the padded shape then call SetDimensionSize to make dynamic.\n +  auto bcastShape = shape;\n-+  for (size_t i = 0; i < input_rank; ++i) {\n-+    int64_t input_dim_size = inputShape[i].size;\n-+    int64_t result_idx = i + rank_diff;\n-+    int64_t result_dim_size = shape[result_idx].size;\n-+    if (input_dim_size != 1 && input_dim_size != result_dim_size)\n++  for (size_t i = 0; i < inputRank; ++i) {\n++    int64_t inputDimSize = inputShape[i].size;\n++    int64_t resultIdx = i + rankDiff;\n++    int64_t resultDimSize = shape[resultIdx].size;\n++    if (inputDimSize != 1 && inputDimSize != resultDimSize)\n +      return emitError(loc, \"Cannot broadcast input: \")\n-+             << input_type << \" to target shape \" << toString(shape);\n++             << inputType << \" to target shape \" << toString(shape);\n +\n +    if (!inputShape[i].boundOp.has_value() &&\n-+        shape[result_idx].boundOp.has_value()) {\n++        shape[resultIdx].boundOp.has_value()) {\n +      // Use padded shape in broadcast.\n-+      bcastShape[result_idx] = DimensionInfo{shape[result_idx].size};\n++      bcastShape[resultIdx] = DimensionInfo{shape[resultIdx].size};\n +    }\n-+    bcast_dims.push_back(result_idx);\n +  }\n +\n +  // Broadcast to padded size for remaining dimensions.\n-+  for (size_t i = input_rank; i < shape.size(); ++i) {\n++  for (size_t i = 0; i < rankDiff; ++i) {\n +    bcastShape[i] = DimensionInfo{shape[i].size};\n +  }\n +\n +  // Insert broadcast ops\n-+  mlir::RankedTensorType bcast_type =\n-+      getRankedTensorType(bcastShape, input_type.getElementType());\n-+  Value bcast_op = stablehlo::BroadcastInDimOp::create(\n-+      builder, loc, bcast_type, input, broadcastDimensions);\n-+  if (bcast_op.getType() == output_type) return bcast_op;\n++  mlir::RankedTensorType bcastType =\n++      getRankedTensorType(bcastShape, inputType.getElementType());\n++  LLVM_DEBUG(\n++      llvm::dbgs() << \"[numpyBroadcastIfNeeded] Broadcast to padded type \"\n++                   << bcastType << \"\\n\");\n++  Value bcastOp = stablehlo::BroadcastInDimOp::create(\n++      builder, loc, bcastType, input, broadcastDimensions);\n++  if (bcastOp.getType() == outputType) return bcastOp;\n +\n +  // Mark the padded broadcast as dynamic where the result is bounded.\n +  // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n +  // dimensions that required broadcasting.\n +  for (size_t i = 0; i < shape.size(); ++i) {\n +    if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n +      Value boundOp = shape[i].boundOp.value();\n-+      auto dim_size = stablehlo::GetDimensionSizeOp::create(\n++      auto dimSize = stablehlo::GetDimensionSizeOp::create(\n +          builder, loc, boundOp, shape[i].boundOpDim);\n-+      bcast_op = stablehlo::SetDimensionSizeOp::create(builder, loc, bcast_op,\n-+                                                       dim_size, i);\n++      bcastOp = stablehlo::SetDimensionSizeOp::create(builder, loc, bcastOp,\n++                                                       dimSize, i);\n +    }\n +  }\n-+  return bcast_op;\n++  return bcastOp;\n +}\n +\n +}  // namespace stablehlo\n +}  // namespace mlir\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n +++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n-@@ -0,0 +1,68 @@\n+@@ -0,0 +1,69 @@\n +/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n +   Copyright 2022 The StableHLO Authors.\n +\n@@ -1113,7 +1191,8 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stabl\n +\n +// Returns the common shape these ops would broadcast to, or an error if the\n +// ops are not broadcastable.\n-+FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops);\n++FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n++                                             ArrayRef<Value> ops);\n +\n +// Apply numpy broadcasting to the given operands, returning an error if any\n +// operands are not broadcastable."
        }
    ],
    "stats": {
        "total": 183,
        "additions": 131,
        "deletions": 52
    }
}