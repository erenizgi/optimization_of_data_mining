{
    "author": "ZixuanJiang",
    "message": "Adjust sharding for same-sized dimensions in reshape when partitions don't divide size.\n\nGiven the following reshape,\n * Source: shape 4, sharding [8]<=[8]\n * Target: shape 2x2\n\nBefore this change, we infer that the target sharding is [2,4]<=[8] and convert it into a single reshape without any collective operations, which is wrong.\n\nWith this change, we infer the sharding to be `[2,2]<=[4]` and generate the correct partitioner result with HALO exchange.\n\nPiperOrigin-RevId: 810621300",
    "sha": "51691c003a44278788c6a10e069b15711a28b4f9",
    "files": [
        {
            "sha": "33e0d54e8215b0b5cd60d7300e142df5b9f5c01b",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51691c003a44278788c6a10e069b15711a28b4f9/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51691c003a44278788c6a10e069b15711a28b4f9/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=51691c003a44278788c6a10e069b15711a28b4f9",
            "patch": "@@ -976,6 +976,10 @@ std::optional<HloSharding> ReshapeSharding(const Shape& source_shape,\n \n     if (s_size == t_size) {\n       // Same dimension size.\n+      if (inplace_add_sharding_dim && s_size % s_partitions != 0) {\n+        append_target_sharding_dim(std::gcd(s_size, s_partitions));\n+        break;\n+      }\n       append_target_sharding_dim(s_partitions);\n     } else if (t_size == 1) {\n       // Trivial dimension added."
        },
        {
            "sha": "43bbce21fe36a553b80b89ebdd55107b3d86d42c",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/51691c003a44278788c6a10e069b15711a28b4f9/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/51691c003a44278788c6a10e069b15711a28b4f9/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc?ref=51691c003a44278788c6a10e069b15711a28b4f9",
            "patch": "@@ -466,6 +466,27 @@ TEST(HloShardingUtilTest, ReshapeShardingTranspose4) {\n   EXPECT_EQ(result.value(), output_sharding);\n }\n \n+TEST(HloShardingUtilTest, ReshapeShardingWithPadding1) {\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {4});\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {2, 2});\n+  HloSharding input_sharding = HloSharding::IotaTile({8});\n+  std::optional<HloSharding> result =\n+      ReshapeSharding(input_shape, output_shape, input_sharding);\n+  EXPECT_FALSE(result.has_value());\n+}\n+\n+TEST(HloShardingUtilTest, ReshapeShardingWithPadding2) {\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {2, 2});\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {4});\n+  HloSharding input_sharding = HloSharding::IotaTile({2, 4});\n+  HloSharding output_sharding =\n+      HloSharding::PartialTile(TileAssignment({4, 2}));\n+  std::optional<HloSharding> result =\n+      ReshapeSharding(input_shape, output_shape, input_sharding);\n+  EXPECT_TRUE(result.has_value());\n+  EXPECT_EQ(result.value(), output_sharding);\n+}\n+\n TEST(HloShardingUtilTest, ReshapeToTileDimension2D) {\n   // The two sharding in the vector are the same. They will be processed in\n   // different branches in ReshapeToTileDimension."
        }
    ],
    "stats": {
        "total": 25,
        "additions": 25,
        "deletions": 0
    }
}