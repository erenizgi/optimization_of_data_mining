{
    "author": "WillFroom",
    "message": "[XLA:CPU/GPU][XTile] Make clamp return max in the case the limits are inverted.\n\nThis matches the xla documentation: https://openxla.org/xla/operation_semantics#clamp (as well as NumPy behavior).\n\nPiperOrigin-RevId: 836126066",
    "sha": "fcd066680332cf3a7cee4f086032c2eec6516475",
    "files": [
        {
            "sha": "843a93c6d8ec0f287101c6df13789d29e678f227",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 32,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fcd066680332cf3a7cee4f086032c2eec6516475/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fcd066680332cf3a7cee4f086032c2eec6516475/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=fcd066680332cf3a7cee4f086032c2eec6516475",
            "patch": "@@ -407,44 +407,29 @@ Value Compare(EmitterLocOpBuilder& b, ValueRange values,\n }\n \n Value Maximum(EmitterLocOpBuilder& b, ValueRange values) {\n-  if (mlir::isa<mlir::FloatType>(mlir::getElementTypeOrSelf(values[0]))) {\n+  auto type = mlir::getElementTypeOrSelf(values[0]);\n+  if (mlir::isa<mlir::FloatType>(type)) {\n     return b.create<ma::MaximumFOp>(values);\n   }\n-  // logic: isNaN(lhs) || (!isNan(rhs) && lhs >= rhs) ? lhs : rhs\n-  // See also: IEEE Std 754-2008 5.11.\n-  //\n-  // This also works, but we wanted to make it similar to minimum.\n-  // logic: isNaN(lhs) || lhs >= rhs ? lhs : rhs\n-  Value lhs_is_nan =\n-      Compare(b, {values[0], values[0]}, mh::ComparisonDirection::NE);\n-  Value rhs_is_not_nan =\n-      Compare(b, {values[1], values[1]}, mh::ComparisonDirection::EQ);\n-  Value lhs_is_ge = Compare(b, values, mh::ComparisonDirection::GE);\n-  return b.create<ma::SelectOp>(\n-      b.create<ma::OrIOp>(lhs_is_nan,\n-                          b.create<ma::AndIOp>(rhs_is_not_nan, lhs_is_ge)),\n-      values[0], values[1]);\n+\n+  if (type.isInteger(1)) {\n+    return b.create<ma::OrIOp>(values);\n+  }\n+\n+  return b.create<ma::MaxSIOp>(values);\n }\n \n Value Minimum(EmitterLocOpBuilder& b, ValueRange values) {\n-  if (mlir::isa<mlir::FloatType>(mlir::getElementTypeOrSelf(values[0]))) {\n+  auto type = mlir::getElementTypeOrSelf(values[0]);\n+  if (mlir::isa<mlir::FloatType>(type)) {\n     return b.create<ma::MinimumFOp>(values);\n   }\n-  // logic: isNaN(lhs) || (!isNan(rhs) && lhs <= rhs) ? lhs : rhs\n-  // See also: IEEE Std 754-2008 5.11.\n-  //\n-  // This should also work, but the tests show that it doesn't work for\n-  // minimum(x, NaN):\n-  // logic: isNaN(lhs) || lhs <= rhs ? lhs : rhs\n-  Value lhs_is_nan =\n-      Compare(b, {values[0], values[0]}, mh::ComparisonDirection::NE);\n-  Value rhs_is_not_nan =\n-      Compare(b, {values[1], values[1]}, mh::ComparisonDirection::EQ);\n-  Value lhs_is_le = Compare(b, values, mh::ComparisonDirection::LE);\n-  return b.create<ma::SelectOp>(\n-      b.create<ma::OrIOp>(lhs_is_nan,\n-                          b.create<ma::AndIOp>(rhs_is_not_nan, lhs_is_le)),\n-      values[0], values[1]);\n+\n+  if (type.isInteger(1)) {\n+    return b.create<ma::AndIOp>(values);\n+  }\n+\n+  return b.create<ma::MinSIOp>(values);\n }\n \n bool IsSupportedElementwiseLibdeviceFunction(const HloInstruction& hlo) {\n@@ -551,7 +536,7 @@ absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n     case HloOpcode::kMinimum:\n       return Minimum(b, inputs);\n     case HloOpcode::kClamp:\n-      return Maximum(b, {Minimum(b, {inputs[1], inputs[2]}), inputs[0]});\n+      return Minimum(b, {Maximum(b, {inputs[0], inputs[1]}), inputs[2]});\n     case HloOpcode::kAnd:\n       return b.create<ma::AndIOp>(inputs[0], inputs[1]);\n     case HloOpcode::kOr:"
        },
        {
            "sha": "1cf29f90d8c82bc64a5179f928fd75d4d1acb937",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_parametrized_test.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fcd066680332cf3a7cee4f086032c2eec6516475/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fcd066680332cf3a7cee4f086032c2eec6516475/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_parametrized_test.cc?ref=fcd066680332cf3a7cee4f086032c2eec6516475",
            "patch": "@@ -2396,6 +2396,48 @@ INSTANTIATE_TEST_SUITE_P(ReductionTypeTestSuite, ReductionTypeTest,\n                          ::testing::ValuesIn(kReductionSupportedDataTypes),\n                          TritonSupportTestTypeToString);\n \n+class ClampTypeTest : public TritonTest,\n+                      public ::testing::WithParamInterface<PrimitiveType> {};\n+\n+TEST_P(ClampTypeTest, CheckInvertedBoundsGivesExpectedResult) {\n+  PrimitiveType data_type = GetParam();\n+\n+  const std::string kHloTestTemplate = R\"hlo(\n+    triton_computation {\n+      param = $0[512] parameter(0)\n+      lower_bound = $0[] constant(2)\n+      lower_bound_tensor = $0[512] broadcast(lower_bound)\n+      upper_bound = $0[] constant(-2)\n+      upper_bound_tensor = $0[512] broadcast(upper_bound)\n+      ROOT clamp = $0[512] clamp(lower_bound_tensor, param, upper_bound_tensor)\n+    }\n+\n+    ENTRY entry_computation {\n+      p = $0[512] parameter(0)\n+      ROOT fusion = $0[512] fusion(p), kind=kCustom, calls=triton_computation,\n+        backend_config={\n+          \"fusion_backend_config\":{\n+          \"kind\":\"__triton\",\n+          \"block_level_fusion_config\":{\n+            \"output_tiles\":[{\"sizes\":[\"512\"]}],\n+            \"num_warps\":\"1\",\n+            \"num_ctas\":\"1\",\n+            \"num_stages\":\"1\"}}}\n+})hlo\";\n+\n+  const std::string hlo_test = absl::Substitute(\n+      kHloTestTemplate, primitive_util::LowercasePrimitiveTypeName(data_type));\n+  EXPECT_TRUE(\n+      RunAndCompareNoHloPasses(hlo_test, ErrorSpec{/*aabs=*/0, /*arel=*/0}));\n+}\n+\n+constexpr PrimitiveType kClampSupportedDataTypes[] = {S8,  S16, S32, S64,\n+                                                      F16, F32, F64, BF16};\n+\n+INSTANTIATE_TEST_SUITE_P(ClampTypeTestSuite, ClampTypeTest,\n+                         ::testing::ValuesIn(kClampSupportedDataTypes),\n+                         TritonSupportTestTypeToString);\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 91,
        "additions": 59,
        "deletions": 32
    }
}