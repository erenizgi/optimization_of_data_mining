{
    "author": "akhilgoe",
    "message": "PR #32934: [XLA:CPU][oneDNN] Fix failing oneDNN tests and F16 regressions\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32934\n\nThis PR addresses the side effects of a previous XLA commit (https://github.com/openxla/xla/commit/bd3f15e30fd588a53c8d7a594bafaa34d4f4adb5) that prevented any fusions for oneDNN F16 custom calls. In particular, this PR:\n1. Expands the type promotion condition to include checks for oneDNN CCs too.\n2. Use oneDNN float support for both oneDNN CCs and graph based execution.\nCopybara import of the project:\n\n--\n00c688e5c73b802591f6266e46e264acc6ab3052 by Akhil Goel <akhil.goel@intel.com>:\n\nInitial commit\n\n--\ne74b6f19977e6095e312707f48b13dc7e1f9ea43 by Akhil Goel <akhil.goel@intel.com>:\n\nAddress review comment\n\nMerging this change closes #32934\n\nPiperOrigin-RevId: 829340050",
    "sha": "ec8876a2558e065b27a4fd2ffca62e4845963e0f",
    "files": [
        {
            "sha": "b62e1a34b1fb73e243dbe6996aca596f0f9ed249",
            "filename": "third_party/xla/xla/service/change_op_data_type.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec8876a2558e065b27a4fd2ffca62e4845963e0f/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec8876a2558e065b27a4fd2ffca62e4845963e0f/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc?ref=ec8876a2558e065b27a4fd2ffca62e4845963e0f",
            "patch": "@@ -68,8 +68,8 @@ absl::StatusOr<bool> ChangeOpDataType::RunImpl(\n #ifdef XLA_ONEDNN\n       // TODO(penporn): Move this logic outside of this pass.\n       const DebugOptions& debug_options = module->config().debug_options();\n-      if (debug_options.xla_cpu_use_onednn() &&\n-          !debug_options.xla_cpu_experimental_onednn_custom_call() &&\n+      if ((debug_options.xla_cpu_use_onednn() ||\n+           debug_options.xla_cpu_experimental_onednn_custom_call()) &&\n           cpu::OneDnnContractionRewriter::ShouldRewriteInstr(instr, true)) {\n         continue;\n       }"
        },
        {
            "sha": "248e91bd13d6a62c20441ef723ad4859f891ffb8",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec8876a2558e065b27a4fd2ffca62e4845963e0f/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec8876a2558e065b27a4fd2ffca62e4845963e0f/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=ec8876a2558e065b27a4fd2ffca62e4845963e0f",
            "patch": "@@ -750,8 +750,11 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   // BF16/F8 lowering for most ops.\n   CpuFloatSupport bf16_support(BF16, call_library_for_dot);\n #ifdef XLA_ONEDNN\n+  bool use_onednn_graph =\n+      module->config().debug_options().xla_cpu_use_onednn() &&\n+      IsOneDnnCompatible(is_aot_compile);\n   OneDnnFloatSupport onednn_bf16_support(BF16);\n-  if (use_onednn_custom_call) {\n+  if (use_onednn_custom_call || use_onednn_graph) {\n     pipeline.AddPass<FloatNormalization>(&onednn_bf16_support);\n   } else {\n     pipeline.AddPass<FloatNormalization>(&bf16_support);"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 6,
        "deletions": 3
    }
}