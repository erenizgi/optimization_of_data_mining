{
    "author": "pemeliya",
    "message": "PR #33854: [ROCM[ Remove padding for gemms\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33854\n\nüìù Summary of Changes\nDisabled padding for bf16/fp16 gemms on ROCM\n\nüéØ Justification\nPadding for gemms was added for parity with NVidia, but upon closer look, it turns out that there is no any advantage of it on ROCM platform. Furthermore, CublasPadForGemms wraps any padded dot op into pad / slice clauses which may prevent certain optimizations (e.g. gemm_rewriter epilogue fusion).\n\nüöÄ Kind of Contribution\n ‚ôªÔ∏è Cleanup\n\nüß™ Unit Tests:\nAdapted the existing gemm_rewriter_test to account for padding on ROCM\n\n@xla-rotation could you have a look please ?\nCopybara import of the project:\n\n--\n8d52de94e4fee78d999cdc0c28082b09ffc472cb by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nremove pad for gemms\n\nMerging this change closes #33854\n\nPiperOrigin-RevId: 831750726",
    "sha": "d8292f2e97c7d1cd596757e20d7ef4f61362dc00",
    "files": [
        {
            "sha": "27e40c90fc029ddf70473f102966345176e7e1cd",
            "filename": "third_party/xla/xla/service/gpu/cublas_padding_requirements.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d8292f2e97c7d1cd596757e20d7ef4f61362dc00/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d8292f2e97c7d1cd596757e20d7ef4f61362dc00/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.h?ref=d8292f2e97c7d1cd596757e20d7ef4f61362dc00",
            "patch": "@@ -42,8 +42,8 @@ constexpr std::array<CublasPaddingRequirement, 3> CublasPaddingRequirements{\n      {se::CudaComputeCapability::Volta(), F16, 8},\n      {se::CudaComputeCapability::Ampere(), BF16, 8}}};\n \n-constexpr std::array<HipblasPaddingRequirement, 2> HipblasPaddingRequirements{\n-    {{/*rocm gpu arch,*/ F16, 8}, {/*rocm gpu arch,*/ BF16, 8}}};\n+// No padding requirements for ROCM\n+constexpr std::array<HipblasPaddingRequirement, 0> HipblasPaddingRequirements;\n \n // Tell if either of the operands of the dot requires padding.\n bool CublasRequiresPadding(const HloDotInstruction& dot,"
        },
        {
            "sha": "43d732d3c666a298285e1687f5d268ad862dd271",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d8292f2e97c7d1cd596757e20d7ef4f61362dc00/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d8292f2e97c7d1cd596757e20d7ef4f61362dc00/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc?ref=d8292f2e97c7d1cd596757e20d7ef4f61362dc00",
            "patch": "@@ -979,8 +979,13 @@ ENTRY bf16gemm {\n   )\";\n   EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{1e-5, 1e-5}));\n \n-  if (!IsCuda() ||\n-      HasCudaComputeCapability(se::CudaComputeCapability::Ampere())) {\n+  if (!IsCuda()) {\n+    MatchOptimizedHlo(hlo_text,\n+                      R\"(\n+; CHECK: {{.*}} custom-call(bf16[12,4]{1,0} {{.*}}, bf16[4,8]{1,0} {{.*}}), custom_call_target=\"<<CUBLAS_CUSTOM_CALL_TARGET_PLACEHOLDER>>\"\n+  )\",\n+                      /*print_operand_shape=*/true);\n+  } else if (HasCudaComputeCapability(se::CudaComputeCapability::Ampere())) {\n     MatchOptimizedHlo(hlo_text,\n                       R\"(\n ; CHECK: {{.*}} custom-call(bf16[16,8]{1,0} {{.*}}, bf16[8,8]{1,0} {{.*}}), custom_call_target=\"<<CUBLAS_CUSTOM_CALL_TARGET_PLACEHOLDER>>\"\n@@ -1004,8 +1009,13 @@ ENTRY bf16gemm {\n   )\";\n   EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{1e-5, 1e-5}));\n \n-  if (!IsCuda() ||\n-      HasCudaComputeCapability(se::CudaComputeCapability::Ampere())) {\n+  if (!IsCuda()) {\n+    MatchOptimizedHlo(hlo_text,\n+                      R\"(\n+    ; CHECK: {{.*}} custom-call(bf16[3,3,4]{2,1,0} {{.*}}, bf16[3,3,2]{2,1,0} {{.*}}), custom_call_target=\"<<CUBLAS_CUSTOM_CALL_TARGET_PLACEHOLDER>>\"\n+    )\",\n+                      /*print_operand_shape=*/true);\n+  } else if (HasCudaComputeCapability(se::CudaComputeCapability::Ampere())) {\n     MatchOptimizedHlo(hlo_text,\n                       R\"(\n     ; CHECK: {{.*}} custom-call(bf16[3,8,8]{2,1,0} {{.*}}, bf16[3,8,8]{2,1,0} {{.*}}), custom_call_target=\"<<CUBLAS_CUSTOM_CALL_TARGET_PLACEHOLDER>>\""
        }
    ],
    "stats": {
        "total": 22,
        "additions": 16,
        "deletions": 6
    }
}