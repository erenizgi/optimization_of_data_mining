{
    "author": "tensorflower-gardener",
    "message": "Reshard on call output if sharding mismatches with the func result.\n\nIt is no-op behaviorally for shardy. Because the call output and func result may mismatch only if dedup-functions-fully options is true, and this option is false by default.\n\nShardy will add explicit reshards (during shardy partitioner) on those operations that use the output of named computation and it will do so assuming the sharding of the named computation is sharded as specified in the out shardings of the named computation.\n\nWhen dedup-functions-fully option is true, however, the function that is actually called may end up having a different output sharding than the corresponding named computation. So, the users of the output shardings should still use sharding as in the output shardings the named computation. Hence, if there is a mismatch between the output sharding of the named computation and the result sharding of the function, we add a reshard on the output of the call.\n\nPiperOrigin-RevId: 823494391",
    "sha": "69c93c6f6a5df5b67d112e2956384174a91ee088",
    "files": [
        {
            "sha": "469db54c4a0b5c55afd5b18e47624086cda70764",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/69c93c6f6a5df5b67d112e2956384174a91ee088/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/69c93c6f6a5df5b67d112e2956384174a91ee088/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2FBUILD?ref=69c93c6f6a5df5b67d112e2956384174a91ee088",
            "patch": "@@ -39,6 +39,7 @@ cc_library(\n     srcs = [\"export_named_computations.cc\"],\n     hdrs = [\"export_named_computations.h\"],\n     deps = [\n+        \"//xla/mlir_hlo\",\n         \"//xla/service/spmd/shardy:constants\",\n         \"//xla/service/spmd/shardy:utils\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "412404e687968f4e8474bc45b78c0e40802c4fc4",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/export_named_computations.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 2,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/69c93c6f6a5df5b67d112e2956384174a91ee088/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/69c93c6f6a5df5b67d112e2956384174a91ee088/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc?ref=69c93c6f6a5df5b67d112e2956384174a91ee088",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/DialectRegistry.h\"\n #include \"mlir/IR/OperationSupport.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/SymbolTable.h\"\n@@ -41,6 +42,7 @@ limitations under the License.\n #include \"shardy/dialect/sdy/ir/constants.h\"\n #include \"shardy/dialect/sdy/ir/dialect.h\"\n #include \"shardy/dialect/sdy/ir/utils.h\"\n+#include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n #include \"xla/service/spmd/shardy/constants.h\"\n #include \"xla/service/spmd/shardy/utils.h\"\n \n@@ -231,13 +233,24 @@ class ExportNamedComputationsPass\n       callOp->setAttrs(callOpAttrs);\n \n       // Copy the func output shardings to the call op.\n-      // TODO(enver): Add explicit reshard if callOp and funcOp result shardings\n-      // mismatch.\n       FuncOp funcOp = symbolTable.lookup<FuncOp>(funcSymName);\n       if (TensorShardingPerValueAttr funcResultShardings =\n               getFuncResultShardings(callOp, funcOp, symbolTable);\n           funcResultShardings) {\n         mlir::sdy::setShardings(callOp, funcResultShardings);\n+        if (outShardings.has_value()) {\n+          for (auto [funcResultSharding, outSharding, result] : llvm::zip_equal(\n+                   funcResultShardings.getShardings(),\n+                   outShardings->getShardings(), callOp.getResults())) {\n+            if (!funcResultSharding.isEquivalent(outSharding)) {\n+              rewriter.setInsertionPointAfterValue(result);\n+              auto copyOp =\n+                  mlir::mhlo::CopyOp::create(rewriter, result.getLoc(), result);\n+              mlir::sdy::setShardings(copyOp, outSharding);\n+              rewriter.replaceAllUsesExcept(result, copyOp, copyOp);\n+            }\n+          }\n+        }\n         if (manualAxesAttr) {\n           callOp->setAttr(kManualAxes, manualAxesAttr);\n         }\n@@ -256,6 +269,10 @@ class ExportNamedComputationsPass\n            \"`NamedComputationOp`s operands/results.\";\n   }\n \n+  void getDependentDialects(mlir::DialectRegistry& registry) const final {\n+    registry.insert<mlir::sdy::SdyDialect, mlir::mhlo::MhloDialect>();\n+  }\n+\n   Option<bool> dedupFunctionsFully{\n       *this, \"dedup-functions-fully\",\n       llvm::cl::desc("
        },
        {
            "sha": "dac7bb953b4b8571c5800d539b5260b0bce69773",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/export_named_computations_deduplicate_functions_fully.mlir",
            "status": "modified",
            "additions": 51,
            "deletions": 14,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/69c93c6f6a5df5b67d112e2956384174a91ee088/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/69c93c6f6a5df5b67d112e2956384174a91ee088/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir?ref=69c93c6f6a5df5b67d112e2956384174a91ee088",
            "patch": "@@ -3,10 +3,12 @@\n sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n \n // CHECK-LABEL: func @multiple_same_named_computations_different_shardings(\n-func.func @multiple_same_named_computations_different_shardings(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}, {\"x\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %0 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: %1 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: return %1 : tensor<8x2xi32>\n+func.func @multiple_same_named_computations_different_shardings(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[CALL0:.*]] = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>]>}\n+  // CHECK-NEXT: %[[CALL1:.*]] = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>]>}\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %[[CALL1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>}\n+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[CALL0]], %[[COPY]]\n+  // CHECK-NEXT: return %[[ADD]]\n   %0 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {}]>] (%arg1: tensor<8x2xi32>) {\n     %2 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {?}]>]>} : tensor<8x2xi32>\n     sdy.return %2 : tensor<8x2xi32>\n@@ -15,7 +17,8 @@ func.func @multiple_same_named_computations_different_shardings(%arg0: tensor<8x\n     %3 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>} : tensor<8x2xi32>\n     sdy.return %3 : tensor<8x2xi32>\n   } : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  return %1 : tensor<8x2xi32>\n+  %4 = stablehlo.add %0, %1 : tensor<8x2xi32>\n+  return %4 : tensor<8x2xi32>\n }\n \n // CHECK-LABEL: func private @baz(\n@@ -29,11 +32,12 @@ func.func @multiple_same_named_computations_different_shardings(%arg0: tensor<8x\n sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n \n // CHECK-LABEL: func @multiple_same_named_computations_different_shardings_different_number_of_call_sites(\n-func.func @multiple_same_named_computations_different_shardings_different_number_of_call_sites(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}, {\"x\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %0 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: %1 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: %2 = call @baz(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: return %2 : tensor<8x2xi32>\n+func.func @multiple_same_named_computations_different_shardings_different_number_of_call_sites(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[CALL0:.*]] = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>}\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %[[CALL0]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>]>}\n+  // CHECK-NEXT: %[[CALL1:.*]] = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>}\n+  // CHECK-NEXT: %[[CALL2:.*]] = call @baz(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>}\n+  // CHECK-NEXT: return %[[CALL2]] : tensor<8x2xi32>\n   %0 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {}]>] (%arg1: tensor<8x2xi32>) {\n     %2 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {?}]>]>} : tensor<8x2xi32>\n     sdy.return %2 : tensor<8x2xi32>\n@@ -59,21 +63,54 @@ func.func @multiple_same_named_computations_different_shardings_different_number\n \n sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n \n+// CHECK-LABEL: func @multiple_same_named_computations_multiple_outputs_different_shardings(\n+func.func @multiple_same_named_computations_multiple_outputs_different_shardings(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[CALL0:.*]]:2 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>}\n+  // CHECK-NEXT: %[[DIVIDE0:.*]] = stablehlo.divide %[[CALL0]]#0, %[[CALL0]]#1\n+  // CHECK-NEXT: %[[CALL1:.*]]:2 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>}\n+  // CHECK-NEXT: %[[COPY0:.*]] = mhlo.copy %[[CALL1]]#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>}\n+  // CHECK-NEXT: %[[COPY1:.*]] = mhlo.copy %[[CALL1]]#0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>}\n+  // CHECK-NEXT: %[[DIVIDE1:.*]] = stablehlo.divide %[[COPY1]], %[[COPY0]]\n+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[DIVIDE0]], %[[DIVIDE1]]\n+  // CHECK-NEXT: return %[[ADD]]\n+  %0:2 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+    %5 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {?}]>]>} : tensor<8x2xi32>\n+    sdy.return %5, %5 : tensor<8x2xi32>, tensor<8x2xi32>\n+  } : (tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n+  %1 = stablehlo.divide %0#0, %0#1 : tensor<8x2xi32>\n+  %2:2 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>, <@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>) {\n+    %5 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>} : tensor<8x2xi32>\n+    sdy.return %5, %5 : tensor<8x2xi32>, tensor<8x2xi32>\n+  } : (tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n+  %3 = stablehlo.divide %2#0, %2#1 : tensor<8x2xi32>\n+  %4 = stablehlo.add %1, %3 : tensor<8x2xi32>\n+  return %4 : tensor<8x2xi32>\n+}\n+\n+// CHECK-LABEL: func private @baz(\n+// CHECK-SAME:    %arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>})\n+// CHECK-SAME:    -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>}, tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>})\n+// CHECK-NEXT:  stablehlo.multiply %arg0, %arg0\n+// CHECK-SAME:  sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {?}]>]>}\n+\n+// -----\n+\n+sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n+\n // CHECK-LABEL: func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(\n-// CHECK-SAME:      %arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}\n-// CHECK-SAME:      -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n // CHECK-NEXT:  %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n // CHECK-NEXT:    %3 = func.call @foo(%arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n // CHECK-NEXT:    %4 = func.call @foo(%3) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n-// CHECK-NEXT:    sdy.return %4 : tensor<4xf32>\n+// CHECK-NEXT:    %5 = mhlo.copy %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}]>]>} : tensor<4xf32>\n+// CHECK-NEXT:    sdy.return %5 : tensor<4xf32>\n // CHECK-NEXT:  } : (tensor<8xf32>) -> tensor<8xf32>\n // CHECK-NEXT:  %1 = call @foo_0(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : (tensor<8xf32>) -> tensor<8xf32>\n // CHECK-NEXT:  %2 = sdy.manual_computation(%1) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n // CHECK-NEXT:    %3 = func.call @foo(%arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n // CHECK-NEXT:    sdy.return %3 : tensor<4xf32>\n // CHECK-NEXT:  } : (tensor<8xf32>) -> tensor<8xf32>\n // CHECK-NEXT:  return %2 : tensor<8xf32>\n-func.func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n+func.func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n     %1 = sdy.named_computation<\"foo\">(%arg1) in_shardings=[<@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}]>] (%arg2: tensor<4xf32>) {\n       %2 = stablehlo.abs %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<4xf32>"
        }
    ],
    "stats": {
        "total": 87,
        "additions": 71,
        "deletions": 16
    }
}