{
    "author": "tensorflower-gardener",
    "message": "Take out shardings of named computations from the result shardings of the function in case call op result shardings is empty.\n\nPiperOrigin-RevId: 801741457",
    "sha": "147bbcd6ec8c7699cc0ef26142186f23da7e3f43",
    "files": [
        {
            "sha": "4333955ccf403b5a699d03e2a0291e700330b7bd",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/import_func_calls.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 1,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/147bbcd6ec8c7699cc0ef26142186f23da7e3f43/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/147bbcd6ec8c7699cc0ef26142186f23da7e3f43/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc?ref=147bbcd6ec8c7699cc0ef26142186f23da7e3f43",
            "patch": "@@ -76,6 +76,8 @@ bool isInlineableCallOp(CallOp callOp) {\n \n // Returns the first non-maximal mesh on the argument shardings, if there is\n // one. Otherwise returns `std::nullopt`.\n+// TODO(enver): Move to utils and potentially with a common helper that takes an\n+// std::function to get the sharding given an index.\n std::optional<mlir::Attribute> getMeshOrRefOnArguments(\n     FuncOp funcOp, const SymbolTable& symbolTable) {\n   for (int64_t argNum = 0; argNum < funcOp.getNumArguments(); ++argNum) {\n@@ -110,6 +112,44 @@ TensorShardingPerValueAttr getFuncArgShardings(CallOp callOp, FuncOp funcOp,\n   return TensorShardingPerValueAttr::get(funcOp.getContext(), argShardings);\n }\n \n+// Returns the first non-maximal mesh on the result shardings, if there is\n+// one. Otherwise returns `std::nullopt`.\n+// TODO(enver): Move to utils and potentially with a common helper that takes an\n+// std::function to get the sharding given an index.\n+std::optional<mlir::Attribute> getMeshOrRefOnResults(\n+    FuncOp funcOp, const SymbolTable& symbolTable) {\n+  for (int64_t resultNum = 0; resultNum < funcOp.getNumResults(); ++resultNum) {\n+    if (TensorShardingAttr sdySharding =\n+            mlir::sdy::getFuncResultSharding(funcOp, resultNum);\n+        sdySharding && !sdySharding.getMesh(symbolTable).isMaximal()) {\n+      return std::make_optional(sdySharding.getMeshOrRef());\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n+TensorShardingPerValueAttr getFuncResultShardings(\n+    CallOp callOp, FuncOp funcOp, const SymbolTable& symbolTable) {\n+  std::optional<mlir::Attribute> meshOrRef =\n+      getMeshOrRefOnResults(funcOp, symbolTable);\n+  if (!meshOrRef) {\n+    return nullptr;\n+  }\n+  mlir::SmallVector<TensorShardingAttr> resultShardings;\n+  resultShardings.reserve(funcOp.getNumResults());\n+  for (int64_t resultNum = 0; resultNum < funcOp.getNumResults(); ++resultNum) {\n+    TensorShardingAttr sdySharding =\n+        mlir::sdy::getFuncResultSharding(funcOp, resultNum);\n+    resultShardings.push_back(\n+        sdySharding\n+            ? sdySharding\n+            : TensorShardingAttr::getFullyOpen(\n+                  funcOp.getContext(),\n+                  getTensorRank(callOp.getResult(resultNum)), *meshOrRef));\n+  }\n+  return TensorShardingPerValueAttr::get(funcOp.getContext(), resultShardings);\n+}\n+\n void importCallOp(\n     CallOp callOp,\n     llvm::SmallDenseMap<StringRef, mlir::Region*>& calleeNameToMovedRegion,\n@@ -126,14 +166,19 @@ void importCallOp(\n   CHECK(funcOp) << \"Failed to lookup function: \" << calleeName.str();\n \n   rewriter.setInsertionPoint(callOp);\n+  TensorShardingPerValueAttr callOpResultShardings =\n+      mlir::sdy::getShardingPerValue(callOp);\n   auto namedCompOp = rewriter.create<NamedComputationOp>(\n       callOp->getLoc(), callOp->getResultTypes(), calleeName,\n       callOp.getOperands(),\n       /*inShardings=*/\n       getFuncArgShardings(callOp, funcOp, symbolTable),\n       // TODO(b/439018088): Take func result shardings if call op result\n       // shardings are empty.\n-      /*outShardings=*/mlir::sdy::getShardingPerValue(callOp));\n+      /*outShardings=*/\n+      callOpResultShardings\n+          ? callOpResultShardings\n+          : getFuncResultShardings(callOp, funcOp, symbolTable));\n   namedCompOp->setAttrs(namedCompAttrs);\n \n   mlir::Region& namedCompRegion = namedCompOp.getRegion();"
        },
        {
            "sha": "3490056ce0dd50524c5341777808a1b30b7fdb10",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/import_func_calls.mlir",
            "status": "modified",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/147bbcd6ec8c7699cc0ef26142186f23da7e3f43/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/147bbcd6ec8c7699cc0ef26142186f23da7e3f43/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir?ref=147bbcd6ec8c7699cc0ef26142186f23da7e3f43",
            "patch": "@@ -426,3 +426,126 @@ func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32> {sdy.shard\n   %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n   return %0 : tensor<8x2xi32>\n }\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @func_has_out_sharding_call_no_out_sharding\n+func.func @func_has_out_sharding_call_no_out_sharding(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0) out_shardings=[<@mesh, [{\"x\", \"y\"}, {}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0) {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}, {}]>}) {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @func_has_out_sharding_call_has_different_out_sharding\n+func.func @func_has_out_sharding_call_has_different_out_sharding(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0) out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}, {}]>}) {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @func_no_out_sharding_call_has_out_sharding\n+func.func @func_no_out_sharding_call_has_out_sharding(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0) out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @func_has_out_sharding_on_one_result_call_has_out_sharding_on_both_results\n+func.func @func_has_out_sharding_on_one_result_call_has_out_sharding_on_both_results(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]]:2 = sdy.named_computation<\"foo\">(%arg0, %arg0) out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>, <@mesh, [{\"x\"}, {}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   %[[TRANSPOSE:.*]] = stablehlo.transpose %arg1, dims = [1, 0] : (tensor<8x2xi32>) -> tensor<2x8xi32>\n+  // CHECK-NEXT:   %[[DOT_1:.*]] = stablehlo.dot %[[TRANSPOSE]], %arg1 : (tensor<2x8xi32>, tensor<8x2xi32>) -> tensor<2x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]], %[[DOT_1]] : tensor<8x2xi32>, tensor<2x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<2x2xi32>)\n+  // CHECK-NEXT: %[[DOT_2:.*]] = stablehlo.dot %[[NC]]#0, %[[NC]]#1 : (tensor<8x2xi32>, tensor<2x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[DOT_2]] : tensor<8x2xi32>\n+  %0:2 = call @foo(%arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>, <@mesh, [{\"x\"}, {}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<2x2xi32>)\n+  %1 = stablehlo.dot %0#0, %0#1 : (tensor<8x2xi32>, tensor<2x2xi32>) -> (tensor<8x2xi32>)\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<2x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}, {}]>}) {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  %1 = stablehlo.transpose %arg0, dims=[1, 0] : (tensor<8x2xi32>) -> tensor<2x8xi32>\n+  %2 = stablehlo.dot %1, %arg0 : (tensor<2x8xi32>, tensor<8x2xi32>) -> tensor<2x2xi32>\n+  return %0, %2 : tensor<8x2xi32>, tensor<2x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @func_has_out_sharding_on_one_result_call_has_no_out_sharding\n+func.func @func_has_out_sharding_on_one_result_call_has_no_out_sharding(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]]:2 = sdy.named_computation<\"foo\">(%arg0, %arg0) out_shardings=[<@mesh, [{?}, {?}]>, <@mesh, [{\"y\"}, {}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   %[[TRANSPOSE:.*]] = stablehlo.transpose %arg1, dims = [1, 0] : (tensor<8x2xi32>) -> tensor<2x8xi32>\n+  // CHECK-NEXT:   %[[DOT_1:.*]] = stablehlo.dot %[[TRANSPOSE]], %arg1 : (tensor<2x8xi32>, tensor<8x2xi32>) -> tensor<2x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]], %[[DOT_1]] : tensor<8x2xi32>, tensor<2x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<2x2xi32>)\n+  // CHECK-NEXT: %[[DOT_2:.*]] = stablehlo.dot %[[NC]]#0, %[[NC]]#1 : (tensor<8x2xi32>, tensor<2x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[DOT_2]] : tensor<8x2xi32>\n+  %0:2 = call @foo(%arg0, %arg0) {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<2x2xi32>)\n+  %1 = stablehlo.dot %0#0, %0#1 : (tensor<8x2xi32>, tensor<2x2xi32>) -> (tensor<8x2xi32>)\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<2x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}, {}]>}) {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  %1 = stablehlo.transpose %arg0, dims=[1, 0] : (tensor<8x2xi32>) -> tensor<2x8xi32>\n+  %2 = stablehlo.dot %1, %arg0 : (tensor<2x8xi32>, tensor<8x2xi32>) -> tensor<2x2xi32>\n+  return %0, %2 : tensor<8x2xi32>, tensor<2x2xi32>\n+}"
        }
    ],
    "stats": {
        "total": 170,
        "additions": 169,
        "deletions": 1
    }
}