{
    "author": "tensorflower-gardener",
    "message": "[stream_executor:cuda] Use Nccl/NvshmemMemoryAllocator to allocate collective memory\n\nIt is a layering violation to depend from SE to XLA:GPU collectives. All memory allocations should be done via correct se::MemoryAllocator instances. Prepare for removing memory allocation APIs from GPU collectives.\n\nPiperOrigin-RevId: 845925468",
    "sha": "6d3c0f702ff1a90769541228ac10ba1fa5774aa8",
    "files": [
        {
            "sha": "622c4fa354e72f20318895962070fbec9928496c",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 58,
            "deletions": 5,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d3c0f702ff1a90769541228ac10ba1fa5774aa8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d3c0f702ff1a90769541228ac10ba1fa5774aa8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=6d3c0f702ff1a90769541228ac10ba1fa5774aa8",
            "patch": "@@ -1030,13 +1030,67 @@ cc_library(\n )\n \n cc_library(\n-    name = \"nvshmem_memory_allocator\",\n-    srcs = [\"nvshmem_memory_allocator.cc\"],\n+    name = \"nvshmem_memory_allocator_if_builtin_used\",\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n+    deps = select({\n+        \"//xla/stream_executor/cuda:no_builtin_used\": [\n+            \":nvshmem_memory_allocator_stub\",\n+        ],\n+        \"//conditions:default\": [\":nvshmem_memory_allocator\"],\n+    }),\n+)\n+\n+cc_library(\n+    name = \"nvshmem_memory_allocator_if_supported\",\n     hdrs = [\"nvshmem_memory_allocator.h\"],\n     tags = [\n         \"cuda-only\",\n         \"gpu\",\n     ],\n+    deps = select({\n+        \"//xla/stream_executor/cuda:nvshmem_supported\": [\n+            \":nvshmem_memory_allocator_if_builtin_used\",\n+        ],\n+        \"//conditions:default\": [\":nvshmem_memory_allocator_stub\"],\n+    }) + [\n+        \"//xla/stream_executor:memory_allocation\",\n+        \"//xla/stream_executor:memory_allocator\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n+# Used when NVSHMEM is not linked or can't be used.\n+cc_library(\n+    name = \"nvshmem_memory_allocator_stub\",\n+    srcs = [\n+        \"nvshmem_memory_allocator.h\",\n+        \"nvshmem_memory_allocator_stub.cc\",\n+    ],\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \"//xla/stream_executor:memory_allocation\",\n+        \"//xla/stream_executor:memory_allocator\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"nvshmem_memory_allocator\",\n+    srcs = [\n+        \"nvshmem_memory_allocator.cc\",\n+        \"nvshmem_memory_allocator.h\",\n+    ],\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n     deps = [\n         \":cuda_memory_allocator\",\n         \":nvshmem\",\n@@ -1267,11 +1321,10 @@ cc_library(\n         \":cuda_timer\",\n         \":cuda_version_parser\",\n         \":cudnn_api_wrappers\",\n+        \":nccl_memory_allocator\",\n+        \":nvshmem_memory_allocator_if_supported\",\n         \":tma_util\",\n         \"//xla:util\",\n-        \"//xla/backends/gpu/collectives:gpu_collectives\",\n-        \"//xla/core/collectives\",\n-        \"//xla/core/collectives:collectives_registry\",\n         \"//xla/stream_executor:activate_context\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:command_buffer\","
        },
        {
            "sha": "7aa0c84b67ce53e616b58679181af78cfadfbd1e",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 65,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d3c0f702ff1a90769541228ac10ba1fa5774aa8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d3c0f702ff1a90769541228ac10ba1fa5774aa8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=6d3c0f702ff1a90769541228ac10ba1fa5774aa8",
            "patch": "@@ -51,9 +51,6 @@ limitations under the License.\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n #include \"third_party/gpus/cuda/include/driver_types.h\"\n #include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n-#include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n-#include \"xla/core/collectives/collectives.h\"\n-#include \"xla/core/collectives/collectives_registry.h\"\n #include \"xla/stream_executor/activate_context.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n@@ -69,6 +66,8 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_timer.h\"\n #include \"xla/stream_executor/cuda/cuda_version_parser.h\"\n #include \"xla/stream_executor/cuda/cudnn_api_wrappers.h\"\n+#include \"xla/stream_executor/cuda/nccl_memory_allocator.h\"\n+#include \"xla/stream_executor/cuda/nvshmem_memory_allocator.h\"\n #include \"xla/stream_executor/cuda/tma_util.h\"\n #include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -841,14 +840,6 @@ CudaExecutor::~CudaExecutor() {\n   CHECK(gpu_binary_to_module_.empty()) << \"CudaExecutor has loaded modules.\";\n }\n \n-absl::StatusOr<xla::gpu::GpuCollectives*> GetGpuCollectives(\n-    StreamExecutor* executor) {\n-  std::unique_ptr<ActivateContext> activation = executor->Activate();\n-  TF_ASSIGN_OR_RETURN(xla::Collectives * collectives,\n-                      xla::CollectivesRegistry::Default(\"gpu\"));\n-  return tsl::down_cast<xla::gpu::GpuCollectives*>(collectives);\n-}\n-\n CudaExecutor::VmmMemoryHandle::~VmmMemoryHandle() { CHECK_OK(Release()); }\n \n absl::Status CudaExecutor::VmmMemoryHandle::Release() {\n@@ -979,27 +970,6 @@ absl::StatusOr<bool> CudaExecutor::VmmDeallocateMemory(void* ptr) {\n   return true;\n }\n \n-absl::StatusOr<void*> CollectiveMemoryAllocate(StreamExecutor* executor,\n-                                               uint64_t bytes) {\n-  if (bytes == 0) {\n-    return nullptr;\n-  }\n-\n-  std::unique_ptr<ActivateContext> activation = executor->Activate();\n-  TF_ASSIGN_OR_RETURN(xla::gpu::GpuCollectives * gpu_collectives,\n-                      GetGpuCollectives(executor));\n-  return gpu_collectives->Allocate(bytes);\n-}\n-\n-absl::Status CollectiveMemoryDeallocate(StreamExecutor* executor,\n-                                        void* location) {\n-  std::unique_ptr<ActivateContext> activation = executor->Activate();\n-\n-  TF_ASSIGN_OR_RETURN(xla::gpu::GpuCollectives * gpu_collectives,\n-                      GetGpuCollectives(executor));\n-  return gpu_collectives->Deallocate(location);\n-}\n-\n absl::StatusOr<std::unique_ptr<MemoryAllocator>>\n CudaExecutor::CreateMemoryAllocator(MemorySpace type) {\n   if (type == MemorySpace::kUnified) {\n@@ -1035,28 +1005,16 @@ CudaExecutor::CreateMemoryAllocator(MemorySpace type) {\n   }\n \n   if (type == MemorySpace::kCollective) {\n-    // TODO(469289220): Use NCCL/NVSHMEM memory allocator here instead.\n-    return std::make_unique<GenericMemoryAllocator>(\n-        [this](uint64_t size)\n-            -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n-          TF_ASSIGN_OR_RETURN(void* ptr, CollectiveMemoryAllocate(this, size));\n-          XLA_VLOG_DEVICE(2, device_ordinal())\n-              << \"allocated \" << ptr << \" for context \" << cuda_context_\n-              << \" of \" << size << \" bytes of collective memory\";\n-          return std::make_unique<GenericMemoryAllocation>(\n-              ptr, size, [this](void* location, uint64_t size) {\n-                auto status = CollectiveMemoryDeallocate(this, location);\n-                if (!status.ok()) {\n-                  XLA_LOG_DEVICE(ERROR, device_ordinal())\n-                      << \"failed to free collective memory at \" << location\n-                      << \"; result: \" << status;\n-                } else {\n-                  XLA_VLOG_DEVICE(2, device_ordinal())\n-                      << \"deallocated collective memory at \" << location\n-                      << \" for context \" << cuda_context_;\n-                }\n-              });\n-        });\n+    switch (collective_allocator_type_) {\n+      case CollectiveAllocatorType::kNvshmem:\n+        return std::make_unique<NvshmemMemoryAllocator>();\n+      case CollectiveAllocatorType::kNccl:\n+        return std::make_unique<NcclMemoryAllocator>(this);\n+      default:\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"Unsupported collective allocator type: \",\n+                         collective_allocator_type_));\n+    }\n   }\n \n   if (type == MemorySpace::kHost) {\n@@ -1405,17 +1363,6 @@ DeviceAddressBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n       << \"CudaExecutor::Allocate size: \" << size\n       << \" memory_space: \" << memory_space;\n \n-  if (memory_space == static_cast<int64_t>(MemorySpace::kCollective)) {\n-    auto result = CollectiveMemoryAllocate(this, size);\n-    if (!result.ok()) {\n-      XLA_LOG_DEVICE(ERROR, device_ordinal())\n-          << \"CudaExecutor::Allocate returns \" << result.value();\n-    }\n-    XLA_VLOG_DEVICE(1, device_ordinal())\n-        << \"CudaExecutor::Allocate returns \" << result.value();\n-    return DeviceAddressBase(result.value(), size);\n-  }\n-\n   if (memory_space == static_cast<int64_t>(MemorySpace::kHost)) {\n     auto result = HostAllocate(cuda_context_, numa_node_, size);\n     if (!result.ok()) {"
        },
        {
            "sha": "d4d124b89af8dbaa281754635420253f1ad2283a",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvshmem_memory_allocator_stub.cc",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d3c0f702ff1a90769541228ac10ba1fa5774aa8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem_memory_allocator_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d3c0f702ff1a90769541228ac10ba1fa5774aa8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem_memory_allocator_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvshmem_memory_allocator_stub.cc?ref=6d3c0f702ff1a90769541228ac10ba1fa5774aa8",
            "patch": "@@ -0,0 +1,29 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <memory>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/stream_executor/cuda/nvshmem_memory_allocator.h\"\n+#include \"xla/stream_executor/memory_allocation.h\"\n+\n+namespace stream_executor::gpu {\n+absl::StatusOr<std::unique_ptr<MemoryAllocation>>\n+NvshmemMemoryAllocator::Allocate(uint64_t size) {\n+  return absl::UnimplementedError(\"NVSHMEM is not supported on this platform.\");\n+}\n+}  // namespace stream_executor::gpu"
        }
    ],
    "stats": {
        "total": 169,
        "additions": 99,
        "deletions": 70
    }
}