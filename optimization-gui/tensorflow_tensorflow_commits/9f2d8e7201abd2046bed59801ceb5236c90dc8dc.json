{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Support 0D tensors in emitting transpose, bitcast, broadcast and dot helpers.\n\nPiperOrigin-RevId: 828398536",
    "sha": "9f2d8e7201abd2046bed59801ceb5236c90dc8dc",
    "files": [
        {
            "sha": "26cb3f644fb1cb33d6b77cc4e35ff535f5f1ce31",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 107,
            "deletions": 119,
            "changes": 226,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f2d8e7201abd2046bed59801ceb5236c90dc8dc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f2d8e7201abd2046bed59801ceb5236c90dc8dc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=9f2d8e7201abd2046bed59801ceb5236c90dc8dc",
            "patch": "@@ -331,36 +331,25 @@ absl::StatusOr<TileInfo> TileInfo::Construct(\n \n // Same as HLO BroadcastInDims. The sorted indices in `dims` specify the mapping\n // of the input dimensions to the output dimensions.\n-ScalarOrTensor BroadcastInDims(EmitterLocOpBuilder b, ScalarOrTensor value,\n-                               ArrayRef<int64_t> output_shape,\n-                               ArrayRef<int64_t> dims) {\n+TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n+                            ArrayRef<int64_t> output_shape,\n+                            ArrayRef<int64_t> dims) {\n   CHECK(llvm::is_sorted(dims)) << \"broadcast dims must be sorted\";\n \n-  mlir::TypedValue<mlir::RankedTensorType> broadcast_in_dim_input;\n-\n-  if (value.IsScalar()) {\n-    CHECK(dims.empty()) << \"scalar broadcast must have empty dims\";\n-    broadcast_in_dim_input = MakeTensor(b, value.UnwrapScalar());\n-  } else {\n-    broadcast_in_dim_input = value.UnwrapTensor();\n-  }\n-\n   auto result_type = mlir::RankedTensorType::get(\n-      output_shape, broadcast_in_dim_input.getType().getElementType());\n+      output_shape, value.getType().getElementType());\n \n-  return ScalarOrTensor(b.create<stablehlo::BroadcastInDimOp>(\n-      result_type, broadcast_in_dim_input, dims));\n+  return b.create<stablehlo::BroadcastInDimOp>(result_type, value, dims);\n }\n \n-ScalarOrTensor Splat(EmitterLocOpBuilder b, ScalarOrTensor value,\n-                     ArrayRef<int64_t> output_shape) {\n-  return BroadcastInDims(b, value, output_shape, /*dims=*/{});\n+TensorValue Splat(EmitterLocOpBuilder b, Value value,\n+                  ArrayRef<int64_t> output_shape) {\n+  return BroadcastInDims(b, MakeTensor(b, value), output_shape, /*dims=*/{});\n }\n \n-ScalarOrTensor Iota(EmitterLocOpBuilder b, int32_t limit) {\n+TensorValue Iota(EmitterLocOpBuilder b, int32_t limit) {\n   auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n-  return ScalarOrTensor(\n-      b.create<stablehlo::IotaOp>(type, /*iota_dimension=*/0));\n+  return b.create<stablehlo::IotaOp>(type, /*iota_dimension=*/0);\n }\n \n ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder b,\n@@ -389,9 +378,8 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n   // dimension using a scalar as a neutral element.\n   const HloReduceInstruction& hlo_reduce =\n       *::xla::Cast<HloReduceInstruction>(tiled_hlo_reduce.hlo());\n-  ScalarOrTensor input = values[tiled_hlo_reduce.operand(0)];\n-  llvm::ArrayRef<int64_t> input_shape =\n-      mlir::cast<ShapedType>(input.getType()).getShape();\n+  TensorValue input = values[tiled_hlo_reduce.operand(0)].UnwrapTensor();\n+  llvm::ArrayRef<int64_t> input_shape = input.getType().getShape();\n   absl::Span<const int64_t> source_tensor_shape =\n       hlo_reduce.operand(0)->shape().dimensions();\n \n@@ -410,26 +398,26 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n   int64_t input_reduction_dimension_size = input_shape[reduction_dimension];\n   if (input_reduction_dimension_size !=\n       source_tensor_reduction_dimension_size) {\n-    ScalarOrTensor range = Iota(b, input_reduction_dimension_size);\n-    ScalarOrTensor bcast =\n+    TensorValue range = Iota(b, input_reduction_dimension_size);\n+    TensorValue bcast =\n         BroadcastInDims(b, range, input_shape, {reduction_dimension});\n     ScalarOrTensor constant = CreateConst(\n         b, b.getI32Type(), source_tensor_reduction_dimension_size, input_shape);\n-    Value mask =\n-        b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast.UnwrapUnsafe(),\n-                                constant.UnwrapUnsafe());\n+    Value mask = b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast,\n+                                         constant.UnwrapUnsafe());\n \n-    ScalarOrTensor neutral = BroadcastInDims(\n-        b, values[tiled_hlo_reduce.operand(1)], input_shape, /*dims=*/{});\n-    input = ScalarOrTensor(b.create<arith::SelectOp>(mask, input.UnwrapUnsafe(),\n-                                                     neutral.UnwrapUnsafe()));\n+    TensorValue neutral = BroadcastInDims(\n+        b, MakeTensor(b, values[tiled_hlo_reduce.operand(1)].UnwrapUnsafe()),\n+        input_shape, /*dims=*/{});\n+    input = mlir::cast<TensorValue>(\n+        b.create<arith::SelectOp>(mask, input, neutral).getResult());\n   }\n \n   Value init_value =\n       MakeTensor(b, values[tiled_hlo_reduce.operand(1)].UnwrapScalar());\n \n-  stablehlo::ReduceOp reduction = b.create<stablehlo::ReduceOp>(\n-      input.UnwrapTensor(), init_value, reduction_dimension);\n+  stablehlo::ReduceOp reduction =\n+      b.create<stablehlo::ReduceOp>(input, init_value, reduction_dimension);\n   {\n     TF_ASSIGN_OR_RETURN(Type result_ty,\n                         TritonType(b, hlo_reduce.shape().element_type()));\n@@ -521,27 +509,28 @@ ArrayRef<T> MakeArrayRef(const absl::Span<const T> span) {\n   return ArrayRef(span.data(), span.size());\n }\n \n-ScalarOrTensor EmitTiledBroadcast(\n+TensorValue EmitTiledBroadcast(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_broadcast,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n   const SmallVector<int64_t>& input_tile_shape =\n       tiled_broadcast.operand(0)->tile_sizes();\n   const SmallVector<int64_t>& output_tile_shape = tiled_broadcast.tile_sizes();\n \n   if (input_tile_shape.empty() && output_tile_shape.empty()) {\n-    return values[tiled_broadcast.operand(0)];\n+    return MakeTensor(b, values[tiled_broadcast.operand(0)].UnwrapUnsafe());\n   }\n   CHECK(!output_tile_shape.empty());\n \n   SmallVector<int64_t> padded_output_tile_shape =\n       GetPaddedTileSizes(output_tile_shape);\n \n-  ScalarOrTensor input = values[tiled_broadcast.operand(0)];\n+  TensorValue input =\n+      MakeTensor(b, values[tiled_broadcast.operand(0)].UnwrapUnsafe());\n   return BroadcastInDims(b, input, padded_output_tile_shape,\n                          MakeArrayRef(tiled_broadcast.hlo()->dimensions()));\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitTiledIota(\n+absl::StatusOr<TensorValue> EmitTiledIota(\n     EmitterLocOpBuilder b, Value pid, const TiledHloInstruction& tiled_iota) {\n   const HloIotaInstruction* hlo_iota =\n       ::xla::Cast<HloIotaInstruction>(tiled_iota.hlo());\n@@ -563,17 +552,15 @@ absl::StatusOr<ScalarOrTensor> EmitTiledIota(\n \n   // First, stride as needed between the iota components.\n   Value range = b.create<arith::MulIOp>(\n-      Iota(b, padded_tile_sizes[iota_dim]).UnwrapTensor(),\n+      Iota(b, padded_tile_sizes[iota_dim]),\n       Splat(b,\n-            CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim]),\n-            padded_tile_sizes[iota_dim])\n-          .UnwrapTensor());\n+            CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim])\n+                .UnwrapUnsafe(),\n+            padded_tile_sizes[iota_dim]));\n \n   // Then, add the base offset to the iota components.\n   range = b.create<arith::AddIOp>(\n-      range,\n-      Splat(b, ScalarOrTensor(iota_dim_offset), padded_tile_sizes[iota_dim])\n-          .UnwrapTensor());\n+      range, Splat(b, iota_dim_offset, padded_tile_sizes[iota_dim]));\n \n   // Cast the result to the targeted type.\n   TF_ASSIGN_OR_RETURN(Type iota_element_type,\n@@ -583,7 +570,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledIota(\n \n   // And finally, produce a broadcast along the non-iota dimensions in order to\n   // produce the whole iota tile.\n-  return BroadcastInDims(b, ScalarOrTensor(range), padded_tile_sizes,\n+  return BroadcastInDims(b, mlir::cast<TensorValue>(range), padded_tile_sizes,\n                          /*dims=*/{iota_dim});\n }\n \n@@ -622,12 +609,13 @@ absl::StatusOr<TensorValue> EmitTiledReshape(EmitterLocOpBuilder b,\n   return b.create<stablehlo::ReshapeOp>(output_tensor_type, input);\n }\n \n-Value EmitTiledTranspose(EmitterLocOpBuilder b, ArrayRef<int64_t> tile_sizes,\n-                         SmallVector<int64_t> dimensions, Value input) {\n+TensorValue EmitTiledTranspose(EmitterLocOpBuilder b,\n+                               ArrayRef<int64_t> tile_sizes,\n+                               SmallVector<int64_t> dimensions,\n+                               TensorValue input) {\n   SmallVector<int64_t> padded_tile_sizes = GetPaddedTileSizes(tile_sizes);\n \n-  Type input_element_type =\n-      mlir::cast<ShapedType>(input.getType()).getElementType();\n+  Type input_element_type = input.getType().getElementType();\n   Type output_tensor_type =\n       mlir::RankedTensorType::get(padded_tile_sizes, input_element_type);\n \n@@ -636,9 +624,9 @@ Value EmitTiledTranspose(EmitterLocOpBuilder b, ArrayRef<int64_t> tile_sizes,\n   return b.create<stablehlo::TransposeOp>(output_tensor_type, input, order);\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n+absl::StatusOr<TensorValue> EmitTiledBitcast(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_bitcast,\n-    Value input) {\n+    TensorValue input) {\n   Shape input_shape = tiled_bitcast.hlo()->operand(0)->shape();\n   const Shape& output_shape = tiled_bitcast.hlo()->shape();\n   // If the bitcast changes the element type to an element type of the same\n@@ -652,13 +640,11 @@ absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n     }\n     TF_ASSIGN_OR_RETURN(Type output_element_type,\n                         TritonType(b, output_shape.element_type()));\n-    Type output_type =\n-        mlir::isa<TensorValue>(input)\n-            ? mlir::RankedTensorType::get(\n-                  GetPaddedTileSizes(tiled_bitcast.operand(0)->tile_sizes()),\n-                  output_element_type)\n-            : output_element_type;\n-    input = b.create<mlir::tensor::BitcastOp>(output_type, input);\n+    auto output_type = mlir::RankedTensorType::get(\n+        GetPaddedTileSizes(tiled_bitcast.operand(0)->tile_sizes()),\n+        output_element_type);\n+    input = mlir::cast<TensorValue>(\n+        b.create<mlir::tensor::BitcastOp>(output_type, input).getResult());\n     input_shape.set_element_type(output_shape.element_type());\n   }\n \n@@ -681,7 +667,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n   // the bitcast, so it's not possible to easily propagate them from the output.\n   std::vector<int64_t> transpose1_tile_sizes =\n       Permute(tiled_bitcast.operand(0)->tile_sizes(), trt->transpose1_dims);\n-  Value normalized_input =\n+  TensorValue normalized_input =\n       trt->IsTranspose1Identity()\n           ? input\n           : EmitTiledTranspose(b, transpose1_tile_sizes,\n@@ -694,7 +680,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n   // the inverse permutation.\n   std::vector<int64_t> reshape_tile_sizes =\n       PermuteInverse(tiled_bitcast.tile_sizes(), trt->transpose2_dims);\n-  Value normalized_reshape;\n+  TensorValue normalized_reshape;\n   if (ShapeUtil::Equal(trt->transpose1_shape, trt->reshape_shape)) {\n     normalized_reshape = normalized_input;\n   } else {\n@@ -705,12 +691,11 @@ absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n \n   // The final transpose simply uses the tile sizes computed for the original\n   // bitcast by the tiling analysis.\n-  return MakeScalarOrTensor(\n-      b, trt->IsTranspose2Identity()\n+  return trt->IsTranspose2Identity()\n              ? normalized_reshape\n              : EmitTiledTranspose(b, tiled_bitcast.tile_sizes(),\n                                   llvm::to_vector(trt->transpose2_dims),\n-                                  normalized_reshape));\n+                                  normalized_reshape);\n }\n \n absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n@@ -749,18 +734,16 @@ absl::StatusOr<int64_t> GetDotLoopIterationCount(\n //\n // Note: we currently assume that contracting_dimension_tile_index is an i32\n // scalar.\n-absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n-                                     const TiledHloInstruction& dot_operand,\n-                                     Value dot_operand_value,\n-                                     Value contracting_dimension_tile_index,\n-                                     int contraction_dimension_index) {\n+absl::StatusOr<TensorValue> MaskDotOperand(\n+    EmitterLocOpBuilder b, const TiledHloInstruction& dot_operand,\n+    TensorValue dot_operand_value, Value contracting_dimension_tile_index,\n+    int contraction_dimension_index) {\n   if (contracting_dimension_tile_index.getType() != b.getI32Type()) {\n     return absl::FailedPreconditionError(\n         \"contracting_dimension_tile_index must be an i32 scalar\");\n   }\n \n-  llvm::ArrayRef<int64_t> tile_shape =\n-      mlir::cast<ShapedType>(dot_operand_value.getType()).getShape();\n+  llvm::ArrayRef<int64_t> tile_shape = dot_operand_value.getType().getShape();\n \n   int64_t contracting_dimension_size =\n       dot_operand.hlo()->shape().dimensions(contraction_dimension_index);\n@@ -792,9 +775,8 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n       // operand = select(broadcast(mask, operand.shape), operand, 0)\n       Value tile_offset = b.create<arith::MulIOp>(\n           contracting_dimension_tile_index, tile_size_value);\n-      Value range = Iota(b, tile_size).UnwrapTensor();\n-      Value broadcasted_tile_offset =\n-          Splat(b, ScalarOrTensor(tile_offset), {tile_size}).UnwrapTensor();\n+      TensorValue range = Iota(b, tile_size);\n+      TensorValue broadcasted_tile_offset = Splat(b, tile_offset, {tile_size});\n       Value indices = b.create<arith::AddIOp>(range, broadcasted_tile_offset);\n \n       Value boundary = CreateConst(b, b.getI32Type(),\n@@ -804,9 +786,8 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n       Value mask =\n           b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, indices, boundary);\n \n-      mask = BroadcastInDims(b, ScalarOrTensor(mask), tile_shape,\n-                             {contraction_dimension_index})\n-                 .UnwrapTensor();\n+      mask = BroadcastInDims(b, mlir::cast<TensorValue>(mask), tile_shape,\n+                             {contraction_dimension_index});\n       TF_ASSIGN_OR_RETURN(\n           auto element_type,\n           TritonType(b, dot_operand.hlo()->shape().element_type()));\n@@ -823,7 +804,7 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n       b.create<mlir::scf::YieldOp>(dot_operand_value);\n     }\n     b.setInsertionPointAfter(if_op);\n-    return if_op.getResult(0);\n+    return mlir::cast<TensorValue>(if_op.getResult(0));\n   }\n \n   return dot_operand_value;\n@@ -861,15 +842,13 @@ enum class DotOperandSide { kLhs, kRhs };\n //   because the last one of the lhs operand is not equal to 1.\n //\n // Returns an error if canonicalization is not possible.\n-absl::StatusOr<Value> CanonicalizeDotOperand(\n-    EmitterLocOpBuilder b, Value operand, int64_t contracting_dim_idx,\n-    DotOperandSide side, Value counterpart_operand = nullptr) {\n-  llvm::ArrayRef<int64_t> shape =\n-      mlir::cast<ShapedType>(operand.getType()).getShape();\n+absl::StatusOr<TensorValue> CanonicalizeDotOperand(\n+    EmitterLocOpBuilder b, TensorValue operand, int64_t contracting_dim_idx,\n+    DotOperandSide side, TensorValue counterpart_operand = nullptr) {\n+  llvm::ArrayRef<int64_t> shape = operand.getType().getShape();\n   llvm::ArrayRef<int64_t> counterpart_shape =\n-      counterpart_operand == nullptr\n-          ? shape\n-          : mlir::cast<ShapedType>(counterpart_operand.getType()).getShape();\n+      counterpart_operand == nullptr ? shape\n+                                     : counterpart_operand.getType().getShape();\n \n   auto [shape_without_unit_dims, non_unit_dims_indices] =\n       CollapseUnitDims(shape, counterpart_shape);\n@@ -880,8 +859,8 @@ absl::StatusOr<Value> CanonicalizeDotOperand(\n   }\n \n   if (shape.size() != shape_without_unit_dims.size()) {\n-    TF_ASSIGN_OR_RETURN(operand, EmitTiledReshape(b, shape_without_unit_dims,\n-                                                  MakeTensor(b, operand)));\n+    TF_ASSIGN_OR_RETURN(operand,\n+                        EmitTiledReshape(b, shape_without_unit_dims, operand));\n   }\n \n   int expected_contracting_dim_position = side == DotOperandSide::kLhs ? 1 : 0;\n@@ -1025,12 +1004,14 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n \n     Value ki_i32 = Cast(b, ki, b.getI32Type());\n     TF_ASSIGN_OR_RETURN(\n-        Value lhs, MaskDotOperand(b, *tiled_hlo_dot.operand(0), dot_args[0],\n-                                  ki_i32, lhs_contracting_dim_idx));\n+        TensorValue lhs,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(0), dot_args[0], ki_i32,\n+                       lhs_contracting_dim_idx));\n \n     TF_ASSIGN_OR_RETURN(\n-        Value rhs, MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1],\n-                                  ki_i32, rhs_contracting_dim_idx));\n+        TensorValue rhs,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1], ki_i32,\n+                       rhs_contracting_dim_idx));\n \n     // Canonicalize the dot operands to match Triton's/the hardware's\n     // expectations.\n@@ -1159,19 +1140,21 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n     // hinders performance for Triton.\n     Value ki_i32 = Cast(b, ki, b.getI32Type());\n     TF_ASSIGN_OR_RETURN(\n-        Value lhs, MaskDotOperand(b, *tiled_hlo_dot.operand(0), dot_args[0],\n-                                  ki_i32, lhs_contracting_dim_idx));\n+        TensorValue lhs,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(0), dot_args[0], ki_i32,\n+                       lhs_contracting_dim_idx));\n     TF_ASSIGN_OR_RETURN(\n-        Value rhs, MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1],\n-                                  ki_i32, rhs_contracting_dim_idx));\n+        TensorValue rhs,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1], ki_i32,\n+                       rhs_contracting_dim_idx));\n \n     TF_ASSIGN_OR_RETURN(\n-        Value lhs_scale,\n+        TensorValue lhs_scale,\n         MaskDotOperand(b, *tiled_hlo_dot.operand(2), dot_args[2], ki_i32,\n                        lhs_contracting_dim_idx));\n \n     TF_ASSIGN_OR_RETURN(\n-        Value rhs_scale,\n+        TensorValue rhs_scale,\n         MaskDotOperand(b, *tiled_hlo_dot.operand(3), dot_args[3], ki_i32,\n                        rhs_contracting_dim_idx));\n \n@@ -1363,32 +1346,29 @@ absl::StatusOr<ScalarOrTensor> EmitPad(\n     }\n \n     // LHS for the compare is an iota broadcasted to the output shape.\n-    ScalarOrTensor range = Iota(b, pad_output_dim_size);\n-    ScalarOrTensor bcast = BroadcastInDims(b, range, padded_tile_sizes,\n-                                           {static_cast<int64_t>(dim_index)});\n+    TensorValue range = Iota(b, pad_output_dim_size);\n+    TensorValue bcast = BroadcastInDims(b, range, padded_tile_sizes,\n+                                        {static_cast<int64_t>(dim_index)});\n \n     // RHS for the compare is splat(pad_input_dim_size - tile_offset).\n     Value tile_offset_i32 = Cast(b, tile_offset, i32_type);\n     Value threshold = b.create<arith::SubIOp>(\n         CreateConst(b, i32_type, pad_input_dim_size).UnwrapScalar(),\n         tile_offset_i32);\n-    ScalarOrTensor threshold_splat =\n-        Splat(b, ScalarOrTensor(threshold), padded_tile_sizes);\n-    Value cmp =\n-        b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast.UnwrapTensor(),\n-                                threshold_splat.UnwrapTensor());\n+    TensorValue threshold_splat = Splat(b, threshold, padded_tile_sizes);\n+    Value cmp = b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast,\n+                                        threshold_splat);\n     mask = mask ? b.create<arith::AndIOp>(mask, cmp) : cmp;\n   }\n   if (!mask) {\n     return values[tiled_operand];\n   }\n   const TiledHloInstruction* padding_value = tiled_pad.operand(1);\n \n-  ScalarOrTensor pad_value_splat =\n-      Splat(b, values[padding_value], padded_tile_sizes);\n-  auto result = ScalarOrTensor(\n-      b.create<arith::SelectOp>(mask, values[tiled_operand].UnwrapUnsafe(),\n-                                pad_value_splat.UnwrapUnsafe()));\n+  TensorValue pad_value_splat =\n+      Splat(b, values[padding_value].UnwrapUnsafe(), padded_tile_sizes);\n+  auto result = ScalarOrTensor(b.create<arith::SelectOp>(\n+      mask, values[tiled_operand].UnwrapUnsafe(), pad_value_splat));\n   return result;\n }\n \n@@ -1471,11 +1451,13 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kIota) {\n-    return EmitTiledIota(b, pid, tiled_hlo);\n+    TF_ASSIGN_OR_RETURN(TensorValue iota_result,\n+                        EmitTiledIota(b, pid, tiled_hlo));\n+    return MakeScalarOrTensor(b, iota_result);\n   }\n \n   if (hlo->opcode() == HloOpcode::kBroadcast) {\n-    return EmitTiledBroadcast(b, tiled_hlo, values);\n+    return MakeScalarOrTensor(b, EmitTiledBroadcast(b, tiled_hlo, values));\n   }\n \n   if (hlo->opcode() == HloOpcode::kReduce) {\n@@ -1504,16 +1486,22 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kBitcast) {\n-    return EmitTiledBitcast(b, tiled_hlo,\n-                            values[tiled_hlo.operand(0)].UnwrapUnsafe());\n+    TF_ASSIGN_OR_RETURN(\n+        TensorValue bitcast_value,\n+        EmitTiledBitcast(\n+            b, tiled_hlo,\n+            MakeTensor(b, values[tiled_hlo.operand(0)].UnwrapUnsafe())));\n+    return MakeScalarOrTensor(b, bitcast_value);\n   }\n \n   if (hlo->opcode() == HloOpcode::kTranspose) {\n     auto transpose =\n         ::xla::Cast<const HloTransposeInstruction>(tiled_hlo.hlo());\n-    return ScalarOrTensor(EmitTiledTranspose(\n-        b, tiled_hlo.tile_sizes(), llvm::to_vector(transpose->dimensions()),\n-        values[tiled_hlo.operand(0)].UnwrapUnsafe()));\n+    return MakeScalarOrTensor(\n+        b,\n+        EmitTiledTranspose(\n+            b, tiled_hlo.tile_sizes(), llvm::to_vector(transpose->dimensions()),\n+            MakeTensor(b, values[tiled_hlo.operand(0)].UnwrapUnsafe())));\n   }\n \n   // Slice is currently supported only as an operation on indices"
        }
    ],
    "stats": {
        "total": 226,
        "additions": 107,
        "deletions": 119
    }
}