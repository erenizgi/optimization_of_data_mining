{
    "author": "WillFroom",
    "message": "[XLA:GPU] Remove unused libdevice_path in generic emitter.\n\nPiperOrigin-RevId: 827514372",
    "sha": "fc9a2e986cd8867aa78130e3e648104e4e87682b",
    "files": [
        {
            "sha": "38cca8c7181495ccab34f2be85ead07221171f34",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 48,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fc9a2e986cd8867aa78130e3e648104e4e87682b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fc9a2e986cd8867aa78130e3e648104e4e87682b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=fc9a2e986cd8867aa78130e3e648104e4e87682b",
            "patch": "@@ -364,16 +364,14 @@ ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder b,\n }\n \n absl::StatusOr<ScalarOrTensor> EmitScope(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values);\n \n absl::StatusOr<ScalarOrTensor> EmitReduce(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_hlo_reduce,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values,\n-    absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info) {\n   // At the moment, we should only emit a full reduction over a single\n   // dimension using a scalar as a neutral element.\n@@ -462,10 +460,9 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n \n     TF_RET_CHECK(!to_emit.empty());\n \n-    TF_ASSIGN_OR_RETURN(\n-        ScalarOrTensor result,\n-        EmitScope(b, libdevice_path, device_info, /*analysis=*/nullptr, to_emit,\n-                  region_values));\n+    TF_ASSIGN_OR_RETURN(ScalarOrTensor result,\n+                        EmitScope(b, device_info, /*analysis=*/nullptr, to_emit,\n+                                  region_values));\n     // Emit from_elements op so that the reducer can be lowered to triton, as\n     // the triton reducer can only work with scalars.\n     auto result_as_scalar = b.create<mlir::tensor::FromElementsOp>(\n@@ -490,8 +487,7 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n //\n // TODO(b/331413981): get rid of this special handling once this is solved.\n absl::StatusOr<ScalarOrTensor> EmitNestedFusion(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction& fusion_instruction,\n     absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values) {\n   // TODO(b/331402498): revisit the order of scope once we completely\n@@ -516,8 +512,8 @@ absl::StatusOr<ScalarOrTensor> EmitNestedFusion(\n \n   TF_RET_CHECK(to_emit.back() == fusion_computation->root_instruction());\n \n-  return EmitScope(b, libdevice_path, device_info, /*analysis=*/nullptr,\n-                   to_emit, region_values);\n+  return EmitScope(b, device_info, /*analysis=*/nullptr, to_emit,\n+                   region_values);\n }\n \n template <typename T>\n@@ -751,8 +747,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n }\n \n absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n@@ -940,8 +935,7 @@ absl::StatusOr<Value> CanonicalizeDotOperand(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitDot(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n@@ -1046,7 +1040,7 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n       TF_ASSIGN_OR_RETURN(\n           std::vector<ScalarOrTensor> result,\n           EmitTiledComputation(\n-              b, libdevice_path, device_info,\n+              b, device_info,\n               ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n               *tiled_fusion_operand->called_computation(),\n               block_level_parameters, fn, computation_index, values));\n@@ -1109,8 +1103,7 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n@@ -1180,7 +1173,7 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n       TF_ASSIGN_OR_RETURN(\n           std::vector<ScalarOrTensor> result,\n           EmitTiledComputation(\n-              b, libdevice_path, device_info,\n+              b, device_info,\n               ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n               *tiled_fusion_operand->called_computation(),\n               block_level_parameters, fn, computation_index, values));\n@@ -1264,8 +1257,7 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_concatenate,\n     const BlockLevelParameters& block_level_parameters,\n@@ -1360,7 +1352,7 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n     TF_ASSIGN_OR_RETURN(\n         std::vector<ScalarOrTensor> result,\n         EmitTiledComputation(\n-            b, libdevice_path, device_info,\n+            b, device_info,\n             ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n             *tiled_fusion_operand->called_computation(), block_level_parameters,\n             fn, pid, values));\n@@ -1440,8 +1432,7 @@ absl::StatusOr<ScalarOrTensor> EmitPad(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion, const TiledHloInstruction& tiled_hlo,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1492,7 +1483,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kConcatenate) {\n-    return EmitConcatenate(b, libdevice_path, device_info, fusion, tiled_hlo,\n+    return EmitConcatenate(b, device_info, fusion, tiled_hlo,\n                            block_level_parameters, fn, pid, values);\n   }\n \n@@ -1501,12 +1492,12 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kDot) {\n-    return EmitDot(b, libdevice_path, device_info, fusion, tiled_hlo,\n-                   block_level_parameters, fn, pid, values);\n+    return EmitDot(b, device_info, fusion, tiled_hlo, block_level_parameters,\n+                   fn, pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kScaledDot) {\n-    return EmitScaledDot(b, libdevice_path, device_info, fusion, tiled_hlo,\n+    return EmitScaledDot(b, device_info, fusion, tiled_hlo,\n                          block_level_parameters, fn, pid, values);\n   }\n \n@@ -1527,7 +1518,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kReduce) {\n-    return EmitReduce(b, tiled_hlo, values, libdevice_path, device_info);\n+    return EmitReduce(b, tiled_hlo, values, device_info);\n   }\n \n   if (hlo->IsElementwise()) {\n@@ -1580,8 +1571,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n // ordered before consumers in `tiled_computation`. Returns the results for the\n // roots of `tiled_computation`.\n absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n@@ -1610,10 +1600,10 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n       VLOG(1) << \"Skipping nested fusion: \" << hlo->ToString();\n       continue;\n     }\n-    TF_ASSIGN_OR_RETURN(ScalarOrTensor result,\n-                        EmitTiledHloInstruction(\n-                            b, libdevice_path, device_info, fusion, *tiled_hlo,\n-                            block_level_parameters, fn, pid, values));\n+    TF_ASSIGN_OR_RETURN(\n+        ScalarOrTensor result,\n+        EmitTiledHloInstruction(b, device_info, fusion, *tiled_hlo,\n+                                block_level_parameters, fn, pid, values));\n     TF_RET_CHECK(values.insert({tiled_hlo, result}).second) << hlo->ToString();\n     VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n   }\n@@ -1628,8 +1618,7 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n // Emit sequence of instructions using compatible tiling ordered producers\n // before consumers.\n absl::StatusOr<ScalarOrTensor> EmitScope(\n-    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n-    const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values) {\n@@ -1674,9 +1663,9 @@ absl::StatusOr<ScalarOrTensor> EmitScope(\n       result = values[hlo->operand(0)];\n     } else if (hlo->opcode() == HloOpcode::kFusion) {\n       const auto* fusion_instruction = ::xla::Cast<HloFusionInstruction>(hlo);\n-      TF_ASSIGN_OR_RETURN(result,\n-                          EmitNestedFusion(b, libdevice_path, device_info,\n-                                           *fusion_instruction, values));\n+      TF_ASSIGN_OR_RETURN(\n+          result,\n+          EmitNestedFusion(b, device_info, *fusion_instruction, values));\n     } else {\n       return absl::InvalidArgumentError(\n           absl::StrCat(\"Unsupported operation \", hlo->ToString()));\n@@ -1753,7 +1742,6 @@ using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n // TODO(b/421837868): `BlockLevelParameters` should hold all the necessary\n // tiling information.\n absl::Status EmitGeneric(mlir::OpBuilder builder,\n-                         absl::string_view libdevice_path,\n                          const se::DeviceDescription& device_info,\n                          const HloFusionInstruction* fusion,\n                          xtile::EntryFuncOp fn,\n@@ -1847,9 +1835,8 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n   absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor> values;\n   TF_ASSIGN_OR_RETURN(\n       auto results,\n-      EmitTiledComputation(b, libdevice_path, device_info, fusion,\n-                           tiled_hlo_computation, block_level_parameters, fn,\n-                           tile_id, values));\n+      EmitTiledComputation(b, device_info, fusion, tiled_hlo_computation,\n+                           block_level_parameters, fn, tile_id, values));\n \n   for (auto [root, result, arg] :\n        llvm::zip(tiled_hlo_computation.GetRoots(), results,\n@@ -2306,9 +2293,6 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n   fn.addEntryBlock();\n   b.setInsertionPointToStart(&fn.front());\n \n-  std::string libdevice_path =\n-      GetLibdevicePath(fusion->GetModule()->config(), device_info);\n-\n   if (fusion_kind == kTritonGemmFusionKind) {\n     if (absl::c_contains(\n             fusion->GetModule()\n@@ -2318,12 +2302,14 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n             DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n       return Internal(\"Legacy GEMM emitter is disabled.\");\n     }\n+    std::string libdevice_path =\n+        GetLibdevicePath(fusion->GetModule()->config(), device_info);\n     TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info, fusion, fn,\n                                   block_level_parameters));\n   } else if (fusion_kind == kTritonFusionKind ||\n              fusion_kind == kTritonNestedGemmFusionKind ||\n              fusion_kind == kTritonScaledDotFusionKind) {\n-    TF_RETURN_IF_ERROR(EmitGeneric(b, libdevice_path, device_info, fusion, fn,\n+    TF_RETURN_IF_ERROR(EmitGeneric(b, device_info, fusion, fn,\n                                    block_level_parameters,\n                                    &symbolic_expr_context));\n   } else {"
        }
    ],
    "stats": {
        "total": 82,
        "additions": 34,
        "deletions": 48
    }
}