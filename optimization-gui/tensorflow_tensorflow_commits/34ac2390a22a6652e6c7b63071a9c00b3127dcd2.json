{
    "author": "felixwqp",
    "message": "Support async reduce-scatter for `GetLatencyBetween` in sol cost estimator && add verbose log\n\nPiperOrigin-RevId: 818699549",
    "sha": "34ac2390a22a6652e6c7b63071a9c00b3127dcd2",
    "files": [
        {
            "sha": "d687b43b73b179bc6ac760e300e8da249721024a",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 5,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/34ac2390a22a6652e6c7b63071a9c00b3127dcd2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/34ac2390a22a6652e6c7b63071a9c00b3127dcd2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=34ac2390a22a6652e6c7b63071a9c00b3127dcd2",
            "patch": "@@ -379,11 +379,18 @@ LatencyEstimator::TimeCost SolLatencyEstimator::GetLatencyBetween(\n   const HloOpcode from_op = from.GetInstr().opcode();\n   if (!config_.schedule_send_recvs &&\n       (from_op == HloOpcode::kSend || from_op == HloOpcode::kRecv)) {\n+    VLOG(10) << \"GetLatencyBetween: Returning kLowLatency for Send/Recv op \"\n+             << from.GetInstr().name();\n     return kLowLatency;\n   }\n \n-  if (!IsAsyncPair(from, target) || !IsSupportedCollectiveOp(from.GetInstr())) {\n-    return latency_estimator_->GetLatencyBetween(from, target);\n+  if (!IsAsyncPair(from, target) && !IsSupportedCollectiveOp(from.GetInstr())) {\n+    TimeCost latency = latency_estimator_->GetLatencyBetween(from, target);\n+    VLOG(10)\n+        << \"GetLatencyBetween: Not an async pair or unsupported collective \"\n+        << from.GetInstr().name() << \", returning latency from wrapped \"\n+        << \"estimator: \" << latency;\n+    return latency;\n   }\n \n   absl::StatusOr<absl::Duration> coll_time = ComputeCollectiveTime(\n@@ -392,22 +399,34 @@ LatencyEstimator::TimeCost SolLatencyEstimator::GetLatencyBetween(\n   if (!coll_time.ok()) {\n     VLOG(1) << \"Failed to compute collective time: \" << coll_time.status()\n             << \" for \" << from.GetInstr().name();\n-    return latency_estimator_->GetLatencyBetween(from, target);\n+    TimeCost latency = latency_estimator_->GetLatencyBetween(from, target);\n+    VLOG(10) << \"GetLatencyBetween: Fallback to wrapped estimator due to \"\n+                \"ComputeCollectiveTime failure for \"\n+             << from.GetInstr().name() << \", returning latency: \" << latency;\n+    return latency;\n   }\n-  return absl::ToDoubleMicroseconds(*coll_time);\n+  TimeCost latency = absl::ToDoubleMicroseconds(*coll_time);\n+  VLOG(10) << \"GetLatencyBetween: Computed collective time for \"\n+           << from.GetInstr().name() << \": \" << latency << \" us\";\n+  return latency;\n }\n \n LatencyEstimator::TimeCost SolLatencyEstimator::NodeCost(\n     const HloInstruction* instr) const {\n   if (hlo_query::IsAsyncCollectiveStartOp(instr, /*include_send_recv=*/true) ||\n       hlo_query::IsAsyncCollectiveDoneOp(instr, /*include_send_recv=*/true)) {\n+    VLOG(10) << \"NodeCost: Returning kLowCost for async start/done op \"\n+             << instr->name();\n     return kLowCost;\n   }\n \n   if (std::optional<absl::Duration> matmul_duration =\n           matmul_interpolator_->EstimatedRuntime(*instr);\n       matmul_duration.has_value()) {\n-    return absl::ToDoubleMicroseconds(*matmul_duration);\n+    TimeCost cost = absl::ToDoubleMicroseconds(*matmul_duration);\n+    VLOG(10) << \"NodeCost: Matmul cost from matmul_interpolator for \"\n+             << instr->name() << \": \" << cost << \" us\";\n+    return cost;\n   }\n \n   LatencyEstimator::TimeCost cost_in_us;\n@@ -420,8 +439,12 @@ LatencyEstimator::TimeCost SolLatencyEstimator::NodeCost(\n             .EstimateRunTimeForInstruction(instr, &*cost_analysis_)\n             .exec_time;\n     cost_in_us = absl::ToDoubleMicroseconds(total_estimated_time);\n+    VLOG(10) << \"NodeCost: Fusion cost from gpu_performance_model for \"\n+             << instr->name() << \": \" << cost_in_us << \" us\";\n   } else {\n     cost_in_us = 0.01 * latency_estimator_->NodeCost(instr);\n+    VLOG(10) << \"NodeCost: Fallback cost for \" << instr->name() << \": \"\n+             << cost_in_us << \" us\";\n   }\n   VLOG(10) << \"Analytical estimator calculated cost for: \" << instr->name()\n            << \". Cost: \" << cost_in_us;"
        },
        {
            "sha": "82dd5907ce44e93abb04e688ed8f198efd9878a1",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 1,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/34ac2390a22a6652e6c7b63071a9c00b3127dcd2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/34ac2390a22a6652e6c7b63071a9c00b3127dcd2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=34ac2390a22a6652e6c7b63071a9c00b3127dcd2",
            "patch": "@@ -53,7 +53,7 @@ using ::testing::ValuesIn;\n using ::testing::WithParamInterface;\n \n // Define CostType to distinguish between collective and node costs\n-enum class CostType { kCollectiveTime, kNodeCost };\n+enum class CostType { kCollectiveTime, kNodeCost, kEdgeCost };\n \n struct EstimatorTestCase {\n   std::string test_name;\n@@ -110,6 +110,18 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n     return absl::Microseconds(static_cast<int64_t>(cost_val));\n   }\n \n+  absl::Duration GetLatencyBetween(const HloGraphNode& from,\n+                                   const HloGraphNode& target,\n+                                   const HloComputation* computation) {\n+    std::unique_ptr<SolLatencyEstimator> estimator =\n+        *SolLatencyEstimator::Create(\n+            scheduler_config_, std::make_unique<DummyLatencyEstimator>(),\n+            gpu_device_info_, shape_size_fn_, computation, &mlir_context_);\n+    LatencyEstimator::TimeCost cost_val =\n+        estimator->GetLatencyBetween(from, target);\n+    return absl::Microseconds(static_cast<int64_t>(cost_val));\n+  }\n+\n   HloCostAnalysis::ShapeSizeFunction shape_size_fn_;\n   const se::DeviceDescription gpu_device_info_;\n   const SolGPUCostModel::Config sol_flags_;\n@@ -133,6 +145,11 @@ TEST_P(SolLatencyEstimatorTest, TestLatencyEstimation) {\n     actual_time_us = absl::Trunc(time_us, absl::Microseconds(1));\n   } else if (test_case.cost_type == CostType::kNodeCost) {\n     actual_time_us = ComputeNodeCost(*instr, module->entry_computation());\n+  } else if (test_case.cost_type == CostType::kEdgeCost) {\n+    actual_time_us = GetLatencyBetween(\n+        HloGraphNode(instr, /*original_position=*/-1),\n+        HloGraphNode(instr->users().front(), /*original_position=*/-1),\n+        module->entry_computation());\n   } else {\n     LOG(FATAL) << \"Unreachable.\";\n   }\n@@ -229,6 +246,37 @@ ENTRY main {\n       /*expected_latency=*/absl::Microseconds(18895),\n   };\n \n+  EstimatorTestCase reduce_scatter_intra_host = {\n+      /*test_name=*/\"reduce_scatter_intra_host\",\n+      /*module_string=*/R\"(\n+HloModule m, num_partitions=8\n+\n+add {\n+  param_0 = bf16[] parameter(0)\n+  param_1 = bf16[] parameter(1)\n+  ROOT t = bf16[] add(param_0, param_1)\n+}\n+\n+async_comp {\n+  param_3 = bf16[8192,128256] parameter(0)\n+  ROOT r = bf16[1024,128256] reduce-scatter(param_3),\n+    dimensions={0},\n+    to_apply=add,\n+    replica_groups=[1,8]<=[8],\n+    channel_id=1,\n+    use_global_device_ids=true\n+}\n+\n+ENTRY main {\n+  p = bf16[8192,128256] parameter(0)\n+  rs-start = ((bf16[8192,128256]), bf16[1024,128256]) async-start(p), calls=async_comp\n+  ROOT rs-done = bf16[1024,128256] async-done(rs-start)\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kAsyncStart,\n+      /*cost_type=*/CostType::kEdgeCost,\n+      /*expected_latency=*/absl::Microseconds(5716),\n+  };\n+\n   EstimatorTestCase matmul_bf16_1024_4096_512 = {\n       /*test_name=*/\"matmul_bf16_1024_4096_512\",\n       /*module_string=*/R\"(\n@@ -363,6 +411,7 @@ ENTRY e {\n           all_gather_inter_host_pairwise,\n           all_gather_all_ranks,\n           reduce_scatter_all_ranks,\n+          reduce_scatter_intra_host,\n           matmul_bf16_1024_4096_512,\n           matmul_f32_batch4_256_1024_256,\n           triton_matmul_bf16_batch1_1024_1024_1024,"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 78,
        "deletions": 6
    }
}