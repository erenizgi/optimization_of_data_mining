{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Emit all-reduce using fusion emitter.\n\nFirst version of emitting an all-reduce kernel using the existing\ntiling infra. Only sanity tests are added in this iteration.\n\nPlumbing into ir_emitter_unnested and actual e2e correctness tests will be done\nin followups.\n\nPiperOrigin-RevId: 831665103",
    "sha": "6f611c82857c4b266821c1cbaef3f220ec260eb0",
    "files": [
        {
            "sha": "97b1c2562645b47258671a54cf5d46e4f4f0ed16",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -240,6 +240,7 @@ cc_library(\n     ),\n     hdrs = [\"fusion_emitter.h\"],\n     deps = [\n+        \":collective_emitter\",\n         \"//xla:autotuning_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n@@ -1196,32 +1197,49 @@ cc_library(\n     srcs = [\"collective_emitter.cc\"],\n     hdrs = [\"collective_emitter.h\"],\n     deps = [\n+        \":emitter_helpers\",\n         \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n         \"//xla:types\",\n         \"//xla:util\",\n+        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/backends/gpu/runtime:all_reduce\",\n+        \"//xla/codegen:emitter_loc_op_builder\",\n+        \"//xla/codegen/tiling:tiled_hlo_instruction\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/gpu:all_reduce_kernel\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:FunctionInterfaces\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:NVVMDialect\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@triton//:TritonDialects\",\n     ],\n )\n \n xla_cc_test(\n     name = \"collective_emitter_test\",\n     srcs = [\"collective_emitter_test.cc\"],\n+    tags = [\"gpu\"],\n     deps = [\n         \":collective_emitter\",\n         \":fusion\",\n+        \":fusion_emitter\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla/backends/gpu/codegen:fusion_emitter\",\n@@ -1242,6 +1260,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n     ],\n )"
        },
        {
            "sha": "6465938173535ea64e3e47f4ca2df57618a2456a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -17,13 +17,29 @@ limitations under the License.\n \n #include <cstdint>\n #include <optional>\n+#include <type_traits>\n #include <utility>\n \n #include \"absl/base/casts.h\"\n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"llvm/Support/MathExtras.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Interfaces/FunctionInterfaces.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/runtime/all_reduce.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -32,19 +48,40 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/all_reduce_kernel.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace xla::gpu {\n namespace {\n \n+using ::mlir::ShapedType;\n+using ::mlir::Value;\n+using ::xla::gpu::triton::TensorValue;\n+using ::xla::gpu::triton::TileInfo;\n using ::xla::se::gpu::AllReduceStrategy;\n \n+namespace ttir = ::mlir::triton;\n+namespace mtx = ::mlir::triton::xla;\n+namespace arith = ::mlir::arith;\n+\n+// The main memory space on a device (HBM).\n+static constexpr auto kGlobalAddressSpace =\n+    static_cast<std::underlying_type_t<mlir::NVVM::NVVMMemorySpace>>(\n+        mlir::NVVM::NVVMMemorySpace::Global);\n+\n+// Metadata arguments for the collective emitter.\n+// device_rank, signal-value, signal_buffers.\n+static constexpr int32_t kNumCollectiveMetadataArgs = 3;\n+\n bool CanAllReduceBeEmitted(const HloAllReduceInstruction* all_reduce,\n                            ReductionKind reduction_kind, int64_t num_devices,\n                            int64_t num_elements, PrimitiveType element_type,\n@@ -119,6 +156,117 @@ GetBlockLevelFusionConfigForAllReduce(\n   }\n   return block_level_config;\n }\n+\n+absl::StatusOr<TensorValue> EmitAllReduce(\n+    EmitterLocOpBuilder b, const HloComputation* computation,\n+    const HloAllReduceInstruction& all_reduce,\n+    const TiledHloInstruction& tiled_hlo_reduce,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, mlir::Value pid,\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n+  const TiledHloInstruction* tiled_input_hlo = tiled_hlo_reduce.operand(0);\n+  TensorValue input_tile = values[tiled_input_hlo];\n+\n+  // Variadics are not supported yet so we can fix inputs to 1.\n+  // Which means 2 arguments for input/output one for scratch buffers and 3\n+  // metadata arguments. Plus 1 for the tile index for a total of 7.\n+  const int32_t num_input_output_args = computation->num_parameters() * 2;\n+  const int32_t num_scratch_buffers = computation->num_parameters();\n+  static constexpr int32_t kNumTileIndexArgs = 1;\n+  TF_RET_CHECK(fn.getNumArguments() ==\n+               (num_input_output_args + num_scratch_buffers +\n+                kNumCollectiveMetadataArgs + kNumTileIndexArgs));\n+  // Opaque arguments start after the input/output arguments.\n+  const int32_t start_idx = num_input_output_args;\n+  mlir::Value device_rank = fn.getArgument(start_idx);\n+  TF_RET_CHECK(device_rank.getType().isInteger(32));\n+  mlir::Value signal_value = fn.getArgument(start_idx + 1);\n+  TF_RET_CHECK(signal_value.getType().isInteger(32));\n+  // !tt.ptr<!tt.ptr<i32>>\n+  mlir::Value signal_buffers = fn.getArgument(start_idx + 2);\n+  // !tt.ptr<!tt.ptr<i64>>\n+  mlir::Value remote_input_buffers = fn.getArgument(start_idx + 3);\n+\n+  TF_ASSIGN_OR_RETURN(\n+      TileInfo tile_info,\n+      TileInfo::Construct(b, pid, /*runtime_values=*/{}, *tiled_input_hlo));\n+\n+  // 1. Scatter phase: Copy local tile to the remote buffer of the current rank.\n+  const auto ptr_to_i64_type =\n+      ttir::PointerType::get(b.getI64Type(), kGlobalAddressSpace);\n+  auto remote_input_buffers_i64 =\n+      b.create<ttir::BitcastOp>(ptr_to_i64_type, remote_input_buffers);\n+  Value remote_buf_ptr_addr = b.create<ttir::AddPtrOp>(\n+      ptr_to_i64_type, remote_input_buffers_i64, device_rank);\n+  Value remote_buf_i64 =\n+      b.create<ttir::LoadOp>(remote_buf_ptr_addr,\n+                             ttir::CacheModifier::NONE,     //\n+                             ttir::EvictionPolicy::NORMAL,  //\n+                             false);                        // isVolatile\n+  const auto elem_type =\n+      mlir::cast<ShapedType>(input_tile.getType()).getElementType();\n+  const auto ptr_to_elem_type =\n+      ttir::PointerType::get(elem_type, kGlobalAddressSpace);\n+  Value remote_buf_ptr =\n+      b.create<ttir::IntToPtrOp>(ptr_to_elem_type, remote_buf_i64);\n+  mlir::ArrayRef<int64_t> remote_shape = tile_info.original_shape();\n+  const mlir::MemRefType remote_memref_type =\n+      mlir::MemRefType::get(remote_shape, elem_type);\n+  mlir::Value remote_buf_memref =\n+      b.create<mtx::PtrToMemrefOp>(remote_memref_type, remote_buf_ptr);\n+  b.create<xtile::InsertTileOp>(\n+      input_tile, remote_buf_memref, tile_info.offsets(),\n+      tile_info.padded_tile_sizes(), tile_info.tile_strides());\n+\n+  // 2. Synchronization phase: Wait for all ranks to complete the scatter.\n+  int64_t world_size = all_reduce.device_list().num_devices_per_group();\n+  b.create<mtx::BlockBarrierOp>(signal_buffers, device_rank, signal_value,\n+                                b.getI32IntegerAttr(world_size));\n+\n+  // 3. Reduce phase: Load tiles from all ranks and reduce them.\n+  HloComputation* reduction_computation = all_reduce.to_apply();\n+  llvm::SmallVector<const HloInstruction*> to_emit;\n+  // There is really only one non-parameter instruction in the computation.\n+  for (const HloInstruction* instr : reduction_computation->instructions()) {\n+    if (instr->opcode() != HloOpcode::kParameter) {\n+      to_emit.push_back(instr);\n+    }\n+  }\n+  // Set accumulator zero.\n+  mlir::Value accumulator_zero =\n+      b.create<arith::ConstantOp>(elem_type, b.getZeroAttr(elem_type));\n+  TensorValue accumulator =\n+      b.create<ttir::SplatOp>(input_tile.getType(), accumulator_zero);\n+  for (int rank = 0; rank < world_size; ++rank) {\n+    Value rank_idx =\n+        b.create<arith::ConstantOp>(b.getI64Type(), b.getI64IntegerAttr(rank));\n+    Value remote_buf_ptr_addr = b.create<ttir::AddPtrOp>(\n+        ptr_to_i64_type, remote_input_buffers_i64, rank_idx);\n+    Value remote_buf_i64 =\n+        b.create<ttir::LoadOp>(remote_buf_ptr_addr,\n+                               ttir::CacheModifier::NONE,     //\n+                               ttir::EvictionPolicy::NORMAL,  //\n+                               false);                        // isVolatile\n+    Value remote_buf_ptr =\n+        b.create<ttir::IntToPtrOp>(ptr_to_elem_type, remote_buf_i64);\n+    Value remote_buf_memref =\n+        b.create<mtx::PtrToMemrefOp>(remote_memref_type, remote_buf_ptr);\n+    TensorValue next_tile =\n+        EmitParameterExtract(b, tile_info, remote_buf_memref);\n+\n+    absl::flat_hash_map<const HloInstruction*, TensorValue> region_values;\n+    region_values[reduction_computation->parameter_instruction(0)] =\n+        accumulator;\n+    region_values[reduction_computation->parameter_instruction(1)] = next_tile;\n+    TF_ASSIGN_OR_RETURN(\n+        accumulator,\n+        triton::EmitScope(b,\n+                          /*analysis=*/nullptr, /*instructions=*/to_emit,\n+                          /*values=*/region_values));\n+  }\n+  return accumulator;\n+}\n+\n }  // namespace\n \n absl::StatusOr<std::optional<BlockLevelFusionConfig>>\n@@ -153,4 +301,55 @@ absl::StatusOr<bool> TrySetGpuBackendConfigForCollective(\n       fusion_instr->set_backend_config(std::move(gpu_backend_config)));\n   return true;\n }\n+\n+absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n+    llvm::SmallVector<mlir::Type>& fn_arg_types, EmitterLocOpBuilder& b,\n+    const HloComputation* hlo_computation) {\n+  // rank: i32\n+  fn_arg_types.push_back(b.getI32Type());\n+  // signal_value: i32\n+  fn_arg_types.push_back(b.getI32Type());\n+  // signal_buffers: !tt.ptr<!tt.ptr<i32>>\n+  fn_arg_types.push_back(ttir::PointerType::get(\n+      ttir::PointerType::get(b.getI32Type(), kGlobalAddressSpace),\n+      kGlobalAddressSpace));\n+  for (HloInstruction* p : hlo_computation->parameter_instructions()) {\n+    PrimitiveType type = p->shape().element_type();\n+    mlir::Type ir_type;\n+    if (type == U16) {\n+      ir_type = b.getI16Type();\n+    } else if (type == S4) {\n+      ir_type = b.getI4Type();\n+    } else {\n+      TF_ASSIGN_OR_RETURN(ir_type, triton::TritonType(b, type));\n+    }\n+    // Also add the remote/scratch buffers for collectives.\n+    // !tt.ptr<!tt.ptr<type>>\n+    fn_arg_types.push_back(ttir::PointerType::get(\n+        ttir::PointerType::get(ir_type, kGlobalAddressSpace),\n+        kGlobalAddressSpace));\n+  }\n+  // num_metadata_args =\n+  return hlo_computation->num_parameters() + kNumCollectiveMetadataArgs;\n+}\n+\n+absl::StatusOr<TensorValue> EmitCollective(\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    const TiledHloInstruction& tiled_hlo_reduce,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, mlir::Value pid,\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n+  const HloComputation* computation = fusion->fused_instructions_computation();\n+  const HloInstruction* root = computation->root_instruction();\n+  switch (root->opcode()) {\n+    case HloOpcode::kAllReduceStart:\n+      return EmitAllReduce(\n+          b, computation, *xla::Cast<HloAllReduceInstruction>(root),\n+          tiled_hlo_reduce, block_level_parameters, fn, pid, values);\n+    default:\n+      return absl::UnimplementedError(\n+          absl::StrCat(\"Unsupported collective fusion: \", root->ToString()));\n+  }\n+}\n+\n }  // namespace xla::gpu"
        },
        {
            "sha": "e9f23515b005837403ab3ee62569c96ad769d8b3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.h",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -16,11 +16,22 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_CODEGEN_TRITON_COLLECTIVE_EMITTER_H_\n #define XLA_BACKENDS_GPU_CODEGEN_TRITON_COLLECTIVE_EMITTER_H_\n \n+#include <cstdint>\n #include <optional>\n \n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/statusor.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Interfaces/FunctionInterfaces.h\"\n+#include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/types.h\"  // IWYU pragma: keep\n \n@@ -44,5 +55,25 @@ absl::StatusOr<bool> TrySetGpuBackendConfigForCollective(\n     const se::DeviceDescription& device_info,\n     HloFusionInstruction* fusion_instr);\n \n+// Adds the metadata arguments to the function's argument list.\n+// For collective some extra metadata arguments are needed such as rank,\n+// and pointers to remote GPU buffers.\n+// The fn_arg_types is updated in place to add these.\n+// Returns the number of metadata arguments added or error.\n+absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n+    llvm::SmallVector<mlir::Type>& fn_arg_types, EmitterLocOpBuilder& b,\n+    const HloComputation* hlo_computation);\n+\n+// Emits tiled XTile/Triton IR for a collective op.\n+// See [EmitTiledHloInstruction] for an overview of how this fits into the\n+// emitter.\n+absl::StatusOr<triton::TensorValue> EmitCollective(\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    const TiledHloInstruction& tiled_hlo_reduce,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, mlir::Value pid,\n+    absl::flat_hash_map<const TiledHloInstruction*, triton::TensorValue>&\n+        values);\n+\n }  // namespace xla::gpu\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_COLLECTIVE_EMITTER_H_"
        },
        {
            "sha": "777de291e9a2e9f75fb8637e00866a6fc0e6c5d0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -27,10 +27,12 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"llvm/IR/Module.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion.h\"\n+#include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -72,6 +74,8 @@ struct ModuleWithEmitter : public ModuleWithFusion {\n   SymbolicExprContext symbolic_expr_context{&mlir_context};\n   std::optional<HloFusionAnalysis> analysis;\n   std::unique_ptr<TritonFusion> emitter;\n+  llvm::LLVMContext llvm_context;\n+  llvm::Module llvm_module{\"test_module\", llvm_context};\n \n   explicit ModuleWithEmitter(std::unique_ptr<HloModule> module_arg)\n       : ModuleWithFusion{std::move(module_arg)} {}\n@@ -146,7 +150,7 @@ class CollectiveEmitterTest : public CollectiveBlockLevelConfigTest {\n     TF_RET_CHECK(triton_emitter != nullptr);\n     fusion_emitter.release();\n     result->emitter = absl::WrapUnique(triton_emitter);\n-    return std::move(result);\n+    return result;\n   }\n };\n \n@@ -218,6 +222,19 @@ TEST_F(CollectiveEmitterTest, AllReduceWithTritonGetLaunchConfig) {\n   EXPECT_EQ(launch_config->launch_dimensions.num_threads_per_block(), 512);\n }\n \n+TEST_F(CollectiveEmitterTest, AllReduceWithTritonGenerateTritonKernel) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<ModuleWithEmitter> result,\n+      BuildModuleWithEmitter(ShapeUtil::MakeShape(F32, {65536}), device_info_));\n+  const TritonFusion* triton_fusion = result->emitter.get();\n+  ASSERT_NE(triton_fusion, nullptr);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      TritonWrapperResult triton_kernel,\n+      triton_fusion->GenerateTritonKernelAndWrapper(\n+          *result->FusionInstr(), \"test-all-reduce-start\", device_info_,\n+          &result->llvm_module, &result->symbolic_expr_context));\n+}\n+\n }  // namespace\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "cb9b4f4f43dba69d94b9754128c370fdf6e435cf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -15,8 +15,6 @@ limitations under the License.\n \n #include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n \n-#include <variant>\n-\n #include \"mlir/Conversion/AffineToStandard/AffineToStandard.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n@@ -35,6 +33,9 @@ void CreateTritonXlaPipeline(\n     bool allow_tma, int num_stages) {\n   pm->addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n+  pm->addPass(mlir::triton::xla::CreateTritonXLALowerBlockBarrierPass());\n+  pm->addPass(mlir::triton::xla::CreateTritonXLALowerAtomicsPass());\n+  pm->addPass(mlir::triton::xla::CreateTritonXLALowerGetTidPass());\n   pm->addPass(mlir::triton::xla::CreateTritonXLALowerXTilePass());\n \n   auto* cuda_cc = gpu_cc.cuda_compute_capability();"
        },
        {
            "sha": "e3ec9bc77579a6268b49154cc96f9d85d45c0c96",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -103,7 +103,8 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n \n   if (fusion_kind == kTritonFusionKind ||\n       fusion_kind == kTritonNestedGemmFusionKind ||\n-      fusion_kind == kTritonScaledDotFusionKind) {\n+      fusion_kind == kTritonScaledDotFusionKind ||\n+      fusion_kind == kTritonCollectiveFusionKind) {\n     if (!analysis_.fusion_backend_config().has_block_level_fusion_config()) {\n       return absl::InvalidArgumentError(absl::StrCat(\n           \"Block level fusion config is required for Triton fusions: \","
        },
        {
            "sha": "5e02cdc06e286f286d2e136d753462643aca957b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -65,6 +65,7 @@ limitations under the License.\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n #include \"mlir/IR/AffineExpr.h\"\n+#include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n@@ -93,6 +94,7 @@ limitations under the License.\n #include \"mlir/Transforms/Passes.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/emitters/ir/xla_gpu_ops.h\"\n+#include \"xla/backends/gpu/codegen/triton/collective_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n@@ -1245,6 +1247,11 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n     return EmitReduce(b, tiled_hlo, values);\n   }\n \n+  if (hlo->opcode() == HloOpcode::kAllReduceStart) {\n+    return EmitCollective(b, fusion, tiled_hlo, block_level_parameters, fn, pid,\n+                          values);\n+  }\n+\n   if (hlo->IsElementwise()) {\n     std::vector<Value> operands;\n     operands.reserve(hlo->operands().size());\n@@ -1982,7 +1989,21 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     fn_arg_types.push_back(GetMemRefType(shape, triton_ty));\n   }\n \n-  auto fn = b.create<xtile::EntryFuncOp>(fn_name, fn_arg_types);\n+  // Add metadata arguments for collectives.\n+  // This is done after the input and output arguments but before the tile\n+  // index.\n+  int32_t num_metadata_arguments = 0;\n+  if (fusion_kind == kTritonCollectiveFusionKind) {\n+    TF_ASSIGN_OR_RETURN(\n+        num_metadata_arguments,\n+        AddCollectiveMetadataArguments(fn_arg_types, b, hlo_computation));\n+  }\n+  // Metadata arguments are opaque to the tiling infra.\n+  llvm::SmallVector<mlir::NamedAttribute> named_attributes{b.getNamedAttr(\n+      \"num_opaque_args\", b.getI32IntegerAttr(num_metadata_arguments))};\n+\n+  auto fn =\n+      b.create<xtile::EntryFuncOp>(fn_name, fn_arg_types, named_attributes, {});\n \n   fn.addEntryBlock();\n   b.setInsertionPointToStart(&fn.front());\n@@ -2002,7 +2023,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n         legacy_matmul_emitter->Emit(b, fusion, fn, block_level_parameters));\n   } else if (fusion_kind == kTritonFusionKind ||\n              fusion_kind == kTritonNestedGemmFusionKind ||\n-             fusion_kind == kTritonScaledDotFusionKind) {\n+             fusion_kind == kTritonScaledDotFusionKind ||\n+             fusion_kind == kTritonCollectiveFusionKind) {\n     TF_RETURN_IF_ERROR(EmitGeneric(b, emitter_specific_constraints_builder,\n                                    fusion, fn, block_level_parameters,\n                                    &symbolic_expr_context));"
        },
        {
            "sha": "4456b14ab4b8fa6d441dfbcaf21d469417022fa2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -60,6 +61,7 @@ limitations under the License.\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/permutation_util.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace mlir::triton::xla {\n \n@@ -307,14 +309,13 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n \n     SmallVector<Type> new_operand_types(input_types);\n     for (auto&& [index, operand_type] : llvm::enumerate(new_operand_types)) {\n-      mlir::BlockArgument func_arg = op.getArgument(index);\n-      auto element_type =\n-          mlir::cast<PointerType>(operand_type).getPointeeType();\n-\n       auto attr = op.getArgAttr(index, \"tt.tma_descriptor\");\n       if (!attr) {\n         continue;\n       }\n+      mlir::BlockArgument func_arg = op.getArgument(index);\n+      auto element_type =\n+          mlir::cast<PointerType>(operand_type).getPointeeType();\n       auto tma_descriptor = mlir::cast<TmaDescriptorAttr>(attr);\n       auto layout = tma_descriptor.getLayout();\n       auto block_shape = tma_descriptor.getTileShape();"
        },
        {
            "sha": "970aae5af97b6e00b6d669c2c15425315ce87239",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -1782,9 +1782,11 @@ HloInstructionIndexing ComputeOutputToInputIndexing(\n     const HloInstruction* instr, int output_id,\n     SymbolicExprContext* symbolic_expr_context) {\n   if (HloInstruction::IsOpElementwise(instr->opcode()) ||\n-      instr->opcode() == HloOpcode::kMap) {\n-    // Note: map has a `dimensions` attribute, but it does nothing. See\n-    // b/65689298.\n+      // Note: map has a `dimensions` attribute, but it does nothing. See\n+      // b/65689298.\n+      instr->opcode() == HloOpcode::kMap ||\n+      // For a single device, all-reduce is an elementwise op.\n+      instr->opcode() == HloOpcode::kAllReduceStart) {\n     return ComputeOutputToInputCwiseOpIndexing(instr, symbolic_expr_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {\n@@ -1874,9 +1876,11 @@ HloInstructionIndexing ComputeInputToOutputIndexing(\n     const HloInstruction* instr, int input_id,\n     SymbolicExprContext* symbolic_expr_context) {\n   if (HloInstruction::IsOpElementwise(instr->opcode()) ||\n-      instr->opcode() == HloOpcode::kMap) {\n-    // Note: map has a `dimensions` attribute, but it does nothing. See\n-    // b/65689298.\n+      // Note: map has a `dimensions` attribute, but it does nothing. See\n+      // b/65689298.\n+      instr->opcode() == HloOpcode::kMap ||\n+      // For a single device, all-reduce has 1:1 output to input mapping.\n+      instr->opcode() == HloOpcode::kAllReduceStart) {\n     return ComputeInputToOutputCwiseOpIndexing(instr, symbolic_expr_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {"
        },
        {
            "sha": "40c118b29b17aab066d8d5c44ed24dd6494209bd",
            "filename": "third_party/xla/xla/service/gpu/model/triton_emitter_constraints.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6f611c82857c4b266821c1cbaef3f220ec260eb0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc?ref=6f611c82857c4b266821c1cbaef3f220ec260eb0",
            "patch": "@@ -338,6 +338,10 @@ absl::StatusOr<bool> TritonEmitterConstraints::ParametersSatisfyConstraints(\n       // invalid. Otherwise we would for example compute the launch config\n       // incorrectly.\n       if ((tile_size & (tile_size - 1)) && tile_size != dim_size) {\n+        VLOG(5)\n+            << \"Found a tile size that is not a power of 2 and is not equal \"\n+               \"to the dimension size. Bailing out.\"\n+            << tile_size << \" \" << dim_size;\n         return false;\n       }\n     }"
        }
    ],
    "stats": {
        "total": 331,
        "additions": 315,
        "deletions": 16
    }
}