{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Enable vectorization in RaggedAllToAll kernel launch.\n\nThe kernel now attempts to vectorize element processing by grouping multiple elements together based on the element type bitwidth and the number of elements per row. This can lead to more efficient memory accesses.\n\nPiperOrigin-RevId: 801787073",
    "sha": "d8eebc4e811b99a83b03017af1d8604627ac71bd",
    "files": [
        {
            "sha": "1881b663ac96614b6fff2a51e5b918202ade35b0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 6,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d8eebc4e811b99a83b03017af1d8604627ac71bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d8eebc4e811b99a83b03017af1d8604627ac71bd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc?ref=d8eebc4e811b99a83b03017af1d8604627ac71bd",
            "patch": "@@ -93,13 +93,22 @@ absl::Status RunRaggedAllToAllKernel(\n   // blockIdx.x is the index of the update.\n   int64_t num_blocks_x = num_updates_per_output * num_outputs;\n \n+  int64_t num_vectorized_row_elements = num_row_elements;\n+  int64_t vectorized_bitwidth = xla::primitive_util::BitWidth(element_type);\n+\n+  while (num_vectorized_row_elements % 2 == 0 && vectorized_bitwidth < 64) {\n+    num_vectorized_row_elements /= 2;\n+    vectorized_bitwidth *= 2;\n+  }\n+\n   // blockIdx.y and threadIdx.x are used to iterate over the elements of the\n   // update. Since the size of each update is not known at compile time, the\n   // kernel assumes the worst case of `num_input_rows * num_row_elements`\n   // elements per update and uses a loop up to `send_size * num_row_elements` to\n   // terminate early.\n   size_t num_blocks_y =\n-      std::min(CeilOfRatio<size_t>(num_input_rows * num_row_elements, kThreads),\n+      std::min(CeilOfRatio<size_t>(num_input_rows * num_vectorized_row_elements,\n+                                   kThreads),\n                kMaxBlocksPerUpdate);\n \n   se::ThreadDim thread_dims(kThreads, 1, 1);\n@@ -113,13 +122,13 @@ absl::Status RunRaggedAllToAllKernel(\n \n   auto launch_kernel = [&](auto type) -> absl::Status {\n     using T = decltype(type);\n-    return LaunchTypedKernel<T>(stream, executor, thread_dims, block_dims,\n-                                input_buffer, output_ptrs, input_offsets_buffer,\n-                                send_sizes_buffer, output_offsets_buffer,\n-                                num_updates_per_output, num_row_elements);\n+    return LaunchTypedKernel<T>(\n+        stream, executor, thread_dims, block_dims, input_buffer, output_ptrs,\n+        input_offsets_buffer, send_sizes_buffer, output_offsets_buffer,\n+        num_updates_per_output, num_vectorized_row_elements);\n   };\n \n-  switch (xla::primitive_util::BitWidth(element_type)) {\n+  switch (vectorized_bitwidth) {\n     case 8:\n       return launch_kernel(uint8_t{});\n     case 16:"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 15,
        "deletions": 6
    }
}