{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 847414761",
    "sha": "f356a762f32477f212f3fe1c5c44a49b6948f083",
    "files": [
        {
            "sha": "d2b9bc71b5d3a339f4ce3c5a81b9d8a09e72b85e",
            "filename": "tensorflow/core/kernels/conv_ops_bfloat16.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f356a762f32477f212f3fe1c5c44a49b6948f083/tensorflow%2Fcore%2Fkernels%2Fconv_ops_bfloat16.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f356a762f32477f212f3fe1c5c44a49b6948f083/tensorflow%2Fcore%2Fkernels%2Fconv_ops_bfloat16.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconv_ops_bfloat16.cc?ref=f356a762f32477f212f3fe1c5c44a49b6948f083",
            "patch": "@@ -110,8 +110,8 @@ void LaunchConvOp<GPUDevice, Eigen::bfloat16>::operator()(\n     Tensor* output) {\n   // Get spatial dims for dilations and strides.\n   int spatial_dims = input.dims() - 2;\n-  gtl::InlinedVector<int64_t, 3> strides_spatial(spatial_dims);\n-  gtl::InlinedVector<int64_t, 3> dilations_spatial(spatial_dims);\n+  absl::InlinedVector<int64_t, 3UL> strides_spatial(spatial_dims);\n+  absl::InlinedVector<int64_t, 3UL> dilations_spatial(spatial_dims);\n   for (int i = 0; i < spatial_dims; ++i) {\n     strides_spatial[i] =\n         GetTensorDim(strides, data_format, static_cast<char>(i + '0'));\n@@ -166,9 +166,9 @@ void LaunchConv2DOp<GPUDevice, Eigen::bfloat16>::operator()(\n     const std::vector<int64_t>& explicit_paddings, Tensor* output,\n     TensorFormat data_format) {\n   // Cast strides and dilations.\n-  gtl::InlinedVector<int64_t, 3> casted_strides = {row_stride, col_stride};\n-  gtl::InlinedVector<int64_t, 3> casted_dilations = {row_dilation,\n-                                                     col_dilation};\n+  absl::InlinedVector<int64_t, 3UL> casted_strides = {row_stride, col_stride};\n+  absl::InlinedVector<int64_t, 3UL> casted_dilations = {row_dilation,\n+                                                        col_dilation};\n \n   auto* stream = ctx->op_device_context()->stream();\n   const bool cast_to_float = !IsBF16SupportedInOps(stream);"
        },
        {
            "sha": "02fe25ff64aa8ab5218510c028a8c332c6b5cabc",
            "filename": "tensorflow/core/kernels/conv_ops_gpu.h",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f356a762f32477f212f3fe1c5c44a49b6948f083/tensorflow%2Fcore%2Fkernels%2Fconv_ops_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f356a762f32477f212f3fe1c5c44a49b6948f083/tensorflow%2Fcore%2Fkernels%2Fconv_ops_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconv_ops_gpu.h?ref=f356a762f32477f212f3fe1c5c44a49b6948f083",
            "patch": "@@ -68,26 +68,26 @@ class DnnScratchAllocator : public se::ScratchAllocator {\n   DnnScratchAllocator(int64_t memory_limit, OpKernelContext* context)\n       : memory_limit_(memory_limit), total_byte_size_(0), context_(context) {}\n   int64 GetMemoryLimitInBytes() override { return memory_limit_; }\n-  tsl::StatusOr<se::DeviceMemory<uint8>> AllocateBytes(\n+  absl::StatusOr<stream_executor::DeviceMemory<uint8>> AllocateBytes(\n       int64_t byte_size) override {\n     Tensor temporary_memory;\n     if (byte_size < 0) {\n-      return tsl::Status{absl::StatusCode::kInvalidArgument,\n-                         \"Requested negative byte size!\"};\n+      return absl::Status{absl::StatusCode::kInvalidArgument,\n+                          \"Requested negative byte size!\"};\n     }\n     if (byte_size > memory_limit_) {\n-      return tsl::Status{absl::StatusCode::kUnavailable,\n-                         absl::StrCat(\"Requested memory size (\", byte_size,\n-                                      \") exceeds the max memory limit (\",\n-                                      memory_limit_, \").\")};\n+      return absl::Status{absl::StatusCode::kUnavailable,\n+                          absl::StrCat(\"Requested memory size (\", byte_size,\n+                                       \") exceeds the max memory limit (\",\n+                                       memory_limit_, \").\")};\n     }\n     AllocationAttributes allocation_attr;\n     allocation_attr.retry_on_failure = false;\n     Status allocation_status(context_->allocate_temp(\n         DT_UINT8, TensorShape({byte_size}), &temporary_memory,\n         AllocatorAttributes(), allocation_attr));\n     if (!allocation_status.ok()) {\n-      return tsl::Status{\n+      return absl::Status{\n           absl::StatusCode::kUnavailable,\n           absl::StrCat(\"Failed to allocate the requested memory size (\",\n                        byte_size, \").\")};\n@@ -96,7 +96,7 @@ class DnnScratchAllocator : public se::ScratchAllocator {\n     // allocator.\n     allocated_tensors_.push_back(temporary_memory);\n     total_byte_size_ += byte_size;\n-    return tsl::StatusOr<se::DeviceMemory<uint8>>(\n+    return absl::StatusOr<stream_executor::DeviceMemory<uint8>>(\n         AsDeviceMemory(temporary_memory.flat<uint8>().data(),\n                        temporary_memory.flat<uint8>().size()));\n   }\n@@ -115,7 +115,8 @@ typedef Eigen::GpuDevice GPUDevice;\n // autotuning with a cache, or by falling back to a default if\n // 'cudnn_use_autotune' is true and cuDNN is the statically-chosen DNN backend.\n template <typename T>\n-StatusOr<AutotuneEntry<se::dnn::FusedConvOp>> AutotuneFusedConv(\n+absl::StatusOr<AutotuneEntry<stream_executor::dnn::FusedConvOp>>\n+AutotuneFusedConv(\n     bool cudnn_use_autotune,\n     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::FusedConvOp>>*\n         autotune_map,\n@@ -132,7 +133,7 @@ StatusOr<AutotuneEntry<se::dnn::FusedConvOp>> AutotuneFusedConv(\n     se::DeviceMemory<T> side_input_ptr, int64_t scratch_size);\n \n template <typename T>\n-StatusOr<AutotuneEntry<se::dnn::ConvOp>> AutotuneUnfusedConv(\n+absl::StatusOr<AutotuneEntry<stream_executor::dnn::ConvOp>> AutotuneUnfusedConv(\n     bool cudnn_use_autotune,\n     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>* autotune_map,\n     const ConvParameters& conv_parameters, OpKernelContext* ctx,\n@@ -155,7 +156,7 @@ AllocateScratchOrFallback(se::ScratchAllocator* scratch_allocator,\n \n   auto workspace_size = selected_runner->GetWorkspaceSize();\n \n-  se::DeviceMemoryBase scratch_memory;\n+  stream_executor::DeviceAddressBase scratch_memory;\n   if (workspace_size > 0) {\n     auto scratch_or = scratch_allocator->AllocateBytes(workspace_size);\n     if (scratch_or.ok()) {\n@@ -206,9 +207,10 @@ Status LaunchAutotunedConv(const AutotuneEntry<se::dnn::ConvOp>& autotune_entry,\n                         AllocateScratchOrFallback<se::dnn::ConvOp::Signature>(\n                             scratch_allocator, primary, no_scratch_fallback));\n     auto& runner = *std::get<const se::dnn::ConvRunner*>(runner_and_scratch);\n-    return runner(stream, nullptr,\n-                  std::get<se::DeviceMemoryBase>(runner_and_scratch), in_ptr,\n-                  filter_ptr, out_ptr);\n+    return runner(\n+        stream, nullptr,\n+        std::get<stream_executor::DeviceAddressBase>(runner_and_scratch),\n+        in_ptr, filter_ptr, out_ptr);\n   } else {\n     auto dnn = stream->parent()->AsDnn();\n     if (dnn == nullptr) {\n@@ -231,7 +233,7 @@ Status LaunchAutotunedConv(const AutotuneEntry<se::dnn::ConvOp>& autotune_entry,\n     std::unique_ptr<const se::dnn::ConvRunner> runner =\n         std::move(runner_or).value();\n \n-    se::DeviceMemoryBase scratch_memory;\n+    stream_executor::DeviceAddressBase scratch_memory;\n     int64_t workspace_size = runner->GetWorkspaceSize();\n     if (workspace_size > 0) {\n       auto scratch_or = scratch_allocator->AllocateBytes(workspace_size);"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 23,
        "deletions": 21
    }
}