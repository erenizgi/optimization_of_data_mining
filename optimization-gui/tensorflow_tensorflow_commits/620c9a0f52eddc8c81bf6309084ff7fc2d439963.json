{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 832623430",
    "sha": "620c9a0f52eddc8c81bf6309084ff7fc2d439963",
    "files": [
        {
            "sha": "d9f6927c09ecd6f87a7657b6b23b1e33dafa6bc9",
            "filename": "tensorflow/compiler/tf2xla/const_analysis_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fconst_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fconst_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fconst_analysis_test.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -180,7 +180,7 @@ TEST(ConstAnalysisTest, RespectExplicitAttr_0) {\n   // not need to be a constant.\n   Output reshape = ops::Reshape(root, arg1, add);\n   reshape.node()->AddAttr(kXlaCompileTimeConstantInputsAttr,\n-                          std::vector<string>());\n+                          std::vector<std::string>());\n \n   Graph graph(OpRegistry::Global());\n   TF_ASSERT_OK(root.ToGraph(&graph));\n@@ -203,7 +203,7 @@ TEST(ConstAnalysisTest, RespectExplicitAttr_1) {\n \n   // Force const analysis to pretend that the first argument to `add` needs to\n   // be a constant.\n-  std::vector<string> add_constant_inputs;\n+  std::vector<std::string> add_constant_inputs;\n   add_constant_inputs.push_back(\"x\");\n   add.node()->AddAttr(kXlaCompileTimeConstantInputsAttr, add_constant_inputs);\n "
        },
        {
            "sha": "2adc83512c6617e64f5237d555f003bfad924eb1",
            "filename": "tensorflow/compiler/tf2xla/functionalize_cond.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -83,11 +83,11 @@ struct ClusterTupleLessThan {\n };\n \n // TODO(jpienaar): Move to OutputTensor.\n-string DebugString(const OutputTensor& tensor) {\n+std::string DebugString(const OutputTensor& tensor) {\n   return absl::StrCat(tensor.node->name(), \":\", tensor.index);\n }\n \n-string Branch_Name(BranchType b) {\n+std::string Branch_Name(BranchType b) {\n   switch (b) {\n     case BranchType::kElseBranch:\n       return \"else\";\n@@ -100,13 +100,13 @@ string Branch_Name(BranchType b) {\n   }\n }\n \n-string DebugString(StateMap::CondId cond_state) {\n+std::string DebugString(StateMap::CondId cond_state) {\n   if (cond_state == nullptr || cond_state->empty()) return \"{}\";\n   using value_type = StateMap::CondState::value_type;\n   return absl::StrCat(\n       \"{\",\n       absl::StrJoin(*cond_state, \", \",\n-                    [](string* output, const value_type& pred_branch) {\n+                    [](std::string* output, const value_type& pred_branch) {\n                       const OutputTensor& pred = pred_branch.first;\n                       const BranchType& branch = pred_branch.second;\n                       if (branch == BranchType::kNeither)\n@@ -200,7 +200,7 @@ struct CondArgNode {\n   explicit CondArgNode(Node* src, int src_output)\n       : src(src), src_output(src_output) {}\n \n-  string ToString() const {\n+  std::string ToString() const {\n     return absl::StrCat(\"src=\", src->name(), \":\", src_output,\n                         \" switches=\", NodesToString(switches));\n   }\n@@ -212,11 +212,11 @@ struct CondArgNode {\n };\n using CondArgNodes = std::vector<CondArgNode>;\n \n-string DebugString(const CondArgNodes& nodes) {\n+std::string DebugString(const CondArgNodes& nodes) {\n   return absl::StrCat(\n       \"[\",\n       absl::StrJoin(nodes, \", \",\n-                    [](string* output, const CondArgNode& node) {\n+                    [](std::string* output, const CondArgNode& node) {\n                       absl::StrAppend(output, node.ToString());\n                     }),\n       \"]\");\n@@ -263,20 +263,20 @@ void StateMap::ResetAncestorId(const Node* node, StateMap::AncestorId id) {\n \n void StateMap::MarkDead(const Node* node) { ResetCondId(node, dead_id_); }\n \n-string StateMap::CondStateToString(const Node* node) const {\n+std::string StateMap::CondStateToString(const Node* node) const {\n   return CondStateToString(LookupCondId(node));\n }\n \n-string StateMap::CondStateToString(StateMap::CondId id) const {\n+std::string StateMap::CondStateToString(StateMap::CondId id) const {\n   return DebugString(id);\n }\n \n-string StateMap::AncestorStateToString(const Node* node) const {\n+std::string StateMap::AncestorStateToString(const Node* node) const {\n   if (auto id = LookupAncestorId(node)) {\n     return absl::StrCat(\n         \"{\",\n         absl::StrJoin(*id, \",\",\n-                      [](string* output, const AncestorNode& ancestor) {\n+                      [](std::string* output, const AncestorNode& ancestor) {\n                         absl::StrAppend(output,\n                                         ancestor.output_tensor.node->name(),\n                                         \":\", ancestor.output_tensor.index);\n@@ -340,7 +340,7 @@ class Conditional {\n \n   // Internal name of conditional. The name is based on the first merge node\n   // added.\n-  string name() const;\n+  std::string name() const;\n \n   // The FunctionalizeCond instance that created this.\n   FunctionalizeCond* parent_;\n@@ -751,7 +751,7 @@ absl::Status Conditional::BuildIfNode(Graph* graph,\n   VLOG(2) << \"Build cond function for \" << name();\n   NodeDebugInfo debug_info((*merges_.begin())->def());\n   NodeDefBuilder builder(name(), \"If\", library, &debug_info);\n-  const string branch_name[] = {\"else_branch\", \"then_branch\"};\n+  const std::string branch_name[] = {\"else_branch\", \"then_branch\"};\n   for (auto branch : {BranchType::kElseBranch, BranchType::kThenBranch}) {\n     int branch_index = static_cast<int>(branch);\n \n@@ -817,7 +817,7 @@ absl::Status Conditional::BuildIfNode(Graph* graph,\n   builder.Attr(\"Tcond\", DT_BOOL);\n   // Add some internal attributes which need to be propagated.\n   for (absl::string_view attr_name : kAttrsToPropagate) {\n-    string attr_val;\n+    std::string attr_val;\n     if (GetNodeAttr(predicate_.node->def(), attr_name, &attr_val).ok()) {\n       builder.Attr(attr_name, attr_val);\n     }\n@@ -949,7 +949,7 @@ absl::Status Conditional::BuildAndReplace(\n   return absl::OkStatus();\n }\n \n-string Conditional::name() const {\n+std::string Conditional::name() const {\n   CHECK(!merges_.empty());\n   return absl::StrCat((*merges_.begin())->name(), \"_if\");\n }\n@@ -958,7 +958,7 @@ absl::Status FunctionalizeCond::AddIdentityNode(const Node* replacee,\n                                                 Node* if_node, int port) {\n   NodeBuilder id_builder(replacee->name(), \"Identity\");\n   id_builder.Input(if_node, port);\n-  string outside_compilation;\n+  std::string outside_compilation;\n   if (GetNodeAttr(if_node->def(), kXlaOutsideCompilationAttr,\n                   &outside_compilation)\n           .ok()) {\n@@ -1580,7 +1580,7 @@ absl::Status FunctionalizeCond::FunctionalizeInternal() {\n   return absl::OkStatus();\n }\n \n-void FunctionalizeCond::DumpGraphWithCondState(const string& name) {\n+void FunctionalizeCond::DumpGraphWithCondState(const std::string& name) {\n   const char* const kCondGroupDebugAttr = \"_XlaFunctionalizeCondGroup\";\n \n   for (Node* n : graph_->nodes()) {"
        },
        {
            "sha": "25d773ad50a10587cdb65a2cee4e8d1dd8826797",
            "filename": "tensorflow/compiler/tf2xla/functionalize_cond.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond.h?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -136,11 +136,11 @@ class StateMap {\n   BranchType FindBranchOf(CondId id, OutputTensor predicate) const;\n \n   // Returns textual representation of node's CondState.\n-  string CondStateToString(const Node* node) const;\n-  string CondStateToString(CondId id) const;\n+  std::string CondStateToString(const Node* node) const;\n+  std::string CondStateToString(CondId id) const;\n \n   // Returns textual representation of node's AncestorState.\n-  string AncestorStateToString(const Node* node) const;\n+  std::string AncestorStateToString(const Node* node) const;\n \n   // Returns whether the cond state is the dead state.\n   bool IsDead(CondId id) const;\n@@ -201,7 +201,7 @@ class FunctionalizeCond {\n   absl::Status PropagateUpdatedState(const Node* replacee);\n \n   // Dump graph with the CondState annotated.\n-  void DumpGraphWithCondState(const string& name);\n+  void DumpGraphWithCondState(const std::string& name);\n \n   // Adds `switch_id` to the list of Switch node ids.\n   void AddSwitchId(int switch_id);"
        },
        {
            "sha": "edb2a7e0ea1b33ee01148170b8776a2205b3c4ca",
            "filename": "tensorflow/compiler/tf2xla/functionalize_cond_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_cond_test.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -48,7 +48,7 @@ class FunctionalizeCondTest : public ::testing::Test {\n     return fc_->state_map_.GetCondId(state);\n   }\n \n-  string GetString(const StateMap::StateMap::CondId id) {\n+  std::string GetString(const StateMap::StateMap::CondId id) {\n     return fc_->state_map_.CondStateToString(id);\n   }\n "
        },
        {
            "sha": "22b9b9187ecd7d05d9256a4b25125d6962540188",
            "filename": "tensorflow/compiler/tf2xla/functionalize_control_flow.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 20,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -51,8 +51,9 @@ namespace tensorflow {\n // Maps function name to\n // - new function name, if the function body was functionalized\n // - std::nullopt, if not\n-using FuncMap = std::map<string, std::optional<string>>;\n-using FuncMapIter = std::map<string, std::optional<string>>::const_iterator;\n+using FuncMap = std::map<std::string, std::optional<std::string>>;\n+using FuncMapIter =\n+    std::map<std::string, std::optional<std::string>>::const_iterator;\n \n // Returns whether function has been processed before.\n bool FunctionHasBeenProcessed(FuncMapIter func_iter, const FuncMap* func_map) {\n@@ -65,8 +66,8 @@ bool FunctionHasBeenModified(FuncMapIter func_iter) {\n }\n \n // Returns a name for the new functionalized version of a function.\n-string GetNewFunctionName(\n-    const string& func_name, Node* n,\n+std::string GetNewFunctionName(\n+    const std::string& func_name, Node* n,\n     AssociatedFunctionInfo::AssociatedFunctionType func_type,\n     FunctionLibraryDefinition* fld) {\n   // For SymbolicGradient, `func_name` is always \"SymbolicGradient\" which\n@@ -79,14 +80,15 @@ string GetNewFunctionName(\n }\n \n // Returns name to which a modified function has been mapped.\n-const string& GetMappedFunctionName(FuncMapIter func_iter) {\n+const std::string& GetMappedFunctionName(FuncMapIter func_iter) {\n   DCHECK(func_iter->second.has_value());\n   return func_iter->second.value();\n }\n \n // Updates `func_map` with function given by `canonicalized_name`.\n-void UpdateFunctionMap(FuncMap* func_map, const string& canonicalized_name,\n-                       const string& new_func_name, bool function_modified) {\n+void UpdateFunctionMap(FuncMap* func_map, const std::string& canonicalized_name,\n+                       const std::string& new_func_name,\n+                       bool function_modified) {\n   // If function was modified store its new name, otherwise add empty entry to\n   // record that function has been processed and does not need to be rewritten.\n   (*func_map)[canonicalized_name] =\n@@ -95,8 +97,9 @@ void UpdateFunctionMap(FuncMap* func_map, const string& canonicalized_name,\n \n // Adds new function def to graph's function library if necessary.\n absl::Status AddFunctionDefToGraphLibrary(\n-    const string& func_name, const AssociatedFunctionInfo& associated_function,\n-    Graph* graph, FunctionLibraryDefinition* fld) {\n+    const std::string& func_name,\n+    const AssociatedFunctionInfo& associated_function, Graph* graph,\n+    FunctionLibraryDefinition* fld) {\n   const OpRegistrationData* op_reg_data;\n   // We have to be careful with adding the function def since there are three\n   // different `OpRegistryInterface`s involved here:\n@@ -129,8 +132,8 @@ absl::Status AddFunctionDefToGraphLibrary(\n \n // Functionalizes function given by `func_name`. Update `func_map` accordingly.\n absl::Status FunctionalizeControlFlowForFunction(\n-    const string& func_name, const string& new_func_name,\n-    const protobuf::Map<string, tensorflow::AttrValue>& attrs,\n+    const std::string& func_name, const std::string& new_func_name,\n+    const protobuf::Map<std::string, tensorflow::AttrValue>& attrs,\n     FunctionLibraryDefinition* fld, FunctionLibraryRuntime* flr,\n     FuncMap* func_map, bool* function_modified,\n     const NodeFilter& node_filter = {});\n@@ -165,11 +168,11 @@ absl::Status FunctionalizeControlFlowForNodeAssociatedFunctions(\n              associated_functions.size() == 1);\n \n       // Process one node-function-pair.\n-      string func_name = associated_function.func_name();\n-      string canonicalized_name =\n+      std::string func_name = associated_function.func_name();\n+      std::string canonicalized_name =\n           Canonicalize(func_name, AttrSlice(&associated_function.attrs()));\n       auto func_iter = func_map->find(canonicalized_name);\n-      string new_func_name;\n+      std::string new_func_name;\n       if (FunctionHasBeenProcessed(func_iter, func_map)) {\n         if (FunctionHasBeenModified(func_iter)) {\n           *any_function_modified = true;\n@@ -202,8 +205,8 @@ absl::Status FunctionalizeControlFlowForNodeAssociatedFunctions(\n }\n \n absl::Status FunctionalizeControlFlowForFunction(\n-    const string& func_name, const string& new_func_name,\n-    const protobuf::Map<string, tensorflow::AttrValue>& attrs,\n+    const std::string& func_name, const std::string& new_func_name,\n+    const protobuf::Map<std::string, tensorflow::AttrValue>& attrs,\n     FunctionLibraryDefinition* fld, FunctionLibraryRuntime* flr,\n     FuncMap* func_map, bool* function_modified, const NodeFilter& node_filter) {\n   *function_modified = false;\n@@ -341,8 +344,8 @@ absl::Status FunctionalizeControlFlowForXlaPass::Run(\n   // Find XLA compile ops and its corresponding FunctionDef.\n   // TPUCompile op is not in the map because graph rewriting might happen\n   // multiple times, and we want to avoid functionalize it again.\n-  static std::map<string, string>* kNodeTypeToFunctionAttrMapping =\n-      new std::map<string, string>{\n+  static std::map<std::string, std::string>* kNodeTypeToFunctionAttrMapping =\n+      new std::map<std::string, std::string>{\n           // _TPUReplicate ops are generated by EncapsulateTPUComputationsPass.\n           {\"_TPUReplicate\", \"computation\"},\n           // XlaLaunch ops are generated by EncapsulateXlaComputationsPass.\n@@ -355,12 +358,12 @@ absl::Status FunctionalizeControlFlowForXlaPass::Run(\n     if (it == kNodeTypeToFunctionAttrMapping->end()) {\n       continue;\n     }\n-    const string func_attr = it->second;\n+    const std::string func_attr = it->second;\n     NameAttrList func;\n     TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), func_attr, &func));\n     VLOG(2) << \"Graph has node \" << n->type_string()\n             << \". Corresponding function: \" << func.name();\n-    string new_func_name = options.flib_def->UniqueFunctionName(\n+    std::string new_func_name = options.flib_def->UniqueFunctionName(\n         absl::StrCat(func.name(), \"_f15n_\"));\n     bool modified;\n     TF_RETURN_IF_ERROR(FunctionalizeControlFlowForFunction("
        },
        {
            "sha": "24fe7f5e13e7e05ed65269af8c8d9f6b0253d999",
            "filename": "tensorflow/compiler/tf2xla/functionalize_control_flow_test.cc",
            "status": "modified",
            "additions": 62,
            "deletions": 57,
            "changes": 119,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_test.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -46,7 +46,7 @@ namespace {\n \n // Returns the names of the \"then\" and \"else\" functions for the If node in a\n // graph.\n-absl::Status FindIfThenAndElse(const GraphDef& graph, string* op_name,\n+absl::Status FindIfThenAndElse(const GraphDef& graph, std::string* op_name,\n                                NameAttrList* then_fn, NameAttrList* else_fn) {\n   for (const NodeDef& node : graph.node()) {\n     if (node.op() == \"If\") {\n@@ -97,7 +97,7 @@ INSTANTIATE_TEST_SUITE_P(\n            info) {\n       bool restrict_to_tpu_nodes = std::get<0>(info.param);\n       bool wrap_cond_in_function = std::get<1>(info.param);\n-      string name =\n+      std::string name =\n           absl::StrCat(restrict_to_tpu_nodes ? \"with_filter\" : \"without_filter\",\n                        wrap_cond_in_function ? \"_in_function\" : \"_in_graph\");\n       return name;\n@@ -114,15 +114,15 @@ void ConditionalTestFixture::BuildCondGraph(Graph* cond_graph) {\n \n     auto identity_t =\n         ops::Identity(scope.WithOpName(\"cond/Identity\"), switch_1.output_true);\n-    auto seventeen = ops::Const<int32>(\n+    auto seventeen = ops::Const<int32_t>(\n         scope.WithOpName(\"cond\").WithControlDependencies(identity_t), 17);\n     auto switch_2 = ops::Switch(scope.WithOpName(\"cond/Switch\"), y, less);\n     auto mul = ops::Multiply(scope.WithOpName(\"cond/Mul\"), switch_2.output_true,\n                              seventeen);\n \n     auto identity_f =\n         ops::Identity(scope.WithOpName(\"cond/Identity\"), switch_1.output_false);\n-    auto twenty_three = ops::Const<int32>(\n+    auto twenty_three = ops::Const<int32_t>(\n         scope.WithOpName(\"cond\").WithControlDependencies(identity_f), 23);\n     auto switch_3 = ops::Switch(scope.WithOpName(\"cond/Switch\"), x, less);\n     auto add = ops::Add(scope.WithOpName(\"cond/false/add\"),\n@@ -146,7 +146,7 @@ void ConditionalTestFixture::BuildCondGraph(Graph* cond_graph) {\n \n void ConditionalTestFixture::CheckGraphDef(\n     const GraphDef& graph_def, const FunctionLibraryDefinition& library) {\n-  string op_name;\n+  std::string op_name;\n   NameAttrList then_fn;\n   NameAttrList else_fn;\n   TF_EXPECT_OK(FindIfThenAndElse(graph_def, &op_name, &then_fn, &else_fn));\n@@ -285,7 +285,7 @@ void ConditionalTestFixture::RunTest() {\n     FunctionLibraryRuntime::Handle handle;\n \n     // Functionalized function name is the type string of `cond_node`.\n-    string func_name;\n+    std::string func_name;\n     for (Node* n : graph.nodes()) {\n       if (n->name() == \"cond_node\") {\n         func_name = n->type_string();\n@@ -341,7 +341,7 @@ TEST(FunctionalizeControlFlow, OneLoopVar) {\n         ops::internal::Enter(scope.WithOpName(\"while/Enter2\"), source, \"aloop\");\n     auto merge = ops::Merge(scope.WithOpName(\"while/Merge\"),\n                             std::initializer_list<Input>{enter, dummy});\n-    auto ten = ops::Const<int32>(\n+    auto ten = ops::Const<int32_t>(\n         scope.WithOpName(\"while/Less/y\").WithControlDependencies(merge.output),\n         10);\n     auto less = ops::Less(scope.WithOpName(\"while/Less\"), merge.output, ten);\n@@ -352,7 +352,7 @@ TEST(FunctionalizeControlFlow, OneLoopVar) {\n                                     switch_.output_false);\n     auto identity =\n         ops::Identity(scope.WithOpName(\"while/Identity\"), switch_.output_true);\n-    auto one = ops::Const<int32>(\n+    auto one = ops::Const<int32_t>(\n         scope.WithOpName(\"while/add/y\").WithControlDependencies(identity), 1);\n     auto add = ops::Add(scope.WithOpName(\"while/add\"), identity, one);\n     auto next_iteration =\n@@ -405,7 +405,7 @@ TEST(FunctionalizeControlFlow, OneLoopVar) {\n     {\n       Scope scope = Scope::NewRootScope().ExitOnError();\n       auto arg = ops::_Arg(scope.WithOpName(\"arg0\"), DT_INT32, 0);\n-      auto ten = ops::Const<int32>(\n+      auto ten = ops::Const<int32_t>(\n           scope.WithOpName(\"while/Less/y\").WithControlDependencies(arg), 10);\n       auto less = ops::Less(scope.WithOpName(\"while/Less\"), arg, ten);\n       auto retval = ops::_Retval(scope.WithOpName(\"retval0_RetVal\"), less, 0);\n@@ -427,7 +427,7 @@ TEST(FunctionalizeControlFlow, OneLoopVar) {\n       Scope scope = Scope::NewRootScope().ExitOnError();\n       auto arg = ops::_Arg(scope.WithOpName(\"arg0\"), DT_INT32, 0);\n       auto identity = ops::Identity(scope.WithOpName(\"while/Identity\"), arg);\n-      auto one = ops::Const<int32>(\n+      auto one = ops::Const<int32_t>(\n           scope.WithOpName(\"while/add/y\").WithControlDependencies(identity), 1);\n       auto add = ops::Add(scope.WithOpName(\"while/add\"), identity, one);\n       auto retval = ops::_Retval(scope.WithOpName(\"retval0_RetVal\"), add, 0);\n@@ -463,7 +463,8 @@ FunctionDef GetNoinlineFunctionDef() {\n //   return [x + 1]\n // Define the above function, and add it to the given graph. It's used as the\n // while loop body in NoinlineLoopBody test.\n-absl::Status AddNoinlineFunctionToGraph(const string& node_name, Graph* graph) {\n+absl::Status AddNoinlineFunctionToGraph(const std::string& node_name,\n+                                        Graph* graph) {\n   FunctionDefLibrary fdef_lib;\n   *(fdef_lib.add_function()) = GetNoinlineFunctionDef();\n   TF_RETURN_IF_ERROR(graph->AddFunctionLibrary(fdef_lib));\n@@ -481,7 +482,7 @@ absl::Status AddNoinlineFunctionToGraph(const string& node_name, Graph* graph) {\n // x = array_ops.placeholder(dtypes.int32)\n // y = control_flow_ops.while_loop(lambda i: i < 10, increment_fn, [x])\n TEST(FunctionalizeControlFlow, NoinlineLoopBody) {\n-  const string& noinline_node_name = \"while/increment_fn\";\n+  const std::string& noinline_node_name = \"while/increment_fn\";\n   Graph graph(OpRegistry::Global());\n   {\n     Scope scope = Scope::NewRootScope().ExitOnError();\n@@ -491,7 +492,7 @@ TEST(FunctionalizeControlFlow, NoinlineLoopBody) {\n                                       \"while/while_context\");\n     auto merge = ops::Merge(scope.WithOpName(\"while/Merge\"),\n                             std::initializer_list<Input>{enter, dummy});\n-    auto ten = ops::Const<int32>(\n+    auto ten = ops::Const<int32_t>(\n         scope.WithOpName(\"while/Less/y\").WithControlDependencies(merge.output),\n         10);\n     auto less = ops::Less(scope.WithOpName(\"while/Less\"), merge.output, ten);\n@@ -585,7 +586,7 @@ TEST(FunctionalizeControlFlow, NoinlineLoopBody) {\n }\n \n TEST(FunctionalizeControlFlow, MissingFunctionDefInLibrary) {\n-  const string& noinline_node_name = \"while/increment_fn\";\n+  const std::string& noinline_node_name = \"while/increment_fn\";\n   Graph graph(OpRegistry::Global());\n   {\n     Scope scope = Scope::NewRootScope().ExitOnError();\n@@ -622,7 +623,7 @@ TEST(FunctionalizeControlFlow, OneLoopVarWithoutExit) {\n         ops::internal::Enter(scope.WithOpName(\"while/Enter\"), source, \"aloop\");\n     auto merge = ops::Merge(scope.WithOpName(\"while/Merge\"),\n                             std::initializer_list<Input>{enter, dummy});\n-    auto ten = ops::Const<int32>(\n+    auto ten = ops::Const<int32_t>(\n         scope.WithOpName(\"while/Less/y\").WithControlDependencies(merge.output),\n         10);\n     auto less = ops::Less(scope.WithOpName(\"while/Less\"), merge.output, ten);\n@@ -631,7 +632,7 @@ TEST(FunctionalizeControlFlow, OneLoopVarWithoutExit) {\n         ops::Switch(scope.WithOpName(\"while/Switch\"), merge.output, loop_cond);\n     auto identity =\n         ops::Identity(scope.WithOpName(\"while/Identity\"), switch_.output_true);\n-    auto one = ops::Const<int32>(\n+    auto one = ops::Const<int32_t>(\n         scope.WithOpName(\"while/add/y\").WithControlDependencies(identity), 1);\n     auto add = ops::Add(scope.WithOpName(\"while/add\"), identity, one);\n     auto next_iteration =\n@@ -673,7 +674,7 @@ TEST(FunctionalizeControlFlow, OneLoopVarWithoutExit) {\n     {\n       Scope scope = Scope::NewRootScope().ExitOnError();\n       auto arg = ops::_Arg(scope.WithOpName(\"arg0\"), DT_INT32, 0);\n-      auto ten = ops::Const<int32>(\n+      auto ten = ops::Const<int32_t>(\n           scope.WithOpName(\"while/Less/y\").WithControlDependencies(arg), 10);\n       auto less = ops::Less(scope.WithOpName(\"while/Less\"), arg, ten);\n       auto retval = ops::_Retval(scope.WithOpName(\"retval0_RetVal\"), less, 0);\n@@ -695,7 +696,7 @@ TEST(FunctionalizeControlFlow, OneLoopVarWithoutExit) {\n       Scope scope = Scope::NewRootScope().ExitOnError();\n       auto arg = ops::_Arg(scope.WithOpName(\"arg0\"), DT_INT32, 0);\n       auto identity = ops::Identity(scope.WithOpName(\"while/Identity\"), arg);\n-      auto one = ops::Const<int32>(\n+      auto one = ops::Const<int32_t>(\n           scope.WithOpName(\"while/add/y\").WithControlDependencies(identity), 1);\n       auto add = ops::Add(scope.WithOpName(\"while/add\"), identity, one);\n       auto retval = ops::_Retval(scope.WithOpName(\"retval0_RetVal\"), add, 0);\n@@ -739,14 +740,15 @@ TEST(FunctionalizeControlFlow, TwoLoopVars) {\n                               std::initializer_list<Input>{enter_y, dummy});\n \n     // Loop condition\n-    auto three = ops::Const<int32>(scope.WithOpName(\"while/cond/three\")\n-                                       .WithControlDependencies(merge_x.output),\n-                                   3);\n+    auto three =\n+        ops::Const<int32_t>(scope.WithOpName(\"while/cond/three\")\n+                                .WithControlDependencies(merge_x.output),\n+                            3);\n     auto cond_add =\n         ops::Add(scope.WithOpName(\"while/cond/Add\"), merge_x.output, three);\n-    auto ten = ops::Const<int32>(scope.WithOpName(\"while/cond/ten\")\n-                                     .WithControlDependencies(merge_x.output),\n-                                 10);\n+    auto ten = ops::Const<int32_t>(scope.WithOpName(\"while/cond/ten\")\n+                                       .WithControlDependencies(merge_x.output),\n+                                   10);\n     auto less = ops::Less(scope.WithOpName(\"while/cond/Less\"), cond_add, ten);\n     auto loop_cond = ops::LoopCond(scope.WithOpName(\"while/LoopCond\"), less);\n \n@@ -765,10 +767,10 @@ TEST(FunctionalizeControlFlow, TwoLoopVars) {\n     auto identity_y = ops::Identity(scope.WithOpName(\"while/Identity/y\"),\n                                     switch_y.output_true);\n \n-    auto one = ops::Const<int32>(\n+    auto one = ops::Const<int32_t>(\n         scope.WithOpName(\"while/add/one\").WithControlDependencies(identity_x),\n         1);\n-    auto two = ops::Const<int32>(\n+    auto two = ops::Const<int32_t>(\n         scope.WithOpName(\"while/mul/two\").WithControlDependencies(identity_x),\n         2);\n \n@@ -825,14 +827,15 @@ TEST(FunctionalizeControlFlow, TwoLoopVars) {\n       Scope scope = Scope::NewRootScope().ExitOnError();\n       auto arg0 = ops::_Arg(scope.WithOpName(\"arg0\"), DT_INT32, 0);\n       auto arg1 = ops::_Arg(scope.WithOpName(\"arg1\"), DT_INT32, 1);\n-      auto three = ops::Const<int32>(scope.WithOpName(\"while/cond/three\")\n-                                         .WithControlDependencies(arg0.output),\n-                                     3);\n+      auto three =\n+          ops::Const<int32_t>(scope.WithOpName(\"while/cond/three\")\n+                                  .WithControlDependencies(arg0.output),\n+                              3);\n       auto cond_add =\n           ops::Add(scope.WithOpName(\"while/cond/Add\"), arg0.output, three);\n-      auto ten = ops::Const<int32>(scope.WithOpName(\"while/cond/ten\")\n-                                       .WithControlDependencies(arg0.output),\n-                                   10);\n+      auto ten = ops::Const<int32_t>(scope.WithOpName(\"while/cond/ten\")\n+                                         .WithControlDependencies(arg0.output),\n+                                     10);\n       auto less = ops::Less(scope.WithOpName(\"while/cond/Less\"), cond_add, ten);\n       auto retval = ops::_Retval(scope.WithOpName(\"retval0_RetVal\"), less, 0);\n \n@@ -859,10 +862,10 @@ TEST(FunctionalizeControlFlow, TwoLoopVars) {\n       auto identity_y =\n           ops::Identity(scope.WithOpName(\"while/Identity/y\"), arg1);\n \n-      auto one = ops::Const<int32>(\n+      auto one = ops::Const<int32_t>(\n           scope.WithOpName(\"while/add/one\").WithControlDependencies(identity_x),\n           1);\n-      auto two = ops::Const<int32>(\n+      auto two = ops::Const<int32_t>(\n           scope.WithOpName(\"while/mul/two\").WithControlDependencies(identity_x),\n           2);\n \n@@ -922,15 +925,15 @@ INSTANTIATE_TEST_SUITE_P(\n       bool mark_inner_loop_tpu = std::get<1>(info.param);\n       bool mark_outer_loop_tpu = std::get<2>(info.param);\n \n-      string node_string;\n+      std::string node_string;\n       if (mark_inner_loop_tpu && mark_outer_loop_tpu)\n         node_string = \"both_loops_tpu\";\n       else if (!mark_inner_loop_tpu && !mark_outer_loop_tpu)\n         node_string = \"no_loop_tpu\";\n       else\n         node_string = mark_inner_loop_tpu ? \"inner_loop_tpu\" : \"outer_loop_tpu\";\n \n-      string name = absl::StrCat(\n+      std::string name = absl::StrCat(\n           restrict_to_tpu_nodes ? \"restricted_\" : \"unrestricted_\", node_string);\n       return name;\n     });\n@@ -961,21 +964,21 @@ void ComplexTestFixture::RunTest() {\n     auto dummy = ops::Placeholder(scope.WithOpName(\"Dummy\"), DT_INT32);\n \n     auto x = ops::Placeholder(scope.WithOpName(\"x\"), DT_INT32);\n-    auto three = ops::Const<int32>(scope.WithOpName(\"three\"), 3);\n+    auto three = ops::Const<int32_t>(scope.WithOpName(\"three\"), 3);\n     auto y = ops::Add(scope.WithOpName(\"y\"), x, three);\n \n     auto var = ops::VarHandleOp(scope.WithOpName(\"Variable\"), DT_INT32,\n                                 TensorShape({}));\n \n     // Outer loop\n-    auto zero = ops::Const<int32>(scope.WithOpName(\"outer/Const\"), 0);\n+    auto zero = ops::Const<int32_t>(scope.WithOpName(\"outer/Const\"), 0);\n     auto enter_i =\n         ops::internal::Enter(scope.WithOpName(\"outer/Enter_i\"), zero, \"outer\");\n     auto merge_i = ops::Merge(scope.WithOpName(\"outer/Merge_i\"),\n                               std::initializer_list<Input>{enter_i, dummy});\n-    auto ten = ops::Const<int32>(scope.WithOpName(\"outer/Less/y\")\n-                                     .WithControlDependencies(merge_i.output),\n-                                 10);\n+    auto ten = ops::Const<int32_t>(scope.WithOpName(\"outer/Less/y\")\n+                                       .WithControlDependencies(merge_i.output),\n+                                   10);\n     auto less_i =\n         ops::Less(scope.WithOpName(\"outer/Less_i\"), merge_i.output, ten);\n     auto outer_loop_cond =\n@@ -998,7 +1001,7 @@ void ComplexTestFixture::RunTest() {\n                              ops::internal::Enter::Attrs().IsConstant(true));\n \n     // Inner loop\n-    auto one_j = ops::Const<int32>(\n+    auto one_j = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/j\").WithControlDependencies(identity_i), 1);\n     auto enter_j = ops::internal::Enter(scope.WithOpName(\"outer/inner/Enter_j\"),\n                                         one_j, \"inner\");\n@@ -1018,9 +1021,10 @@ void ComplexTestFixture::RunTest() {\n     auto merge_k = ops::Merge(scope.WithOpName(\"outer/inner/Merge_k\"),\n                               std::initializer_list<Input>{enter_k, dummy});\n \n-    auto five = ops::Const<int32>(scope.WithOpName(\"outer/inner/Five\")\n-                                      .WithControlDependencies(merge_j.output),\n-                                  5);\n+    auto five =\n+        ops::Const<int32_t>(scope.WithOpName(\"outer/inner/Five\")\n+                                .WithControlDependencies(merge_j.output),\n+                            5);\n     auto less_j =\n         ops::Less(scope.WithOpName(\"outer/inner/Less_j\"), merge_j.output, five);\n     auto loop_cond =\n@@ -1047,7 +1051,7 @@ void ComplexTestFixture::RunTest() {\n     auto assign = ops::AssignAddVariableOp(\n         scope.WithOpName(\"outer/inner/assign_add\"), enter_var, add_jkx);\n \n-    auto one = ops::Const<int32>(\n+    auto one = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/inner/One\")\n             .WithControlDependencies(\n                 absl::Span<const Operation>{assign.operation}),\n@@ -1061,7 +1065,7 @@ void ComplexTestFixture::RunTest() {\n         scope.WithOpName(\"outer/inner/NextIteration_k\"), identity_k);\n \n     // Body and backedge for outer loop.\n-    auto one_outer = ops::Const<int32>(\n+    auto one_outer = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/add/y\").WithControlDependencies(identity_i), 1);\n     auto add_i =\n         ops::Add(scope.WithOpName(\"outer/add\")\n@@ -1086,9 +1090,10 @@ void ComplexTestFixture::RunTest() {\n   }\n   // Add '_tpu_replicate' attributes as specified.\n   for (Node* n : graph.nodes()) {\n-    string name = n->name();\n-    bool is_inner_node = name.find(\"outer/inner/\") != string::npos;\n-    bool is_outer_node = !is_inner_node && name.find(\"outer/\") != string::npos;\n+    std::string name = n->name();\n+    bool is_inner_node = name.find(\"outer/inner/\") != std::string::npos;\n+    bool is_outer_node =\n+        !is_inner_node && name.find(\"outer/\") != std::string::npos;\n     if ((is_inner_node && mark_inner_loop_tpu_) ||\n         (is_outer_node && mark_outer_loop_tpu_)) {\n       n->AddAttr(\"_tpu_replicate\", \"cluster\");\n@@ -1159,13 +1164,13 @@ void ComplexTestFixture::CheckOuterNodesFunctionalized(\n   {\n     Scope scope = Scope::NewRootScope().ExitOnError();\n     auto x = ops::Placeholder(scope.WithOpName(\"x\"), DT_INT32);\n-    auto three = ops::Const<int32>(scope.WithOpName(\"three\"), 3);\n+    auto three = ops::Const<int32_t>(scope.WithOpName(\"three\"), 3);\n     auto y = ops::Add(scope.WithOpName(\"y\"), x, three);\n \n     auto var = ops::VarHandleOp(scope.WithOpName(\"Variable\"), DT_INT32,\n                                 TensorShape({}));\n \n-    auto zero = ops::Const<int32>(scope.WithOpName(\"outer/Const\"), 0);\n+    auto zero = ops::Const<int32_t>(scope.WithOpName(\"outer/Const\"), 0);\n \n     auto while_op = ops::While(scope.WithOpName(\"outer/LoopCond\"),\n                                std::initializer_list<Input>{zero, y, x, var},\n@@ -1184,7 +1189,7 @@ void ComplexTestFixture::CheckOuterNodesFunctionalized(\n     auto arg2 = ops::_Arg(scope.WithOpName(\"arg2\"), DT_INT32, 2);\n     auto arg3 = ops::_Arg(scope.WithOpName(\"arg3\"), DT_RESOURCE, 3);\n \n-    auto ten = ops::Const<int32>(\n+    auto ten = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/Less/y\").WithControlDependencies(arg0.output),\n         10);\n     auto less = ops::Less(scope.WithOpName(\"outer/Less_i\"), arg0, ten);\n@@ -1220,14 +1225,14 @@ void ComplexTestFixture::CheckOuterNodesFunctionalized(\n     auto arg3 = ops::_Arg(scope.WithOpName(\"arg3\"), DT_RESOURCE, 3);\n \n     auto identity_i = ops::Identity(scope.WithOpName(\"outer/Identity\"), arg0);\n-    auto one_j = ops::Const<int32>(\n+    auto one_j = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/j\").WithControlDependencies(identity_i), 1);\n     auto while_op =\n         ops::While(scope.WithOpName(\"outer/inner/LoopCond\"),\n                    std::initializer_list<Input>{one_j, arg1, arg2, arg3},\n                    inner_cond_fn, inner_body_fn);\n \n-    auto one_outer = ops::Const<int32>(\n+    auto one_outer = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/add/y\").WithControlDependencies(identity_i), 1);\n     auto add_i =\n         ops::Add(scope.WithOpName(\"outer/add\")\n@@ -1262,7 +1267,7 @@ void ComplexTestFixture::CheckInnerNodesFunctionalized(\n     auto arg2 = ops::_Arg(scope.WithOpName(\"arg2\"), DT_INT32, 2);\n     auto arg3 = ops::_Arg(scope.WithOpName(\"arg3\"), DT_RESOURCE, 3);\n \n-    auto five = ops::Const<int32>(\n+    auto five = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/inner/Five\").WithControlDependencies(arg0), 5);\n     auto less_j = ops::Less(scope.WithOpName(\"outer/inner/Less_j\"), arg0, five);\n     auto retval = ops::_Retval(scope.WithOpName(\"retval0_RetVal\"), less_j, 0);\n@@ -1299,7 +1304,7 @@ void ComplexTestFixture::CheckInnerNodesFunctionalized(\n     auto assign = ops::AssignAddVariableOp(\n         scope.WithOpName(\"outer/inner/assign_add\"), arg3, add_jkx);\n \n-    auto one = ops::Const<int32>(\n+    auto one = ops::Const<int32_t>(\n         scope.WithOpName(\"outer/inner/One\")\n             .WithControlDependencies(\n                 absl::Span<const Operation>{assign.operation}),"
        },
        {
            "sha": "d8558e7fb2b5fe1bd8b1af5dfc2727c3047311c1",
            "filename": "tensorflow/compiler/tf2xla/functionalize_control_flow_util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_util.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -42,7 +42,7 @@ absl::StatusOr<Node*> BuildRetvalNode(Graph* graph, DataType type, int index) {\n \n absl::Status ExtractWhileLoopFrames(\n     const std::vector<ControlFlowInfo>& cf_info, const Graph* graph,\n-    std::unordered_map<string, WhileLoopFrame>* frames,\n+    std::unordered_map<std::string, WhileLoopFrame>* frames,\n     const NodeFilter& node_filter) {\n   for (Node* node : graph->op_nodes()) {\n     const ControlFlowInfo& cf = cf_info[node->id()];"
        },
        {
            "sha": "90c50f75e36387e7653c0ca07849dfa0b5f103ba",
            "filename": "tensorflow/compiler/tf2xla/functionalize_control_flow_util.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_control_flow_util.h?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -47,7 +47,7 @@ struct WhileLoopArg {\n \n // Information about a loop frame.\n struct WhileLoopFrame {\n-  string name;\n+  std::string name;\n \n   // Pointer to the parent frame. The root frame has a pointer to itself.\n   WhileLoopFrame* parent = nullptr;\n@@ -76,7 +76,7 @@ struct WhileLoopFrame {\n // `FunctionalizeControlFlow` for more details about node filters).\n absl::Status ExtractWhileLoopFrames(\n     const std::vector<ControlFlowInfo>& cf_info, const Graph* graph,\n-    std::unordered_map<string, WhileLoopFrame>* frames,\n+    std::unordered_map<std::string, WhileLoopFrame>* frames,\n     const NodeFilter& node_filter = {});\n \n // Check that the graph has no cycle containing the given node.\n@@ -97,10 +97,10 @@ absl::StatusOr<Node*> BuildRetvalNode(Graph* graph, DataType type, int index);\n \n // Returns a textual representation of the names of the nodes in the input.\n template <typename T>\n-string NodesToString(const T& nodes) {\n+std::string NodesToString(const T& nodes) {\n   return absl::StrCat(\"{\",\n                       absl::StrJoin(nodes, \",\",\n-                                    [](string* output, const Node* node) {\n+                                    [](std::string* output, const Node* node) {\n                                       absl::StrAppend(output, node->name());\n                                     }),\n                       \"}\");"
        },
        {
            "sha": "b8183afd59481a626aed98bdb6d458b03c9f26c0",
            "filename": "tensorflow/compiler/tf2xla/functionalize_while.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_while.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_while.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffunctionalize_while.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -438,7 +438,7 @@ absl::Status FunctionalizeLoop(Graph* graph, WhileLoopFrame* frame,\n   builder.Attr(\"body\", body_name);\n   // Add some internal attributes which need to be propagated.\n   for (absl::string_view attr_name : kAttrsToPropagate) {\n-    string attr_val;\n+    std::string attr_val;\n     if (GetNodeAttr(frame->loop_cond->def(), attr_name, &attr_val).ok()) {\n       builder.Attr(attr_name, attr_val);\n     }\n@@ -513,7 +513,7 @@ absl::Status FunctionalizeWhileLoop(Graph* graph,\n   // connected to all source nodes in the graph. Many graphs violate this\n   // invariant.\n   std::vector<ControlFlowInfo> cf_info;\n-  std::vector<string> unreachable_nodes;\n+  std::vector<std::string> unreachable_nodes;\n   TF_RETURN_IF_ERROR(BuildControlFlowInfo(graph, &cf_info, &unreachable_nodes));\n   if (!unreachable_nodes.empty()) {\n     return errors::InvalidArgument(\n@@ -522,7 +522,7 @@ absl::Status FunctionalizeWhileLoop(Graph* graph,\n   }\n \n   // Builds Frames, indexed by name.\n-  std::unordered_map<string, WhileLoopFrame> frames;\n+  std::unordered_map<std::string, WhileLoopFrame> frames;\n   TF_RETURN_IF_ERROR(\n       ExtractWhileLoopFrames(cf_info, graph, &frames, node_filter));\n "
        },
        {
            "sha": "b331272a2c9504d57c1f11012bea7a5270195e42",
            "filename": "tensorflow/compiler/tf2xla/fused_batchnorm_reserve_space_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffused_batchnorm_reserve_space_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Ffused_batchnorm_reserve_space_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ffused_batchnorm_reserve_space_test.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n \n namespace tensorflow {\n namespace {\n-absl::Status GetTestDevice(Session* session, string* test_device) {\n+absl::Status GetTestDevice(Session* session, std::string* test_device) {\n   std::vector<DeviceAttributes> devices;\n   TF_RETURN_IF_ERROR(session->ListDevices(&devices));\n \n@@ -85,7 +85,7 @@ TEST(FusedBatchnormReserveSpaceTest, Test) {\n   std::unique_ptr<tensorflow::Session> session(\n       tensorflow::NewSession(tensorflow::SessionOptions{}));\n \n-  string test_device;\n+  std::string test_device;\n   TF_ASSERT_OK(GetTestDevice(session.get(), &test_device));\n \n   Scope root = tensorflow::Scope::NewRootScope();\n@@ -108,8 +108,8 @@ TEST(FusedBatchnormReserveSpaceTest, Test) {\n   Output variance =\n       Const(root.WithOpName(\"variance\"), Input::Initializer(variance_data));\n \n-  string tf_device = absl::StrCat(\"/device:\", test_device, \":0\");\n-  string xla_device = absl::StrCat(\"/device:XLA_\", test_device, \":0\");\n+  std::string tf_device = absl::StrCat(\"/device:\", test_device, \":0\");\n+  std::string xla_device = absl::StrCat(\"/device:XLA_\", test_device, \":0\");\n \n   FusedBatchNorm fused_batch_norm_tf(\n       root.WithOpName(\"fused_batch_norm_tf\").WithDevice(tf_device), input,"
        },
        {
            "sha": "5f794005b7c7c007e91a5595598c3569d1345ee3",
            "filename": "tensorflow/compiler/tf2xla/graph_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -292,12 +292,12 @@ absl::Status GraphCompiler::CompileFunctionalNode(Node* n,\n     }\n   }\n   if (add_token_input_output) {\n-    std::vector<string> token_input_nodes;\n+    std::vector<std::string> token_input_nodes;\n     TF_RETURN_IF_ERROR(GetNodeAttr(AttrSlice(&func.attr()),\n                                    kXlaTokenInputNodesAttrName,\n                                    &token_input_nodes));\n     std::vector<xla::XlaOp> token_inputs;\n-    for (const string& node_name : token_input_nodes) {\n+    for (const std::string& node_name : token_input_nodes) {\n       auto token_or = compiler->GetNodeToken(node_name);\n       TF_RETURN_IF_ERROR(token_or.status());\n       token_inputs.push_back(std::move(token_or).value());"
        },
        {
            "sha": "2dcb2ea0b52d4574e34f31bb363697d5bd2ac316",
            "filename": "tensorflow/compiler/tf2xla/graph_compiler_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler_test.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -104,8 +104,8 @@ class GraphCompilerTest : public ::testing::Test {\n     core::ScopedUnref context_unref(xla_context);\n     xla_context->Ref();\n \n-    auto step_container =\n-        std::make_unique<ScopedStepContainer>(0, [this](const string& name) {\n+    auto step_container = std::make_unique<ScopedStepContainer>(\n+        0, [this](const std::string& name) {\n           absl::Status status =\n               this->device_->resource_manager()->Cleanup(name);\n         });"
        },
        {
            "sha": "116c1e68f66fe6a605de3922cec18c07fe059144",
            "filename": "tensorflow/compiler/tf2xla/graph_compiler_util.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/620c9a0f52eddc8c81bf6309084ff7fc2d439963/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fgraph_compiler_util.cc?ref=620c9a0f52eddc8c81bf6309084ff7fc2d439963",
            "patch": "@@ -44,7 +44,7 @@ const char* const kFetchIdAttr = \"_fetch_id\";\n const char* const kShapeAttr = \"_shape\";\n const char* const kDebugNameAttr = \"_debug_name\";\n \n-typedef std::unordered_map<string, Node*> NodeMap;\n+typedef std::unordered_map<std::string, Node*> NodeMap;\n \n // Each feed id identifies the positional output of some node, which may consist\n // of multiple edges. AddPlaceholdersForFeeds has already replaced each fed\n@@ -54,14 +54,14 @@ typedef std::unordered_map<string, Node*> NodeMap;\n absl::Status AddArgNodes(\n     Graph* graph, const NodeMap& node_map,\n     const protobuf::RepeatedPtrField<tf2xla::Feed>& feeds,\n-    const std::unordered_map<string, string>& feed_remapping,\n+    const std::unordered_map<std::string, std::string>& feed_remapping,\n     std::unordered_set<const Node*>* arg_nodes) {\n   for (int arg_index = 0; arg_index < feeds.size(); ++arg_index) {\n     const tf2xla::Feed& feed = feeds[arg_index];\n     // All feeds have been replaced by placeholders.\n     const int output_index = 0;\n \n-    const string key = TensorIdToString(feed.id());\n+    const std::string key = TensorIdToString(feed.id());\n     const auto remap_it = feed_remapping.find(key);\n     auto node_it = node_map.find(remap_it->second);\n     if (node_it == node_map.end()) {\n@@ -149,7 +149,7 @@ absl::Status AddRetvalNodes(\n // execution to know the input and output args for the generated function.\n absl::Status RewriteAndPruneGraph(\n     Graph* graph, const tf2xla::Config& config,\n-    const std::unordered_map<string, string>& feed_remapping) {\n+    const std::unordered_map<std::string, std::string>& feed_remapping) {\n   NodeMap node_map;\n   for (Node* n : graph->nodes()) {\n     node_map[n->name()] = n;\n@@ -164,7 +164,7 @@ absl::Status RewriteAndPruneGraph(\n   FixupSourceAndSinkEdges(graph);\n   VLOG(2) << \"Post prune: \" << DumpGraphToFile(\"tfcompile_post_prune\", *graph);\n   // Sanity-check, to make sure the feeds and fetches still exist post-pruning.\n-  std::set<string> missing_feeds, missing_fetches;\n+  std::set<std::string> missing_feeds, missing_fetches;\n   for (const tf2xla::Feed& feed : config.feed()) {\n     missing_feeds.insert(TensorIdToString(feed.id()));\n   }\n@@ -173,14 +173,14 @@ absl::Status RewriteAndPruneGraph(\n   }\n   for (const Node* n : graph->op_nodes()) {\n     if (n->type_string() == FunctionLibraryDefinition::kArgOp) {\n-      string feed_id;\n+      std::string feed_id;\n       TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), kFeedIdAttr, &feed_id));\n       if (missing_feeds.erase(feed_id) == 0) {\n         return errors::Aborted(FunctionLibraryDefinition::kArgOp,\n                                \" node found with unknown feed id: \", feed_id);\n       }\n     } else if (n->type_string() == FunctionLibraryDefinition::kRetOp) {\n-      string fetch_id;\n+      std::string fetch_id;\n       TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), kFetchIdAttr, &fetch_id));\n       if (missing_fetches.erase(fetch_id) == 0) {\n         return errors::Aborted(FunctionLibraryDefinition::kRetOp,\n@@ -277,7 +277,7 @@ absl::Status InitGraph(const GraphDef& graph_def, const tf2xla::Config& config,\n   GraphDef first_copy_def = graph_def;\n \n   // Maps from name:port of a feed to the name:port of the placeholder to use.\n-  std::unordered_map<string, string> feed_remapping;\n+  std::unordered_map<std::string, std::string> feed_remapping;\n   TF_RETURN_IF_ERROR(AddPlaceholdersForFeeds(config, g->op_registry(),\n                                              &feed_remapping, &first_copy_def));\n "
        }
    ],
    "stats": {
        "total": 258,
        "additions": 133,
        "deletions": 125
    }
}