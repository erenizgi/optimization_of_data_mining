{
    "author": "Cjkkkk",
    "message": "PR #32718: [XLA:GPU] add conv fusion support in cudnn fusion compiler\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32718\n\nüìù Summary of Changes\nThis PR adds conv fusion support in cudnn fusion compiler.\n\n* add conv type in `CuDnnFusionConfig` to represent different types of conv. We are getting rid of the conv custom call target so this info has be preserved in fusion config.\n* add `ConvDimensionAdapter` to generate NCHW **logical layout** for cudnn frontend while physical layout could be NHWC (most preferable layout) or NCHW (for int conv). Only NHWC layout is used in the unit tests because layout assignment currently doesn't handle conv fusion to transform other layouts to NHWC, this needs to be addressed in separate PR.\n* add conv translation rule from XLA conv to cudnn frontend graph API.\n* Other parts of the lowering is taken care automatically by current cudnn fusion compiler: workspace allocation/graph validation/graph  compilation/graph serialization.\n\nüéØ Justification\nThis is the first step to unify the conv as cudnn fusion in XLA. Conv custom call will be replaced with conv fusions in the future.\n\nüöÄ Kind of Contribution\n‚ú® New Feature\n\nüìä Benchmark (for Performance Improvements)\nNo Performance changes are expected.\n\nüß™ Unit Tests:\nAdded 3 hand written NHWC conv unit tests for conv_fprop/conv_dgrad/conv_wgrad.\n\nüß™ Execution Tests:\nAdded 3 hand written NHWC conv unit tests for conv_fprop/conv_dgrad/conv_wgrad.\nCopybara import of the project:\n\n--\n57555cd0e3759aacb7a98135c3261f4cc3f642c2 by Cjkkkk <ske@nvidia.com>:\n\ninit\n\n--\nd6edecfa42a6371a0908e22daeb8deaf32998ece by Cjkkkk <ske@nvidia.com>:\n\naddress comments\n\n--\n17df6f8451274f070d7d332a126cfefa1ef7df83 by Cjkkkk <ske@nvidia.com>:\n\nremoved one comment\n\n--\n1b7c63b1ade7751cf8f68c7fb11cd68491440081 by Cjkkkk <ske@nvidia.com>:\n\nadd const\n\nMerging this change closes #32718\n\nPiperOrigin-RevId: 820574737",
    "sha": "f573329cc67b179f1380a52704c751638326b323",
    "files": [
        {
            "sha": "13a5cf2baf170727426aee09e268e19d074f639f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 130,
            "deletions": 0,
            "changes": 130,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f573329cc67b179f1380a52704c751638326b323/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f573329cc67b179f1380a52704c751638326b323/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=f573329cc67b179f1380a52704c751638326b323",
            "patch": "@@ -868,6 +868,79 @@ ENTRY r {\n                             ErrorSpec{/*aabs=*/1, /*arel=*/1e-3}));\n }\n \n+TEST_F(CuDnnFusionExecutionTest, ConvFpropWithNHWCLayoutExecutesCorrectly) {\n+  EXPECT_TRUE(RunAndCompare(R\"(\n+fusion {\n+  zero = f32[] constant(0)\n+  zeros = f32[2,9,9,32] broadcast(zero), dimensions={}\n+  input = f32[2,9,9,17] parameter(0)\n+  filter = f32[32,3,3,17] parameter(1)\n+  conv = f32[2,9,9,32] convolution(input, filter), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, feature_group_count=1\n+  ROOT relu = f32[2,9,9,32] maximum(zeros, conv)\n+}\n+\n+\n+ENTRY Test {\n+  input = f32[2,9,9,17] parameter(0)\n+  filter = f32[32,3,3,17] parameter(1)\n+  ROOT conv = f32[2,9,9,32] fusion(input, filter), kind=kCustom, calls=fusion, backend_config={\"fusion_backend_config\": {kind: \"__cudnn$fusion\", cudnn_fusion_config: {\"kind\":\"CONV_FPROP\"}}}\n+})\",\n+                            ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-5}));\n+}\n+\n+TEST_F(CuDnnFusionExecutionTest, ConvWgradWithNHWCLayoutExecutesCorrectly) {\n+  EXPECT_TRUE(RunAndCompare(R\"(\n+fusion {\n+  zero = f32[] constant(0)\n+  zeros = f32[32,3,3,17] broadcast(zero), dimensions={}\n+  input = f32[2,9,9,17] parameter(0)\n+  dout = f32[2,9,9,32] parameter(1)\n+  conv = f32[32,3,3,17] convolution(input, dout), window={size=9x9 pad=1_1x1_1}, dim_labels=f01b_i01o->f01b, feature_group_count=1\n+  ROOT relu = f32[32,3,3,17] maximum(zeros, conv)\n+}\n+\n+\n+ENTRY Test {\n+  input = f32[2,9,9,17] parameter(0)\n+  dout = f32[2,9,9,32] parameter(1)\n+  ROOT conv = f32[32,3,3,17] fusion(input, dout), kind=kCustom, calls=fusion, backend_config={\"fusion_backend_config\": {kind: \"__cudnn$fusion\", cudnn_fusion_config: {\"kind\":\"CONV_WGRAD\"}}}\n+})\",\n+                            ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-5}));\n+}\n+\n+TEST_F(CuDnnFusionExecutionTest, ConvDgradWithNHWCLayoutExecutesCorrectly) {\n+  const std::string kHloReference = R\"(\n+ENTRY main {\n+  zero = f32[] constant(0)\n+  zeros = f32[2,9,9,17] broadcast(zero), dimensions={}\n+  dout = f32[2,9,9,32] parameter(0)\n+  filter = f32[32,3,3,17] parameter(1)\n+  reverse = f32[32,3,3,17] reverse(filter), dimensions={1,2}\n+  conv = f32[2,9,9,17] convolution(dout, reverse), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_i01o->b01f, feature_group_count=1\n+  ROOT relu = f32[2,9,9,17] maximum(zeros, conv)\n+})\";\n+\n+  const std::string kHlo = R\"(\n+fusion {\n+  zero = f32[] constant(0)\n+  zeros = f32[2,9,9,17] broadcast(zero), dimensions={}\n+  dout = f32[2,9,9,32] parameter(0)\n+  filter = f32[32,3,3,17] parameter(1)\n+  conv = f32[2,9,9,17] convolution(dout, filter), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_i01o->b01f, feature_group_count=1\n+  ROOT relu = f32[2,9,9,17] maximum(zeros, conv)\n+}\n+\n+\n+ENTRY Test {\n+  dout = f32[2,9,9,32] parameter(0)\n+  filter = f32[32,3,3,17] parameter(1)\n+  ROOT conv = f32[2,9,9,17] fusion(dout, filter), kind=kCustom, calls=fusion, backend_config={\"fusion_backend_config\": {kind: \"__cudnn$fusion\", cudnn_fusion_config: {\"kind\":\"CONV_DGRAD\"}}}\n+})\";\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(kHlo, kHloReference,\n+                                      ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-5}));\n+}\n+\n class ElementwiseTest : public CuDnnFusionExecutionTest,\n                         public ::testing::WithParamInterface<\n                             std::tuple<PrimitiveType, HloOpcode, float>> {};\n@@ -1190,6 +1263,63 @@ CHECK: \"stride\": [{{[[:space:]]*1,[[:space:]]*1,[[:space:]]*4[[:space:]]*}}]\n )\"));\n }\n \n+TEST_F(CuDnnFusionFileCheckTest, ConvFpropGraphConvertedCorrectly) {\n+  const std::string kHloText = R\"(\n+fusion {\n+  input = f32[2,9,9,17] parameter(0)\n+  filter = f32[32,3,3,17] parameter(1)\n+  ROOT conv = f32[2,9,9,32] convolution(input, filter), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, feature_group_count=1\n+}\n+\n+\n+ENTRY Test {\n+  input = f32[2,9,9,17] parameter(0)\n+  filter = f32[32,3,3,17] parameter(1)\n+  ROOT conv = f32[2,9,9,32] fusion(input, filter), kind=kCustom, calls=fusion, backend_config={\"fusion_backend_config\": {kind: \"__cudnn$fusion\", cudnn_fusion_config: {\"kind\":\"CONV_FPROP\"}}}\n+})\";\n+\n+  EXPECT_TRUE(*RunCuDnnFileCheck(kHloText, R\"(\n+CHECK: \"nodes\": [\n+CHECK:  {\n+CHECK:   \"compute_data_type\": \"FLOAT\",\n+CHECK:   \"dilation\": [{{[[:space:]]*1,[[:space:]]*1[[:space:]]*}}],\n+CHECK:   \"inputs\": {\n+CHECK:    \"W\": \"filter\",\n+CHECK:    \"X\": \"input\"\n+CHECK:   },\n+CHECK:   \"math_mode\": \"CROSS_CORRELATION\",\n+CHECK:   \"name\": \"0\",\n+CHECK:   \"outputs\": {\n+CHECK:    \"Y\": \"conv\"\n+CHECK:   },\n+CHECK:   \"post_padding\": [{{[[:space:]]*1,[[:space:]]*1[[:space:]]*}}],\n+CHECK:   \"pre_padding\": [{{[[:space:]]*1,[[:space:]]*1[[:space:]]*}}],\n+CHECK:   \"stride\": [{{[[:space:]]*1,[[:space:]]*1[[:space:]]*}}],\n+CHECK:   \"tag\": \"CONV_FPROP\"\n+CHECK:  }\n+CHECK: ],\n+CHECK:\"tensors\": {\n+CHECK:  \"conv\": {\n+CHECK:   \"data_type\": \"FLOAT\",\n+CHECK:   \"dim\": [{{[[:space:]]*2,[[:space:]]*32,[[:space:]]*9,[[:space:]]*9[[:space:]]*}}],\n+CHECK:   \"name\": \"conv\",\n+CHECK:   \"stride\": [{{[[:space:]]*2592,[[:space:]]*1,[[:space:]]*288,[[:space:]]*32[[:space:]]*}}],\n+CHECK:  },\n+CHECK:  \"filter\": {\n+CHECK:   \"data_type\": \"FLOAT\",\n+CHECK:   \"dim\": [{{[[:space:]]*32,[[:space:]]*17,[[:space:]]*3,[[:space:]]*3[[:space:]]*}}],\n+CHECK:   \"name\": \"filter\",\n+CHECK:   \"stride\": [{{[[:space:]]*153,[[:space:]]*1,[[:space:]]*51,[[:space:]]*17[[:space:]]*}}],\n+CHECK:  },\n+CHECK:  \"input\": {\n+CHECK:   \"data_type\": \"FLOAT\",\n+CHECK:   \"dim\": [{{[[:space:]]*2,[[:space:]]*17,[[:space:]]*9,[[:space:]]*9[[:space:]]*}}],\n+CHECK:   \"name\": \"input\",\n+CHECK:   \"stride\": [{{[[:space:]]*1377,[[:space:]]*1,[[:space:]]*153,[[:space:]]*17[[:space:]]*}}],\n+CHECK:  }\n+CHECK: }\n+)\"));\n+}\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "405a43600d2b9655c89f83dfc1a0eae3265f92d3",
            "filename": "third_party/xla/xla/service/gpu/backend_configs.proto",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f573329cc67b179f1380a52704c751638326b323/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f573329cc67b179f1380a52704c751638326b323/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto?ref=f573329cc67b179f1380a52704c751638326b323",
            "patch": "@@ -170,6 +170,14 @@ message CustomFusionConfig {\n \n message CuDnnFusionConfig {\n   int64 plan_id = 1;\n+\n+  // Conv type.\n+  enum Kind {\n+    CONV_FPROP = 0;\n+    CONV_WGRAD = 1;\n+    CONV_DGRAD = 2;\n+  }\n+  Kind kind = 2;\n }\n \n // Output tile sizes for a fusion root."
        },
        {
            "sha": "025a5da61ef8c7de6b1876f267a89096fd7ab562",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fusion_compiler.cc",
            "status": "modified",
            "additions": 216,
            "deletions": 28,
            "changes": 244,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f573329cc67b179f1380a52704c751638326b323/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f573329cc67b179f1380a52704c751638326b323/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc?ref=f573329cc67b179f1380a52704c751638326b323",
            "patch": "@@ -182,6 +182,12 @@ inline std::optional<fe::DataType_t> GetComputeDataType(\n \n // Extracts dimensions and strides from HLO tensors in the format expected by\n // cuDNN.\n+struct Result {\n+  std::vector<int64_t> sizes;\n+  std::vector<int64_t> strides;\n+  std::optional<std::vector<std::pair<int64_t, int64_t>>> slices;\n+};\n+\n class GemmDimensionAdapter {\n   explicit GemmDimensionAdapter(const HloInstruction& dot,\n                                 TritonFusionAnalysis analysis)\n@@ -213,12 +219,6 @@ class GemmDimensionAdapter {\n     return GemmDimensionAdapter{*dot, std::move(analysis)};\n   }\n \n-  struct Result {\n-    std::vector<int64_t> sizes;\n-    std::vector<int64_t> strides;\n-    std::optional<std::vector<std::pair<int64_t, int64_t>>> slices;\n-  };\n-\n   std::optional<Result> DimensionsAndStrides(\n       const HloInstruction& hlo, const TritonFusionAnalysis::Scope scope) {\n     const DotDimensionNumbers& dims = dot_.dot_dimension_numbers();\n@@ -359,6 +359,111 @@ class GemmDimensionAdapter {\n   const HloInstruction& dot_;\n };\n \n+class ConvDimensionAdapter {\n+  explicit ConvDimensionAdapter(const HloInstruction& conv,\n+                                CuDnnFusionConfig_Kind conv_kind,\n+                                ConvolutionDimensionNumbers dums)\n+      : conv_(conv), conv_kind_(conv_kind), dums_(dums) {}\n+\n+ public:\n+  const HloInstruction& conv_;\n+  CuDnnFusionConfig_Kind conv_kind_;\n+\n+  static absl::StatusOr<std::optional<ConvDimensionAdapter>> Create(\n+      const HloFusionInstruction& fusion, const HloComputation& computation) {\n+    const HloInstruction* maybe_conv = hlo_query::GetFirstInstructionWithOpcode(\n+        computation, HloOpcode::kConvolution);\n+    if (maybe_conv == nullptr) {\n+      VLOG(3) << \"Not a Conv fusion.\";\n+      return std::nullopt;\n+    }\n+\n+    // get conv type from backend config\n+    TF_ASSIGN_OR_RETURN(auto gpu_config,\n+                        fusion.backend_config<GpuBackendConfig>());\n+    const FusionBackendConfig& fusion_backend_config =\n+        gpu_config.fusion_backend_config();\n+    if (!fusion_backend_config.has_cudnn_fusion_config()) {\n+      VLOG(3) << \"Can't find cudnn fusion config for cudnn conv fusion.\";\n+      return std::nullopt;\n+    }\n+    CuDnnFusionConfig_Kind conv_kind =\n+        fusion_backend_config.cudnn_fusion_config().kind();\n+\n+    const ConvolutionDimensionNumbers& dnums =\n+        DynCast<HloConvolutionInstruction>(maybe_conv)\n+            ->convolution_dimension_numbers();\n+    // for fprop, we do nothing, copy directly\n+    ConvolutionDimensionNumbers dnums_for_layout = dnums;\n+    if (conv_kind == CuDnnFusionConfig::CONV_WGRAD) {\n+      dnums_for_layout.set_input_batch_dimension(\n+          dnums.input_feature_dimension());\n+      dnums_for_layout.set_input_feature_dimension(\n+          dnums.input_batch_dimension());\n+      dnums_for_layout.set_output_batch_dimension(\n+          dnums.output_feature_dimension());\n+      dnums_for_layout.set_output_feature_dimension(\n+          dnums.output_batch_dimension());\n+      dnums_for_layout.set_kernel_input_feature_dimension(\n+          dnums.kernel_output_feature_dimension());\n+      dnums_for_layout.set_kernel_output_feature_dimension(\n+          dnums.kernel_input_feature_dimension());\n+    } else if (conv_kind == CuDnnFusionConfig::CONV_DGRAD) {\n+      dnums_for_layout.set_kernel_input_feature_dimension(\n+          dnums.kernel_output_feature_dimension());\n+      dnums_for_layout.set_kernel_output_feature_dimension(\n+          dnums.kernel_input_feature_dimension());\n+    }\n+\n+    // make sure input/kernel/output has the same layout\n+    TF_RET_CHECK(dnums_for_layout.input_batch_dimension() ==\n+                     dnums_for_layout.kernel_output_feature_dimension() &&\n+                 dnums_for_layout.kernel_output_feature_dimension() ==\n+                     dnums_for_layout.output_batch_dimension());\n+    TF_RET_CHECK(dnums_for_layout.input_feature_dimension() ==\n+                     dnums_for_layout.kernel_input_feature_dimension() &&\n+                 dnums_for_layout.kernel_input_feature_dimension() ==\n+                     dnums_for_layout.output_feature_dimension());\n+    for (auto i = 0; i < dnums_for_layout.input_spatial_dimensions_size();\n+         ++i) {\n+      TF_RET_CHECK(dnums_for_layout.input_spatial_dimensions(i) ==\n+                       dnums_for_layout.kernel_spatial_dimensions(i) &&\n+                   dnums_for_layout.kernel_spatial_dimensions(i) ==\n+                       dnums_for_layout.output_spatial_dimensions(i));\n+    }\n+    return ConvDimensionAdapter{*maybe_conv, conv_kind, dnums_for_layout};\n+  }\n+\n+  std::optional<Result> DimensionsAndStrides(const HloInstruction& hlo) {\n+    // placeholder FP32 data type here, it is not used\n+    auto desc = se::dnn::TensorDescriptor::For(\n+        se::dnn::DataType::kFloat, hlo.shape().dimensions(),\n+        hlo.shape().layout().minor_to_major());\n+    // logical layout and physical layout should be the same after layout\n+    // assignment.\n+    std::vector<int64_t> logical_dims = desc.dimensions();\n+    std::vector<int64_t> logical_strides = desc.GetLogicalStrides();\n+    // cuDNN conv frontend requires logical layout to be NCHW. return logical\n+    // NCHW layout. we shouldn't need to know if this hlo is LHS, RHS or Output,\n+    // they should have same layout after layout assignment. Use input dums\n+    // here.\n+    Result result;\n+    result.sizes.push_back(logical_dims[dums_.input_batch_dimension()]);\n+    result.sizes.push_back(logical_dims[dums_.input_feature_dimension()]);\n+    result.strides.push_back(logical_strides[dums_.input_batch_dimension()]);\n+    result.strides.push_back(logical_strides[dums_.input_feature_dimension()]);\n+    for (auto i = 0; i < dums_.input_spatial_dimensions_size(); ++i) {\n+      result.sizes.push_back(logical_dims[dums_.input_spatial_dimensions(i)]);\n+      result.strides.push_back(\n+          logical_strides[dums_.input_spatial_dimensions(i)]);\n+    }\n+    return result;\n+  }\n+\n+ private:\n+  ConvolutionDimensionNumbers dums_;\n+};\n+\n template <PrimitiveType XlaT, typename T>\n std::shared_ptr<graph::Tensor_attributes> LiteralToCudnnTensor(\n     const HloInstruction& hlo, graph::Graph& graph) {\n@@ -431,13 +536,17 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n   absl::flat_hash_map<const HloInstruction*,\n                       std::shared_ptr<graph::Tensor_attributes>>\n       hlo_to_cudnn;\n-  TF_ASSIGN_OR_RETURN(std::optional<GemmDimensionAdapter> adapter,\n+  TF_ASSIGN_OR_RETURN(std::optional<GemmDimensionAdapter> gemm_adapter,\n                       GemmDimensionAdapter::Create(computation));\n-  if (!adapter.has_value()) {\n+  TF_ASSIGN_OR_RETURN(std::optional<ConvDimensionAdapter> conv_adapter,\n+                      ConvDimensionAdapter::Create(fusion, computation));\n+  if (!gemm_adapter.has_value() && !conv_adapter.has_value()) {\n+    VLOG(3) << \"No dot or conv found inside cudnn fusion.\";\n     return std::nullopt;\n   }\n+\n   auto add_parameter = [&](const HloInstruction& parameter,\n-                           const GemmDimensionAdapter::Result& dims) {\n+                           const Result& dims) {\n     const std::optional<fe::DataType_t> data_type =\n         ToCudnnDataType(parameter.shape().element_type());\n     if (!data_type.has_value()) {\n@@ -458,21 +567,21 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n     }\n     return true;\n   };\n-  for (const TritonFusionAnalysis::Scope scope :\n-       {TritonFusionAnalysis::Scope::LHS,\n-        TritonFusionAnalysis::Scope::LHS_SCALE,\n-        TritonFusionAnalysis::Scope::RHS,\n-        TritonFusionAnalysis::Scope::RHS_SCALE,\n-        TritonFusionAnalysis::Scope::OUTPUT}) {\n-    if (!adapter->analysis_.is_scaled_dot() &&\n-        (scope == TritonFusionAnalysis::Scope::LHS_SCALE ||\n-         scope == TritonFusionAnalysis::Scope::RHS_SCALE)) {\n-      continue;\n+\n+  if (conv_adapter.has_value()) {\n+    for (const HloInstruction* operand : conv_adapter->conv_.operands()) {\n+      if (!HloPredicateIsOp<HloOpcode::kParameter>(operand)) {\n+        VLOG(3) << \"Conv operands are expected to be parameters.\";\n+        return std::nullopt;\n+      }\n     }\n     for (const HloInstruction* parameter :\n-         adapter->analysis_.ScopeParameters(scope)) {\n-      const std::optional<GemmDimensionAdapter::Result> dims =\n-          adapter->DimensionsAndStrides(*parameter, scope);\n+         computation.parameter_instructions()) {\n+      // for now, we assume all parameters have same layout even if they are not\n+      // inputs to conv, for example, bias add after conv.\n+      const std::optional<Result> dims =\n+          conv_adapter->DimensionsAndStrides(*parameter);\n+      VLOG(3) << \"parameter: \" << parameter->ToString() << \"\\n\";\n       if (!dims.has_value()) {\n         VLOG(3) << \"Unsupported dimensions.\";\n         return std::nullopt;\n@@ -481,6 +590,33 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n         return std::nullopt;\n       }\n     }\n+  } else {\n+    // dot and scale dot\n+    for (const TritonFusionAnalysis::Scope scope :\n+         {TritonFusionAnalysis::Scope::LHS,\n+          TritonFusionAnalysis::Scope::LHS_SCALE,\n+          TritonFusionAnalysis::Scope::RHS,\n+          TritonFusionAnalysis::Scope::RHS_SCALE,\n+          TritonFusionAnalysis::Scope::OUTPUT}) {\n+      if (!gemm_adapter->analysis_.is_scaled_dot() &&\n+          (scope == TritonFusionAnalysis::Scope::LHS_SCALE ||\n+           scope == TritonFusionAnalysis::Scope::RHS_SCALE)) {\n+        continue;\n+      }\n+      for (const HloInstruction* parameter :\n+           gemm_adapter->analysis_.ScopeParameters(scope)) {\n+        const std::optional<Result> dims =\n+            gemm_adapter->DimensionsAndStrides(*parameter, scope);\n+        VLOG(3) << \"parameter: \" << parameter->ToString() << \"\\n\";\n+        if (!dims.has_value()) {\n+          VLOG(3) << \"Unsupported dimensions.\";\n+          return std::nullopt;\n+        }\n+        if (!add_parameter(*parameter, *dims)) {\n+          return std::nullopt;\n+        }\n+      }\n+    }\n   }\n \n   for (const HloInstruction* hlo : instructions) {\n@@ -550,13 +686,13 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n           // the cuDNN graph.\n           if (hlo->operand(0)->opcode() == HloOpcode::kBroadcast) {\n             const std::optional<TritonFusionAnalysis::Scope> scope =\n-                adapter->analysis_.QueryInstructionScope(*hlo);\n+                gemm_adapter->analysis_.QueryInstructionScope(*hlo);\n             if (!scope.has_value()) {\n               LOG(FATAL) << \"No scope for instruction: \"\n                          << hlo->ToShortString();\n             }\n-            const std::optional<GemmDimensionAdapter::Result> dims =\n-                adapter->DimensionsAndStrides(*hlo, *scope);\n+            const std::optional<Result> dims =\n+                gemm_adapter->DimensionsAndStrides(*hlo, *scope);\n             if (!dims.has_value()) {\n               VLOG(3) << \"Unsupported hlo for querying dimensions: \"\n                       << hlo->ToShortString();\n@@ -619,6 +755,55 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n           graph.matmul(dot_operands[0], dot_operands[1],\n                        graph::Matmul_attributes().set_compute_data_type(\n                            compute_dtype.value()));\n+    } else if (HloPredicateIsOp<HloOpcode::kConvolution>(hlo)) {\n+      // translate conv windows to cudnn conv attr\n+      const Window& window = DynCast<HloConvolutionInstruction>(hlo)->window();\n+      std::vector<int64_t> pre_padding, post_padding, stride, dilation;\n+      for (int64_t i = 0; i < window.dimensions_size(); ++i) {\n+        const auto& dim = window.dimensions(i);\n+        pre_padding.push_back(dim.padding_low());\n+        post_padding.push_back(dim.padding_high());\n+        stride.push_back(dim.stride());\n+        dilation.push_back(dim.window_dilation());\n+      }\n+      const auto compute_dtype =\n+          GetComputeDataType(hlo->shape().element_type());\n+      if (!compute_dtype.has_value()) {\n+        return std::nullopt;\n+      }\n+\n+      // lower to different conv based on conv_kind set in cudnn fusion backend\n+      // config\n+      auto set_conv_attr = [&](auto conv_attr) {\n+        return conv_attr.set_pre_padding(pre_padding)\n+            .set_post_padding(post_padding)\n+            .set_stride(stride)\n+            .set_dilation(dilation)\n+            .set_compute_data_type(compute_dtype.value());\n+      };\n+      if (conv_adapter->conv_kind_ == CuDnnFusionConfig::CONV_FPROP) {\n+        hlo_to_cudnn[hlo] =\n+            graph.conv_fprop(operand(0), operand(1),\n+                             set_conv_attr(graph::Conv_fprop_attributes()));\n+      } else if (conv_adapter->conv_kind_ == CuDnnFusionConfig::CONV_DGRAD) {\n+        hlo_to_cudnn[hlo] =\n+            graph.conv_dgrad(operand(0), operand(1),\n+                             set_conv_attr(graph::Conv_dgrad_attributes()));\n+      } else if (conv_adapter->conv_kind_ == CuDnnFusionConfig::CONV_WGRAD) {\n+        // cudnn frontend accepts operand in the order of dout, input, but xla\n+        // uses reverse order\n+        hlo_to_cudnn[hlo] =\n+            graph.conv_wgrad(operand(1), operand(0),\n+                             set_conv_attr(graph::Conv_wgrad_attributes()));\n+      } else {\n+        VLOG(3) << \"Unimplemented conv type.\";\n+        return std::nullopt;\n+      }\n+      // cuDNN requires output dims to be set for conv dgrad and wgrad, it is\n+      // not required for fprop but we do it anyway for simplicity\n+      const std::optional<Result> dims =\n+          conv_adapter->DimensionsAndStrides(*hlo);\n+      hlo_to_cudnn[hlo]->set_dim(dims->sizes);\n     } else {\n       VLOG(3) << \"Unimplemented operation.\";\n       return std::nullopt;\n@@ -642,9 +827,12 @@ absl::StatusOr<std::optional<se::gpu::CudnnGraph>> HloFusionToCuDnnGraph(\n   if (instructions.back()->shape().IsTuple()) {\n     output = instructions.back()->operand(0);\n   }\n-  const std::optional<GemmDimensionAdapter::Result> dims =\n-      adapter->DimensionsAndStrides(*output,\n-                                    TritonFusionAnalysis::Scope::OUTPUT);\n+\n+  const std::optional<Result> dims =\n+      conv_adapter.has_value()\n+          ? conv_adapter->DimensionsAndStrides(*output)\n+          : gemm_adapter->DimensionsAndStrides(\n+                *output, TritonFusionAnalysis::Scope::OUTPUT);\n   if (!dims.has_value()) {\n     VLOG(3) << \"Unsupported dimensions.\";\n     return std::nullopt;"
        }
    ],
    "stats": {
        "total": 382,
        "additions": 354,
        "deletions": 28
    }
}