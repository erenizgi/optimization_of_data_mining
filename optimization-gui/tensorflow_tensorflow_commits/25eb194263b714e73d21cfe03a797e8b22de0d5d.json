{
    "author": "tensorflower-gardener",
    "message": "Map CudaGraph Node in traceviewer could show per-node framework namescope by rewrite their annotation to the value during its corresponing creation.\n\nPiperOrigin-RevId: 839044555",
    "sha": "25eb194263b714e73d21cfe03a797e8b22de0d5d",
    "files": [
        {
            "sha": "e29b5c701169e19f2d21bb67d5b79f4b13c14a42",
            "filename": "third_party/xla/xla/backends/profiler/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2FBUILD?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -217,6 +217,7 @@ cuda_library(\n         \"@com_google_googletest//:gtest_for_library\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@local_config_cuda//cuda:cuda_runtime\",\n+        \"@local_tsl//tsl/profiler/lib:scoped_annotation\",\n     ],\n )\n "
        },
        {
            "sha": "a7a832e737c360d2948f6d3e9d01bd6451f467fc",
            "filename": "third_party/xla/xla/backends/profiler/gpu/cuda_test.cu.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 10,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcuda_test.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcuda_test.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcuda_test.cu.cc?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -24,11 +24,14 @@ limitations under the License.\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n #include \"third_party/gpus/cuda/include/driver_types.h\"\n #include \"xla/backends/profiler/gpu/cuda_test.h\"\n+#include \"tsl/profiler/lib/scoped_annotation.h\"\n \n namespace xla {\n namespace profiler {\n namespace test {\n \n+using tsl::profiler::ScopedAnnotation;\n+\n namespace {\n \n // Simple printf kernel.\n@@ -175,11 +178,19 @@ void CudaGraphCreateAndExecute() {\n   memcpy_params.extent.width = kNumBytes;\n   memcpy_params.extent.height = 1;\n   memcpy_params.extent.depth = 1;\n-  cudaGraphAddMemcpyNode(&nodes[0], graph, nullptr, 0, &memcpy_params);\n+  {\n+    ScopedAnnotation memcpy_1_annotation(\n+        \"Thunk:#name=my_module/prep,hlo_op=memcpy.1#\");\n+    cudaGraphAddMemcpyNode(&nodes[0], graph, nullptr, 0, &memcpy_params);\n+  }\n \n   memcpy_params.srcPtr.ptr = vec_b.data();\n   memcpy_params.dstPtr.ptr = d_b;\n-  cudaGraphAddMemcpyNode(&nodes[1], graph, nullptr, 0, &memcpy_params);\n+  {\n+    ScopedAnnotation memcpy_2_annotation(\n+        \"Thunk:#name=my_module/prep,hlo_op=memcpy.2#\");\n+    cudaGraphAddMemcpyNode(&nodes[1], graph, nullptr, 0, &memcpy_params);\n+  }\n \n   // Init kernel params.\n   int num = kNumElements;\n@@ -191,25 +202,37 @@ void CudaGraphCreateAndExecute() {\n   kernel_params.sharedMemBytes = 0;\n   kernel_params.kernelParams = (void **)kernelArgs;\n   kernel_params.extra = nullptr;\n-\n-  cudaGraphAddKernelNode(&nodes[2], graph, &nodes[0], 2, &kernel_params);\n+  {\n+    ScopedAnnotation add_1_annotation(\n+        \"Thunk:#name=my_module/body,hlo_op=add.1#\");\n+    cudaGraphAddKernelNode(&nodes[2], graph, &nodes[0], 2, &kernel_params);\n+  }\n \n   kernel_params.func = (void *)VecSub;\n-  cudaGraphAddKernelNode(&nodes[3], graph, &nodes[2], 1, &kernel_params);\n-\n+  {\n+    ScopedAnnotation sub_1_annotation(\n+        \"Thunk:#name=my_module/body,hlo_op=sub.1#\");\n+    cudaGraphAddKernelNode(&nodes[3], graph, &nodes[2], 1, &kernel_params);\n+  }\n   memcpy_params.kind = cudaMemcpyDeviceToHost;\n   memcpy_params.srcPtr.ptr = d_c;\n   memcpy_params.dstPtr.ptr = vec_c.data();\n   memcpy_params.extent.width = kNumBytes;\n   memcpy_params.extent.height = 1;\n   memcpy_params.extent.depth = 1;\n-  cudaGraphAddMemcpyNode(&nodes[4], graph, &nodes[3], 1, &memcpy_params);\n-\n+  {\n+    ScopedAnnotation memcpy_3_annotation(\n+        \"Thunk:#name=my_module/post,hlo_op=memcpy.3#\");\n+    cudaGraphAddMemcpyNode(&nodes[4], graph, &nodes[3], 1, &memcpy_params);\n+  }\n   cudaGraphClone(&cloned_graph, graph);\n \n-  cudaGraphInstantiate(&graph_exec, cloned_graph, nullptr, nullptr, 0);\n+  cudaGraphInstantiate(&graph_exec, graph, nullptr, nullptr, 0);\n \n-  cudaGraphLaunch(graph_exec, stream);\n+  {\n+    ScopedAnnotation module_annotation(\"Thunk:#name=my_module,hlo_op=call.1#\");\n+    cudaGraphLaunch(graph_exec, stream);\n+  }\n \n   cudaStreamSynchronize(stream);\n "
        },
        {
            "sha": "076b2bfba68dd245ab36d2eff591ce128384511c",
            "filename": "third_party/xla/xla/backends/profiler/gpu/cupti_buffer_events.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 16,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_buffer_events.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_buffer_events.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_buffer_events.cc?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -159,8 +159,12 @@ const char *getActivityUnifiedMemoryKindString(\n template <typename CuptiActivity>\n void SetEventGraphId(CuptiTracerEvent &event,\n                      const CuptiActivity *cupti_activity) {\n+  // In current implementation, CuptiActivityKernelTy, CuptiActivityMemcpyTy,\n+  // CuptiActivityMemcpyP2PTy and CuptiActivityMemsetTy all have graphNodeId\n+  // when they have graphId.\n   if constexpr (CuptiActivityHasGraphId<CuptiActivity>::value) {\n     event.graph_id = cupti_activity->graphId;\n+    event.graph_node_id = cupti_activity->graphNodeId;\n   }\n }\n \n@@ -690,23 +694,27 @@ absl::string_view StringDeduper::Dedup(absl::string_view str,\n   return absl::string_view();\n }\n \n-void AnnotationMap::Add(uint32_t device_id, uint32_t correlation_id,\n-                        const absl::string_view annotation,\n-                        const absl::string_view nvtx_range,\n-                        int64_t scope_range_id) {\n-  if (annotation.empty() && nvtx_range.empty()) return;\n-  VLOG(3) << \"Add annotation: device_id: \" << device_id\n-          << \" correlation_id: \" << correlation_id\n-          << \" annotation: \" << annotation;\n-  if (device_id >= per_device_map_.size()) return;\n-  auto &per_device_map = per_device_map_[device_id];\n-  if (per_device_map.annotation_deduper.Size() < max_size_) {\n-    AnnotationInfo info;\n-    info.annotation = per_device_map.annotation_deduper.Dedup(annotation);\n-    info.nvtx_range = per_device_map.nvtx_range_deduper.Dedup(nvtx_range);\n-    info.scope_range_id = scope_range_id;\n-    per_device_map.correlation_map.emplace(correlation_id, info);\n+absl::string_view AnnotationMap::Add(uint32_t device_id,\n+                                     uint32_t correlation_id,\n+                                     const absl::string_view annotation,\n+                                     const absl::string_view nvtx_range,\n+                                     int64_t scope_range_id) {\n+  if ((!annotation.empty() || !nvtx_range.empty()) &&\n+      device_id < per_device_map_.size()) {\n+    VLOG(3) << \"Add annotation: device_id: \" << device_id\n+            << \" correlation_id: \" << correlation_id\n+            << \" annotation: \" << annotation;\n+    auto& per_device_map = per_device_map_[device_id];\n+    if (per_device_map.annotation_deduper.Size() < max_size_) {\n+      AnnotationInfo info;\n+      info.annotation = per_device_map.annotation_deduper.Dedup(annotation);\n+      info.nvtx_range = per_device_map.nvtx_range_deduper.Dedup(nvtx_range);\n+      info.scope_range_id = scope_range_id;\n+      per_device_map.correlation_map.emplace(correlation_id, info);\n+      return info.annotation;\n+    }\n   }\n+  return \"\";\n }\n \n AnnotationMap::AnnotationInfo AnnotationMap::LookUp("
        },
        {
            "sha": "3660fdf2482cb9596f62e02f04921a4523cc5abd",
            "filename": "third_party/xla/xla/backends/profiler/gpu/cupti_buffer_events.h",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_buffer_events.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_buffer_events.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_buffer_events.h?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -287,9 +287,14 @@ class AnnotationMap {\n   explicit AnnotationMap(uint64_t max_size, uint32_t num_gpus)\n       : max_size_(max_size), per_device_map_(num_gpus) {}\n \n-  void Add(uint32_t device_id, uint32_t correlation_id,\n-           absl::string_view annotation, absl::string_view nvtx_range,\n-           int64_t scope_range_id = 0);\n+  // Returns a string_view of the dedupped annotation string. The string_view is\n+  // valid as long as the AnnotationMap is alive. If the annotation is dropped\n+  // due to size limit or any other reason, an empty string_view will be\n+  // returned.\n+  absl::string_view Add(uint32_t device_id, uint32_t correlation_id,\n+                        absl::string_view annotation,\n+                        absl::string_view nvtx_range,\n+                        int64_t scope_range_id = 0);\n \n   AnnotationInfo LookUp(uint32_t device_id, uint32_t correlation_id) const\n       ABSL_ATTRIBUTE_LIFETIME_BOUND;"
        },
        {
            "sha": "8b8fb84fd092fb2dbe0614d03c9faa349aee7d54",
            "filename": "third_party/xla/xla/backends/profiler/gpu/cupti_collector.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 76,
            "changes": 118,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_collector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_collector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_collector.cc?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -420,60 +420,6 @@ class PerDeviceCollector {\n \n  public:\n   PerDeviceCollector() = default;\n-  void SetCudaGraphIdMap(\n-      absl::flat_hash_map<uint32_t, uint32_t>& cuda_graph_id_map) {\n-    per_device_cuda_graph_id_map_.insert(cuda_graph_id_map.begin(),\n-                                         cuda_graph_id_map.end());\n-  }\n-\n-  void SetCudaGraphNodeIdMap(\n-      absl::flat_hash_map<uint32_t, absl::flat_hash_map<uint64_t, uint64_t>>&\n-          cuda_graph_node_id_map) {\n-    for (const auto& [graph_id, node_map] : cuda_graph_node_id_map) {\n-      per_device_cuda_graph_node_id_map_[graph_id].insert(node_map.begin(),\n-                                                          node_map.end());\n-    }\n-  }\n-\n-  void AddGraphIdMapsToPlane(XPlaneBuilder* device_plane) {\n-    // Create a new line for graph metadata\n-    XLineBuilder line =\n-        device_plane->GetOrCreateLine(StatType::kGraphMetadataLineId);\n-    line.SetName(GetStatTypeStr(StatType::kGraphMetadataLineId));\n-    line.SetTimestampNs(0);\n-\n-    // Add the graph id map to the device plane\n-    for (const auto& [graph_id, orig_graph_id] :\n-         per_device_cuda_graph_id_map_) {\n-      XEventBuilder event =\n-          line.AddEvent(*device_plane->GetOrCreateEventMetadata(\n-              GetStatTypeStr(StatType::kCudaGraphMapId)));\n-      event.AddStatValue(*device_plane->GetOrCreateStatMetadata(\n-                             GetStatTypeStr(StatType::kCudaGraphId)),\n-                         graph_id);\n-      event.AddStatValue(*device_plane->GetOrCreateStatMetadata(\n-                             GetStatTypeStr(StatType::kCudaGraphOrigId)),\n-                         orig_graph_id);\n-    }\n-    // Add the node id map to the device plane\n-    for (const auto& [graph_id, node_map] :\n-         per_device_cuda_graph_node_id_map_) {\n-      for (const auto& [node_id, orig_node_id] : node_map) {\n-        XEventBuilder event =\n-            line.AddEvent(*device_plane->GetOrCreateEventMetadata(\n-                GetStatTypeStr(StatType::kCudaGraphNodeMapId)));\n-        event.AddStatValue(*device_plane->GetOrCreateStatMetadata(\n-                               GetStatTypeStr(StatType::kCudaGraphId)),\n-                           graph_id);\n-        event.AddStatValue(*device_plane->GetOrCreateStatMetadata(\n-                               GetStatTypeStr(StatType::kCudaGraphNodeId)),\n-                           node_id);\n-        event.AddStatValue(*device_plane->GetOrCreateStatMetadata(\n-                               GetStatTypeStr(StatType::kCudaGraphOrigNodeId)),\n-                           orig_node_id);\n-      }\n-    }\n-  }\n \n   void AddEvent(CuptiTracerEvent&& event) {\n     absl::MutexLock l(m_);\n@@ -641,9 +587,6 @@ class PerDeviceCollector {\n   std::vector<CuptiTracerEvent> events_ TF_GUARDED_BY(m_);\n   cudaOccDeviceProp device_properties_;\n   absl::flat_hash_map<DeviceOccupancyParams, OccupancyStats> occupancy_cache_;\n-  absl::flat_hash_map<uint32_t, uint32_t> per_device_cuda_graph_id_map_;\n-  absl::flat_hash_map<uint32_t, absl::flat_hash_map<uint64_t, uint64_t>>\n-      per_device_cuda_graph_node_id_map_;\n };\n \n // Using two iterator of the CuptiTracerEvent queue to mark the current and\n@@ -758,17 +701,26 @@ void CuptiTraceCollector::OnTracerCollectedCallbackData(\n     auto event_in_queue = min_heap.top();\n     min_heap.pop();\n     auto& event = event_in_queue.Event();\n+    absl::string_view deduped_annotation{};\n     if (event.type == CuptiTracerEventType::Generic &&\n         event.generic_info.cbid ==\n             CUPTI_DRIVER_TRACE_CBID_cuLaunchCooperativeKernelMultiDevice) {\n       for (uint32_t device = 0; device < options_.num_gpus; ++device) {\n-        annotation_map_.Add(device, event.correlation_id, event.annotation,\n-                            event.nvtx_range, event.scope_range_id);\n+        deduped_annotation =\n+            annotation_map_.Add(device, event.correlation_id, event.annotation,\n+                                event.nvtx_range, event.scope_range_id);\n       }\n     } else {\n-      annotation_map_.Add(event.device_id, event.correlation_id,\n-                          event.annotation, event.nvtx_range,\n-                          event.scope_range_id);\n+      deduped_annotation = annotation_map_.Add(\n+          event.device_id, event.correlation_id, event.annotation,\n+          event.nvtx_range, event.scope_range_id);\n+    }\n+    if (event.type == CuptiTracerEventType::CudaGraph && event.graph_id != 0 &&\n+        event.graph_node_id != 0) {\n+      if (!deduped_annotation.empty()) {\n+        graph_node_annotations_.insert(\n+            {{event.graph_id, event.graph_node_id}, deduped_annotation});\n+      }\n     }\n     // Clear the annotation and nvtx_range of the Callback API events, as they\n     // are now in the combined AnnotationMap which will be used by the\n@@ -847,20 +799,39 @@ class CuptiTraceCollectorImpl : public CuptiTraceCollector {\n       // followed AddEvent() processing.\n       if (!AddNvtxMarker(event)) return;\n     }\n+\n+    // If this is a CudaGraphNodeMap event, we need to record the mapping from\n+    // graph_id/graph_node_id to orig_graph_id/orig_graph_node_id.\n     if (event.type == CuptiTracerEventType::CudaGraphNodeMap) {\n+      cuda_graph_id_map_[event.graph_id] = event.cuda_graph_info.orig_graph_id;\n       cuda_graph_node_id_map_[event.graph_id][event.graph_node_id] =\n           event.cuda_graph_info.orig_graph_node_id;\n-      cuda_graph_id_map_[event.graph_id] = event.cuda_graph_info.orig_graph_id;\n+      return;\n     }\n-    if (event.type != CuptiTracerEventType::CudaGraphNodeMap) {\n-      per_device_collector_[event.device_id].AddEvent(std::move(event));\n+\n+    // For activity events with graph_id and graph_node_id, we need to rewrite\n+    // the annotation to reflect the detail framework information which are got\n+    // during the callback for cuda graph creation and nodes insertion.\n+    if (event.source == CuptiTracerEventSource::Activity &&\n+        (event.graph_id != 0 && event.graph_node_id != 0)) {\n+      // We need to rewrite the annotation of this inner node event in cuda\n+      // graph device plane.\n+      auto orig_graph_id_it = cuda_graph_id_map_.find(event.graph_id);\n+      if (orig_graph_id_it != cuda_graph_id_map_.end()) {\n+        uint32_t orig_graph_id = orig_graph_id_it->second;\n+        const auto& node_id_2_orig = cuda_graph_node_id_map_[event.graph_id];\n+        auto orig_node_id_it = node_id_2_orig.find(event.graph_node_id);\n+        if (orig_node_id_it != node_id_2_orig.end()) {\n+          uint64_t orig_node_id = orig_node_id_it->second;\n+          auto annotation_it =\n+              graph_node_annotations_.find({orig_graph_id, orig_node_id});\n+          if (annotation_it != graph_node_annotations_.end()) {\n+            event.annotation = annotation_it->second;\n+          }\n+        }\n+      }\n     }\n-    per_device_collector_[event.device_id].SetCudaGraphIdMap(\n-        cuda_graph_id_map_);\n-    per_device_collector_[event.device_id].SetCudaGraphNodeIdMap(\n-        cuda_graph_node_id_map_);\n-    cuda_graph_node_id_map_.clear();\n-    cuda_graph_id_map_.clear();\n+    per_device_collector_[event.device_id].AddEvent(std::move(event));\n   }\n   void OnEventsDropped(const std::string& reason,\n                        uint32_t num_events) override {\n@@ -910,11 +881,6 @@ class CuptiTraceCollectorImpl : public CuptiTraceCollector {\n           device_ordinal, &device_plane);\n       num_events += per_device_collector_[device_ordinal].Flush(\n           start_gpu_ns_, end_gpu_ns, &device_plane, &host_plane, &nvtx_plane);\n-      if (options_.dump_graph_nope_mapping) {\n-        // Add the graph id maps to the device plane\n-        per_device_collector_[device_ordinal].AddGraphIdMapsToPlane(\n-            &device_plane);\n-      }\n       NormalizeTimeStamps(&device_plane, start_walltime_ns_);\n     }\n     NormalizeTimeStamps(&host_plane, start_walltime_ns_);"
        },
        {
            "sha": "8255d12cadeafd58d10d3260f9916e21e211b669",
            "filename": "third_party/xla/xla/backends/profiler/gpu/cupti_collector.h",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_collector.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_collector.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_collector.h?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -24,6 +24,8 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/backends/profiler/gpu/cupti_buffer_events.h\"\n #include \"xla/tsl/profiler/utils/xplane_builder.h\"\n #include \"tsl/profiler/protobuf/xplane.pb.h\"\n@@ -42,8 +44,6 @@ struct CuptiTracerCollectorOptions {\n   uint64_t max_annotation_strings = 1024 * 1024;\n   // Number of GPUs involved.\n   uint32_t num_gpus;\n-  // Whether to dump the graph nope mapping.\n-  bool dump_graph_nope_mapping = false;\n };\n // This struct will be used to store the PM Sampling data.\n // Same as CUDA 12.6.2 extras/CUPTI/samples/pm_sampling/pm_sampling.h\n@@ -132,6 +132,9 @@ class CuptiTraceCollector {\n   CuptiTracerCollectorOptions options_;\n   // map of child_scope_id -> parent_scope_id\n   ScopeRangeIdTree scope_range_id_tree_;\n+  // <graph_id, graph_node_id> to annotation string during creation of the node.\n+  absl::flat_hash_map<std::pair<uint32_t, uint64_t>, absl::string_view>\n+      graph_node_annotations_ = {};\n \n  private:\n   AnnotationMap annotation_map_;"
        },
        {
            "sha": "e56516095f5955fcac5df1df23ba086920ccc352",
            "filename": "third_party/xla/xla/backends/profiler/gpu/cupti_tracer.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_tracer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_tracer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_tracer.cc?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -621,11 +621,11 @@ void SetCuMemHostUnregisterEventUponApiExit(\n struct GraphResourceCreationInfo {\n   uint32_t graph_id = 0;\n   uint32_t orig_graph_id = 0;\n-  absl::flat_hash_map<uint64_t, uint64_t> node_id_map;\n+  absl::flat_hash_map<uint64_t, uint64_t> node_id_map = {};\n };\n \n static GraphResourceCreationInfo& GetGraphResourceCreationInfo() {\n-  static thread_local GraphResourceCreationInfo per_thread_graph_info;\n+  static thread_local GraphResourceCreationInfo per_thread_graph_info{};\n   return per_thread_graph_info;\n }\n "
        },
        {
            "sha": "05d1738a92c2e74a6378d83bb808b856d123fff9",
            "filename": "third_party/xla/xla/backends/profiler/gpu/cupti_tracer_options_utils.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_tracer_options_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/25eb194263b714e73d21cfe03a797e8b22de0d5d/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_tracer_options_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fprofiler%2Fgpu%2Fcupti_tracer_options_utils.cc?ref=25eb194263b714e73d21cfe03a797e8b22de0d5d",
            "patch": "@@ -70,10 +70,6 @@ absl::Status UpdateCuptiTracerOptionsFromProfilerOptions(\n                           collector_options.max_annotation_strings = value;\n                         }));\n \n-  TF_RETURN_IF_ERROR(SetValue<bool>(\n-      profile_options, \"gpu_dump_graph_node_mapping\", input_keys,\n-      [&](bool value) { collector_options.dump_graph_nope_mapping = value; }));\n-\n   TF_RETURN_IF_ERROR(SetValue<int64_t>(\n       profile_options, \"gpu_num_chips_to_profile_per_task\", input_keys,\n       [&](int64_t value) {"
        }
    ],
    "stats": {
        "total": 228,
        "additions": 115,
        "deletions": 113
    }
}