{
    "author": "chsigg",
    "message": "Refactor: Use OpType::create instead of rewriter.create<OpType>\n\nThis change updates the creation of MLIR operations to use the `OpType::create(rewriter, ...)` static method instead of `rewriter.create<OpType>(...)`.\n\nOpTy::create allows inspecting overloads and jumping to source, which rewriter.create does not.\nPiperOrigin-RevId: 836566542",
    "sha": "c82ee8e5355dc601efe77af61382e6904c18cf84",
    "files": [
        {
            "sha": "888983ae05acecc0dc2562c1fa39e6de82ba5d3e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -229,7 +229,7 @@ Value EmitterBase::EmitWorkGroupId(mlir::ImplicitLocOpBuilder& builder,\n   int64_t count = dim == WorkGroupDimension::x   ? counts.x\n                   : dim == WorkGroupDimension::y ? counts.y\n                                                  : counts.z;\n-  auto block_id = builder.create<WorkGroupIdOp>(dim);\n+  auto block_id = WorkGroupIdOp::create(builder, dim);\n   block_id->setAttr(\"xla.range\", builder.getIndexArrayAttr({0, count - 1}));\n   return block_id;\n }\n@@ -238,8 +238,8 @@ Value EmitterBase::EmitBlockId(mlir::ImplicitLocOpBuilder& builder,\n                                int dim) const {\n   const auto& counts = launch_dimensions().block_counts();\n   int64_t count = dim == 0 ? counts.x : dim == 1 ? counts.y : counts.z;\n-  auto block_id = builder.create<mlir::gpu::BlockIdOp>(\n-      static_cast<mlir::gpu::Dimension>(dim));\n+  auto block_id = mlir::gpu::BlockIdOp::create(\n+      builder, static_cast<mlir::gpu::Dimension>(dim));\n   block_id->setAttr(\"xla.range\", builder.getIndexArrayAttr({0, count - 1}));\n   return block_id;\n }\n@@ -248,8 +248,8 @@ Value EmitterBase::EmitThreadId(mlir::ImplicitLocOpBuilder& builder,\n                                 int dim) const {\n   const auto& counts = launch_dimensions().thread_counts_per_block();\n   int64_t count = dim == 0 ? counts.x : dim == 1 ? counts.y : counts.z;\n-  auto thread_id = builder.create<mlir::gpu::ThreadIdOp>(\n-      static_cast<mlir::gpu::Dimension>(dim));\n+  auto thread_id = mlir::gpu::ThreadIdOp::create(\n+      builder, static_cast<mlir::gpu::Dimension>(dim));\n   thread_id->setAttr(\"xla.range\", builder.getIndexArrayAttr({0, count - 1}));\n   return thread_id;\n }"
        },
        {
            "sha": "151456f686069adf88a46c484d4b7075078f61fa",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -75,11 +75,11 @@ class DummyCopyEmitter : public EmitterBase {\n     mlir::ImplicitLocOpBuilder b(entry_function.getLoc(), entry_function);\n     b.setInsertionPointToStart(entry_function.addEntryBlock());\n     auto thread_id = EmitThreadId(b, 0);\n-    auto value = b.create<mlir::tensor::ExtractOp>(\n-        entry_function.getArgument(0), mlir::ValueRange{thread_id});\n-    auto result = b.create<mlir::tensor::InsertOp>(\n-        value, entry_function.getArgument(1), mlir::ValueRange{thread_id});\n-    b.create<mlir::func::ReturnOp>(result->getResults());\n+    auto value = mlir::tensor::ExtractOp::create(\n+        b, entry_function.getArgument(0), mlir::ValueRange{thread_id});\n+    auto result = mlir::tensor::InsertOp::create(\n+        b, value, entry_function.getArgument(1), mlir::ValueRange{thread_id});\n+    mlir::func::ReturnOp::create(b, result->getResults());\n     return absl::OkStatus();\n   }\n };"
        },
        {
            "sha": "192292d8f81964d3d9a28b913acdd746013e9409",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 20,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -223,7 +223,7 @@ PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n         addf->setAttr(\"fastmath\", no_signed_zeros);\n       });\n       absl::c_copy(\n-          nested_b.create<PureCallOp>(reducer, reduce_args).getResults(),\n+          PureCallOp::create(nested_b, reducer, reduce_args).getResults(),\n           results.begin() + start);\n     }\n     struct SideOutput {\n@@ -245,8 +245,8 @@ PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n          llvm::zip(side_outputs, side_output_values)) {\n       // The first iter args are the outputs.\n       int offset = OutputIndex(side_output, 0);\n-      results[offset] = builder.create<mlir::tensor::InsertOp>(\n-          values.scalar, iter_args[offset], values.indices);\n+      results[offset] = mlir::tensor::InsertOp::create(\n+          builder, values.scalar, iter_args[offset], values.indices);\n     }\n     return results;\n   };\n@@ -286,8 +286,8 @@ SmallVector<Value> ReductionFusion::EmitterState::WriteToSharedMemory(\n     for (int i = 0; i < reduction->operand_count() / 2; ++i) {\n       auto tile_shape = ShapeUtil::MakeShapeWithDescendingLayout(\n           reduction->operand(i)->shape().element_type(), shape);\n-      tiles.push_back(builder.create<AllocateSharedOp>(\n-          emitters::TensorShapeToMlirType(tile_shape, builder)));\n+      tiles.push_back(AllocateSharedOp::create(\n+          builder, emitters::TensorShapeToMlirType(tile_shape, builder)));\n     }\n   }\n \n@@ -302,19 +302,20 @@ SmallVector<Value> ReductionFusion::EmitterState::WriteToSharedMemory(\n         for (auto* hero : reductions) {\n           for (auto value : values.at(hero)) {\n             if (mlir::isa<mlir::VectorType>(value.getType())) {\n-              value = builder.create<mlir::vector::ExtractOp>(\n-                  value, symbol_values.back());\n+              value = mlir::vector::ExtractOp::create(builder, value,\n+                                                      symbol_values.back());\n             }\n             auto& tile = written[shared_index++];\n-            tile = builder.create<mlir::tensor::InsertOp>(value, tile, indices);\n+            tile =\n+                mlir::tensor::InsertOp::create(builder, value, tile, indices);\n           }\n         }\n         return written;\n       });\n \n   // Wait for the entire tile to be written.\n   auto synced_tiles =\n-      builder.create<SyncThreadsOp>(mlir::TypeRange(tiles), written_tiles)\n+      SyncThreadsOp::create(builder, mlir::TypeRange(tiles), written_tiles)\n           .getResults();\n \n   return synced_tiles;\n@@ -325,8 +326,8 @@ HloValueMap ReductionFusion::EmitterState::ShuffleReduce(\n     const HloValueMap& per_thread_values, int max_dist) {\n   HloValueMap results;\n   for (auto* hero : reductions) {\n-    auto reduce = builder.create<ShuffleReduceOp>(\n-        GetReducer(hero), per_thread_values.at(hero), max_dist);\n+    auto reduce = ShuffleReduceOp::create(builder, GetReducer(hero),\n+                                          per_thread_values.at(hero), max_dist);\n     results[hero] = reduce.getResults();\n   }\n   return results;\n@@ -366,8 +367,8 @@ mlir::ValueRange ReductionFusion::EmitterState::ReduceViaSharedMemory(\n           auto& args = reduce_args[hero];\n           for (auto init : inits.at(hero)) {\n             // If a warp didn't write anything, use the init values instead.\n-            auto extract = builder.create<PredicatedExtractOp>(\n-                read_condition, init, tiles[tile_index++], indices);\n+            auto extract = PredicatedExtractOp::create(\n+                builder, read_condition, init, tiles[tile_index++], indices);\n             args.push_back(extract.getResult());\n           }\n         }\n@@ -483,17 +484,18 @@ absl::Status ReductionFusion::EmitEntryFunction(\n   b.setInsertionPointToStart(entry_function.addEntryBlock());\n   state.thread_and_block_ids = EmitThreadAndBlockIds(b);\n   if (reduction_heroes_.size() == 1) {\n-    b.create<mlir::func::ReturnOp>(EmitReduction(0, state));\n+    mlir::func::ReturnOp::create(b, EmitReduction(0, state));\n     return absl::OkStatus();\n   }\n   SmallVector<int64_t> cases(reduction_heroes_.size() - 1);\n   absl::c_iota(cases, 1);  // `default` is region 0.\n-  auto switch_op = b.create<mlir::scf::IndexSwitchOp>(\n-      entry_function.getResultTypes(), EmitBlockId(b, 1), cases, cases.size());\n-  b.create<mlir::func::ReturnOp>(switch_op.getResults());\n+  auto switch_op =\n+      mlir::scf::IndexSwitchOp::create(b, entry_function.getResultTypes(),\n+                                       EmitBlockId(b, 1), cases, cases.size());\n+  mlir::func::ReturnOp::create(b, switch_op.getResults());\n   for (auto [id, region] : llvm::enumerate(switch_op->getRegions())) {\n     b.setInsertionPointToStart(&region.emplaceBlock());\n-    b.create<mlir::scf::YieldOp>(EmitReduction(id, state));\n+    mlir::scf::YieldOp::create(b, EmitReduction(id, state));\n   }\n   return absl::OkStatus();\n }\n@@ -602,8 +604,8 @@ SmallVector<Value> ReductionFusion::EvaluateEpilogue(\n                                 state.thread_and_block_ids, symbol_values, b);\n     for (auto [result_index, result] : llvm::enumerate(values.at(root))) {\n       auto& output = outputs[state.OutputIndex(root, result_index)];\n-      output = b.create<PredicatedInsertOp>(thread_has_output, result, output,\n-                                            output_indices);\n+      output = PredicatedInsertOp::create(b, thread_has_output, result, output,\n+                                          output_indices);\n     }\n   }\n   return outputs;"
        },
        {
            "sha": "0b43566026296b84f3866e4207f537cdf6925108",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 41,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -125,11 +125,11 @@ ValueRange EmitUpdateIf(\n           condition,\n           [&](OpBuilder& then_b, Location then_loc) -> void {\n             ImplicitLocOpBuilder implicit_then_b(then_loc, then_b);\n-            then_b.create<scf::YieldOp>(then_loc,\n-                                        updated_values_fn(implicit_then_b));\n+            scf::YieldOp::create(then_b, then_loc,\n+                                 updated_values_fn(implicit_then_b));\n           },\n           [&](OpBuilder& else_b, Location else_loc) -> void {\n-            else_b.create<scf::YieldOp>(else_loc, values);\n+            scf::YieldOp::create(else_b, else_loc, values);\n           })\n       .getResults();\n }\n@@ -140,10 +140,10 @@ Value EmitBoundsCheck(ImplicitLocOpBuilder& b,\n                       absl::Span<const int64_t> slice_shape,\n                       absl::Span<const int64_t> operand_shape,\n                       ValueRange offsets) {\n-  Value in_bounds = b.create<arith::ConstantIntOp>(b.getI1Type(), 1);\n+  Value in_bounds = arith::ConstantIntOp::create(b, b.getI1Type(), 1);\n   for (auto [update_dim, operand_dim, offset] :\n        llvm::zip(slice_shape, operand_shape, offsets)) {\n-    Value ub = b.create<arith::ConstantIndexOp>(operand_dim - update_dim);\n+    Value ub = arith::ConstantIndexOp::create(b, operand_dim - update_dim);\n     // One bounds check is enough even for signed indices: `sge 0` is\n     // implied by `ule ub`, because `ub >= 0`.\n     in_bounds = b.createOrFold<arith::AndIOp>(\n@@ -155,7 +155,7 @@ Value EmitBoundsCheck(ImplicitLocOpBuilder& b,\n \n Value EmitInequalityCheck(ImplicitLocOpBuilder& b, ValueRange lhs,\n                           ValueRange rhs) {\n-  Value not_equal = b.create<arith::ConstantIntOp>(b.getI1Type(), 0);\n+  Value not_equal = arith::ConstantIntOp::create(b, b.getI1Type(), 0);\n   for (auto [lhs_elem, rhs_elem] : llvm::zip(lhs, rhs)) {\n     not_equal = b.createOrFold<arith::OrIOp>(\n         not_equal, b.createOrFold<arith::CmpIOp>(arith::CmpIPredicate::ne,\n@@ -209,7 +209,7 @@ SmallVector<Value, 4> PadWithZeros(ValueRange values, int64_t size,\n                                    ImplicitLocOpBuilder& b) {\n   SmallVector<Value, 4> padded_values(values.begin(), values.end());\n   if (values.size() >= size) return padded_values;\n-  auto zero = b.create<arith::ConstantIndexOp>(0);\n+  auto zero = arith::ConstantIndexOp::create(b, 0);\n   for (int i = values.size(); i < size; ++i) {\n     padded_values.push_back(zero);\n   }\n@@ -283,13 +283,13 @@ SmallVector<Value, 4> EmitterHelper::ExtractOffsets(ImplicitLocOpBuilder& b,\n   offsets.reserve(description_->index_vector_length);\n   for (int i = 0; i < description_->index_vector_length; ++i) {\n     SmallVector<Value, 4> indices_tensor_indices = {\n-        slice_id, b.create<arith::ConstantIndexOp>(i)};\n+        slice_id, arith::ConstantIndexOp::create(b, i)};\n     auto index = GetIndicesElement(b, indices_tensor_indices);\n     index =\n         IsUnsignedIntegralType(\n             description_->scatter->scatter_indices()->shape().element_type())\n-            ? b.create<arith::IndexCastUIOp>(index_type, index).getResult()\n-            : b.create<arith::IndexCastOp>(index_type, index).getResult();\n+            ? arith::IndexCastUIOp::create(b, index_type, index).getResult()\n+            : arith::IndexCastOp::create(b, index_type, index).getResult();\n     offsets.push_back(index);\n   }\n   return offsets;\n@@ -304,27 +304,27 @@ Value EmitterHelper::EmitScatterComputation(ImplicitLocOpBuilder& b,\n     auto operand_elem = GetOperandElement(b, indices);\n     auto reduced_val = emitters::InlineBlock(b, reducer.getBody().front(),\n                                              {operand_elem, update_elem})[0];\n-    return b.create<tensor::InsertOp>(reduced_val, output_tensor, indices);\n+    return tensor::InsertOp::create(b, reduced_val, output_tensor, indices);\n   }\n-  auto atomic_rmw = b.create<AtomicRMWOp>(output_tensor, indices);\n+  auto atomic_rmw = AtomicRMWOp::create(b, output_tensor, indices);\n   OpBuilder body_b = atomic_rmw.getBodyBuilder();\n   auto reduced_val =\n       emitters::InlineBlock(body_b, reducer.getBody().front(),\n                             {atomic_rmw.getCurrentValue(), update_elem})[0];\n-  body_b.create<xla::YieldOp>(reducer->getLoc(), reduced_val);\n+  xla::YieldOp::create(body_b, reducer->getLoc(), reduced_val);\n   return atomic_rmw->getResult(0);\n }\n \n SmallVector<Value> EmitterHelper::WriteAccumulatedElementToOutput(\n     ImplicitLocOpBuilder& b, Value accumulator, ValueRange accumulator_indices,\n     ValueRange slice_indices, ValueRange offsets, Value output_tensor) const {\n-  Value accumulator_elem = b.create<vector::ExtractOp>(\n-      accumulator, mlir::getAsOpFoldResult(accumulator_indices));\n+  Value accumulator_elem = vector::ExtractOp::create(\n+      b, accumulator, mlir::getAsOpFoldResult(accumulator_indices));\n \n   SmallVector<Value, 4> output_indices(offsets.begin(), offsets.end());\n   for (int i = 0; i < output_indices.size(); ++i) {\n     output_indices[i] =\n-        b.create<arith::AddIOp>(slice_indices[i + 1], output_indices[i]);\n+        arith::AddIOp::create(b, slice_indices[i + 1], output_indices[i]);\n   }\n   return {EmitScatterComputation(b, output_indices, accumulator_elem,\n                                  output_tensor)};\n@@ -488,7 +488,7 @@ void EmitNaiveImplementation(ImplicitLocOpBuilder& b,\n           .front();\n   Value index_id_in_bounds = b.createOrFold<arith::CmpIOp>(\n       arith::CmpIPredicate::ult, thread_id_to_index_id_value,\n-      b.create<arith::ConstantIndexOp>(description.num_slices));\n+      arith::ConstantIndexOp::create(b, description.num_slices));\n   auto result = EmitUpdateIf(\n       b, index_id_in_bounds, {output_tensor},\n       [&](ImplicitLocOpBuilder& outer_nested_b) -> SmallVector<Value> {\n@@ -515,8 +515,8 @@ void EmitNaiveImplementation(ImplicitLocOpBuilder& b,\n                     output_indices = PadWithZeros(output_indices, output_rank,\n                                                   update_loop_b);\n                     for (int i = 0; i < output_indices.size(); ++i) {\n-                      output_indices[i] = update_loop_b.create<arith::AddIOp>(\n-                          map_results[i + 1], output_indices[i]);\n+                      output_indices[i] = arith::AddIOp::create(\n+                          update_loop_b, map_results[i + 1], output_indices[i]);\n                     }\n                     Value output_tensor = output_tensors.front();\n                     Value updated_output = helper.EmitScatterComputation(\n@@ -527,7 +527,7 @@ void EmitNaiveImplementation(ImplicitLocOpBuilder& b,\n             });\n         return predicated_update;\n       });\n-  b.create<ReturnOp>(result.front());\n+  ReturnOp::create(b, result.front());\n }\n \n absl::Status ScatterWithDistributedUpdates::EmitEntryFunctionImpl(\n@@ -647,8 +647,9 @@ Value ScatterWithDistributedIndices::InitializeAccumulator(\n       num_elements_per_slice, num_warps_per_slice_ * warp_size_ * vector_size_);\n   auto accumulator_type =\n       VectorType::get({update_iterations_per_thread, vector_size_}, elem_type);\n-  return b.create<arith::ConstantOp>(\n-      accumulator_type, emitters::GetZeroDenseElementsAttr(accumulator_type));\n+  return arith::ConstantOp::create(\n+      b, accumulator_type,\n+      emitters::GetZeroDenseElementsAttr(accumulator_type));\n }\n \n absl::Status ScatterWithDistributedIndices::EmitEntryFunctionImpl(\n@@ -686,10 +687,10 @@ absl::Status ScatterWithDistributedIndices::EmitEntryFunctionImpl(\n \n   // Prepare loop initial values. Inits are packed as\n   // [index_changed, is_inbounds, index_0,  ..., accumulator].\n-  Value is_inbounds_init = b.create<arith::ConstantIntOp>(b.getI1Type(), 0);\n-  Value slice_id_init = b.create<arith::ConstantIndexOp>(0);\n+  Value is_inbounds_init = arith::ConstantIntOp::create(b, b.getI1Type(), 0);\n+  Value slice_id_init = arith::ConstantIndexOp::create(b, 0);\n   std::vector<Value> indices_init(description_.index_vector_length,\n-                                  b.create<arith::ConstantIndexOp>(-1));\n+                                  arith::ConstantIndexOp::create(b, -1));\n   Value accumulator_init = InitializeAccumulator(b);\n   SmallVector<Value> inits =\n       Pack({slice_id_init, indices_init, is_inbounds_init, accumulator_init,\n@@ -711,10 +712,11 @@ absl::Status ScatterWithDistributedIndices::EmitEntryFunctionImpl(\n     CHECK_EQ(ivs.size(), 2);\n     Value index_loop_id = ivs.front();\n     Value index_vector_id = ivs.back();\n-    Value iter_slice_id = nested_b.create<arith::AddIOp>(\n-        nested_b.create<arith::MulIOp>(\n-            index_loop_id,\n-            nested_b.create<arith::ConstantIndexOp>(indices_vector_size_)),\n+    Value iter_slice_id = arith::AddIOp::create(\n+        nested_b,\n+        arith::MulIOp::create(\n+            nested_b, index_loop_id,\n+            arith::ConstantIndexOp::create(nested_b, indices_vector_size_)),\n         index_vector_id);\n \n     SmallVector<Value> offsets =\n@@ -728,19 +730,20 @@ absl::Status ScatterWithDistributedIndices::EmitEntryFunctionImpl(\n         EmitInequalityCheck(nested_b, trimmed_offsets, new_trimmed_offsets);\n \n     for (int i = 0; i < description_.index_vector_length; ++i) {\n-      new_trimmed_offsets[i] = nested_b.create<arith::SelectOp>(\n-          offsets_changed, new_trimmed_offsets[i], trimmed_offsets[i]);\n+      new_trimmed_offsets[i] =\n+          arith::SelectOp::create(nested_b, offsets_changed,\n+                                  new_trimmed_offsets[i], trimmed_offsets[i]);\n     }\n \n     auto new_offsets = PadWithZeros(new_trimmed_offsets, output_rank, nested_b);\n \n     // Write accumulated values into the tensor if the offsets changed.\n     Value is_not_first_iteration =\n-        b.create<arith::CmpIOp>(arith::CmpIPredicate::ne, iter_slice_id,\n-                                b.create<arith::ConstantIndexOp>(0));\n-    Value write_to_output_required = b.create<arith::AndIOp>(\n-        is_not_first_iteration,\n-        b.create<arith::AndIOp>(offsets_changed, iter_is_inbounds));\n+        arith::CmpIOp::create(b, arith::CmpIPredicate::ne, iter_slice_id,\n+                              arith::ConstantIndexOp::create(b, 0));\n+    Value write_to_output_required = arith::AndIOp::create(\n+        b, is_not_first_iteration,\n+        arith::AndIOp::create(b, offsets_changed, iter_is_inbounds));\n     iter_output = helper.WriteAccumulatorToOutput(\n         b, write_to_output_required, thread_and_block_ids, iter_slice_id,\n         slice_indexing, offsets, iter_acc, iter_output);\n@@ -770,7 +773,7 @@ absl::Status ScatterWithDistributedIndices::EmitEntryFunctionImpl(\n                                           acc_ind_opfold)\n                 ->getResults();\n           });\n-      implicit_then_b.create<scf::YieldOp>(then_loc, then_results);\n+      scf::YieldOp::create(implicit_then_b, then_loc, then_results);\n     };\n     // Emits a loop that combines the accumulator with the new update elements\n     // if the offsets did not change.\n@@ -787,16 +790,16 @@ absl::Status ScatterWithDistributedIndices::EmitEntryFunctionImpl(\n             auto update_elem =\n                 helper.GetUpdateElement(update_loop_b, slice_indices);\n             auto acc_ind_opfold = mlir::getAsOpFoldResult(accumulator_indices);\n-            Value accumulator_elem = update_loop_b.create<vector::ExtractOp>(\n-                acc_arg, acc_ind_opfold);\n+            Value accumulator_elem = vector::ExtractOp::create(\n+                update_loop_b, acc_arg, acc_ind_opfold);\n             auto reduced_val = emitters::InlineBlock(\n                 update_loop_b, helper.GetReducer().getBody().front(),\n                 {accumulator_elem, update_elem})[0];\n             return update_loop_b\n                 .create<vector::InsertOp>(reduced_val, acc_arg, acc_ind_opfold)\n                 ->getResults();\n           });\n-      implicit_else_b.create<scf::YieldOp>(else_results);\n+      scf::YieldOp::create(implicit_else_b, else_results);\n     };\n     auto updated_accumulator =\n         EmitUpdateIf(nested_b, new_is_inbounds, {iter_acc},\n@@ -831,7 +834,7 @@ absl::Status ScatterWithDistributedIndices::EmitEntryFunctionImpl(\n       b, result_is_inbounds, thread_and_block_ids, result_slice_id,\n       slice_indexing, result_offsets, result_acc, result_output);\n \n-  b.create<ReturnOp>(result_output);\n+  ReturnOp::create(b, result_output);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "dcd790726be1655a5560a2a156eb1af34f48f6ee",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/convert_float_amd.cc",
            "status": "modified",
            "additions": 76,
            "deletions": 72,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_amd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_amd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_amd.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -217,10 +217,10 @@ struct RewriteFp8TruncFPattern : public Fp8OpRewritePattern<arith::TruncFOp> {\n \n     llvm::transform(inputs, inputs.begin(), [&](mlir::Value v) -> mlir::Value {\n       if (v.getType().getIntOrFloatBitWidth() < f32_ty.getWidth()) {\n-        return b.create<arith::ExtFOp>(f32_ty, v);\n+        return arith::ExtFOp::create(b, f32_ty, v);\n       }\n       if (v.getType() != f32_ty) {\n-        return b.create<arith::TruncFOp>(f32_ty, v);\n+        return arith::TruncFOp::create(b, f32_ty, v);\n       } else {\n         return v;\n       }\n@@ -237,37 +237,38 @@ struct RewriteFp8TruncFPattern : public Fp8OpRewritePattern<arith::TruncFOp> {\n     size_t num_chunks = (num_elements + 2) / 4;\n \n     mlir::Type chunks_ty = mlir::VectorType::get(num_chunks, i32_ty);\n-    mlir::Value chunks = b.create<LLVM::UndefOp>(chunks_ty);\n+    mlir::Value chunks = LLVM::UndefOp::create(b, chunks_ty);\n     bool pos = false;\n     for (size_t i = 0; i < inputs.size() / 2; i++) {\n-      mlir::Value chunk_pos = b.create<LLVM::ConstantOp>(i32_ty, 2 * i / 4);\n-      mlir::Value chunk = b.create<LLVM::ExtractElementOp>(chunks, chunk_pos);\n-      LLVM::CallIntrinsicOp cvtOp = b.create<LLVM::CallIntrinsicOp>(\n-          i32_ty, cvtIntr,\n+      mlir::Value chunk_pos = LLVM::ConstantOp::create(b, i32_ty, 2 * i / 4);\n+      mlir::Value chunk = LLVM::ExtractElementOp::create(b, chunks, chunk_pos);\n+      LLVM::CallIntrinsicOp cvtOp = LLVM::CallIntrinsicOp::create(\n+          b, i32_ty, cvtIntr,\n           mlir::ValueRange{inputs[2 * i], inputs[2 * i + 1], chunk,\n-                           b.create<LLVM::ConstantOp>(i1_ty, pos)});\n-      chunks = b.create<LLVM::InsertElementOp>(chunks, cvtOp.getResult(0),\n-                                               chunk_pos);\n+                           LLVM::ConstantOp::create(b, i1_ty, pos)});\n+      chunks = LLVM::InsertElementOp::create(b, chunks, cvtOp.getResult(0),\n+                                             chunk_pos);\n       pos ^= true;\n     }\n \n     if (num_elements == 2) {\n       return b\n           .create<mlir::UnrealizedConversionCastOp>(\n               to_ty,\n-              mlir::ValueRange{b.create<LLVM::BitcastOp>(\n-                  mlir::VectorType::get(num_elements, i8_ty),\n-                  b.create<LLVM::ExtractElementOp>(\n-                      b.create<LLVM::BitcastOp>(\n-                          mlir::VectorType::get(2, b.getI16Type()), chunks),\n-                      b.create<LLVM::ConstantOp>(i32_ty, 0)))})\n+              mlir::ValueRange{LLVM::BitcastOp::create(\n+                  b, mlir::VectorType::get(num_elements, i8_ty),\n+                  LLVM::ExtractElementOp::create(\n+                      b,\n+                      LLVM::BitcastOp::create(\n+                          b, mlir::VectorType::get(2, b.getI16Type()), chunks),\n+                      LLVM::ConstantOp::create(b, i32_ty, 0)))})\n           .getResult(0);\n     }\n \n     return b\n         .create<mlir::UnrealizedConversionCastOp>(\n-            to_ty, mlir::ValueRange{b.create<LLVM::BitcastOp>(\n-                       mlir::VectorType::get(num_elements, i8_ty), chunks)})\n+            to_ty, mlir::ValueRange{LLVM::BitcastOp::create(\n+                       b, mlir::VectorType::get(num_elements, i8_ty), chunks)})\n         .getResult(0);\n   }\n \n@@ -278,22 +279,22 @@ struct RewriteFp8TruncFPattern : public Fp8OpRewritePattern<arith::TruncFOp> {\n     mlir::FloatType f32_ty = b.getF32Type();\n     mlir::IntegerType i32_ty = b.getI32Type();\n     if (value.getType().getIntOrFloatBitWidth() < f32_ty.getWidth()) {\n-      value = b.create<arith::ExtFOp>(f32_ty, value);\n+      value = arith::ExtFOp::create(b, f32_ty, value);\n     } else if (value.getType() != f32_ty) {\n-      value = b.create<arith::TruncFOp>(f32_ty, value);\n+      value = arith::TruncFOp::create(b, f32_ty, value);\n     }\n \n     mlir::StringAttr cvtIntr =\n         b.getStringAttr(isFp8(to_ty) ? \"llvm.amdgcn.cvt.pk.fp8.f32\"\n                                      : \"llvm.amdgcn.cvt.pk.bf8.f32\");\n \n-    LLVM::CallIntrinsicOp cvtOp = b.create<LLVM::CallIntrinsicOp>(\n-        i32_ty, cvtIntr,\n-        mlir::ValueRange{value, b.create<LLVM::UndefOp>(f32_ty),\n-                         b.create<LLVM::UndefOp>(i32_ty),\n-                         b.create<LLVM::ConstantOp>(b.getI1Type(), 0)});\n+    LLVM::CallIntrinsicOp cvtOp = LLVM::CallIntrinsicOp::create(\n+        b, i32_ty, cvtIntr,\n+        mlir::ValueRange{value, LLVM::UndefOp::create(b, f32_ty),\n+                         LLVM::UndefOp::create(b, i32_ty),\n+                         LLVM::ConstantOp::create(b, b.getI1Type(), 0)});\n     mlir::Value res =\n-        b.create<LLVM::TruncOp>(b.getI8Type(), cvtOp.getResults());\n+        LLVM::TruncOp::create(b, b.getI8Type(), cvtOp.getResults());\n     return b\n         .create<mlir::UnrealizedConversionCastOp>(to_ty, mlir::ValueRange{res})\n         .getResult(0);\n@@ -419,20 +420,20 @@ struct RewriteFp8ExtFPattern : public Fp8OpRewritePattern<arith::ExtFOp> {\n     }\n \n     if (to_ty.getWidth() > f32_ty.getWidth()) {\n-      return b.create<arith::ExtFOp>(to_ty, v);\n+      return arith::ExtFOp::create(b, to_ty, v);\n     }\n \n     if (to_ty.isBF16()) {\n-      return b.create<LLVM::BitcastOp>(\n-          to_ty,\n-          b.create<LLVM::TruncOp>(\n-              b.getI16Type(),\n-              b.create<LLVM::LShrOp>(b.create<LLVM::BitcastOp>(i32_ty, v),\n-                                     b.create<LLVM::ConstantOp>(i32_ty, 16))));\n+      return LLVM::BitcastOp::create(\n+          b, to_ty,\n+          LLVM::TruncOp::create(\n+              b, b.getI16Type(),\n+              LLVM::LShrOp::create(b, LLVM::BitcastOp::create(b, i32_ty, v),\n+                                   LLVM::ConstantOp::create(b, i32_ty, 16))));\n     }\n \n     assert(to_ty.getWidth() < f32_ty.getWidth());\n-    return b.create<arith::TruncFOp>(to_ty, v);\n+    return arith::TruncFOp::create(b, to_ty, v);\n   }\n \n   llvm::SmallVector<mlir::Value, 4> EmitVectorizedExtFromF8Intrinsic(\n@@ -443,8 +444,8 @@ struct RewriteFp8ExtFPattern : public Fp8OpRewritePattern<arith::ExtFOp> {\n     mlir::IntegerType i16_ty = b.getI16Type();\n     mlir::IntegerType i8_ty = b.getI8Type();\n     mlir::IntegerType i1_ty = b.getI1Type();\n-    mlir::Value zero_cst = b.create<LLVM::ConstantOp>(i32_ty, 0);\n-    mlir::Value one_cst = b.create<LLVM::ConstantOp>(i32_ty, 1);\n+    mlir::Value zero_cst = LLVM::ConstantOp::create(b, i32_ty, 0);\n+    mlir::Value one_cst = LLVM::ConstantOp::create(b, i32_ty, 1);\n \n     size_t num_elements = value.getType().getNumElements();\n     assert(num_elements == 2 || num_elements % 4 == 0);\n@@ -454,22 +455,24 @@ struct RewriteFp8ExtFPattern : public Fp8OpRewritePattern<arith::ExtFOp> {\n     mlir::Value chunks;\n \n     if (num_elements == 2) {\n-      chunks = b.create<LLVM::BitcastOp>(\n-          chunks_ty,\n-          b.create<LLVM::InsertElementOp>(\n-              b.create<LLVM::UndefOp>(mlir::VectorType::get(2, i16_ty)),\n-              b.create<LLVM::BitcastOp>(\n-                  i16_ty, b.create<mlir::UnrealizedConversionCastOp>(\n-                               mlir::VectorType::get(num_elements, i8_ty),\n-                               mlir::ValueRange{value})\n-                              .getResult(0)),\n+      chunks = LLVM::BitcastOp::create(\n+          b, chunks_ty,\n+          LLVM::InsertElementOp::create(\n+              b, LLVM::UndefOp::create(b, mlir::VectorType::get(2, i16_ty)),\n+              LLVM::BitcastOp::create(\n+                  b, i16_ty,\n+                  mlir::UnrealizedConversionCastOp::create(\n+                      b, mlir::VectorType::get(num_elements, i8_ty),\n+                      mlir::ValueRange{value})\n+                      .getResult(0)),\n               zero_cst));\n     } else {\n-      chunks = b.create<LLVM::BitcastOp>(\n-          chunks_ty, b.create<mlir::UnrealizedConversionCastOp>(\n-                          mlir::VectorType::get(num_elements, i8_ty),\n-                          mlir::ValueRange{value})\n-                         .getResult(0));\n+      chunks = LLVM::BitcastOp::create(\n+          b, chunks_ty,\n+          mlir::UnrealizedConversionCastOp::create(\n+              b, mlir::VectorType::get(num_elements, i8_ty),\n+              mlir::ValueRange{value})\n+              .getResult(0));\n     }\n \n     llvm::SmallVector<mlir::Value, 4> results;\n@@ -480,32 +483,32 @@ struct RewriteFp8ExtFPattern : public Fp8OpRewritePattern<arith::ExtFOp> {\n     LLVM::FastmathFlagsAttr flags =\n         LLVM::FastmathFlagsAttr::get(b.getContext(), LLVM::FastmathFlags::ninf);\n     for (size_t i = 0; i < num_elements / 2; i++) {\n-      mlir::Value chunk_pos = b.create<LLVM::ConstantOp>(i32_ty, (2 * i) / 4);\n-      mlir::Value chunk = b.create<LLVM::ExtractElementOp>(chunks, chunk_pos);\n-      LLVM::CallIntrinsicOp cvtOp = b.create<LLVM::CallIntrinsicOp>(\n-          result_ty, cvtIntr,\n+      mlir::Value chunk_pos = LLVM::ConstantOp::create(b, i32_ty, (2 * i) / 4);\n+      mlir::Value chunk = LLVM::ExtractElementOp::create(b, chunks, chunk_pos);\n+      LLVM::CallIntrinsicOp cvtOp = LLVM::CallIntrinsicOp::create(\n+          b, result_ty, cvtIntr,\n           mlir::ValueRange{\n-              chunk, b.create<LLVM::ConstantOp>(i1_ty, ((2 * i) % 4) != 0)},\n+              chunk, LLVM::ConstantOp::create(b, i1_ty, ((2 * i) % 4) != 0)},\n           flags);\n \n       results.push_back(\n-          b.create<LLVM::ExtractElementOp>(cvtOp.getResult(0), zero_cst));\n+          LLVM::ExtractElementOp::create(b, cvtOp.getResult(0), zero_cst));\n       results.push_back(\n-          b.create<LLVM::ExtractElementOp>(cvtOp.getResult(0), one_cst));\n+          LLVM::ExtractElementOp::create(b, cvtOp.getResult(0), one_cst));\n     }\n \n     if (to_ty.isF16()) {\n       result_ty = mlir::VectorType::get(2, b.getF16Type());\n       cvtIntr = b.getStringAttr(\"llvm.amdgcn.cvt.pkrtz\");\n       for (size_t i = 0; i < num_elements / 2; i++) {\n-        LLVM::CallIntrinsicOp cvtOp = b.create<LLVM::CallIntrinsicOp>(\n-            result_ty, cvtIntr,\n+        LLVM::CallIntrinsicOp cvtOp = LLVM::CallIntrinsicOp::create(\n+            b, result_ty, cvtIntr,\n             mlir::ValueRange{results[2 * i], results[2 * i + 1]}, flags);\n \n         results[2 * i] =\n-            b.create<LLVM::ExtractElementOp>(cvtOp.getResult(0), zero_cst);\n+            LLVM::ExtractElementOp::create(b, cvtOp.getResult(0), zero_cst);\n         results[2 * i + 1] =\n-            b.create<LLVM::ExtractElementOp>(cvtOp.getResult(0), one_cst);\n+            LLVM::ExtractElementOp::create(b, cvtOp.getResult(0), one_cst);\n       }\n     } else if (to_ty != f32_ty) {\n       llvm::transform(results, results.begin(),\n@@ -524,23 +527,24 @@ struct RewriteFp8ExtFPattern : public Fp8OpRewritePattern<arith::ExtFOp> {\n     mlir::FloatType f32_ty = b.getF32Type();\n     mlir::IntegerType i32_ty = b.getI32Type();\n     mlir::IntegerType i8_ty = b.getI8Type();\n-    mlir::Value zero_cst = b.create<LLVM::ConstantOp>(i32_ty, 0);\n+    mlir::Value zero_cst = LLVM::ConstantOp::create(b, i32_ty, 0);\n     // Emulate anyext\n-    mlir::Value input = b.create<LLVM::BitcastOp>(\n-        i32_ty, b.create<LLVM::InsertElementOp>(\n-                    b.create<LLVM::UndefOp>(mlir::VectorType::get(4, i8_ty)),\n-                    b.create<mlir::UnrealizedConversionCastOp>(\n-                         i8_ty, mlir::ValueRange{value})\n-                        .getResult(0),\n-                    zero_cst));\n+    mlir::Value input = LLVM::BitcastOp::create(\n+        b, i32_ty,\n+        LLVM::InsertElementOp::create(\n+            b, LLVM::UndefOp::create(b, mlir::VectorType::get(4, i8_ty)),\n+            mlir::UnrealizedConversionCastOp::create(b, i8_ty,\n+                                                     mlir::ValueRange{value})\n+                .getResult(0),\n+            zero_cst));\n     mlir::StringAttr cvtIntr =\n         b.getStringAttr(isFp8(value.getType()) ? \"llvm.amdgcn.cvt.f32.fp8\"\n                                                : \"llvm.amdgcn.cvt.f32.bf8\");\n     LLVM::FastmathFlagsAttr flags =\n         LLVM::FastmathFlagsAttr::get(b.getContext(), LLVM::FastmathFlags::ninf);\n-    LLVM::CallIntrinsicOp cvtOp = b.create<LLVM::CallIntrinsicOp>(\n-        mlir::TypeRange{f32_ty}, cvtIntr, mlir::ValueRange{input, zero_cst},\n-        flags);\n+    LLVM::CallIntrinsicOp cvtOp =\n+        LLVM::CallIntrinsicOp::create(b, mlir::TypeRange{f32_ty}, cvtIntr,\n+                                      mlir::ValueRange{input, zero_cst}, flags);\n \n     return ConvertFromFloat(cvtOp.getResult(0), to_ty, b);\n   }"
        },
        {
            "sha": "2e7112771a93d21b3d977ca877b24bbcdd4addda",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/convert_float_nvidia.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 39,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_nvidia.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_nvidia.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_float_nvidia.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -59,9 +59,9 @@ Value ConvertToF32(Value v, mlir::ImplicitLocOpBuilder& b) {\n     return v;\n   }\n   if (v.getType().getIntOrFloatBitWidth() < f32_ty.getWidth()) {\n-    return b.create<ma::ExtFOp>(f32_ty, v);\n+    return ma::ExtFOp::create(b, f32_ty, v);\n   }\n-  return b.create<ma::TruncFOp>(f32_ty, v);\n+  return ma::TruncFOp::create(b, f32_ty, v);\n }\n \n struct RewriteTruncFPattern : public mlir::OpRewritePattern<ma::TruncFOp> {\n@@ -104,15 +104,15 @@ struct RewriteTruncFPattern : public mlir::OpRewritePattern<ma::TruncFOp> {\n         value.getType() == b.getF16Type()) {\n       // Fast path for truncating F16 type.\n       Value vec =\n-          b.create<ml::UndefOp>(mlir::VectorType::get(2, value.getType()));\n-      vec = b.create<ml::InsertElementOp>(vec, value,\n-                                          b.create<ma::ConstantIntOp>(0, 8));\n+          ml::UndefOp::create(b, mlir::VectorType::get(2, value.getType()));\n+      vec = ml::InsertElementOp::create(b, vec, value,\n+                                        ma::ConstantIntOp::create(b, 0, 8));\n       const std::string cvtIntr = llvm::isa<mlir::Float8E4M3FNType>(to_ty)\n                                       ? \"llvm.nvvm.f16x2.to.e4m3x2.rn\"\n                                       : \"llvm.nvvm.f16x2.to.e5m2x2.rn\";\n-      cvtOp = b.create<ml::CallIntrinsicOp>(b.getIntegerType(16),\n-                                            b.getStringAttr(cvtIntr),\n-                                            mlir::ValueRange{vec});\n+      cvtOp = ml::CallIntrinsicOp::create(b, b.getIntegerType(16),\n+                                          b.getStringAttr(cvtIntr),\n+                                          mlir::ValueRange{vec});\n     } else {\n       // Other FP types get converted to F32 first.\n       value = ConvertToF32(value, b);\n@@ -121,24 +121,24 @@ struct RewriteTruncFPattern : public mlir::OpRewritePattern<ma::TruncFOp> {\n                                   : llvm::isa<mlir::Float8E4M3FNType>(to_ty)\n                                       ? \"llvm.nvvm.ff.to.e4m3x2.rn\"\n                                       : \"llvm.nvvm.ff.to.e5m2x2.rn\";\n-      cvtOp = b.create<ml::CallIntrinsicOp>(b.getIntegerType(16),\n-                                            b.getStringAttr(cvtIntr),\n-                                            mlir::ValueRange{value, value});\n+      cvtOp = ml::CallIntrinsicOp::create(b, b.getIntegerType(16),\n+                                          b.getStringAttr(cvtIntr),\n+                                          mlir::ValueRange{value, value});\n     }\n \n-    Value res = b.create<ml::TruncOp>(\n-        b.getIntegerType(to_ty.getIntOrFloatBitWidth()), cvtOp.getResults());\n+    Value res = ml::TruncOp::create(\n+        b, b.getIntegerType(to_ty.getIntOrFloatBitWidth()), cvtOp.getResults());\n \n     if (llvm::isa<mlir::Float4E2M1FNType>(to_ty)) {\n-      return b.create<ma::BitcastOp>(to_ty, res);\n+      return ma::BitcastOp::create(b, to_ty, res);\n     }\n \n     // Downcasting to float8 saturates the value (uses \"satfinite\" modifier).\n     // Handle infinity separately to mitigate the issue.\n     mlir::Type src_int_ty =\n         b.getIntegerType(value.getType().getIntOrFloatBitWidth());\n     return FixInfinityConversionValue(\n-        b.create<ma::BitcastOp>(src_int_ty, value),\n+        ma::BitcastOp::create(b, src_int_ty, value),\n         mlir::cast<mlir::FloatType>(value.getType()), res, to_ty, b);\n   }\n \n@@ -152,11 +152,12 @@ struct RewriteTruncFPattern : public mlir::OpRewritePattern<ma::TruncFOp> {\n                                           mlir::ImplicitLocOpBuilder& b) {\n     // Extract and discard sign bit.\n     auto make_const = [&](int64_t c) {\n-      return b.create<ma::ConstantIntOp>(src.getType(), c);\n+      return ma::ConstantIntOp::create(b, src.getType(), c);\n     };\n     int sign_pos = src.getType().getIntOrFloatBitWidth() - 1;\n-    Value sign_bit = b.create<ma::ShRUIOp>(src, make_const(sign_pos));\n-    Value input = b.create<ma::AndIOp>(src, make_const((1ull << sign_pos) - 1));\n+    Value sign_bit = ma::ShRUIOp::create(b, src, make_const(sign_pos));\n+    Value input =\n+        ma::AndIOp::create(b, src, make_const((1ull << sign_pos) - 1));\n \n     // Values in the interval that contains all the values above the largest\n     // representable in the destination type, as well as the infinity (source),\n@@ -165,23 +166,25 @@ struct RewriteTruncFPattern : public mlir::OpRewritePattern<ma::TruncFOp> {\n     int64_t upper = llvm::APFloat::getInf(src_type.getFloatSemantics())\n                         .bitcastToAPInt()\n                         .getZExtValue();\n-    Value is_inf = b.create<ma::AndIOp>(\n-        b.create<ma::CmpIOp>(ma::CmpIPredicate::ugt, input, make_const(lower)),\n-        b.create<ma::CmpIOp>(ma::CmpIPredicate::ule, input, make_const(upper)));\n+    Value is_inf = ma::AndIOp::create(\n+        b,\n+        ma::CmpIOp::create(b, ma::CmpIPredicate::ugt, input, make_const(lower)),\n+        ma::CmpIOp::create(b, ma::CmpIPredicate::ule, input,\n+                           make_const(upper)));\n \n     // Build signed infinity result value.\n     int64_t inf_val = llvm::APFloat::getInf(dst_type.getFloatSemantics())\n                           .bitcastToAPInt()\n                           .getZExtValue();\n     Value sign_dst =\n-        b.create<ma::ShLIOp>(b.create<ml::TruncOp>(dst.getType(), sign_bit),\n-                             b.create<ma::ConstantIntOp>(dst.getType(), 7));\n-    Value inf = b.create<ma::OrIOp>(\n-        b.create<ma::ConstantIntOp>(dst.getType(), inf_val), sign_dst);\n+        ma::ShLIOp::create(b, ml::TruncOp::create(b, dst.getType(), sign_bit),\n+                           ma::ConstantIntOp::create(b, dst.getType(), 7));\n+    Value inf = ma::OrIOp::create(\n+        b, ma::ConstantIntOp::create(b, dst.getType(), inf_val), sign_dst);\n \n     // Select result based on the predicate.\n-    Value res = b.create<ma::SelectOp>(is_inf, inf, dst);\n-    return b.create<ma::BitcastOp>(dst_type, res);\n+    Value res = ma::SelectOp::create(b, is_inf, inf, dst);\n+    return ma::BitcastOp::create(b, dst_type, res);\n   }\n \n   // Calculate the minimum raw value (represented as an integer) that would\n@@ -253,24 +256,25 @@ struct RewriteExtFPattern : public mlir::OpRewritePattern<ma::ExtFOp> {\n         : llvm::isa<mlir::Float8E4M3FNType>(value.getType())\n             ? \"llvm.nvvm.e4m3x2.to.f16x2.rn\"\n             : \"llvm.nvvm.e5m2x2.to.f16x2.rn\";\n-    Value input = b.create<ml::ZExtOp>(\n-        b.getIntegerType(16),\n-        b.create<ma::BitcastOp>(\n-            b.getIntegerType(value.getType().getIntOrFloatBitWidth()), value));\n+    Value input = ml::ZExtOp::create(\n+        b, b.getIntegerType(16),\n+        ma::BitcastOp::create(\n+            b, b.getIntegerType(value.getType().getIntOrFloatBitWidth()),\n+            value));\n \n     mlir::FloatType f16_ty = b.getF16Type();\n-    auto cvtOp = b.create<ml::CallIntrinsicOp>(mlir::VectorType::get(2, f16_ty),\n-                                               b.getStringAttr(cvtIntr),\n-                                               mlir::ValueRange{input});\n-    Value res = b.create<ml::ExtractElementOp>(\n-        cvtOp.getResults(), b.create<ma::ConstantIntOp>(0, 8));\n+    auto cvtOp = ml::CallIntrinsicOp::create(\n+        b, mlir::VectorType::get(2, f16_ty), b.getStringAttr(cvtIntr),\n+        mlir::ValueRange{input});\n+    Value res = ml::ExtractElementOp::create(\n+        b, cvtOp.getResults(), ma::ConstantIntOp::create(b, 0, 8));\n     if (to_ty.getWidth() > f16_ty.getWidth()) {\n-      res = b.create<ma::ExtFOp>(to_ty, res);\n+      res = ma::ExtFOp::create(b, to_ty, res);\n     } else if (to_ty != f16_ty) {\n       if (to_ty == b.getBF16Type()) {\n-        res = b.create<ma::ExtFOp>(b.getF32Type(), res);\n+        res = ma::ExtFOp::create(b, b.getF32Type(), res);\n       }\n-      res = b.create<ma::TruncFOp>(to_ty, res);\n+      res = ma::TruncFOp::create(b, to_ty, res);\n     }\n     return res;\n   }"
        },
        {
            "sha": "f6a47e965b9ba9b2c022791eefa9f35b361d5c79",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/convert_index_type.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_index_type.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_index_type.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fconvert_index_type.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -85,13 +85,13 @@ class RewriteIndexBinaryElementwiseOp\n \n     Type index_type = IndexType::get(op->getContext());\n     Type dst_type = b.getIntegerType(index_bitwidth_);\n-    auto lhs = b.create<arith::IndexCastOp>(dst_type, op->getOperand(0));\n-    auto rhs = b.create<arith::IndexCastOp>(dst_type, op->getOperand(1));\n-    auto new_op = b.create<BinaryElementwiseOp>(lhs, rhs);\n+    auto lhs = arith::IndexCastOp::create(b, dst_type, op->getOperand(0));\n+    auto rhs = arith::IndexCastOp::create(b, dst_type, op->getOperand(1));\n+    auto new_op = BinaryElementwiseOp::create(b, lhs, rhs);\n \n     rewriter.replaceAllUsesWith(\n         op.getResult(),\n-        b.create<arith::IndexCastOp>(index_type, new_op.getResult()));\n+        arith::IndexCastOp::create(b, index_type, new_op.getResult()));\n \n     return mlir::success();\n   }"
        },
        {
            "sha": "b6ccf47acb2068115b74d985b014c67fa191126a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/lower_xla_shared.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_shared.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_shared.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_shared.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -70,8 +70,8 @@ struct LowerForall : mlir::OpRewritePattern<mlir::scf::ForallOp> {\n       // xla.range is a closed interval so we subtract 1 from the size which is\n       // half-open.\n       int64_t upper_range = size - 1;\n-      auto thread_id = b.create<mlir::gpu::ThreadIdOp>(\n-          static_cast<mlir::gpu::Dimension>(idx));\n+      auto thread_id = mlir::gpu::ThreadIdOp::create(\n+          b, static_cast<mlir::gpu::Dimension>(idx));\n       thread_id->setAttr(\"xla.range\", b.getIndexArrayAttr({0, upper_range}));\n       new_args.push_back(thread_id);\n     }\n@@ -118,7 +118,7 @@ struct LowerWorkgroupId : mlir::OpRewritePattern<WorkGroupIdOp> {\n                                          mlir::gpu::Dimension dimension) const {\n     mlir::Location loc = op.getLoc();\n     mlir::ImplicitLocOpBuilder b(loc, rewriter);\n-    auto block_id = b.create<mlir::gpu::BlockIdOp>(dimension);\n+    auto block_id = mlir::gpu::BlockIdOp::create(b, dimension);\n     if (mlir::Attribute range = op->getAttr(\"xla.range\")) {\n       block_id->setAttr(\"xla.range\", op->getAttr(\"xla.range\"));\n     }"
        },
        {
            "sha": "438528dfe65959221378557c49085eb526700e66",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/peel_loops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpeel_loops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpeel_loops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Fpeel_loops.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -109,8 +109,8 @@ struct PeelLoop : public OpRewritePattern<LoopOp> {\n       if (indexing_map.IsKnownEmpty()) {\n         continue;\n       }\n-      auto tail_loop = rewriter.create<LoopOp>(\n-          loc, indexing_map, loop_op.getDims(), inits,\n+      auto tail_loop = LoopOp::create(\n+          rewriter, loc, indexing_map, loop_op.getDims(), inits,\n           [&](OpBuilder& nested_b, Location nested_loc, ValueRange ivs,\n               ValueRange map_results, ValueRange iter_args) {\n             OpBuilder::InsertionGuard guard(nested_b);"
        },
        {
            "sha": "f277cdee0973039b57a737f56b9d9f845c4733f4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 28,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -117,17 +117,18 @@ Value ReadVectorTileFromShmem(ImplicitLocOpBuilder& b, Value shmem,\n       mlir::cast<mlir::RankedTensorType>(shmem.getType()).getElementType();\n   auto vector_type = mlir::VectorType::get({vector_size}, elem_type);\n   for (int64_t i = 0; i < vector_size; ++i) {\n-    Value loaded_vector = b.create<mv::TransferReadOp>(\n-        vector_type, shmem, shmem_indices_vec, /*padding=*/std::nullopt,\n+    Value loaded_vector = mv::TransferReadOp::create(\n+        b, vector_type, shmem, shmem_indices_vec, /*padding=*/std::nullopt,\n         llvm::ArrayRef<bool>{true});\n     for (int64_t j = 0; j < vector_size; ++j) {\n       Value elem =\n-          b.create<mv::ExtractOp>(loaded_vector, SmallVector<int64_t>{j});\n-      vector_tile =\n-          b.create<mv::InsertOp>(elem, vector_tile, SmallVector<int64_t>{j, i});\n+          mv::ExtractOp::create(b, loaded_vector, SmallVector<int64_t>{j});\n+      vector_tile = mv::InsertOp::create(b, elem, vector_tile,\n+                                         SmallVector<int64_t>{j, i});\n     }\n-    shmem_indices_vec.front() = b.create<mlir::arith::AddIOp>(\n-        shmem_indices_vec.front(), b.create<mlir::arith::ConstantIndexOp>(1));\n+    shmem_indices_vec.front() =\n+        mlir::arith::AddIOp::create(b, shmem_indices_vec.front(),\n+                                    mlir::arith::ConstantIndexOp::create(b, 1));\n   }\n   return vector_tile;\n }\n@@ -351,8 +352,8 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n   for (auto* transpose : shmem_transposes_) {\n     auto elem_type = emitters::PrimitiveTypeToMlirType(\n         transpose->shape().element_type(), builder);\n-    inits.push_back(builder.create<AllocateSharedOp>(\n-        RankedTensorType::get(shmem_tensor_size, elem_type)));\n+    inits.push_back(AllocateSharedOp::create(\n+        builder, RankedTensorType::get(shmem_tensor_size, elem_type)));\n   }\n \n   // Add output arguments for side outputs.\n@@ -382,8 +383,8 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n           root_computation, transpose,\n           /*operand_index=*/0, input_indices(transpose->operand(0)),\n           call_target_provider, entry_function, builder)[0];\n-      result_tensors.push_back(builder.create<mlir::tensor::InsertOp>(\n-          result_scalar, output, shmem_indices));\n+      result_tensors.push_back(mlir::tensor::InsertOp::create(\n+          builder, result_scalar, output, shmem_indices));\n     }\n \n     // Produce all side outputs and then write them.\n@@ -403,7 +404,7 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n          llvm::zip(side_outputs, side_output_indices,\n                    output_tensors.take_back(side_output_roots_.size()))) {\n       result_tensors.push_back(\n-          nested_b.create<mt::InsertOp>(value, output, indices));\n+          mt::InsertOp::create(nested_b, value, output, indices));\n     }\n \n     return result_tensors;\n@@ -452,7 +453,7 @@ void TransposeFusion::EmitReadFromShMemMlir(\n         for (auto [transpose, shmem] :\n              llvm::zip(shmem_transposes_, written.shmem_tensors)) {\n           transpose_values[transpose].push_back(\n-              nested_b.create<mt::ExtractOp>(shmem, shmem_indices));\n+              mt::ExtractOp::create(nested_b, shmem, shmem_indices));\n         }\n         llvm::SmallVector<Value> epilogue_indices = thread_and_block_ids;\n         absl::c_copy(symbol_values, std::back_inserter(epilogue_indices));\n@@ -466,13 +467,14 @@ void TransposeFusion::EmitReadFromShMemMlir(\n                        shmem_transpose_root_indices_)) {\n           llvm::SmallVector<Value> indices = ApplyIndexing(\n               indexing, thread_and_block_ids, symbol_values, nested_b);\n-          results[root_index] = nested_b.create<mt::InsertOp>(\n-              result_scalars.at(root).front(), results[root_index], indices);\n+          results[root_index] =\n+              mt::InsertOp::create(nested_b, result_scalars.at(root).front(),\n+                                   results[root_index], indices);\n         }\n         return results;\n       });\n \n-  builder.create<ReturnOp>(result_tensors);\n+  ReturnOp::create(builder, result_tensors);\n }\n \n llvm::SmallVector<mlir::AffineExpr, 4> TransposeFusion::GetThreadOffsets(\n@@ -633,8 +635,8 @@ PackedTranspose::WriteResult PackedTranspose::EmitWriteToShMemMlir(\n   for (auto* transpose : shmem_transposes_) {\n     Type elem_type = emitters::PrimitiveTypeToMlirType(\n         transpose->shape().element_type(), builder);\n-    Value shmem = builder.create<AllocateSharedOp>(\n-        RankedTensorType::get({shmem_dim, shmem_dim}, elem_type));\n+    Value shmem = AllocateSharedOp::create(\n+        builder, RankedTensorType::get({shmem_dim, shmem_dim}, elem_type));\n \n     auto tids_and_bids = EmitThreadAndBlockIds(builder);\n     Value updated_shmem =\n@@ -690,7 +692,7 @@ PackedTranspose::WriteResult PackedTranspose::EmitWriteToShMemMlir(\n     for (const auto& [value, indices, output] :\n          llvm::zip(side_outputs, side_output_indices, output_tensors)) {\n       result_tensors.push_back(\n-          nested_b.create<mt::InsertOp>(value, output, indices));\n+          mt::InsertOp::create(nested_b, value, output, indices));\n     }\n \n     return result_tensors;\n@@ -719,8 +721,9 @@ Value GetZeroVector(ImplicitLocOpBuilder& b, PrimitiveType elem_type,\n                     llvm::ArrayRef<int64_t> shape) {\n   auto mlir_elem_type = emitters::PrimitiveTypeToMlirType(elem_type, b);\n   auto accumulator_type = mlir::VectorType::get(shape, mlir_elem_type);\n-  return b.create<mlir::arith::ConstantOp>(\n-      accumulator_type, emitters::GetZeroDenseElementsAttr(accumulator_type));\n+  return mlir::arith::ConstantOp::create(\n+      b, accumulator_type,\n+      emitters::GetZeroDenseElementsAttr(accumulator_type));\n }\n \n void PackedTranspose::EmitReadFromShMemMlir(\n@@ -735,7 +738,7 @@ void PackedTranspose::EmitReadFromShMemMlir(\n   auto output_indexing_over_vectors = ConvertRangeVariablesToDimensions(\n       output_indexing, /*range_var_indices=*/{0});\n \n-  auto c0 = builder.create<mlir::arith::ConstantIndexOp>(0);\n+  auto c0 = mlir::arith::ConstantIndexOp::create(builder, 0);\n   SmallVector<Value> grid_and_vector_ids{thread_and_block_ids};\n   grid_and_vector_ids.append({c0, c0});\n   absl::flat_hash_map<PrimitiveType, Value> elem_type_to_vector_tile;\n@@ -775,8 +778,8 @@ void PackedTranspose::EmitReadFromShMemMlir(\n                 ValueRange output_tensors) -> SmallVector<Value> {\n               for (auto [transpose, shmem] :\n                    llvm::zip(shmem_transposes_, written.shmem_tensors)) {\n-                Value elem = nested_b_2.create<mv::ExtractOp>(\n-                    transpose_values[transpose].front(),\n+                Value elem = mv::ExtractOp::create(\n+                    nested_b_2, transpose_values[transpose].front(),\n                     getAsOpFoldResult(ivs));\n                 transpose_values[transpose] = {elem};\n               }\n@@ -795,15 +798,15 @@ void PackedTranspose::EmitReadFromShMemMlir(\n                 symbols.append(ivs.begin(), ivs.end());\n                 llvm::SmallVector<Value> indices = ApplyIndexing(\n                     indexing, thread_and_block_ids, symbols, nested_b);\n-                results[root_index] = nested_b_2.create<mt::InsertOp>(\n-                    result_scalars.at(root).front(), results[root_index],\n-                    indices);\n+                results[root_index] = mt::InsertOp::create(\n+                    nested_b_2, result_scalars.at(root).front(),\n+                    results[root_index], indices);\n               }\n               return results;\n             });\n         return inner_loop_results;\n       });\n-  builder.create<ReturnOp>(outer_loop_results);\n+  ReturnOp::create(builder, outer_loop_results);\n }\n \n IndexingMap PackedTranspose::GetInputIndexing(MLIRContext* mlir_context) const {"
        },
        {
            "sha": "5e04600a6d85ea961623c82a7e1fc7542237bdd2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 25,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -195,33 +195,32 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n   const auto ptr_to_i64_type =\n       ttir::PointerType::get(b.getI64Type(), kGlobalAddressSpace);\n   auto remote_input_buffers_i64 =\n-      b.create<ttir::BitcastOp>(ptr_to_i64_type, remote_input_buffers);\n-  Value remote_buf_ptr_addr = b.create<ttir::AddPtrOp>(\n-      ptr_to_i64_type, remote_input_buffers_i64, device_rank);\n-  Value remote_buf_i64 =\n-      b.create<ttir::LoadOp>(remote_buf_ptr_addr,\n-                             ttir::CacheModifier::NONE,     //\n-                             ttir::EvictionPolicy::NORMAL,  //\n-                             false);                        // isVolatile\n+      ttir::BitcastOp::create(b, ptr_to_i64_type, remote_input_buffers);\n+  Value remote_buf_ptr_addr = ttir::AddPtrOp::create(\n+      b, ptr_to_i64_type, remote_input_buffers_i64, device_rank);\n+  Value remote_buf_i64 = ttir::LoadOp::create(b, remote_buf_ptr_addr,\n+                                              ttir::CacheModifier::NONE,     //\n+                                              ttir::EvictionPolicy::NORMAL,  //\n+                                              false);  // isVolatile\n   const auto elem_type =\n       mlir::cast<ShapedType>(input_tile.getType()).getElementType();\n   const auto ptr_to_elem_type =\n       ttir::PointerType::get(elem_type, kGlobalAddressSpace);\n   Value remote_buf_ptr =\n-      b.create<ttir::IntToPtrOp>(ptr_to_elem_type, remote_buf_i64);\n+      ttir::IntToPtrOp::create(b, ptr_to_elem_type, remote_buf_i64);\n   mlir::ArrayRef<int64_t> remote_shape = tile_info.original_shape();\n   const mlir::MemRefType remote_memref_type =\n       mlir::MemRefType::get(remote_shape, elem_type);\n   mlir::Value remote_buf_memref =\n-      b.create<mtx::PtrToMemrefOp>(remote_memref_type, remote_buf_ptr);\n-  b.create<xtile::InsertTileOp>(\n-      input_tile, remote_buf_memref, tile_info.offsets(),\n+      mtx::PtrToMemrefOp::create(b, remote_memref_type, remote_buf_ptr);\n+  xtile::InsertTileOp::create(\n+      b, input_tile, remote_buf_memref, tile_info.offsets(),\n       tile_info.padded_tile_sizes(), tile_info.tile_strides());\n \n   // 2. Synchronization phase: Wait for all ranks to complete the scatter.\n   int64_t world_size = all_reduce.device_list().num_devices_per_group();\n-  b.create<mtx::BlockBarrierOp>(signal_buffers, device_rank, signal_value,\n-                                b.getI32IntegerAttr(world_size));\n+  mtx::BlockBarrierOp::create(b, signal_buffers, device_rank, signal_value,\n+                              b.getI32IntegerAttr(world_size));\n \n   // 3. Reduce phase: Load tiles from all ranks and reduce them.\n   HloComputation* reduction_computation = all_reduce.to_apply();\n@@ -234,23 +233,23 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n   }\n   // Set accumulator zero.\n   mlir::Value accumulator_zero =\n-      b.create<arith::ConstantOp>(elem_type, b.getZeroAttr(elem_type));\n+      arith::ConstantOp::create(b, elem_type, b.getZeroAttr(elem_type));\n   TensorValue accumulator =\n-      b.create<ttir::SplatOp>(input_tile.getType(), accumulator_zero);\n+      ttir::SplatOp::create(b, input_tile.getType(), accumulator_zero);\n   for (int rank = 0; rank < world_size; ++rank) {\n     Value rank_idx =\n-        b.create<arith::ConstantOp>(b.getI64Type(), b.getI64IntegerAttr(rank));\n-    Value remote_buf_ptr_addr = b.create<ttir::AddPtrOp>(\n-        ptr_to_i64_type, remote_input_buffers_i64, rank_idx);\n+        arith::ConstantOp::create(b, b.getI64Type(), b.getI64IntegerAttr(rank));\n+    Value remote_buf_ptr_addr = ttir::AddPtrOp::create(\n+        b, ptr_to_i64_type, remote_input_buffers_i64, rank_idx);\n     Value remote_buf_i64 =\n-        b.create<ttir::LoadOp>(remote_buf_ptr_addr,\n-                               ttir::CacheModifier::NONE,     //\n-                               ttir::EvictionPolicy::NORMAL,  //\n-                               false);                        // isVolatile\n+        ttir::LoadOp::create(b, remote_buf_ptr_addr,\n+                             ttir::CacheModifier::NONE,     //\n+                             ttir::EvictionPolicy::NORMAL,  //\n+                             false);                        // isVolatile\n     Value remote_buf_ptr =\n-        b.create<ttir::IntToPtrOp>(ptr_to_elem_type, remote_buf_i64);\n+        ttir::IntToPtrOp::create(b, ptr_to_elem_type, remote_buf_i64);\n     Value remote_buf_memref =\n-        b.create<mtx::PtrToMemrefOp>(remote_memref_type, remote_buf_ptr);\n+        mtx::PtrToMemrefOp::create(b, remote_memref_type, remote_buf_ptr);\n     TensorValue next_tile =\n         EmitParameterExtract(b, tile_info, remote_buf_memref);\n "
        },
        {
            "sha": "5951d2a21a995142add0d49087efdb5c178d2fba",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -118,19 +118,19 @@ absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n   Value rhs_scale;\n   if (rhs_dot_elem_type != ttir::ScaleDotElemType::BF16) {\n     rhs_scale = Bitcast(b, operands.rhs_scale, b.getI8Type());\n-    rhs_scale = b.create<mlir::stablehlo::TransposeOp>(\n-        rhs_scale, b.getDenseI64ArrayAttr({1, 0}));\n+    rhs_scale = mlir::stablehlo::TransposeOp::create(\n+        b, rhs_scale, b.getDenseI64ArrayAttr({1, 0}));\n   }\n \n-  auto dot_scaled_op =\n-      b.create<xtile::DotScaledOp>(operands.accumulator.getType(), operands.lhs,\n-                                   operands.rhs, lhs_scale, rhs_scale, true);\n+  auto dot_scaled_op = xtile::DotScaledOp::create(\n+      b, operands.accumulator.getType(), operands.lhs, operands.rhs, lhs_scale,\n+      rhs_scale, true);\n \n   auto add_result =\n       mlir::isa<mlir::IntegerType>(\n           dot_scaled_op.getResult().getType().getElementType())\n-          ? b.create<mlir::arith::AddIOp>(operands.accumulator, dot_scaled_op)\n-          : b.create<mlir::arith::AddFOp>(operands.accumulator, dot_scaled_op);\n+          ? mlir::arith::AddIOp::create(b, operands.accumulator, dot_scaled_op)\n+          : mlir::arith::AddFOp::create(b, operands.accumulator, dot_scaled_op);\n   return add_result->getResult(0);\n }\n \n@@ -157,16 +157,16 @@ Value EmitStableHloDotAndAdd(EmitterLocOpBuilder b, Value lhs, Value rhs,\n   auto precision_config = mlir::stablehlo::PrecisionConfigAttr::get(\n       b.getContext(), {precision_spec.lhs_operand_precision,\n                        precision_spec.rhs_operand_precision});\n-  auto dot = b.create<mlir::stablehlo::DotGeneralOp>(\n-      acc.getType(), lhs, rhs, dot_dimension_numbers,\n+  auto dot = mlir::stablehlo::DotGeneralOp::create(\n+      b, acc.getType(), lhs, rhs, dot_dimension_numbers,\n       /*precision_config=*/precision_config,\n       /*algorithm=*/\n       stablehlo::ConvertDotAlgorithm(precision_spec.algorithm, &b));\n \n   auto add_result =\n       mlir::isa<mlir::IntegerType>(dot.getResult().getType().getElementType())\n-          ? b.create<mlir::arith::AddIOp>(acc, dot)\n-          : b.create<mlir::arith::AddFOp>(acc, dot);\n+          ? mlir::arith::AddIOp::create(b, acc, dot)\n+          : mlir::arith::AddFOp::create(b, acc, dot);\n   return add_result->getResult(0);\n }\n "
        },
        {
            "sha": "c28733e667f6bf37c813c86021815af90d2b09a4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 89,
            "deletions": 86,
            "changes": 175,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -101,10 +101,10 @@ using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n                        int64_t upper) {\n   Value clamped_index =\n-      b.create<ma::MaxSIOp>(value, CreateConst(b, value.getType(), lower));\n-  clamped_index = b.create<ma::MinSIOp>(clamped_index,\n-                                        CreateConst(b, value.getType(), upper));\n-  return b.create<ma::IndexCastOp>(b.getIndexType(), clamped_index);\n+      ma::MaxSIOp::create(b, value, CreateConst(b, value.getType(), lower));\n+  clamped_index = ma::MinSIOp::create(b, clamped_index,\n+                                      CreateConst(b, value.getType(), upper));\n+  return ma::IndexCastOp::create(b, b.getIndexType(), clamped_index);\n }\n \n absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n@@ -267,17 +267,17 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n   }\n \n   if (src_ty.isIndex() || dst_ty.isIndex()) {\n-    return b.create<ma::IndexCastOp>(dst_ty, value);\n+    return ma::IndexCastOp::create(b, dst_ty, value);\n   }\n \n   // All operations on bf16 are done through f32.\n   if (src_element_ty.isBF16()) {\n-    return Cast(b, b.create<ma::ExtFOp>(fp32_ty, value), dst_element_ty);\n+    return Cast(b, ma::ExtFOp::create(b, fp32_ty, value), dst_element_ty);\n   }\n   if (dst_element_ty.isBF16()) {\n     // S8 -> BF16 is directly supported and doesn't need to go through f32.\n     if (!src_element_ty.isInteger(8)) {\n-      return b.create<ma::TruncFOp>(dst_ty, Cast(b, value, b.getF32Type()));\n+      return ma::TruncFOp::create(b, dst_ty, Cast(b, value, b.getF32Type()));\n     }\n   }\n \n@@ -287,15 +287,15 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n   if (src_fp_element_ty && dst_fp_element_ty) {\n     if (IsFp8Type(src_element_ty) && IsFp8Type(dst_element_ty)) {\n       // FP8 <-> FP8 conversion needs to go through FP16\n-      auto fp16_value = b.create<ma::ExtFOp>(fp16_ty, value);\n-      return b.create<ma::TruncFOp>(dst_ty, fp16_value);\n+      auto fp16_value = ma::ExtFOp::create(b, fp16_ty, value);\n+      return ma::TruncFOp::create(b, dst_ty, fp16_value);\n     }\n \n     if (src_fp_element_ty.getFPMantissaWidth() >\n         dst_fp_element_ty.getFPMantissaWidth()) {\n-      return b.create<ma::TruncFOp>(dst_ty, value);\n+      return ma::TruncFOp::create(b, dst_ty, value);\n     } else {\n-      return b.create<ma::ExtFOp>(dst_ty, value);\n+      return ma::ExtFOp::create(b, dst_ty, value);\n     }\n   }\n   // int => int\n@@ -304,30 +304,30 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n     if (src_element_ty.getIntOrFloatBitWidth() <\n         dst_element_ty.getIntOrFloatBitWidth()) {\n       if (src_element_ty.isInteger(1)) {\n-        return b.create<ma::ExtUIOp>(dst_ty, value);\n+        return ma::ExtUIOp::create(b, dst_ty, value);\n       }\n-      return b.create<ma::ExtSIOp>(dst_ty, value);\n+      return ma::ExtSIOp::create(b, dst_ty, value);\n     }\n     // int => bool is always value != 0.\n     if (dst_element_ty.isInteger(1)) {\n-      return b.create<ma::CmpIOp>(ma::CmpIPredicate::ne, value,\n-                                  ZerosLike(b, value));\n+      return ma::CmpIOp::create(b, ma::CmpIPredicate::ne, value,\n+                                ZerosLike(b, value));\n     }\n-    return b.create<ma::TruncIOp>(dst_ty, value);\n+    return ma::TruncIOp::create(b, dst_ty, value);\n   }\n   // int => float\n   if (mlir::isa<mlir::IntegerType>(src_element_ty) && dst_fp_element_ty) {\n     // The current logic handles signed integer types only.\n     if (src_element_ty.isInteger(1)) {\n-      return b.create<ma::UIToFPOp>(dst_ty, value);\n+      return ma::UIToFPOp::create(b, dst_ty, value);\n     }\n-    return b.create<ma::SIToFPOp>(dst_ty, value);\n+    return ma::SIToFPOp::create(b, dst_ty, value);\n   }\n   // float => int\n   if (src_fp_element_ty && mlir::isa<mlir::IntegerType>(dst_element_ty)) {\n     if (dst_element_ty.isInteger(1)) {\n-      return b.create<ma::CmpFOp>(ma::CmpFPredicate::UNE, value,\n-                                  ZerosLike(b, value));\n+      return ma::CmpFOp::create(b, ma::CmpFPredicate::UNE, value,\n+                                ZerosLike(b, value));\n     }\n     // The current logic handles signed integer types only. Additional handling\n     // is needed for unsigned integer types.\n@@ -345,22 +345,22 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n         return CreateConst(b, src_fp_element_ty, x);\n       }\n     };\n-    auto fptosi = b.create<ma::FPToSIOp>(dst_ty, value);\n+    auto fptosi = ma::FPToSIOp::create(b, dst_ty, value);\n     int64_t min = llvm::minIntN(dst_element_ty.getIntOrFloatBitWidth());\n     int64_t max = llvm::maxIntN(dst_element_ty.getIntOrFloatBitWidth());\n \n     // value <= static_cast<float>(INT_MIN) ? INT_MIN : ...\n-    auto clamped = b.create<ma::SelectOp>(\n-        b.create<ma::CmpFOp>(ma::CmpFPredicate::OLE, value, cst_float(min)),\n+    auto clamped = ma::SelectOp::create(\n+        b, ma::CmpFOp::create(b, ma::CmpFPredicate::OLE, value, cst_float(min)),\n         cst_int(min), fptosi);\n     // value >= static_cast<float>(INT_MAX) ? INT_MAX : ...\n-    clamped = b.create<ma::SelectOp>(\n-        b.create<ma::CmpFOp>(ma::CmpFPredicate::OGE, value, cst_float(max)),\n+    clamped = ma::SelectOp::create(\n+        b, ma::CmpFOp::create(b, ma::CmpFPredicate::OGE, value, cst_float(max)),\n         cst_int(max), clamped);\n     // isnan(value) ? 0 : ...\n-    return b.create<ma::SelectOp>(\n-        b.create<ma::CmpFOp>(ma::CmpFPredicate::UNO, value, value), cst_int(0),\n-        clamped);\n+    return ma::SelectOp::create(\n+        b, ma::CmpFOp::create(b, ma::CmpFPredicate::UNO, value, value),\n+        cst_int(0), clamped);\n   }\n \n   LOG(FATAL) << \"Type conversion not supported: \"\n@@ -370,23 +370,25 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n \n Value Subtract(EmitterLocOpBuilder& b, ValueRange values) {\n   if (mlir::isa<mlir::IntegerType>(mlir::getElementTypeOrSelf(values[0]))) {\n-    return b.create<ma::SubIOp>(values[0], values[1]);\n+    return ma::SubIOp::create(b, values[0], values[1]);\n   } else {\n-    return b.create<ma::SubFOp>(values[0], values[1]);\n+    return ma::SubFOp::create(b, values[0], values[1]);\n   }\n }\n \n Value Compare(EmitterLocOpBuilder& b, ValueRange values,\n               mh::ComparisonDirection direction) {\n   const Type type = mlir::getElementTypeOrSelf(values[0]);\n   if (mlir::isa<mlir::IntegerType>(type)) {\n-    return b.create<ma::CmpIOp>(mh::impl::getCmpPredicate<ma::CmpIPredicate>(\n-                                    direction,\n-                                    /*isSigned=*/!type.isInteger(1))\n-                                    .value(),\n-                                values[0], values[1]);\n-  }\n-  return b.create<ma::CmpFOp>(\n+    return ma::CmpIOp::create(b,\n+                              mh::impl::getCmpPredicate<ma::CmpIPredicate>(\n+                                  direction,\n+                                  /*isSigned=*/!type.isInteger(1))\n+                                  .value(),\n+                              values[0], values[1]);\n+  }\n+  return ma::CmpFOp::create(\n+      b,\n       mh::impl::getCmpPredicate<ma::CmpFPredicate>(direction,\n                                                    /*isSigned=*/true)\n           .value(),\n@@ -396,27 +398,27 @@ Value Compare(EmitterLocOpBuilder& b, ValueRange values,\n Value Maximum(EmitterLocOpBuilder& b, ValueRange values) {\n   auto type = mlir::getElementTypeOrSelf(values[0]);\n   if (mlir::isa<mlir::FloatType>(type)) {\n-    return b.create<ma::MaximumFOp>(values);\n+    return ma::MaximumFOp::create(b, values);\n   }\n \n   if (type.isInteger(1)) {\n-    return b.create<ma::OrIOp>(values);\n+    return ma::OrIOp::create(b, values);\n   }\n \n-  return b.create<ma::MaxSIOp>(values);\n+  return ma::MaxSIOp::create(b, values);\n }\n \n Value Minimum(EmitterLocOpBuilder& b, ValueRange values) {\n   auto type = mlir::getElementTypeOrSelf(values[0]);\n   if (mlir::isa<mlir::FloatType>(type)) {\n-    return b.create<ma::MinimumFOp>(values);\n+    return ma::MinimumFOp::create(b, values);\n   }\n \n   if (type.isInteger(1)) {\n-    return b.create<ma::AndIOp>(values);\n+    return ma::AndIOp::create(b, values);\n   }\n \n-  return b.create<ma::MinSIOp>(values);\n+  return ma::MinSIOp::create(b, values);\n }\n \n bool IsSupportedElementwiseLibdeviceFunction(const HloInstruction& hlo) {\n@@ -460,8 +462,8 @@ absl::StatusOr<Value> EmitElementwiseLibdeviceFunction(\n   } else {\n     casted_inputs.assign(inputs.begin(), inputs.end());\n   }\n-  Value res = b.create<mt::ExternElementwiseOp>(\n-      casted_inputs[0].getType(), casted_inputs, \"libdevice\", libdevice_path,\n+  Value res = mt::ExternElementwiseOp::create(\n+      b, casted_inputs[0].getType(), casted_inputs, \"libdevice\", libdevice_path,\n       ObtainDeviceFunctionName(dev_fn_id.value(), output_type, triple),\n       /*pure=*/true);\n   if (output_type == PrimitiveType::BF16 || output_type == PrimitiveType::F16) {\n@@ -484,15 +486,15 @@ absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n       return inputs[0];\n     case HloOpcode::kAbs:\n       if (is_integer) {\n-        return b.create<mm::AbsIOp>(inputs[0]);\n+        return mm::AbsIOp::create(b, inputs[0]);\n       }\n-      return b.create<mm::AbsFOp>(inputs[0]);\n+      return mm::AbsFOp::create(b, inputs[0]);\n     case HloOpcode::kCeil:\n-      return b.create<mm::CeilOp>(inputs[0]);\n+      return mm::CeilOp::create(b, inputs[0]);\n     case HloOpcode::kFloor:\n-      return b.create<mm::FloorOp>(inputs[0]);\n+      return mm::FloorOp::create(b, inputs[0]);\n     case HloOpcode::kNot:\n-      return b.create<ma::XOrIOp>(inputs[0], OnesLike(b, inputs[0]));\n+      return ma::XOrIOp::create(b, inputs[0], OnesLike(b, inputs[0]));\n     case HloOpcode::kNegate:\n       // NegFOp is not supported by Triton.\n       return Subtract(b, {ZerosLike(b, inputs[0]), inputs[0]});\n@@ -506,94 +508,95 @@ absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n         // XLA add semantics for predicates is equal to bitwise OR, while Arith\n         // defines it differently. Replace add with or in this case.\n         if (getElementTypeOrSelf(inputs[0]).isInteger(1)) {\n-          return b.create<ma::OrIOp>(inputs[0], inputs[1]);\n+          return ma::OrIOp::create(b, inputs[0], inputs[1]);\n         }\n-        return b.create<ma::AddIOp>(inputs[0], inputs[1]);\n+        return ma::AddIOp::create(b, inputs[0], inputs[1]);\n       }\n-      return b.create<ma::AddFOp>(inputs[0], inputs[1]);\n+      return ma::AddFOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kSubtract:\n       return Subtract(b, inputs);\n     case HloOpcode::kMultiply:\n       if (is_integer) {\n-        return b.create<ma::MulIOp>(inputs[0], inputs[1]);\n+        return ma::MulIOp::create(b, inputs[0], inputs[1]);\n       }\n-      return b.create<ma::MulFOp>(inputs[0], inputs[1]);\n+      return ma::MulFOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kMaximum:\n       return Maximum(b, inputs);\n     case HloOpcode::kMinimum:\n       return Minimum(b, inputs);\n     case HloOpcode::kClamp:\n       return Minimum(b, {Maximum(b, {inputs[0], inputs[1]}), inputs[2]});\n     case HloOpcode::kAnd:\n-      return b.create<ma::AndIOp>(inputs[0], inputs[1]);\n+      return ma::AndIOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kOr:\n-      return b.create<ma::OrIOp>(inputs[0], inputs[1]);\n+      return ma::OrIOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kXor:\n-      return b.create<ma::XOrIOp>(inputs[0], inputs[1]);\n+      return ma::XOrIOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kDivide:\n       if (is_integer) {\n         // Unsigned not supported yet.\n-        return b.create<ma::DivSIOp>(inputs[0], inputs[1]);\n+        return ma::DivSIOp::create(b, inputs[0], inputs[1]);\n       }\n-      return b.create<ma::DivFOp>(inputs[0], inputs[1]);\n+      return ma::DivFOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kCompare:\n       return Compare(\n           b, inputs,\n           mh::symbolizeComparisonDirection(\n               ComparisonDirectionToString(hlo.comparison_direction()))\n               .value());\n     case HloOpcode::kSelect:\n-      return b.create<ma::SelectOp>(\n+      return ma::SelectOp::create(\n+          b,\n           Compare(b, {inputs[0], ZerosLike(b, inputs[0])},\n                   mh::ComparisonDirection::NE),\n           inputs[1], inputs[2]);\n     case HloOpcode::kReducePrecision:\n       return mh::reducePrecision<mlir::tensor::BitcastOp>(\n           b.getLoc(), inputs[0], hlo.exponent_bits(), hlo.mantissa_bits(), &b);\n     case HloOpcode::kAcos:\n-      return b.create<mm::AcosOp>(inputs[0]);\n+      return mm::AcosOp::create(b, inputs[0]);\n     case HloOpcode::kAcosh:\n-      return b.create<mm::AcoshOp>(inputs[0]);\n+      return mm::AcoshOp::create(b, inputs[0]);\n     case HloOpcode::kAsin:\n-      return b.create<mm::AsinOp>(inputs[0]);\n+      return mm::AsinOp::create(b, inputs[0]);\n     case HloOpcode::kAsinh:\n-      return b.create<mm::AsinhOp>(inputs[0]);\n+      return mm::AsinhOp::create(b, inputs[0]);\n     case HloOpcode::kAtan2:\n-      return b.create<mm::Atan2Op>(inputs[0], inputs[1]);\n+      return mm::Atan2Op::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kAtanh:\n-      return b.create<mm::AtanhOp>(inputs[0]);\n+      return mm::AtanhOp::create(b, inputs[0]);\n     case HloOpcode::kCos:\n-      return b.create<mm::CosOp>(inputs[0]);\n+      return mm::CosOp::create(b, inputs[0]);\n     case HloOpcode::kCosh:\n-      return b.create<mm::CoshOp>(inputs[0]);\n+      return mm::CoshOp::create(b, inputs[0]);\n     case HloOpcode::kExp:\n-      return b.create<mm::ExpOp>(inputs[0]);\n+      return mm::ExpOp::create(b, inputs[0]);\n     case HloOpcode::kErf:\n-      return b.create<mm::ErfOp>(inputs[0]);\n+      return mm::ErfOp::create(b, inputs[0]);\n     case HloOpcode::kExpm1:\n-      return b.create<mm::ExpM1Op>(inputs[0]);\n+      return mm::ExpM1Op::create(b, inputs[0]);\n     case HloOpcode::kLog:\n-      return b.create<mm::LogOp>(inputs[0]);\n+      return mm::LogOp::create(b, inputs[0]);\n     case HloOpcode::kLog1p:\n-      return b.create<mm::Log1pOp>(inputs[0]);\n+      return mm::Log1pOp::create(b, inputs[0]);\n     case HloOpcode::kPower:\n-      return b.create<mm::PowFOp>(inputs[0], inputs[1]);\n+      return mm::PowFOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kRemainder:\n-      return b.create<ma::RemFOp>(inputs[0], inputs[1]);\n+      return ma::RemFOp::create(b, inputs[0], inputs[1]);\n     case HloOpcode::kRsqrt:\n-      return b.create<mm::RsqrtOp>(inputs[0]);\n+      return mm::RsqrtOp::create(b, inputs[0]);\n     case HloOpcode::kSin:\n-      return b.create<mm::SinOp>(inputs[0]);\n+      return mm::SinOp::create(b, inputs[0]);\n     case HloOpcode::kSinh:\n-      return b.create<mm::SinhOp>(inputs[0]);\n+      return mm::SinhOp::create(b, inputs[0]);\n     case HloOpcode::kSqrt:\n-      return b.create<mm::SqrtOp>(inputs[0]);\n+      return mm::SqrtOp::create(b, inputs[0]);\n     case HloOpcode::kTan:\n-      return b.create<mm::TanOp>(inputs[0]);\n+      return mm::TanOp::create(b, inputs[0]);\n     case HloOpcode::kTanh:\n-      return b.create<mm::TanhOp>(inputs[0]);\n+      return mm::TanhOp::create(b, inputs[0]);\n     case HloOpcode::kCbrt:\n-      return b.create<mm::CbrtOp>(inputs[0]);\n+      return mm::CbrtOp::create(b, inputs[0]);\n     default:\n       return absl::InvalidArgumentError(\n           absl::StrCat(\"Unsupported elementwise operation \", hlo.ToString()));\n@@ -621,7 +624,7 @@ absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n   auto value_type = value.getType();\n   value_type = mlir::dyn_cast<ShapedType>(value_type).clone(type);\n-  return b.create<mlir::arith::BitcastOp>(value_type, value);\n+  return mlir::arith::BitcastOp::create(b, value_type, value);\n }\n \n std::vector<llvm::Metadata*> ExtractNvvmAnnotations(\n@@ -745,8 +748,8 @@ TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n   auto tensor_type = mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n                                                  tile_info.storage_type());\n \n-  return b.create<xla::xtile::ExtractTileOp>(\n-      tensor_type, arg, tile_info.offsets(), tile_info.padded_tile_sizes(),\n+  return xla::xtile::ExtractTileOp::create(\n+      b, tensor_type, arg, tile_info.offsets(), tile_info.padded_tile_sizes(),\n       tile_info.tile_strides());\n }\n "
        },
        {
            "sha": "d99474f05eaa7d42928d8a22d9f5470c8e125524",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 58,
            "deletions": 55,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -187,7 +187,7 @@ using ::xla::gpu::triton::TritonType;\n namespace {\n \n Value MakeIndex(EmitterLocOpBuilder& b, int64_t value) {\n-  return b.create<arith::ConstantIndexOp>(value);\n+  return arith::ConstantIndexOp::create(b, value);\n }\n \n // Same as HLO BroadcastInDims. The sorted indices in `dims` specify the mapping\n@@ -200,22 +200,22 @@ TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n   auto result_type = mlir::RankedTensorType::get(\n       output_shape, value.getType().getElementType());\n \n-  return b.create<stablehlo::BroadcastInDimOp>(result_type, value, dims);\n+  return stablehlo::BroadcastInDimOp::create(b, result_type, value, dims);\n }\n \n TensorValue Splat(EmitterLocOpBuilder b, Value value,\n                   ArrayRef<int64_t> output_shape) {\n   auto tensor_value = mlir::dyn_cast<TensorValue>(value);\n   if (!tensor_value) {\n-    tensor_value = b.create<mlir::tensor::FromElementsOp>(\n-        mlir::RankedTensorType::get({}, value.getType()), value);\n+    tensor_value = mlir::tensor::FromElementsOp::create(\n+        b, mlir::RankedTensorType::get({}, value.getType()), value);\n   }\n   return BroadcastInDims(b, tensor_value, output_shape, /*dims=*/{});\n }\n \n TensorValue Iota(EmitterLocOpBuilder b, int32_t limit) {\n   auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n-  return b.create<stablehlo::IotaOp>(type, /*iota_dimension=*/0);\n+  return stablehlo::IotaOp::create(b, type, /*iota_dimension=*/0);\n }\n \n absl::StatusOr<TensorValue> EmitReduce(\n@@ -257,8 +257,8 @@ absl::StatusOr<TensorValue> EmitReduce(\n \n   Value init_value = values[tiled_hlo_reduce.operand(1)];\n \n-  stablehlo::ReduceOp reduction =\n-      b.create<stablehlo::ReduceOp>(input, init_value, hlo_reduce.dimensions());\n+  stablehlo::ReduceOp reduction = stablehlo::ReduceOp::create(\n+      b, input, init_value, hlo_reduce.dimensions());\n   {\n     TF_ASSIGN_OR_RETURN(Type result_ty,\n                         TritonType(b, hlo_reduce.shape().element_type()));\n@@ -295,7 +295,7 @@ absl::StatusOr<TensorValue> EmitReduce(\n \n     TF_ASSIGN_OR_RETURN(TensorValue result, EmitScope(b, /*analysis=*/nullptr,\n                                                       to_emit, region_values));\n-    b.create<stablehlo::ReturnOp>(SmallVector<Value>({result}));\n+    stablehlo::ReturnOp::create(b, SmallVector<Value>({result}));\n     b.setInsertionPointAfter(reduction);\n   }\n \n@@ -348,15 +348,15 @@ absl::StatusOr<TensorValue> EmitTiledIota(\n            b.getI32Type());\n \n   // First, stride as needed between the iota components.\n-  Value range = b.create<arith::MulIOp>(\n-      Iota(b, padded_tile_sizes[iota_dim]),\n+  Value range = arith::MulIOp::create(\n+      b, Iota(b, padded_tile_sizes[iota_dim]),\n       Splat(b,\n             CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim]),\n             padded_tile_sizes[iota_dim]));\n \n   // Then, add the base offset to the iota components.\n-  range = b.create<arith::AddIOp>(\n-      range, Splat(b, iota_dim_offset, padded_tile_sizes[iota_dim]));\n+  range = arith::AddIOp::create(\n+      b, range, Splat(b, iota_dim_offset, padded_tile_sizes[iota_dim]));\n \n   // Cast the result to the targeted type.\n   TF_ASSIGN_OR_RETURN(Type iota_element_type,\n@@ -406,7 +406,7 @@ absl::StatusOr<TensorValue> EmitTiledReshape(EmitterLocOpBuilder b,\n                      absl::StrJoin(output_tensor_type.getShape(), \"x\")));\n   }\n \n-  return b.create<stablehlo::ReshapeOp>(output_tensor_type, input);\n+  return stablehlo::ReshapeOp::create(b, output_tensor_type, input);\n }\n \n TensorValue EmitTiledTranspose(EmitterLocOpBuilder b,\n@@ -421,7 +421,7 @@ TensorValue EmitTiledTranspose(EmitterLocOpBuilder b,\n \n   mlir::DenseI64ArrayAttr order = b.getDenseI64ArrayAttr(dimensions);\n \n-  return b.create<stablehlo::TransposeOp>(output_tensor_type, input, order);\n+  return stablehlo::TransposeOp::create(b, output_tensor_type, input, order);\n }\n \n absl::StatusOr<TensorValue> EmitTiledBitcast(\n@@ -444,7 +444,7 @@ absl::StatusOr<TensorValue> EmitTiledBitcast(\n         GetPaddedTileSizes(tiled_bitcast.operand(0)->tile_sizes()),\n         output_element_type);\n     input = mlir::cast<TensorValue>(\n-        b.create<mlir::tensor::BitcastOp>(output_type, input).getResult());\n+        mlir::tensor::BitcastOp::create(b, output_type, input).getResult());\n     input_shape.set_element_type(output_shape.element_type());\n   }\n \n@@ -553,33 +553,33 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n     // full tiles (tiles without padding).\n     Type result_type = dot_operand_value.getType();\n     Value tile_size_value = CreateConst(b, b.getI32Type(), tile_size);\n-    Value num_full_tiles = b.create<arith::DivSIOp>(\n-        CreateConst(b, b.getI32Type(), contracting_dimension_size),\n+    Value num_full_tiles = arith::DivSIOp::create(\n+        b, CreateConst(b, b.getI32Type(), contracting_dimension_size),\n         tile_size_value);\n     // if tile_index >= num_full_tiles...\n-    auto cond = b.create<arith::CmpIOp>(arith::CmpIPredicate::sge,\n-                                        contracting_dimension_tile_index,\n-                                        num_full_tiles);\n-    auto if_op = b.create<mlir::scf::IfOp>(mlir::TypeRange(result_type), cond,\n-                                           /*withElseRegion=*/true);\n+    auto cond =\n+        arith::CmpIOp::create(b, arith::CmpIPredicate::sge,\n+                              contracting_dimension_tile_index, num_full_tiles);\n+    auto if_op = mlir::scf::IfOp::create(b, mlir::TypeRange(result_type), cond,\n+                                         /*withElseRegion=*/true);\n     // then ...\n     {\n       b.setInsertionPointToStart(if_op.thenBlock());\n       // indices =\n       //   contracting_dimension_tile_index * tile_size + range(0, tile_size)\n       // mask = indices < contracting_dimension_size\n       // operand = select(broadcast(mask, operand.shape), operand, 0)\n-      Value tile_offset = b.create<arith::MulIOp>(\n-          contracting_dimension_tile_index, tile_size_value);\n+      Value tile_offset = arith::MulIOp::create(\n+          b, contracting_dimension_tile_index, tile_size_value);\n       TensorValue range = Iota(b, tile_size);\n       TensorValue broadcasted_tile_offset = Splat(b, tile_offset, {tile_size});\n-      Value indices = b.create<arith::AddIOp>(range, broadcasted_tile_offset);\n+      Value indices = arith::AddIOp::create(b, range, broadcasted_tile_offset);\n \n       Value boundary = CreateConst(b, b.getI32Type(),\n                                    contracting_dimension_size, {tile_size});\n \n-      Value mask =\n-          b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, indices, boundary);\n+      Value mask = arith::CmpIOp::create(b, arith::CmpIPredicate::slt, indices,\n+                                         boundary);\n \n       mask = BroadcastInDims(b, mlir::cast<TensorValue>(mask), tile_shape,\n                              {contraction_dimension_index});\n@@ -590,13 +590,13 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n       TensorValue zero = CreateConst(b, element_type, 0.0f, tile_shape);\n \n       Value masked_dot_operand =\n-          b.create<arith::SelectOp>(mask, dot_operand_value, zero);\n-      b.create<mlir::scf::YieldOp>(masked_dot_operand);\n+          arith::SelectOp::create(b, mask, dot_operand_value, zero);\n+      mlir::scf::YieldOp::create(b, masked_dot_operand);\n     }\n     // else ...\n     {\n       b.setInsertionPointToStart(if_op.elseBlock());\n-      b.create<mlir::scf::YieldOp>(dot_operand_value);\n+      mlir::scf::YieldOp::create(b, dot_operand_value);\n     }\n     b.setInsertionPointAfter(if_op);\n     return mlir::cast<TensorValue>(if_op.getResult(0));\n@@ -753,7 +753,8 @@ absl::StatusOr<TensorValue> EmitDot(\n       {IndexingMap::Variable{{0, loop_iteration_count - 1}, \"k\"}},\n       /*rt_vars=*/{}};\n \n-  auto for_op = b.create<mlir::scf::ForOp>(\n+  auto for_op = mlir::scf::ForOp::create(\n+      b,\n       /*lowerBound=*/MakeIndex(b, 0),\n       /*upperBound=*/MakeIndex(b, loop_iteration_count),\n       /*step=*/MakeIndex(b, 1), accumulator);\n@@ -766,8 +767,8 @@ absl::StatusOr<TensorValue> EmitDot(\n     mlir::OpBuilder::InsertionGuard g(b);\n     b.setInsertionPointToStart(for_op.getBody());\n     Value ki = for_op.getInductionVar();\n-    Value computation_index = b.create<xla::ApplyIndexingOp>(\n-                                   ValueRange{pid, ki}, computation_index_map)\n+    Value computation_index = xla::ApplyIndexingOp::create(\n+                                  b, ValueRange{pid, ki}, computation_index_map)\n                                   .getResult(0);\n     SmallVector<TensorValue> dot_args;\n     for (const TiledHloInstruction* operand : tiled_hlo_dot.operands()) {\n@@ -817,7 +818,7 @@ absl::StatusOr<TensorValue> EmitDot(\n     TF_ASSIGN_OR_RETURN(\n         Value acc_next,\n         triton::EmitSingleTileDot(b, dot, triton::DotOperands{lhs, rhs, acc}));\n-    b.create<mlir::scf::YieldOp>(acc_next);\n+    mlir::scf::YieldOp::create(b, acc_next);\n   }\n \n   // The output of the loop may not match the expected output type of the dot.\n@@ -889,16 +890,17 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n \n   // TODO(b/449668102): Consider adding warp specialization support for scaled\n   // dot. At the moment, there are no benchmarks that use scaled dot.\n-  auto for_op = b.create<mlir::scf::ForOp>(\n+  auto for_op = mlir::scf::ForOp::create(\n+      b,\n       /*lowerBound=*/MakeIndex(b, 0),\n       /*upperBound=*/MakeIndex(b, loop_iteration_count),\n       /*step=*/MakeIndex(b, 1), accumulator);\n   {  // Loop body.\n     mlir::OpBuilder::InsertionGuard g(b);\n     b.setInsertionPointToStart(for_op.getBody());\n     Value ki = for_op.getInductionVar();\n-    Value computation_index = b.create<xla::ApplyIndexingOp>(\n-                                   ValueRange{pid, ki}, computation_index_map)\n+    Value computation_index = xla::ApplyIndexingOp::create(\n+                                  b, ValueRange{pid, ki}, computation_index_map)\n                                   .getResult(0);\n     SmallVector<TensorValue> dot_args;\n     for (const TiledHloInstruction* operand : tiled_hlo_dot.operands()) {\n@@ -969,7 +971,7 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n         triton::EmitSingleTileScaledDot(\n             b, scaled_dot,\n             triton::ScaledDotOperands{lhs, rhs, lhs_scale, rhs_scale, acc}));\n-    b.create<mlir::scf::YieldOp>(acc_next);\n+    mlir::scf::YieldOp::create(b, acc_next);\n   }\n \n   // The output of the loop may not match the expected output type of the dot.\n@@ -1066,15 +1068,16 @@ absl::StatusOr<TensorValue> EmitConcatenate(\n       Value offset_limit = CreateConst(b, b.getIndexType(), limit);\n \n       auto cond =\n-          b.create<arith::CmpIOp>(arith::CmpIPredicate::slt,\n-                                  concatenate_dimension_offset, offset_limit);\n-      auto if_op = b.create<mlir::scf::IfOp>(mlir::TypeRange(result_type), cond,\n-                                             /*withElseRegion=*/true);\n+          arith::CmpIOp::create(b, arith::CmpIPredicate::slt,\n+                                concatenate_dimension_offset, offset_limit);\n+      auto if_op =\n+          mlir::scf::IfOp::create(b, mlir::TypeRange(result_type), cond,\n+                                  /*withElseRegion=*/true);\n \n       // Propagate the result from the nested `if_op` if we were already within\n       // an `if_op`.\n       if (!if_ops.empty()) {\n-        b.create<mlir::scf::YieldOp>(if_op.getResult(0));\n+        mlir::scf::YieldOp::create(b, if_op.getResult(0));\n       }\n \n       b.setInsertionPointToStart(if_op.thenBlock());\n@@ -1091,7 +1094,7 @@ absl::StatusOr<TensorValue> EmitConcatenate(\n             *tiled_fusion_operand->called_computation(), block_level_parameters,\n             fn, pid, values));\n     CHECK_EQ(result.size(), 1);\n-    b.create<mlir::scf::YieldOp>(result.front());\n+    mlir::scf::YieldOp::create(b, result.front());\n   }\n \n   b.setInsertionPointAfter(if_ops.front());\n@@ -1136,12 +1139,12 @@ absl::StatusOr<TensorValue> EmitPad(\n \n     // RHS for the compare is splat(pad_input_dim_size - tile_offset).\n     Value tile_offset_i32 = Cast(b, tile_offset, i32_type);\n-    Value threshold = b.create<arith::SubIOp>(\n-        CreateConst(b, i32_type, pad_input_dim_size), tile_offset_i32);\n+    Value threshold = arith::SubIOp::create(\n+        b, CreateConst(b, i32_type, pad_input_dim_size), tile_offset_i32);\n     TensorValue threshold_splat = Splat(b, threshold, padded_tile_sizes);\n-    Value cmp = b.create<arith::CmpIOp>(arith::CmpIPredicate::slt, bcast,\n-                                        threshold_splat);\n-    mask = mask ? b.create<arith::AndIOp>(mask, cmp) : cmp;\n+    Value cmp = arith::CmpIOp::create(b, arith::CmpIPredicate::slt, bcast,\n+                                      threshold_splat);\n+    mask = mask ? arith::AndIOp::create(b, mask, cmp) : cmp;\n   }\n   if (!mask) {\n     return values[tiled_operand];\n@@ -1151,7 +1154,7 @@ absl::StatusOr<TensorValue> EmitPad(\n   TensorValue pad_value_splat =\n       Splat(b, values[padding_value], padded_tile_sizes);\n   return mlir::cast<TensorValue>(\n-      b.create<arith::SelectOp>(mask, values[tiled_operand], pad_value_splat)\n+      arith::SelectOp::create(b, mask, values[tiled_operand], pad_value_splat)\n           .getResult());\n }\n \n@@ -1512,9 +1515,9 @@ absl::Status EmitGeneric(\n         auto tile_info,\n         TileInfo::Construct(b, tile_id, /*runtime_values=*/{}, *root));\n \n-    b.create<xtile::InsertTileOp>(result, arg, tile_info.offsets(),\n-                                  tile_info.padded_tile_sizes(),\n-                                  tile_info.tile_strides());\n+    xtile::InsertTileOp::create(b, result, arg, tile_info.offsets(),\n+                                tile_info.padded_tile_sizes(),\n+                                tile_info.tile_strides());\n   }\n \n   return absl::OkStatus();\n@@ -2012,8 +2015,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n   llvm::SmallVector<mlir::NamedAttribute> named_attributes{b.getNamedAttr(\n       \"num_opaque_args\", b.getI32IntegerAttr(num_metadata_arguments))};\n \n-  auto fn =\n-      b.create<xtile::EntryFuncOp>(fn_name, fn_arg_types, named_attributes, {});\n+  auto fn = xtile::EntryFuncOp::create(b, fn_name, fn_arg_types,\n+                                       named_attributes, {});\n \n   fn.addEntryBlock();\n   b.setInsertionPointToStart(&fn.front());"
        },
        {
            "sha": "0c4fc82066bdc047330ded83a5ce242950894bef",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -130,7 +130,8 @@ ENTRY e {\n     EXPECT_THAT(RunFileCheck(annotated_ir, R\"(\n       CHECK:  [[SOMETHING:.*]] \"triton_dot -> [[FILE_LINE:fusion_emitter.*:.*]]\"\n     )\"),\n-                absl_testing::IsOkAndHolds(true));\n+                absl_testing::IsOkAndHolds(true))\n+        << annotated_ir;\n   } else {\n     EXPECT_THAT(RunFileCheck(annotated_ir, R\"(\n       CHECK:  [[SOMETHING:.*]] \"triton_dot\""
        },
        {
            "sha": "d4138468bfcdeaab015a13bcc8871d5e4f53808a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/generalize_kernel_signature.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fgeneralize_kernel_signature.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fgeneralize_kernel_signature.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fgeneralize_kernel_signature.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -73,10 +73,11 @@ void StripParameterAddressSpaces(RewriterBase& rewriter,\n   SmallVector<DictionaryAttr> arg_attrs(llvm::map_range(\n       func.getArgAttrsAttr().getValue(),\n       [](Attribute attr) { return cast<DictionaryAttr>(attr); }));\n-  auto generic_func = rewriter.create<LLVM::LLVMFuncOp>(\n-      func.getLoc(), func.getSymName(), generic_func_ty, func.getLinkage(),\n-      func.getDsoLocal(), func.getCConv(), /*comdat=*/nullptr,\n-      GetExtraAttrs(func), arg_attrs, func.getFunctionEntryCount());\n+  auto generic_func = LLVM::LLVMFuncOp::create(\n+      rewriter, func.getLoc(), func.getSymName(), generic_func_ty,\n+      func.getLinkage(), func.getDsoLocal(), func.getCConv(),\n+      /*comdat=*/nullptr, GetExtraAttrs(func), arg_attrs,\n+      func.getFunctionEntryCount());\n \n   // Convert generic address spaces back to original ones within the function\n   // body.\n@@ -88,7 +89,7 @@ void StripParameterAddressSpaces(RewriterBase& rewriter,\n     Value converted = arg;\n     if (arg.getType() != type) {\n       converted =\n-          rewriter.create<LLVM::AddrSpaceCastOp>(arg.getLoc(), type, arg);\n+          LLVM::AddrSpaceCastOp::create(rewriter, arg.getLoc(), type, arg);\n     }\n     converted_args.push_back(converted);\n   }"
        },
        {
            "sha": "0853937ecffea71126c158a944776bc5af35b86d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/int4_passes.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -416,21 +416,22 @@ class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n     auto packed_type = converter_.convertType(input_type);\n     VLOG(5) << \"ExtSIInt4ToInt8Pattern: Regular int4 to int8 conversion\";\n     Value shift4_const =\n-        r.create<ma::ConstantOp>(loc, r.getIntegerAttr(r.getI8Type(), 4));\n-    Value shift4 = r.create<mt::SplatOp>(loc, packed_type, shift4_const);\n+        ma::ConstantOp::create(r, loc, r.getIntegerAttr(r.getI8Type(), 4));\n+    Value shift4 = mt::SplatOp::create(r, loc, packed_type, shift4_const);\n     Value shifted_lo =\n-        r.create<ma::ShLIOp>(loc, packed_type, adaptor.getIn(), shift4);\n-    Value lo = r.create<ma::ShRSIOp>(loc, packed_type, shifted_lo, shift4);\n-    Value hi = r.create<ma::ShRSIOp>(loc, packed_type, adaptor.getIn(), shift4);\n-    Value hi_lo = r.create<mt::JoinOp>(loc, lo, hi);\n+        ma::ShLIOp::create(r, loc, packed_type, adaptor.getIn(), shift4);\n+    Value lo = ma::ShRSIOp::create(r, loc, packed_type, shifted_lo, shift4);\n+    Value hi =\n+        ma::ShRSIOp::create(r, loc, packed_type, adaptor.getIn(), shift4);\n+    Value hi_lo = mt::JoinOp::create(r, loc, lo, hi);\n     if (converter_.packed_dimension() + 1 != input_type.getRank()) {\n       // Move the minor (joined) dimension to just after the packed dimension.\n       SmallVector<int32_t> trans_order(input_type.getRank() + 1);\n       absl::c_iota(trans_order, 0);\n       std::rotate(trans_order.begin() + converter_.packed_dimension() + 1,\n                   std::prev(trans_order.end()), trans_order.end());\n       auto trans_attr = r.getDenseI32ArrayAttr(trans_order);\n-      hi_lo = r.create<mt::TransOp>(loc, hi_lo, trans_attr);\n+      hi_lo = mt::TransOp::create(r, loc, hi_lo, trans_attr);\n     }\n     auto unpacked_type = input_type.clone(r.getI8Type());\n     r.replaceOpWithNewOp<mt::ReshapeOp>(ext_si_op, unpacked_type, hi_lo,\n@@ -476,21 +477,21 @@ class ExtSIInt4ToInt8Pattern : public OpConversionPattern<ma::ExtSIOp> {\n         sub.bf16x2 $3, $3, bias;\n       }\n     )\";\n-    auto elementwise_op = r.create<ElementwiseInlineAsmOp>(\n-        loc, std::vector<Type>{bf16_packed_type, bf16_packed_type},\n+    auto elementwise_op = ElementwiseInlineAsmOp::create(\n+        r, loc, std::vector<Type>{bf16_packed_type, bf16_packed_type},\n         kInt4ToBF16Asm, \"=r,=r,=r,=r,r\",\n         /*pure=*/true, /*pack_result=*/4, adaptor.getOperands());\n     Value lo = elementwise_op->getResult(0);\n     Value hi = elementwise_op->getResult(1);\n-    Value hi_lo = r.create<mt::JoinOp>(loc, lo, hi);\n+    Value hi_lo = mt::JoinOp::create(r, loc, lo, hi);\n     if (converter_.packed_dimension() + 1 != input_type.getRank()) {\n       // Move the minor (joined) dimension to just after the packed dimension.\n       SmallVector<int32_t> trans_order(input_type.getRank() + 1);\n       absl::c_iota(trans_order, 0);\n       std::rotate(trans_order.begin() + converter_.packed_dimension() + 1,\n                   std::prev(trans_order.end()), trans_order.end());\n       auto trans_attr = r.getDenseI32ArrayAttr(trans_order);\n-      hi_lo = r.create<mt::TransOp>(loc, hi_lo, trans_attr);\n+      hi_lo = mt::TransOp::create(r, loc, hi_lo, trans_attr);\n     }\n \n     r.replaceOpWithNewOp<mt::ReshapeOp>(si_to_fp_op, bf16_type, hi_lo,\n@@ -679,7 +680,7 @@ LogicalResult SitofpInt4ToInt8Rewrite(ma::SIToFPOp op, PatternRewriter &r) {\n   if (auto tensor_type = dyn_cast<RankedTensorType>(op.getType())) {\n     type = tensor_type.clone(type);\n   }\n-  auto ext_si_op = r.create<ma::ExtSIOp>(op.getLoc(), type, op.getIn());\n+  auto ext_si_op = ma::ExtSIOp::create(r, op.getLoc(), type, op.getIn());\n   r.replaceOpWithNewOp<ma::SIToFPOp>(op, op.getType(), ext_si_op);\n   return success();\n }\n@@ -733,8 +734,9 @@ LogicalResult SitofpToExtFpSitofpRewrite(ma::SIToFPOp sitofp_op,\n              \"ExtFOp(SiToFp(i4): bf16): Fp32: type:\"\n           << DumpToString(type_ranked);\n   auto loc = sitofp_op.getLoc();\n-  auto sitofp_bf16_op = rewriter.create<ma::SIToFPOp>(\n-      loc, type_ranked.clone(rewriter.getBF16Type()), sitofp_op.getIn());\n+  auto sitofp_bf16_op = ma::SIToFPOp::create(\n+      rewriter, loc, type_ranked.clone(rewriter.getBF16Type()),\n+      sitofp_op.getIn());\n   rewriter.replaceOpWithNewOp<ma::ExtFOp>(sitofp_op, type, sitofp_bf16_op,\n                                           ma::FastMathFlagsAttr{});\n   return success();"
        },
        {
            "sha": "00c791c02a4dc8dd34c4d9ba2a8b18fae8c66873",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/round_f32_to_tf32_for_tf32_dot_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fround_f32_to_tf32_for_tf32_dot_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fround_f32_to_tf32_for_tf32_dot_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fround_f32_to_tf32_for_tf32_dot_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -56,10 +56,10 @@ class Tf32DotPattern : public OpRewritePattern<mt::DotOp> {\n     if (op->hasAttr(tf32_args_rounded)) return failure();\n \n     auto f32ToTF32 = [&](Value value) -> Value {\n-      return rewriter\n-          .create<ElementwiseInlineAsmOp>(\n-              op.getLoc(), value.getType(), \"cvt.rna.tf32.f32 $0, $1;\", \"=r,r\",\n-              /*isPure=*/true, /*pack=*/1, ArrayRef<Value>{value})\n+      return ElementwiseInlineAsmOp::create(\n+                 rewriter, op.getLoc(), value.getType(),\n+                 \"cvt.rna.tf32.f32 $0, $1;\", \"=r,r\",\n+                 /*isPure=*/true, /*pack=*/1, ArrayRef<Value>{value})\n           ->getResult(0);\n     };\n     auto lhs = f32ToTF32(op.getA());"
        },
        {
            "sha": "d0176d3f3ba9a986fcaef779a92309cf46416db6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/stablehlo_lower_to_triton.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -154,7 +154,7 @@ class LowerBroadcastInDim\n         axis = output_dim_id + 1;\n         continue;\n       }\n-      input_tensor = builder.create<ttir::ExpandDimsOp>(input_tensor, axis);\n+      input_tensor = ttir::ExpandDimsOp::create(builder, input_tensor, axis);\n     }\n     rewriter.replaceOpWithNewOp<ttir::BroadcastOp>(op, op.getResult().getType(),\n                                                    input_tensor);\n@@ -424,8 +424,8 @@ absl::StatusOr<Value> EmitDotAlgUnset(\n     max_num_imprecise_acc = std::numeric_limits<int>::max();\n   }\n \n-  return b.create<ttir::DotOp>(\n-      lhs, rhs, acc,\n+  return ttir::DotOp::create(\n+      b, lhs, rhs, acc,\n       /*inputPrecision=*/precision_spec.ttir_input_precision,\n       /*maxNumImpreciseAcc=*/max_num_imprecise_acc);\n }\n@@ -458,8 +458,8 @@ absl::StatusOr<Value> EmitRegularDot(\n     }\n   }\n \n-  return b.create<ttir::DotOp>(\n-      dot_operands.lhs, dot_operands.rhs, dot_operands.accumulator,\n+  return ttir::DotOp::create(\n+      b, dot_operands.lhs, dot_operands.rhs, dot_operands.accumulator,\n       /*inputPrecision=*/precision_spec.ttir_input_precision,\n       /*maxNumImpreciseAcc=*/max_num_imprecise_acc);\n }\n@@ -475,11 +475,11 @@ Value ZeroNaNs(::xla::EmitterLocOpBuilder b, Value input) {\n   Value positive_inf = ::xla::gpu::triton::CreateConst<float>(\n       b, b.getF32Type(), std::numeric_limits<float>::infinity(),\n       mlir::cast<ShapedType>(input.getType()).getShape());\n-  Value abs_input = b.create<math::AbsFOp>(input);\n-  Value is_finite = b.create<arith::CmpFOp>(arith::CmpFPredicate::OGT,\n-                                            positive_inf, abs_input);\n-  return b.create<arith::SelectOp>(is_finite, input,\n-                                   ::xla::gpu::triton::ZerosLike(b, input));\n+  Value abs_input = math::AbsFOp::create(b, input);\n+  Value is_finite = arith::CmpFOp::create(b, arith::CmpFPredicate::OGT,\n+                                          positive_inf, abs_input);\n+  return arith::SelectOp::create(b, is_finite, input,\n+                                 ::xla::gpu::triton::ZerosLike(b, input));\n }\n \n absl::Status ExpectType(Value v, Type expected_type) {\n@@ -506,17 +506,17 @@ std::vector<Value> SplitF32(::xla::EmitterLocOpBuilder b, Value input,\n     if (i != split_count - 1) {\n       Value input_as_f32 =\n           ::xla::gpu::triton::Cast(b, input_as_bf16, b.getF32Type());\n-      input = b.create<arith::SubFOp>(input, input_as_f32);\n+      input = arith::SubFOp::create(b, input, input_as_f32);\n     }\n     split_inputs.push_back(input_as_bf16);\n   }\n   return split_inputs;\n }\n \n Value IEEEDot(::xla::EmitterLocOpBuilder b, Value lhs, Value rhs, Value acc) {\n-  return b.create<ttir::DotOp>(lhs, rhs, acc,\n-                               /*inputPrecision=*/ttir::InputPrecision::IEEE,\n-                               /*maxNumImpreciseAcc=*/0);\n+  return ttir::DotOp::create(b, lhs, rhs, acc,\n+                             /*inputPrecision=*/ttir::InputPrecision::IEEE,\n+                             /*maxNumImpreciseAcc=*/0);\n }\n \n // Leverages BF16 datatype for F32 matmul computation. It follows the guidance\n@@ -554,7 +554,7 @@ absl::StatusOr<Value> EmitBF16x9Matmul(\n \n   result = ZeroNaNs(b, result);\n   result = IEEEDot(b, lhs_parts[kHigh], rhs_parts[kHigh], result);\n-  result = b.create<arith::AddFOp>(dot_operands.accumulator, result);\n+  result = arith::AddFOp::create(b, dot_operands.accumulator, result);\n   return result;\n }\n \n@@ -589,7 +589,7 @@ absl::StatusOr<Value> EmitBF16x6Matmul(\n \n   result = ZeroNaNs(b, result);\n   result = IEEEDot(b, lhs_parts[kHigh], rhs_parts[kHigh], result);\n-  result = b.create<arith::AddFOp>(dot_operands.accumulator, result);\n+  result = arith::AddFOp::create(b, dot_operands.accumulator, result);\n   return result;\n }\n \n@@ -616,7 +616,7 @@ absl::StatusOr<Value> EmitBF16x3Matmul(\n   result = IEEEDot(b, lhs_bf16[kHigh], rhs_bf16[kLow], result);\n   result = ZeroNaNs(b, result);\n   result = IEEEDot(b, lhs_bf16[kHigh], rhs_bf16[kHigh], result);\n-  result = b.create<arith::AddFOp>(dot_operands.accumulator, result);\n+  result = arith::AddFOp::create(b, dot_operands.accumulator, result);\n   return result;\n }\n "
        },
        {
            "sha": "abe34672b4e23a2a14b675c72b176ffbcdfaa0a9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -95,7 +95,7 @@ SmallVector<Value> IndexCast(::xla::EmitterLocOpBuilder& builder, Type type,\n   SmallVector<Value> result;\n   result.reserve(values.size());\n   for (auto value : values) {\n-    result.push_back(builder.create<arith::IndexCastOp>(type, value));\n+    result.push_back(arith::IndexCastOp::create(builder, type, value));\n   }\n   return result;\n }\n@@ -326,8 +326,8 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n           builder.getContext(),\n           RankedTensorType::get(ordered_block_shape, element_type));\n       // !tt.tensordesc<tensor<block_shape x element_type>> -> !tt.ptr<>\n-      auto cast_to_orig_type = builder.create<mlir::UnrealizedConversionCastOp>(\n-          operand_type, func_arg);\n+      auto cast_to_orig_type = mlir::UnrealizedConversionCastOp::create(\n+          builder, operand_type, func_arg);\n       func_arg.replaceAllUsesExcept(cast_to_orig_type.getResult(0),\n                                     cast_to_orig_type);\n     }\n@@ -354,8 +354,8 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n \n     // Currently not propagating any function attributes to the new function.\n     ArrayRef<NamedAttribute> attrs;\n-    auto new_func = builder.create<triton::FuncOp>(\n-        op.getName(), new_function_type, attrs, arg_attrs);\n+    auto new_func = triton::FuncOp::create(builder, op.getName(),\n+                                           new_function_type, attrs, arg_attrs);\n \n     for (int i = 0; i < new_func.getNumArguments(); ++i) {\n       // TMA arguments don't require tt.divisibility.\n@@ -372,7 +372,7 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n \n     auto terminator = new_func.getBody().front().getTerminator();\n     rewriter.setInsertionPoint(terminator);\n-    rewriter.create<triton::ReturnOp>(new_func.getLoc());\n+    triton::ReturnOp::create(rewriter, new_func.getLoc());\n     rewriter.eraseOp(terminator);\n \n     return mlir::success();\n@@ -414,7 +414,7 @@ Value ExpandAndBroadcastValue(::xla::EmitterLocOpBuilder& builder, Value value,\n                               int dim, RankedTensorType tile_type) {\n   for (int i = 0; i < tile_type.getRank(); ++i) {\n     if (i != dim) {\n-      value = builder.create<ExpandDimsOp>(value, i);\n+      value = ExpandDimsOp::create(builder, value, i);\n     }\n   }\n   return BroadcastOp::create(builder, tile_type, value);\n@@ -597,12 +597,13 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n         reduced_dims, tile_shape);\n     Value other;\n     if (mask) {\n-      other = builder.create<arith::ConstantOp>(builder.getZeroAttr(\n-          RankedTensorType::get(tile_shape, tile_type.getElementType())));\n+      other = arith::ConstantOp::create(\n+          builder, builder.getZeroAttr(RankedTensorType::get(\n+                       tile_shape, tile_type.getElementType())));\n     }\n-    auto load = builder.create<LoadOp>(ptr, mask, other, CacheModifier::NONE,\n-                                       EvictionPolicy::NORMAL,\n-                                       /*isVolatile=*/false);\n+    auto load = LoadOp::create(builder, ptr, mask, other, CacheModifier::NONE,\n+                               EvictionPolicy::NORMAL,\n+                               /*isVolatile=*/false);\n     rewriter.replaceOp(op, load);\n     return mlir::success();\n   }\n@@ -678,7 +679,7 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n         // Transpose to a major-to-minor tensor by simply reversing the layout.\n         auto transpose_order = llvm::to_vector_of<int32_t>(dst_layout);\n         std::reverse(transpose_order.begin(), transpose_order.end());\n-        src = builder.create<TransOp>(src, transpose_order);\n+        src = TransOp::create(builder, src, transpose_order);\n       }\n \n       auto ordered_offsets = GetMajorToMinorOrder(offsets, dst_layout);\n@@ -713,9 +714,9 @@ class RewriteScalarInsert : public mlir::OpRewritePattern<tensor::InsertOp> {\n     }\n     ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n     auto ptr_type = GetTensorPtrType(op.getScalar().getType());\n-    auto cast_dst_to_tensor_ptr_type =\n-        builder.create<mlir::UnrealizedConversionCastOp>(ptr_type, op.getDest())\n-            .getResult(0);\n+    auto cast_dst_to_tensor_ptr_type = mlir::UnrealizedConversionCastOp::create(\n+                                           builder, ptr_type, op.getDest())\n+                                           .getResult(0);\n     StoreOp::create(builder, cast_dst_to_tensor_ptr_type, op.getScalar(),\n                     /*boundary_checks=*/std::vector<int32_t>{},\n                     CacheModifier::NONE, EvictionPolicy::NORMAL);"
        },
        {
            "sha": "e443ef532a3e2103269016bc3d8b64cf3a6099d9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_fold_transpose_pass.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_fold_transpose_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -132,8 +132,8 @@ LogicalResult PushTransposeUpThroughBroadcast(TransOp op,\n     return rewriter.notifyMatchFailure(  //\n         op, \"Transpose source is not a broadcast.\");\n   }\n-  Value new_trans = rewriter.create<TransOp>(op.getLoc(), broadcast.getSrc(),\n-                                             op.getOrderAttr());\n+  Value new_trans = TransOp::create(rewriter, op.getLoc(), broadcast.getSrc(),\n+                                    op.getOrderAttr());\n   rewriter.replaceOpWithNewOp<BroadcastOp>(op, op.getType(), new_trans);\n   return success();\n }\n@@ -162,7 +162,7 @@ LogicalResult PushTransposeUpThroughExpandDims(TransOp op,\n   }\n \n   Value new_trans =\n-      rewriter.create<TransOp>(op.getLoc(), expand_dims.getSrc(), new_order);\n+      TransOp::create(rewriter, op.getLoc(), expand_dims.getSrc(), new_order);\n   rewriter.replaceOpWithNewOp<ExpandDimsOp>(op, op.getType(), new_trans,\n                                             new_axis);\n   return success();\n@@ -181,8 +181,8 @@ LogicalResult PushTransposeUpThroughElementwise(TransOp op,\n   new_operands.reserve(elementwise->getNumOperands());\n   for (Value operand : elementwise->getOperands()) {\n     if (auto tensor_type = dyn_cast<RankedTensorType>(operand.getType())) {\n-      operand = rewriter.create<TransOp>(elementwise->getLoc(), operand,\n-                                         op.getOrderAttr());\n+      operand = TransOp::create(rewriter, elementwise->getLoc(), operand,\n+                                op.getOrderAttr());\n     }\n     new_operands.push_back(operand);\n   }\n@@ -223,9 +223,10 @@ LogicalResult PushTransposeUpIntoIf(TransOp op, PatternRewriter& rewriter) {\n   auto new_types = llvm::to_vector(if_op.getResultTypes());\n   new_types[result_number] = op.getType();\n \n-  auto new_if_op = rewriter.create<scf::IfOp>(\n-      op.getLoc(), new_types, if_op.getCondition(), /*addThenBlock=*/false,\n-      /*addElseBlock=*/false);\n+  auto new_if_op =\n+      scf::IfOp::create(rewriter, op.getLoc(), new_types, if_op.getCondition(),\n+                        /*addThenBlock=*/false,\n+                        /*addElseBlock=*/false);\n \n   // Update then and else regions.\n   for (auto [old_region, new_region] :\n@@ -236,9 +237,9 @@ LogicalResult PushTransposeUpIntoIf(TransOp op, PatternRewriter& rewriter) {\n     }\n     auto yield_op = new_region->front().getTerminator();\n     OpBuilder::InsertionGuard guard = SetInsertionPoint(rewriter, yield_op);\n-    auto trans_op = rewriter.create<TransOp>(\n-        op.getLoc(), op.getType(), yield_op->getOperand(result_number),\n-        op.getOrderAttr());\n+    auto trans_op =\n+        TransOp::create(rewriter, op.getLoc(), op.getType(),\n+                        yield_op->getOperand(result_number), op.getOrderAttr());\n     yield_op->setOperand(result_number, trans_op);\n   }\n   rewriter.replaceOp(op, new_if_op.getResult(result_number));\n@@ -312,7 +313,7 @@ LogicalResult PushTransposeUpThroughReshape(TransOp op,\n   }\n \n   auto new_trans =\n-      rewriter.create<TransOp>(reshape.getLoc(), reshape.getSrc(), new_order);\n+      TransOp::create(rewriter, reshape.getLoc(), reshape.getSrc(), new_order);\n   rewriter.replaceOpWithNewOp<ReshapeOp>(op, op.getType(), new_trans);\n   return success();\n }"
        },
        {
            "sha": "28bbd7b9390141ae9a8b9b541f3d9cff43898dd4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_atomics_pass.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -116,7 +116,8 @@ LogicalResult LowerAtomicWriteOp(AtomicWriteOp atomic_write,\n   if (mask) {\n     const std::string atomic_write_asm_with_mask = absl::StrFormat(\n         kAtomicWriteAsmWithMaskTemplate, scope, memory_semantic);\n-    builder.create<triton::ElementwiseInlineAsmOp>(\n+    triton::ElementwiseInlineAsmOp::create(\n+        builder,\n         /*result_types=*/result_type,\n         /*asm_string=*/rewriter.getStringAttr(atomic_write_asm_with_mask),\n         /*constraints=*/rewriter.getStringAttr(\"=r,l,r,r\"),\n@@ -126,7 +127,8 @@ LogicalResult LowerAtomicWriteOp(AtomicWriteOp atomic_write,\n   } else {\n     const std::string atomic_write_asm =\n         absl::StrFormat(kAtomicWriteAsmTemplate, scope, memory_semantic);\n-    builder.create<triton::ElementwiseInlineAsmOp>(\n+    triton::ElementwiseInlineAsmOp::create(\n+        builder,\n         /*result_types=*/result_type,\n         /*asm_string=*/rewriter.getStringAttr(atomic_write_asm),\n         /*constraints=*/rewriter.getStringAttr(\"=r,l,r\"),\n@@ -180,7 +182,8 @@ LogicalResult LowerAtomicSpinWaitOp(AtomicSpinWaitOp atomic_wait,\n   if (mask) {\n     const std::string atomic_wait_asm_with_mask = absl::StrFormat(\n         kAtomicSpinWaitAsmWithMaskTemplate, scope, memory_semantic, comparator);\n-    builder.create<triton::ElementwiseInlineAsmOp>(\n+    triton::ElementwiseInlineAsmOp::create(\n+        builder,\n         /*result_types=*/result_type,\n         /*asm_string=*/rewriter.getStringAttr(atomic_wait_asm_with_mask),\n         /*constraints=*/rewriter.getStringAttr(\"=r,l,r,r\"),\n@@ -190,7 +193,8 @@ LogicalResult LowerAtomicSpinWaitOp(AtomicSpinWaitOp atomic_wait,\n   } else {\n     const std::string atomic_wait_asm = absl::StrFormat(\n         kAtomicSpinWaitAsmTemplate, scope, memory_semantic, comparator);\n-    builder.create<triton::ElementwiseInlineAsmOp>(\n+    triton::ElementwiseInlineAsmOp::create(\n+        builder,\n         /*result_types=*/result_type,\n         /*asm_string=*/rewriter.getStringAttr(atomic_wait_asm),\n         /*constraints=*/rewriter.getStringAttr(\"=r,l,r\"),"
        },
        {
            "sha": "89da80500bc22c6955dbab087b1ad0093560ac2c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_block_barrier_pass.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 42,
            "changes": 87,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -54,17 +54,18 @@ LogicalResult LowerBlockBarrierOp(BlockBarrierOp block_barrier,\n   constexpr int32_t kGlobalAddressSpace = 1;\n \n   const mlir::TypedValue<mlir::Type> world_size_op =\n-      builder.create<mlir::arith::ConstantOp>(\n-          builder.getI32IntegerAttr(world_size));\n+      mlir::arith::ConstantOp::create(builder,\n+                                      builder.getI32IntegerAttr(world_size));\n   const mlir::TypedValue<mlir::IntegerType> thread_id =\n-      builder.create<triton::xla::GetTidOp>();\n+      triton::xla::GetTidOp::create(builder);\n   const mlir::TypedValue<mlir::IntegerType> block_id =\n-      builder.create<triton::GetProgramIdOp>(0);\n-  auto tid_is_lt_world_size = builder.create<mlir::arith::CmpIOp>(\n-      mlir::arith::CmpIPredicate::ult, thread_id, world_size_op);\n+      triton::GetProgramIdOp::create(builder, 0);\n+  auto tid_is_lt_world_size = mlir::arith::CmpIOp::create(\n+      builder, mlir::arith::CmpIPredicate::ult, thread_id, world_size_op);\n \n   // Only the first `world_size` threads will execute this block.\n-  builder.create<mlir::scf::IfOp>(\n+  mlir::scf::IfOp::create(\n+      builder,\n       /*cond=*/tid_is_lt_world_size,\n       // Inside if block so tid must be less than world_size.\n       /*thenBuilder=*/\n@@ -95,44 +96,45 @@ LogicalResult LowerBlockBarrierOp(BlockBarrierOp block_barrier,\n         // Triton seems to fail to do pointer arithmetic on pointer of\n         // pointers. So we cast the inner one to i64.\n         // -> !tt.ptr<i64>\n-        auto signal_buffers_i64 = builder.create<mlir::triton::BitcastOp>(\n-            ptr_to_i64_type, signal_buffers_arg);\n+        auto signal_buffers_i64 = mlir::triton::BitcastOp::create(\n+            builder, ptr_to_i64_type, signal_buffers_arg);\n         // SignalBuffers[WorldSize][BlockSize][WorldSize]\n         // -> tensor<world_size x !tt.ptr<i64>>\n-        auto signal_buffers_tensor = builder.create<mlir::triton::SplatOp>(\n-            tensor_of_i64_ptr_type, signal_buffers_i64);\n+        auto signal_buffers_tensor = mlir::triton::SplatOp::create(\n+            builder, tensor_of_i64_ptr_type, signal_buffers_i64);\n         // -> tensor<world_size x i32>\n-        auto all_ranks = builder.create<mlir::triton::MakeRangeOp>(\n-            i32_tensor_type, 0, world_size);\n+        auto all_ranks = mlir::triton::MakeRangeOp::create(\n+            builder, i32_tensor_type, 0, world_size);\n         // Pointer to SignalBuffers[0..WorldSize]\n         // -> tensor<world_size x !tt.ptr<i64>>\n-        auto signal_buffer_ptr = builder.create<mlir::triton::AddPtrOp>(\n-            tensor_of_i64_ptr_type, signal_buffers_tensor, all_ranks);\n+        auto signal_buffer_ptr = mlir::triton::AddPtrOp::create(\n+            builder, tensor_of_i64_ptr_type, signal_buffers_tensor, all_ranks);\n         // SignalBuffers[0..WorldSize]\n         // -> tensor<world_size x i64>\n-        auto signal_buffer_i64 = builder.create<mlir::triton::LoadOp>(\n+        auto signal_buffer_i64 = mlir::triton::LoadOp::create(\n+            builder,\n             /*ptr=*/signal_buffer_ptr,\n             /*cache=*/mlir::triton::CacheModifier::NONE,\n             /*evict=*/mlir::triton::EvictionPolicy::NORMAL,\n             /*isVolatile=*/false);\n         // -> tensor<world_size x !tt.ptr<i32>>\n-        auto signal_buffer = builder.create<mlir::triton::IntToPtrOp>(\n-            tensor_of_ptr_to_i32_type, signal_buffer_i64);\n-        auto block_offset = builder.create<mlir::arith::MulIOp>(\n-            i32_type, block_id, world_size_op);\n+        auto signal_buffer = mlir::triton::IntToPtrOp::create(\n+            builder, tensor_of_ptr_to_i32_type, signal_buffer_i64);\n+        auto block_offset = mlir::arith::MulIOp::create(\n+            builder, i32_type, block_id, world_size_op);\n         auto block_offset_plus_rank =\n-            builder.create<mlir::arith::AddIOp>(i32_type, block_offset, rank);\n+            mlir::arith::AddIOp::create(builder, i32_type, block_offset, rank);\n         // -> tensor<world_size x i32>\n-        auto block_offset_plus_rank_tensor =\n-            builder.create<mlir::triton::SplatOp>(i32_tensor_type,\n-                                                  block_offset_plus_rank);\n+        auto block_offset_plus_rank_tensor = mlir::triton::SplatOp::create(\n+            builder, i32_tensor_type, block_offset_plus_rank);\n         // SignalBuffers[0..WorldSize][block_id][rank]\n         // -> tensor<world_size x !tt.ptr<i32>>\n-        auto signal_addresses = builder.create<mlir::triton::AddPtrOp>(\n-            tensor_of_ptr_to_i32_type, signal_buffer,\n+        auto signal_addresses = mlir::triton::AddPtrOp::create(\n+            builder, tensor_of_ptr_to_i32_type, signal_buffer,\n             block_offset_plus_rank_tensor);\n         // Signal all ranks on the same block id.\n-        builder.create<mlir::triton::xla::AtomicWriteOp>(\n+        mlir::triton::xla::AtomicWriteOp::create(\n+            builder,\n             /*result_types=*/mlir::TypeRange{},\n             /*ptr=*/signal_addresses,\n             /*signal_value=*/signal_value,\n@@ -141,34 +143,35 @@ LogicalResult LowerBlockBarrierOp(BlockBarrierOp block_barrier,\n             /*sem=*/mlir::triton::MemSemantic::RELEASE);\n         // Pointer to SignalBuffers[rank]\n         // -> !tt.ptr<i64>\n-        auto read_address_ptr_to_i64 = builder.create<mlir::triton::AddPtrOp>(\n-            signal_buffers_i64.getType(), signal_buffers_i64, rank);\n+        auto read_address_ptr_to_i64 = mlir::triton::AddPtrOp::create(\n+            builder, signal_buffers_i64.getType(), signal_buffers_i64, rank);\n         // SignalBuffers[rank]\n         // -> i64\n-        auto read_address_i64 = builder.create<mlir::triton::LoadOp>(\n+        auto read_address_i64 = mlir::triton::LoadOp::create(\n+            builder,\n             /*ptr=*/read_address_ptr_to_i64,\n             /*cache=*/mlir::triton::CacheModifier::NONE,\n             /*evict=*/mlir::triton::EvictionPolicy::NORMAL,\n             /*isVolatile=*/false);\n         // -> !tt.ptr<i32>\n-        auto read_address = builder.create<mlir::triton::IntToPtrOp>(\n-            ptr_to_i32_type, read_address_i64);\n+        auto read_address = mlir::triton::IntToPtrOp::create(\n+            builder, ptr_to_i32_type, read_address_i64);\n         // Pointer to SignalBuffers[rank][block_id]\n         // -> !tt.ptr<i32>\n-        auto read_address_at_block_offset =\n-            builder.create<mlir::triton::AddPtrOp>(ptr_to_i32_type,\n-                                                   read_address, block_offset);\n+        auto read_address_at_block_offset = mlir::triton::AddPtrOp::create(\n+            builder, ptr_to_i32_type, read_address, block_offset);\n         // -> tensor<world_size x !tt.ptr<i32>>\n         auto read_address_at_block_offset_tensor =\n-            builder.create<mlir::triton::SplatOp>(tensor_of_ptr_to_i32_type,\n-                                                  read_address_at_block_offset);\n+            mlir::triton::SplatOp::create(builder, tensor_of_ptr_to_i32_type,\n+                                          read_address_at_block_offset);\n         // SignalBuffers[rank][block_id][0..WorldSize]\n         // -> tensor<world_size x !tt.ptr<i32>>\n-        auto wait_addresses = builder.create<mlir::triton::AddPtrOp>(\n-            tensor_of_ptr_to_i32_type, read_address_at_block_offset_tensor,\n-            all_ranks);\n+        auto wait_addresses = mlir::triton::AddPtrOp::create(\n+            builder, tensor_of_ptr_to_i32_type,\n+            read_address_at_block_offset_tensor, all_ranks);\n         // Wait for all ranks on the same block id to signal.\n-        builder.create<mlir::triton::xla::AtomicSpinWaitOp>(\n+        mlir::triton::xla::AtomicSpinWaitOp::create(\n+            builder,\n             /*result_types=*/mlir::TypeRange{},\n             /*ptr=*/wait_addresses,\n             /*expected=*/signal_value,\n@@ -177,7 +180,7 @@ LogicalResult LowerBlockBarrierOp(BlockBarrierOp block_barrier,\n             /*sem=*/mlir::triton::MemSemantic::ACQUIRE,\n             /*comparator=*/Comparator::LT);\n         // Terminate the block.\n-        builder.create<mlir::scf::YieldOp>();\n+        mlir::scf::YieldOp::create(builder);\n       });\n   rewriter.eraseOp(block_barrier);\n   return success();"
        },
        {
            "sha": "0b034f0892e4e9e00524edc07f7ad67220da5680",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_get_tid_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_get_tid_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_get_tid_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_get_tid_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -45,8 +45,8 @@ LogicalResult LowerGetTidOp(GetTidOp get_flat_tid, PatternRewriter& rewriter) {\n   const absl::string_view get_tid_asm = R\"(\n     mov.u32 $0, %tid.x;\n   )\";\n-  auto tid_op = rewriter.create<mlir::triton::ElementwiseInlineAsmOp>(\n-      loc,\n+  auto tid_op = mlir::triton::ElementwiseInlineAsmOp::create(\n+      rewriter, loc,\n       /*result_types=*/i32_type,\n       /*asm_string=*/rewriter.getStringAttr(get_tid_asm),\n       /*constraints=*/rewriter.getStringAttr(\"=r\"),"
        },
        {
            "sha": "5d7a889020f1f3435f0c7ad792859a60f135f4bd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_remote_access_pass.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 31,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -54,8 +54,8 @@ LogicalResult LowerGetRankOp(GetRankOp get_rank, PatternRewriter& rewriter) {\n   }\n \n   // The rank id is stored as a first element under the metadata pointer.\n-  Value loadOp = rewriter.create<LoadOp>(\n-      get_rank.getLoc(), expected_result_type, metadata,\n+  Value loadOp = LoadOp::create(\n+      rewriter, get_rank.getLoc(), expected_result_type, metadata,\n       /*mask=*/nullptr, /*other=*/nullptr, /*boundaryCheck=*/nullptr,\n       /*padding=*/nullptr,\n       CacheModifierAttr::get(get_rank.getContext(), CacheModifier::NONE),\n@@ -95,10 +95,10 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n \n   // Size of the pointer in bytes.\n   Value pointer_size_bytes_const =\n-      builder.create<arith::ConstantIntOp>(type_i64, sizeof(int64_t));\n+      arith::ConstantIntOp::create(builder, type_i64, sizeof(int64_t));\n \n   // 1. Load metadata->rank.\n-  Value current_rank_load_op = builder.create<GetRankOp>(metadata);\n+  Value current_rank_load_op = GetRankOp::create(builder, metadata);\n \n   // 2. Calculate argument_offset = num_ranks * argument_index.\n   const int32_t argument_index = get_peer_ptr.getArgumentIndex();\n@@ -107,25 +107,25 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n       world_size * argument_index * sizeof(uint64_t);\n \n   // 3. Load metadata->param_to_peers[argument_offset + metadata->rank].\n-  Value local_buffers_ptrs_offset = builder.create<arith::ConstantIntOp>(\n-      type_i64, offsetof(CollectiveKernelMetadata, param_to_peers));\n+  Value local_buffers_ptrs_offset = arith::ConstantIntOp::create(\n+      builder, type_i64, offsetof(CollectiveKernelMetadata, param_to_peers));\n \n   Value rank_offset =\n-      builder.create<arith::ExtUIOp>(type_i64, current_rank_load_op);\n+      arith::ExtUIOp::create(builder, type_i64, current_rank_load_op);\n   Value argument_offset_bytes =\n-      builder.create<arith::ConstantIntOp>(type_i64, argument_offset);\n+      arith::ConstantIntOp::create(builder, type_i64, argument_offset);\n   Value current_rank_offset_bytes =\n-      builder.create<arith::MulIOp>(rank_offset, pointer_size_bytes_const);\n-  Value argument_ptr_offset_bytes = builder.create<arith::AddIOp>(\n-      local_buffers_ptrs_offset, argument_offset_bytes);\n-  Value current_ptr_offset_bytes = builder.create<arith::AddIOp>(\n-      argument_ptr_offset_bytes, current_rank_offset_bytes);\n+      arith::MulIOp::create(builder, rank_offset, pointer_size_bytes_const);\n+  Value argument_ptr_offset_bytes = arith::AddIOp::create(\n+      builder, local_buffers_ptrs_offset, argument_offset_bytes);\n+  Value current_ptr_offset_bytes = arith::AddIOp::create(\n+      builder, argument_ptr_offset_bytes, current_rank_offset_bytes);\n \n-  Value current_range_address = builder.create<AddPtrOp>(\n-      metadata.getType(), metadata, current_ptr_offset_bytes);\n+  Value current_range_address = AddPtrOp::create(\n+      builder, metadata.getType(), metadata, current_ptr_offset_bytes);\n \n-  Value current_range_address_value = builder.create<LoadOp>(\n-      type_i64, current_range_address,\n+  Value current_range_address_value = LoadOp::create(\n+      builder, type_i64, current_range_address,\n       /*mask=*/nullptr, /*other=*/nullptr, /*boundaryCheck=*/nullptr,\n       /*padding=*/nullptr, CacheModifierAttr::get(ctx, CacheModifier::NONE),\n       EvictionPolicyAttr::get(ctx, EvictionPolicy::NORMAL),\n@@ -134,30 +134,30 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n   // 4. Calculate offset =\n   //      address - metadata->param_to_peers[argument_offset + metadata->rank].\n   Value current_range_address_int =\n-      builder.create<PtrToIntOp>(type_i64, address);\n-  Value offsetInt = builder.create<arith::SubIOp>(current_range_address_int,\n-                                                  current_range_address_value);\n+      PtrToIntOp::create(builder, type_i64, address);\n+  Value offsetInt = arith::SubIOp::create(builder, current_range_address_int,\n+                                          current_range_address_value);\n \n   // 5. Load metadata->param_to_peers[argument_offset + peer_id].\n-  Value peer_index = builder.create<arith::ExtUIOp>(type_i64, peer_id);\n+  Value peer_index = arith::ExtUIOp::create(builder, type_i64, peer_id);\n   Value peer_index_offset_bytes =\n-      builder.create<arith::MulIOp>(peer_index, pointer_size_bytes_const);\n-  Value peer_range_offset_bytes = builder.create<arith::AddIOp>(\n-      argument_ptr_offset_bytes, peer_index_offset_bytes);\n-  Value peer_range_address = builder.create<AddPtrOp>(\n-      metadata.getType(), metadata, peer_range_offset_bytes);\n-\n-  Value peer_range_address_value = builder.create<LoadOp>(\n-      type_i64, peer_range_address,\n+      arith::MulIOp::create(builder, peer_index, pointer_size_bytes_const);\n+  Value peer_range_offset_bytes = arith::AddIOp::create(\n+      builder, argument_ptr_offset_bytes, peer_index_offset_bytes);\n+  Value peer_range_address = AddPtrOp::create(\n+      builder, metadata.getType(), metadata, peer_range_offset_bytes);\n+\n+  Value peer_range_address_value = LoadOp::create(\n+      builder, type_i64, peer_range_address,\n       /*mask=*/nullptr, /*other=*/nullptr, /*boundaryCheck=*/nullptr,\n       /*padding=*/nullptr, CacheModifierAttr::get(ctx, CacheModifier::NONE),\n       EvictionPolicyAttr::get(ctx, EvictionPolicy::NORMAL),\n       /*isVolatile=*/rewriter.getBoolAttr(false));\n \n   // 6. Calculate the result address: peerBasePtr + offset.\n   Value result_int =\n-      builder.create<arith::AddIOp>(peer_range_address_value, offsetInt);\n-  Value result_address = builder.create<IntToPtrOp>(result_type, result_int);\n+      arith::AddIOp::create(builder, peer_range_address_value, offsetInt);\n+  Value result_address = IntToPtrOp::create(builder, result_type, result_int);\n   rewriter.replaceOp(get_peer_ptr, result_address);\n   return success();\n }"
        },
        {
            "sha": "f8bf050cc3e424885af029a2a7ec97a82d614d12",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_xtile_pass.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -133,7 +133,7 @@ MemrefToPtrOp CreateMemrefToPtr(mlir::OpBuilder& builder,\n                                 mlir::TypedValue<mlir::MemRefType> memref) {\n   PointerType ptr_type = ::xla::gpu::triton::GetGlobalPointerType(\n       memref.getType().getElementType());\n-  return builder.create<MemrefToPtrOp>(memref.getLoc(), ptr_type, memref);\n+  return MemrefToPtrOp::create(builder, memref.getLoc(), ptr_type, memref);\n }\n \n // Rewrite a xtile entry to a func.func with the same body, but with memref\n@@ -152,8 +152,9 @@ class XTileEntryToTriton\n \n     const int64_t num_buffer_args = entry_op.getBufferArgs().size();\n     auto new_arg_types = GetTransformedArgTypes(entry_op);\n-    auto new_func_op = builder.create<mlir::func::FuncOp>(\n-        entry_op.getName(), builder.getFunctionType(new_arg_types, {}));\n+    auto new_func_op =\n+        mlir::func::FuncOp::create(builder, entry_op.getName(),\n+                                   builder.getFunctionType(new_arg_types, {}));\n \n     // Move the old function's body to the new function\n     rewriter.inlineRegionBefore(\n@@ -169,9 +170,9 @@ class XTileEntryToTriton\n \n     BlockArgument tile_id_arg = old_args.back();\n \n-    auto pid = builder.create<ttir::GetProgramIdOp>(ttir::ProgramIDDim::X);\n+    auto pid = ttir::GetProgramIdOp::create(builder, ttir::ProgramIDDim::X);\n     Value pid_idx =\n-        builder.create<ma::IndexCastOp>(builder.getIndexType(), pid);\n+        ma::IndexCastOp::create(builder, builder.getIndexType(), pid);\n     rewriter.replaceAllUsesWith(tile_id_arg, pid_idx);\n \n     // Handle memref arguments.\n@@ -182,7 +183,7 @@ class XTileEntryToTriton\n           mlir::cast<mlir::MemRefType>(old_arg.getType());\n \n       mlir::Value memref_cast =\n-          builder.create<PtrToMemrefOp>(memref_type, new_arg);\n+          PtrToMemrefOp::create(builder, memref_type, new_arg);\n \n       // Replace all uses of the old argument with the result of the cast.\n       rewriter.replaceAllUsesWith(old_arg, memref_cast);\n@@ -223,9 +224,10 @@ class XTileExtractToTriton\n         CreateMemrefToPtr(rewriter, extract_op.getSource());\n \n     if (result_type.getRank() == 0) {\n-      mlir::Value scalar_value = rewriter.create<ttir::LoadOp>(\n-          extract_op->getLoc(), memref_to_ptr, ttir::CacheModifier::NONE,\n-          ttir::EvictionPolicy::NORMAL, /*isVolatile=*/false);\n+      mlir::Value scalar_value = ttir::LoadOp::create(\n+          rewriter, extract_op->getLoc(), memref_to_ptr,\n+          ttir::CacheModifier::NONE, ttir::EvictionPolicy::NORMAL,\n+          /*isVolatile=*/false);\n \n       rewriter.replaceOpWithNewOp<mlir::tensor::FromElementsOp>(\n           extract_op, result_type, scalar_value);\n@@ -239,8 +241,8 @@ class XTileExtractToTriton\n                                          minor_to_major_or.status().ToString());\n     }\n     const SmallVector<int64_t>& minor_to_major = *minor_to_major_or;\n-    auto triton_extract_op = rewriter.create<ExtractOp>(\n-        extract_op.getLoc(), result_type, memref_to_ptr,\n+    auto triton_extract_op = ExtractOp::create(\n+        rewriter, extract_op.getLoc(), result_type, memref_to_ptr,\n         extract_op.getOffsets(), extract_op.getFullTileShape(),\n         extract_op.getStrides(), source_type.getShape(), minor_to_major);\n \n@@ -280,8 +282,8 @@ class XTileInsertToTriton\n                                          minor_to_major_or.status().ToString());\n     }\n     const SmallVector<int64_t>& minor_to_major = *minor_to_major_or;\n-    auto triton_insert_op = rewriter.create<InsertOp>(\n-        insert_op.getLoc(), insert_op.getSource(), memref_to_ptr,\n+    auto triton_insert_op = InsertOp::create(\n+        rewriter, insert_op.getLoc(), insert_op.getSource(), memref_to_ptr,\n         insert_op.getOffsets(), insert_op.getFullTileShape(),\n         insert_op.getStrides(), destination_type.getShape(), minor_to_major);\n "
        },
        {
            "sha": "5b3046b489f757f0d55b167b46f98d74ef6e64e1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_squeeze_dims_pass.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -91,8 +91,8 @@ void ReplaceOpWithExpandDimsOf(PatternRewriter& rewriter, Operation* op,\n     }\n     // If any users remain, replace the op with expand_dims.\n     if (!result.use_empty()) {\n-      Value expand_dims = rewriter.create<ExpandDimsOp>(\n-          op->getLoc(), result.getType(), value, axis);\n+      Value expand_dims = ExpandDimsOp::create(rewriter, op->getLoc(),\n+                                               result.getType(), value, axis);\n       rewriter.replaceAllUsesWith(result, expand_dims);\n     }\n   }\n@@ -146,7 +146,7 @@ Value SqueezeTensorValue(PatternRewriter& rewriter, Value value,\n   for (uint32_t i = 0; i < squeeze_dims.size(); ++i) {\n     uint32_t dim = squeeze_dims[i] - i;\n     Type type = SqueezeTensorType(cast<RankedTensorType>(value.getType()), dim);\n-    value = rewriter.create<SqueezeDimsOp>(value.getLoc(), type, value, dim);\n+    value = SqueezeDimsOp::create(rewriter, value.getLoc(), type, value, dim);\n   }\n   return value;\n }\n@@ -160,8 +160,8 @@ LogicalResult FoldSqueezeDimsOfExtractTile(::xla::xtile::ExtractTileOp op,\n \n   auto squeezed_type = SqueezeTensorType(op.getType(), *axis);\n \n-  Value new_op = rewriter.create<::xla::xtile::ExtractTileOp>(\n-      op.getLoc(), squeezed_type, op.getSource(), op.getOffsets(),\n+  Value new_op = ::xla::xtile::ExtractTileOp::create(\n+      rewriter, op.getLoc(), squeezed_type, op.getSource(), op.getOffsets(),\n       op.getFullTileShape(), op.getStrides());\n   ReplaceOpWithExpandDimsOf(rewriter, op, new_op, *axis);\n   rewriter.eraseOp(op);\n@@ -213,11 +213,12 @@ LogicalResult ExpandReshapeResult(ReshapeOp op, PatternRewriter& rewriter) {\n     return rewriter.notifyMatchFailure(op, \"No unit dimensions.\");\n   }\n \n-  Value result = rewriter.create<ReshapeOp>(\n-      op.getLoc(), SqueezeTensorType(op.getType(), expand_dims), op.getSrc());\n+  Value result = ReshapeOp::create(rewriter, op.getLoc(),\n+                                   SqueezeTensorType(op.getType(), expand_dims),\n+                                   op.getSrc());\n   for (int32_t i = expand_dims.size() - 1; i >= 0; --i) {\n     uint32_t dim = expand_dims[i] - i;\n-    result = rewriter.create<ExpandDimsOp>(op.getLoc(), result, dim);\n+    result = ExpandDimsOp::create(rewriter, op.getLoc(), result, dim);\n   }\n   rewriter.replaceOp(op, result);\n   return success();\n@@ -277,7 +278,7 @@ LogicalResult PushSqueezeDimsUpThroughBroadcast(SqueezeDimsOp op,\n   OpBuilder::InsertionGuard guard = SetInsertionPoint(rewriter, broadcast);\n   Value value = SqueezeTensorValue(rewriter, broadcast.getSrc(), op.getAxis());\n   Value new_broadcast =\n-      rewriter.create<BroadcastOp>(broadcast.getLoc(), op.getType(), value);\n+      BroadcastOp::create(rewriter, broadcast.getLoc(), op.getType(), value);\n   ReplaceOpWithExpandDimsOf(rewriter, broadcast, new_broadcast, op.getAxis());\n   return success();\n }\n@@ -308,7 +309,7 @@ LogicalResult PushSqueezeDimsUpThroughTrans(SqueezeDimsOp op,\n \n   OpBuilder::InsertionGuard guard = SetInsertionPoint(rewriter, trans);\n   Value value = SqueezeTensorValue(rewriter, trans.getSrc(), src_axis);\n-  Value new_trans = rewriter.create<TransOp>(trans.getLoc(), value, new_order);\n+  Value new_trans = TransOp::create(rewriter, trans.getLoc(), value, new_order);\n   ReplaceOpWithExpandDimsOf(rewriter, trans, new_trans, dst_axis);\n   return success();\n }\n@@ -336,7 +337,7 @@ LogicalResult PushSqueezeDimsUpThroughJoin(SqueezeDimsOp op,\n   }\n \n   Value new_join =\n-      rewriter.create<JoinOp>(join.getLoc(), op.getType(), operands);\n+      JoinOp::create(rewriter, join.getLoc(), op.getType(), operands);\n   ReplaceOpWithExpandDimsOf(rewriter, join, new_join, op.getAxis());\n   return success();\n }\n@@ -366,8 +367,8 @@ LogicalResult PushSqueezeDimsUpThroughReduce(SqueezeDimsOp op,\n     operands.push_back(SqueezeTensorValue(rewriter, operand, squeeze_axis));\n   }\n \n-  auto new_reduce = rewriter.create<ReduceOp>(reduce.getLoc(), op.getType(),\n-                                              operands, reduce_axis);\n+  auto new_reduce = ReduceOp::create(rewriter, reduce.getLoc(), op.getType(),\n+                                     operands, reduce_axis);\n   rewriter.cloneRegionBefore(reduce->getRegion(0), new_reduce->getRegion(0),\n                              new_reduce->getRegion(0).begin());\n   ReplaceOpWithExpandDimsOf(rewriter, reduce, new_reduce->getResult(0),\n@@ -440,9 +441,10 @@ LogicalResult PushSqueezeDimsUpIntoIf(SqueezeDimsOp op,\n   auto new_types = llvm::to_vector(if_op.getResultTypes());\n   new_types[result_number] = op.getType();\n \n-  auto new_if_op = rewriter.create<scf::IfOp>(\n-      op.getLoc(), new_types, if_op.getCondition(), /*addThenBlock=*/false,\n-      /*addElseBlock=*/false);\n+  auto new_if_op =\n+      scf::IfOp::create(rewriter, op.getLoc(), new_types, if_op.getCondition(),\n+                        /*addThenBlock=*/false,\n+                        /*addElseBlock=*/false);\n \n   // Update then and else regions.\n   for (auto [old_region, new_region] :\n@@ -453,9 +455,9 @@ LogicalResult PushSqueezeDimsUpIntoIf(SqueezeDimsOp op,\n     }\n     auto yield_op = new_region->front().getTerminator();\n     OpBuilder::InsertionGuard guard = SetInsertionPoint(rewriter, yield_op);\n-    auto squeeze_op = rewriter.create<SqueezeDimsOp>(\n-        op.getLoc(), op.getType(), yield_op->getOperand(result_number),\n-        op.getAxis());\n+    auto squeeze_op = SqueezeDimsOp::create(rewriter, op.getLoc(), op.getType(),\n+                                            yield_op->getOperand(result_number),\n+                                            op.getAxis());\n     yield_op->setOperand(result_number, squeeze_op);\n   }\n   rewriter.replaceOp(op, new_if_op.getResult(result_number));"
        },
        {
            "sha": "92a3116c0ad7caa590fcf04e782a9b9bb693f83b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_unswitch_loops_pass.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_unswitch_loops_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c82ee8e5355dc601efe77af61382e6904c18cf84/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_unswitch_loops_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_unswitch_loops_pass.cc?ref=c82ee8e5355dc601efe77af61382e6904c18cf84",
            "patch": "@@ -80,13 +80,13 @@ LogicalResult UnswitchLoop(mlir::scf::ForOp for_op,\n   for (int body_index : {0, 1}) {\n     auto builder = OpBuilder::atBlockEnd(new_if.getBody(body_index),\n                                          rewriter.getListener());\n-    arith::ConstantOp condition = builder.create<arith::ConstantOp>(\n-        for_op.getLoc(),\n+    arith::ConstantOp condition = arith::ConstantOp::create(\n+        builder, for_op.getLoc(),\n         rewriter.getIntegerAttr(rewriter.getI1Type(), body_index == 0));\n     IRMapping mapping;\n     mapping.map(if_op.getCondition(), condition);\n     Operation* new_for = builder.clone(*for_op, mapping);\n-    builder.create<scf::YieldOp>(for_op.getLoc(), new_for->getResults());\n+    scf::YieldOp::create(builder, for_op.getLoc(), new_for->getResults());\n   }\n   rewriter.replaceOp(for_op, new_if);\n   return success();"
        }
    ],
    "stats": {
        "total": 1196,
        "additions": 617,
        "deletions": 579
    }
}