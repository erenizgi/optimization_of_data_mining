{
    "author": "akuegel",
    "message": "Remove buffer sharing logic related to GPU SliceInput fusions\n\nThese fusions were created by the horizontal fusion passes. Those have been\ndeleted already.\n\nPiperOrigin-RevId: 829347828",
    "sha": "8640317764ff90922900a820c03554d36200cca7",
    "files": [
        {
            "sha": "d496939370968eccd2e0f2d44a11d15b8857212e",
            "filename": "third_party/xla/xla/hlo/analysis/hlo_dataflow_analysis.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 181,
            "changes": 181,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8640317764ff90922900a820c03554d36200cca7/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8640317764ff90922900a820c03554d36200cca7/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis.cc?ref=8640317764ff90922900a820c03554d36200cca7",
            "patch": "@@ -136,175 +136,6 @@ bool HloDataflowAnalysis::AreTransitiveUsesElementwiseOrTuple(\n   return true;\n }\n \n-namespace {\n-bool Is1dSliceWithoutStrides(const HloInstruction* instr) {\n-  return instr->opcode() == HloOpcode::kSlice &&\n-         1 == instr->slice_starts().size() &&\n-         1 == instr->slice_limits().size() &&\n-         1 == instr->slice_strides().size() &&\n-         1 == instr->slice_strides().at(0);\n-}\n-\n-bool IsSliceInputFusion(const HloInstruction& unnested_hlo) {\n-  if (!unnested_hlo.IsInputFusion()) {\n-    return false;\n-  }\n-  const HloInstruction* root = unnested_hlo.fused_expression_root();\n-  if (root->opcode() != HloOpcode::kTuple) {\n-    return false;\n-  }\n-  return absl::c_all_of(root->operands(), [](const HloInstruction* instr) {\n-    return Is1dSliceWithoutStrides(instr);\n-  });\n-}\n-\n-struct ConcatUsageInfo {\n-  // Pointer to a previously seen concat. nullptr if no previously seen concat.\n-  const HloInstruction* prev_concat;\n-  // The opnd id of the seen concat.\n-  int64_t concat_opnd_idx;\n-  // The slice that recovers the opnd in the concat outputs.\n-  const HloInstruction* slice_to_recover_opnd;\n-};\n-\n-// Returns an optional concat usage info to denote whether the concat is used in\n-// an elementwise manner. A concat followed by slices is considered effectively\n-// elementwise if the slices combinedly is a reverse function of the concat.\n-std::optional<ConcatUsageInfo> ConcatIsEffectivelyElementwise(\n-    const HloInstruction& concat, const HloInstruction& operand,\n-    const ConcatUsageInfo& info) {\n-  // First, check if this concat is in the below pattern. Also, we check\n-  // that the slices combinedly are in effect a reverse function of the concat.\n-  //\n-  //     Concat\n-  //     |    |\n-  //     v    v\n-  //   Slice Slice\n-  //\n-  std::vector<HloInstruction*> users = concat.users();\n-  if (!absl::c_all_of(users, Is1dSliceWithoutStrides)) {\n-    // Limit our supported cases to 1 dimensional slices.\n-    return std::optional<ConcatUsageInfo>();\n-  }\n-  // Verify that each operand to the concat is reversed by a slice.\n-  if (users.size() != concat.operand_count() ||\n-      concat.operand_count() != concat.unique_operands().size()) {\n-    return std::optional<ConcatUsageInfo>();\n-  }\n-  absl::c_sort(users, [](const HloInstruction* a, const HloInstruction* b) {\n-    return a->slice_starts().at(0) < b->slice_starts().at(0);\n-  });\n-  int64_t prev_limit = 0;\n-  for (int64_t i = 0; i < users.size(); ++i) {\n-    const HloInstruction* u = users[i];\n-    int64_t slice_size = u->slice_limits().at(0) - u->slice_starts().at(0);\n-    if (u->slice_starts().at(0) != prev_limit ||\n-        slice_size != ShapeUtil::ElementsIn(concat.operand(i)->shape())) {\n-      return std::optional<ConcatUsageInfo>();\n-    }\n-    prev_limit = u->slice_limits().at(0);\n-  }\n-\n-  // If we have seen other concats, make sure they are identical. Multiple\n-  // concats exist because horizontal fusion inserts one concat for each output\n-  // of the fusion candidates. Check that all concats and operand ids are the\n-  // same to know that the \"transitive use closure\" will be computed in the same\n-  // iteration space.\n-  int64_t operand_idx = concat.operand_index(&operand);\n-  if (info.prev_concat != nullptr) {\n-    bool is_concat_identical = info.prev_concat->Identical(\n-        concat,\n-        /*eq_operands=*/[](const HloInstruction*, const HloInstruction*) {\n-          // Operands don't need to be the same.\n-          return true;\n-        });\n-    if (!is_concat_identical || info.concat_opnd_idx != operand_idx) {\n-      return std::optional<ConcatUsageInfo>();\n-    }\n-  }\n-\n-  const HloInstruction* slice_to_recover_opnd = users.at(operand_idx);\n-  return std::optional<ConcatUsageInfo>(\n-      ConcatUsageInfo{&concat, operand_idx, slice_to_recover_opnd});\n-}\n-\n-// Returns whether we can prove the transitive uses of `param` are in effect\n-// elementwise. In other words, we prove that the \"transitive use closure\" will\n-// all be computed in the same iteration space without any reorder of elements.\n-// In addition, we check that the \"transitive use closure\" includes the output\n-// in the `root_tuple`.\n-// Theoretically, We can prove more patterns but our primary use case is\n-// SliceInputFusion.\n-bool AreTransitiveUsesEffectivelyElementwise(const HloInstruction* param,\n-                                             const HloInstruction* root_tuple,\n-                                             const ShapeIndex& out_shape_idx) {\n-  CHECK_EQ(root_tuple->opcode(), HloOpcode::kTuple);\n-  CHECK_EQ(out_shape_idx.size(), 1);\n-  absl::flat_hash_set<const HloInstruction*> visited;\n-  absl::InlinedVector<const HloInstruction*, 4> stack;\n-  stack.push_back(param);\n-  ConcatUsageInfo concat_usage_info{nullptr, 0, nullptr};\n-  bool is_output_reachable = false;\n-  while (!stack.empty()) {\n-    const HloInstruction* current = stack.back();\n-    stack.pop_back();\n-    visited.insert(current);\n-    for (const HloInstruction* user : current->users()) {\n-      VLOG(3) << \"Visiting: \" << user->ToString();\n-      switch (user->opcode()) {\n-        case HloOpcode::kTuple:\n-          if (user == root_tuple &&\n-              current == root_tuple->operand(out_shape_idx.back())) {\n-            // We need to know if the output is reachable by the `param` to make\n-            // sure that they will be computed in the same iteration space.\n-            is_output_reachable = true;\n-          }\n-          break;\n-        case HloOpcode::kReshape:\n-          if (!ShapeUtil::ReshapeIsBitcast(current->shape(), user->shape())) {\n-            return false;\n-          }\n-          break;\n-        case HloOpcode::kConcatenate: {\n-          std::optional<ConcatUsageInfo> optional_concat_info =\n-              ConcatIsEffectivelyElementwise(*user, *current,\n-                                             concat_usage_info);\n-          if (!optional_concat_info) {\n-            return false;\n-          }\n-          concat_usage_info = *optional_concat_info;\n-          // Early continue as we only want to traverse through the slice that\n-          // recovers the operand. It is guaranteed that the operand to the\n-          // concat and the slice have the same iteration space. Insert the\n-          // slice instead of the concat.\n-          CHECK(!visited.contains(concat_usage_info.slice_to_recover_opnd));\n-          stack.push_back(concat_usage_info.slice_to_recover_opnd);\n-          continue;\n-        }\n-        default:\n-          for (const int64_t use_index : user->OperandIndices(current)) {\n-            if (!user->IsElementwiseOnOperand(use_index)) {\n-              // Found a user that is non-elementwise on the current\n-              // instruction.\n-              return false;\n-            }\n-          }\n-          if (!LayoutUtil::Equal(current->shape().layout(),\n-                                 user->shape().layout())) {\n-            // Make sure the layout is not changed by the elementwise op.\n-            return false;\n-          }\n-          break;\n-      }  // end of switch\n-      if (!visited.contains(user)) {\n-        stack.push_back(user);\n-      }\n-    }\n-  }\n-  return is_output_reachable;\n-}\n-}  // namespace\n-\n bool HloDataflowAnalysis::ValueIsDefinedAt(const HloInstruction* instruction,\n                                            const ShapeIndex& index) const {\n   const HloValueSet& value_set = GetValueSet(instruction, index);\n@@ -1800,18 +1631,6 @@ bool HloDataflowAnalysis::CanShareOperandBufferWithUser(\n       ShapeUtil::GetSubshape(operand->shape(), operand_index);\n   const Shape& user_subshape =\n       ShapeUtil::GetSubshape(user->shape(), user_index);\n-  if (IsSliceInputFusion(*user)) {\n-    HloInstruction* fusion_param =\n-        user->fused_parameter(user->operand_index(operand));\n-    // We don't require the same dimensions but only the same number of elements\n-    // and type (to make sure the same buffer size).\n-    return operand_subshape.IsArray() && user_subshape.IsArray() &&\n-           ShapeUtil::ElementsIn(operand_subshape) ==\n-               ShapeUtil::ElementsIn(user_subshape) &&\n-           ShapeUtil::SameElementType(operand_subshape, user_subshape) &&\n-           AreTransitiveUsesEffectivelyElementwise(\n-               fusion_param, user->fused_expression_root(), user_index);\n-  }\n \n   auto shapes_equal = ShapeUtil::Equal(operand_subshape, user_subshape);\n   // Check that operand and user emit the same shape and layout."
        },
        {
            "sha": "755e184eb7f7aa7f554b1dcbd257dced131b5932",
            "filename": "third_party/xla/xla/hlo/analysis/hlo_dataflow_analysis_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 145,
            "changes": 145,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8640317764ff90922900a820c03554d36200cca7/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8640317764ff90922900a820c03554d36200cca7/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis_test.cc?ref=8640317764ff90922900a820c03554d36200cca7",
            "patch": "@@ -3079,151 +3079,6 @@ TEST_F(CanShareOperandBufferWithUserTest, CallToComputationWithFusionRoot) {\n       reverse, {}, call, {}, &alias_info_));\n }\n \n-TEST_F(CanShareOperandBufferWithUserTest, ConcatSliceWithElementwise) {\n-  const char* kModule = R\"(\n-    HloModule test\n-\n-    fused_computation {\n-      p0 = f32[10,20] parameter(0)\n-      p1 = f32[10,20] parameter(1)\n-      p2 = f32[10,10] parameter(2)\n-      p3 = f32[10,10] parameter(3)\n-      add0 = f32[10, 20] add(p0, p1)\n-      sub0 = f32[10, 10] subtract(p2, p3)\n-      reshape0 = f32[200] reshape(add0)\n-      reshape1 = f32[100] reshape(sub0)\n-      concat0 = f32[300] concatenate(reshape0, reshape1), dimensions={0}\n-      slice0 = f32[200] slice(concat0), slice={[0:200]}\n-      slice1 = f32[100] slice(concat0), slice={[200:300]}\n-      ROOT tuple = (f32[200], f32[100]) tuple(slice0, slice1)\n-    }\n-\n-    ENTRY test {\n-      p0 = f32[10,20] parameter(0)\n-      p1 = f32[10,20] parameter(1)\n-      p2 = f32[10,10] parameter(2)\n-      p3 = f32[10,10] parameter(3)\n-      ROOT fusion = (f32[200], f32[100]) fusion(p0, p1, p2, p3), kind=kInput, calls=fused_computation\n-    }\n-  )\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n-  auto* fusion = module->entry_computation()->root_instruction();\n-  auto* param0 = module->entry_computation()->parameter_instruction(0);\n-  auto* param1 = module->entry_computation()->parameter_instruction(1);\n-  auto* param2 = module->entry_computation()->parameter_instruction(2);\n-  auto* param3 = module->entry_computation()->parameter_instruction(3);\n-\n-  auto dataflow_analysis = RunAnalysis(*module);\n-  EXPECT_TRUE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {0}, &alias_info_));\n-  EXPECT_TRUE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param1, {}, fusion, {0}, &alias_info_));\n-  EXPECT_TRUE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param2, {}, fusion, {1}, &alias_info_));\n-  EXPECT_TRUE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param3, {}, fusion, {1}, &alias_info_));\n-  // Tensors of different sizes cannot share buffer.\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {1}, &alias_info_));\n-}\n-\n-TEST_F(CanShareOperandBufferWithUserTest, ConcatSliceNegativeTest) {\n-  const char* kModule = R\"(\n-    HloModule test\n-\n-    fused_computation {\n-      // p0 has multiple transitive uses fed to concat. So, p0 cannot share\n-      // buffer with outputs because the aliased output could be written before\n-      // all the uses of p0 are finished.\n-      p0 = f32[100] parameter(0)\n-      p1 = f32[100] parameter(1)\n-      add0 = f32[100] add(p0, p1)\n-      concat0 = f32[200] concatenate(p0, add0), dimensions={0}\n-      slice0 = f32[100] slice(concat0), slice={[0:100]}\n-      slice1 = f32[100] slice(concat0), slice={[100:200]}\n-      ROOT tuple = (f32[100], f32[100]) tuple(slice0, slice1)\n-    }\n-\n-    ENTRY test {\n-      p0 = f32[100] parameter(0)\n-      p1 = f32[100] parameter(1)\n-      ROOT fusion = (f32[100], f32[100]) fusion(p0, p1),\n-                        kind=kInput, calls=fused_computation\n-    }\n-  )\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n-  auto* fusion = module->entry_computation()->root_instruction();\n-  auto* param0 = module->entry_computation()->parameter_instruction(0);\n-  auto* param1 = module->entry_computation()->parameter_instruction(1);\n-\n-  auto dataflow_analysis = RunAnalysis(*module);\n-  // p0 cannot share with either fusion{0} or fusion{1}.\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {0}, &alias_info_));\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {1}, &alias_info_));\n-  // p1 cannot share with fusion{0} because we're not sure about their\n-  // relationship.\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param1, {}, fusion, {0}, &alias_info_));\n-  // p1 can share with fusion{1} because they will be executed in an\n-  // elementwise manner.\n-  EXPECT_TRUE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param1, {}, fusion, {1}, &alias_info_));\n-}\n-\n-TEST_F(CanShareOperandBufferWithUserTest, MultipleConcatenates) {\n-  const char* kModule = R\"(\n-    HloModule test\n-\n-    fused_computation {\n-      p0 = f32[100] parameter(0)\n-      p1 = f32[100] parameter(1)\n-      add0 = f32[100] add(p0, p1)\n-      sub0 = f32[100] subtract(p1, p1)\n-      concat0 = f32[200] concatenate(p0, add0), dimensions={0}\n-      slice0 = f32[100] slice(concat0), slice={[0:100]}\n-      slice1 = f32[100] slice(concat0), slice={[100:200]}\n-      concat1 = f32[200] concatenate(p0, sub0), dimensions={0}\n-      slice2 = f32[100] slice(concat1), slice={[0:100]}\n-      slice3 = f32[100] slice(concat1), slice={[100:200]}\n-      ROOT tuple = (f32[100], f32[100], f32[100], f32[100])\n-                       tuple(slice0, slice1, slice2, slice3)\n-    }\n-\n-    ENTRY test {\n-      p0 = f32[100] parameter(0)\n-      p1 = f32[100] parameter(1)\n-      ROOT fusion = (f32[100], f32[100], f32[100], f32[100])\n-          fusion(p0, p1), kind=kInput, calls=fused_computation\n-    }\n-  )\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n-  auto* fusion = module->entry_computation()->root_instruction();\n-  auto* param0 = module->entry_computation()->parameter_instruction(0);\n-  auto* param1 = module->entry_computation()->parameter_instruction(1);\n-\n-  auto dataflow_analysis = RunAnalysis(*module);\n-  // p0 cannot share.\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {0}, &alias_info_));\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {1}, &alias_info_));\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {2}, &alias_info_));\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param0, {}, fusion, {3}, &alias_info_));\n-  // p1 can share with either fusion{1} or fusion{3}.\n-  EXPECT_TRUE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param1, {}, fusion, {1}, &alias_info_));\n-  EXPECT_TRUE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param1, {}, fusion, {3}, &alias_info_));\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param1, {}, fusion, {0}, &alias_info_));\n-  EXPECT_FALSE(dataflow_analysis->CanShareOperandBufferWithUser(\n-      param1, {}, fusion, {2}, &alias_info_));\n-}\n-\n using GetInPlaceInputOutputPairsTest = HloHardwareIndependentTestBase;\n \n TEST_F(GetInPlaceInputOutputPairsTest, DUS) {"
        },
        {
            "sha": "728755427add1486aec42c57ab5648392fca544e",
            "filename": "third_party/xla/xla/service/copy_insertion_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8640317764ff90922900a820c03554d36200cca7/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8640317764ff90922900a820c03554d36200cca7/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcopy_insertion_test.cc?ref=8640317764ff90922900a820c03554d36200cca7",
            "patch": "@@ -2923,56 +2923,6 @@ ENTRY main {\n                   op::Fusion(), op::Iota(), op::Iota(), op::Iota()));\n }\n \n-TEST_F(CopyInsertionTest, HorizontalLoopFusionNoCopy) {\n-  const std::string& hlo_string = R\"(\n-    HloModule test\n-\n-    fused_computation {\n-      p0 = f32[10,20] parameter(0)\n-      p1 = f32[10,20] parameter(1)\n-      p2 = f32[10,10] parameter(2)\n-      p3 = f32[10,10] parameter(3)\n-      add0 = f32[10, 20] add(p0, p1)\n-      sub0 = f32[10, 10] subtract(p2, p3)\n-      reshape0 = f32[200] reshape(add0)\n-      reshape1 = f32[100] reshape(sub0)\n-      concat0 = f32[300] concatenate(reshape0, reshape1), dimensions={0}\n-      slice0 = f32[200] slice(concat0), slice={[0:200]}\n-      slice1 = f32[100] slice(concat0), slice={[200:300]}\n-      ROOT tuple = (f32[200], f32[100]) tuple(slice0, slice1)\n-    }\n-\n-    ENTRY test {\n-      p0 = f32[10,20] parameter(0)\n-      p1 = f32[10,20] parameter(1)\n-      p2 = f32[10,10] parameter(2)\n-      p3 = f32[10,10] parameter(3)\n-      fusion = (f32[200], f32[100]) fusion(p0, p1, p2, p3), kind=kInput, calls=fused_computation\n-      gte0 = f32[200] get-tuple-element(fusion), index=0\n-      gte1 = f32[100] get-tuple-element(fusion), index=1\n-      bitcast0 = f32[10,20] bitcast(gte0)\n-      bitcast1 = f32[10,10] bitcast(gte1)\n-      ROOT tuple = (f32[10,20], f32[10,10]) tuple(bitcast0, bitcast1)\n-    }\n-  )\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  ASSERT_IS_OK(module->input_output_alias_config().SetUpAlias(\n-      /*output_index=*/{0},\n-      /*param_number=*/0,\n-      /*param_index=*/{}));\n-  ASSERT_IS_OK(module->input_output_alias_config().SetUpAlias(\n-      /*output_index=*/{1},\n-      /*param_number=*/3,\n-      /*param_index=*/{}));\n-\n-  InsertCopies(module.get());\n-\n-  // There should be no copies inserted.\n-  EXPECT_EQ(CountCopies(*module), 0);\n-}\n-\n TEST_F(CopyInsertionTest, NestedWhileAndConditional3) {\n   const std::string& hlo_string = R\"(\n HloModule TestModule"
        }
    ],
    "stats": {
        "total": 376,
        "additions": 0,
        "deletions": 376
    }
}