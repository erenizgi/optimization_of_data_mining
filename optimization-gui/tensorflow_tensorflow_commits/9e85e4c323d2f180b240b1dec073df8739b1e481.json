{
    "author": "beckerhe",
    "message": "Fix FP8 data type handling for cublasLt gemms\n\nA lot of our GEMM code paths assume that LHS and RHS need to be the same for cublas(Lt)\ngemms. But that's not true anymore with FP8 data types on cublasLt. In fact\nF8E5M2 x F8E5M2 is not supported while F8E5M2 x F8E4M3FN is.\n\nThe details are here: https://docs.nvidia.com/cuda/cublas/#cublasltmatmul\n\nSo this change cleans up `AlgorithmUtil` and `AlgorithmChecker` so that they\nhandle the mixed FP8 datatype matmuls in cublasLt correctly.\n\nPiperOrigin-RevId: 807591260",
    "sha": "9e85e4c323d2f180b240b1dec073df8739b1e481",
    "files": [
        {
            "sha": "0441fd2167d2b8b14ec73962b80ee680e84a2d29",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=9e85e4c323d2f180b240b1dec073df8739b1e481",
            "patch": "@@ -6053,6 +6053,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\","
        },
        {
            "sha": "9af1b73358ebcb04f2563649c48c086ca91dcd02",
            "filename": "third_party/xla/xla/service/algorithm_util.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 13,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.cc?ref=9e85e4c323d2f180b240b1dec073df8739b1e481",
            "patch": "@@ -15,14 +15,17 @@ limitations under the License.\n \n #include \"xla/service/algorithm_util.h\"\n \n+#include <cstdint>\n #include <variant>\n #include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/stream_executor/blas.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/protobuf.h\"\n@@ -216,7 +219,8 @@ bool IsSupportedByElementalIrEmitter(PrecisionConfig::Algorithm algorithm) {\n bool IsSupportedDotAlgorithmOnGpu(\n     PrecisionConfig::Algorithm algorithm,\n     const stream_executor::GpuComputeCapability& gpu_compute_capability,\n-    PrimitiveType input_storage_type, PrimitiveType output_storage_type) {\n+    PrimitiveType lhs_storage_type, PrimitiveType rhs_storage_type,\n+    PrimitiveType output_storage_type) {\n   // Note: We may want to add some complex types here if people request that.\n   const bool is_cuda_ge_ampere =\n       std::holds_alternative<se::CudaComputeCapability>(\n@@ -245,18 +249,35 @@ bool IsSupportedDotAlgorithmOnGpu(\n   switch (algorithm) {\n     case PrecisionConfig::ALG_DOT_ANY_F8_ANY_F8_F32:\n     case PrecisionConfig::ALG_DOT_ANY_F8_ANY_F8_F32_FAST_ACCUM:\n+      if (!is_cuda_ge_ada && !is_rocm_mi100_and_above) {\n+        return false;\n+      }\n+      if (output_storage_type != BF16 && output_storage_type != F16 &&\n+          output_storage_type != F32 && output_storage_type != F8E4M3FN &&\n+          output_storage_type != F8E5M2) {\n+        return false;\n+      }\n       // Other F8 types are actually not supported by NVIDIA GPUs.\n-      return (is_cuda_ge_ada || is_rocm_mi100_and_above) &&\n-             (input_storage_type == F8E5M2 || input_storage_type == F8E4M3FN) &&\n-             (output_storage_type == F8E5M2 ||\n-              output_storage_type == F8E4M3FN || output_storage_type == F16 ||\n-              output_storage_type == BF16 || output_storage_type == F32);\n+      // Reference: https://docs.nvidia.com/cuda/cublas/#cublasltmatmul\n+      if (lhs_storage_type == F8E5M2 && rhs_storage_type == F8E4M3FN) {\n+        return true;\n+      }\n+      if (lhs_storage_type == F8E4M3FN &&\n+          (rhs_storage_type == F8E5M2 || rhs_storage_type == F8E4M3FN)) {\n+        return true;\n+      }\n+      return false;\n     case PrecisionConfig::ALG_DOT_F16_F16_F32:\n-      return input_storage_type == F16 &&\n+      return lhs_storage_type == rhs_storage_type && lhs_storage_type == F16 &&\n              (output_storage_type == F16 || output_storage_type == F32);\n     case PrecisionConfig::ALG_DOT_BF16_BF16_F32:\n-      if (!is_cuda_ge_ampere && !is_rocm_bf16) return false;\n-      switch (input_storage_type) {\n+      if (!is_cuda_ge_ampere && !is_rocm_bf16) {\n+        return false;\n+      }\n+      if (lhs_storage_type != rhs_storage_type) {\n+        return false;\n+      }\n+      switch (lhs_storage_type) {\n         case BF16:\n           return output_storage_type == BF16 || output_storage_type == F32;\n         case F32:\n@@ -267,16 +288,20 @@ bool IsSupportedDotAlgorithmOnGpu(\n     case PrecisionConfig::ALG_DOT_BF16_BF16_F32_X3:\n     case PrecisionConfig::ALG_DOT_BF16_BF16_F32_X6:\n     case PrecisionConfig::ALG_DOT_BF16_BF16_F32_X9:\n-      return (is_cuda_ge_ampere || is_rocm_bf16) && input_storage_type == F32 &&\n+      return (is_cuda_ge_ampere || is_rocm_bf16) &&\n+             lhs_storage_type == rhs_storage_type && lhs_storage_type == F32 &&\n              output_storage_type == F32;\n     case PrecisionConfig::ALG_DOT_TF32_TF32_F32_X3:\n     case PrecisionConfig::ALG_DOT_TF32_TF32_F32:\n       return (is_cuda_ge_ampere || is_rocm_mi100_and_above) &&\n-             input_storage_type == F32 && output_storage_type == F32;\n+             lhs_storage_type == rhs_storage_type && lhs_storage_type == F32 &&\n+             output_storage_type == F32;\n     case PrecisionConfig::ALG_DOT_F32_F32_F32:\n-      return input_storage_type == F32 && output_storage_type == F32;\n+      return lhs_storage_type == rhs_storage_type && lhs_storage_type == F32 &&\n+             output_storage_type == F32;\n     case PrecisionConfig::ALG_DOT_F64_F64_F64:\n-      return input_storage_type == F64 && output_storage_type == F64;\n+      return lhs_storage_type == rhs_storage_type && lhs_storage_type == F64 &&\n+             output_storage_type == F64;\n     default:\n       return false;\n   }"
        },
        {
            "sha": "34925a9cc537bc316b824934e80e604887e371b7",
            "filename": "third_party/xla/xla/service/algorithm_util.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.h?ref=9e85e4c323d2f180b240b1dec073df8739b1e481",
            "patch": "@@ -88,7 +88,8 @@ bool IsSupportedByElementalIrEmitter(PrecisionConfig::Algorithm algorithm);\n bool IsSupportedDotAlgorithmOnGpu(\n     PrecisionConfig::Algorithm algorithm,\n     const stream_executor::GpuComputeCapability& gpu_compute_capability,\n-    PrimitiveType input_storage_type, PrimitiveType output_storage_type);\n+    PrimitiveType lhs_storage_type, PrimitiveType rhs_storage_type,\n+    PrimitiveType output_storage_type);\n \n }  // namespace algorithm_util\n }  // namespace xla"
        },
        {
            "sha": "923e69e3f276e705197252c36d62d4659e6d61db",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=9e85e4c323d2f180b240b1dec073df8739b1e481",
            "patch": "@@ -3191,6 +3191,14 @@ cc_library(\n     ],\n     deps = [\n         \":gpu_compiler\",\n+        \":target_constants\",\n+        \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/stream_executor:semantic_version\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/sycl:sycl_platform_id\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@llvm-project//llvm:ir_headers\",\n     ],\n )\n "
        },
        {
            "sha": "1e4f8878842523e2c13d72d6faa40fa2d0b6ba11",
            "filename": "third_party/xla/xla/service/gpu/dot_algorithm_support_test.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 34,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc?ref=9e85e4c323d2f180b240b1dec073df8739b1e481",
            "patch": "@@ -61,12 +61,14 @@ struct Sizes {\n };\n \n struct TestParams {\n-  using TupleType = std::tuple<PrecisionConfig::Algorithm, PrimitiveType,\n-                               PrimitiveType, se::CudaComputeCapability,\n-                               SemanticVersion, BackendRestriction, Sizes>;\n+  using TupleType =\n+      std::tuple<PrecisionConfig::Algorithm, PrimitiveType, PrimitiveType,\n+                 PrimitiveType, se::CudaComputeCapability, SemanticVersion,\n+                 BackendRestriction, Sizes>;\n \n   PrecisionConfig::Algorithm algorithm;\n-  PrimitiveType input_storage_type;\n+  PrimitiveType lhs_storage_type;\n+  PrimitiveType rhs_storage_type;\n   PrimitiveType output_storage_type;\n   se::CudaComputeCapability min_cuda_capability;\n   SemanticVersion min_rocm_version;\n@@ -75,21 +77,23 @@ struct TestParams {\n \n   explicit TestParams(TupleType t)\n       : algorithm(std::get<0>(t)),\n-        input_storage_type(std::get<1>(t)),\n-        output_storage_type(std::get<2>(t)),\n-        min_cuda_capability(std::get<3>(t)),\n-        min_rocm_version(std::get<4>(t)),\n-        backend_restriction(std::get<5>(t)),\n-        sizes(std::get<6>(t)) {}\n+        lhs_storage_type(std::get<1>(t)),\n+        rhs_storage_type(std::get<2>(t)),\n+        output_storage_type(std::get<3>(t)),\n+        min_cuda_capability(std::get<4>(t)),\n+        min_rocm_version(std::get<5>(t)),\n+        backend_restriction(std::get<6>(t)),\n+        sizes(std::get<7>(t)) {}\n };\n \n std::string TestParamsToString(\n-    const TestParamInfo<TestParams::TupleType> &info) {\n+    const TestParamInfo<TestParams::TupleType>& info) {\n   const TestParams params(info.param);\n   return absl::StrFormat(\n-      \"%s_with_input_%s_output_%s_from_cc_%d_%d_rocm_%d%d_%s_c_%d_nc_%d\",\n+      \"%s_with_lhs_%s_rhs_%s_output_%s_from_cc_%d_%d_rocm_%d%d_%s_c_%d_nc_%d\",\n       AlgorithmToString(params.algorithm),\n-      primitive_util::LowercasePrimitiveTypeName(params.input_storage_type),\n+      primitive_util::LowercasePrimitiveTypeName(params.lhs_storage_type),\n+      primitive_util::LowercasePrimitiveTypeName(params.rhs_storage_type),\n       primitive_util::LowercasePrimitiveTypeName(params.output_storage_type),\n       params.min_cuda_capability.major, params.min_cuda_capability.minor,\n       params.min_rocm_version.major(), params.min_rocm_version.minor(),\n@@ -136,32 +140,35 @@ TEST_P(DotAlgorithmSupportTest, AlgorithmIsSupportedFromCudaCapability) {\n     HloModule test\n \n     ENTRY test {\n-      x = $1[$4,$3] parameter(0)\n-      y = $1[$3,$4] parameter(1)\n+      x = $1[$5,$4] parameter(0)\n+      y = $2[$4,$5] parameter(1)\n \n-      ROOT out = $2[$4,$4] dot(x, y),\n+      ROOT out = $3[$5,$5] dot(x, y),\n                  lhs_contracting_dims={1},\n                  rhs_contracting_dims={0},\n                  algorithm=$0\n     }\n   )\",\n       AlgorithmToString(params.algorithm),\n-      primitive_util::LowercasePrimitiveTypeName(params.input_storage_type),\n+      primitive_util::LowercasePrimitiveTypeName(params.lhs_storage_type),\n+      primitive_util::LowercasePrimitiveTypeName(params.rhs_storage_type),\n       primitive_util::LowercasePrimitiveTypeName(params.output_storage_type),\n       params.sizes.contracting_size, params.sizes.non_contracting_size);\n \n   bool is_algorithm_supported = false;\n   auto gpu_cc = GetGpuComputeCapability();\n \n-  if (const auto *ccc = std::get_if<se::CudaComputeCapability>(&gpu_cc)) {\n+  if (const auto* ccc = std::get_if<se::CudaComputeCapability>(&gpu_cc)) {\n     is_algorithm_supported =\n         ccc->SupportsAllFeaturesOf(params.min_cuda_capability);\n-  } else if (const auto *rcc =\n+  } else if (const auto* rcc =\n                  std::get_if<se::RocmComputeCapability>(&gpu_cc)) {\n     is_algorithm_supported = rcc->gfx9_mi100_or_later();\n     if (GetDeviceDescription().runtime_version() < params.min_rocm_version &&\n-        (params.input_storage_type == F8E5M2 ||\n-         params.input_storage_type == F8E4M3FN) &&\n+        (params.lhs_storage_type == F8E5M2 ||\n+         params.lhs_storage_type == F8E4M3FN ||\n+         params.rhs_storage_type == F8E5M2 ||\n+         params.rhs_storage_type == F8E4M3FN) &&\n         params.output_storage_type == BF16) {\n       GTEST_SKIP() << \"TODO: Unsupported F8 to BF16 in ROCm version < 6.3\";\n     }\n@@ -170,7 +177,7 @@ TEST_P(DotAlgorithmSupportTest, AlgorithmIsSupportedFromCudaCapability) {\n     }\n   }\n   if (is_algorithm_supported) {\n-    EXPECT_TRUE(Run(hlo_text));\n+    EXPECT_TRUE(Run(hlo_text)) << \"Failed to run HLO: \" << hlo_text;\n \n     if (params.backend_restriction == BackendRestriction::kTritonOnly) {\n       MatchOptimizedHlo(hlo_text, R\"(\n@@ -192,7 +199,8 @@ INSTANTIATE_TEST_SUITE_P(\n     F8E5M2Tests, DotAlgorithmSupportTest,\n     Combine(Values(PC::ALG_DOT_ANY_F8_ANY_F8_F32,\n                    PC::ALG_DOT_ANY_F8_ANY_F8_F32_FAST_ACCUM),\n-            Values(F8E5M2), Values(F8E5M2, F16, BF16, F32), Values(CC(8, 9)),\n+            Values(F8E5M2), Values(F8E4M3FN),\n+            Values(F8E5M2, F8E4M3FN, F16, BF16, F32), Values(CC(8, 9)),\n             Values(SemanticVersion{6, 3, 0}),\n             Values(BackendRestriction::kNoRestriction),\n             Values(Sizes{32, 32}, Sizes{16, 2})),\n@@ -202,79 +210,85 @@ INSTANTIATE_TEST_SUITE_P(\n     F8E4M3FNTests, DotAlgorithmSupportTest,\n     Combine(Values(PC::ALG_DOT_ANY_F8_ANY_F8_F32,\n                    PC::ALG_DOT_ANY_F8_ANY_F8_F32_FAST_ACCUM),\n-            Values(F8E4M3FN), Values(F8E4M3FN, F16, BF16, F32),\n-            Values(CC(8, 9)), Values(SemanticVersion{6, 3, 0}),\n+            Values(F8E4M3FN), Values(F8E4M3FN),\n+            Values(F8E5M2, F8E4M3FN, F16, BF16, F32), Values(CC(8, 9)),\n+            Values(SemanticVersion{6, 3, 0}),\n             Values(BackendRestriction::kNoRestriction),\n             Values(Sizes{32, 32}, Sizes{16, 2})),\n     TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotF16F16F32Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_F16_F16_F32), Values(F16),\n-                                 Values(F16, F32), Values(CC(0, 0)),\n+                                 Values(F16), Values(F16, F32),\n+                                 Values(CC(0, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotF32ForBf16Bf16F32Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_BF16_BF16_F32), Values(F32),\n-                                 Values(F32), Values(CC(8, 0)),\n+                                 Values(F32), Values(F32), Values(CC(8, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotBf16Bf16F32X3Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_BF16_BF16_F32_X3),\n-                                 Values(F32), Values(F32), Values(CC(8, 0)),\n+                                 Values(F32), Values(F32), Values(F32),\n+                                 Values(CC(8, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotBf16Bf16F32X6Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_BF16_BF16_F32_X6),\n-                                 Values(F32), Values(F32), Values(CC(8, 0)),\n+                                 Values(F32), Values(F32), Values(F32),\n+                                 Values(CC(8, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotBf16Bf16F32X9Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_BF16_BF16_F32_X9),\n-                                 Values(F32), Values(F32), Values(CC(8, 0)),\n+                                 Values(F32), Values(F32), Values(F32),\n+                                 Values(CC(8, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotTf32Tf32F32Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_TF32_TF32_F32), Values(F32),\n-                                 Values(F32), Values(CC(8, 0)),\n+                                 Values(F32), Values(F32), Values(CC(8, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotTf32Tf32F32X3Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_TF32_TF32_F32_X3),\n-                                 Values(F32), Values(F32), Values(CC(8, 0)),\n+                                 Values(F32), Values(F32), Values(F32),\n+                                 Values(CC(8, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotF32F32F32Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_F32_F32_F32), Values(F32),\n-                                 Values(F32), Values(CC(0, 0)),\n+                                 Values(F32), Values(F32), Values(CC(0, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),\n                          TestParamsToString);\n \n INSTANTIATE_TEST_SUITE_P(DotF64F64F64Tests, DotAlgorithmSupportTest,\n                          Combine(Values(PC::ALG_DOT_F64_F64_F64), Values(F64),\n-                                 Values(F64), Values(CC(0, 0)),\n+                                 Values(F64), Values(F64), Values(CC(0, 0)),\n                                  Values(SemanticVersion{6, 0, 0}),\n                                  Values(BackendRestriction::kNoRestriction),\n                                  Values(Sizes{32, 32}, Sizes{16, 2})),"
        },
        {
            "sha": "69ac80a67ce8902c6f064d1fa30c263e5ffbfef0",
            "filename": "third_party/xla/xla/service/gpu/transforms/algorithm_checker.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Falgorithm_checker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9e85e4c323d2f180b240b1dec073df8739b1e481/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Falgorithm_checker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Falgorithm_checker.cc?ref=9e85e4c323d2f180b240b1dec073df8739b1e481",
            "patch": "@@ -78,15 +78,9 @@ class AlgorithmCheckerVisitor : public ConstDfsHloVisitorWithDefault {\n     PrimitiveType rhs_storage_type = hlo->operand(1)->shape().element_type();\n     PrimitiveType output_storage_type = hlo->shape().element_type();\n \n-    if (lhs_storage_type != rhs_storage_type) {\n-      return absl::UnimplementedError(absl::StrFormat(\n-          \"Dot operands must have the same type when using an algorithm: %s\",\n-          hlo->ToString()));\n-    }\n-\n     return algorithm_util::IsSupportedDotAlgorithmOnGpu(\n                config.algorithm(), gpu_compute_capability_, lhs_storage_type,\n-               output_storage_type)\n+               rhs_storage_type, output_storage_type)\n                ? absl::OkStatus()\n                : absl::UnimplementedError(absl::StrFormat(\n                      \"Unsupported algorithm on the current device(s): %s\","
        }
    ],
    "stats": {
        "total": 153,
        "additions": 98,
        "deletions": 55
    }
}