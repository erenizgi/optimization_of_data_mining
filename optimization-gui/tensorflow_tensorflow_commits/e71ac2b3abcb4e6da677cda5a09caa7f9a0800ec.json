{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 834197050",
    "sha": "e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
    "files": [
        {
            "sha": "8efefb7245fc0aa4bde1faaf085d8325ff6387d9",
            "filename": "tensorflow/core/ops/cudnn_rnn_ops.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -92,9 +92,9 @@ REGISTER_OP(\"CudnnRNN\")\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n \n-      string direction;\n+      std::string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n-      string rnn_mode;\n+      std::string rnn_mode;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n       int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n       DimensionHandle output_size;\n@@ -140,9 +140,9 @@ REGISTER_OP(\"CudnnRNNV2\")\n       auto seq_length = c->Dim(input_shape, 0);\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n-      string direction;\n+      std::string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n-      string rnn_mode;\n+      std::string rnn_mode;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n       int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n       DimensionHandle output_size;\n@@ -195,9 +195,9 @@ REGISTER_OP(\"CudnnRNNV3\")\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n \n-      string direction;\n+      std::string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n-      string rnn_mode;\n+      std::string rnn_mode;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n       if (rnn_mode == \"lstm\") {\n         TF_RETURN_IF_ERROR(c->WithRank(input_c_shape, 3, &unused));"
        },
        {
            "sha": "5bc206c1392496cc42b2513983065701351efe78",
            "filename": "tensorflow/core/ops/cudnn_rnn_ops_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fcudnn_rnn_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -24,10 +24,10 @@ limitations under the License.\n \n namespace tensorflow {\n \n-static string JoinedCopies(const string& s, int copies) {\n-  string res;\n+static std::string JoinedCopies(const std::string& s, int copies) {\n+  std::string res;\n   for (int i = 0; i < copies; ++i) {\n-    strings::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n+    absl::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n   }\n   return res;\n }\n@@ -58,12 +58,12 @@ TEST(CudnnRNNOpsTest, ForwardLstm_ShapeFn) {\n   std::vector<int> output_shape = {seq_length, batch_size,\n                                    num_units * dir_count};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_h_shape), \";\", \"[?]\");\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNN\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNN\")\n@@ -95,12 +95,12 @@ TEST(CudnnRNNOpsTest, ForwardV2Lstm_ShapeFn) {\n   std::vector<int> output_shape = {seq_length, batch_size,\n                                    num_units * dir_count};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_h_shape), \";\", \"[?]\");\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNNV2\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV2\")\n@@ -135,13 +135,13 @@ TEST(CudnnRNNOpsTest, ForwardV3Lstm_ShapeFn) {\n                                    num_units * dir_count};\n   std::vector<int> seq_lengths_shape = {batch_size};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n       shape_to_str(seq_lengths_shape));\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in2;?;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in2;?;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNNV3\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n@@ -177,13 +177,13 @@ TEST(CudnnRNNOpsTest, ForwardV3Gru) {\n                                    num_units * dir_count};\n   std::vector<int> seq_lengths_shape = {batch_size};\n   auto shape_to_str = [](const std::vector<int>& v) {\n-    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+    return absl::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n   };\n-  string input_shapes_desc = strings::StrCat(\n+  std::string input_shapes_desc = strings::StrCat(\n       shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n       shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n       shape_to_str(seq_lengths_shape));\n-  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;[];?;?\";\n+  std::string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;[];?;?\";\n \n   ShapeInferenceTestOp op(\"CudnnRNNV3\");\n   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n@@ -207,7 +207,7 @@ TEST(CudnnRNNOpsTest, LSTMBlockCell_ShapeFn) {\n   ShapeInferenceTestOp op(\"LSTMBlockCell\");\n \n   // Last 6 inputs don't affect shape inference.\n-  string input_suffix = strings::StrCat(\";\", JoinedCopies(\"?\", 6));\n+  std::string input_suffix = absl::StrCat(\";\", JoinedCopies(\"?\", 6));\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 2\", op, \"[?];?\" + input_suffix);\n@@ -234,7 +234,7 @@ TEST(CudnnRNNOpsTest, BlockLSTM_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Middle inputs don't affect shape inference.\n-  string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n+  std::string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?]\" + infix + \"?\");\n@@ -266,7 +266,7 @@ TEST(CudnnRNNOpsTest, BlockLSTMV2_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Middle inputs don't affect shape inference.\n-  string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n+  std::string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?]\" + infix + \"?\");"
        },
        {
            "sha": "e85532695ffeb4b3ffa21d2ae540aa3881a143a6",
            "filename": "tensorflow/core/ops/data_flow_ops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fdata_flow_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fdata_flow_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fdata_flow_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -120,7 +120,7 @@ absl::Status DynamicStitchShapeFunction(InferenceContext* c) {\n \n     if (indices_t != nullptr) {\n       // The length is based on the highest index from flattened indices.\n-      const int32* indices = indices_t->flat<int32>().data();\n+      const int32_t* indices = indices_t->flat<int32_t>().data();\n       int64_t count = indices_t->NumElements();\n       for (int64_t i = 0; i < count; ++i) {\n         if (indices[i] > max_index) {\n@@ -340,7 +340,7 @@ REGISTER_OP(\"QueueDequeueManyV2\")\n       if (c->input_tensor(1) == nullptr) {\n         n_shape = c->Vector(InferenceContext::kUnknownDim);\n       } else {\n-        const int32_t n = c->input_tensor(1)->scalar<int32>()();\n+        const int32_t n = c->input_tensor(1)->scalar<int32_t>()();\n         if (n < 0) {\n           return errors::InvalidArgument(\"Input 'n' must be >= 0, but is \", n);\n         }"
        },
        {
            "sha": "30165ffe914129e1ad412a987262811c6558bb55",
            "filename": "tensorflow/core/ops/data_flow_ops_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fdata_flow_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fdata_flow_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fdata_flow_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -138,12 +138,12 @@ TEST(DataFlowOpsTest, DynamicStitch) {\n   INFER_OK(op, \"[2,3];[5,6];[2,3,4,5];[5,6,4,5]\", \"[?,d2_2,d2_3]\");\n \n   // 1 known input tensors, not enough to change answer.\n-  Tensor tensor_2 = test::AsTensor<int32>(\n-      std::vector<int32>{2, 4, 6, 0, 10, 11}, TensorShape({2, 3}));\n-  Tensor tensor_5 = test::AsTensor<int32>(\n-      std::vector<int32>{0,    1,  2,  3,  4,  5,  6,  7,  8,  9,\n-                         10,   11, 12, 13, 14, 15, 16, 17, 18, 19,\n-                         1000, 21, 22, 23, 24, 25, 26, 27, 28, 29},\n+  Tensor tensor_2 = test::AsTensor<int32_t>(\n+      std::vector<int32_t>{2, 4, 6, 0, 10, 11}, TensorShape({2, 3}));\n+  Tensor tensor_5 = test::AsTensor<int32_t>(\n+      std::vector<int32_t>{0,    1,  2,  3,  4,  5,  6,  7,  8,  9,\n+                           10,   11, 12, 13, 14, 15, 16, 17, 18, 19,\n+                           1000, 21, 22, 23, 24, 25, 26, 27, 28, 29},\n       TensorShape({5, 6}));\n   op.input_tensors.push_back(nullptr);\n   op.input_tensors.push_back(&tensor_5);\n@@ -157,7 +157,7 @@ TEST(DataFlowOpsTest, DynamicStitch) {\n   op.input_tensors[1] = &tensor_5;\n   INFER_OK(op, \"[2,3];[5,6];[2,3,4,5];[5,6,4,5]\", \"[1001,d2_2,d2_3]\");\n \n-  tensor_2.flat<int32>()(3) = 10000;\n+  tensor_2.flat<int32_t>()(3) = 10000;\n   INFER_OK(op, \"[2,3];[5,6];[2,3,4,5];[5,6,4,5]\", \"[10001,d2_2,d2_3]\");\n }\n \n@@ -254,7 +254,7 @@ TEST(DataFlowOpsTest, QueueDequeueManyV2ShapeFn) {\n   shapes_and_types.emplace_back(\"[?,2]\", DT_FLOAT);\n   INFER_OK(op, \"?;?\", \"[12,1,?,3];[12,?,2]\");\n \n-  n_tensor = test::AsScalar<int32>(-1);  // invalid value of n.\n+  n_tensor = test::AsScalar<int32_t>(-1);  // invalid value of n.\n   INFER_ERROR(\"must be >= 0\", op, \"?;?\");\n }\n "
        },
        {
            "sha": "5d36e15f0f24f6ca1d3a7f2078a1b26502be2cf5",
            "filename": "tensorflow/core/ops/image_ops.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fimage_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fimage_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fimage_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -57,7 +57,7 @@ absl::Status SetOutputToSizedImage(InferenceContext* c,\n           DataTypeString(size_tensor->dtype()), \" for input #\", size_input_idx,\n           \" in \", c->DebugString());\n     }\n-    auto vec = size_tensor->vec<int32>();\n+    auto vec = size_tensor->vec<int32_t>();\n     height = c->MakeDim(vec(0));\n     width = c->MakeDim(vec(1));\n   }\n@@ -415,7 +415,7 @@ REGISTER_OP(\"ResizeNearestNeighborGrad\")\n         TF_RETURN_IF_ERROR(c->ReplaceDim(input, 1, c->UnknownDim(), &input));\n         TF_RETURN_IF_ERROR(c->ReplaceDim(input, 2, c->UnknownDim(), &input));\n       } else {\n-        auto size_vec = size->vec<int32>();\n+        auto size_vec = size->vec<int32_t>();\n         TF_RETURN_IF_ERROR(\n             c->ReplaceDim(input, 1, c->MakeDim(size_vec(0)), &input));\n         TF_RETURN_IF_ERROR(\n@@ -516,7 +516,7 @@ REGISTER_OP(\"DecodeAndCropJpeg\")\n \n       const Tensor* crop_window = c->input_tensor(1);\n       if (crop_window != nullptr) {\n-        auto crop_window_vec = crop_window->vec<int32>();\n+        auto crop_window_vec = crop_window->vec<int32_t>();\n         h = c->MakeDim(crop_window_vec(2));\n         w = c->MakeDim(crop_window_vec(3));\n       }\n@@ -861,7 +861,7 @@ REGISTER_OP(\"ExtractGlimpse\")\n \n       bool uniform_noise = false;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"uniform_noise\", &uniform_noise));\n-      string noise;\n+      std::string noise;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"noise\", &noise));\n       if (uniform_noise && (!noise.empty() && noise != \"uniform\")) {\n         return errors::InvalidArgument(\n@@ -895,7 +895,7 @@ REGISTER_OP(\"ExtractGlimpseV2\")\n \n       bool uniform_noise = false;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"uniform_noise\", &uniform_noise));\n-      string noise;\n+      std::string noise;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"noise\", &noise));\n       if (uniform_noise && (!noise.empty() && noise != \"uniform\")) {\n         return errors::InvalidArgument("
        },
        {
            "sha": "d11204c47d437c9781e0aa95b718fb0a99fc76ed",
            "filename": "tensorflow/core/ops/image_ops_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fimage_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fimage_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fimage_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -45,7 +45,7 @@ TEST(ImageOpsTest, Resize_ShapeFn) {\n     // When the size tensor is not a constant, the middle dims are unknown.\n     INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,?,?,d0_3]\");\n \n-    Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+    Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n     op.input_tensors[1] = &size_tensor;\n     INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,20,30,d0_3]\");\n   }\n@@ -244,7 +244,7 @@ TEST(ImageOpsTest, ExtractGlimpse_ShapeFn) {\n   // When the size tensor is not a constant, the middle dims are unknown.\n   INFER_OK(op, \"[1,?,3,?];[2];?\", \"[d0_0,?,?,d0_3]\");\n \n-  Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors[1] = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];[2];?\", \"[d0_0,20,30,d0_3]\");\n \n@@ -272,7 +272,7 @@ TEST(ImageOpsTest, CropAndResize_ShapeFn) {\n   // When the size tensor is not a constant, the middle dims are unknown.\n   INFER_OK(op, \"[1,?,3,?];?;?;[2]\", \"[?,?,?,d0_3]\");\n \n-  Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors[3] = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];?;?;[2]\", \"[?,20,30,d0_3]\");\n \n@@ -298,7 +298,7 @@ TEST(ImageOpsTest, ResizeNearestNeighborGrad_ShapeFn) {\n   // When the size tensor is not a constant, the middle dims are unknown.\n   INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,?,?,d0_3]\");\n \n-  Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors[1] = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];[2]\", \"[d0_0,20,30,d0_3]\");\n }\n@@ -314,7 +314,7 @@ TEST(ImageOpsTest, CropAndResizeGradImage_ShapeFn) {\n   INFER_OK(op, \"?;?;?;?\", \"[?,?,?,?]\");\n \n   // Known image_size should result in full shape information.\n-  Tensor image_size = test::AsTensor<int32>({10, 20, 30, 40});\n+  Tensor image_size = test::AsTensor<int32_t>({10, 20, 30, 40});\n   op.input_tensors[3] = &image_size;\n   INFER_OK(op, \"?;?;?;[1]\", \"[10, 20, 30, 40]\");\n }\n@@ -357,7 +357,7 @@ TEST(ImageOpsTest, QuantizedResizeBilinear_ShapeFn) {\n   INFER_ERROR(\"must be rank 0\", op, \"[1,?,3,?];[2];[?];[]\");\n   INFER_ERROR(\"must be rank 0\", op, \"[1,?,3,?];[2];[];[?]\");\n \n-  const Tensor size_tensor = test::AsTensor<int32>({20, 30});\n+  const Tensor size_tensor = test::AsTensor<int32_t>({20, 30});\n   op.input_tensors.at(1) = &size_tensor;\n   INFER_OK(op, \"[1,?,3,?];[2];[];[]\", \"[d0_0,20,30,d0_3];[];[]\");\n }"
        },
        {
            "sha": "63fa08218976333bfee63d430bfbe9c952322583",
            "filename": "tensorflow/core/ops/io_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fio_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fio_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fio_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -112,7 +112,7 @@ REGISTER_OP(\"RestoreV2\")\n               \"The number of shape_and_slice doesn't match tensor outputs.\");\n         }\n         for (int i = 0; i < shape_and_slices_flat.size(); ++i) {\n-          const string& shape_and_slice = shape_and_slices_flat(i);\n+          const std::string& shape_and_slice = shape_and_slices_flat(i);\n           if (shape_and_slice.empty()) {\n             c->set_output(i, c->UnknownShape());\n             continue;"
        },
        {
            "sha": "19abdfd6779d41ea66313d571f13059af91113d5",
            "filename": "tensorflow/core/ops/linalg_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Flinalg_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Flinalg_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Flinalg_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -41,7 +41,7 @@ TEST(LinalgOpsTest, UnchangedSquare_ShapeFn) {\n   for (const char* op_name : {\"Cholesky\", \"CholeskyGrad\", \"MatrixInverse\"}) {\n     ShapeInferenceTestOp op(op_name);\n \n-    const string extra_shape = (op.name == \"CholeskyGrad\" ? \";?\" : \"\");\n+    const std::string extra_shape = (op.name == \"CholeskyGrad\" ? \";?\" : \"\");\n \n     INFER_OK(op, \"?\" + extra_shape, \"?\");\n     INFER_ERROR(\"Shape must be at least rank 2 but is rank 1\", op,"
        },
        {
            "sha": "75f6e9f8b53721f46118bb1e73de34c917693996",
            "filename": "tensorflow/core/ops/lookup_ops.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Flookup_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Flookup_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Flookup_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -90,9 +90,9 @@ REGISTER_OP(\"LookupTableFind\")\n \n absl::Status ValidateTableType(InferenceContext* c,\n                                const ShapeAndType& key_shape_and_type,\n-                               const string& key_dtype_attr,\n+                               const std::string& key_dtype_attr,\n                                const ShapeAndType& value_shape_and_type,\n-                               const string& value_dtype_attr) {\n+                               const std::string& value_dtype_attr) {\n   DataType key_dtype;\n   TF_RETURN_IF_ERROR(c->GetAttr(key_dtype_attr, &key_dtype));\n   if (key_shape_and_type.dtype != key_dtype) {\n@@ -115,8 +115,8 @@ absl::Status ValidateTableType(InferenceContext* c,\n }\n \n absl::Status ValidateTableResourceHandle(InferenceContext* c, ShapeHandle keys,\n-                                         const string& key_dtype_attr,\n-                                         const string& value_dtype_attr,\n+                                         const std::string& key_dtype_attr,\n+                                         const std::string& value_dtype_attr,\n                                          ShapeAndType* output_shape_and_type) {\n   auto* handle_data = c->input_handle_shapes_and_types(0);\n   if (handle_data == nullptr || handle_data->size() != 2) {"
        },
        {
            "sha": "cecfa31bace6116716f2a48b720b8f4820321c8f",
            "filename": "tensorflow/core/ops/math_grad.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_grad.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -630,7 +630,7 @@ absl::Status SquaredDifferenceGrad(const AttrSlice& attrs, FunctionDef* g) {\n }\n REGISTER_OP_GRADIENT(\"SquaredDifference\", SquaredDifferenceGrad);\n \n-absl::Status MaximumMinimumGradHelper(const string& comparator,\n+absl::Status MaximumMinimumGradHelper(const std::string& comparator,\n                                       const AttrSlice& attrs, FunctionDef* g) {\n   // clang-format off\n   return GradForBinaryCwise(g, {\n@@ -770,7 +770,7 @@ REGISTER_OP_GRADIENT(\"Mean\", MeanGrad);\n // REGISTER_OP_GRADIENT(\"UnsortedSegmentSum\", UnsortedSegmentSumGrad);\n // REGISTER_OP_GRADIENT(\"UnsortedSegmentMax\", UnsortedSegmentMaxGrad);\n \n-absl::Status MinMaxGradHelper(const string& op, const AttrSlice& attrs,\n+absl::Status MinMaxGradHelper(const std::string& op, const AttrSlice& attrs,\n                               FunctionDef* g) {\n   // clang-format off\n   *g = FDH::Define(\n@@ -807,13 +807,11 @@ absl::Status MinGrad(const AttrSlice& attrs, FunctionDef* g) {\n }\n REGISTER_OP_GRADIENT(\"Min\", MinGrad);\n \n-static absl::Status MatMulGradHelper(FunctionDef* g, const string& opname,\n-                                     const string& attr_adj_x,\n-                                     const string& attr_adj_y, const string& x0,\n-                                     bool ax0, const string& x1, bool ax1,\n-                                     const string& y0, bool ay0,\n-                                     const string& y1, bool ay1,\n-                                     bool enable_broadcasting) {\n+static absl::Status MatMulGradHelper(\n+    FunctionDef* g, const std::string& opname, const std::string& attr_adj_x,\n+    const std::string& attr_adj_y, const std::string& x0, bool ax0,\n+    const std::string& x1, bool ax1, const std::string& y0, bool ay0,\n+    const std::string& y1, bool ay1, bool enable_broadcasting) {\n   // The final outputs are \"dx\" and \"dy\". If we're broadcasting compute\n   // intermediate nodes for now.\n   std::vector<FDH::Node> nodes = {\n@@ -831,9 +829,9 @@ static absl::Status MatMulGradHelper(FunctionDef* g, const string& opname,\n   // broadcasting-specific ops.\n   if (enable_broadcasting) {\n     std::vector<FDH::Node> unbroadcast_gradients = {\n-        FDH::Const<int32>(\"zero\", absl::Span<const int32>{0}),\n-        FDH::Const<int32>(\"one\", absl::Span<const int32>{1}),\n-        FDH::Const<int32>(\"minustwo\", absl::Span<const int32>{-2}),\n+        FDH::Const<int32_t>(\"zero\", absl::Span<const int32_t>{0}),\n+        FDH::Const<int32_t>(\"one\", absl::Span<const int32_t>{1}),\n+        FDH::Const<int32_t>(\"minustwo\", absl::Span<const int32_t>{-2}),\n         // Compute the batch shapes of the inputs (all but last two dims).\n         {{\"sx\"}, \"Shape\", {\"x\"}, {{\"T\", \"$T\"}}},\n         {{\"sy\"}, \"Shape\", {\"y\"}, {{\"T\", \"$T\"}}},\n@@ -866,9 +864,11 @@ static absl::Status MatMulGradHelper(FunctionDef* g, const string& opname,\n   return absl::OkStatus();\n }\n \n-absl::Status MatMulGradCommon(const string& opname, const string& attr_adj_x,\n-                              const string& attr_adj_y, const AttrSlice& attrs,\n-                              FunctionDef* g, bool enable_broadcasting) {\n+absl::Status MatMulGradCommon(const std::string& opname,\n+                              const std::string& attr_adj_x,\n+                              const std::string& attr_adj_y,\n+                              const AttrSlice& attrs, FunctionDef* g,\n+                              bool enable_broadcasting) {\n   DataType T;\n   TF_RETURN_IF_ERROR(GetNodeAttr(attrs, \"T\", &T));\n   if (T == DT_COMPLEX64 || T == DT_COMPLEX128) {"
        },
        {
            "sha": "5ef72e958292eab50c2b21e9140911a8ca088e8f",
            "filename": "tensorflow/core/ops/math_grad_test.cc",
            "status": "modified",
            "additions": 60,
            "deletions": 59,
            "changes": 119,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_grad_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_grad_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_grad_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -43,9 +43,9 @@ class MathGradTest : public ::testing::Test {\n   absl::Status Unary(const FDH::Node& op_node, const Tensor& x,\n                      const DataType dst, Tensor* y) {\n     const DataType src = x.dtype();\n-    auto adef = [](const string& name,\n+    auto adef = [](const std::string& name,\n                    const DataType type) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(type));\n+      return absl::StrCat(name, \":\", DataTypeString(type));\n     };\n     // Sum(op(x)), sum all output of op(x).\n     auto test = FDH::Define(\"Test\", {adef(\"x\", src)}, {adef(\"l\", dst)}, {},\n@@ -94,13 +94,13 @@ class MathGradTest : public ::testing::Test {\n     return s;\n   }\n \n-  absl::Status Unary(const string& op, const Tensor& x, Tensor* y) {\n+  absl::Status Unary(const std::string& op, const Tensor& x, Tensor* y) {\n     const FDH::Node op_node = {{\"y\"}, op, {\"x\"}, {{\"T\", x.dtype()}}};\n     return Unary(op_node, x, x.dtype(), y);\n   }\n \n   // Unary op expecting OK.\n-  Tensor SymGrad(const string& op, const Tensor& x) {\n+  Tensor SymGrad(const std::string& op, const Tensor& x) {\n     Tensor ret;\n     TF_CHECK_OK(Unary(op, x, &ret));\n     return ret;\n@@ -115,11 +115,11 @@ class MathGradTest : public ::testing::Test {\n   }\n \n   // Binary\n-  void SymGrad(const string& op, const Tensor& x, const Tensor& y, Tensor* dx,\n-               Tensor* dy) {\n+  void SymGrad(const std::string& op, const Tensor& x, const Tensor& y,\n+               Tensor* dx, Tensor* dy) {\n     const DataType T = x.dtype();\n-    auto adef = [T](const string& name) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(T));\n+    auto adef = [T](const std::string& name) {  // E.g., x:float, dy:double\n+      return absl::StrCat(name, \":\", DataTypeString(T));\n     };\n     // Sum(op(x)), sum all output of op(x).\n     auto test = FDH::Define(\"Test\", {adef(\"x\"), adef(\"y\")}, {adef(\"l\")}, {},\n@@ -171,11 +171,11 @@ class MathGradTest : public ::testing::Test {\n   }\n \n   // Reduction grad\n-  void ReductionGrad(const string& op, const Tensor& x, const Tensor& idx,\n+  void ReductionGrad(const std::string& op, const Tensor& x, const Tensor& idx,\n                      Tensor* dx, Tensor* di) {\n     const DataType T = x.dtype();\n-    auto adef = [T](const string& name) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(T));\n+    auto adef = [T](const std::string& name) {  // E.g., x:float, dy:double\n+      return absl::StrCat(name, \":\", DataTypeString(T));\n     };\n     // Sum(op(x, idx)), sum all output of op(x, idx).\n     auto test = FDH::Define(\"Test\", {adef(\"x\"), \"i:int32\"}, {adef(\"l\")}, {},\n@@ -225,11 +225,11 @@ class MathGradTest : public ::testing::Test {\n     *di = outputs[1];\n   }\n \n-  Tensor ReduceSum(const Tensor& x, absl::Span<const int32> axes) {\n+  Tensor ReduceSum(const Tensor& x, absl::Span<const int32_t> axes) {\n     int num_axes = axes.length();\n     Tensor y(DT_INT32, TensorShape({num_axes}));\n     for (size_t i = 0; i < axes.size(); ++i) {\n-      y.flat<int32>()(i) = axes[i];\n+      y.flat<int32_t>()(i) = axes[i];\n     }\n     auto T = x.dtype();\n     auto gdef = test::function::GDef(\n@@ -248,8 +248,8 @@ class MathGradTest : public ::testing::Test {\n     return outputs[0];\n   }\n \n-  Tensor MatMulCommon(const string& opname, const string& attr_adj_x,\n-                      const string& attr_adj_y, const Tensor& x, bool ax,\n+  Tensor MatMulCommon(const std::string& opname, const std::string& attr_adj_x,\n+                      const std::string& attr_adj_y, const Tensor& x, bool ax,\n                       const Tensor& y, bool ay) {\n     auto T = x.dtype();\n     auto gdef = test::function::GDef(\n@@ -281,12 +281,13 @@ class MathGradTest : public ::testing::Test {\n     return MatMulCommon(\"BatchMatMulV2\", \"adj_x\", \"adj_y\", x, ax, y, ay);\n   }\n \n-  void MatMulGradCommon(const string& opname, const string& attr_adj_x,\n-                        const string& attr_adj_y, const Tensor& x, bool ax,\n+  void MatMulGradCommon(const std::string& opname,\n+                        const std::string& attr_adj_x,\n+                        const std::string& attr_adj_y, const Tensor& x, bool ax,\n                         const Tensor& y, bool ay, Tensor* dx, Tensor* dy) {\n     const DataType T = x.dtype();\n-    auto adef = [T](const string& name) {  // E.g., x:float, dy:double\n-      return strings::StrCat(name, \":\", DataTypeString(T));\n+    auto adef = [T](const std::string& name) {  // E.g., x:float, dy:double\n+      return absl::StrCat(name, \":\", DataTypeString(T));\n     };\n     // Sum(op(x)), sum all output of op(x).\n     auto test =\n@@ -412,7 +413,7 @@ class MathGradTest : public ::testing::Test {\n   }\n };\n \n-void HasError(const absl::Status& s, const string& substr) {\n+void HasError(const absl::Status& s, const std::string& substr) {\n   EXPECT_TRUE(absl::StrContains(s.ToString(), substr))\n       << s << \", expected substring \" << substr;\n }\n@@ -1363,187 +1364,187 @@ TEST_F(MathGradTest, BatchMatMulV2_BroadcastWhileAdjointed) {\n TEST_F(MathGradTest, Sum_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Sum\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 1.f, 1.f, 1.f, 1.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Sum_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Sum\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 1.f, 1.f, 1.f, 1.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Mean_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Mean\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>(\n               {1.f / 2, 1.f / 2, 1.f / 2, 1.f / 2, 1.f / 2, 1.f / 2},\n               TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Mean_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Mean\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>(\n               {1.f / 3, 1.f / 3, 1.f / 3, 1.f / 3, 1.f / 3, 1.f / 3},\n               TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Mean_dim0_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Mean\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>(\n               {1.f / 6, 1.f / 6, 1.f / 6, 1.f / 6, 1.f / 6, 1.f / 6},\n               TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Min_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 1.f, 1.f, 0.f, 0.f, 0.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Min_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 0.f, 0.f, 1.f, 0.f, 0.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Min_dim0_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({1.f, 0.f, 0.f, 0.f, 0.f, 0.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Min_dim0_dim1_Dups) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, -3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Min\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({.5f, 0.f, 0.f, 0.f, 0.f, .5f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Max_dim0) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({0}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   LOG(INFO) << dx.SummarizeValue(6);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({0.f, 0.f, 0.f, 1.f, 1.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Max_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({1}, TensorShape({}));\n+  auto i = test::AsTensor<int32_t>({1}, TensorShape({}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({0.f, 0.f, 1.f, 0.f, 0.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(di,\n-                                 test::AsTensor<int32>({0}, TensorShape({})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0}, TensorShape({})));\n }\n \n TEST_F(MathGradTest, Max_dim0_dim1) {\n   auto x = test::AsTensor<float>({-3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({0.f, 0.f, 0.f, 0.f, 0.f, 1.f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n TEST_F(MathGradTest, Max_dim0_dim1_Dups) {\n   auto x = test::AsTensor<float>({3.f, -2.f, -1.f, 1.f, 2.f, 3.f},\n                                  TensorShape({2, 3}));\n-  auto i = test::AsTensor<int32>({0, 1}, TensorShape({2}));\n+  auto i = test::AsTensor<int32_t>({0, 1}, TensorShape({2}));\n   Tensor dx;\n   Tensor di;\n   ReductionGrad(\"Max\", x, i, &dx, &di);\n   test::ExpectTensorEqual<float>(\n       dx, test::AsTensor<float>({.5f, 0.f, 0.f, 0.f, 0.f, .5f},\n                                 TensorShape({2, 3})));\n-  test::ExpectTensorEqual<int32>(\n-      di, test::AsTensor<int32>({0, 0}, TensorShape({2})));\n+  test::ExpectTensorEqual<int32_t>(\n+      di, test::AsTensor<int32_t>({0, 0}, TensorShape({2})));\n }\n \n }  // namespace"
        },
        {
            "sha": "e594e3d5c4aca01312a6610c27cf9d028b96b243",
            "filename": "tensorflow/core/ops/math_ops.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -1029,7 +1029,7 @@ absl::Status ArgOpShape(shape_inference::InferenceContext* c) {\n \n   int64_t dimension_val;\n   if (dim_t->dtype() == DT_INT32) {\n-    dimension_val = dim_t->scalar<int32>()();\n+    dimension_val = dim_t->scalar<int32_t>()();\n   } else {\n     dimension_val = dim_t->scalar<int64_t>()();\n   }\n@@ -1142,7 +1142,7 @@ absl::Status SparseSegmentReductionGradShapeFnImpl(\n     // shape is unknown.\n     dim0_shape = c->Vector(InferenceContext::kUnknownDim);\n   } else {\n-    auto dim0_value = dim0->scalar<int32>()();\n+    auto dim0_value = dim0->scalar<int32_t>()();\n     if (dim0_value < 0) {\n       return errors::InvalidArgument(\n           \"Cannot specify a negative value for output_dim0\");\n@@ -1198,7 +1198,7 @@ absl::Status SparseSegmentReductionWithNumSegmentsShapeFn(InferenceContext* c) {\n     TF_RETURN_IF_ERROR(c->Concatenate(c->Vector(InferenceContext::kUnknownDim),\n                                       subshape, &out));\n   } else {\n-    auto dim0_value = dim0->scalar<int32>()();\n+    auto dim0_value = dim0->scalar<int32_t>()();\n     if (dim0_value < 0) {\n       return errors::InvalidArgument(\n           \"Cannot specify a negative value for num_segments\");\n@@ -1573,19 +1573,19 @@ REGISTER_OP(\"Range\")\n         return absl::OkStatus();\n       }\n       if (dtype == DT_INT32) {\n-        return RangeSize<int32>(start_t, limit_t, delta_t, c);\n+        return RangeSize<int32_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_INT16) {\n-        return RangeSize<int16>(start_t, limit_t, delta_t, c);\n+        return RangeSize<int16_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_INT8) {\n-        return RangeSize<int8>(start_t, limit_t, delta_t, c);\n+        return RangeSize<int8_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_INT64) {\n         return RangeSize<int64_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_UINT16) {\n-        return RangeSize<uint16>(start_t, limit_t, delta_t, c);\n+        return RangeSize<uint16_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_UINT32) {\n-        return RangeSize<uint32>(start_t, limit_t, delta_t, c);\n+        return RangeSize<uint32_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_UINT64) {\n-        return RangeSize<uint64>(start_t, limit_t, delta_t, c);\n+        return RangeSize<uint64_t>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_FLOAT) {\n         return RangeSize<float>(start_t, limit_t, delta_t, c);\n       } else if (dtype == DT_DOUBLE) {\n@@ -1621,7 +1621,7 @@ REGISTER_OP(\"LinSpace\")\n \n       int64_t num;\n       if (num_t->dtype() == DT_INT32) {\n-        num = num_t->scalar<int32>()();\n+        num = num_t->scalar<int32_t>()();\n       } else {\n         num = num_t->scalar<int64_t>()();\n       }\n@@ -1760,7 +1760,7 @@ REGISTER_OP(\"Bincount\")\n       }\n \n       // Return `[size]` shape if size is known.\n-      int32_t size_val = size_tensor->scalar<int32>()();\n+      int32_t size_val = size_tensor->scalar<int32_t>()();\n       if (size_val < 0) {\n         return errors::InvalidArgument(\"size (\", size_val,\n                                        \") must be non-negative\");\n@@ -1801,7 +1801,7 @@ REGISTER_OP(\"DenseBincount\")\n       DataType dtype;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n       if (dtype == DT_INT32) {\n-        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n+        size_val = static_cast<int64_t>(size_tensor->scalar<int32_t>()());\n       } else if (dtype == DT_INT64) {\n         size_val = size_tensor->scalar<int64_t>()();\n       } else {\n@@ -1846,7 +1846,7 @@ REGISTER_OP(\"SparseBincount\")\n       DataType dtype;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n       if (dtype == DT_INT32) {\n-        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n+        size_val = static_cast<int64_t>(size_tensor->scalar<int32_t>()());\n       } else if (dtype == DT_INT64) {\n         size_val = size_tensor->scalar<int64_t>()();\n       } else {\n@@ -2136,11 +2136,11 @@ REGISTER_OP(\"SobolSample\")\n       const Tensor* num_results_t = c->input_tensor(1);\n \n       int32_t dim = dim_t == nullptr ? InferenceContext::kUnknownDim\n-                                     : dim_t->scalar<int32>()();\n+                                     : dim_t->scalar<int32_t>()();\n \n       int32_t num_results = num_results_t == nullptr\n                                 ? InferenceContext::kUnknownDim\n-                                : num_results_t->scalar<int32>()();\n+                                : num_results_t->scalar<int32_t>()();\n \n       c->set_output(0, c->Matrix(num_results, dim));\n       return absl::OkStatus();"
        },
        {
            "sha": "b4392dbe439bf56315db203db78526e4f9f3a190",
            "filename": "tensorflow/core/ops/math_ops_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fmath_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fmath_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -160,18 +160,18 @@ TEST(MathOpsTest, BroadcastBinaryOps_ShapeFn) {\n     }\n   };\n \n-  for (string op_name : {\"Add\",        \"Complex\",\n-                         \"Div\",        \"Equal\",\n-                         \"Greater\",    \"GreaterEqual\",\n-                         \"Igamma\",     \"Igammac\",\n-                         \"Zeta\",       \"Polygamma\",\n-                         \"Less\",       \"LessEqual\",\n-                         \"LogicalAnd\", \"LogicalOr\",\n-                         \"Maximum\",    \"Minimum\",\n-                         \"Mod\",        \"Mul\",\n-                         \"NotEqual\",   \"Pow\",\n-                         \"Sub\",        \"SquaredDifference\",\n-                         \"DivNoNan\"}) {\n+  for (std::string op_name : {\"Add\",        \"Complex\",\n+                              \"Div\",        \"Equal\",\n+                              \"Greater\",    \"GreaterEqual\",\n+                              \"Igamma\",     \"Igammac\",\n+                              \"Zeta\",       \"Polygamma\",\n+                              \"Less\",       \"LessEqual\",\n+                              \"LogicalAnd\", \"LogicalOr\",\n+                              \"Maximum\",    \"Minimum\",\n+                              \"Mod\",        \"Mul\",\n+                              \"NotEqual\",   \"Pow\",\n+                              \"Sub\",        \"SquaredDifference\",\n+                              \"DivNoNan\"}) {\n     ShapeInferenceTestOp op(op_name);\n     AddNodeAttr(\"incompatible_shape_error\", true, &op.node_def);\n     test_shapes(op, true);"
        },
        {
            "sha": "b0da798463cc38529604eee20fd20317bf2f051a",
            "filename": "tensorflow/core/ops/nn_grad.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fnn_grad.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fnn_grad.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fnn_grad.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -37,7 +37,7 @@ absl::Status SoftmaxGrad(const AttrSlice& attrs, FunctionDef* g) {\n       {\n         {{\"softmax\"}, \"Softmax\", {\"x\"}, {{\"T\", \"$T\"}}},\n         {{\"n0\"}, \"Mul\", {\"grad_softmax\", \"softmax\"}, {{\"T\", \"$T\"}}},\n-        FDH::Const<int32>(\"indices\", {-1}),\n+        FDH::Const<int32_t>(\"indices\", {-1}),\n         {{\"n1\"}, \"Sum\", {\"n0\", \"indices\"}, {{\"keep_dims\", true}, {\"T\", \"$T\"}}},\n         {{\"n2\"}, \"Sub\", {\"grad_softmax\", \"n1\"}, {{\"T\", \"$T\"}}},\n         {{\"grad_x\"}, \"Mul\", {\"n2\", \"softmax\"}, {{\"T\", \"$T\"}}}\n@@ -61,7 +61,7 @@ absl::Status LogSoftmaxGrad(const AttrSlice& attrs, FunctionDef* g) {\n       // Based on _LogSoftmaxGrad in nn_grad.py.\n       {\n         {{\"softmax\"}, \"Softmax\", {\"x\"}, {{\"T\", \"$T\"}}},\n-        FDH::Const<int32>(\"indices\", {-1}),\n+        FDH::Const<int32_t>(\"indices\", {-1}),\n         {{\"n0\"}, \"Sum\", {\"grad_logsoftmax\", \"indices\"},\n          {{\"keep_dims\", true}, {\"T\", \"$T\"}}},\n         {{\"n1\"}, \"Mul\", {\"n0\", \"softmax\"}, {{\"T\", \"$T\"}}},"
        },
        {
            "sha": "c9a70cc3622cf126c7d35d6575f898ceb5e935eb",
            "filename": "tensorflow/core/ops/nn_ops.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fnn_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fnn_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fnn_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -539,8 +539,8 @@ absl::Status CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n     DimensionHandle new_height = c->UnknownDim();\n     DimensionHandle new_width = c->UnknownDim();\n     if (size != nullptr) {\n-      new_height = c->MakeDim(size->flat<int32>()(0));\n-      new_width = c->MakeDim(size->flat<int32>()(1));\n+      new_height = c->MakeDim(size->flat<int32_t>()(0));\n+      new_width = c->MakeDim(size->flat<int32_t>()(1));\n     }\n     TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 1, new_height, &resized));\n     TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 2, new_width, &resized));\n@@ -559,8 +559,8 @@ absl::Status CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n     std::vector<DimensionHandle> output_dims;\n     for (int i = 0; i < 4; ++i) {\n       DimensionHandle dim = c->Dim(resized, i);\n-      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 0));\n-      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 1));\n+      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32_t>()(i, 0));\n+      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32_t>()(i, 1));\n       if (p0 < 0 || p1 < 0) {\n         return errors::InvalidArgument(\"Paddings must be non-negative\");\n       }\n@@ -576,7 +576,7 @@ absl::Status CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n   // Work out the convolution's effect with 'padded' as the input.\n   ShapeHandle filter;\n   TF_RETURN_IF_ERROR(c->WithRank(c->input(filter_index), 4, &filter));\n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n   if (strides.size() != 4) {\n     return errors::InvalidArgument(\n@@ -1026,7 +1026,7 @@ REGISTER_OP(\"MaxPoolWithArgmax\")\n     .Output(\"argmax: Targmax\")\n     .Attr(\"T: realnumbertype\")\n     .SetShapeFn([](InferenceContext* c) {\n-      std::vector<int32> ksize;\n+      std::vector<int32_t> ksize;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"ksize\", &ksize));\n       for (int i = 0; i < ksize.size(); ++i) {\n         if (ksize[i] <= 0) {\n@@ -1091,7 +1091,7 @@ REGISTER_OP(\"Dilation2D\")\n       ShapeHandle filter_shape;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &filter_shape));\n \n-      std::vector<int32> strides;\n+      std::vector<int32_t> strides;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n       if (strides.size() != 4) {\n         return errors::InvalidArgument(\n@@ -1100,7 +1100,7 @@ REGISTER_OP(\"Dilation2D\")\n             strides.size());\n       }\n \n-      std::vector<int32> rates;\n+      std::vector<int32_t> rates;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n       if (rates.size() != 4) {\n         return errors::InvalidArgument("
        },
        {
            "sha": "23ddf12e08305e393475458fe437d71edac39423",
            "filename": "tensorflow/core/ops/nn_ops_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fnn_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fnn_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fnn_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -59,7 +59,7 @@ TEST(NNOpsTest, TopKV2_ShapeFn) {\n   Tensor k_t;\n   op.input_tensors[1] = &k_t;\n \n-  k_t = test::AsScalar<int32>(20);\n+  k_t = test::AsScalar<int32_t>(20);\n   // With known input, each output is an unknown shape.\n   INFER_OK(op, \"?;[]\", \"?;?\");\n   // With vector input, each output is [k].\n@@ -75,7 +75,7 @@ TEST(NNOpsTest, TopKV2_ShapeFn) {\n               \"[1];[]\");\n   INFER_ERROR(\"input must have last dimension >= k = 20 but is 4\", op,\n               \"[1,2,3,4];[]\");\n-  k_t = test::AsScalar<int32>(-1);\n+  k_t = test::AsScalar<int32_t>(-1);\n   INFER_ERROR(\n       \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n       op, \"[1,2,3,4];[]\");\n@@ -87,7 +87,7 @@ TEST(NNOpsTest, NthElement_ShapeFn) {\n \n   Tensor n_t;\n   op.input_tensors[1] = &n_t;\n-  n_t = test::AsScalar<int32>(20);\n+  n_t = test::AsScalar<int32_t>(20);\n \n   INFER_OK(op, \"?;[]\", \"?\");\n   INFER_OK(op, \"[21];[]\", \"[]\");\n@@ -98,7 +98,7 @@ TEST(NNOpsTest, NthElement_ShapeFn) {\n   INFER_ERROR(\"Input must have last dimension > n = 20 but is 1\", op, \"[1];[]\");\n   INFER_ERROR(\"Input must have last dimension > n = 20 but is 20\", op,\n               \"[1,2,3,20];[]\");\n-  n_t = test::AsScalar<int32>(-1);\n+  n_t = test::AsScalar<int32_t>(-1);\n   INFER_ERROR(\n       \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n       op, \"[1,2,3,4];[]\");\n@@ -182,7 +182,7 @@ TEST(NNOpsTest, FusedBatchNorm_ShapeFn) {\n   ShapeInferenceTestOp op(\"FusedBatchNorm\");\n \n   auto set_op = [&op](bool is_training, float exponential_avg_factor,\n-                      string data_format) {\n+                      std::string data_format) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNorm\")\n                      .Input(FakeInput(DT_FLOAT))\n                      .Input(FakeInput(DT_FLOAT))\n@@ -276,7 +276,7 @@ TEST(NNOpsTest, FusedBatchNorm_ShapeFn) {\n \n TEST(NNOpsTest, FusedBatchNormGrad_ShapeFn) {\n   ShapeInferenceTestOp op(\"FusedBatchNormGrad\");\n-  auto set_op = [&op](string data_format) {\n+  auto set_op = [&op](std::string data_format) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNormGrad\")\n                      .Input(FakeInput(DT_FLOAT))\n                      .Input(FakeInput(DT_FLOAT))\n@@ -490,8 +490,9 @@ TEST(NNOpsTest, InTopK_ShapeFn) {\n \n TEST(NNOpsTest, Dilation2DShapeTest) {\n   ShapeInferenceTestOp op(\"Dilation2D\");\n-  auto set_op = [&op](const std::vector<int32>& strides,\n-                      const std::vector<int32>& rates, const string& padding) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::vector<int32_t>& rates,\n+                      const std::string& padding) {\n     TF_ASSERT_OK(NodeDefBuilder(\"test\", \"Dilation2D\")\n                      .Input(\"input\", 0, DT_FLOAT)\n                      .Input(\"filter\", 0, DT_FLOAT)\n@@ -568,8 +569,8 @@ TEST(NNOpsTest, FractionalAvgPoolGrad) {\n   INFER_OK(op, \"?;?;?;?\", \"[?,?,?,?]\");\n \n   // When input tensor is known, its values determine output shape.\n-  std::vector<int32> shape{1, 2, 3, 4};\n-  Tensor shape_t = test::AsTensor<int32>(shape);\n+  std::vector<int32_t> shape{1, 2, 3, 4};\n+  Tensor shape_t = test::AsTensor<int32_t>(shape);\n   op.input_tensors[0] = &shape_t;\n   INFER_OK(op, \"[5];?;?;?\", \"[1,2,3,4]\");\n }"
        },
        {
            "sha": "e433ac7ae3e5e106b20f0968f83a8959e4451bb8",
            "filename": "tensorflow/core/ops/parsing_ops_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fparsing_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fparsing_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fparsing_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -157,7 +157,7 @@ TEST(ParsingOpsTest, ParseSequenceExample_ShapeFn) {\n                            bool add_extra_shape = false) {\n     using NodeOutList = std::vector<NodeDefBuilder::NodeOut>;\n     using DataTypeList = std::vector<DataType>;\n-    string string_in(\"test\");\n+    std::string string_in(\"test\");\n     NodeDefBuilder::NodeOut node_in{\"a\", 0, DT_STRING};\n     TF_ASSERT_OK(\n         NodeDefBuilder(\"test\", \"ParseSequenceExample\")\n@@ -169,15 +169,15 @@ TEST(ParsingOpsTest, ParseSequenceExample_ShapeFn) {\n             .Attr(\"Nfeature_list_sparse\", num_feature_list_sparse)\n             .Attr(\"Nfeature_list_dense\", num_feature_list_dense)\n             .Attr(\"feature_list_dense_missing_assumed_empty\",\n-                  std::vector<string>(num_feature_list_dense, string_in))\n+                  std::vector<std::string>(num_feature_list_dense, string_in))\n             .Attr(\"context_sparse_keys\",\n-                  std::vector<string>(num_context_sparse, string_in))\n+                  std::vector<std::string>(num_context_sparse, string_in))\n             .Attr(\"context_dense_keys\",\n-                  std::vector<string>(num_context_dense, string_in))\n+                  std::vector<std::string>(num_context_dense, string_in))\n             .Attr(\"feature_list_sparse_keys\",\n-                  std::vector<string>(num_feature_list_sparse, string_in))\n+                  std::vector<std::string>(num_feature_list_sparse, string_in))\n             .Attr(\"feature_list_dense_keys\",\n-                  std::vector<string>(num_feature_list_dense, string_in))\n+                  std::vector<std::string>(num_feature_list_dense, string_in))\n             .Attr(\"context_sparse_types\",\n                   DataTypeList(num_context_sparse, DT_FLOAT))\n             .Attr(\"context_dense_types\",\n@@ -395,7 +395,7 @@ TEST(ParsingOpsTest, ParseSequenceExampleV2_ShapeFn) {\n                            bool add_extra_shape = false) {\n     using NodeOutList = std::vector<NodeDefBuilder::NodeOut>;\n     using DataTypeList = std::vector<DataType>;\n-    string string_in(\"test\");\n+    std::string string_in(\"test\");\n     NodeDefBuilder::NodeOut node_in{\"a\", 0, DT_STRING};\n     TF_ASSERT_OK(\n         NodeDefBuilder(\"test\", \"ParseSequenceExampleV2\")"
        },
        {
            "sha": "7d9e2dc7b4745de5851e42b6d60fc93141c3d75a",
            "filename": "tensorflow/core/ops/ragged_array_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fragged_array_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fragged_array_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fragged_array_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -106,7 +106,7 @@ REGISTER_OP(\"RaggedCross\")\n       int dense_start = num_ragged * 2 + num_sparse * 3;\n       for (int i = 0; i < dense_types.size(); ++i) {\n         ShapeHandle dense_input = c->input(i + dense_start);\n-        int32 rank = c->Rank(dense_input);\n+        int32_t rank = c->Rank(dense_input);\n         if (rank == InferenceContext::kUnknownRank) {\n           continue;\n         } else if (rank != 2) {"
        },
        {
            "sha": "016b35539805de0ea0ddf2abb0f7d88b595c9989",
            "filename": "tensorflow/core/ops/random_index_shuffle_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Frandom_index_shuffle_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Frandom_index_shuffle_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Frandom_index_shuffle_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -38,9 +38,9 @@ static absl::Status StatelessRandomPermuteShape(InferenceContext* c) {\n   TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &max_index_shape));\n \n   // Figure out if the output is a scalar or tensor.\n-  const int32 index_rank = c->Rank(index_shape);\n-  const int32 seed_rank = c->Rank(seed_shape);\n-  const int32 max_index_rank = c->Rank(max_index_shape);\n+  const int32_t index_rank = c->Rank(index_shape);\n+  const int32_t seed_rank = c->Rank(seed_shape);\n+  const int32_t max_index_rank = c->Rank(max_index_shape);\n \n   // Check that last dimension of seed is 3.\n   if (seed_rank == 1 && c->Value(c->Dim(seed_shape, 0)) != 3) {"
        },
        {
            "sha": "1f2cbafcea4c7d1ec59e2756e2a47e9f641216d8",
            "filename": "tensorflow/core/ops/random_ops_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Frandom_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Frandom_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Frandom_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -30,10 +30,10 @@ TEST(RandomOpsTest, Multinomial_ShapeFn) {\n   INFER_OK(op, \"[?,?];?\", \"[d0_0,?]\");\n   INFER_OK(op, \"[2,?];?\", \"[d0_0,?]\");\n   INFER_OK(op, \"[2,1];?\", \"[d0_0,?]\");\n-  Tensor num_samples = test::AsScalar<int32>(3);\n+  Tensor num_samples = test::AsScalar<int32_t>(3);\n   op.input_tensors[1] = &num_samples;\n   INFER_OK(op, \"[2,1];[]\", \"[d0_0,3]\");\n-  num_samples = test::AsTensor<int32>({1, 2, 3});\n+  num_samples = test::AsTensor<int32_t>({1, 2, 3});\n   INFER_ERROR(\"Shape must be rank 0 but is rank 1\", op, \"[2,1];[3]\");\n }\n \n@@ -45,7 +45,7 @@ TEST(RandomOpsTest, RandomGamma_ShapeFn) {\n   INFER_OK(op, \"?;[3]\", \"?\");\n   INFER_OK(op, \"[1];?\", \"?\");\n   INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"[1,2];[3,4]\");\n-  Tensor shape = test::AsTensor<int32>({1, 2, 3});\n+  Tensor shape = test::AsTensor<int32_t>({1, 2, 3});\n   op.input_tensors[0] = &shape;\n   INFER_OK(op, \"[3];[4,?]\", \"[1,2,3,d1_0,d1_1]\");\n   INFER_OK(op, \"[3];[4,5]\", \"[1,2,3,d1_0,d1_1]\");\n@@ -60,7 +60,7 @@ TEST(RandomOpsTest, RandomPoisson_ShapeFn) {\n   INFER_OK(op, \"?;[3]\", \"?\");\n   INFER_OK(op, \"[1];?\", \"?\");\n   INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"[1,2];[3,4]\");\n-  Tensor shape = test::AsTensor<int32>({1, 2, 3});\n+  Tensor shape = test::AsTensor<int32_t>({1, 2, 3});\n   op.input_tensors[0] = &shape;\n   INFER_OK(op, \"[3];[4,?]\", \"[1,2,3,d1_0,d1_1]\");\n   INFER_OK(op, \"[3];[4,5]\", \"[1,2,3,d1_0,d1_1]\");"
        },
        {
            "sha": "157d1f3dafb8e7739c4e8c6f7ba666f9867fe640",
            "filename": "tensorflow/core/ops/rnn_ops_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Frnn_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Frnn_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Frnn_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -23,10 +23,10 @@ limitations under the License.\n \n namespace tensorflow {\n \n-static string JoinedCopies(const string& s, int copies) {\n-  string res;\n+static std::string JoinedCopies(const std::string& s, int copies) {\n+  std::string res;\n   for (int i = 0; i < copies; ++i) {\n-    strings::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n+    absl::StrAppend(&res, i > 0 ? \";\" : \"\", s);\n   }\n   return res;\n }\n@@ -62,7 +62,7 @@ TEST(RnnOpsTest, LSTMBlockCell_ShapeFn) {\n   ShapeInferenceTestOp op(\"LSTMBlockCell\");\n \n   // Last 6 inputs don't affect shape inference.\n-  string input_suffix = strings::StrCat(\";\", JoinedCopies(\"?\", 6));\n+  std::string input_suffix = absl::StrCat(\";\", JoinedCopies(\"?\", 6));\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 2\", op, \"[?];?\" + input_suffix);\n@@ -77,7 +77,7 @@ TEST(RnnOpsTest, LSTMBlockCellGrad_ShapeFn) {\n   ShapeInferenceTestOp op(\"LSTMBlockCellGrad\");\n \n   // Last 14 inputs don't affect shape inference.\n-  string input_suffix = strings::StrCat(\";\", JoinedCopies(\"?\", 14));\n+  std::string input_suffix = absl::StrCat(\";\", JoinedCopies(\"?\", 14));\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 2\", op, \"[?];?\" + input_suffix);\n@@ -107,7 +107,7 @@ TEST(RnnOpsTest, BlockLSTM_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Middle inputs don't affect shape inference.\n-  string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n+  std::string infix = \";\" + JoinedCopies(\"?\", 6) + \";\";\n \n   // Rank checks.\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?]\" + infix + \"?\");\n@@ -147,7 +147,7 @@ TEST(RnnOpsTest, BlockLSTMGrad_ShapeFn) {\n                    .Finalize(&op.node_def));\n \n   // Last inputs don't affect shape inference.\n-  string suffix = \";\" + JoinedCopies(\"?\", 9);\n+  std::string suffix = \";\" + JoinedCopies(\"?\", 9);\n \n   // Rank check for x\n   INFER_ERROR(\"must be rank 3\", op, \"?;[?];?;?;?;?;?;?;?\" + suffix);\n@@ -167,11 +167,11 @@ TEST(RnnOpsTest, BlockLSTMGrad_ShapeFn) {\n       \"[?,?,?];\" + JoinedCopies(\"[?,?]\", 3) + \";\" + JoinedCopies(\"[?]\", 4));\n \n   // Output with copies input shapes to output.\n-  string input = strings::StrCat(\"?;[?,?,?];\", JoinedCopies(\"[?,?]\", 3), \";\",\n-                                 JoinedCopies(\"[?]\", 4), suffix);\n-  string expected = \"in1\";\n+  std::string input = strings::StrCat(\"?;[?,?,?];\", JoinedCopies(\"[?,?]\", 3),\n+                                      \";\", JoinedCopies(\"[?]\", 4), suffix);\n+  std::string expected = \"in1\";\n   for (int i = 1; i < 8; ++i) {\n-    strings::StrAppend(&expected, \";in\", (i + 1));\n+    absl::StrAppend(&expected, \";in\", i + 1);\n   }\n   INFER_OK(op, input, expected);\n }"
        },
        {
            "sha": "927e74a89f6db3fa0cede880f38f22dd3dd373fc",
            "filename": "tensorflow/core/ops/sparse_csr_matrix_ops_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fsparse_csr_matrix_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fsparse_csr_matrix_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fsparse_csr_matrix_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -189,7 +189,8 @@ TEST(SparseMatrixOpsTest, SparseMatrixAdd_ShapeFn) {\n   op.input_resource_handle_shapes_and_types.push_back(nullptr);\n   op.input_resource_handle_shapes_and_types.push_back(nullptr);\n   auto set_shapes = [&a_shapes_and_types, &b_shapes_and_types](\n-                        const string& a_shape, const string& b_shape) {\n+                        const std::string& a_shape,\n+                        const std::string& b_shape) {\n     a_shapes_and_types[0].first = a_shape;\n     b_shapes_and_types[0].first = b_shape;\n   };\n@@ -225,7 +226,8 @@ TEST(SparseMatrixOpsTest, SparseMatrixSparseMatMul_ShapeFn) {\n   op.input_resource_handle_shapes_and_types.push_back(&a_shapes_and_types);\n   op.input_resource_handle_shapes_and_types.push_back(&b_shapes_and_types);\n   auto set_shapes = [&a_shapes_and_types, &b_shapes_and_types](\n-                        const string& a_shape, const string& b_shape) {\n+                        const std::string& a_shape,\n+                        const std::string& b_shape) {\n     a_shapes_and_types[0].first = a_shape;\n     b_shapes_and_types[0].first = b_shape;\n   };\n@@ -323,7 +325,8 @@ TEST(SparseMatrixOpsTest, SparseMatrixSoftmaxGrad_ShapeFn) {\n   op.input_resource_handle_shapes_and_types.push_back(&a_shapes_and_types);\n   op.input_resource_handle_shapes_and_types.push_back(&b_shapes_and_types);\n   auto set_shapes = [&a_shapes_and_types, &b_shapes_and_types](\n-                        const string& a_shape, const string& b_shape) {\n+                        const std::string& a_shape,\n+                        const std::string& b_shape) {\n     a_shapes_and_types[0].first = a_shape;\n     b_shapes_and_types[0].first = b_shape;\n   };"
        },
        {
            "sha": "3ca5baa9351894e3f1655e535f9e82c5408e5e08",
            "filename": "tensorflow/core/ops/sparse_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fsparse_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fsparse_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fsparse_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -126,7 +126,7 @@ TEST(SparseOpsTest, SparseToDense_ShapeFn) {\n   INFER_OK(op, \"?;?;?;?\", \"?\");\n   INFER_OK(op, \"?;[?];?;?\", \"?\");\n   INFER_OK(op, \"?;[4];?;?\", \"[?,?,?,?]\");\n-  Tensor in_t = test::AsTensor<int32>({1, 2, 3, 4});\n+  Tensor in_t = test::AsTensor<int32_t>({1, 2, 3, 4});\n   op.input_tensors[1] = &in_t;\n   INFER_OK(op, \"?;[4];?;?\", \"[1,2,3,4]\");\n }"
        },
        {
            "sha": "f0e9b434c38196c6182657ac4f6537269d6beede",
            "filename": "tensorflow/core/ops/spectral_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fspectral_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fspectral_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fspectral_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -115,7 +115,7 @@ absl::Status RFFTShape(InferenceContext* c, const bool forward,\n       TF_RETURN_IF_ERROR(c->ReplaceDim(out, -rank + i, c->UnknownDim(), &out));\n     }\n   } else {\n-    auto fft_length_as_vec = fft_length_tensor->vec<int32>();\n+    auto fft_length_as_vec = fft_length_tensor->vec<int32_t>();\n     for (int i = 0; i < rank; ++i) {\n       // For RFFT, replace the last dimension with fft_length/2 + 1.\n       auto dim = forward && i == rank - 1 && fft_length_as_vec(i) != 0"
        },
        {
            "sha": "49de445d57fd0760129595b143569d442d41a2ba",
            "filename": "tensorflow/core/ops/spectral_ops_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fspectral_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fspectral_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fspectral_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -72,7 +72,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n \n     // Tests with known values for fft_length input.\n     op.input_tensors.resize(2);\n-    Tensor fft_length = test::AsTensor<int32>({10});\n+    Tensor fft_length = test::AsTensor<int32_t>({10});\n     op.input_tensors[1] = &fft_length;\n \n     // The inner-most dimension of the RFFT is n/2+1 while for IRFFT it's n.\n@@ -86,7 +86,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1];[1]\", \"[d0_0,10]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({11});\n+    fft_length = test::AsTensor<int32_t>({11});\n     if (forward) {\n       INFER_OK(op, \"[?];[1]\", \"[6]\");\n       INFER_OK(op, \"[1];[1]\", \"[6]\");\n@@ -97,7 +97,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1];[1]\", \"[d0_0,11]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({12});\n+    fft_length = test::AsTensor<int32_t>({12});\n     if (forward) {\n       INFER_OK(op, \"[?];[1]\", \"[7]\");\n       INFER_OK(op, \"[1];[1]\", \"[7]\");\n@@ -132,7 +132,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n \n     // Tests with known values for fft_length input.\n     op.input_tensors.resize(2);\n-    Tensor fft_length = test::AsTensor<int32>({9, 10});\n+    Tensor fft_length = test::AsTensor<int32_t>({9, 10});\n     op.input_tensors[1] = &fft_length;\n \n     // The inner-most dimension of the RFFT is n/2+1 while for IRFFT it's n.\n@@ -146,7 +146,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1];[2]\", \"[d0_0,9,10]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({10, 11});\n+    fft_length = test::AsTensor<int32_t>({10, 11});\n     if (forward) {\n       INFER_OK(op, \"[?,?];[2]\", \"[10,6]\");\n       INFER_OK(op, \"[1,1];[2]\", \"[10,6]\");\n@@ -157,7 +157,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1];[2]\", \"[d0_0,10,11]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({11, 12});\n+    fft_length = test::AsTensor<int32_t>({11, 12});\n     if (forward) {\n       INFER_OK(op, \"[?,?];[2]\", \"[11,7]\");\n       INFER_OK(op, \"[1,1];[2]\", \"[11,7]\");\n@@ -192,7 +192,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n \n     // Tests with known values for fft_length input.\n     op.input_tensors.resize(2);\n-    Tensor fft_length = test::AsTensor<int32>({10, 11, 12});\n+    Tensor fft_length = test::AsTensor<int32_t>({10, 11, 12});\n     op.input_tensors[1] = &fft_length;\n \n     // The inner-most dimension of the RFFT is n/2+1 while for IRFFT it's n.\n@@ -206,7 +206,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1,1];[3]\", \"[d0_0,10,11,12]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({11, 12, 13});\n+    fft_length = test::AsTensor<int32_t>({11, 12, 13});\n     if (forward) {\n       INFER_OK(op, \"[?,?,?];[3]\", \"[11,12,7]\");\n       INFER_OK(op, \"[1,1,1];[3]\", \"[11,12,7]\");\n@@ -217,7 +217,7 @@ TEST(MathOpsTest, RFFT_ShapeFn) {\n       INFER_OK(op, \"[1,1,1,1];[3]\", \"[d0_0,11,12,13]\");\n     }\n \n-    fft_length = test::AsTensor<int32>({12, 13, 14});\n+    fft_length = test::AsTensor<int32_t>({12, 13, 14});\n     if (forward) {\n       INFER_OK(op, \"[?,?,?];[3]\", \"[12,13,8]\");\n       INFER_OK(op, \"[1,1,1];[3]\", \"[12,13,8]\");"
        },
        {
            "sha": "702791f04ef090b095a5bd3fe0da4e4b1168d41b",
            "filename": "tensorflow/core/ops/string_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fstring_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Fstring_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Fstring_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -145,16 +145,16 @@ REGISTER_OP(\"StringFormat\")\n     .Attr(\"placeholder: string = '%s'\")\n     .Attr(\"summarize: int = 3\")\n     .SetShapeFn([](InferenceContext* c) {\n-      string template_;\n-      string placeholder;\n+      std::string template_;\n+      std::string placeholder;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"template\", &template_));\n       TF_RETURN_IF_ERROR(c->GetAttr(\"placeholder\", &placeholder));\n \n       std::vector<std::string> split_template;\n       split_template = absl::StrSplit(template_, placeholder);\n       int64_t num_placeholders = split_template.size() - 1;\n       if (c->num_inputs() != num_placeholders) {\n-        return errors::InvalidArgument(strings::StrCat(\n+        return errors::InvalidArgument(absl::StrCat(\n             \"num placeholders in template and num inputs must match: \",\n             num_placeholders, \" vs. \", c->num_inputs()));\n       }"
        },
        {
            "sha": "1f966749f8a3107afc8752285e3e4497d8f8534e",
            "filename": "tensorflow/core/ops/tpu_embedding_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Ftpu_embedding_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Ftpu_embedding_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Ftpu_embedding_ops.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -137,7 +137,7 @@ REGISTER_OP(\"EnqueueTPUEmbeddingSparseBatch\")\n     .Attr(\"combiners: list(string) = []\")\n     .SetIsStateful()\n     .SetShapeFn([](shape_inference::InferenceContext* c) -> absl::Status {\n-      std::vector<string> combiners;\n+      std::vector<std::string> combiners;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"combiners\", &combiners));\n       int n;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"N\", &n));"
        },
        {
            "sha": "b92f897d346946d32100ef7ac34c66e9e6150d2a",
            "filename": "tensorflow/core/ops/training_ops_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Ftraining_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec/tensorflow%2Fcore%2Fops%2Ftraining_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fops%2Ftraining_ops_test.cc?ref=e71ac2b3abcb4e6da677cda5a09caa7f9a0800ec",
            "patch": "@@ -20,9 +20,9 @@ limitations under the License.\n namespace tensorflow {\n \n // Used for testing the grad+indices handling for SparseApplyXYZ tests.\n-static void TestGradAndIndicesErrorHandling(const ShapeInferenceTestOp& op,\n-                                            string shape_spec_middle,\n-                                            const string& shape_spec_end = \"\") {\n+static void TestGradAndIndicesErrorHandling(\n+    const ShapeInferenceTestOp& op, std::string shape_spec_middle,\n+    const std::string& shape_spec_end = \"\") {\n   auto shape_spec = [&shape_spec_middle, shape_spec_end](\n                         const char* var_spec, const char* grad_indices_spec) {\n     return strings::StrCat(var_spec, \";\", shape_spec_middle, \";\","
        }
    ],
    "stats": {
        "total": 443,
        "additions": 224,
        "deletions": 219
    }
}