{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Implement vectorized reduce.\n\nPiperOrigin-RevId: 825027697",
    "sha": "de7a63363c1cb71beb7bd4d384a39101c17c9cec",
    "files": [
        {
            "sha": "993702d1d6fcbb655c874506bd55f72cc8dbef7d",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -198,6 +198,7 @@ cc_library(\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@llvm-project//mlir:VectorToLLVM\",\n         \"@llvm-project//mlir:VectorToSCF\",\n+        \"@llvm-project//mlir:VectorTransforms\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n         \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n         \"@stablehlo//:stablehlo_passes\","
        },
        {
            "sha": "6b668da55b9b69b7cbe5328ba5e888aa95b1614c",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -61,6 +61,8 @@ limitations under the License.\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/Dialect/Vector/Transforms/Passes.h\"\n+#include \"mlir/Dialect/Vector/Transforms/VectorRewritePatterns.h\"\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n@@ -244,15 +246,23 @@ static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n // The input IR is from the xtile dialect which uses tensors that are converted\n // first to the vector dialect and then to LLVM.\n static void AddTiledLoweringPasses(mlir::OpPassManager& pm) {\n+  pm.addPass(CreateShloToVectorPass());\n   pm.addPass(CreateXTileToVectorPass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n+  pm.addPass(CreateRewriteDynamicVectorExtractPass());\n   pm.addPass(CreateElementalTensorToVectorPass());\n-  pm.addPass(CreateShloToVectorPass());\n   pm.addPass(CreateLowerXTileEntryPass());\n+  pm.addNestedPass<mlir::func::FuncOp>(\n+      mlir::vector::createLowerVectorMultiReductionPass(\n+          mlir::vector::VectorMultiReductionLowering::InnerParallel));\n   pm.addPass(CreateTensorOpsToVectorPass());\n   pm.addPass(cpu::createLowerToLLVMPass());\n   pm.addPass(mlir::createConvertVectorToSCFPass(\n       mlir::VectorTransferToSCFOptions().enableFullUnroll(false)));\n-  pm.addPass(mlir::createConvertVectorToLLVMPass());\n+  mlir::ConvertVectorToLLVMPassOptions options;\n+  options.vectorTransposeLowering =\n+      mlir::vector::VectorTransposeLowering::Shuffle1D;\n+  pm.addPass(mlir::createConvertVectorToLLVMPass(options));\n \n   pm.addPass(mlir::createConvertComplexToStandardPass());\n   pm.addPass(mlir::memref::createExpandStridedMetadataPass());"
        },
        {
            "sha": "38a7c722e419a0b81d3fef09b13e9daa95e6e9db",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/tiled_kernel_test.py",
            "status": "modified",
            "additions": 138,
            "deletions": 0,
            "changes": 138,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -269,6 +269,144 @@ def test_dot_fusion_single_tile(self):\n         maxulp=5,\n     )\n \n+  def test_reduction_add_inner(self):\n+    ir = \"\"\"\n+      module @reduction_add_inner {\n+        xtile.entry_func @reduction_add_inner(\n+            %input: memref<1024x32xf32>,\n+            %init: memref<f32>,\n+            %output: memref<1024xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:128, tiles_per_workgroup:32>} {\n+          %c_0 = arith.constant 0 : index\n+          %c_8 = arith.constant 8 : index\n+          %init_tile = xtile.extract %init[][][] : memref<f32> -> tensor<f32>\n+          %index = arith.muli %tile_id, %c_8 : index\n+          %input_tile = xtile.extract %input[%index, %c_0][8, 32][1, 1] : memref<1024x32xf32> -> tensor<8x32xf32>\n+          %result = stablehlo.reduce(%input_tile init: %init_tile)\n+                    across dimensions = [1]\n+                    : (tensor<8x32xf32>, tensor<f32>) -> tensor<8xf32>\n+            reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+              %add = arith.addf %arg0, %arg1 : tensor<f32>\n+              stablehlo.return %add : tensor<f32>\n+            }\n+          xtile.insert %result into %output[%index][8][1] : tensor<8xf32> -> memref<1024xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"reduction_add_inner\",\n+        4,\n+        [(1024, 32), (1,)],\n+        (1024,),\n+        np.int32,\n+        lambda input, init: np.sum(input, axis=1) + init,\n+    )\n+\n+  def test_reduction_add_outer(self):\n+    ir = \"\"\"\n+      module @reduction_add_outer {\n+        xtile.entry_func @reduction_add_outer(\n+            %input: memref<1024x32xf32>,\n+            %init: memref<f32>,\n+            %output: memref<32xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:4, tiles_per_workgroup:1>} {\n+          %c_0 = arith.constant 0 : index\n+          %c_8 = arith.constant 8 : index\n+          %init_tile = xtile.extract %init[][][] : memref<f32> -> tensor<f32>\n+          %index = arith.muli %tile_id, %c_8 : index\n+          %input_tile = xtile.extract %input[%c_0, %index][1024, 8][1, 1] : memref<1024x32xf32> -> tensor<1024x8xf32>\n+          %result = stablehlo.reduce(%input_tile init: %init_tile)\n+                    across dimensions = [0]\n+                    : (tensor<1024x8xf32>, tensor<f32>) -> tensor<8xf32>\n+            reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+              %add = arith.addf %arg0, %arg1 : tensor<f32>\n+              stablehlo.return %add : tensor<f32>\n+            }\n+          xtile.insert %result into %output[%index][8][1] : tensor<8xf32> -> memref<32xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"reduction_add_outer\",\n+        4,\n+        [(1024, 32), (1,)],\n+        (32,),\n+        np.float32,\n+        lambda input, init: np.sum(input, axis=0) + init,\n+    )\n+\n+  def test_reduction_middle(self):\n+    ir = \"\"\"\n+      module @reduction_add_middle {\n+        xtile.entry_func @reduction_add_middle(\n+            %input: memref<8x4x2xf32>,\n+            %init: memref<f32>,\n+            %output: memref<8x2xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:1, tiles_per_workgroup:1>} {\n+          %init_val = xtile.extract %init[][][] : memref<f32> -> tensor<f32>\n+          %input_tile = xtile.extract %input[%tile_id, %tile_id, %tile_id][8, 4, 2][1, 1, 1] : memref<8x4x2xf32> -> tensor<8x4x2xf32>\n+          %result = stablehlo.reduce(%input_tile init: %init_val)\n+                    across dimensions = [1]\n+                    : (tensor<8x4x2xf32>, tensor<f32>) -> tensor<8x2xf32>\n+            reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+              %add = arith.addf %arg0, %arg1 : tensor<f32>\n+              stablehlo.return %add : tensor<f32>\n+            }\n+          xtile.insert %result into %output[%tile_id, %tile_id][8, 2][1, 1] : tensor<8x2xf32> -> memref<8x2xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"reduction_add_middle\",\n+        1,\n+        [(8, 4, 2), (1,)],\n+        (8, 2),\n+        np.float32,\n+        lambda input, init: np.sum(input, axis=1) + init,\n+    )\n+\n+  def test_reduction_outer_inner(self):\n+    ir = \"\"\"\n+      module @reduction_add_outer_inner {\n+        xtile.entry_func @reduction_add_outer_inner(\n+            %input: memref<8x4x2xf32>,\n+            %init: memref<f32>,\n+            %output: memref<4xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:1, tiles_per_workgroup:1>} {\n+          %init_val = xtile.extract %init[][][] : memref<f32> -> tensor<f32>\n+          %input_tile = xtile.extract %input[%tile_id, %tile_id, %tile_id][8, 4, 2][1, 1, 1] : memref<8x4x2xf32> -> tensor<8x4x2xf32>\n+          %result = stablehlo.reduce(%input_tile init: %init_val)\n+                    across dimensions = [0, 2]\n+                    : (tensor<8x4x2xf32>, tensor<f32>) -> tensor<4xf32>\n+            reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+              %add = arith.addf %arg0, %arg1 : tensor<f32>\n+              stablehlo.return %add : tensor<f32>\n+            }\n+          xtile.insert %result into %output[%tile_id][4][1] : tensor<4xf32> -> memref<4xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"reduction_add_outer_inner\",\n+        1,\n+        [(8, 4, 2), (1,)],\n+        (4,),\n+        np.float32,\n+        lambda input, init: np.sum(input, axis=(0, 2)) + init,\n+    )\n+\n \n if __name__ == \"__main__\":\n   absltest.main()"
        },
        {
            "sha": "2f52982cad6f3ff788a87c146d589cfc4bf0db21",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -36,7 +36,9 @@ cc_library(\n     hdrs = [\"lowering_utils.h\"],\n     visibility = [\"//visibility:private\"],\n     deps = [\n+        \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:MemRefDialect\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:VectorDialect\",\n@@ -57,10 +59,14 @@ cc_library(\n     deps = [\n         \":lowering_utils\",\n         \":passes_inc_gen\",\n+        \":vectorized_reduce_emitter\",\n         \"//xla/backends/cpu/codegen/emitters/ir:xla_cpu\",\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//llvm:Support\",\n@@ -72,6 +78,7 @@ cc_library(\n         \"@llvm-project//mlir:FuncTransforms\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:LLVMDialect\",\n+        \"@llvm-project//mlir:LinalgTransforms\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MathOpsIncGen\",\n         \"@llvm-project//mlir:MemRefDialect\",\n@@ -86,3 +93,24 @@ cc_library(\n         \"@stablehlo//:stablehlo_ops\",\n     ],\n )\n+\n+cc_library(\n+    name = \"vectorized_reduce_emitter\",\n+    srcs = [\"vectorized_reduce_emitter.cc\"],\n+    hdrs = [\"vectorized_reduce_emitter.h\"],\n+    deps = [\n+        \":lowering_utils\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:LinalgTransforms\",\n+        \"@llvm-project//mlir:MemRefDialect\",\n+        \"@llvm-project//mlir:SCFDialect\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@llvm-project//mlir:VectorDialect\",\n+    ],\n+)"
        },
        {
            "sha": "89163807f8b95a237d3503a2961840a4d7d59061",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/lowering_utils.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.cc?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -15,19 +15,20 @@ limitations under the License.\n \n #include \"xla/backends/cpu/codegen/tiled/transforms/lowering_utils.h\"\n \n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Support/LLVM.h\"\n \n namespace xla::cpu {\n \n-mlir::VectorType GetVectorType(mlir::RankedTensorType tensor_type) {\n-  return mlir::VectorType::get(tensor_type.getShape(),\n-                               tensor_type.getElementType());\n+mlir::VectorType GetVectorType(mlir::ShapedType type) {\n+  return mlir::VectorType::get(type.getShape(), type.getElementType());\n }\n \n mlir::TypedValue<mlir::VectorType> CastToVector(mlir::OpBuilder& builder,\n@@ -45,9 +46,8 @@ mlir::TypedValue<mlir::VectorType> CastToVector(mlir::OpBuilder& builder,\n   return mlir::cast<mlir::TypedValue<mlir::VectorType>>(cast_op.getResult(0));\n }\n \n-mlir::RankedTensorType GetTensorType(mlir::VectorType vector_type) {\n-  return mlir::RankedTensorType::get(vector_type.getShape(),\n-                                     vector_type.getElementType());\n+mlir::RankedTensorType GetTensorType(mlir::ShapedType type) {\n+  return mlir::RankedTensorType::get(type.getShape(), type.getElementType());\n }\n \n mlir::TypedValue<mlir::RankedTensorType> CastToTensor(mlir::OpBuilder& builder,\n@@ -66,4 +66,12 @@ mlir::TypedValue<mlir::RankedTensorType> CastToTensor(mlir::OpBuilder& builder,\n       cast_op.getResult(0));\n }\n \n+mlir::TypedValue<mlir::MemRefType> CreateBufferOfShape(mlir::OpBuilder& builder,\n+                                                       mlir::Location loc,\n+                                                       mlir::ShapedType shape) {\n+  mlir::MemRefType memrefType =\n+      mlir::MemRefType::get(shape.getShape(), shape.getElementType());\n+  return mlir::memref::AllocaOp::create(builder, loc, memrefType);\n+}\n+\n }  // namespace xla::cpu"
        },
        {
            "sha": "9b6e43716e76bc3b3504374adce7065a427786d7",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/lowering_utils.h",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flowering_utils.h?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -18,13 +18,14 @@ limitations under the License.\n \n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/Location.h\"\n #include \"mlir/IR/Value.h\"\n \n namespace xla::cpu {\n \n // Get the vector type that has the same shape and element type as the tensor\n // type.\n-mlir::VectorType GetVectorType(mlir::RankedTensorType tensor_type);\n+mlir::VectorType GetVectorType(mlir::ShapedType tensor_type);\n \n // Cast the input to a vector value.\n // If the input is a scalar it will be simply constructed as a\n@@ -36,7 +37,7 @@ mlir::TypedValue<mlir::VectorType> CastToVector(mlir::OpBuilder& builder,\n \n // Get the tensor type that has the same shape and element type as the vector\n // type.\n-mlir::RankedTensorType GetTensorType(mlir::VectorType vector_type);\n+mlir::RankedTensorType GetTensorType(mlir::ShapedType vector_type);\n \n // Cast the input to a tensor value.\n // If the input is a scalar it will be simply constructed as a\n@@ -46,6 +47,10 @@ mlir::RankedTensorType GetTensorType(mlir::VectorType vector_type);\n mlir::TypedValue<mlir::RankedTensorType> CastToTensor(mlir::OpBuilder& builder,\n                                                       mlir::Value input);\n \n+mlir::TypedValue<mlir::MemRefType> CreateBufferOfShape(mlir::OpBuilder& builder,\n+                                                       mlir::Location loc,\n+                                                       mlir::ShapedType shape);\n+\n }  // namespace xla::cpu\n \n #endif  // XLA_BACKENDS_CPU_CODEGEN_TILED_TRANSFORMS_LOWERING_UTILS_H_"
        },
        {
            "sha": "de620ccec8c6ddc37f09337a1a361034994b7944",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -49,6 +49,8 @@ def ShloToVectorPass : Pass<\"xtile-cpu-shlo-to-vector\", \"mlir::ModuleOp\"> {\n     \"mlir::tensor::TensorDialect\",\n     \"mlir::vector::VectorDialect\",\n     \"mlir::stablehlo::StablehloDialect\",\n+    \"mlir::scf::SCFDialect\",\n+    \"mlir::memref::MemRefDialect\",\n   ];\n }\n "
        },
        {
            "sha": "6c16b283a960e18d0ae2a49df3613f6aa8e437f6",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/shlo_to_vector.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 2,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -18,20 +18,25 @@ limitations under the License.\n #include <memory>\n #include <utility>\n \n+#include \"absl/algorithm/container.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Dialect/SCF/Utils/Utils.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/IR/Location.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/Visitors.h\"\n@@ -41,6 +46,7 @@ limitations under the License.\n #include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/cpu/codegen/tiled/transforms/lowering_utils.h\"\n #include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.h\"\n \n namespace xla::cpu {\n \n@@ -218,14 +224,56 @@ struct LowerTranspose : mlir::OpRewritePattern<mlir::stablehlo::TransposeOp> {\n   }\n };\n \n+// Lower stablehlo.reduce to vector operations.\n+//\n+\n+struct LowerReduce : mlir::OpRewritePattern<mlir::stablehlo::ReduceOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::stablehlo::ReduceOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    if (op.getNumResults() != 1) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"reduce op with multiple results is not supported\");\n+    }\n+\n+    mlir::TypedValue<mlir::VectorType> source_vector =\n+        CastToVector(rewriter, op.getInputs().front());\n+    mlir::VectorType source_vector_type = source_vector.getType();\n+\n+    mlir::Value init_value = rewriter.create<mlir::tensor::ExtractOp>(\n+        op->getLoc(), source_vector_type.getElementType(),\n+        op.getInitValues().front());\n+\n+    mlir::Value result_tensor = op.getResult(0);\n+    auto result_tensor_type =\n+        mlir::cast<mlir::RankedTensorType>(result_tensor.getType());\n+    auto result_vector_type = GetVectorType(result_tensor_type);\n+\n+    // Ensure the reduction dimensions are sorted so we can easily check if the\n+    // minor dimension is reduced.\n+    llvm::SmallVector<int64_t> reduction_dims(op.getDimensions());\n+    absl::c_sort(reduction_dims);\n+\n+    mlir::Value reduced_vector = EmitVectorizedReduction(\n+        rewriter, op->getLoc(), result_vector_type, source_vector, init_value,\n+        reduction_dims, op.getBody().front());\n+\n+    rewriter.replaceOp(op, CastToTensor(rewriter, reduced_vector));\n+\n+    return mlir::success();\n+  }\n+};\n+\n class ShloToVectorPass : public impl::ShloToVectorPassBase<ShloToVectorPass> {\n  public:\n   using ShloToVectorPassBase::ShloToVectorPassBase;\n \n   void runOnOperation() override {\n     mlir::MLIRContext* context = &getContext();\n     mlir::RewritePatternSet patterns(context);\n-    patterns.add<LowerTranspose, LowerDotGeneral>(context);\n+    patterns.add<LowerTranspose, LowerDotGeneral, LowerReduce>(context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       signalPassFailure();"
        },
        {
            "sha": "4fb3c334ca1f36cad9d41a8da98314c581c78050",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/shlo_to_vector.mlir",
            "status": "modified",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -37,3 +37,107 @@ func.func @dot_scalar_output(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -\n   // CHECK: return %[[RESULT_TENSOR]] : tensor<f32>\n   return %result : tensor<f32>\n }\n+\n+// -----\n+\n+\n+func.func @reduce_outer(%input : tensor<1024x32xf32>, %init : tensor<f32>) -> tensor<32xf32> {\n+  %result = stablehlo.reduce(%input init: %init) across dimensions = [0] : (tensor<1024x32xf32>, tensor<f32>) -> tensor<32xf32>\n+    reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+      %add = arith.addf %arg0, %arg1 : tensor<f32>\n+      stablehlo.return %add : tensor<f32>\n+    }\n+  return %result : tensor<32xf32>\n+}\n+\n+// CHECK: func.func @reduce_outer\n+// CHECK:   memref.alloca() : memref<32xf32>\n+// CHECK:   vector.extract %{{.*}} : vector<32xf32> from vector<1024x32xf32>\n+// CHECK:   scf.for\n+// CHECK:     vector.extract %{{.*}} : vector<32xf32> from vector<1024x32xf32>\n+// CHECK:     arith.addf {{.*}} : vector<32xf32>\n+// CHECK:     scf.yield %{{.*}} : vector<32xf32>\n+// CHECK:   }\n+// CHECK:   vector.transfer_write %{{.*}} : vector<32xf32>, memref<32xf32>\n+// CHECK:   vector.transfer_read %{{.*}} : memref<32xf32>, vector<32xf32>\n+// CHECK:   vector.broadcast %{{.*}} : f32 to vector<32xf32>\n+// CHECK:   arith.addf {{.*}} : vector<32xf32>\n+\n+// -----\n+\n+\n+func.func @reduce_inner(%input : tensor<1024x32xf32>, %init : tensor<f32>) -> tensor<1024xf32> {\n+  %result = stablehlo.reduce(%input init: %init) across dimensions = [1] : (tensor<1024x32xf32>, tensor<f32>) -> tensor<1024xf32>\n+    reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+      %add = arith.addf %arg0, %arg1 : tensor<f32>\n+      stablehlo.return %add : tensor<f32>\n+    }\n+  return %result : tensor<1024xf32>\n+}\n+\n+// CHECK: func.func @reduce_inner\n+// CHECK:   memref.alloca() : memref<1024xf32>\n+// CHECK:   scf.for\n+// CHECK:     vector.extract {{.*}} : vector<32xf32> from vector<1024x32xf32>\n+// CHECK:     vector.reduction <add>, {{.*}} : vector<32xf32> into f32\n+// CHECK:     memref.store {{.*}} : memref<1024xf32>\n+// CHECK:   }\n+// CHECK:   vector.transfer_read {{.*}} : memref<1024xf32>, vector<1024xf32>\n+\n+// -----\n+\n+func.func @reduce_middle(%input : tensor<1024x32x8xf32>, %init : tensor<f32>) -> tensor<1024x8xf32> {\n+  %result = stablehlo.reduce(%input init: %init) across dimensions = [1] : (tensor<1024x32x8xf32>, tensor<f32>) -> tensor<1024x8xf32>\n+    reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+      %add = arith.addf %arg0, %arg1 : tensor<f32>\n+      stablehlo.return %add : tensor<f32>\n+    }\n+  return %result : tensor<1024x8xf32>\n+}\n+\n+// CHECK: func.func @reduce_middle\n+// CHECK:   memref.alloca() : memref<1024x8xf32>\n+// CHECK:   scf.for\n+// CHECK:     vector.extract {{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:     scf.for\n+// CHECK:       vector.extract %{{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:       arith.addf {{.*}} : vector<8xf32>\n+// CHECK:       scf.yield {{.*}} : vector<8xf32>\n+// CHECK:     }\n+// CHECK:     vector.transfer_write {{.*}} : vector<8xf32>, memref<1024x8xf32>\n+// CHECK:   }\n+// CHECK:   vector.transfer_read {{.*}} : memref<1024x8xf32>, vector<1024x8xf32>\n+// CHECK:   vector.broadcast {{.*}} : f32 to vector<1024x8xf32>\n+// CHECK:   arith.addf {{.*}} : vector<1024x8xf32>\n+// CHECK: }\n+\n+// -----\n+\n+func.func @reduce_outer_and_inner(%input : tensor<1024x32x8xf32>, %init : tensor<f32>) -> tensor<32xf32> {\n+  %result = stablehlo.reduce(%input init: %init) across dimensions = [0, 2] : (tensor<1024x32x8xf32>, tensor<f32>) -> tensor<32xf32>\n+    reducer(%arg0: tensor<f32>, %arg1: tensor<f32>) {\n+      %add = arith.addf %arg0, %arg1 : tensor<f32>\n+      stablehlo.return %add : tensor<f32>\n+    }\n+  return %result : tensor<32xf32>\n+}\n+\n+// CHECK: func.func @reduce_outer_and_inner\n+// CHECK:   %[[BUFFER0:.*]] = memref.alloca() : memref<32x8xf32>\n+// CHECK:   scf.for\n+// CHECK:     vector.extract %{{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:     scf.for\n+// CHECK:       vector.extract %{{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:       arith.addf %{{.*}} : vector<8xf32>\n+// CHECK:       scf.yield {{.*}} : vector<8xf32>\n+// CHECK:     }\n+// CHECK:     vector.transfer_write {{.*}} : vector<8xf32>, memref<32x8xf32>\n+// CHECK:   }\n+// CHECK:   %[[BUFFER1:.*]] = memref.alloca() : memref<32xf32>\n+// CHECK:   scf.for\n+// CHECK:     vector.transfer_read %[[BUFFER0]]{{.*}} : memref<32x8xf32>, vector<8xf32>\n+// CHECK:     vector.reduction <add>, {{.*}} : vector<8xf32> into f32\n+// CHECK:     memref.store %{{.*}}, %[[BUFFER1]]{{.*}} : memref<32xf32>\n+// CHECK:   }\n+// CHECK:   vector.transfer_read %[[BUFFER1]]{{.*}} : memref<32xf32>, vector<32xf32>\n+// CHECK: }"
        },
        {
            "sha": "3d793a4d909747e0ac54ec921f712c9567bef1dd",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.cc",
            "status": "added",
            "additions": 361,
            "deletions": 0,
            "changes": 361,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -0,0 +1,361 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.h\"\n+\n+#include <algorithm>\n+#include <array>\n+#include <cstdint>\n+#include <optional>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/SmallVectorExtras.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Linalg/Transforms/Transforms.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/lowering_utils.h\"\n+\n+namespace xla::cpu {\n+\n+static absl::StatusOr<mlir::vector::CombiningKind> GetCombiningKind(\n+    mlir::Block& reduction_body) {\n+  mlir::Operation* op =\n+      reduction_body.getTerminator()->getOperand(0).getDefiningOp();\n+  if (!op) {\n+    return absl::InternalError(\"No reduction combiner\");\n+  }\n+\n+  for (mlir::Value operand : op->getOperands()) {\n+    if (operand.getDefiningOp()) {\n+      return absl::InternalError(\"Non trivial reduction combiner\");\n+    }\n+  }\n+\n+  if (auto kind = mlir::linalg::getCombinerOpKind(op)) {\n+    return *kind;\n+  }\n+\n+  return absl::InternalError(\"Unsupported reduction combiner\");\n+}\n+\n+static mlir::Value ExtractVector(mlir::OpBuilder& builder, mlir::Location loc,\n+                                 mlir::Value source, mlir::ValueRange indices) {\n+  return mlir::vector::ExtractOp::create(\n+      builder, loc, source, llvm::map_to_vector(indices, [](mlir::Value idx) {\n+        return mlir::OpFoldResult(idx);\n+      }));\n+}\n+\n+static void InsertVectorIntoBuffer(mlir::OpBuilder& builder, mlir::Location loc,\n+                                   mlir::Value value,\n+                                   mlir::TypedValue<mlir::MemRefType> buffer,\n+                                   mlir::ValueRange indices) {\n+  llvm::SmallVector<mlir::Value> padded_indices(indices);\n+  while (padded_indices.size() < buffer.getType().getRank()) {\n+    padded_indices.push_back(\n+        builder.create<mlir::arith::ConstantIndexOp>(loc, 0));\n+  }\n+\n+  if (mlir::isa<mlir::VectorType>(value.getType())) {\n+    mlir::vector::TransferWriteOp::create(builder, loc, value, buffer,\n+                                          padded_indices);\n+  } else {\n+    mlir::memref::StoreOp::create(builder, loc, value, buffer, padded_indices);\n+  }\n+}\n+\n+static mlir::TypedValue<mlir::VectorType> ExtractVectorFromBuffer(\n+    mlir::OpBuilder& builder, mlir::Location loc,\n+    mlir::TypedValue<mlir::MemRefType> buffer, mlir::ValueRange indices = {}) {\n+  llvm::SmallVector<mlir::Value> padded_indices(indices);\n+  while (padded_indices.size() < buffer.getType().getRank()) {\n+    padded_indices.push_back(\n+        builder.create<mlir::arith::ConstantIndexOp>(loc, 0));\n+  }\n+  mlir::VectorType vector_type = mlir::VectorType::get(\n+      buffer.getType().getShape().drop_front(indices.size()),\n+      buffer.getType().getElementType());\n+  return mlir::vector::TransferReadOp::create(builder, loc, vector_type, buffer,\n+                                              padded_indices,\n+                                              /*padding=*/std::nullopt);\n+}\n+\n+static std::array<llvm::SmallVector<mlir::Value>, 3> GetLoopBounds(\n+    mlir::OpBuilder& builder, mlir::Location loc,\n+    llvm::ArrayRef<int64_t> upper_bounds, int64_t lower_bound = 0) {\n+  llvm::SmallVector<mlir::Value> lbs(\n+      upper_bounds.size(),\n+      builder.create<mlir::arith::ConstantIndexOp>(loc, lower_bound));\n+  llvm::SmallVector<mlir::Value> ubs =\n+      llvm::map_to_vector(upper_bounds, [&](int64_t size) -> mlir::Value {\n+        return builder.create<mlir::arith::ConstantIndexOp>(loc, size);\n+      });\n+  llvm::SmallVector<mlir::Value> step(\n+      upper_bounds.size(),\n+      builder.create<mlir::arith::ConstantIndexOp>(loc, 1));\n+  return {lbs, ubs, step};\n+}\n+\n+mlir::Value VectorizeBody(mlir::OpBuilder& builder, mlir::Location loc,\n+                          mlir::Block& old_body, mlir::Value lhs_vector,\n+                          mlir::Value rhs_vector) {\n+  mlir::IRMapping mapping;\n+\n+  mapping.map(old_body.getArgument(0), lhs_vector);\n+  mapping.map(old_body.getArgument(1), rhs_vector);\n+\n+  for (mlir::Operation& op : old_body.without_terminator()) {\n+    // TODO(willfroom): Check\n+    // mlir::OpTrait::hasElementwiseMappableTraits\n+    auto new_operands = llvm::map_to_vector(\n+        op.getOperands(),\n+        [&](mlir::Value operand) { return mapping.lookup(operand); });\n+    mlir::Operation* new_op = op.create(\n+        loc, op.getName(), {lhs_vector.getType()}, new_operands, op.getAttrs(),\n+        op.getPropertiesStorage(), op.getSuccessors(), op.getNumRegions());\n+    mapping.map(&op, new_op);\n+    for (auto [old_res, new_res] :\n+         llvm::zip(op.getResults(), new_op->getResults())) {\n+      mapping.map(old_res, new_res);\n+    }\n+    builder.insert(new_op);\n+  }\n+  return mapping.lookup(old_body.getTerminator()->getOperand(0));\n+}\n+\n+mlir::Value EmitNonMinorReduction(\n+    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n+    mlir::TypedValue<mlir::VectorType> source_vector,\n+    llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body,\n+    bool minor_dim_reduced) {\n+  mlir::VectorType source_vector_type = source_vector.getType();\n+  int64_t rank = source_vector_type.getRank();\n+  int64_t minor_dim = rank - 1;\n+  int64_t minor_dim_size = source_vector_type.getDimSize(minor_dim);\n+  llvm::SmallVector<int64_t> non_reduced_dims(rank);\n+  absl::c_iota(non_reduced_dims, 0);\n+  non_reduced_dims.erase(\n+      std::remove_if(non_reduced_dims.begin(), non_reduced_dims.end(),\n+                     [&](int64_t dim) {\n+                       return absl::c_find(reduction_dims, dim) !=\n+                              reduction_dims.end();\n+                     }),\n+      non_reduced_dims.end());\n+\n+  // The set of non-reduced dimensions that are not the minor dimension.\n+  llvm::SmallVector<int64_t> non_reduced_non_minor_dims(non_reduced_dims);\n+  if (auto itr = absl::c_find(non_reduced_non_minor_dims, minor_dim);\n+      itr != non_reduced_non_minor_dims.end()) {\n+    non_reduced_non_minor_dims.erase(itr);\n+  }\n+\n+  // The set of reduced dimensions that are not the minor dimension.\n+  llvm::SmallVector<int64_t> non_minor_reduced_dims(reduction_dims);\n+  if (auto itr = absl::c_find(non_minor_reduced_dims, minor_dim);\n+      itr != non_minor_reduced_dims.end()) {\n+    non_minor_reduced_dims.erase(itr);\n+  }\n+\n+  // The shape of the of the non-minor-reduced output.\n+  llvm::SmallVector<int64_t> output_shape(result_type.getShape());\n+  if (minor_dim_reduced) {\n+    output_shape.push_back(minor_dim_size);\n+  }\n+  auto output_buffer_shape =\n+      mlir::MemRefType::get(output_shape, result_type.getElementType());\n+  auto buffer = CreateBufferOfShape(builder, loc, output_buffer_shape);\n+\n+  auto get_source_vector_dim_size = [&](llvm::ArrayRef<int64_t> dims) {\n+    return llvm::map_to_vector(\n+        dims, [&](int64_t dim) { return source_vector_type.getDimSize(dim); });\n+  };\n+\n+  // Outer loop is non-minor non-reduced dimensions.\n+  auto [lbs, ubs, step] = GetLoopBounds(\n+      builder, loc, get_source_vector_dim_size(non_reduced_non_minor_dims));\n+\n+  mlir::scf::buildLoopNest(\n+      builder, loc, lbs, ubs, step,\n+      [&](mlir::OpBuilder& builder, mlir::Location loc,\n+          mlir::ValueRange outer_induction_vars) {\n+        auto [lbs, ubs, step] = GetLoopBounds(\n+            builder, loc, get_source_vector_dim_size(non_minor_reduced_dims),\n+            1);\n+\n+        llvm::SmallVector<mlir::Value> zeroth_step_indices(\n+            rank - 1, mlir::arith::ConstantIndexOp::create(builder, loc, 0));\n+        for (auto [idx, var] :\n+             llvm::zip(non_reduced_non_minor_dims, outer_induction_vars)) {\n+          zeroth_step_indices[idx] = var;\n+        }\n+        // Get the first iteration\n+        mlir::Value minor_accumilator =\n+            ExtractVector(builder, loc, source_vector, zeroth_step_indices);\n+        // Inner loop is the non-minor reduced dimension.\n+        mlir::scf::LoopNest loop_nest = mlir::scf::buildLoopNest(\n+            builder, loc, lbs, ubs, step, minor_accumilator,\n+            [&](mlir::OpBuilder& builder, mlir::Location loc,\n+                mlir::ValueRange inner_induction_vars,\n+                mlir::ValueRange minor_accumilator)\n+                -> mlir::SmallVector<mlir::Value> {\n+              llvm::SmallVector<mlir::Value> indices(rank - 1);\n+              for (auto [idx, var] : llvm::zip(non_reduced_non_minor_dims,\n+                                               outer_induction_vars)) {\n+                indices[idx] = var;\n+              }\n+              for (auto [idx, var] :\n+                   llvm::zip(non_minor_reduced_dims, inner_induction_vars)) {\n+                indices[idx] = var;\n+              }\n+\n+              mlir::Value vector_slice =\n+                  ExtractVector(builder, loc, source_vector, indices);\n+\n+              return {VectorizeBody(builder, loc, body, vector_slice,\n+                                    minor_accumilator.front())};\n+            });\n+\n+        InsertVectorIntoBuffer(builder, loc, loop_nest.results.front(), buffer,\n+                               outer_induction_vars);\n+        return;\n+      });\n+\n+  // If the minor dimension is also reduced then it extracts directly from the\n+  // buffer to avoid the additional vector -> subvector operation.\n+  if (minor_dim_reduced) {\n+    return buffer;\n+  }\n+\n+  return ExtractVectorFromBuffer(builder, loc, buffer);\n+}\n+\n+mlir::TypedValue<mlir::VectorType> EmitMinorReduction(\n+    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n+    mlir::Value input, mlir::Value init_value, mlir::Block& body) {\n+  absl::StatusOr<mlir::vector::CombiningKind> kind_or = GetCombiningKind(body);\n+  if (!kind_or.ok()) {\n+    body.getParentOp()->emitRemark() << kind_or.status().ToString();\n+  }\n+\n+  // TODO(willfroom): we could reuse the non minor result buffer.\n+  auto minor_result_buffer = CreateBufferOfShape(builder, loc, result_type);\n+  auto maybe_input_buffer =\n+      mlir::dyn_cast<mlir::TypedValue<mlir::MemRefType>>(input);\n+\n+  auto maybe_input_type =\n+      llvm::TypeSwitch<mlir::Type, std::optional<mlir::ShapedType>>(\n+          input.getType())\n+          .Case<mlir::MemRefType>([&](auto op) { return input.getType(); })\n+          .Case<mlir::VectorType>([&](auto op) { return input.getType(); })\n+          .Default([&](auto op) { return std::nullopt; });\n+\n+  if (!maybe_input_type.has_value()) {\n+    return nullptr;\n+  }\n+\n+  int64_t minor_dim_size = maybe_input_type->getShape().back();\n+\n+  auto [lbs, ubs, step] = GetLoopBounds(builder, loc, result_type.getShape());\n+\n+  mlir::scf::buildLoopNest(\n+      builder, loc, lbs, ubs, step,\n+      [&](mlir::OpBuilder& builder, mlir::Location loc,\n+          mlir::ValueRange induction_vars) {\n+        mlir::Value vector_slice =\n+            maybe_input_buffer\n+                ? ExtractVectorFromBuffer(builder, loc, maybe_input_buffer,\n+                                          induction_vars)\n+                : ExtractVector(builder, loc, input, induction_vars);\n+\n+        if (kind_or.ok()) {\n+          // TODO(willfroom): Investigate tree-reduction to split the reduction\n+          // op into natural sizes (2, 4, 8, 16, ...) and then remove the\n+          // reassociation flag.\n+          mlir::Value reduced_scalar =\n+              builder.create<mlir::vector::ReductionOp>(\n+                  loc, *kind_or, vector_slice, init_value,\n+                  mlir::arith::FastMathFlags::reassoc);\n+          InsertVectorIntoBuffer(builder, loc, reduced_scalar,\n+                                 minor_result_buffer, induction_vars);\n+          return;\n+        }\n+\n+        auto [lbs, ubs, step] = GetLoopBounds(builder, loc, {minor_dim_size});\n+        mlir::scf::LoopNest minor_reduction_loop = mlir::scf::buildLoopNest(\n+            builder, loc, lbs, ubs, step, {init_value},\n+            [&](mlir::OpBuilder& builder, mlir::Location loc,\n+                mlir::ValueRange index, mlir::ValueRange carry_value)\n+                -> mlir::SmallVector<mlir::Value> {\n+              mlir::Value element =\n+                  ExtractVector(builder, loc, vector_slice, index);\n+              return {VectorizeBody(builder, loc, body, element,\n+                                    carry_value.front())};\n+            });\n+\n+        InsertVectorIntoBuffer(builder, loc,\n+                               minor_reduction_loop.results.front(),\n+                               minor_result_buffer, induction_vars);\n+        return;\n+      });\n+\n+  return ExtractVectorFromBuffer(builder, loc, minor_result_buffer);\n+}\n+\n+mlir::Value EmitVectorizedReduction(\n+    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n+    mlir::TypedValue<mlir::VectorType> source, mlir::Value init_value,\n+    llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body) {\n+  int64_t rank = source.getType().getRank();\n+  int64_t minor_dim = rank - 1;\n+\n+  bool minor_dim_reduced = reduction_dims.back() == minor_dim;\n+  bool non_minor_dim_reduced = reduction_dims.size() > 1 || !minor_dim_reduced;\n+\n+  mlir::Value non_minor_result;\n+  if (non_minor_dim_reduced) {\n+    non_minor_result =\n+        EmitNonMinorReduction(builder, loc, result_type, source, reduction_dims,\n+                              body, minor_dim_reduced);\n+  }\n+  if (!minor_dim_reduced) {\n+    // We add the init value during the minor reduction loop, if that wasn't\n+    // done then we must apply it here.\n+    mlir::Value init_value_vector =\n+        builder.create<mlir::vector::BroadcastOp>(loc, result_type, init_value);\n+\n+    return VectorizeBody(builder, loc, body, non_minor_result,\n+                         init_value_vector);\n+  }\n+\n+  return EmitMinorReduction(builder, loc, result_type,\n+                            non_minor_result ? non_minor_result : source,\n+                            init_value, body);\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "fb4ef413ddfd1fc705666cfc64575c9d3a124cbe",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.h",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.h?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -0,0 +1,47 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_CODEGEN_TILED_TRANSFORMS_VECTORIZED_REDUCE_EMITTER_H_\n+#define XLA_BACKENDS_CPU_CODEGEN_TILED_TRANSFORMS_VECTORIZED_REDUCE_EMITTER_H_\n+\n+#include <cstdint>\n+\n+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/Value.h\"\n+\n+namespace xla::cpu {\n+\n+// Create a vectorized reduction of the given source vector.\n+//\n+// The implementation is as follows:\n+// 1. If the reduction dimension is only the most minor we convert it into a\n+//    nested scf.loop of horizonal reductions and if the body of the reduce is a\n+//    single binary operation that is supported by ReductionOp we use that,\n+//    otherwise we simply loop over the scalar values.\n+// 2. If the reduction dimensions does not include the most minor dimension, we\n+//    loop over the reductions dimensions and apply the body with vectorized\n+//    inputs.\n+// 3. If the dimensions are a combindation of minor & non-minor dimensions we\n+//    simply apply strategy 2 followed by strategy 1.\n+mlir::Value EmitVectorizedReduction(\n+    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n+    mlir::TypedValue<mlir::VectorType> source, mlir::Value init_value,\n+    llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body);\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_CODEGEN_TILED_TRANSFORMS_VECTORIZED_REDUCE_EMITTER_H_"
        },
        {
            "sha": "a3a66bc216ed03ec0335442bca90a12b393b89f0",
            "filename": "third_party/xla/xla/codegen/xtile/ir/xtile_ops.td",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de7a63363c1cb71beb7bd4d384a39101c17c9cec/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fxtile%2Fir%2Fxtile_ops.td?ref=de7a63363c1cb71beb7bd4d384a39101c17c9cec",
            "patch": "@@ -71,6 +71,7 @@ def TiledBufferInterface : OpInterface<\"TiledBufferInterface\"> {\n def EntryFuncOp : XTile_Op<\"entry_func\", [\n     Symbol,\n     IsolatedFromAbove,\n+    AutomaticAllocationScope,\n     FunctionOpInterface]>\n {\n   let summary = \"My custom entry function operation\";"
        }
    ],
    "stats": {
        "total": 777,
        "additions": 765,
        "deletions": 12
    }
}