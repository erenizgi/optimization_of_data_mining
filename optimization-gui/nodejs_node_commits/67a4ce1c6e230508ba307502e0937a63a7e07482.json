{
    "author": "davisjam",
    "message": "fs: partition readFile against pool exhaustion\n\nProblem:\n\nNode implements fs.readFile as:\n- a call to stat, then\n- a C++ -> libuv request to read the entire file using the stat size\n\nWhy is this bad?\nThe effect is to place on the libuv threadpool a potentially-large\nread request, occupying the libuv thread until it completes.\nWhile readFile certainly requires buffering the entire file contents,\nit can partition the read into smaller buffers\n(as is done on other read paths)\nalong the way to avoid threadpool exhaustion.\n\nIf the file is relatively large or stored on a slow medium, reading\nthe entire file in one shot seems particularly harmful,\nand presents a possible DoS vector.\n\nSolution:\n\nPartition the read into multiple smaller requests.\n\nConsiderations:\n\n1. Correctness\n\nI don't think partitioning the read like this raises\nany additional risk of read-write races on the FS.\nIf the application is concurrently readFile'ing and modifying the file,\nit will already see funny behavior. Though libuv uses preadv where\navailable, this doesn't guarantee read atomicity in the presence of\nconcurrent writes.\n\n2. Performance\n\nDownside: Partitioning means that a single large readFile will\n  require into many \"out and back\" requests to libuv,\n  introducing overhead.\nUpside: In between each \"out and back\", other work pending on the\n  threadpool can take a turn.\n\nIn short, although partitioning will slow down a large request,\nit will lead to better throughput if the threadpool is handling\nmore than one type of request.\n\nFixes: https://github.com/nodejs/node/issues/17047\n\nPR-URL: https://github.com/nodejs/node/pull/17054\nReviewed-By: Benjamin Gruenbaum <benjamingr@gmail.com>\nReviewed-By: Tiancheng \"Timothy\" Gu <timothygu99@gmail.com>\nReviewed-By: Gireesh Punathil <gpunathi@in.ibm.com>\nReviewed-By: James M Snell <jasnell@gmail.com>\nReviewed-By: Matteo Collina <matteo.collina@gmail.com>\nReviewed-By: Sakthipriyan Vairamani <thechargingvolcano@gmail.com>\nReviewed-By: Ruben Bridgewater <ruben@bridgewater.de>",
    "sha": "67a4ce1c6e230508ba307502e0937a63a7e07482",
    "files": [
        {
            "sha": "be3b7fd057bbe0af0ada9012aae52c70fea8f549",
            "filename": "benchmark/fs/readfile-partitioned.js",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/nodejs/node/blob/67a4ce1c6e230508ba307502e0937a63a7e07482/benchmark%2Ffs%2Freadfile-partitioned.js",
            "raw_url": "https://github.com/nodejs/node/raw/67a4ce1c6e230508ba307502e0937a63a7e07482/benchmark%2Ffs%2Freadfile-partitioned.js",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/benchmark%2Ffs%2Freadfile-partitioned.js?ref=67a4ce1c6e230508ba307502e0937a63a7e07482",
            "patch": "@@ -0,0 +1,86 @@\n+// Submit a mix of short and long jobs to the threadpool.\n+// Report total job throughput.\n+// If we partition the long job, overall job throughput goes up significantly.\n+// However, this comes at the cost of the long job throughput.\n+//\n+// Short jobs: small zip jobs.\n+// Long jobs: fs.readFile on a large file.\n+\n+'use strict';\n+\n+const path = require('path');\n+const common = require('../common.js');\n+const filename = path.resolve(__dirname,\n+                              `.removeme-benchmark-garbage-${process.pid}`);\n+const fs = require('fs');\n+const zlib = require('zlib');\n+const assert = require('assert');\n+\n+const bench = common.createBenchmark(main, {\n+  dur: [5],\n+  len: [1024, 16 * 1024 * 1024],\n+  concurrent: [1, 10]\n+});\n+\n+function main(conf) {\n+  const len = +conf.len;\n+  try { fs.unlinkSync(filename); } catch (e) {}\n+  var data = Buffer.alloc(len, 'x');\n+  fs.writeFileSync(filename, data);\n+  data = null;\n+\n+  var zipData = Buffer.alloc(1024, 'a');\n+\n+  var reads = 0;\n+  var zips = 0;\n+  var benchEnded = false;\n+  bench.start();\n+  setTimeout(function() {\n+    const totalOps = reads + zips;\n+    benchEnded = true;\n+    bench.end(totalOps);\n+    try { fs.unlinkSync(filename); } catch (e) {}\n+  }, +conf.dur * 1000);\n+\n+  function read() {\n+    fs.readFile(filename, afterRead);\n+  }\n+\n+  function afterRead(er, data) {\n+    if (er) {\n+      if (er.code === 'ENOENT') {\n+        // Only OK if unlinked by the timer from main.\n+        assert.ok(benchEnded);\n+        return;\n+      }\n+      throw er;\n+    }\n+\n+    if (data.length !== len)\n+      throw new Error('wrong number of bytes returned');\n+\n+    reads++;\n+    if (!benchEnded)\n+      read();\n+  }\n+\n+  function zip() {\n+    zlib.deflate(zipData, afterZip);\n+  }\n+\n+  function afterZip(er, data) {\n+    if (er)\n+      throw er;\n+\n+    zips++;\n+    if (!benchEnded)\n+      zip();\n+  }\n+\n+  // Start reads\n+  var cur = +conf.concurrent;\n+  while (cur--) read();\n+\n+  // Start a competing zip\n+  zip();\n+}"
        },
        {
            "sha": "282b4ac762134072098ece35235ba70e53be1e58",
            "filename": "benchmark/fs/readfile.js",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/nodejs/node/blob/67a4ce1c6e230508ba307502e0937a63a7e07482/benchmark%2Ffs%2Freadfile.js",
            "raw_url": "https://github.com/nodejs/node/raw/67a4ce1c6e230508ba307502e0937a63a7e07482/benchmark%2Ffs%2Freadfile.js",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/benchmark%2Ffs%2Freadfile.js?ref=67a4ce1c6e230508ba307502e0937a63a7e07482",
            "patch": "@@ -8,6 +8,7 @@ const common = require('../common.js');\n const filename = path.resolve(process.env.NODE_TMPDIR || __dirname,\n                               `.removeme-benchmark-garbage-${process.pid}`);\n const fs = require('fs');\n+const assert = require('assert');\n \n const bench = common.createBenchmark(main, {\n   dur: [5],\n@@ -22,10 +23,10 @@ function main({ len, dur, concurrent }) {\n   data = null;\n \n   var reads = 0;\n-  var bench_ended = false;\n+  var benchEnded = false;\n   bench.start();\n   setTimeout(function() {\n-    bench_ended = true;\n+    benchEnded = true;\n     bench.end(reads);\n     try { fs.unlinkSync(filename); } catch (e) {}\n     process.exit(0);\n@@ -36,14 +37,20 @@ function main({ len, dur, concurrent }) {\n   }\n \n   function afterRead(er, data) {\n-    if (er)\n+    if (er) {\n+      if (er.code === 'ENOENT') {\n+        // Only OK if unlinked by the timer from main.\n+        assert.ok(benchEnded);\n+        return;\n+      }\n       throw er;\n+    }\n \n     if (data.length !== len)\n       throw new Error('wrong number of bytes returned');\n \n     reads++;\n-    if (!bench_ended)\n+    if (!benchEnded)\n       read();\n   }\n "
        },
        {
            "sha": "00c6d946a0b355136a186b347f6111936190c455",
            "filename": "doc/api/fs.md",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/nodejs/node/blob/67a4ce1c6e230508ba307502e0937a63a7e07482/doc%2Fapi%2Ffs.md",
            "raw_url": "https://github.com/nodejs/node/raw/67a4ce1c6e230508ba307502e0937a63a7e07482/doc%2Fapi%2Ffs.md",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/doc%2Fapi%2Ffs.md?ref=67a4ce1c6e230508ba307502e0937a63a7e07482",
            "patch": "@@ -2250,10 +2250,9 @@ Any specified file descriptor has to support reading.\n *Note*: If a file descriptor is specified as the `path`, it will not be closed\n automatically.\n \n-*Note*: `fs.readFile()` reads the entire file in a single threadpool request.\n-To minimize threadpool task length variation, prefer the partitioned APIs\n-`fs.read()` and `fs.createReadStream()` when reading files as part of\n-fulfilling a client request.\n+*Note*: `fs.readFile()` buffers the entire file.\n+To minimize memory costs, when possible prefer streaming via\n+`fs.createReadStream()`.\n \n ## fs.readFileSync(path[, options])\n <!-- YAML"
        },
        {
            "sha": "0b9cf0cc9b819011204253e9dcaf43b7325d6160",
            "filename": "lib/fs.js",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/nodejs/node/blob/67a4ce1c6e230508ba307502e0937a63a7e07482/lib%2Ffs.js",
            "raw_url": "https://github.com/nodejs/node/raw/67a4ce1c6e230508ba307502e0937a63a7e07482/lib%2Ffs.js",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/lib%2Ffs.js?ref=67a4ce1c6e230508ba307502e0937a63a7e07482",
            "patch": "@@ -508,7 +508,7 @@ ReadFileContext.prototype.read = function() {\n   } else {\n     buffer = this.buffer;\n     offset = this.pos;\n-    length = this.size - this.pos;\n+    length = Math.min(kReadFileBufferLength, this.size - this.pos);\n   }\n \n   var req = new FSReqWrap();"
        },
        {
            "sha": "689f98ff849dedd421ae69963a007a18298ad542",
            "filename": "test/parallel/test-fs-readfile.js",
            "status": "added",
            "additions": 58,
            "deletions": 0,
            "changes": 58,
            "blob_url": "https://github.com/nodejs/node/blob/67a4ce1c6e230508ba307502e0937a63a7e07482/test%2Fparallel%2Ftest-fs-readfile.js",
            "raw_url": "https://github.com/nodejs/node/raw/67a4ce1c6e230508ba307502e0937a63a7e07482/test%2Fparallel%2Ftest-fs-readfile.js",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/test%2Fparallel%2Ftest-fs-readfile.js?ref=67a4ce1c6e230508ba307502e0937a63a7e07482",
            "patch": "@@ -0,0 +1,58 @@\n+'use strict';\n+const common = require('../common');\n+\n+// This test ensures that fs.readFile correctly returns the\n+// contents of varying-sized files.\n+\n+const assert = require('assert');\n+const fs = require('fs');\n+const path = require('path');\n+\n+const prefix = `.removeme-fs-readfile-${process.pid}`;\n+\n+common.refreshTmpDir();\n+\n+const fileInfo = [\n+  { name: path.join(common.tmpDir, `${prefix}-1K.txt`),\n+    len: 1024,\n+  },\n+  { name: path.join(common.tmpDir, `${prefix}-64K.txt`),\n+    len: 64 * 1024,\n+  },\n+  { name: path.join(common.tmpDir, `${prefix}-64KLessOne.txt`),\n+    len: (64 * 1024) - 1,\n+  },\n+  { name: path.join(common.tmpDir, `${prefix}-1M.txt`),\n+    len: 1 * 1024 * 1024,\n+  },\n+  { name: path.join(common.tmpDir, `${prefix}-1MPlusOne.txt`),\n+    len: (1 * 1024 * 1024) + 1,\n+  },\n+];\n+\n+// Populate each fileInfo (and file) with unique fill.\n+const sectorSize = 512;\n+for (const e of fileInfo) {\n+  e.contents = Buffer.allocUnsafe(e.len);\n+\n+  // This accounts for anything unusual in Node's implementation of readFile.\n+  // Using e.g. 'aa...aa' would miss bugs like Node re-reading\n+  // the same section twice instead of two separate sections.\n+  for (let offset = 0; offset < e.len; offset += sectorSize) {\n+    const fillByte = 256 * Math.random();\n+    const nBytesToFill = Math.min(sectorSize, e.len - offset);\n+    e.contents.fill(fillByte, offset, offset + nBytesToFill);\n+  }\n+\n+  fs.writeFileSync(e.name, e.contents);\n+}\n+// All files are now populated.\n+\n+// Test readFile on each size.\n+for (const e of fileInfo) {\n+  fs.readFile(e.name, common.mustCall((err, buf) => {\n+    console.log(`Validating readFile on file ${e.name} of length ${e.len}`);\n+    assert.ifError(err, 'An error occurred');\n+    assert.deepStrictEqual(buf, e.contents, 'Incorrect file contents');\n+  }));\n+}"
        }
    ],
    "stats": {
        "total": 168,
        "additions": 159,
        "deletions": 9
    }
}