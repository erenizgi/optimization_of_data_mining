{
    "author": "addaleax",
    "message": "src: unify thread pool work\n\nInstead of using the libuv mechanism directly, provide an internal\n`ThreadPoolWork` wrapper that takes care of increasing/decreasing\nthe waiting request counter.\n\nPR-URL: https://github.com/nodejs/node/pull/19377\nReviewed-By: Ben Noordhuis <info@bnoordhuis.nl>\nReviewed-By: James M Snell <jasnell@gmail.com>",
    "sha": "c0720570498895d06dcec4e8f01e8922a81ac78b",
    "files": [
        {
            "sha": "b456ade2d4ca744ad66ea5c6c14f9024dd55cb0b",
            "filename": "src/node_api.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 45,
            "changes": 73,
            "blob_url": "https://github.com/nodejs/node/blob/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_api.cc",
            "raw_url": "https://github.com/nodejs/node/raw/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_api.cc",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/src%2Fnode_api.cc?ref=c0720570498895d06dcec4e8f01e8922a81ac78b",
            "patch": "@@ -3338,7 +3338,7 @@ static napi_status ConvertUVErrorCode(int code) {\n }\n \n // Wrapper around uv_work_t which calls user-provided callbacks.\n-class Work : public node::AsyncResource {\n+class Work : public node::AsyncResource, public node::ThreadPoolWork {\n  private:\n   explicit Work(napi_env env,\n                 v8::Local<v8::Object> async_resource,\n@@ -3349,15 +3349,14 @@ class Work : public node::AsyncResource {\n     : AsyncResource(env->isolate,\n                     async_resource,\n                     *v8::String::Utf8Value(env->isolate, async_resource_name)),\n-    _env(env),\n-    _data(data),\n-    _execute(execute),\n-    _complete(complete) {\n-    memset(&_request, 0, sizeof(_request));\n-    _request.data = this;\n+      ThreadPoolWork(node::Environment::GetCurrent(env->isolate)),\n+      _env(env),\n+      _data(data),\n+      _execute(execute),\n+      _complete(complete) {\n   }\n \n-  ~Work() { }\n+  virtual ~Work() { }\n \n  public:\n   static Work* New(napi_env env,\n@@ -3374,47 +3373,36 @@ class Work : public node::AsyncResource {\n     delete work;\n   }\n \n-  static void ExecuteCallback(uv_work_t* req) {\n-    Work* work = static_cast<Work*>(req->data);\n-    work->_execute(work->_env, work->_data);\n+  void DoThreadPoolWork() override {\n+    _execute(_env, _data);\n   }\n \n-  static void CompleteCallback(uv_work_t* req, int status) {\n-    Work* work = static_cast<Work*>(req->data);\n+  void AfterThreadPoolWork(int status) {\n+    if (_complete == nullptr)\n+      return;\n \n-    if (work->_complete != nullptr) {\n-      napi_env env = work->_env;\n+    // Establish a handle scope here so that every callback doesn't have to.\n+    // Also it is needed for the exception-handling below.\n+    v8::HandleScope scope(_env->isolate);\n \n-      // Establish a handle scope here so that every callback doesn't have to.\n-      // Also it is needed for the exception-handling below.\n-      v8::HandleScope scope(env->isolate);\n-      node::Environment* env_ = node::Environment::GetCurrent(env->isolate);\n-      env_->DecreaseWaitingRequestCounter();\n+    CallbackScope callback_scope(this);\n \n-      CallbackScope callback_scope(work);\n+    NAPI_CALL_INTO_MODULE(_env,\n+        _complete(_env, ConvertUVErrorCode(status), _data),\n+        [this] (v8::Local<v8::Value> local_err) {\n+          // If there was an unhandled exception in the complete callback,\n+          // report it as a fatal exception. (There is no JavaScript on the\n+          // callstack that can possibly handle it.)\n+          v8impl::trigger_fatal_exception(_env, local_err);\n+        });\n \n-      NAPI_CALL_INTO_MODULE(env,\n-          work->_complete(env, ConvertUVErrorCode(status), work->_data),\n-          [env] (v8::Local<v8::Value> local_err) {\n-            // If there was an unhandled exception in the complete callback,\n-            // report it as a fatal exception. (There is no JavaScript on the\n-            // callstack that can possibly handle it.)\n-            v8impl::trigger_fatal_exception(env, local_err);\n-          });\n-\n-      // Note: Don't access `work` after this point because it was\n-      // likely deleted by the complete callback.\n-    }\n-  }\n-\n-  uv_work_t* Request() {\n-    return &_request;\n+    // Note: Don't access `work` after this point because it was\n+    // likely deleted by the complete callback.\n   }\n \n  private:\n   napi_env _env;\n   void* _data;\n-  uv_work_t _request;\n   napi_async_execute_callback _execute;\n   napi_async_complete_callback _complete;\n };\n@@ -3491,12 +3479,7 @@ napi_status napi_queue_async_work(napi_env env, napi_async_work work) {\n \n   uvimpl::Work* w = reinterpret_cast<uvimpl::Work*>(work);\n \n-  node::Environment* env_ = node::Environment::GetCurrent(env->isolate);\n-  env_->IncreaseWaitingRequestCounter();\n-  CALL_UV(env, uv_queue_work(event_loop,\n-                             w->Request(),\n-                             uvimpl::Work::ExecuteCallback,\n-                             uvimpl::Work::CompleteCallback));\n+  w->ScheduleWork();\n \n   return napi_clear_last_error(env);\n }\n@@ -3507,7 +3490,7 @@ napi_status napi_cancel_async_work(napi_env env, napi_async_work work) {\n \n   uvimpl::Work* w = reinterpret_cast<uvimpl::Work*>(work);\n \n-  CALL_UV(env, uv_cancel(reinterpret_cast<uv_req_t*>(w->Request())));\n+  CALL_UV(env, w->CancelWork());\n \n   return napi_clear_last_error(env);\n }"
        },
        {
            "sha": "8235b8b01ca4c65326996716a57da647c1a66ddd",
            "filename": "src/node_crypto.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 69,
            "changes": 99,
            "blob_url": "https://github.com/nodejs/node/blob/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_crypto.cc",
            "raw_url": "https://github.com/nodejs/node/raw/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_crypto.cc",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/src%2Fnode_crypto.cc?ref=c0720570498895d06dcec4e8f01e8922a81ac78b",
            "patch": "@@ -4556,7 +4556,7 @@ bool ECDH::IsKeyPairValid() {\n }\n \n \n-class PBKDF2Request : public AsyncWrap {\n+class PBKDF2Request : public AsyncWrap, public ThreadPoolWork {\n  public:\n   PBKDF2Request(Environment* env,\n                 Local<Object> object,\n@@ -4566,6 +4566,7 @@ class PBKDF2Request : public AsyncWrap {\n                 int keylen,\n                 int iteration_count)\n       : AsyncWrap(env, object, AsyncWrap::PROVIDER_PBKDF2REQUEST),\n+        ThreadPoolWork(env),\n         digest_(digest),\n         success_(false),\n         pass_(std::move(pass)),\n@@ -4574,21 +4575,14 @@ class PBKDF2Request : public AsyncWrap {\n         iteration_count_(iteration_count) {\n   }\n \n-  uv_work_t* work_req() {\n-    return &work_req_;\n-  }\n-\n   size_t self_size() const override { return sizeof(*this); }\n \n-  static void Work(uv_work_t* work_req);\n-  void Work();\n+  void DoThreadPoolWork() override;\n+  void AfterThreadPoolWork(int status) override;\n \n-  static void After(uv_work_t* work_req, int status);\n   void After(Local<Value> (*argv)[2]);\n-  void After();\n \n  private:\n-  uv_work_t work_req_;\n   const EVP_MD* digest_;\n   bool success_;\n   MallocedBuffer<char> pass_;\n@@ -4598,7 +4592,7 @@ class PBKDF2Request : public AsyncWrap {\n };\n \n \n-void PBKDF2Request::Work() {\n+void PBKDF2Request::DoThreadPoolWork() {\n   success_ =\n       PKCS5_PBKDF2_HMAC(\n           pass_.data, pass_.size,\n@@ -4611,12 +4605,6 @@ void PBKDF2Request::Work() {\n }\n \n \n-void PBKDF2Request::Work(uv_work_t* work_req) {\n-  PBKDF2Request* req = ContainerOf(&PBKDF2Request::work_req_, work_req);\n-  req->Work();\n-}\n-\n-\n void PBKDF2Request::After(Local<Value> (*argv)[2]) {\n   if (success_) {\n     (*argv)[0] = Null(env()->isolate());\n@@ -4629,7 +4617,12 @@ void PBKDF2Request::After(Local<Value> (*argv)[2]) {\n }\n \n \n-void PBKDF2Request::After() {\n+void PBKDF2Request::AfterThreadPoolWork(int status) {\n+  std::unique_ptr<PBKDF2Request> req(this);\n+  if (status == UV_ECANCELED)\n+    return;\n+  CHECK_EQ(status, 0);\n+\n   HandleScope handle_scope(env()->isolate());\n   Context::Scope context_scope(env()->context());\n   Local<Value> argv[2];\n@@ -4638,17 +4631,6 @@ void PBKDF2Request::After() {\n }\n \n \n-void PBKDF2Request::After(uv_work_t* work_req, int status) {\n-  std::unique_ptr<PBKDF2Request> req(\n-      ContainerOf(&PBKDF2Request::work_req_, work_req));\n-  req->env()->DecreaseWaitingRequestCounter();\n-  if (status == UV_ECANCELED)\n-    return;\n-  CHECK_EQ(status, 0);\n-  req->After();\n-}\n-\n-\n void PBKDF2(const FunctionCallbackInfo<Value>& args) {\n   Environment* env = Environment::GetCurrent(args);\n \n@@ -4695,14 +4677,10 @@ void PBKDF2(const FunctionCallbackInfo<Value>& args) {\n   if (args[5]->IsFunction()) {\n     obj->Set(env->context(), env->ondone_string(), args[5]).FromJust();\n \n-    env->IncreaseWaitingRequestCounter();\n-    uv_queue_work(env->event_loop(),\n-                  req.release()->work_req(),\n-                  PBKDF2Request::Work,\n-                  PBKDF2Request::After);\n+    req.release()->ScheduleWork();\n   } else {\n     env->PrintSyncTrace();\n-    req->Work();\n+    req->DoThreadPoolWork();\n     Local<Value> argv[2];\n     req->After(&argv);\n \n@@ -4715,7 +4693,7 @@ void PBKDF2(const FunctionCallbackInfo<Value>& args) {\n \n \n // Only instantiate within a valid HandleScope.\n-class RandomBytesRequest : public AsyncWrap {\n+class RandomBytesRequest : public AsyncWrap, public ThreadPoolWork {\n  public:\n   enum FreeMode { FREE_DATA, DONT_FREE_DATA };\n \n@@ -4725,16 +4703,13 @@ class RandomBytesRequest : public AsyncWrap {\n                      char* data,\n                      FreeMode free_mode)\n       : AsyncWrap(env, object, AsyncWrap::PROVIDER_RANDOMBYTESREQUEST),\n+        ThreadPoolWork(env),\n         error_(0),\n         size_(size),\n         data_(data),\n         free_mode_(free_mode) {\n   }\n \n-  uv_work_t* work_req() {\n-    return &work_req_;\n-  }\n-\n   inline size_t size() const {\n     return size_;\n   }\n@@ -4772,7 +4747,8 @@ class RandomBytesRequest : public AsyncWrap {\n \n   size_t self_size() const override { return sizeof(*this); }\n \n-  uv_work_t work_req_;\n+  void DoThreadPoolWork() override;\n+  void AfterThreadPoolWork(int status) override;\n \n  private:\n   unsigned long error_;  // NOLINT(runtime/int)\n@@ -4782,21 +4758,17 @@ class RandomBytesRequest : public AsyncWrap {\n };\n \n \n-void RandomBytesWork(uv_work_t* work_req) {\n-  RandomBytesRequest* req =\n-      ContainerOf(&RandomBytesRequest::work_req_, work_req);\n-\n+void RandomBytesRequest::DoThreadPoolWork() {\n   // Ensure that OpenSSL's PRNG is properly seeded.\n   CheckEntropy();\n \n-  const int r = RAND_bytes(reinterpret_cast<unsigned char*>(req->data()),\n-                           req->size());\n+  const int r = RAND_bytes(reinterpret_cast<unsigned char*>(data_), size_);\n \n   // RAND_bytes() returns 0 on error.\n   if (r == 0) {\n-    req->set_error(ERR_get_error());  // NOLINT(runtime/int)\n+    set_error(ERR_get_error());  // NOLINT(runtime/int)\n   } else if (r == -1) {\n-    req->set_error(static_cast<unsigned long>(-1));  // NOLINT(runtime/int)\n+    set_error(static_cast<unsigned long>(-1));  // NOLINT(runtime/int)\n   }\n }\n \n@@ -4834,27 +4806,24 @@ void RandomBytesCheck(RandomBytesRequest* req, Local<Value> (*argv)[2]) {\n }\n \n \n-void RandomBytesAfter(uv_work_t* work_req, int status) {\n-  std::unique_ptr<RandomBytesRequest> req(\n-      ContainerOf(&RandomBytesRequest::work_req_, work_req));\n-  Environment* env = req->env();\n-  env->DecreaseWaitingRequestCounter();\n+void RandomBytesRequest::AfterThreadPoolWork(int status) {\n+  std::unique_ptr<RandomBytesRequest> req(this);\n   if (status == UV_ECANCELED)\n     return;\n   CHECK_EQ(status, 0);\n-  HandleScope handle_scope(env->isolate());\n-  Context::Scope context_scope(env->context());\n+  HandleScope handle_scope(env()->isolate());\n+  Context::Scope context_scope(env()->context());\n   Local<Value> argv[2];\n-  RandomBytesCheck(req.get(), &argv);\n-  req->MakeCallback(env->ondone_string(), arraysize(argv), argv);\n+  RandomBytesCheck(this, &argv);\n+  MakeCallback(env()->ondone_string(), arraysize(argv), argv);\n }\n \n \n void RandomBytesProcessSync(Environment* env,\n                             std::unique_ptr<RandomBytesRequest> req,\n                             Local<Value> (*argv)[2]) {\n   env->PrintSyncTrace();\n-  RandomBytesWork(req->work_req());\n+  req->DoThreadPoolWork();\n   RandomBytesCheck(req.get(), argv);\n \n   if (!(*argv)[0]->IsNull())\n@@ -4881,11 +4850,7 @@ void RandomBytes(const FunctionCallbackInfo<Value>& args) {\n   if (args[1]->IsFunction()) {\n     obj->Set(env->context(), env->ondone_string(), args[1]).FromJust();\n \n-    env->IncreaseWaitingRequestCounter();\n-    uv_queue_work(env->event_loop(),\n-                  req.release()->work_req(),\n-                  RandomBytesWork,\n-                  RandomBytesAfter);\n+    req.release()->ScheduleWork();\n     args.GetReturnValue().Set(obj);\n   } else {\n     Local<Value> argv[2];\n@@ -4921,11 +4886,7 @@ void RandomBytesBuffer(const FunctionCallbackInfo<Value>& args) {\n   if (args[3]->IsFunction()) {\n     obj->Set(env->context(), env->ondone_string(), args[3]).FromJust();\n \n-    env->IncreaseWaitingRequestCounter();\n-    uv_queue_work(env->event_loop(),\n-                  req.release()->work_req(),\n-                  RandomBytesWork,\n-                  RandomBytesAfter);\n+    req.release()->ScheduleWork();\n     args.GetReturnValue().Set(obj);\n   } else {\n     Local<Value> argv[2];"
        },
        {
            "sha": "8aa46318803985a27a0e02ed3a8fe19f22ae909c",
            "filename": "src/node_internals.h",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/nodejs/node/blob/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_internals.h",
            "raw_url": "https://github.com/nodejs/node/raw/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_internals.h",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/src%2Fnode_internals.h?ref=c0720570498895d06dcec4e8f01e8922a81ac78b",
            "patch": "@@ -503,6 +503,41 @@ class InternalCallbackScope {\n   bool closed_ = false;\n };\n \n+class ThreadPoolWork {\n+ public:\n+  explicit inline ThreadPoolWork(Environment* env) : env_(env) {}\n+  inline void ScheduleWork();\n+  inline int CancelWork();\n+\n+  virtual void DoThreadPoolWork() = 0;\n+  virtual void AfterThreadPoolWork(int status) = 0;\n+\n+ private:\n+  Environment* env_;\n+  uv_work_t work_req_;\n+};\n+\n+void ThreadPoolWork::ScheduleWork() {\n+  env_->IncreaseWaitingRequestCounter();\n+  int status = uv_queue_work(\n+      env_->event_loop(),\n+      &work_req_,\n+      [](uv_work_t* req) {\n+        ThreadPoolWork* self = ContainerOf(&ThreadPoolWork::work_req_, req);\n+        self->DoThreadPoolWork();\n+      },\n+      [](uv_work_t* req, int status) {\n+        ThreadPoolWork* self = ContainerOf(&ThreadPoolWork::work_req_, req);\n+        self->env_->DecreaseWaitingRequestCounter();\n+        self->AfterThreadPoolWork(status);\n+      });\n+  CHECK_EQ(status, 0);\n+}\n+\n+int ThreadPoolWork::CancelWork() {\n+  return uv_cancel(reinterpret_cast<uv_req_t*>(&work_req_));\n+}\n+\n static inline const char *errno_string(int errorno) {\n #define ERRNO_CASE(e)  case e: return #e;\n   switch (errorno) {"
        },
        {
            "sha": "c77e6d3297df5d4549dad13467a6247f7de96565",
            "filename": "src/node_zlib.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/nodejs/node/blob/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_zlib.cc",
            "raw_url": "https://github.com/nodejs/node/raw/c0720570498895d06dcec4e8f01e8922a81ac78b/src%2Fnode_zlib.cc",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/src%2Fnode_zlib.cc?ref=c0720570498895d06dcec4e8f01e8922a81ac78b",
            "patch": "@@ -70,10 +70,11 @@ enum node_zlib_mode {\n /**\n  * Deflate/Inflate\n  */\n-class ZCtx : public AsyncWrap {\n+class ZCtx : public AsyncWrap, public ThreadPoolWork {\n  public:\n   ZCtx(Environment* env, Local<Object> wrap, node_zlib_mode mode)\n       : AsyncWrap(env, wrap, AsyncWrap::PROVIDER_ZLIB),\n+        ThreadPoolWork(env),\n         dictionary_(nullptr),\n         dictionary_len_(0),\n         err_(0),\n@@ -191,9 +192,6 @@ class ZCtx : public AsyncWrap {\n     CHECK(Buffer::IsWithinBounds(out_off, out_len, Buffer::Length(out_buf)));\n     out = reinterpret_cast<Bytef *>(Buffer::Data(out_buf) + out_off);\n \n-    // build up the work request\n-    uv_work_t* work_req = &(ctx->work_req_);\n-\n     ctx->strm_.avail_in = in_len;\n     ctx->strm_.next_in = in;\n     ctx->strm_.avail_out = out_len;\n@@ -203,7 +201,7 @@ class ZCtx : public AsyncWrap {\n     if (!async) {\n       // sync version\n       env->PrintSyncTrace();\n-      Process(work_req);\n+      ctx->DoThreadPoolWork();\n       if (CheckError(ctx)) {\n         ctx->write_result_[0] = ctx->strm_.avail_out;\n         ctx->write_result_[1] = ctx->strm_.avail_in;\n@@ -214,18 +212,24 @@ class ZCtx : public AsyncWrap {\n     }\n \n     // async version\n-    env->IncreaseWaitingRequestCounter();\n-    uv_queue_work(env->event_loop(), work_req, ZCtx::Process, ZCtx::After);\n+    ctx->ScheduleWork();\n   }\n \n+  // TODO(addaleax): Make these methods non-static. It's a significant bunch\n+  // of churn that's better left for a separate PR.\n+  void DoThreadPoolWork() {\n+    Process(this);\n+  }\n+\n+  void AfterThreadPoolWork(int status) {\n+    After(this, status);\n+  }\n \n   // thread pool!\n   // This function may be called multiple times on the uv_work pool\n   // for a single write() call, until all of the input bytes have\n   // been consumed.\n-  static void Process(uv_work_t* work_req) {\n-    ZCtx *ctx = ContainerOf(&ZCtx::work_req_, work_req);\n-\n+  static void Process(ZCtx* ctx) {\n     const Bytef* next_expected_header_byte = nullptr;\n \n     // If the avail_out is left at 0, then it means that it ran out\n@@ -361,12 +365,10 @@ class ZCtx : public AsyncWrap {\n \n \n   // v8 land!\n-  static void After(uv_work_t* work_req, int status) {\n-    ZCtx* ctx = ContainerOf(&ZCtx::work_req_, work_req);\n+  static void After(ZCtx* ctx, int status) {\n     Environment* env = ctx->env();\n     ctx->write_in_progress_ = false;\n \n-    env->DecreaseWaitingRequestCounter();\n     if (status == UV_ECANCELED) {\n       ctx->Close();\n       return;\n@@ -685,7 +687,6 @@ class ZCtx : public AsyncWrap {\n   int strategy_;\n   z_stream strm_;\n   int windowBits_;\n-  uv_work_t work_req_;\n   bool write_in_progress_;\n   bool pending_close_;\n   unsigned int refs_;"
        }
    ],
    "stats": {
        "total": 236,
        "additions": 108,
        "deletions": 128
    }
}