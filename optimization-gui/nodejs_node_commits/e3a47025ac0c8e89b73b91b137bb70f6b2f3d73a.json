{
    "author": "addaleax",
    "message": "fs: reduce memory retention when streaming small files\n\nFixes: https://github.com/nodejs/node/issues/21967\n\nPR-URL: https://github.com/nodejs/node/pull/21968\nReviewed-By: Сковорода Никита Андреевич <chalkerx@gmail.com>\nReviewed-By: Matteo Collina <matteo.collina@gmail.com>\nReviewed-By: Benjamin Gruenbaum <benjamingr@gmail.com>\nReviewed-By: Trivikram Kamat <trivikr.dev@gmail.com>\nReviewed-By: James M Snell <jasnell@gmail.com>",
    "sha": "e3a47025ac0c8e89b73b91b137bb70f6b2f3d73a",
    "files": [
        {
            "sha": "92bd9a4c15fa236496189e53a786eecbfd79761b",
            "filename": "lib/internal/fs/streams.js",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/nodejs/node/blob/e3a47025ac0c8e89b73b91b137bb70f6b2f3d73a/lib%2Finternal%2Ffs%2Fstreams.js",
            "raw_url": "https://github.com/nodejs/node/raw/e3a47025ac0c8e89b73b91b137bb70f6b2f3d73a/lib%2Finternal%2Ffs%2Fstreams.js",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/lib%2Finternal%2Ffs%2Fstreams.js?ref=e3a47025ac0c8e89b73b91b137bb70f6b2f3d73a",
            "patch": "@@ -21,9 +21,18 @@ const util = require('util');\n const kMinPoolSpace = 128;\n \n let pool;\n+// It can happen that we expect to read a large chunk of data, and reserve\n+// a large chunk of the pool accordingly, but the read() call only filled\n+// a portion of it. If a concurrently executing read() then uses the same pool,\n+// the \"reserved\" portion cannot be used, so we allow it to be re-used as a\n+// new pool later.\n+const poolFragments = [];\n \n function allocNewPool(poolSize) {\n-  pool = Buffer.allocUnsafe(poolSize);\n+  if (poolFragments.length > 0)\n+    pool = poolFragments.pop();\n+  else\n+    pool = Buffer.allocUnsafe(poolSize);\n   pool.used = 0;\n }\n \n@@ -171,6 +180,14 @@ ReadStream.prototype._read = function(n) {\n       this.emit('error', er);\n     } else {\n       let b = null;\n+      // Now that we know how much data we have actually read, re-wind the\n+      // 'used' field if we can, and otherwise allow the remainder of our\n+      // reservation to be used as a new pool later.\n+      if (start + toRead === thisPool.used && thisPool === pool)\n+        thisPool.used += bytesRead - toRead;\n+      else if (toRead - bytesRead > kMinPoolSpace)\n+        poolFragments.push(thisPool.slice(start + bytesRead, start + toRead));\n+\n       if (bytesRead > 0) {\n         this.bytesRead += bytesRead;\n         b = thisPool.slice(start, start + bytesRead);"
        },
        {
            "sha": "32a6cd62363f05064f16a71a83c2ae66195938fc",
            "filename": "test/parallel/test-fs-read-stream-concurrent-reads.js",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/nodejs/node/blob/e3a47025ac0c8e89b73b91b137bb70f6b2f3d73a/test%2Fparallel%2Ftest-fs-read-stream-concurrent-reads.js",
            "raw_url": "https://github.com/nodejs/node/raw/e3a47025ac0c8e89b73b91b137bb70f6b2f3d73a/test%2Fparallel%2Ftest-fs-read-stream-concurrent-reads.js",
            "contents_url": "https://api.github.com/repos/nodejs/node/contents/test%2Fparallel%2Ftest-fs-read-stream-concurrent-reads.js?ref=e3a47025ac0c8e89b73b91b137bb70f6b2f3d73a",
            "patch": "@@ -0,0 +1,47 @@\n+'use strict';\n+const common = require('../common');\n+const fixtures = require('../common/fixtures');\n+const assert = require('assert');\n+const fs = require('fs');\n+\n+// Test that concurrent file read streams don’t interfere with each other’s\n+// contents, and that the chunks generated by the reads only retain a\n+// 'reasonable' amount of memory.\n+\n+// Refs: https://github.com/nodejs/node/issues/21967\n+\n+const filename = fixtures.path('loop.js');  // Some small non-homogeneous file.\n+const content = fs.readFileSync(filename);\n+\n+const N = 1000;\n+let started = 0;\n+let done = 0;\n+\n+const arrayBuffers = new Set();\n+\n+function startRead() {\n+  ++started;\n+  const chunks = [];\n+  fs.createReadStream(filename)\n+    .on('data', (chunk) => {\n+      chunks.push(chunk);\n+      arrayBuffers.add(chunk.buffer);\n+      if (started < N)\n+        startRead();\n+    })\n+    .on('end', common.mustCall(() => {\n+      assert.deepStrictEqual(Buffer.concat(chunks), content);\n+      if (++done === N) {\n+        const retainedMemory =\n+          [...arrayBuffers].map((ab) => ab.byteLength).reduce((a, b) => a + b);\n+        assert(retainedMemory / (N * content.length) <= 3,\n+               `Retaining ${retainedMemory} bytes in ABs for ${N} ` +\n+               `chunks of size ${content.length}`);\n+      }\n+    }));\n+}\n+\n+// Don’t start the reads all at once – that way we would have to allocate\n+// a large amount of memory upfront.\n+for (let i = 0; i < 4; ++i)\n+  startRead();"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 65,
        "deletions": 1
    }
}